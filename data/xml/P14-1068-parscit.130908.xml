<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000095">
<title confidence="0.993908">
Learning Grounded Meaning Representations with Autoencoders
</title>
<author confidence="0.990706">
Carina Silberer and Mirella Lapata
</author>
<affiliation confidence="0.998984">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.992341">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.998536">
c.silberer@ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993939" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999048307692308">
In this paper we address the problem of
grounding distributional representations of
lexical meaning. We introduce a new
model which uses stacked autoencoders to
learn higher-level embeddings from tex-
tual and visual input. The two modali-
ties are encoded as vectors of attributes
and are obtained automatically from text
and images, respectively. We evaluate our
model on its ability to simulate similar-
ity judgments and concept categorization.
On both tasks, our approach outperforms
baselines and related models.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999000560606061">
Recent years have seen a surge of interest in sin-
gle word vector spaces (Turney and Pantel, 2010;
Collobert et al., 2011; Mikolov et al., 2013) and
their successful use in many natural language ap-
plications. Examples include information retrieval
(Manning et al., 2008), search query expansions
(Jones et al., 2006), document classification (Se-
bastiani, 2002), and question answering (Yih et al.,
2013). Vector spaces have been also popular in
cognitive science figuring prominently in simula-
tions of human behavior involving semantic prim-
ing, deep dyslexia, text comprehension, synonym
selection, and similarity judgments (see Griffiths
et al., 2007). In general, these models specify
mechanisms for constructing semantic representa-
tions from text corpora based on the distributional
hypothesis (Harris, 1970): words that appear in
similar linguistic contexts are likely to have related
meanings.
Word meaning, however, is also tied to the
physical world. Words are grounded in the exter-
nal environment and relate to sensorimotor experi-
ence (Regier, 1996; Landau et al., 1998; Barsalou,
2008). To account for this, new types of perceptu-
ally grounded distributional models have emerged.
These models learn the meaning of words based
on textual and perceptual input. The latter is ap-
proximated by feature norms elicited from humans
(Andrews et al., 2009; Steyvers, 2010; Silberer
and Lapata, 2012), visual information extracted
automatically from images, (Feng and Lapata,
2010; Bruni et al., 2012a; Silberer et al., 2013)
or a combination of both (Roller and Schulte im
Walde, 2013). Despite differences in formulation,
most existing models conceptualize the problem
of meaning representation as one of learning from
multiple views corresponding to different modali-
ties. These models still represent words as vectors
resulting from the combination of representations
with different statistical properties that do not nec-
essarily have a natural correspondence (e.g., text
and images).
In this work, we introduce a model, illus-
trated in Figure 1, which learns grounded mean-
ing representations by mapping words and im-
ages into a common embedding space. Our model
uses stacked autoencoders (Bengio et al., 2007)
to induce semantic representations integrating vi-
sual and textual information. The literature de-
scribes several successful approaches to multi-
modal learning using different variants of deep
networks (Ngiam et al., 2011; Srivastava and
Salakhutdinov, 2012) and data sources including
text, images, audio, and video. Unlike most pre-
vious work, our model is defined at a finer level
of granularity — it computes meaning representa-
tions for individual words and is unique in its use
of attributes as a means of representing the textual
and visual modalities. We follow Silberer et al.
(2013) in arguing that an attribute-centric repre-
sentation is expedient for several reasons.
Firstly, attributes provide a natural way of ex-
pressing salient properties of word meaning as
demonstrated in norming studies (e.g., McRae
et al., 2005) where humans often employ attributes
when asked to describe a concept. Secondly, from
</bodyText>
<page confidence="0.964083">
721
</page>
<note confidence="0.8309795">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999896363636363">
a modeling perspective, attributes allow for eas-
ier integration of different modalities, since these
are rendered in the same medium, namely, lan-
guage. Thirdly, attributes are well-suited to de-
scribing visual phenomena (e.g., objects, scenes,
actions). They allow to generalize to new in-
stances for which there are no training exam-
ples available and to transcend category and task
boundaries whilst offering a generic description of
visual data (Farhadi et al., 2009).
Our model learns multimodal representations
from attributes which are automatically inferred
from text and images. We evaluate the embed-
dings it produces on two tasks, namely word sim-
ilarity and categorization. In the first task, model
estimates of word similarity (e.g., gem–jewel are
similar but glass–magician are not) are compared
against elicited similarity ratings. We performed
a large-scale evaluation on a new dataset consist-
ing of human similarity judgments for 7,576 word
pairs. Unlike previous efforts such as the widely
used WordSim353 collection (Finkelstein et al.,
2002), our dataset contains ratings for visual and
textual similarity, thus allowing to study the two
modalities (and their contribution to meaning rep-
resentation) together and in isolation. We also
assess whether the learnt representations are ap-
propriate for categorization, i.e., grouping a set
of objects into meaningful semantic categories
(e.g., peach and apple are members of FRUIT,
whereas chair and table are FURNITURE). On both
tasks, our model outperforms baselines and related
models.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999888029850746">
The presented model has connections to several
lines of work in NLP, computer vision research,
and more generally multimodal learning. We re-
view related work in these areas below.
Grounded Semantic Spaces Grounded seman-
tic spaces are essentially distributional models
augmented with perceptual information. A model
akin to Latent Semantic Analysis (Landauer and
Dumais, 1997) is proposed in Bruni et al. (2012b)
who concatenate two independently constructed
textual and visual spaces and subsequently project
them onto a lower-dimensional space using Singu-
lar Value Decomposition.
Several other models have been extensions of
Latent Dirichlet Allocation (Blei et al., 2003)
where topic distributions are learned from words
and other perceptual units. Feng and Lapata
(2010) use visual words which they extract from a
corpus of multimodal documents (i.e., BBC news
articles and their associated images), whereas oth-
ers (Steyvers, 2010; Andrews et al., 2009; Silberer
and Lapata, 2012) use feature norms obtained in
longitudinal elicitation studies (see McRae et al.
(2005) for an example) as an approximation of the
visual environment. More recently, topic mod-
els which combine both feature norms and vi-
sual words have also been introduced (Roller and
Schulte im Walde, 2013). Drawing inspiration
from the successful application of attribute clas-
sifiers in object recognition, Silberer et al. (2013)
show that automatically predicted visual attributes
act as substitutes for feature norms without any
critical information loss.
The visual and textual modalities on which our
model is trained are decoupled in that they are not
derived from the same corpus (we would expect
co-occurring images and text to correlate to some
extent) but unified in their representation by natu-
ral language attributes. The use of stacked autoen-
coders to extract a shared lexical meaning repre-
sentation is new to our knowledge, although, as
we explain below related to a large body of work
on deep learning.
Multimodal Deep Learning Our work employs
deep learning (a.k.a deep networks) to project lin-
guistic and visual information onto a unified rep-
resentation that fuses the two modalities together.
The goal of deep learning is to learn multiple lev-
els of representations through a hierarchy of net-
work architectures, where higher-level representa-
tions are expected to help define higher-level con-
cepts.
A large body of work has focused on projecting
words and images into a common space using a va-
riety of deep learning methods ranging from deep
and restricted Boltzman machines (Srivastava and
Salakhutdinov, 2012; Feng et al., 2013), to au-
toencoders (Wu et al., 2013), and recursive neural
networks (Socher et al., 2013b). Similar methods
have been employed to combine other modalities
such as speech and video (Ngiam et al., 2011) or
images (Huang and Kingsbury, 2013). Although
our model is conceptually similar to these studies
(especially those applying stacked autoencoders),
it differs considerably from them in at least two
aspects. Firstly, most of these approaches aim to
learn a shared representation between modalities
</bodyText>
<page confidence="0.994995">
722
</page>
<bodyText confidence="0.999905076923077">
so as to infer some missing modality from others
(e.g., to infer text from images and vice versa); in
contrast, we aim to learn an optimal representa-
tion for each modality and their optimal combi-
nation. Secondly, our problem setting is different
from the former studies, which usually deal with
classification tasks and fine-tune the deep neural
networks using training data with explicit class la-
bels; in contrast we fine-tune our autoencoders us-
ing a semi-supervised criterion. That is, we use
indirect supervision in the form of object classifi-
cation in addition to the objective of reconstruct-
ing the attribute-centric input representation.
</bodyText>
<sectionHeader confidence="0.997066" genericHeader="method">
3 Autoencoders for Grounded Semantics
</sectionHeader>
<subsectionHeader confidence="0.988982">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.990723512195122">
Our model learns higher-level meaning represen-
tations for single words from textual and visual
input in a joint fashion. We first briefly review
autoencoders in Section 3.1 with emphasis on as-
pects relevant to our model which we then de-
scribe in Section 3.2.
Autoencoders An autoencoder is an unsuper-
vised neural network which is trained to recon-
struct a given input from its latent representation
(Bengio, 2009). It consists of an encoder fθ which
maps an input vector x(i) to a latent representa-
tion y(i) = fθ(x(i)) = s(Wx(i) + b), with s being
a non-linear activation function, such as a sig-
moid function. A decoder gθ0 then aims to recon-
struct input x(i) from y(i), i.e., ˆx(i) = gθ0(y(i)) =
s(W0y(i) + b0). The training objective is the de-
termination of parameters θˆ = {W,b} and ˆθ0 =
{W0,b0} that minimize the average reconstruction
error over a set of input vectors {x(1),...,x(n)}:
L(x(i),gθ0(fθ(x(i)))), (1)
where L is a loss function, such as cross-entropy.
Parameters θ and θ0 can be optimized by gradient
descent methods.
Autoencoders are a means to learn representa-
tions of some input by retaining useful features in
the encoding phase which help to reconstruct the
input, whilst discarding useless or noisy ones. To
this end, different strategies have been employed
to guide parameter learning and constrain the hid-
den representation. Examples include imposing
a bottleneck to produce an under-complete rep-
resentation of the input, using sparse representa-
tions, or denoising.
Denoising Autoencoders The training criterion
with denoising autoencoders is the reconstruction
of clean input x(i) given a corrupted version ˜x(i)
(Vincent et al., 2010). The underlying idea is that
the learned latent representation is good if the au-
toencoder is capable of reconstructing the actual
input from its corruption. The reconstruction error
for an input x(i) with loss function L then is:
</bodyText>
<equation confidence="0.993143">
L(x(i),gθ0(fθ(˜x(i)))) (2)
</equation>
<bodyText confidence="0.9999118">
One possible corruption process is masking noise,
where the corrupted version ˜x(i) results from ran-
domly setting a fraction v of x(i) to 0.
Stacked Autoencoders Several (denoising) au-
toencoders can be used as building blocks to form
a deep neural network (Bengio et al., 2007; Vin-
cent et al., 2010). For that purpose, the autoen-
coders are pre-trained layer by layer, with the cur-
rent layer being fed the latent representation of the
previous autoencoder as input. Using this unsuper-
vised pre-training procedure, initial parameters are
found which approximate a good solution. Subse-
quently, the original input layer and hidden repre-
sentations of all the autoencoders are stacked and
all network parameters are fine-tuned with back-
propagation.
To further optimize the parameters of the net-
work, a supervised criterion can be imposed on top
of the last hidden layer such as the minimization
of a prediction error on a supervised task (Bengio,
2009). Another approach is to unfold the stacked
autoencoders and fine-tune them with respect to
the minimization of the global reconstruction error
(Hinton and Salakhutdinov, 2006). Alternatively,
a semi-supervised criterion can be used (Ranzato
and Szummer, 2008; Socher et al., 2011) through
combination of the unsupervised training criterion
(global reconstruction) with a supervised criterion
(prediction of some target given the latent repre-
sentation).
</bodyText>
<subsectionHeader confidence="0.999671">
3.2 Semantic Representations
</subsectionHeader>
<bodyText confidence="0.999981555555556">
To learn meaning representations of single words
from textual and visual input, we employ stacked
(denoising) autoencoders (SAEs). Both input
modalities are vector-based representations of
words, or, more precisely, the objects they refer to
(e.g., canary, trolley). The vector dimensions cor-
respond to textual and visual attributes, examples
of which are shown in Table 1. We explain how
these representations are obtained in more detail
</bodyText>
<equation confidence="0.997822285714286">
ˆθ, ˆθ0 =argmin
θ,θ0
1
n
n
∑
i=1
</equation>
<page confidence="0.996408">
723
</page>
<figureCaption confidence="0.994579333333333">
Figure 1: Stacked autoencoder trained with semi-supervised objective. Input to the model are single-
word vector representations obtained from text and images. Vector dimensions correspond to textual and
visual attributes, respectively (see Table 1).
</figureCaption>
<figure confidence="0.995980290322581">
softmax ˆt
bimodal coding y˘
W(50)
...
W(5)
...
...
input x
IMAGES
TEXT
reconstruction xˆ
...
...
W(10)
W(20)
W(30)
...
...
W(40)
...
...
...
W(6)
...
...
W(3)
W(4)
...
...
W(1)
W(2)
</figure>
<bodyText confidence="0.991641595744681">
in Section 4.1. We first train SAEs with two hid-
den layers (codings) for each modality separately.
Then, we join these two SAEs by feeding their re-
spective second coding simultaneously to another
autoencoder, whose hidden layer thus yields the
fused meaning representation. Finally, we stack
all layers and unfold them in order to fine-tune
the SAE. Figure 1 illustrates the model.
Unimodal Autoencoders For both modalities,
we use the hyperbolic tangent function as activa-
tion function for encoder fθ and decoder gθ0 and an
entropic loss function for L. The weights of each
autoencoder are tied, i.e., W0 = WT. We employ
denoising autoencoders (DAEs) for pre-training
the textual modality. Regarding the visual autoen-
coder, we derive a new (‘denoised’) target vector
to be reconstructed for each input vector x(i), and
treat x(i) itself as corrupted input. The unimodal
autoencoder is thus trained to denoise a given in-
put. The target vector is derived as follows: each
object o in our data is represented by multiple im-
ages, and each image is in turn represented by a
visual attribute vector x(i). The target vector is the
sum of x(i) and the centroid x(j) of the remaining
attribute vectors representing object o.
Bimodal Autoencoder The bimodal autoen-
coder is fed with the concatenated final hidden
codings of the visual and textual modalities as in-
put and maps these inputs to a joint hidden layer y˘
with B units. We normalize both unimodal input
codings to unit length. Again, we use tied weights
for the bimodal autoencoder. We also encourage
the autoencoder to detect dependencies between
the two modalities while learning the mapping
to the bimodal hidden layer. We therefore apply
masking noise to one modality with a masking fac-
tor v (see Section 3.1), so that the corrupted modal-
ity optimally has to rely on the other modality in
order to reconstruct its missing input features.
Stacked Bimodal Autoencoder We finally
build a stacked bimodal autoencoder (SAE) with
all pre-trained layers and fine-tune them with re-
spect to a semi-supervised criterion. That is, we
unfold the stacked autoencoder and furthermore
add a softmax output layer on top of the bimodal
layer y˘ that outputs predictions ˆt with respect to
the inputs’ object labels (e.g., boat):
</bodyText>
<equation confidence="0.9438026">
exp(W(6)˘y(i) +b(6))
t(i) = , (3)
∑O k=1 exp(W(6)
k. ˘y(i) + b(6)
k )
</equation>
<bodyText confidence="0.9946142">
with weights W(6) ∈ RO×B, b(6) ∈ RO×1, where O
is the number of unique object labels. The over-
all objective to be minimized is therefore the
weighted sum of the reconstruction error Lr and
the classification error Lc:
</bodyText>
<equation confidence="0.9987335">
n �∑ δrLr(x(i), ˆx(i))+δcLc(t(i),ˆt(i)) +λR (4)
i=1
</equation>
<bodyText confidence="0.99998">
where δr and δc are weighting parameters that
give different importance to the partial objectives,
</bodyText>
<equation confidence="0.5940345">
1
L= n
</equation>
<page confidence="0.980831">
724
</page>
<table confidence="0.936609666666667">
eats seeds has beak has claws has handlebar has wheels has wings is yellow made of wood
canary 0.05 0.24 0.15 0.00 –0.10 0.19 0.34 0.00
trolley 0.00 0.00 0.00 0.30 0.32 0.00 0.00 0.25
bird:n breed:v cage:n chirp:v fly:v track:n ride:v run:v rail:n wheel:n
canary 0.16 0.19 0.39 0.13 0.13 0.00 0.00 0.00 0.00 –0.05
trolley –0.40 0.00 0.00 0.00 0.00 0.14 0.16 0.33 0.17 0.20
</table>
<tableCaption confidence="0.999706">
Table 1: Examples of attribute-based representations provided as input to our autoencoders.
</tableCaption>
<bodyText confidence="0.961044545454546">
Visual
Textual
Lc and Lr are entropic loss functions, and R is
a regularization term with R = ∑5j=12||W(j)||2 +
||W(6)||2. Finally, ˆt(i) is the object label vector pre-
dicted by the softmax layer for input vector x(i),
and t(i) is the correct object label, represented as a
O-dimensional one-hot vector1.
The additional supervised criterion drives the
learning towards a representation capable of dis-
criminating between different objects. Further-
more, the semi-supervised setting affords flexibil-
ity, allowing to adapt the architecture to specific
tasks. For example, by setting the corruption pa-
rameter v for the textual modality to one and δr
to zero, a standard object classification model for
images can be trained. Setting v close to one for ei-
ther modality enables the model to infer the other
(missing) modality. As our input consists of nat-
ural language attributes, the model would infer
textual attributes given visual attributes and vice
versa.
</bodyText>
<sectionHeader confidence="0.997979" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999989857142857">
In this section we present our experimental setup
for assessing the performance of our model. We
give details on the tasks and datasets used for eval-
uation, we explain how the textual and visual in-
puts were constructed, how the SAE model was
trained, and describe the approaches used for com-
parison with our own work.
</bodyText>
<subsectionHeader confidence="0.993091">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99928575">
We learn meaning representations for the nouns
contained in McRae et al.’s (2005) feature norms.
These are 541 concrete animate and inanimate ob-
jects (e.g., animals, clothing, vehicles, utensils,
fruits, and vegetables). The norms were elicited
by asking participants to list properties (e.g., barks,
an animal, has legs) describing the nouns they were
presented with.
</bodyText>
<footnote confidence="0.9840315">
1In a one-hot vector, the element corresponding to the ob-
ject label is one and the others are zero.
</footnote>
<bodyText confidence="0.999971026315789">
As shown in Figure 1, our model takes as in-
put two (real-valued) vectors representing the vi-
sual and textual modalities. Vector dimensions
correspond to textual and visual attributes, respec-
tively. Textual attributes were extracted by run-
ning Strudel (Baroni et al., 2010) on a 2009 dump
of the English Wikipedia.2 Strudel is a fully
automatic method for extracting weighted word-
attribute pairs (e.g., bat–species:n, bat–bite:v) from
a lemmatized and POS-tagged corpus. Weights
are log-likelihood ratio scores expressing how
strongly an attribute and a word are associated. We
only retained the ten highest scored attributes for
each target word. This returned a total of 2,362
dimensions for the textual vectors. Association
scores were scaled to the [−1,1] range.
To obtain visual vectors, we followed the
methodology put forward in Silberer et al. (2013).
Specifically, we used an updated version of their
dataset to train SVM-based attribute classifiers
that predict visual attributes for images (Farhadi
et al., 2009). The dataset is a taxonomy of 636 vi-
sual attributes (e.g., has wings, made of wood) and
nearly 700K images from ImageNet (Deng et al.,
2009) describing more than 500 of McRae et al.’s
(2005) nouns. The classifiers perform reason-
ably well with an interpolated average precision
of 0.52. We only considered attributes assigned
to at least two nouns in the dataset, obtaining a
414 dimensional vector for each noun. Analo-
gously to the textual representations, visual vec-
tors were scaled to the [−1,1] range.
We follow Silberer et al.’s (2013) partition of the
dataset into training, validation, and test set and
acquire visual vectors for each of the sets. We use
the visual vectors of the training and development
set for training the autoencoders, and the vectors
for the test set for evaluation.
</bodyText>
<footnote confidence="0.992437">
2The corpus is downloadable from http://wacky.
sslmit.unibo.it/doku.php?id=corpora.
</footnote>
<page confidence="0.989715">
725
</page>
<subsectionHeader confidence="0.987957">
4.2 Model Architecture
</subsectionHeader>
<bodyText confidence="0.999963738095238">
Model parameters were optimized on a subset of
the word association norms collected by Nelson
et al. (1998).3 These were established by present-
ing participants with a cue word (e.g., canary) and
asking them to name an associate word in response
(e.g., bird, sing, yellow). For each cue, the norms
provide a set of associates and the frequencies
with which they were named. The dataset con-
tains a very large number of cue-associate pairs
(63,619 in total) some of which luckily are cov-
ered in McRae et al. (2005).4 During training
we used correlation analysis (Spearman’s p) to
monitor the degree of linear relationship between
model cue-associate (cosine) similarities and hu-
man probabilities.
The best autoencoder on the word association
task obtained a correlation coefficient of 0.33.
This performance is superior to the results re-
ported in Silberer et al. (2013) (their correlation
coefficients range from 0.16 to 0.28). This model
has the following architecture: the textual autoen-
coder (see Figure 1, left-hand side) consists of 700
hidden units which are then mapped to the sec-
ond hidden layer with 500 units (the corruption
parameter was set to v = 0.1); the visual autoen-
coder (see Figure 1, right-hand side) has 170 and
100 hidden units, in the first and second layer, re-
spectively. The 500 textual and 100 visual hidden
units were fed to a bimodal autoencoder contain-
ing 500 latent units, and masking noise was ap-
plied to the textual modality with v = 0.2. The
weighting parameters for the joint training objec-
tive of the stacked autoencoder were set to Sr = 0.8
and Sc = 1 (see Equation (4)).
We used the model described above and the
meaning representations obtained from the out-
put of the bimodal latent layer for all the eval-
uation tasks detailed below. Some performance
gains could be expected if parameter optimization
took place separately for each task. However, we
wanted to avoid overfitting, and show that our pa-
rameters are robust across tasks and datasets.
</bodyText>
<subsectionHeader confidence="0.995215">
4.3 Evaluation Tasks
</subsectionHeader>
<bodyText confidence="0.793814666666667">
Word Similarity We first evaluated how well
our model predicts word similarity ratings. Al-
though several relevant datasets exist, such as
</bodyText>
<footnote confidence="0.831616">
3http://w3.usf.edu/Freeassociation.
</footnote>
<bodyText confidence="0.983347">
4435 word pairs constitute the overlap between Nelson et
al.’s norms (1998) and McRae et al.’s (2005) nouns.
the widely used WordSim353 (Finkelstein et al.,
2002) or the more recent Rel-122 norms (Szum-
lanski et al., 2013), they contain many abstract
words, (e.g., love–sex or arrest–detention) which
are not covered in McRae et al. (2005). This is for
a good reason, as most abstract words do not have
discernible attributes, or at least attributes that par-
ticipants would agree upon. We thus created a
new dataset consisting exclusively of McRae et al.
(2005) nouns which we hope will be useful for the
development and evaluation of grounded semantic
space models.5
Initially, we created all possible pairings over
McRae et al.’s (2005) nouns and computed their
semantic relatedness using Patwardhan and Peder-
sen (2006)’s WordNet-based measure. We opted
for this specific measure as it achieves high corre-
lation with human ratings and has a high coverage
on our nouns. Next, for each word we randomly
selected 30 pairs under the assumption that they
are representative of the full variation of semantic
similarity. This resulted in 7,576 word pairs for
which we obtained similarity ratings using Ama-
zon Mechanical Turk (AMT). Participants were
asked to rate a pair on two dimensions, visual
and semantic similarity using a Likert scale of 1
(highly dissimilar) to 5 (highly similar). Each task
consisted of 32 pairs covering examples of weak
to very strong semantic relatedness. Two con-
trol pairs from Miller and Charles (1991) were in-
cluded in each task to potentially help identify and
eliminate data from participants who assigned ran-
dom scores. Examples of the stimuli and mean
ratings are shown in Table 2.
The elicitation study comprised overall 255
tasks, each task was completed by five volun-
teers. The similarity data was post-processed so
as to identify and remove outliers. We consid-
ered an outlier to be any individual whose mean
pairwise correlation fell outside two standard de-
viations from the mean correlation. 11.5% of
the annotations were detected as outliers and re-
moved. After outlier removal, we further ex-
amined how well the participants agreed in their
similarity judgments. We measured inter-subject
agreement as the average pairwise correlation co-
efficient (Spearman’s p) between the ratings of all
annotators for each task. For semantic similarity,
the mean correlation was 0.76 (Min =0.34, Max
</bodyText>
<footnote confidence="0.9921065">
5Available from http://homepages.inf.ed.ac.uk/
mlap/index.php?page=resources.
</footnote>
<page confidence="0.992463">
726
</page>
<table confidence="0.999866">
Word Pairs Semantic Visual
football–pillow 1.0 1.2
dagger–pencil 1.0 2.2
motorcycle–wheel 2.4 1.8
orange–pumpkin 2.5 3.0
cherry–pineapple 3.6 1.2
pickle–zucchini 3.6 4.0
canary–owl 4.0 2.4
jeans–sweater 4.5 2.2
pan–pot 4.7 4.0
hornet–wasp 4.8 4.8
airplane–jet 5.0 5.0
</table>
<tableCaption confidence="0.9977">
Table 2: Mean semantic and visual similarity rat-
</tableCaption>
<bodyText confidence="0.98157290625">
ings for the McRae et al. (2005) nouns using a
scale of 1 (highly dissimilar) to 5 (highly similar).
=0.97, StD =0.11) and for visual similarity 0.63
(Min =0.19, Max =0.90, SD =0.14). These re-
sults indicate that the participants found the task
relatively straightforward and produced similarity
ratings with a reasonable level of consistency. For
comparison, Patwardhan and Pedersen’s (2006)
measure achieved a coefficient of 0.56 on the
dataset for semantic similarity and 0.48 for vi-
sual similarity. The correlation between the aver-
age ratings of the AMT annotators and the Miller
and Charles (1991) dataset was ρ = 0.91. In our
experiments (see Section 5), we correlate model-
based cosine similarities with mean similarity rat-
ings (again using Spearman’s ρ).
Categorization The task of categorization
(i.e., grouping objects into meaningful categories)
is a classic problem in the field of cognitive
science, central to perception, learning, and the
use of language. We evaluated model output
against a gold standard set of categories created
by Fountain and Lapata (2010). The dataset
contains a classification, produced by human
participants, of McRae et al.’s (2005) nouns into
(possibly multiple) semantic categories (40 in
total).6
To obtain a clustering of nouns, we used Chi-
nese Whispers (Biemann, 2006), a randomized
graph-clustering algorithm. In the categorization
setting, Chinese Whispers (CW) produces a hard
clustering over a weighted graph whose nodes cor-
</bodyText>
<footnote confidence="0.990177">
6The dataset can be downloaded from http:
//homepages.inf.ed.ac.uk/s0897549/data/.
</footnote>
<bodyText confidence="0.999861321428571">
respond to words and edges to cosine similarity
scores between vectors representing their mean-
ing. CW is a non-parametric model, it induces the
number of clusters (i.e., categories) from the data
as well as which nouns belong to these clusters.
In our experiments, we initialized Chinese Whis-
pers with different graphs resulting from different
vector-based representations of the McRae et al.
(2005) nouns. We also transformed the dataset
into hard categorizations by assigning each noun
to its most typical category as extrapolated from
human typicality ratings (for details see Foun-
tain and Lapata, 2010). CW can optionally ap-
ply a minimum weight threshold which we opti-
mized using the categorization dataset from Ba-
roni et al. (2010). The latter contains a classifica-
tion of 82 McRae et al. (2005) nouns into 10 cate-
gories. These nouns were excluded from the gold
standard (Fountain and Lapata, 2010) in our final
evaluation.
We evaluated the clusters produced by CW us-
ing the F-score measure introduced in the Se-
mEval 2007 task (Agirre and Soroa, 2007); it is
the harmonic mean of precision and recall defined
as the number of correct members of a cluster di-
vided by the number of items in the cluster and
the number of items in the gold-standard class, re-
spectively.
</bodyText>
<subsectionHeader confidence="0.999485">
4.4 Comparison with Other Models
</subsectionHeader>
<bodyText confidence="0.999993095238095">
Throughout our experiments we compare a bi-
modal stacked autoencoder against unimodal au-
toencoders based solely on textual and visual in-
put (left- and right-hand sides in Figure 1, respec-
tively). We also compare our model against two
approaches that differ in their fusion mechanisms.
The first one is based on kernelized canonical cor-
relation (kCCA, Hardoon et al., 2004) with a lin-
ear kernel which was the best performing model
in Silberer et al. (2013). The second one emulates
Bruni et al.’s (2014) fusion mechanism. Specifi-
cally, we concatenate the textual and visual vec-
tors and project them onto a lower dimensional la-
tent space using SVD (Golub and Reinsch, 1970).
All these models run on the same datasets/items
and are given input identical to our model, namely
attribute-based textual and visual representations.
We furthermore report results obtained with
Bruni et al.’s (2014) bimodal distributional model,
which employs SVD to integrate co-occurrence-
based textual representations with visual repre-
</bodyText>
<page confidence="0.993604">
727
</page>
<table confidence="0.999898">
McRae 0.71 0.49 0.68 0.58 0.52 0.62
Attributes 0.58 0.61 0.68 0.46 0.56 0.58
SAE 0.65 0.60 0.70 0.52 0.60 0.64
SVD — — 0.67 — — 0.57
kCCA — — 0.57 — — 0.55
Bruni — — 0.52 — — 0.46
RNN-640 0.41 — — 0.34 — —
</table>
<tableCaption confidence="0.755274">
Table 3: Correlation of model predictions against
similarity ratings for McRae et al. (2005) noun
pairs (using Spearman’s p).
</tableCaption>
<bodyText confidence="0.996646764705882">
sentations constructed from low-level image fea-
tures. In their model, the textual modality is
represented by the 30K-dimensional vectors ex-
tracted from UKWaC and WaCkypedia.7 The
visual modality is represented by bag-of-visual-
words histograms built on the basis of clustered
SIFT features (Lowe, 2004). We rebuilt their
model on the ESP image dataset (von Ahn and
Dabbish, 2004) using Bruni et al.’s (2013) publicly
available system.
Finally, we also compare to the word embed-
dings obtained using Mikolov et al.’s (2011) re-
current neural network based language model.
These were pre-trained on Broadcast news data
(400M words) using the word2vec tool.8 We re-
port results with the 640-dimensional embeddings
as they performed best.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999650933333333">
Table 3 presents our results on the word simi-
larity task. We report correlation coefficients of
model predictions against similarity ratings. As an
indicator to how well automatically extracted at-
tributes can approach the performance of clean hu-
man generated attributes, we also report results of
a distributional model induced from McRae et al.’s
(2005) norms (see the row labeled McRae in the
table). Each noun is represented as a vector with
dimensions corresponding to attributes elicited by
participants of the norming study. Vector compo-
nents are set to the (normalized) frequency with
which participants generated the corresponding at-
tribute. We show results for three models, using all
attributes except those classified as visual (T), only
</bodyText>
<footnote confidence="0.9996865">
7We thank Elia Bruni for providing us with their data.
8Available from http://www.rnnlm.org/.
</footnote>
<table confidence="0.999876909090909">
# Pair # Pair
1 pliers–tongs 11 cello–violin
2 cathedral–church 12 cottage–house
3 cathedral–chapel 13 horse–pony
4 pistol–revolver 14 gun–rifle
5 chapel–church 15 cedar–oak
6 airplane–helicopter 16 bull–ox
7 dagger–sword 17 dress–gown
8 pistol–rifle 18 bolts–screws
9 cloak–robe 19 salmon–trout
10 nylons–trousers 20 oven–stove
</table>
<tableCaption confidence="0.978926">
Table 4: Word pairs with highest semantic and vi-
</tableCaption>
<bodyText confidence="0.997177470588235">
sual similarity according to SAE model. Pairs are
ranked from highest to lowest similarity.
visual attributes (V), and all available attributes
(V+T).9 As baselines, we also report the perfor-
mance of a model based solely on textual attributes
(which we obtain from Strudel), visual attributes
(obtained from our classifiers), and their concate-
nation (see row Attributes in Table 3, and columns
T, V, and T+V, respectively). The automatically
obtained textual and visual attribute vectors serve
as input to SVD, kCCA, and our stacked autoen-
coder (SAE). The third row in the table presents
three variants of our model trained on textual and
visual attributes only (T and V, respectively) and
on both modalities jointly (T+V).
Recall that participants were asked to provide
ratings on two dimensions, namely semantic and
visual similarity. We would expect the textual
modality to be more dominant when modeling se-
mantic similarity and conversely the perceptual
modality to be stronger with respect to visual sim-
ilarity. This is borne out in our unimodal SAEs.
The textual SAE correlates better with seman-
tic similarity judgments (p = 0.65) than its vi-
sual equivalent (p = 0.60). And the visual SAE
correlates better with visual similarity judgments
(p = 0.60) compared to the textual SAE (p = 0.52).
Interestingly, the bimodal SAE is better than the
unimodal variants on both types of similarity judg-
ments, semantic and visual. This suggests that
both modalities contribute complementary infor-
mation and that the SAE model is able to extract
a shared representation which improves general-
ization performance across tasks by learning them
</bodyText>
<footnote confidence="0.719679">
9Classification of attributes into categories is provided by
McRae et al. (2005) in their dataset.
</footnote>
<figure confidence="0.519446666666667">
Semantic Visual
Models
T V T+V T V T+V
</figure>
<page confidence="0.749508">
728
</page>
<table confidence="0.616767">
Models T V T+V
</table>
<tableCaption confidence="0.968425">
Table 5: F-score results on concept categorization.
</tableCaption>
<bodyText confidence="0.999380102564102">
jointly. The bimodal autoencoder (SAE, T+V)
outperforms all other bimodal models on both sim-
ilarity tasks. It yields a correlation coefficient
of ρ = 0.70 on semantic similarity and ρ = 0.64 on
visual similarity. Human agreement on the former
task is 0.76 and 0.63 on the latter. Table 4 shows
examples of word pairs with highest semantic and
visual similarity according to the SAE model.
We also observe that simply concatenating
textual and visual attributes (Attributes, T+V)
performs competitively with SVD and better
than kCCA. This indicates that the attribute-based
representation is a powerful predictor on its own.
Interestingly, both Bruni et al. (2013) and Mikolov
et al. (2011) which do not make use of attributes
are out-performed by all other attribute-based sys-
tems (see columns T and T+V in Table 3).
Our results on the categorization task are given
in Table 5. In this task, simple concatenation of vi-
sual and textual attributes does not yield improved
performance over the individual modalities (see
row Attributes in Table 5). In contrast, all bimodal
models (SVD, kCCA, and SAE) are better than
their unimodal equivalents and RNN-640. The
SAE outperforms both kCCA and SVD by a large
margin delivering clustering performance similar
to the McRae et al.’s (2005) norms. Table 6 shows
examples of clusters produced by Chinese Whis-
pers when using vector representations provided
by the SAE model.
In sum, our experiments show that the bi-
modal SAE model delivers superior performance
across the board when compared against competi-
tive baselines and related models. It is interesting
to note that the unimodal SAEs are in most cases
better than the raw textual or visual attributes.
This indicates that higher level embeddings may
be beneficial to NLP tasks in general, not only to
those requiring multimodal information.
</bodyText>
<table confidence="0.999663304347826">
STICK-LIKE UTENSILS baton, ladle, peg, spatula,
spoon
RELIGIOUS BUILDINGS cathedral, chapel, church
WIND INSTRUMENTS clarinet, flute, saxophone, trom-
bone, trumpet, tuba
AXES axe, hatchet, machete, toma-
hawk
FURNITURE W/ LEGS bed, bench, chair, couch, desk,
rocker, sofa, stool, table
FURNITURE W/O LEGS bookcase, bureau, cabinet,
closet, cupboard, dishwasher,
dresser
LIGHTINGS candle, chandelier, lamp,
lantern
ENTRY POINTS door, elevator, gate
UNGULATES bison, buffalo, bull, calf, camel,
cow, donkey, elephant, goat,
horse, lamb, ox, pig, pony,
sheep
BIRDS crow, dove, eagle, falcon, hawk,
ostrich, owl, penguin, pigeon,
raven, stork, vulture, wood-
pecker
</table>
<tableCaption confidence="0.888821333333333">
Table 6: Examples of clusters produced by CW
using the representations obtained from the SAE
model.
</tableCaption>
<sectionHeader confidence="0.99427" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.912776115384615">
In this paper, we presented a model that uses
stacked autoencoders to learn grounded meaning
representations by simultaneously combining tex-
tual and visual modalities. The two modalities are
encoded as vectors of natural language attributes
and are obtained automatically from decoupled
text and image data. To the best of our knowl-
edge, our model is novel in its use of attribute-
based input in a deep neural network. Experimen-
tal results in two tasks, namely simulation of word
similarity and word categorization, show that our
model outperforms competitive baselines and re-
lated models trained on the same attribute-based
input. Our evaluation also reveals that the bimodal
models are superior to their unimodal counterparts
and that higher-level unimodal representations are
better than the raw input. In the future, we would
like to apply our model to other tasks, such as im-
age and text retrieval (Hodosh et al., 2013; Socher
et al., 2013b), zero-shot learning (Socher et al.,
2013a), and word learning (Yu and Ballard, 2007).
Acknowledgment We would like to thank Vit-
torio Ferrari, Iain Murray and members of the
ILCC at the School of Informatics for their valu-
able feedback. We acknowledge the support of
EPSRC through project grant EP/I037415/1.
</bodyText>
<table confidence="0.657590615384615">
McRae 0.52 0.31 0.42
Attributes 0.35 0.37 0.33
SAE 0.36 0.35 0.43
SVD
—
—
0.39
kCCA
Bruni
RNN-640
— — 0.37
— — 0.34
0.32 — —
</table>
<page confidence="0.993416">
729
</page>
<sectionHeader confidence="0.989581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998142263157894">
Agirre, Eneko and Aitor Soroa. 2007. SemEval-
2007 Task 02: Evaluating Word Sense Induc-
tion and Discrimination Systems. In Proceed-
ings of the Fourth International Workshop on
Semantic Evaluations. Prague, Czech Republic,
pages 7–12.
Andrews, M., G. Vigliocco, and D. Vinson. 2009.
Integrating Experiential and Distributional Data
to Learn Semantic Representations. Psycholog-
ical Review 116(3):463–498.
Baroni, M., B. Murphy, E. Barbu, and M. Poe-
sio. 2010. Strudel: A Corpus-Based Semantic
Model Based on Properties and Types. Cogni-
tive Science 34(2):222–254.
Barsalou, Lawrence W. 2008. Grounded Cogni-
tion. Annual Review of Psychology 59:617–845.
Bengio, Y., P. Lamblin, D. Popovici, and
H. Larochelle. 2007. Greedy Layer-Wise Train-
ing of Deep Networks. In Bernhard Sch¨olkopf,
John Platt, and Thomas Hoffman, editors, Ad-
vances in Neural Information Processing Sys-
tems 19. MIT Press, pages 153–160.
Bengio, Yoshua. 2009. Learning Deep Architec-
tures for AI. Foundations and Trends in Ma-
chine Learning 2(1):1–127.
Biemann, Chris. 2006. Chinese Whispers – an Ef-
ficient Graph Clustering Algorithm and its Ap-
plication to Natural Language Processing Prob-
lems. In Proceedings of TextGraphs: the 1st
Workshop on Graph Based Methods for Natu-
ral Language Processing. New York, NY, pages
73–80.
Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine
Learning Research 3:993–1022.
Bruni, E., G. Boleda, M. Baroni, and N. Tran.
2012a. Distributional Semantics in Technicolor.
In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics.
Jeju Island, Korea, pages 136–145.
Bruni, E., U. Bordignon, A. Liska, J. Uijlings, and
I. Sergienya. 2013. Vsem: An open library for
visual semantics representation. In Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics: System Demon-
strations. Sofia, Bulgaria, pages 187–192.
Bruni, E., N. Tran, and M. Baroni. 2014. Multi-
modal distributional semantics. J. Artif. Intell.
Res. (JAIR) 49:1–47.
Bruni, E., J. Uijlings, M. Baroni, and N. Sebe.
2012b. Distributional Semantics with Eyes: Us-
ing Image Analysis to Improve Computational
Representations of Word Meaning. In Proceed-
ings of the 20th ACM International Conference
on Multimedia. Nara, Japan, pages 1219–1228.
Collobert, R., J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural
Language Processing (almost) from Scratch.
Journal of Machine Learning Research
12:2493–2537.
Deng, J., W. Dong, R. Socher, L. Li, K. Li, and
L. Fei-Fei. 2009. ImageNet: A Large-Scale
Hierarchical Image Database. In Proceedings
of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition. Mi-
ami, Florida, pages 248–255.
Farhadi, A., I. Endres, D. Hoiem, and D. Forsyth.
2009. Describing Objects by their Attributes.
In Proceedings of the IEEE Computer Soci-
ety Conference on Computer Vision and Pat-
tern Recognition. Miami Beach, Florida, pages
1778–1785.
Feng, Fangxiang, Ruifan Li, and Xiaojie Wang.
2013. Constructing Hierarchical Image-tags Bi-
modal Representations for Word Tags Alter-
native Choice. In Proceedings of the ICML
2013 Workshop on Challenges in Representa-
tion Learning. Atlanta, Georgia.
Feng, Yansong and Mirella Lapata. 2010. Visual
Information in Semantic Representation. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics.
Los Angeles, California, pages 91–99.
Finkelstein, L., E. Gabrilovich, Y. Matias,
E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.
2002. Placing Search in Context: The Concept
Revisited. ACM Transactions on Information
Systems 20(1):116–131.
Fountain, Trevor and Mirella Lapata. 2010. Mean-
ing Representation in Natural Language Cat-
egorization. In Proceedings of the 31st An-
nual Conference of the Cognitive Science Soci-
ety. Amsterdam, The Netherlands, pages 1916–
1921.
</reference>
<page confidence="0.98472">
730
</page>
<reference confidence="0.999294642857143">
Golub, Gene and Christian Reinsch. 1970. Sin-
gular Value Decomposition and Least Squares
Solutions. Numerische Mathematik 14(5):403–
420.
Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum.
2007. Topics in Semantic Representation. Psy-
chological Review 114(2):211–244.
Hardoon, D. R., S. R. Szedmak, and J. R. Shawe-
Taylor. 2004. Canonical Correlation Analy-
sis: An Overview with Application to Learning
Methods. Neural Computation 16(12):2639–
2664.
Harris, Zellig. 1970. Distributional Structure. In
Papers in Structural and Transformational Lin-
guistics, pages 775–794.
Hinton, Geoffrey E. and Ruslan R. Salakhutdinov.
2006. Reducing the Dimensionality of Data
with Neural Networks. Science 313(5786):504–
507.
Hodosh, Micah, Peter Young, and Julia Hocken-
maier. 2013. Framing Image Description as a
Ranking Task: Data, Models and Evaluation
Metrics. Journal of Artificial Intelligence Re-
search 47:853–899.
Huang, Jing and Brian Kingsbury. 2013. Audio-
visual Deep Learning for Noise Robust Speech
Recognition. In Proceedings of the 38th Inter-
national Conference on Acoustics, Speech, and
Signal Processing. Vancouver, Canada, pages
7596–7599.
Jones, R., B. Rey, O. Madani, and W. Greiner.
2006. Generating Query Substititions. In Pro-
ceedings of the 15th International Conference
on the World-Wide Web. Edinburgh, Scotland,
pages 387–396.
Landau, B., L. Smith, and S. Jones. 1998. Object
Perception and Object Naming in Early Devel-
opment. Trends in Cognitive Science 27:19–24.
Landauer, Thomas and Susan T. Dumais. 1997. A
Solution to Plato’s Problem: the Latent Seman-
tic Analysis Theory of Acquisition, Induction,
and Representation of Knowledge. Psychologi-
cal Review 104(2):211–240.
Lowe, D. 2004. Distinctive Image Features from
Scale-invariant Keypoints. International Jour-
nal of Computer Vision 60(2):91–110.
Manning, C. D., P. Raghavan, and H. Sch¨utze.
2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY.
McRae, K., G. S. Cree, M. S. Seidenberg, and
C. McNorgan. 2005. Semantic Feature Pro-
duction Norms for a Large Set of Living and
Nonliving Things. Behavior Research Methods
37(4):547–559.
Mikolov, T., S. Kombrink, L. Burget, J. ˇCernock´y,
and S. Khudanpur. 2011. Extensions of Recur-
rent Neural Network Language Model. In Pro-
ceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Pro-
cessing. Prague, Czech Republic, pages 5528–
5531.
Mikolov, T., Wen-tau Yih, and G. Zweig. 2013.
Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the
2013 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies. At-
lanta, Georgia, pages 746–751.
Miller, George A. and Walter G. Charles. 1991.
Contextual Correlates of Semantic Similarity.
Language and Cognitive Processes 6(1).
Nelson, D. L., C. L. McEvoy, and T. A.
Schreiber. 1998. The University of South
Florida Word Association, Rhyme, and Word
Fragment Norms.
Ngiam, Jiquan, Aditya Khosla, Mingyu Kim,
Juhan Nam, Honglak Lee, and Andrew Ng.
2011. Multimodal Deep Learning. In Pro-
ceedings of the 28th International Conference
on Machine Learning. Bellevue, Washington,
pages 689–696.
Patwardhan, Siddharth and Ted Pedersen. 2006.
Using WordNet-based Context Vectors to Es-
timate the Semantic Relatedness of Concepts.
In Proceedings of the EACL 2006 Workshop
on Making Sense of Sense: Bringing Compu-
tational Linguistics and Psycholinguistics To-
gether. Trento, Italy, pages 1–8.
Ranzato, Marc’Aurelio and Martin Szummer.
2008. Semi-supervised Learning of Com-
pact Document Representations with Deep Net-
works. In Proceedings of the 25th International
Conference on Machine Learning. Helsinki,
Finland, pages 792–799.
Regier, Terry. 1996. The Human Semantic Poten-
tial. MIT Press, Cambridge, Massachusetts.
Roller, Stephen and Sabine Schulte im Walde.
2013. A Multimodal LDA Model integrating
</reference>
<page confidence="0.972439">
731
</page>
<reference confidence="0.9997875375">
Textual, Cognitive and Visual Modalities. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing.
Seattle, Washington, pages 1146–1157.
Sebastiani, Fabrizio. 2002. Machine Learning in
Automated Text Categorization. ACM Comput-
ing Surveys 34:1–47.
Silberer, C., V. Ferrari, and M. Lapata. 2013. Mod-
els of Semantic Representation with Visual At-
tributes. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics. Sofia, Bulgaria, pages 572–582.
Silberer, Carina and Mirella Lapata. 2012.
Grounded Models of Semantic Representation.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning. Jeju Island, Korea, pages 1423–1433.
Socher, R., M. Ganjoo, C. D. Manning, and A. Y.
Ng. 2013a. Zero-shot learning through cross-
modal transfer. In Advances in Neural Informa-
tion Processing Systems 26, pages 935–943.
Socher, R., Quoc V. Le, C. D. Manning, and A. Y.
Ng. 2013b. Grounded Compositional Seman-
tics for Finding and Describing Images with
Sentences. In Proceedings of the NIPS Deep
Learning Workshop.
Socher, R., J. Pennington, E. H. Huang, A. Y. Ng,
and C. D. Manning. 2011. Semi-Supervised Re-
cursive Autoencoders for Predicting Sentiment
Distributions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Edinburgh, Scotland, pages
151–161.
Srivastava, Nitish and Ruslan Salakhutdinov.
2012. Multimodal Learning with Deep Boltz-
mann Machines. In Advances in Neural In-
formation Processing Systems 25, pages 2231–
2239.
Steyvers, Mark. 2010. Combining Feature Norms
and Text Data with Topic Models. Acta Psycho-
logica 133(3):234–342.
Szumlanski, S. R., F. Gomez, and V. K. Sims.
2013. A New Set of Norms for Semantic Re-
latedness Measures. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics. Sofia, Bulgaria, pages 890–
895.
Turney, Peter D. and Patrick Pantel. 2010. From
Frequency to Meaning: Vector Space Models
of Semantics. Journal of Artificial Intelligence
Research 37(1):141–188.
Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio,
and P. Manzagol. 2010. Stacked Denoising Au-
toencoders: Learning Useful Representations in
a Deep Network with a Local Denoising Cri-
terion. Journal of Machine Learning Research
11:3371–3408.
von Ahn, Luis and Laura Dabbish. 2004. Labeling
Images with a Computer Game. In Proceedings
of the SIGCHI Conference on Human Factors
in Computing Systems. Vienna, Austria, pages
319–326.
Wu, Pengcheng, Steven C. H. Hoi, Hao Xia, Peilin
Zhao, Dayong Wang, and Chunyan Miao. 2013.
Online Multimodal Deep Similarity Learning
with Application to Image Retrieval. In Pro-
ceedings of the 21st ACM International Con-
ference on Multimedia. Barcelona, Spain, pages
153–162.
Yih, Wen-tau, Ming-Wei Chang, Christopher
Meek, and Andrzej Pastusiak. 2013. Question
Answering Using Enhanced Lexical Semantic
Models. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics. Sofia, Bulgaria, pages 1744–1753.
Yu, C. and D. H. Ballard. 2007. A Unified Model
of Early Word Learning Integrating Statistical
and Social Cues. Neurocomputing 70:2149–
2165.
</reference>
<page confidence="0.997145">
732
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814492">
<title confidence="0.999698">Learning Grounded Meaning Representations with Autoencoders</title>
<author confidence="0.885676">Silberer</author>
<affiliation confidence="0.989941">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.933597">10 Crichton Street, Edinburgh EH8</address>
<abstract confidence="0.998331">In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>SemEval2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations.</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28249" citStr="Agirre and Soroa, 2007" startWordPosition="4438" endWordPosition="4441">taset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see Fountain and Lapata, 2010). CW can optionally apply a minimum weight threshold which we optimized using the categorization dataset from Baroni et al. (2010). The latter contains a classification of 82 McRae et al. (2005) nouns into 10 categories. These nouns were excluded from the gold standard (Fountain and Lapata, 2010) in our final evaluation. We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 4.4 Comparison with Other Models Throughout our experiments we compare a bimodal stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1, respectively). We also compare our model against two approaches that differ in their fusion mechanisms. The first one is based on kernelized canonical correlation (kCCA, H</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Agirre, Eneko and Aitor Soroa. 2007. SemEval2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations. Prague, Czech Republic, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Andrews</author>
<author>G Vigliocco</author>
<author>D Vinson</author>
</authors>
<title>Integrating Experiential and Distributional Data to Learn Semantic Representations. Psychological Review 116(3):463–498.</title>
<date>2009</date>
<contexts>
<context position="2157" citStr="Andrews et al., 2009" startWordPosition="312" endWordPosition="315">ations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspo</context>
<context position="6643" citStr="Andrews et al., 2009" startWordPosition="984" endWordPosition="987">tic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss. The visual and textual modal</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Andrews, M., G. Vigliocco, and D. Vinson. 2009. Integrating Experiential and Distributional Data to Learn Semantic Representations. Psychological Review 116(3):463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>B Murphy</author>
<author>E Barbu</author>
<author>M Poesio</author>
</authors>
<title>Strudel: A Corpus-Based Semantic Model Based on Properties and Types.</title>
<date>2010</date>
<journal>Cognitive Science</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="18984" citStr="Baroni et al., 2010" startWordPosition="2971" endWordPosition="2974">nimate and inanimate objects (e.g., animals, clothing, vehicles, utensils, fruits, and vegetables). The norms were elicited by asking participants to list properties (e.g., barks, an animal, has legs) describing the nouns they were presented with. 1In a one-hot vector, the element corresponding to the object label is one and the others are zero. As shown in Figure 1, our model takes as input two (real-valued) vectors representing the visual and textual modalities. Vector dimensions correspond to textual and visual attributes, respectively. Textual attributes were extracted by running Strudel (Baroni et al., 2010) on a 2009 dump of the English Wikipedia.2 Strudel is a fully automatic method for extracting weighted wordattribute pairs (e.g., bat–species:n, bat–bite:v) from a lemmatized and POS-tagged corpus. Weights are log-likelihood ratio scores expressing how strongly an attribute and a word are associated. We only retained the ten highest scored attributes for each target word. This returned a total of 2,362 dimensions for the textual vectors. Association scores were scaled to the [−1,1] range. To obtain visual vectors, we followed the methodology put forward in Silberer et al. (2013). Specifically,</context>
<context position="27929" citStr="Baroni et al. (2010)" startWordPosition="4381" endWordPosition="4385">etric model, it induces the number of clusters (i.e., categories) from the data as well as which nouns belong to these clusters. In our experiments, we initialized Chinese Whispers with different graphs resulting from different vector-based representations of the McRae et al. (2005) nouns. We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see Fountain and Lapata, 2010). CW can optionally apply a minimum weight threshold which we optimized using the categorization dataset from Baroni et al. (2010). The latter contains a classification of 82 McRae et al. (2005) nouns into 10 categories. These nouns were excluded from the gold standard (Fountain and Lapata, 2010) in our final evaluation. We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 4.4 Comparison with Other Models Throughout our experiments we c</context>
</contexts>
<marker>Baroni, Murphy, Barbu, Poesio, 2010</marker>
<rawString>Baroni, M., B. Murphy, E. Barbu, and M. Poesio. 2010. Strudel: A Corpus-Based Semantic Model Based on Properties and Types. Cognitive Science 34(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
</authors>
<title>Grounded Cognition. Annual Review of Psychology 59:617–845.</title>
<date>2008</date>
<contexts>
<context position="1898" citStr="Barsalou, 2008" startWordPosition="272" endWordPosition="273">ntly in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation </context>
</contexts>
<marker>Barsalou, 2008</marker>
<rawString>Barsalou, Lawrence W. 2008. Grounded Cognition. Annual Review of Psychology 59:617–845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>P Lamblin</author>
<author>D Popovici</author>
<author>H Larochelle</author>
</authors>
<title>Greedy Layer-Wise Training of Deep Networks.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19.</booktitle>
<pages>153--160</pages>
<editor>In Bernhard Sch¨olkopf, John Platt, and Thomas Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="3012" citStr="Bengio et al., 2007" startWordPosition="440" endWordPosition="443">pite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by mapping words and images into a common embedding space. Our model uses stacked autoencoders (Bengio et al., 2007) to induce semantic representations integrating visual and textual information. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing </context>
<context position="11698" citStr="Bengio et al., 2007" startWordPosition="1789" endWordPosition="1792">nstruction of clean input x(i) given a corrupted version ˜x(i) (Vincent et al., 2010). The underlying idea is that the learned latent representation is good if the autoencoder is capable of reconstructing the actual input from its corruption. The reconstruction error for an input x(i) with loss function L then is: L(x(i),gθ0(fθ(˜x(i)))) (2) One possible corruption process is masking noise, where the corrupted version ˜x(i) results from randomly setting a fraction v of x(i) to 0. Stacked Autoencoders Several (denoising) autoencoders can be used as building blocks to form a deep neural network (Bengio et al., 2007; Vincent et al., 2010). For that purpose, the autoencoders are pre-trained layer by layer, with the current layer being fed the latent representation of the previous autoencoder as input. Using this unsupervised pre-training procedure, initial parameters are found which approximate a good solution. Subsequently, the original input layer and hidden representations of all the autoencoders are stacked and all network parameters are fine-tuned with backpropagation. To further optimize the parameters of the network, a supervised criterion can be imposed on top of the last hidden layer such as the </context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2007</marker>
<rawString>Bengio, Y., P. Lamblin, D. Popovici, and H. Larochelle. 2007. Greedy Layer-Wise Training of Deep Networks. In Bernhard Sch¨olkopf, John Platt, and Thomas Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press, pages 153–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning Deep Architectures for AI. Foundations and Trends</title>
<date>2009</date>
<booktitle>in Machine Learning</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="9915" citStr="Bengio, 2009" startWordPosition="1504" endWordPosition="1505">ndirect supervision in the form of object classification in addition to the objective of reconstructing the attribute-centric input representation. 3 Autoencoders for Grounded Semantics 3.1 Background Our model learns higher-level meaning representations for single words from textual and visual input in a joint fashion. We first briefly review autoencoders in Section 3.1 with emphasis on aspects relevant to our model which we then describe in Section 3.2. Autoencoders An autoencoder is an unsupervised neural network which is trained to reconstruct a given input from its latent representation (Bengio, 2009). It consists of an encoder fθ which maps an input vector x(i) to a latent representation y(i) = fθ(x(i)) = s(Wx(i) + b), with s being a non-linear activation function, such as a sigmoid function. A decoder gθ0 then aims to reconstruct input x(i) from y(i), i.e., ˆx(i) = gθ0(y(i)) = s(W0y(i) + b0). The training objective is the determination of parameters θˆ = {W,b} and ˆθ0 = {W0,b0} that minimize the average reconstruction error over a set of input vectors {x(1),...,x(n)}: L(x(i),gθ0(fθ(x(i)))), (1) where L is a loss function, such as cross-entropy. Parameters θ and θ0 can be optimized by gra</context>
<context position="12368" citStr="Bengio, 2009" startWordPosition="1899" endWordPosition="1900">rs are pre-trained layer by layer, with the current layer being fed the latent representation of the previous autoencoder as input. Using this unsupervised pre-training procedure, initial parameters are found which approximate a good solution. Subsequently, the original input layer and hidden representations of all the autoencoders are stacked and all network parameters are fine-tuned with backpropagation. To further optimize the parameters of the network, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task (Bengio, 2009). Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error (Hinton and Salakhutdinov, 2006). Alternatively, a semi-supervised criterion can be used (Ranzato and Szummer, 2008; Socher et al., 2011) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent representation). 3.2 Semantic Representations To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoenc</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Bengio, Yoshua. 2009. Learning Deep Architectures for AI. Foundations and Trends in Machine Learning 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems.</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing.</booktitle>
<pages>73--80</pages>
<location>New York, NY,</location>
<contexts>
<context position="26950" citStr="Biemann, 2006" startWordPosition="4237" endWordPosition="4238">ities with mean similarity ratings (again using Spearman’s ρ). Categorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language. We evaluated model output against a gold standard set of categories created by Fountain and Lapata (2010). The dataset contains a classification, produced by human participants, of McRae et al.’s (2005) nouns into (possibly multiple) semantic categories (40 in total).6 To obtain a clustering of nouns, we used Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor6The dataset can be downloaded from http: //homepages.inf.ed.ac.uk/s0897549/data/. respond to words and edges to cosine similarity scores between vectors representing their meaning. CW is a non-parametric model, it induces the number of clusters (i.e., categories) from the data as well as which nouns belong to these clusters. In our experiments, we initialized Chinese Whispers with different graphs resulting from different vector-based </context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Biemann, Chris. 2006. Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems. In Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing. New York, NY, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<contexts>
<context position="6361" citStr="Blei et al., 2003" startWordPosition="940" endWordPosition="943"> work in NLP, computer vision research, and more generally multimodal learning. We review related work in these areas below. Grounded Semantic Spaces Grounded semantic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruni</author>
<author>G Boleda</author>
<author>M Baroni</author>
<author>N Tran</author>
</authors>
<title>Distributional Semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Jeju Island, Korea,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="2301" citStr="Bruni et al., 2012" startWordPosition="332" endWordPosition="335">e related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by ma</context>
<context position="6097" citStr="Bruni et al. (2012" startWordPosition="904" endWordPosition="907">jects into meaningful semantic categories (e.g., peach and apple are members of FRUIT, whereas chair and table are FURNITURE). On both tasks, our model outperforms baselines and related models. 2 Related Work The presented model has connections to several lines of work in NLP, computer vision research, and more generally multimodal learning. We review related work in these areas below. Grounded Semantic Spaces Grounded semantic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtaine</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Bruni, E., G. Boleda, M. Baroni, and N. Tran. 2012a. Distributional Semantics in Technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Jeju Island, Korea, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruni</author>
<author>U Bordignon</author>
<author>A Liska</author>
<author>J Uijlings</author>
<author>I Sergienya</author>
</authors>
<title>Vsem: An open library for visual semantics representation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</booktitle>
<pages>187--192</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="34312" citStr="Bruni et al. (2013)" startWordPosition="5410" endWordPosition="5413">outperforms all other bimodal models on both similarity tasks. It yields a correlation coefficient of ρ = 0.70 on semantic similarity and ρ = 0.64 on visual similarity. Human agreement on the former task is 0.76 and 0.63 on the latter. Table 4 shows examples of word pairs with highest semantic and visual similarity according to the SAE model. We also observe that simply concatenating textual and visual attributes (Attributes, T+V) performs competitively with SVD and better than kCCA. This indicates that the attribute-based representation is a powerful predictor on its own. Interestingly, both Bruni et al. (2013) and Mikolov et al. (2011) which do not make use of attributes are out-performed by all other attribute-based systems (see columns T and T+V in Table 3). Our results on the categorization task are given in Table 5. In this task, simple concatenation of visual and textual attributes does not yield improved performance over the individual modalities (see row Attributes in Table 5). In contrast, all bimodal models (SVD, kCCA, and SAE) are better than their unimodal equivalents and RNN-640. The SAE outperforms both kCCA and SVD by a large margin delivering clustering performance similar to the McR</context>
</contexts>
<marker>Bruni, Bordignon, Liska, Uijlings, Sergienya, 2013</marker>
<rawString>Bruni, E., U. Bordignon, A. Liska, J. Uijlings, and I. Sergienya. 2013. Vsem: An open library for visual semantics representation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Sofia, Bulgaria, pages 187–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruni</author>
<author>N Tran</author>
<author>M Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>J. Artif. Intell. Res. (JAIR)</journal>
<pages>49--1</pages>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Bruni, E., N. Tran, and M. Baroni. 2014. Multimodal distributional semantics. J. Artif. Intell. Res. (JAIR) 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruni</author>
<author>J Uijlings</author>
<author>M Baroni</author>
<author>N Sebe</author>
</authors>
<title>Distributional Semantics with Eyes: Using Image Analysis to Improve Computational Representations of Word Meaning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Multimedia.</booktitle>
<pages>1219--1228</pages>
<location>Nara, Japan,</location>
<contexts>
<context position="2301" citStr="Bruni et al., 2012" startWordPosition="332" endWordPosition="335">e related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by ma</context>
<context position="6097" citStr="Bruni et al. (2012" startWordPosition="904" endWordPosition="907">jects into meaningful semantic categories (e.g., peach and apple are members of FRUIT, whereas chair and table are FURNITURE). On both tasks, our model outperforms baselines and related models. 2 Related Work The presented model has connections to several lines of work in NLP, computer vision research, and more generally multimodal learning. We review related work in these areas below. Grounded Semantic Spaces Grounded semantic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtaine</context>
</contexts>
<marker>Bruni, Uijlings, Baroni, Sebe, 2012</marker>
<rawString>Bruni, E., J. Uijlings, M. Baroni, and N. Sebe. 2012b. Distributional Semantics with Eyes: Using Image Analysis to Improve Computational Representations of Word Meaning. In Proceedings of the 20th ACM International Conference on Multimedia. Nara, Japan, pages 1219–1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural Language Processing (almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research</journal>
<pages>12--2493</pages>
<contexts>
<context position="925" citStr="Collobert et al., 2011" startWordPosition="128" endWordPosition="131">he problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. 1 Introduction Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semant</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Collobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Deng</author>
<author>W Dong</author>
<author>R Socher</author>
<author>L Li</author>
<author>K Li</author>
<author>L Fei-Fei</author>
</authors>
<title>ImageNet: A Large-Scale Hierarchical Image Database.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.</booktitle>
<pages>248--255</pages>
<location>Miami, Florida,</location>
<contexts>
<context position="19875" citStr="Deng et al., 2009" startWordPosition="3111" endWordPosition="3114">e and a word are associated. We only retained the ten highest scored attributes for each target word. This returned a total of 2,362 dimensions for the textual vectors. Association scores were scaled to the [−1,1] range. To obtain visual vectors, we followed the methodology put forward in Silberer et al. (2013). Specifically, we used an updated version of their dataset to train SVM-based attribute classifiers that predict visual attributes for images (Farhadi et al., 2009). The dataset is a taxonomy of 636 visual attributes (e.g., has wings, made of wood) and nearly 700K images from ImageNet (Deng et al., 2009) describing more than 500 of McRae et al.’s (2005) nouns. The classifiers perform reasonably well with an interpolated average precision of 0.52. We only considered attributes assigned to at least two nouns in the dataset, obtaining a 414 dimensional vector for each noun. Analogously to the textual representations, visual vectors were scaled to the [−1,1] range. We follow Silberer et al.’s (2013) partition of the dataset into training, validation, and test set and acquire visual vectors for each of the sets. We use the visual vectors of the training and development set for training the autoenc</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Deng, J., W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Miami, Florida, pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Farhadi</author>
<author>I Endres</author>
<author>D Hoiem</author>
<author>D Forsyth</author>
</authors>
<title>Describing Objects by their Attributes.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.</booktitle>
<pages>1778--1785</pages>
<location>Miami Beach, Florida,</location>
<contexts>
<context position="4593" citStr="Farhadi et al., 2009" startWordPosition="679" endWordPosition="682">the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics a modeling perspective, attributes allow for easier integration of different modalities, since these are rendered in the same medium, namely, language. Thirdly, attributes are well-suited to describing visual phenomena (e.g., objects, scenes, actions). They allow to generalize to new instances for which there are no training examples available and to transcend category and task boundaries whilst offering a generic description of visual data (Farhadi et al., 2009). Our model learns multimodal representations from attributes which are automatically inferred from text and images. We evaluate the embeddings it produces on two tasks, namely word similarity and categorization. In the first task, model estimates of word similarity (e.g., gem–jewel are similar but glass–magician are not) are compared against elicited similarity ratings. We performed a large-scale evaluation on a new dataset consisting of human similarity judgments for 7,576 word pairs. Unlike previous efforts such as the widely used WordSim353 collection (Finkelstein et al., 2002), our datase</context>
<context position="19734" citStr="Farhadi et al., 2009" startWordPosition="3085" endWordPosition="3088"> bat–species:n, bat–bite:v) from a lemmatized and POS-tagged corpus. Weights are log-likelihood ratio scores expressing how strongly an attribute and a word are associated. We only retained the ten highest scored attributes for each target word. This returned a total of 2,362 dimensions for the textual vectors. Association scores were scaled to the [−1,1] range. To obtain visual vectors, we followed the methodology put forward in Silberer et al. (2013). Specifically, we used an updated version of their dataset to train SVM-based attribute classifiers that predict visual attributes for images (Farhadi et al., 2009). The dataset is a taxonomy of 636 visual attributes (e.g., has wings, made of wood) and nearly 700K images from ImageNet (Deng et al., 2009) describing more than 500 of McRae et al.’s (2005) nouns. The classifiers perform reasonably well with an interpolated average precision of 0.52. We only considered attributes assigned to at least two nouns in the dataset, obtaining a 414 dimensional vector for each noun. Analogously to the textual representations, visual vectors were scaled to the [−1,1] range. We follow Silberer et al.’s (2013) partition of the dataset into training, validation, and tes</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Farhadi, A., I. Endres, D. Hoiem, and D. Forsyth. 2009. Describing Objects by their Attributes. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Miami Beach, Florida, pages 1778–1785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangxiang Feng</author>
<author>Ruifan Li</author>
<author>Xiaojie Wang</author>
</authors>
<title>Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice.</title>
<date>2013</date>
<booktitle>In Proceedings of the ICML 2013 Workshop on Challenges in Representation Learning.</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="8303" citStr="Feng et al., 2013" startWordPosition="1248" endWordPosition="1251">Deep Learning Our work employs deep learning (a.k.a deep networks) to project linguistic and visual information onto a unified representation that fuses the two modalities together. The goal of deep learning is to learn multiple levels of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts. A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa);</context>
</contexts>
<marker>Feng, Li, Wang, 2013</marker>
<rawString>Feng, Fangxiang, Ruifan Li, and Xiaojie Wang. 2013. Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice. In Proceedings of the ICML 2013 Workshop on Challenges in Representation Learning. Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual Information in Semantic Representation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<pages>91--99</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="2281" citStr="Feng and Lapata, 2010" startWordPosition="328" endWordPosition="331">texts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning r</context>
<context position="6461" citStr="Feng and Lapata (2010)" startWordPosition="955" endWordPosition="958">d work in these areas below. Grounded Semantic Spaces Grounded semantic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognitio</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Feng, Yansong and Mirella Lapata. 2010. Visual Information in Semantic Representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Los Angeles, California, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="5181" citStr="Finkelstein et al., 2002" startWordPosition="766" endWordPosition="769">of visual data (Farhadi et al., 2009). Our model learns multimodal representations from attributes which are automatically inferred from text and images. We evaluate the embeddings it produces on two tasks, namely word similarity and categorization. In the first task, model estimates of word similarity (e.g., gem–jewel are similar but glass–magician are not) are compared against elicited similarity ratings. We performed a large-scale evaluation on a new dataset consisting of human similarity judgments for 7,576 word pairs. Unlike previous efforts such as the widely used WordSim353 collection (Finkelstein et al., 2002), our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning representation) together and in isolation. We also assess whether the learnt representations are appropriate for categorization, i.e., grouping a set of objects into meaningful semantic categories (e.g., peach and apple are members of FRUIT, whereas chair and table are FURNITURE). On both tasks, our model outperforms baselines and related models. 2 Related Work The presented model has connections to several lines of work in NLP, computer vision research</context>
<context position="22980" citStr="Finkelstein et al., 2002" startWordPosition="3615" endWordPosition="3618">tent layer for all the evaluation tasks detailed below. Some performance gains could be expected if parameter optimization took place separately for each task. However, we wanted to avoid overfitting, and show that our parameters are robust across tasks and datasets. 4.3 Evaluation Tasks Word Similarity We first evaluated how well our model predicts word similarity ratings. Although several relevant datasets exist, such as 3http://w3.usf.edu/Freeassociation. 4435 word pairs constitute the overlap between Nelson et al.’s norms (1998) and McRae et al.’s (2005) nouns. the widely used WordSim353 (Finkelstein et al., 2002) or the more recent Rel-122 norms (Szumlanski et al., 2013), they contain many abstract words, (e.g., love–sex or arrest–detention) which are not covered in McRae et al. (2005). This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon. We thus created a new dataset consisting exclusively of McRae et al. (2005) nouns which we hope will be useful for the development and evaluation of grounded semantic space models.5 Initially, we created all possible pairings over McRae et al.’s (2005) nouns and computed their</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Fountain</author>
<author>Mirella Lapata</author>
</authors>
<title>Meaning Representation in Natural Language Categorization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</booktitle>
<pages>pages</pages>
<location>Amsterdam, The</location>
<contexts>
<context position="26712" citStr="Fountain and Lapata (2010)" startWordPosition="4199" endWordPosition="4202">r semantic similarity and 0.48 for visual similarity. The correlation between the average ratings of the AMT annotators and the Miller and Charles (1991) dataset was ρ = 0.91. In our experiments (see Section 5), we correlate modelbased cosine similarities with mean similarity ratings (again using Spearman’s ρ). Categorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language. We evaluated model output against a gold standard set of categories created by Fountain and Lapata (2010). The dataset contains a classification, produced by human participants, of McRae et al.’s (2005) nouns into (possibly multiple) semantic categories (40 in total).6 To obtain a clustering of nouns, we used Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor6The dataset can be downloaded from http: //homepages.inf.ed.ac.uk/s0897549/data/. respond to words and edges to cosine similarity scores between vectors representing their meaning. CW is a non-parametr</context>
<context position="28096" citStr="Fountain and Lapata, 2010" startWordPosition="4411" endWordPosition="4414">ed Chinese Whispers with different graphs resulting from different vector-based representations of the McRae et al. (2005) nouns. We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see Fountain and Lapata, 2010). CW can optionally apply a minimum weight threshold which we optimized using the categorization dataset from Baroni et al. (2010). The latter contains a classification of 82 McRae et al. (2005) nouns into 10 categories. These nouns were excluded from the gold standard (Fountain and Lapata, 2010) in our final evaluation. We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 4.4 Comparison with Other Models Throughout our experiments we compare a bimodal stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1, respectively). We </context>
</contexts>
<marker>Fountain, Lapata, 2010</marker>
<rawString>Fountain, Trevor and Mirella Lapata. 2010. Meaning Representation in Natural Language Categorization. In Proceedings of the 31st Annual Conference of the Cognitive Science Society. Amsterdam, The Netherlands, pages 1916– 1921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene Golub</author>
<author>Christian Reinsch</author>
</authors>
<title>Singular Value Decomposition and Least Squares Solutions.</title>
<date>1970</date>
<journal>Numerische Mathematik</journal>
<volume>14</volume>
<issue>5</issue>
<pages>420</pages>
<contexts>
<context position="29168" citStr="Golub and Reinsch, 1970" startWordPosition="4595" endWordPosition="4598">l stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1, respectively). We also compare our model against two approaches that differ in their fusion mechanisms. The first one is based on kernelized canonical correlation (kCCA, Hardoon et al., 2004) with a linear kernel which was the best performing model in Silberer et al. (2013). The second one emulates Bruni et al.’s (2014) fusion mechanism. Specifically, we concatenate the textual and visual vectors and project them onto a lower dimensional latent space using SVD (Golub and Reinsch, 1970). All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations. We furthermore report results obtained with Bruni et al.’s (2014) bimodal distributional model, which employs SVD to integrate co-occurrencebased textual representations with visual repre727 McRae 0.71 0.49 0.68 0.58 0.52 0.62 Attributes 0.58 0.61 0.68 0.46 0.56 0.58 SAE 0.65 0.60 0.70 0.52 0.60 0.64 SVD — — 0.67 — — 0.57 kCCA — — 0.57 — — 0.55 Bruni — — 0.52 — — 0.46 RNN-640 0.41 — — 0.34 — — Table 3: Correlation of model predictions against </context>
</contexts>
<marker>Golub, Reinsch, 1970</marker>
<rawString>Golub, Gene and Christian Reinsch. 1970. Singular Value Decomposition and Least Squares Solutions. Numerische Mathematik 14(5):403– 420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>J B Tenenbaum</author>
</authors>
<title>Topics in Semantic Representation.</title>
<date>2007</date>
<journal>Psychological Review</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="1456" citStr="Griffiths et al., 2007" startWordPosition="204" endWordPosition="207">ge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and per</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum. 2007. Topics in Semantic Representation. Psychological Review 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Hardoon</author>
<author>S R Szedmak</author>
<author>J R ShaweTaylor</author>
</authors>
<title>Canonical Correlation Analysis: An Overview with Application to Learning Methods.</title>
<date>2004</date>
<journal>Neural Computation</journal>
<volume>16</volume>
<issue>12</issue>
<pages>2664</pages>
<contexts>
<context position="28869" citStr="Hardoon et al., 2004" startWordPosition="4543" endWordPosition="4546">); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 4.4 Comparison with Other Models Throughout our experiments we compare a bimodal stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1, respectively). We also compare our model against two approaches that differ in their fusion mechanisms. The first one is based on kernelized canonical correlation (kCCA, Hardoon et al., 2004) with a linear kernel which was the best performing model in Silberer et al. (2013). The second one emulates Bruni et al.’s (2014) fusion mechanism. Specifically, we concatenate the textual and visual vectors and project them onto a lower dimensional latent space using SVD (Golub and Reinsch, 1970). All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations. We furthermore report results obtained with Bruni et al.’s (2014) bimodal distributional model, which employs SVD to integrate co-occurrencebased te</context>
</contexts>
<marker>Hardoon, Szedmak, ShaweTaylor, 2004</marker>
<rawString>Hardoon, D. R., S. R. Szedmak, and J. R. ShaweTaylor. 2004. Canonical Correlation Analysis: An Overview with Application to Learning Methods. Neural Computation 16(12):2639– 2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional Structure.</title>
<date>1970</date>
<booktitle>In Papers in Structural and Transformational Linguistics,</booktitle>
<pages>775--794</pages>
<contexts>
<context position="1615" citStr="Harris, 1970" startWordPosition="227" endWordPosition="228">lications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual infor</context>
</contexts>
<marker>Harris, 1970</marker>
<rawString>Harris, Zellig. 1970. Distributional Structure. In Papers in Structural and Transformational Linguistics, pages 775–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<date>2006</date>
<journal>Reducing the Dimensionality of Data with Neural Networks. Science</journal>
<volume>313</volume>
<issue>5786</issue>
<pages>507</pages>
<contexts>
<context position="12544" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="1922" endWordPosition="1925">e-training procedure, initial parameters are found which approximate a good solution. Subsequently, the original input layer and hidden representations of all the autoencoders are stacked and all network parameters are fine-tuned with backpropagation. To further optimize the parameters of the network, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task (Bengio, 2009). Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error (Hinton and Salakhutdinov, 2006). Alternatively, a semi-supervised criterion can be used (Ranzato and Szummer, 2008; Socher et al., 2011) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent representation). 3.2 Semantic Representations To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs). Both input modalities are vector-based representations of words, or, more precisely, the objects they refer to (e.g., canary, trolley). The vector dimensions corr</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>Hinton, Geoffrey E. and Ruslan R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science 313(5786):504– 507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>47--853</pages>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Hodosh, Micah, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence Research 47:853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Huang</author>
<author>Brian Kingsbury</author>
</authors>
<title>Audiovisual Deep Learning for Noise Robust Speech Recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<pages>7596--7599</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="8540" citStr="Huang and Kingsbury, 2013" startWordPosition="1287" endWordPosition="1290">levels of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts. A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their optimal combination. Secondly, our problem setting is different from the former studies, which usually deal with classification tasks and fine-tune the d</context>
</contexts>
<marker>Huang, Kingsbury, 2013</marker>
<rawString>Huang, Jing and Brian Kingsbury. 2013. Audiovisual Deep Learning for Noise Robust Speech Recognition. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing. Vancouver, Canada, pages 7596–7599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
<author>B Rey</author>
<author>O Madani</author>
<author>W Greiner</author>
</authors>
<title>Generating Query Substititions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th International Conference on the World-Wide Web.</booktitle>
<pages>387--396</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1120" citStr="Jones et al., 2006" startWordPosition="157" endWordPosition="160">e two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. 1 Introduction Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, how</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Jones, R., B. Rey, O. Madani, and W. Greiner. 2006. Generating Query Substititions. In Proceedings of the 15th International Conference on the World-Wide Web. Edinburgh, Scotland, pages 387–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Landau</author>
<author>L Smith</author>
<author>S Jones</author>
</authors>
<title>Object Perception and Object Naming in Early Development.</title>
<date>1998</date>
<booktitle>Trends in Cognitive Science</booktitle>
<pages>27--19</pages>
<contexts>
<context position="1881" citStr="Landau et al., 1998" startWordPosition="268" endWordPosition="271">ence figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meanin</context>
</contexts>
<marker>Landau, Smith, Jones, 1998</marker>
<rawString>Landau, B., L. Smith, and S. Jones. 1998. Object Perception and Object Naming in Early Development. Trends in Cognitive Science 27:19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A Solution to Plato’s Problem: the Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.</title>
<date>1997</date>
<journal>Psychological Review</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="6063" citStr="Landauer and Dumais, 1997" startWordPosition="897" endWordPosition="900">categorization, i.e., grouping a set of objects into meaningful semantic categories (e.g., peach and apple are members of FRUIT, whereas chair and table are FURNITURE). On both tasks, our model outperforms baselines and related models. 2 Related Work The presented model has connections to several lines of work in NLP, computer vision research, and more generally multimodal learning. We review related work in these areas below. Grounded Semantic Spaces Grounded semantic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapat</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas and Susan T. Dumais. 1997. A Solution to Plato’s Problem: the Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lowe</author>
</authors>
<title>Distinctive Image Features from Scale-invariant Keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="30145" citStr="Lowe, 2004" startWordPosition="4757" endWordPosition="4758">.58 0.52 0.62 Attributes 0.58 0.61 0.68 0.46 0.56 0.58 SAE 0.65 0.60 0.70 0.52 0.60 0.64 SVD — — 0.67 — — 0.57 kCCA — — 0.57 — — 0.55 Bruni — — 0.52 — — 0.46 RNN-640 0.41 — — 0.34 — — Table 3: Correlation of model predictions against similarity ratings for McRae et al. (2005) noun pairs (using Spearman’s p). sentations constructed from low-level image features. In their model, the textual modality is represented by the 30K-dimensional vectors extracted from UKWaC and WaCkypedia.7 The visual modality is represented by bag-of-visualwords histograms built on the basis of clustered SIFT features (Lowe, 2004). We rebuilt their model on the ESP image dataset (von Ahn and Dabbish, 2004) using Bruni et al.’s (2013) publicly available system. Finally, we also compare to the word embeddings obtained using Mikolov et al.’s (2011) recurrent neural network based language model. These were pre-trained on Broadcast news data (400M words) using the word2vec tool.8 We report results with the 640-dimensional embeddings as they performed best. 5 Results Table 3 presents our results on the word similarity task. We report correlation coefficients of model predictions against similarity ratings. As an indicator to</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>Lowe, D. 2004. Distinctive Image Features from Scale-invariant Keypoints. International Journal of Computer Vision 60(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Manning, C. D., P. Raghavan, and H. Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>G S Cree</author>
<author>M S Seidenberg</author>
<author>C McNorgan</author>
</authors>
<title>Semantic Feature Production Norms for a Large Set of Living and Nonliving Things.</title>
<date>2005</date>
<journal>Behavior Research Methods</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="3839" citStr="McRae et al., 2005" startWordPosition="568" endWordPosition="571">t al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing that an attribute-centric representation is expedient for several reasons. Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g., McRae et al., 2005) where humans often employ attributes when asked to describe a concept. Secondly, from 721 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics a modeling perspective, attributes allow for easier integration of different modalities, since these are rendered in the same medium, namely, language. Thirdly, attributes are well-suited to describing visual phenomena (e.g., objects, scenes, actions). They allow to generalize to new instances for which there ar</context>
<context position="6759" citStr="McRae et al. (2005)" startWordPosition="1001" endWordPosition="1004">ucted textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss. The visual and textual modalities on which our model is trained are decoupled in that they are not derived from the same corpus (we would expect</context>
<context position="21153" citStr="McRae et al. (2005)" startWordPosition="3319" endWordPosition="3322">e corpus is downloadable from http://wacky. sslmit.unibo.it/doku.php?id=corpora. 725 4.2 Model Architecture Model parameters were optimized on a subset of the word association norms collected by Nelson et al. (1998).3 These were established by presenting participants with a cue word (e.g., canary) and asking them to name an associate word in response (e.g., bird, sing, yellow). For each cue, the norms provide a set of associates and the frequencies with which they were named. The dataset contains a very large number of cue-associate pairs (63,619 in total) some of which luckily are covered in McRae et al. (2005).4 During training we used correlation analysis (Spearman’s p) to monitor the degree of linear relationship between model cue-associate (cosine) similarities and human probabilities. The best autoencoder on the word association task obtained a correlation coefficient of 0.33. This performance is superior to the results reported in Silberer et al. (2013) (their correlation coefficients range from 0.16 to 0.28). This model has the following architecture: the textual autoencoder (see Figure 1, left-hand side) consists of 700 hidden units which are then mapped to the second hidden layer with 500 u</context>
<context position="23156" citStr="McRae et al. (2005)" startWordPosition="3644" endWordPosition="3647">avoid overfitting, and show that our parameters are robust across tasks and datasets. 4.3 Evaluation Tasks Word Similarity We first evaluated how well our model predicts word similarity ratings. Although several relevant datasets exist, such as 3http://w3.usf.edu/Freeassociation. 4435 word pairs constitute the overlap between Nelson et al.’s norms (1998) and McRae et al.’s (2005) nouns. the widely used WordSim353 (Finkelstein et al., 2002) or the more recent Rel-122 norms (Szumlanski et al., 2013), they contain many abstract words, (e.g., love–sex or arrest–detention) which are not covered in McRae et al. (2005). This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon. We thus created a new dataset consisting exclusively of McRae et al. (2005) nouns which we hope will be useful for the development and evaluation of grounded semantic space models.5 Initially, we created all possible pairings over McRae et al.’s (2005) nouns and computed their semantic relatedness using Patwardhan and Pedersen (2006)’s WordNet-based measure. We opted for this specific measure as it achieves high correlation with human ratings and ha</context>
<context position="25670" citStr="McRae et al. (2005)" startWordPosition="4037" endWordPosition="4040">t as the average pairwise correlation coefficient (Spearman’s p) between the ratings of all annotators for each task. For semantic similarity, the mean correlation was 0.76 (Min =0.34, Max 5Available from http://homepages.inf.ed.ac.uk/ mlap/index.php?page=resources. 726 Word Pairs Semantic Visual football–pillow 1.0 1.2 dagger–pencil 1.0 2.2 motorcycle–wheel 2.4 1.8 orange–pumpkin 2.5 3.0 cherry–pineapple 3.6 1.2 pickle–zucchini 3.6 4.0 canary–owl 4.0 2.4 jeans–sweater 4.5 2.2 pan–pot 4.7 4.0 hornet–wasp 4.8 4.8 airplane–jet 5.0 5.0 Table 2: Mean semantic and visual similarity ratings for the McRae et al. (2005) nouns using a scale of 1 (highly dissimilar) to 5 (highly similar). =0.97, StD =0.11) and for visual similarity 0.63 (Min =0.19, Max =0.90, SD =0.14). These results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency. For comparison, Patwardhan and Pedersen’s (2006) measure achieved a coefficient of 0.56 on the dataset for semantic similarity and 0.48 for visual similarity. The correlation between the average ratings of the AMT annotators and the Miller and Charles (1991) dataset was ρ = 0.91. In our e</context>
<context position="27592" citStr="McRae et al. (2005)" startWordPosition="4327" endWordPosition="4330">ustering algorithm. In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor6The dataset can be downloaded from http: //homepages.inf.ed.ac.uk/s0897549/data/. respond to words and edges to cosine similarity scores between vectors representing their meaning. CW is a non-parametric model, it induces the number of clusters (i.e., categories) from the data as well as which nouns belong to these clusters. In our experiments, we initialized Chinese Whispers with different graphs resulting from different vector-based representations of the McRae et al. (2005) nouns. We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see Fountain and Lapata, 2010). CW can optionally apply a minimum weight threshold which we optimized using the categorization dataset from Baroni et al. (2010). The latter contains a classification of 82 McRae et al. (2005) nouns into 10 categories. These nouns were excluded from the gold standard (Fountain and Lapata, 2010) in our final evaluation. We evaluated the clusters produced by CW using the F-score measure int</context>
<context position="29810" citStr="McRae et al. (2005)" startWordPosition="4706" endWordPosition="4709">on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations. We furthermore report results obtained with Bruni et al.’s (2014) bimodal distributional model, which employs SVD to integrate co-occurrencebased textual representations with visual repre727 McRae 0.71 0.49 0.68 0.58 0.52 0.62 Attributes 0.58 0.61 0.68 0.46 0.56 0.58 SAE 0.65 0.60 0.70 0.52 0.60 0.64 SVD — — 0.67 — — 0.57 kCCA — — 0.57 — — 0.55 Bruni — — 0.52 — — 0.46 RNN-640 0.41 — — 0.34 — — Table 3: Correlation of model predictions against similarity ratings for McRae et al. (2005) noun pairs (using Spearman’s p). sentations constructed from low-level image features. In their model, the textual modality is represented by the 30K-dimensional vectors extracted from UKWaC and WaCkypedia.7 The visual modality is represented by bag-of-visualwords histograms built on the basis of clustered SIFT features (Lowe, 2004). We rebuilt their model on the ESP image dataset (von Ahn and Dabbish, 2004) using Bruni et al.’s (2013) publicly available system. Finally, we also compare to the word embeddings obtained using Mikolov et al.’s (2011) recurrent neural network based language model</context>
<context position="33520" citStr="McRae et al. (2005)" startWordPosition="5281" endWordPosition="5284">semantic similarity judgments (p = 0.65) than its visual equivalent (p = 0.60). And the visual SAE correlates better with visual similarity judgments (p = 0.60) compared to the textual SAE (p = 0.52). Interestingly, the bimodal SAE is better than the unimodal variants on both types of similarity judgments, semantic and visual. This suggests that both modalities contribute complementary information and that the SAE model is able to extract a shared representation which improves generalization performance across tasks by learning them 9Classification of attributes into categories is provided by McRae et al. (2005) in their dataset. Semantic Visual Models T V T+V T V T+V 728 Models T V T+V Table 5: F-score results on concept categorization. jointly. The bimodal autoencoder (SAE, T+V) outperforms all other bimodal models on both similarity tasks. It yields a correlation coefficient of ρ = 0.70 on semantic similarity and ρ = 0.64 on visual similarity. Human agreement on the former task is 0.76 and 0.63 on the latter. Table 4 shows examples of word pairs with highest semantic and visual similarity according to the SAE model. We also observe that simply concatenating textual and visual attributes (Attribute</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>McRae, K., G. S. Cree, M. S. Seidenberg, and C. McNorgan. 2005. Semantic Feature Production Norms for a Large Set of Living and Nonliving Things. Behavior Research Methods 37(4):547–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>S Kombrink</author>
<author>L Burget</author>
<author>J ˇCernock´y</author>
<author>S Khudanpur</author>
</authors>
<title>Extensions of Recurrent Neural Network Language Model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<pages>5528--5531</pages>
<location>Prague, Czech Republic,</location>
<marker>Mikolov, Kombrink, Burget, ˇCernock´y, Khudanpur, 2011</marker>
<rawString>Mikolov, T., S. Kombrink, L. Burget, J. ˇCernock´y, and S. Khudanpur. 2011. Extensions of Recurrent Neural Network Language Model. In Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing. Prague, Czech Republic, pages 5528– 5531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>Wen-tau Yih</author>
<author>G Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</booktitle>
<pages>746--751</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="948" citStr="Mikolov et al., 2013" startWordPosition="132" endWordPosition="135">distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. 1 Introduction Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Mikolov, T., Wen-tau Yih, and G. Zweig. 2013. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta, Georgia, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<date>1991</date>
<booktitle>Contextual Correlates of Semantic Similarity. Language and Cognitive Processes</booktitle>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="24346" citStr="Miller and Charles (1991)" startWordPosition="3838" endWordPosition="3841">rrelation with human ratings and has a high coverage on our nouns. Next, for each word we randomly selected 30 pairs under the assumption that they are representative of the full variation of semantic similarity. This resulted in 7,576 word pairs for which we obtained similarity ratings using Amazon Mechanical Turk (AMT). Participants were asked to rate a pair on two dimensions, visual and semantic similarity using a Likert scale of 1 (highly dissimilar) to 5 (highly similar). Each task consisted of 32 pairs covering examples of weak to very strong semantic relatedness. Two control pairs from Miller and Charles (1991) were included in each task to potentially help identify and eliminate data from participants who assigned random scores. Examples of the stimuli and mean ratings are shown in Table 2. The elicitation study comprised overall 255 tasks, each task was completed by five volunteers. The similarity data was post-processed so as to identify and remove outliers. We considered an outlier to be any individual whose mean pairwise correlation fell outside two standard deviations from the mean correlation. 11.5% of the annotations were detected as outliers and removed. After outlier removal, we further ex</context>
<context position="26239" citStr="Miller and Charles (1991)" startWordPosition="4126" endWordPosition="4129">nd visual similarity ratings for the McRae et al. (2005) nouns using a scale of 1 (highly dissimilar) to 5 (highly similar). =0.97, StD =0.11) and for visual similarity 0.63 (Min =0.19, Max =0.90, SD =0.14). These results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency. For comparison, Patwardhan and Pedersen’s (2006) measure achieved a coefficient of 0.56 on the dataset for semantic similarity and 0.48 for visual similarity. The correlation between the average ratings of the AMT annotators and the Miller and Charles (1991) dataset was ρ = 0.91. In our experiments (see Section 5), we correlate modelbased cosine similarities with mean similarity ratings (again using Spearman’s ρ). Categorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language. We evaluated model output against a gold standard set of categories created by Fountain and Lapata (2010). The dataset contains a classification, produced by human participants, of McRae et al.’s (2005) nouns into (possibly multiple</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>Miller, George A. and Walter G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes 6(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Nelson</author>
<author>C L McEvoy</author>
<author>T A Schreiber</author>
</authors>
<date>1998</date>
<institution>The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</institution>
<contexts>
<context position="20749" citStr="Nelson et al. (1998)" startWordPosition="3248" endWordPosition="3251">tor for each noun. Analogously to the textual representations, visual vectors were scaled to the [−1,1] range. We follow Silberer et al.’s (2013) partition of the dataset into training, validation, and test set and acquire visual vectors for each of the sets. We use the visual vectors of the training and development set for training the autoencoders, and the vectors for the test set for evaluation. 2The corpus is downloadable from http://wacky. sslmit.unibo.it/doku.php?id=corpora. 725 4.2 Model Architecture Model parameters were optimized on a subset of the word association norms collected by Nelson et al. (1998).3 These were established by presenting participants with a cue word (e.g., canary) and asking them to name an associate word in response (e.g., bird, sing, yellow). For each cue, the norms provide a set of associates and the frequencies with which they were named. The dataset contains a very large number of cue-associate pairs (63,619 in total) some of which luckily are covered in McRae et al. (2005).4 During training we used correlation analysis (Spearman’s p) to monitor the degree of linear relationship between model cue-associate (cosine) similarities and human probabilities. The best auto</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>Nelson, D. L., C. L. McEvoy, and T. A. Schreiber. 1998. The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiquan Ngiam</author>
<author>Aditya Khosla</author>
<author>Mingyu Kim</author>
<author>Juhan Nam</author>
<author>Honglak Lee</author>
<author>Andrew Ng</author>
</authors>
<title>Multimodal Deep Learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning.</booktitle>
<pages>689--696</pages>
<location>Bellevue, Washington,</location>
<contexts>
<context position="3231" citStr="Ngiam et al., 2011" startWordPosition="471" endWordPosition="474"> vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by mapping words and images into a common embedding space. Our model uses stacked autoencoders (Bengio et al., 2007) to induce semantic representations integrating visual and textual information. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing that an attribute-centric representation is expedient for several reasons. Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g., McRae et al</context>
<context position="8502" citStr="Ngiam et al., 2011" startWordPosition="1281" endWordPosition="1284"> learning is to learn multiple levels of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts. A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their optimal combination. Secondly, our problem setting is different from the former studies, which usually deal with cl</context>
</contexts>
<marker>Ngiam, Khosla, Kim, Nam, Lee, Ng, 2011</marker>
<rawString>Ngiam, Jiquan, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Ng. 2011. Multimodal Deep Learning. In Proceedings of the 28th International Conference on Machine Learning. Bellevue, Washington, pages 689–696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together.</booktitle>
<pages>1--8</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="23638" citStr="Patwardhan and Pedersen (2006)" startWordPosition="3721" endWordPosition="3725">norms (Szumlanski et al., 2013), they contain many abstract words, (e.g., love–sex or arrest–detention) which are not covered in McRae et al. (2005). This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon. We thus created a new dataset consisting exclusively of McRae et al. (2005) nouns which we hope will be useful for the development and evaluation of grounded semantic space models.5 Initially, we created all possible pairings over McRae et al.’s (2005) nouns and computed their semantic relatedness using Patwardhan and Pedersen (2006)’s WordNet-based measure. We opted for this specific measure as it achieves high correlation with human ratings and has a high coverage on our nouns. Next, for each word we randomly selected 30 pairs under the assumption that they are representative of the full variation of semantic similarity. This resulted in 7,576 word pairs for which we obtained similarity ratings using Amazon Mechanical Turk (AMT). Participants were asked to rate a pair on two dimensions, visual and semantic similarity using a Likert scale of 1 (highly dissimilar) to 5 (highly similar). Each task consisted of 32 pairs cov</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Patwardhan, Siddharth and Ted Pedersen. 2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together. Trento, Italy, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc’Aurelio Ranzato</author>
<author>Martin Szummer</author>
</authors>
<title>Semi-supervised Learning of Compact Document Representations with Deep Networks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning.</booktitle>
<pages>792--799</pages>
<location>Helsinki, Finland,</location>
<contexts>
<context position="12627" citStr="Ranzato and Szummer, 2008" startWordPosition="1933" endWordPosition="1936">sequently, the original input layer and hidden representations of all the autoencoders are stacked and all network parameters are fine-tuned with backpropagation. To further optimize the parameters of the network, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task (Bengio, 2009). Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error (Hinton and Salakhutdinov, 2006). Alternatively, a semi-supervised criterion can be used (Ranzato and Szummer, 2008; Socher et al., 2011) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent representation). 3.2 Semantic Representations To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs). Both input modalities are vector-based representations of words, or, more precisely, the objects they refer to (e.g., canary, trolley). The vector dimensions correspond to textual and visual attributes, examples of which are shown in Table 1. We</context>
</contexts>
<marker>Ranzato, Szummer, 2008</marker>
<rawString>Ranzato, Marc’Aurelio and Martin Szummer. 2008. Semi-supervised Learning of Compact Document Representations with Deep Networks. In Proceedings of the 25th International Conference on Machine Learning. Helsinki, Finland, pages 792–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>The Human Semantic Potential.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1860" citStr="Regier, 1996" startWordPosition="266" endWordPosition="267"> cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize </context>
</contexts>
<marker>Regier, 1996</marker>
<rawString>Regier, Terry. 1996. The Human Semantic Potential. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>1146--1157</pages>
<location>Seattle, Washington,</location>
<marker>Roller, Walde, 2013</marker>
<rawString>Roller, Stephen and Sabine Schulte im Walde. 2013. A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, Washington, pages 1146–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine Learning in Automated Text Categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys</journal>
<pages>34--1</pages>
<contexts>
<context position="1164" citStr="Sebastiani, 2002" startWordPosition="163" endWordPosition="165">ributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. 1 Introduction Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Wo</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Sebastiani, Fabrizio. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys 34:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Silberer</author>
<author>V Ferrari</author>
<author>M Lapata</author>
</authors>
<title>Models of Semantic Representation with Visual Attributes.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>572--582</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2326" citStr="Silberer et al., 2013" startWordPosition="336" endWordPosition="339">ord meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by mapping words and images in</context>
<context position="3600" citStr="Silberer et al. (2013)" startWordPosition="532" endWordPosition="535">autoencoders (Bengio et al., 2007) to induce semantic representations integrating visual and textual information. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing that an attribute-centric representation is expedient for several reasons. Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g., McRae et al., 2005) where humans often employ attributes when asked to describe a concept. Secondly, from 721 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics a modeling perspective, attributes allow for easier integration of differe</context>
<context position="7086" citStr="Silberer et al. (2013)" startWordPosition="1052" endWordPosition="1055">e visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss. The visual and textual modalities on which our model is trained are decoupled in that they are not derived from the same corpus (we would expect co-occurring images and text to correlate to some extent) but unified in their representation by natural language attributes. The use of stacked autoencoders to extract a shared lexical meaning representation is new to our knowledge, although, as we explain below related to a large body of work on deep learning. Multimodal D</context>
<context position="19569" citStr="Silberer et al. (2013)" startWordPosition="3061" endWordPosition="3064">running Strudel (Baroni et al., 2010) on a 2009 dump of the English Wikipedia.2 Strudel is a fully automatic method for extracting weighted wordattribute pairs (e.g., bat–species:n, bat–bite:v) from a lemmatized and POS-tagged corpus. Weights are log-likelihood ratio scores expressing how strongly an attribute and a word are associated. We only retained the ten highest scored attributes for each target word. This returned a total of 2,362 dimensions for the textual vectors. Association scores were scaled to the [−1,1] range. To obtain visual vectors, we followed the methodology put forward in Silberer et al. (2013). Specifically, we used an updated version of their dataset to train SVM-based attribute classifiers that predict visual attributes for images (Farhadi et al., 2009). The dataset is a taxonomy of 636 visual attributes (e.g., has wings, made of wood) and nearly 700K images from ImageNet (Deng et al., 2009) describing more than 500 of McRae et al.’s (2005) nouns. The classifiers perform reasonably well with an interpolated average precision of 0.52. We only considered attributes assigned to at least two nouns in the dataset, obtaining a 414 dimensional vector for each noun. Analogously to the te</context>
<context position="21508" citStr="Silberer et al. (2013)" startWordPosition="3371" endWordPosition="3374"> (e.g., bird, sing, yellow). For each cue, the norms provide a set of associates and the frequencies with which they were named. The dataset contains a very large number of cue-associate pairs (63,619 in total) some of which luckily are covered in McRae et al. (2005).4 During training we used correlation analysis (Spearman’s p) to monitor the degree of linear relationship between model cue-associate (cosine) similarities and human probabilities. The best autoencoder on the word association task obtained a correlation coefficient of 0.33. This performance is superior to the results reported in Silberer et al. (2013) (their correlation coefficients range from 0.16 to 0.28). This model has the following architecture: the textual autoencoder (see Figure 1, left-hand side) consists of 700 hidden units which are then mapped to the second hidden layer with 500 units (the corruption parameter was set to v = 0.1); the visual autoencoder (see Figure 1, right-hand side) has 170 and 100 hidden units, in the first and second layer, respectively. The 500 textual and 100 visual hidden units were fed to a bimodal autoencoder containing 500 latent units, and masking noise was applied to the textual modality with v = 0.2</context>
<context position="28952" citStr="Silberer et al. (2013)" startWordPosition="4559" endWordPosition="4562">t members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 4.4 Comparison with Other Models Throughout our experiments we compare a bimodal stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1, respectively). We also compare our model against two approaches that differ in their fusion mechanisms. The first one is based on kernelized canonical correlation (kCCA, Hardoon et al., 2004) with a linear kernel which was the best performing model in Silberer et al. (2013). The second one emulates Bruni et al.’s (2014) fusion mechanism. Specifically, we concatenate the textual and visual vectors and project them onto a lower dimensional latent space using SVD (Golub and Reinsch, 1970). All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations. We furthermore report results obtained with Bruni et al.’s (2014) bimodal distributional model, which employs SVD to integrate co-occurrencebased textual representations with visual repre727 McRae 0.71 0.49 0.68 0.58 0.52 0.62 Attr</context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Silberer, C., V. Ferrari, and M. Lapata. 2013. Models of Semantic Representation with Visual Attributes. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, pages 572–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded Models of Semantic Representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Jeju Island, Korea,</booktitle>
<pages>1423--1433</pages>
<contexts>
<context position="2201" citStr="Silberer and Lapata, 2012" startWordPosition="318" endWordPosition="321">distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work</context>
<context position="6671" citStr="Silberer and Lapata, 2012" startWordPosition="988" endWordPosition="991"> and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss. The visual and textual modalities on which our model is </context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Silberer, Carina and Mirella Lapata. 2012. Grounded Models of Semantic Representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Jeju Island, Korea, pages 1423–1433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>M Ganjoo</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Zero-shot learning through crossmodal transfer.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>935--943</pages>
<contexts>
<context position="8390" citStr="Socher et al., 2013" startWordPosition="1263" endWordPosition="1266">ic and visual information onto a unified representation that fuses the two modalities together. The goal of deep learning is to learn multiple levels of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts. A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their opt</context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>Socher, R., M. Ganjoo, C. D. Manning, and A. Y. Ng. 2013a. Zero-shot learning through crossmodal transfer. In Advances in Neural Information Processing Systems 26, pages 935–943.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>Quoc V Le</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Grounded Compositional Semantics for Finding and Describing Images with Sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the NIPS Deep Learning Workshop.</booktitle>
<contexts>
<context position="8390" citStr="Socher et al., 2013" startWordPosition="1263" endWordPosition="1266">ic and visual information onto a unified representation that fuses the two modalities together. The goal of deep learning is to learn multiple levels of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts. A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their opt</context>
</contexts>
<marker>Socher, Le, Manning, Ng, 2013</marker>
<rawString>Socher, R., Quoc V. Le, C. D. Manning, and A. Y. Ng. 2013b. Grounded Compositional Semantics for Finding and Describing Images with Sentences. In Proceedings of the NIPS Deep Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E H Huang</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>151--161</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="12649" citStr="Socher et al., 2011" startWordPosition="1937" endWordPosition="1940">ut layer and hidden representations of all the autoencoders are stacked and all network parameters are fine-tuned with backpropagation. To further optimize the parameters of the network, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task (Bengio, 2009). Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error (Hinton and Salakhutdinov, 2006). Alternatively, a semi-supervised criterion can be used (Ranzato and Szummer, 2008; Socher et al., 2011) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent representation). 3.2 Semantic Representations To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs). Both input modalities are vector-based representations of words, or, more precisely, the objects they refer to (e.g., canary, trolley). The vector dimensions correspond to textual and visual attributes, examples of which are shown in Table 1. We explain how these rep</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Socher, R., J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Edinburgh, Scotland, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Multimodal Learning with Deep Boltzmann Machines.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25,</booktitle>
<pages>2231--2239</pages>
<contexts>
<context position="3268" citStr="Srivastava and Salakhutdinov, 2012" startWordPosition="475" endWordPosition="478">rom the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images). In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by mapping words and images into a common embedding space. Our model uses stacked autoencoders (Bengio et al., 2007) to induce semantic representations integrating visual and textual information. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing that an attribute-centric representation is expedient for several reasons. Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g., McRae et al., 2005) where humans often employ at</context>
<context position="8283" citStr="Srivastava and Salakhutdinov, 2012" startWordPosition="1244" endWordPosition="1247">f work on deep learning. Multimodal Deep Learning Our work employs deep learning (a.k.a deep networks) to project linguistic and visual information onto a unified representation that fuses the two modalities together. The goal of deep learning is to learn multiple levels of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts. A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from ima</context>
</contexts>
<marker>Srivastava, Salakhutdinov, 2012</marker>
<rawString>Srivastava, Nitish and Ruslan Salakhutdinov. 2012. Multimodal Learning with Deep Boltzmann Machines. In Advances in Neural Information Processing Systems 25, pages 2231– 2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
</authors>
<title>Combining Feature Norms and Text Data with Topic Models.</title>
<date>2010</date>
<journal>Acta Psychologica</journal>
<volume>133</volume>
<issue>3</issue>
<contexts>
<context position="2173" citStr="Steyvers, 2010" startWordPosition="316" endWordPosition="317">ra based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., te</context>
<context position="6621" citStr="Steyvers, 2010" startWordPosition="982" endWordPosition="983"> to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss. The vi</context>
</contexts>
<marker>Steyvers, 2010</marker>
<rawString>Steyvers, Mark. 2010. Combining Feature Norms and Text Data with Topic Models. Acta Psychologica 133(3):234–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Szumlanski</author>
<author>F Gomez</author>
<author>V K Sims</author>
</authors>
<title>A New Set of Norms for Semantic Relatedness Measures.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>890--895</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="23039" citStr="Szumlanski et al., 2013" startWordPosition="3625" endWordPosition="3629"> performance gains could be expected if parameter optimization took place separately for each task. However, we wanted to avoid overfitting, and show that our parameters are robust across tasks and datasets. 4.3 Evaluation Tasks Word Similarity We first evaluated how well our model predicts word similarity ratings. Although several relevant datasets exist, such as 3http://w3.usf.edu/Freeassociation. 4435 word pairs constitute the overlap between Nelson et al.’s norms (1998) and McRae et al.’s (2005) nouns. the widely used WordSim353 (Finkelstein et al., 2002) or the more recent Rel-122 norms (Szumlanski et al., 2013), they contain many abstract words, (e.g., love–sex or arrest–detention) which are not covered in McRae et al. (2005). This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon. We thus created a new dataset consisting exclusively of McRae et al. (2005) nouns which we hope will be useful for the development and evaluation of grounded semantic space models.5 Initially, we created all possible pairings over McRae et al.’s (2005) nouns and computed their semantic relatedness using Patwardhan and Pedersen (2006)’</context>
</contexts>
<marker>Szumlanski, Gomez, Sims, 2013</marker>
<rawString>Szumlanski, S. R., F. Gomez, and V. K. Sims. 2013. A New Set of Norms for Semantic Relatedness Measures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, pages 890– 895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="901" citStr="Turney and Pantel, 2010" startWordPosition="124" endWordPosition="127">n this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. 1 Introduction Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, Peter D. and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vincent</author>
<author>H Larochelle</author>
<author>I Lajoie</author>
<author>Y Bengio</author>
<author>P Manzagol</author>
</authors>
<title>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research</journal>
<pages>11--3371</pages>
<contexts>
<context position="11164" citStr="Vincent et al., 2010" startWordPosition="1703" endWordPosition="1706">ncoders are a means to learn representations of some input by retaining useful features in the encoding phase which help to reconstruct the input, whilst discarding useless or noisy ones. To this end, different strategies have been employed to guide parameter learning and constrain the hidden representation. Examples include imposing a bottleneck to produce an under-complete representation of the input, using sparse representations, or denoising. Denoising Autoencoders The training criterion with denoising autoencoders is the reconstruction of clean input x(i) given a corrupted version ˜x(i) (Vincent et al., 2010). The underlying idea is that the learned latent representation is good if the autoencoder is capable of reconstructing the actual input from its corruption. The reconstruction error for an input x(i) with loss function L then is: L(x(i),gθ0(fθ(˜x(i)))) (2) One possible corruption process is masking noise, where the corrupted version ˜x(i) results from randomly setting a fraction v of x(i) to 0. Stacked Autoencoders Several (denoising) autoencoders can be used as building blocks to form a deep neural network (Bengio et al., 2007; Vincent et al., 2010). For that purpose, the autoencoders are pr</context>
</contexts>
<marker>Vincent, Larochelle, Lajoie, Bengio, Manzagol, 2010</marker>
<rawString>Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. 2010. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. Journal of Machine Learning Research 11:3371–3408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling Images with a Computer Game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.</booktitle>
<pages>319--326</pages>
<location>Vienna, Austria,</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>von Ahn, Luis and Laura Dabbish. 2004. Labeling Images with a Computer Game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. Vienna, Austria, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pengcheng Wu</author>
<author>Steven C H Hoi</author>
<author>Hao Xia</author>
<author>Peilin Zhao</author>
<author>Dayong Wang</author>
<author>Chunyan Miao</author>
</authors>
<title>Online Multimodal Deep Similarity Learning with Application to Image Retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Multimedia.</booktitle>
<pages>153--162</pages>
<location>Barcelona,</location>
<contexts>
<context position="8338" citStr="Wu et al., 2013" startWordPosition="1255" endWordPosition="1258">earning (a.k.a deep networks) to project linguistic and visual information onto a unified representation that fuses the two modalities together. The goal of deep learning is to learn multiple levels of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts. A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an op</context>
</contexts>
<marker>Wu, Hoi, Xia, Zhao, Wang, Miao, 2013</marker>
<rawString>Wu, Pengcheng, Steven C. H. Hoi, Hao Xia, Peilin Zhao, Dayong Wang, and Chunyan Miao. 2013. Online Multimodal Deep Similarity Learning with Application to Image Retrieval. In Proceedings of the 21st ACM International Conference on Multimedia. Barcelona, Spain, pages 153–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Ming-Wei Chang</author>
<author>Christopher Meek</author>
<author>Andrzej Pastusiak</author>
</authors>
<title>Question Answering Using Enhanced Lexical Semantic Models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>1744--1753</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1207" citStr="Yih et al., 2013" startWordPosition="169" endWordPosition="172"> text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. 1 Introduction Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environmen</context>
</contexts>
<marker>Yih, Chang, Meek, Pastusiak, 2013</marker>
<rawString>Yih, Wen-tau, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, pages 1744–1753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>D H Ballard</author>
</authors>
<title>A Unified Model of Early Word Learning Integrating Statistical and Social Cues.</title>
<date>2007</date>
<journal>Neurocomputing</journal>
<volume>70</volume>
<pages>2165</pages>
<marker>Yu, Ballard, 2007</marker>
<rawString>Yu, C. and D. H. Ballard. 2007. A Unified Model of Early Word Learning Integrating Statistical and Social Cues. Neurocomputing 70:2149– 2165.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>