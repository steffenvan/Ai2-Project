<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.997752">
Efficient Parsing for Bilexical Context-Free Grammars
and Head Automaton Grammars*
</title>
<author confidence="0.999356">
Jason Eisner
</author>
<affiliation confidence="0.9985675">
Dept. of Computer &amp; Information Science Dip.
University of Pennsylvania
</affiliation>
<address confidence="0.9689145">
200 South 33rd Street,
Philadelphia, PA 19104 USA
</address>
<email confidence="0.998551">
jeisner@linc.cis.upenn.edu
</email>
<author confidence="0.730204">
Giorgio Satta
</author>
<affiliation confidence="0.4078455">
di Elettronica e Informatica
Universita di Padova
</affiliation>
<address confidence="0.873817">
via Gradenigo 6/A,
35131 Padova, Italy
</address>
<email confidence="0.99629">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.995586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878666666667">
Several recent stochastic parsers use bilexical
grammars, where each word type idiosyncrat-
ically prefers particular complements with par-
ticular head words. We present 0(n4) parsing
algorithms for two bilexical formalisms, improv-
ing the prior upper bounds of 0(n5). For a com-
mon special case that was known to allow 0(n3)
parsing (Eisner, 1997), we present an 0(n3) al-
gorithm with an improved grammar constant.
</bodyText>
<sectionHeader confidence="0.997892" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.947051244444444">
Lexicalized grammar formalisms are of both
theoretical and practical interest to the com-
putational linguistics community. Such for-
malisms specify syntactic facts about each word
of the language—in particular, the type of
arguments that the word can or must take.
Early mechanisms of this sort included catego-
rial grammar (Bar-Hillel, 1953) and subcatego-
rization frames (Chomsky, 1965). Other lexi-
calized formalisms include (Schabes et al., 1988;
Mel&apos;euk, 1988; Pollard and Sag, 1994).
Besides the possible arguments of a word, a
natural-language grammar does well to specify
possible head words for those arguments. &amp;quot;Con-
vene&amp;quot; requires an NP object, but some NPs are
more semantically or lexically appropriate here
than others, and the appropriateness depends
largely on the NP&apos;s head (e.g., &amp;quot;meeting&amp;quot;). We
use the general term bilexical for a grammar
that records such facts. A bilexical grammar
makes many stipulations about the compatibil-
ity of particular pairs of words in particular
roles. The acceptability of &amp;quot;Nora convened the
* The authors were supported respectively under ARPA
Grant N6600194-C-6043 &amp;quot;Human Language Technology&amp;quot;
and Ministero dell&apos;Universita e della Ricerca Scientifica
e Tecnologica project &amp;quot;Methodologies and Tools of High
Performance Systems for Multimedia Applications.&amp;quot;
party&amp;quot; then depends on the grammar writer&apos;s
assessment of whether parties can be convened.
Several recent real-world parsers have im-
proved state-of-the-art parsing accuracy by re-
lying on probabilistic or weighted versions of
bilexical grammars (Alshawi, 1996; Eisner,
1996; Charniak, 1997; Collins, 1997). The ra-
tionale is that soft selectional restrictions play
a crucial role in disambiguation.1
The chart parsing algorithms used by most of
the above authors run in time 0(n5), because
bilexical grammars are enormous (the part of
the grammar relevant to a length-n input has
size 0(n2) in practice). Heavy probabilistic
pruning is therefore needed to get acceptable
runtimes. But in this paper we show that the
complexity is not so bad after all:
</bodyText>
<listItem confidence="0.9944744">
• For bilexicalized context-free grammars,
0(n4) is possible.
• The 0(n4) result also holds for head au-
tomaton grammars.
• For a very common special case of these
</listItem>
<bodyText confidence="0.9651535">
grammars where an 0(n3) algorithm was
previously known (Eisner, 1997), the gram-
mar constant can be reduced without
harming the 0(n3) property.
Our algorithmic technique throughout is to pro-
pose new kinds of subderivations that are not
constituents. We use dynamic programming to
assemble such subderivations into a full parse.
</bodyText>
<sectionHeader confidence="0.9789695" genericHeader="method">
2 Notation for context-free
grammars
</sectionHeader>
<bodyText confidence="0.949445">
The reader is assumed to be familiar with
context-free grammars. Our notation fol-
</bodyText>
<footnote confidence="0.8811345">
&apos;Other relevant parsers simultaneously consider two
or more words that are not necessarily in a dependency
relationship (Lafferty et al., 1992; Magerman, 1995;
Collins and Brooks, 1995; Chelba and Jelinek, 1998).
</footnote>
<page confidence="0.995352">
457
</page>
<bodyText confidence="0.761742892857143">
lows (Harrison, 1978; Hoperoft and Ullman,
1979). A context-free grammar (CFG) is a tuple
G = (VN, VT , P, S), where VN and VT are finite,
disjoint sets of nonterminal and terminal sym-
bols, respectively, and S E VN is the start sym-
bol. Set P is a finite set of productions having
the form A —* a, where A E VN , a E (VN U VT)*.
If every production in P has the form A —4 BC
or A a, for A, B,C E VN , a E VT, then the
grammar is said to be in Chomsky Normal Form
(CNF).2 Every language that can be generated
by a CFG can also be generated by a CFG in
CNF.
In this paper we adopt the following conven-
tions: a, b, c, d denote symbols in VT, w, X, y de-
note strings in Vat, and a, 0, denote strings
in (VN U VT)*. The input to the parser will be a
CFG G together with a string of terminal sym-
bols to be parsed, w = d1d2 • • • dn. Also h,i, j,k
denote positive integers, which are assumed to
be &lt; n when we are treating them as indices
into w. We write wzo for the input substring
di • • • di (and put wj = e for i &gt; j).
A &amp;quot;derives&amp;quot; relation, written is associated
with a CFG as usual. We also use the reflexive
and transitive closure of written and
define L(G) accordingly. We write a 8 =*
a-0 for a derivation in which only /3 is rewritten.
</bodyText>
<sectionHeader confidence="0.713246" genericHeader="method">
3 Bilexical context-free grammars
</sectionHeader>
<bodyText confidence="0.99977175">
We introduce next a grammar formalism that
captures lexical dependencies among pairs of
words in VT. This formalism closely resem-
bles stochastic grammatical formalisms that are
used in several existing natural language pro-
cessing systems (see §1). We will specify a non-
stochastic version, noting that probabilities or
other weights may be attached to the rewrite
rules exactly as in stochastic CFG (Gonzales
and Thomason, 1978; Wetherell, 1980). (See
§4 for brief discussion.)
Suppose G = (VN, VT , P,T[$]) is a CFG in
CNF.3 We say that G is bilexical if there exists
a set of &amp;quot;delexicalized nonterminals&amp;quot; VD such
that VN = {A[a] : A E VD, a E VT} and every
production in P has one of the following forms:
</bodyText>
<footnote confidence="0.971425">
2Production S c is also allowed in a CNF grammar
if S never appears on the right side of any production.
However, S e is not allowed in our bilexical CFGs.
3We have a more general definition that drops the
restriction to CNF, but do not give it here.
</footnote>
<listItem confidence="0.999063666666667">
• A[a] B [b] C[a] (1)
• A[a] C[a] B[b] (2)
• A[a] —&gt; a (3)
</listItem>
<bodyText confidence="0.993956769230769">
Thus every nonterminal is lexicalized at some
terminal a. A constituent of nonterminal type
A[a] is said to have terminal symbol a as its lex-
ical head, &amp;quot;inherited&amp;quot; from the constituent&apos;s
head child in the parse tree (e.g., C[a]).
Notice that the start symbol is necessarily a
lexicalized nonterminal, T[$]. Hence $ appears
in every string of L(G); it is usually convenient
to define G so that the language of interest is
actually L&apos; (G) = {x : x$ E L (G)}
Such a grammar can encode lexically specific
preferences. For example, P might contain the
productions
</bodyText>
<listItem confidence="0.9999388">
• VP[solve] V[solve] NP[puzzles]
• NP[puzzles] DET[two] N[puzzles]
• V[solve] solve
• N[puzzles] -4 puzzles
• DET[two] -4 two
</listItem>
<bodyText confidence="0.862191666666667">
in order to allow the derivation VP[solve]
solve two puzzles, but meanwhile omit the sim-
ilar productions
</bodyText>
<listItem confidence="0.99983825">
• VP[eat] V[eat] NP[puzzles]
• VP[solve] -4 V[solve] NP[goat]
• VP[sleep] V[sleep] NP[goati
• NP[goat] DET[two] N[goat]
</listItem>
<bodyText confidence="0.999413157894737">
since puzzles are not edible, a goat is not solv-
able, &amp;quot;sleep&amp;quot; is intransitive, and &amp;quot;goat&amp;quot; cannot
take plural determiners. (A stochastic version
of the grammar could implement &amp;quot;soft prefer-
ences&amp;quot; by allowing the rules in the second group
but assigning them various low probabilities.)
The cost of this expressiveness is a very large
grammar. Standard context-free parsing algo-
rithms are inefficient in such a case. The CKY
algorithm (Younger, 1967; Aho and Ullman,
1972) is time 0(n3- IPI), where in the worst case
I P1 = IVNI3 (one ignores unary productions).
For a bilexical grammar, the worst case is IPI =
VD I 3 &apos; I VT12, which is large for a large vocabulary
VT. We may improve the analysis somewhat by
observing that when parsing d1 • • • dn, the CKY
algorithm only considers nonterminals of the
form A[di]; by restricting to the relevant pro-
ductions we obtain 0(n3 • IVDI3 • min(n, IVTI)2)•
</bodyText>
<page confidence="0.994061">
458
</page>
<bodyText confidence="0.999738888888889">
We observe that in practical applications we
always have n &lt; IVTI• Let us then restrict
our analysis to the (infinite) set of input in-
stances of the parsing problem that satisfy re-
lation n &lt; WTI. With this assumption, the
asymptotic time complexity of the CKY algo-
rithm becomes 0(n5 • IVD13). In other words,
it is a factor of n2 slower than a comparable
non-lexicalized CFG.
</bodyText>
<sectionHeader confidence="0.95628" genericHeader="method">
4 Bilexical CFG in time 0(n4)
</sectionHeader>
<bodyText confidence="0.998376613636364">
In this section we give a recognition algorithm
for bilexical CNF context-free grammars, which
runs in time 0(n4 • max(p, IVO)) 0(n4 •
VDI). Here p is the maximum number of pro-
ductions sharing the same pair of terminal sym-
bols (e.g., the pair (b, a) in production (1)). The
new algorithm is asymptotically more efficient
than the CKY algorithm, when restricted to in-
put instances satisfying the relation n &lt; IVTI.
Where CKY recognizes only constituent sub-
strings of the input, the new algorithm can rec-
ognize three types of subderivations, shown and
described in Figure 1(a). A declarative specifi-
cation of the algorithm is given in Figure 1(b).
The derivability conditions of (a) are guaran-
teed by (b), by induction, and the correctness of
the acceptance condition (see caption) follows.
This declarative specification, like CKY, may
be implemented by bottom-up dynamic pro-
gramming. We sketch one such method. For
each possible item, as shown in (a), we maintain
a bit (indexed by the parameters of the item)
that records whether the item has been derived
yet. All these bits are initially zero. The algo-
rithm makes a single pass through the possible
items, setting the bit for each if it can be derived
using any rule in (b) from items whose bits are
already set. At the end of this pass it is straight-
forward to test whether to accept w (see cap-
tion). The pass considers the items in increas-
ing order of width, where the width of an item
in (a) is defined as max{h, j} — min{h, j}.
Among items of the same width, those of type
.L should be considered last.
The algorithm requires space proportional to
the number of possible items, which is at most
n3IVD12. Each of the five rule templates can
instantiate its free variables in at most n4p or
(for COMPLETE rules) n41VD12 different ways,
each of which is tested once and in constant
time; so the runtime is 0(n4 max(P, IVO)).
By comparison, the CKY algorithm uses only
the first type of item, and relies on rules whose
inputs are pairs Such rules
</bodyText>
<equation confidence="0.929901">
h 3 +
</equation>
<bodyText confidence="0.9981945">
can be instantiated in 0(n5) different ways for a
fixed grammar, yielding 0(n5) time complexity.
The new algorithm saves a factor of n by com-
bining those two constituents in two steps, one
of which is insensitive to k and abstracts over its
possible values, the other of which is insensitive
to h&apos; and abstracts over its possible values.
It is straightforward to turn the new 0(n4)
recognition algorithm into a parser for stochas-
tic bilexical CFCs (or other weighted bilexical
CFGs). In a stochastic CFG, each nonterminal
A[a] is accompanied by a probability distribu-
tion over productions of the form A[a] —&gt; a. A
parse is just a derivation (proof tree) of lhn
and its probability—like that of any derivation
we find—is defined as the product of the prob-
abilities of all productions used to condition in-
ference rules in the proof tree. The highest-
probability derivation for any item can be re-
constructed recursively at the end of the parse,
provided that each item maintains not only a
bit indicating whether it can be derived, but
also the probability and instantiated root rule
of its highest-probability derivation tree.
</bodyText>
<sectionHeader confidence="0.975258" genericHeader="method">
5 A more efficient variant
</sectionHeader>
<bodyText confidence="0.999971842105263">
We now give a variant of the algorithm of §4; the
variant has the same asymptotic complexity but
will often be faster in practice.
Notice that the ATTACH-LEFT rule of Fig-
ure 1(b) tries to combine the nonterminal label
B[dhd of a previously derived constituent with
every possible nonterminal label of the form
C[dh]. The improved version, shown in Figure 2,
restricts C[dh] to be the label of a previously de-
rived adjacent constituent. This improves speed
if there are not many such constituents and we
can enumerate them in 0(1) time apiece (using
a sparse parse table to store the derived items).
It is necessary to use an agenda data struc-
ture (Kay, 1986) when implementing the declar-
ative algorithm of Figure 2. Deriving narrower
items before wider ones as before will not work
here because the rule HALVE derives narrow
items from wide ones.
</bodyText>
<page confidence="0.982544">
459
</page>
<figure confidence="0.997803740740741">
A
i j h
A
h i j
(b) START:
(i &lt; h &lt;j, A E VD)
(i &lt; j &lt;h, A, C E VD)
(h &lt;i &lt; j, A, C E VD)
A[dh] dh
is derived if A[dh]
is derived if A[dh] = B[dhdC[dh] wi,3C[dh] for some B, h&apos;
is derived if A[dh] = C[dh]./3[dh,] = C[dh]wi,i for some B, h&apos;
ATTACH-LEFT:
A A[dh] B[dh,]C[dh]
i j h
ATTACH-RIGHT: B
A[dh] C[dh]B[dhd
COMPLETE-RIGHT: A
ZcC
i jh j
A
ihk
COMPLETE-LEFT: c A
1 h j k
A
A
h i j
</figure>
<figureCaption confidence="0.8277858">
Figure 1: An 0(n4) recognition algorithm for CNF bilexical CFG. (a) Types of items in the
parse table (chart). The first is syntactic sugar for the tuple [A, A, i, h, j], and so on. The stated
conditions assume that d1, ... dr, are all distinct. (b) Inference rules. The algorithm derives the
item below if the items above have already been derived and any condition to the right
of is met. It accepts input w just if item [A, T,1, h, ri] is derived for some h such that dh = $.
</figureCaption>
<table confidence="0.7392078">
(a) (i &lt;h &lt;j, A E VD) is derived if A[dh] wi
(i &lt;h, A E VD) is derived if A[dhl wi,j for some j &gt; h
(h &lt;j, A E VD) is derived if A[dh] = wi,j for some i &lt; h
(i &lt;3 &lt;h, A, C E VD) is derived if A[dh] B[dh,]C[dh] wC[dh] Wi,k for
some B, h&apos;, k Wk,j for
(h&lt; &lt; j, A, C E VD) is derived if A[dhl C[dh]B[dh,] C[dhjtvi,i
i
some B , h! , k
(b) As in Figure 1(b) above, but add HALVE and change ATTACH-LEFT and ATTACH-RIGHT as shown.
HALVE: ATTACH-LEFT: ATTACH-RIGHT:
</table>
<figure confidence="0.981102">
A B C C B
ij 3 j -1-411
A[dh] -- B[dh,]C[dh] h j — 1 j&apos;k
.\
A[dh] —&gt; C[dh)B[dh,)
A A A A
ith hNj ..0
h j k
</figure>
<figureCaption confidence="0.998799">
Figure 2: A more efficient variant of the 0(n4) algorithm in Figure 1, in the same format.
</figureCaption>
<page confidence="0.996936">
460
</page>
<sectionHeader confidence="0.963761" genericHeader="method">
6 Multiple word senses
</sectionHeader>
<bodyText confidence="0.995100146341463">
Rather than parsing an input string directly, it
is often desirable to parse another string related
by a (possibly stochastic) transduction. Let T
be a finite-state transducer that maps a mor-
pheme sequence w E Vit to its orthographic re-
alization, a grapheme sequence fo T may re-
alize arbitrary morphological processes, includ-
ing affixation, local clitic movement, deletion
of phonological nulls, forbidden or dispreferred
k-grams, typographical errors, and mapping of
multiple senses onto the same grapheme. Given
grammar G and an input ti), we ask whether
E T(L(G)). We have extended all the algo-
rithms in this paper to this case: the items sim-
ply keep track of the transducer state as well.
Due to space constraints, we sketch only the
special case of multiple senses. Suppose that
the input is iD= d1 • • • dn, and each d2 has up to
g possible senses. Each item now needs to track
its head&apos;s sense along with its head&apos;s position in
ID. Wherever an item formerly recorded a head
position h (similarly h&apos;), it must now record a
pair (h, dh), where dh E VT is a specific sense of
dh. No rule in Figures 1-2 (or Figure 3 below)
will mention more than two such pairs. So the
time complexity increases by a factor of 0(g2).
7 Head automaton grammars in
time 0(n4)
In this section we show that a length-n string
generated by a head automaton grammar (Al-
shawi, 1996) can be parsed in time 0(n4). We
do this by providing a translation from head
automaton grammars to bilexical CFGs.4 This
result improves on the head-automaton parsing
algorithm given by Alshawi, which is analogous
to the CKY algorithm on bilexical CFGs and is
likewise 0(n5) in practice (see §3).
A head automaton grammar (HAG) is a
function H : a 1-4 Ha that defines a head au-
tomaton (HA) for each element of its (finite)
domain. Let VT = domain(H) and D =
</bodyText>
<listItem confidence="0.782629">
1. A special symbol $ E VT plays the role of
start symbol. For each a E VT Ha is a tuple
(Qa, VT, Sa, Fa), where
• Qa is a finite set of states;
4Translation in the other direction is possible if the
HAG formalism is extended to allow multiple senses per
word (see §6). This makes the formalisms equivalent.
• &apos;a, Fa C Qa are sets of initial and final
states, respectively;
• 5a is a transition function mapping Qa X
VT X D to 2Qa the power set of Qa.
</listItem>
<bodyText confidence="0.993971714285714">
A single head automaton is an acceptor for a
language of string pairs (zi, zr) E V x V. In-
formally, if b is the leftmost symbol of Zr and
q&apos; E a(q, b, -4), then Ha can move from state q
to state q&apos;, matching symbol b and removing it
from the left end of Zr. Symmetrically, if b is the
rightmost symbol of zi and q&apos; E Sa(q,b,&lt;---) then
from q Ha can move to q&apos;, matching symbol b
and removing it from the right end of z1.5
More formally, we associate with the head au-
tomaton Ha a &amp;quot;derives&amp;quot; relation ha, defined as
a binary relation on Qa X 1 /4 X V. . For ev-
ery q E Q, x,y E V, b E VT, de D, and
q&apos; E &amp;(q, b, d), we specify that
</bodyText>
<equation confidence="0.889877333333333">
(q, xb, y) Ha (q&apos; ,x, y) if d =4—;
(q, x, by) Ha (q1 , x, y) if d=—*
The
</equation>
<bodyText confidence="0.994761">
The reflexive and transitive closure of ha is writ-
ten Ha*. The language generated by Ha is the set
</bodyText>
<equation confidence="0.958728">
L(Ha) = zr) (q, zi, zr)H*a (r,E,E),
q C Ia, r E Fal.
</equation>
<bodyText confidence="0.998589625">
We may now define the language generated
by the entire grammar H. To generate, we ex-
pand the start word $ E VT into x$y for some
(x, y) E L (Hs), and then recursively expand the
words in strings x and y. More formally, given
H, we simultaneously define La for all a E VT
to be minimal such that if (x, y) E L(H a),
x&apos; e Lx, y&apos; E Ly, then x&apos;ayi E La, where
stands for the concatenation language
Lai • • Lai,. Then H generates language L.
We next present a simple construction that
transforms a HAG H into a bilexical CFG G
generating the same language. The construc-
tion also preserves derivation ambiguity. This
means that for each string w, there is a linear-
time 1-to-1 mapping between (appropriately de-
5 Alshawi (1996) describes HAs as accepting (or equiv-
alently, generating) zi and z from the outside in. To
make Figure 3 easier to follow, we have defined HAs as
accepting symbols in the opposite order, from the in-
side out. This amounts to the same thing if transitions
are reversed, I. is exchanged with F., and any transi-
tion probabilities are replaced by those of the reversed
Markov chain.
</bodyText>
<page confidence="0.998927">
461
</page>
<bodyText confidence="0.981488375">
fined) canonical derivations of w by H and
canonical derivations of w by G.
We adopt the notation above for H and the
components of its head automata. Let VD be
an arbitrary set of size t = max{IQa I : a E VT},
and for each a, define an arbitrary injection fa :
Q. -4 VD. We define G = (VN, P,T[$]),
where
</bodyText>
<listItem confidence="0.9718192">
(i) VN =- {A[a] : A E VD , a E VT}, in the usual
manner for bilexical CFG;
(ii) P is the set of all productions having one
of the following forms, where a, b E VT:
• A[a] —&gt; B[b] C[a] where
</listItem>
<figure confidence="0.961473285714286">
A =- fa(r), B = fb(q9, C = fa(q) for
some q&apos; G -rb, q E Q., r E 5(q, b, f-)
• A[a] C[a] B[b] where
A = fa(r), B fb(V), C = fa(q) for
some q&apos; E q E Qa, r
• A[a] a &apos;where
A = fa(q) for some q E Fa
</figure>
<bodyText confidence="0.979821873239437">
(iii) T fs(q), where we assume WLOG that
Is is a singleton set {q}.
We omit the formal proof that G and H
admit isomorphic derivations and hence gen-
erate the same languages, observing only that
if (x, y) = (bib2 • • • bi,b3+1- • • bk) E L(Ha)—
a condition used in defining La above—then
A[a] [bi] • • • B3[MaB3+1[bi+11 • • • Bk[bk],
for any A, B1, . Bk that map to initial states
in Ha, Hbl, Hb, respectively.
In general, G has p = 0(IVD13) = 0(t3). The
construction therefore implies that we can parse
a length-n sentence under H in time 0(n4t3). If
the HAs in H happen to be deterministic, then
in each binary production given by (ii) above,
symbol A is fully determined by a, b, and C. In
this case p = 0(t2), so the parser will operate
in time 0(n4t2).
We note that this construction can be
straightforwardly extended to convert stochas-
tic HAGs as in (Alshawi, 1996) into stochastic
CFGs. Probabilities that Ha assigns to state q&apos;s
various transition and halt actions are copied
onto the corresponding productions A[a] a
of G, where A = fa(q).
8 Split head automaton grammars
in time 0 (n3 )
For many bilexical CFGs or HAGs of practical
significance, just as for the bilexical version of
link grammars (Lafferty et al., 1992), it is possi-
ble to parse length-n inputs even faster, in time
0(n3) (Eisner, 1997). In this section we de-
scribe and discuss this special case, and give a
new 0(n3) algorithm that has a smaller gram-
mar constant than previously reported.
A head automaton Ha is called split if it has
no states that can be entered on a ÷- transi-
tion and exited on a ---&gt; transition. Such an au-
tomaton can accept (x, y) only by reading all of
y—immediately after which it is said to be in
a flip state—and then reading all of x. For-
mally, a flip state is one that allows entry on a
—&gt; transition and that either allows exit on a
transition or is a final state.
We are concerned here with head automa-
ton grammars H such that every Ha is split.
These correspond to bilexical CFGs in which
any derivation A[a] = xay has the form
A[a] = xB[a] xay. That is, a word&apos;s left
dependents are more oblique than its right de-
pendents and c-command them.
Such grammars are broadly applicable. Even
if Ha is not split, there usually exists a split head
automaton H&amp;quot;, recognizing the same language.
H la exists if {x#y : (x, y) E L(Ha)} is regular
(where # VT). In particular, lei&apos;, must exist
unless Ha has a cycle that includes both ÷- and
-4 transitions. Such cycles would be necessary
for Ha itself to accept a formal language such
as {(bn, cn) : n &gt; 0}, where word a takes 2n de-
pendents, but we know of no natural-language
motivation for ever using them in a HAG.
One more definition will help us bound the
complexity. A split head automaton Ha is said
to be g-split if its set of flip states, denoted
C Qa, has size &lt; g. The languages that can
be recognized by g-split HAs are those that can
be written as 1..g 1 Li x R, where the Li and
Ri are regular languages over VT. Eisner (1997)
actually defined (g-split) bilexical grammars in
terms of the latter property.6
</bodyText>
<footnote confidence="0.9341992">
6That paper associated a product language Li xR,, or
equivalently a 1-split HA, with each of g senses of a word
(see §6). One could do the same without penalty in our
present approach: confining to 1-split automata would
remove the g 2 complexity factor, and then allowing g
</footnote>
<page confidence="0.998153">
462
</page>
<bodyText confidence="0.9621403125">
We now present our result: Figure 3 specifies
an 0(n3g2t2) recognition algorithm for a head
automaton grammar H in which every H, is
g-split. For deterministic automata, the run-
time is 0(n3g2t)—a considerable improvement
on the 0(n3g3t2) result of (Eisner, 1997), which
also assumes deterministic automata. As in §4,
a simple bottom-up implementation will suffice.
For a practical speedup, add h\j as an an-
tecedent to the MID rule (and fill in the parse
table from right to left).
Like our previous algorithms, this one takes
two steps (ATTACH, COMPLETE) to attach a
child constituent to a parent constituent. But
instead of full constituents—strings xd,y e
d, —it uses only half-constituents like xdi and
</bodyText>
<figure confidence="0.5950498">
diy. Where CKY combines
z h 3 j +lh
we save two degrees of freedom i, k (so improv-
ing 0(n5) to 0(n3)) and combine \ .
hi 3 + h
</figure>
<bodyText confidence="0.996109363636364">
The other halves of these constituents can be at-
tached later, because to find an accepting path
for (zi, Zr) in a split head automaton, one can
separately find the half-path before the flip state
(which accepts zr) and the half-path after the
flip state (which accepts zi). These two half-
paths can subsequently be joined into an ac-
cepting path if they have the same flip state s,
i.e., one path starts where the other ends. An-
notating our left half-constituents with s makes
this check possible.
</bodyText>
<sectionHeader confidence="0.987947" genericHeader="conclusions">
9 Final remarks
</sectionHeader>
<bodyText confidence="0.972459068965517">
We have formally described, and given faster
parsing algorithms for, three practical gram-
matical rewriting systems that capture depen-
dencies between pairs of words. All three sys-
tems admit naive 0(n5) algorithms. We give
the first 0(n4) results for the natural formalism
of bilexical context-free grammar, and for Al-
shawi&apos;s (1996) head automaton grammars. For
the usual case, split head automaton grammars
or equivalent bilexical CFGs, we replace the
0(n3) algorithm of (Eisner, 1997) by one with a
smaller grammar constant. Note that, e.g., all
senses would restore the g 2 factor. Indeed, this approach
gives added flexibility: a word&apos;s sense, unlike its choice
of flip state, is visible to the HA that reads it.
three models in (Collins, 1997) are susceptible
to the 0(n3) method (cf. Collins&apos;s 0(n5)).
Our dynamic programming techniques for
cheaply attaching head information to deriva-
tions can also be exploited in parsing formalisms
other than rewriting systems. The authors have
developed an 0(n7)-time parsing algorithm for
bilexicalized tree adjoining grammars (Schabes,
1992), improving the naive 0(n8) method.
The results mentioned in §6 are related to the
closure property of CFGs under generalized se-
quential machine mapping (Hoperoft and Ull-
man, 1979). This property also holds for our
class of bilexical CFGs.
</bodyText>
<sectionHeader confidence="0.996015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998476105263158">
A. V. Aho and J. D. Ullman. 1972. The Theory
of Parsing, Translation and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
H. Alshawi. 1996. Head automata and bilingual
tiling: Translation with minimal representations.
In Proc. of ACL, pages 167-176, Santa Cruz, CA.
Y. Bar-Hillel. 1953. A quasi-arithmetical notation
for syntactic description. Language, 29:47-58.
E. Charniak. 1997, Statistical parsing with a
context-free grammar and word statistics. In
Proc. of the 14th AAAI, Menlo Park.
C. Chelba and F. Jelinek. 1998. Exploiting syntac-
tic structure for language modeling. In Proc. of
COLING-ACL.
N. Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, MA.
M. Collins and J. Brooks. 1995. Prepositional
phrase attachment through a backed-off model.
In Proc. of the Third Workshop on Very Large
Corpora, Cambridge, MA.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th
ACL and 8th European ACL, Madrid, July.
J. Eisner. 1996. An empirical comparison of proba-
bility models for dependency grammar. Technical
Report IRCS-96-11, IRCS, Univ. of Pennsylvania.
J. Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the
4th Int. Workshop on Parsing Technologies, MIT,
Cambridge, MA, September.
R. C. Gonzales and M. G. Thomason. 1978. Syntac-
tic Pattern Recognition. Addison-Wesley, Read-
ing, MA.
M. A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley, Reading, MA.
J. E. Hoperoft and J. D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages and Com-
putation. Addison-Wesley, Reading, MA.
</reference>
<page confidence="0.999009">
463
</page>
<figure confidence="0.895885135135135">
(h&lt; j,qEChh)
(i &lt; h, q E QdhU {F}, S E Qdh)
(h &lt; h&apos;, q E Qdh, E -Qdh,)
(h&apos; &lt; h, q E Qdh, s E Qdh, 81 E Qdh
is derived if dh : I q where wh-Fi,i E Lx
is derived if dh : s where wi,h—i E Lx
and dh, : F 41— s&apos; where
dh, y
and dh : q 4-- s where
is derived xdh,
iff dh : I q
E Lxy
is derived if dh, : I
E Lxy
S,
(b)
START: q E MID: S E Qdh
h h
FINISH:
q E Fdh
COMPLETE-RIGHT: q SI
h&apos; h&apos; i
Ni
h
COMPLETE-LEFT: F q
Sir-15
h&apos; h&apos; h
As
i h
ATTACH-RIGHT: q
h - 1 i
r E -Sdh (q,dh&apos; )-4)
h h&apos;
ATTACH-LEFT: st
Qdh, r E Odh (q, dh&apos;, 4—)
(c) Accept input w just if As and h\n are derived for some h, s such that dh = $.
1 h
</figure>
<figureCaption confidence="0.883074333333333">
Figure 3: An 0(n3) recognition algorithm for split head automaton grammars. The format is as
in Figure 1, except that (c) gives the acceptance condition. The following notation indicates that
a head automaton can consume a string x from its left or right input: a : q q&apos; means that
</figureCaption>
<reference confidence="0.985770774193548">
(q, e, (q&apos;, e, e), and a : I -14 q&apos; means this is true for some q E /a. Similarly, a: q means
that (q, x, e) I-a* (V , 6, e), and a : F q means this is true for some q&apos; E Fa. The special symbol
F also appears as a literal in some items, and effectively means &amp;quot;an unspecified final state.&amp;quot;
M. Kay. 1986. Algorithm schemata and data struc-
tures in syntactic processing. In K. Sparck Jones
B. J. Grosz and B. L. Webber, editors, Natu-
ral Language Processing, pages 35-70. Kaufmann,
Los Altos, CA.
J. Lafferty, D. Sleator, and D. Temperley. 1992.
Grammatical trigrams: A probabilistic model of
link grammar. In Proc. of the AAAI Conf. on
Probabilistic Approaches to Nat. Lang., October.
D. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of the 33rd ACL.
I. Mereuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
C. Pollard and I. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Y. Schabes, A. Abeille, and A. Joshi. 1988. Parsing
strategies with `lexicalized&apos; grammars: Applica-
tion to Tree Adjoining Grammars. In Proceedings
of COLING-88, Budapest, August.
Yves Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proc. of the 1.4th COL-
ING, pages 426-432, Nantes, France, August.
C. S. Wetherell. 1980. Probabilistic languages: A
review and some open questions. Computing Sur-
veys, 12(4):361-379.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information
and Control, 10(2):189-208, February.
</reference>
<page confidence="0.999518">
464
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358621">
<title confidence="0.925372">Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars*</title>
<author confidence="0.99998">Jason Eisner</author>
<affiliation confidence="0.9996125">Dept. of Computer &amp; Information Science Dip. University of Pennsylvania</affiliation>
<address confidence="0.9984735">200 South 33rd Street, Philadelphia, PA 19104 USA</address>
<email confidence="0.999817">jeisner@linc.cis.upenn.edu</email>
<author confidence="0.999852">Giorgio Satta</author>
<affiliation confidence="0.9971475">di Elettronica e Informatica Universita di Padova</affiliation>
<address confidence="0.7475815">via Gradenigo 6/A, 35131 Padova, Italy</address>
<email confidence="0.998922">satta@dei.unipd.it</email>
<abstract confidence="0.981729">stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words. We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>The Theory of Parsing,</title>
<date>1972</date>
<journal>Translation and Compiling,</journal>
<volume>1</volume>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="7341" citStr="Aho and Ullman, 1972" startWordPosition="1230" endWordPosition="1233">ile omit the similar productions • VP[eat] V[eat] NP[puzzles] • VP[solve] -4 V[solve] NP[goat] • VP[sleep] V[sleep] NP[goati • NP[goat] DET[two] N[goat] since puzzles are not edible, a goat is not solvable, &amp;quot;sleep&amp;quot; is intransitive, and &amp;quot;goat&amp;quot; cannot take plural determiners. (A stochastic version of the grammar could implement &amp;quot;soft preferences&amp;quot; by allowing the rules in the second group but assigning them various low probabilities.) The cost of this expressiveness is a very large grammar. Standard context-free parsing algorithms are inefficient in such a case. The CKY algorithm (Younger, 1967; Aho and Ullman, 1972) is time 0(n3- IPI), where in the worst case I P1 = IVNI3 (one ignores unary productions). For a bilexical grammar, the worst case is IPI = VD I 3 &apos; I VT12, which is large for a large vocabulary VT. We may improve the analysis somewhat by observing that when parsing d1 • • • dn, the CKY algorithm only considers nonterminals of the form A[di]; by restricting to the relevant productions we obtain 0(n3 • IVDI3 • min(n, IVTI)2)• 458 We observe that in practical applications we always have n &lt; IVTI• Let us then restrict our analysis to the (infinite) set of input instances of the parsing problem th</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata and bilingual tiling: Translation with minimal representations.</title>
<date>1996</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>167--176</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="2359" citStr="Alshawi, 1996" startWordPosition="340" endWordPosition="341">f particular pairs of words in particular roles. The acceptability of &amp;quot;Nora convened the * The authors were supported respectively under ARPA Grant N6600194-C-6043 &amp;quot;Human Language Technology&amp;quot; and Ministero dell&apos;Universita e della Ricerca Scientifica e Tecnologica project &amp;quot;Methodologies and Tools of High Performance Systems for Multimedia Applications.&amp;quot; party&amp;quot; then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, 0(n4) is possible. • The 0(n4) result also holds for head automat</context>
<context position="15094" citStr="Alshawi, 1996" startWordPosition="2672" endWordPosition="2674">ase of multiple senses. Suppose that the input is iD= d1 • • • dn, and each d2 has up to g possible senses. Each item now needs to track its head&apos;s sense along with its head&apos;s position in ID. Wherever an item formerly recorded a head position h (similarly h&apos;), it must now record a pair (h, dh), where dh E VT is a specific sense of dh. No rule in Figures 1-2 (or Figure 3 below) will mention more than two such pairs. So the time complexity increases by a factor of 0(g2). 7 Head automaton grammars in time 0(n4) In this section we show that a length-n string generated by a head automaton grammar (Alshawi, 1996) can be parsed in time 0(n4). We do this by providing a translation from head automaton grammars to bilexical CFGs.4 This result improves on the head-automaton parsing algorithm given by Alshawi, which is analogous to the CKY algorithm on bilexical CFGs and is likewise 0(n5) in practice (see §3). A head automaton grammar (HAG) is a function H : a 1-4 Ha that defines a head automaton (HA) for each element of its (finite) domain. Let VT = domain(H) and D = 1. A special symbol $ E VT plays the role of start symbol. For each a E VT Ha is a tuple (Qa, VT, Sa, Fa), where • Qa is a finite set of stat</context>
<context position="17577" citStr="Alshawi (1996)" startWordPosition="3181" endWordPosition="3182"> E VT into x$y for some (x, y) E L (Hs), and then recursively expand the words in strings x and y. More formally, given H, we simultaneously define La for all a E VT to be minimal such that if (x, y) E L(H a), x&apos; e Lx, y&apos; E Ly, then x&apos;ayi E La, where stands for the concatenation language Lai • • Lai,. Then H generates language L. We next present a simple construction that transforms a HAG H into a bilexical CFG G generating the same language. The construction also preserves derivation ambiguity. This means that for each string w, there is a lineartime 1-to-1 mapping between (appropriately de5 Alshawi (1996) describes HAs as accepting (or equivalently, generating) zi and z from the outside in. To make Figure 3 easier to follow, we have defined HAs as accepting symbols in the opposite order, from the inside out. This amounts to the same thing if transitions are reversed, I. is exchanged with F., and any transition probabilities are replaced by those of the reversed Markov chain. 461 fined) canonical derivations of w by H and canonical derivations of w by G. We adopt the notation above for H and the components of its head automata. Let VD be an arbitrary set of size t = max{IQa I : a E VT}, and for</context>
<context position="19520" citStr="Alshawi, 1996" startWordPosition="3580" endWordPosition="3581">used in defining La above—then A[a] [bi] • • • B3[MaB3+1[bi+11 • • • Bk[bk], for any A, B1, . Bk that map to initial states in Ha, Hbl, Hb, respectively. In general, G has p = 0(IVD13) = 0(t3). The construction therefore implies that we can parse a length-n sentence under H in time 0(n4t3). If the HAs in H happen to be deterministic, then in each binary production given by (ii) above, symbol A is fully determined by a, b, and C. In this case p = 0(t2), so the parser will operate in time 0(n4t2). We note that this construction can be straightforwardly extended to convert stochastic HAGs as in (Alshawi, 1996) into stochastic CFGs. Probabilities that Ha assigns to state q&apos;s various transition and halt actions are copied onto the corresponding productions A[a] a of G, where A = fa(q). 8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997). In this section we describe and discuss this special case, and give a new 0(n3) algorithm that has a smaller grammar constant than previously reported. A he</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata and bilingual tiling: Translation with minimal representations. In Proc. of ACL, pages 167-176, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
</authors>
<title>A quasi-arithmetical notation for syntactic description.</title>
<date>1953</date>
<journal>Language,</journal>
<pages>29--47</pages>
<contexts>
<context position="1140" citStr="Bar-Hillel, 1953" startWordPosition="163" endWordPosition="164">d words. We present 0(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of 0(n5). For a common special case that was known to allow 0(n3) parsing (Eisner, 1997), we present an 0(n3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language—in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;euk, 1988; Pollard and Sag, 1994). Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments. &amp;quot;Convene&amp;quot; requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP&apos;s head (e.g., &amp;quot;meeting&amp;quot;). We use the general term bilexical for a grammar that records such facts. A bilexical grammar makes many stipulations about the compatibi</context>
</contexts>
<marker>Bar-Hillel, 1953</marker>
<rawString>Y. Bar-Hillel. 1953. A quasi-arithmetical notation for syntactic description. Language, 29:47-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. of the 14th AAAI, Menlo Park.</booktitle>
<contexts>
<context position="2389" citStr="Charniak, 1997" startWordPosition="344" endWordPosition="345">n particular roles. The acceptability of &amp;quot;Nora convened the * The authors were supported respectively under ARPA Grant N6600194-C-6043 &amp;quot;Human Language Technology&amp;quot; and Ministero dell&apos;Universita e della Ricerca Scientifica e Tecnologica project &amp;quot;Methodologies and Tools of High Performance Systems for Multimedia Applications.&amp;quot; party&amp;quot; then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, 0(n4) is possible. • The 0(n4) result also holds for head automaton grammars. • For a very comm</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997, Statistical parsing with a context-free grammar and word statistics. In Proc. of the 14th AAAI, Menlo Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="3670" citStr="Chelba and Jelinek, 1998" startWordPosition="545" endWordPosition="548">hm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol&apos;Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). 457 lows (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT , P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*. If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this paper we adopt the following co</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>C. Chelba and F. Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1185" citStr="Chomsky, 1965" startWordPosition="169" endWordPosition="170"> two bilexical formalisms, improving the prior upper bounds of 0(n5). For a common special case that was known to allow 0(n3) parsing (Eisner, 1997), we present an 0(n3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language—in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;euk, 1988; Pollard and Sag, 1994). Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments. &amp;quot;Convene&amp;quot; requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP&apos;s head (e.g., &amp;quot;meeting&amp;quot;). We use the general term bilexical for a grammar that records such facts. A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particul</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>N. Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Proc. of the Third Workshop on Very Large Corpora,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="3643" citStr="Collins and Brooks, 1995" startWordPosition="541" endWordPosition="544">ars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol&apos;Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). 457 lows (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT , P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*. If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this pape</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>M. Collins and J. Brooks. 1995. Prepositional phrase attachment through a backed-off model. In Proc. of the Third Workshop on Very Large Corpora, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th ACL and 8th European ACL,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="2405" citStr="Collins, 1997" startWordPosition="346" endWordPosition="347">es. The acceptability of &amp;quot;Nora convened the * The authors were supported respectively under ARPA Grant N6600194-C-6043 &amp;quot;Human Language Technology&amp;quot; and Ministero dell&apos;Universita e della Ricerca Scientifica e Tecnologica project &amp;quot;Methodologies and Tools of High Performance Systems for Multimedia Applications.&amp;quot; party&amp;quot; then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, 0(n4) is possible. • The 0(n4) result also holds for head automaton grammars. • For a very common special case </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. of the 35th ACL and 8th European ACL, Madrid, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>An empirical comparison of probability models for dependency grammar.</title>
<date>1996</date>
<tech>Technical Report IRCS-96-11,</tech>
<institution>IRCS, Univ. of Pennsylvania.</institution>
<contexts>
<context position="2373" citStr="Eisner, 1996" startWordPosition="342" endWordPosition="343">irs of words in particular roles. The acceptability of &amp;quot;Nora convened the * The authors were supported respectively under ARPA Grant N6600194-C-6043 &amp;quot;Human Language Technology&amp;quot; and Ministero dell&apos;Universita e della Ricerca Scientifica e Tecnologica project &amp;quot;Methodologies and Tools of High Performance Systems for Multimedia Applications.&amp;quot; party&amp;quot; then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, 0(n4) is possible. • The 0(n4) result also holds for head automaton grammars. •</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. An empirical comparison of probability models for dependency grammar. Technical Report IRCS-96-11, IRCS, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and a cubictime probabilistic parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 4th Int. Workshop on Parsing Technologies, MIT,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="719" citStr="Eisner, 1997" startWordPosition="99" endWordPosition="100">omputer &amp; Information Science Dip. University of Pennsylvania 200 South 33rd Street, Philadelphia, PA 19104 USA jeisner@linc.cis.upenn.edu Giorgio Satta di Elettronica e Informatica Universita di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei.unipd.it Abstract Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present 0(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of 0(n5). For a common special case that was known to allow 0(n3) parsing (Eisner, 1997), we present an 0(n3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language—in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;euk, 1988; Pollard and Sag, 1994). Besides the possible arguments of</context>
<context position="3083" citStr="Eisner, 1997" startWordPosition="458" endWordPosition="459">ial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, 0(n4) is possible. • The 0(n4) result also holds for head automaton grammars. • For a very common special case of these grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol&apos;Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). 457 lows (H</context>
<context position="19962" citStr="Eisner, 1997" startWordPosition="3656" endWordPosition="3657">se p = 0(t2), so the parser will operate in time 0(n4t2). We note that this construction can be straightforwardly extended to convert stochastic HAGs as in (Alshawi, 1996) into stochastic CFGs. Probabilities that Ha assigns to state q&apos;s various transition and halt actions are copied onto the corresponding productions A[a] a of G, where A = fa(q). 8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997). In this section we describe and discuss this special case, and give a new 0(n3) algorithm that has a smaller grammar constant than previously reported. A head automaton Ha is called split if it has no states that can be entered on a ÷- transition and exited on a ---&gt; transition. Such an automaton can accept (x, y) only by reading all of y—immediately after which it is said to be in a flip state—and then reading all of x. Formally, a flip state is one that allows entry on a —&gt; transition and that either allows exit on a transition or is a final state. We are concerned here with head automaton</context>
<context position="21635" citStr="Eisner (1997)" startWordPosition="3981" endWordPosition="3982">ei&apos;, must exist unless Ha has a cycle that includes both ÷- and -4 transitions. Such cycles would be necessary for Ha itself to accept a formal language such as {(bn, cn) : n &gt; 0}, where word a takes 2n dependents, but we know of no natural-language motivation for ever using them in a HAG. One more definition will help us bound the complexity. A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size &lt; g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 6That paper associated a product language Li xR,, or equivalently a 1-split HA, with each of g senses of a word (see §6). One could do the same without penalty in our present approach: confining to 1-split automata would remove the g 2 complexity factor, and then allowing g 462 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split. For deterministic automata, the runtime is 0(n3g2t)—a considerable improvement on the 0(n3g3t2) re</context>
<context position="23824" citStr="Eisner, 1997" startWordPosition="4352" endWordPosition="4353">., one path starts where the other ends. Annotating our left half-constituents with s makes this check possible. 9 Final remarks We have formally described, and given faster parsing algorithms for, three practical grammatical rewriting systems that capture dependencies between pairs of words. All three systems admit naive 0(n5) algorithms. We give the first 0(n4) results for the natural formalism of bilexical context-free grammar, and for Alshawi&apos;s (1996) head automaton grammars. For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant. Note that, e.g., all senses would restore the g 2 factor. Indeed, this approach gives added flexibility: a word&apos;s sense, unlike its choice of flip state, is visible to the HA that reads it. three models in (Collins, 1997) are susceptible to the 0(n3) method (cf. Collins&apos;s 0(n5)). Our dynamic programming techniques for cheaply attaching head information to derivations can also be exploited in parsing formalisms other than rewriting systems. The authors have developed an 0(n7)-time parsing algorithm for bilexicalized tree adjoining grammars (Schabes, 1992</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>J. Eisner. 1997. Bilexical grammars and a cubictime probabilistic parser. In Proceedings of the 4th Int. Workshop on Parsing Technologies, MIT, Cambridge, MA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Gonzales</author>
<author>M G Thomason</author>
</authors>
<title>Syntactic Pattern Recognition.</title>
<date>1978</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="5373" citStr="Gonzales and Thomason, 1978" startWordPosition="883" endWordPosition="886">as usual. We also use the reflexive and transitive closure of written and define L(G) accordingly. We write a 8 =* a-0 for a derivation in which only /3 is rewritten. 3 Bilexical context-free grammars We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT. This formalism closely resembles stochastic grammatical formalisms that are used in several existing natural language processing systems (see §1). We will specify a nonstochastic version, noting that probabilities or other weights may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980). (See §4 for brief discussion.) Suppose G = (VN, VT , P,T[$]) is a CFG in CNF.3 We say that G is bilexical if there exists a set of &amp;quot;delexicalized nonterminals&amp;quot; VD such that VN = {A[a] : A E VD, a E VT} and every production in P has one of the following forms: 2Production S c is also allowed in a CNF grammar if S never appears on the right side of any production. However, S e is not allowed in our bilexical CFGs. 3We have a more general definition that drops the restriction to CNF, but do not give it here. • A[a] B [b] C[a] (1) • A[a] C[a] B[b] (2) • A[a] —&gt; a (3) Thus every</context>
</contexts>
<marker>Gonzales, Thomason, 1978</marker>
<rawString>R. C. Gonzales and M. G. Thomason. 1978. Syntactic Pattern Recognition. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="3696" citStr="Harrison, 1978" startWordPosition="551" endWordPosition="552">), the grammar constant can be reduced without harming the 0(n3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol&apos;Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). 457 lows (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT , P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*. If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this paper we adopt the following conventions: a, b, c, d deno</context>
</contexts>
<marker>Harrison, 1978</marker>
<rawString>M. A. Harrison. 1978. Introduction to Formal Language Theory. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="3724" citStr="Hoperoft and Ullman, 1979" startWordPosition="553" endWordPosition="556">onstant can be reduced without harming the 0(n3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol&apos;Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). 457 lows (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT , P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*. If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this paper we adopt the following conventions: a, b, c, d denote symbols in VT, w, X, y de</context>
</contexts>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>J. E. Hoperoft and J. D. Ullman. 1979. Introduction to Automata Theory, Languages and Computation. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="false">
<title>(q&apos;, e, e), and a : I -14 q&apos; means this is true for some q E /a. Similarly, a: q means that (q, x,</title>
<journal>e) I-a* (V ,</journal>
<volume>6</volume>
<marker></marker>
<rawString>(q, e, (q&apos;, e, e), and a : I -14 q&apos; means this is true for some q E /a. Similarly, a: q means that (q, x, e) I-a* (V , 6, e), and a : F q means this is true for some q&apos; E Fa. The special symbol F also appears as a literal in some items, and effectively means &amp;quot;an unspecified final state.&amp;quot;</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm schemata and data structures in syntactic processing. In</title>
<date>1986</date>
<booktitle>Natural Language Processing,</booktitle>
<pages>35--70</pages>
<editor>K. Sparck Jones B. J. Grosz and B. L. Webber, editors,</editor>
<publisher>Kaufmann,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="11995" citStr="Kay, 1986" startWordPosition="2056" endWordPosition="2057">e variant has the same asymptotic complexity but will often be faster in practice. Notice that the ATTACH-LEFT rule of Figure 1(b) tries to combine the nonterminal label B[dhd of a previously derived constituent with every possible nonterminal label of the form C[dh]. The improved version, shown in Figure 2, restricts C[dh] to be the label of a previously derived adjacent constituent. This improves speed if there are not many such constituents and we can enumerate them in 0(1) time apiece (using a sparse parse table to store the derived items). It is necessary to use an agenda data structure (Kay, 1986) when implementing the declarative algorithm of Figure 2. Deriving narrower items before wider ones as before will not work here because the rule HALVE derives narrow items from wide ones. 459 A i j h A h i j (b) START: (i &lt; h &lt;j, A E VD) (i &lt; j &lt;h, A, C E VD) (h &lt;i &lt; j, A, C E VD) A[dh] dh is derived if A[dh] is derived if A[dh] = B[dhdC[dh] wi,3C[dh] for some B, h&apos; is derived if A[dh] = C[dh]./3[dh,] = C[dh]wi,i for some B, h&apos; ATTACH-LEFT: A A[dh] B[dh,]C[dh] i j h ATTACH-RIGHT: B A[dh] C[dh]B[dhd COMPLETE-RIGHT: A ZcC i jh j A ihk COMPLETE-LEFT: c A 1 h j k A A h i j Figure 1: An 0(n4) reco</context>
</contexts>
<marker>Kay, 1986</marker>
<rawString>M. Kay. 1986. Algorithm schemata and data structures in syntactic processing. In K. Sparck Jones B. J. Grosz and B. L. Webber, editors, Natural Language Processing, pages 35-70. Kaufmann, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>D Sleator</author>
<author>D Temperley</author>
</authors>
<title>Grammatical trigrams: A probabilistic model of link grammar.</title>
<date>1992</date>
<booktitle>In Proc. of the AAAI Conf. on Probabilistic Approaches to Nat.</booktitle>
<location>Lang.,</location>
<contexts>
<context position="3601" citStr="Lafferty et al., 1992" startWordPosition="535" endWordPosition="538">very common special case of these grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol&apos;Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). 457 lows (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT , P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*. If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also </context>
<context position="19879" citStr="Lafferty et al., 1992" startWordPosition="3639" endWordPosition="3642">nary production given by (ii) above, symbol A is fully determined by a, b, and C. In this case p = 0(t2), so the parser will operate in time 0(n4t2). We note that this construction can be straightforwardly extended to convert stochastic HAGs as in (Alshawi, 1996) into stochastic CFGs. Probabilities that Ha assigns to state q&apos;s various transition and halt actions are copied onto the corresponding productions A[a] a of G, where A = fa(q). 8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997). In this section we describe and discuss this special case, and give a new 0(n3) algorithm that has a smaller grammar constant than previously reported. A head automaton Ha is called split if it has no states that can be entered on a ÷- transition and exited on a ---&gt; transition. Such an automaton can accept (x, y) only by reading all of y—immediately after which it is said to be in a flip state—and then reading all of x. Formally, a flip state is one that allows entry on a —&gt; transition and that either allows </context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>J. Lafferty, D. Sleator, and D. Temperley. 1992. Grammatical trigrams: A probabilistic model of link grammar. In Proc. of the AAAI Conf. on Probabilistic Approaches to Nat. Lang., October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd ACL.</booktitle>
<contexts>
<context position="3617" citStr="Magerman, 1995" startWordPosition="539" endWordPosition="540">e of these grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol&apos;Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). 457 lows (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT , P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*. If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also be generated by </context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mereuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<marker>Mereuk, 1988</marker>
<rawString>I. Mereuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1284" citStr="Pollard and Sag, 1994" startWordPosition="182" endWordPosition="185">case that was known to allow 0(n3) parsing (Eisner, 1997), we present an 0(n3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language—in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;euk, 1988; Pollard and Sag, 1994). Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments. &amp;quot;Convene&amp;quot; requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP&apos;s head (e.g., &amp;quot;meeting&amp;quot;). We use the general term bilexical for a grammar that records such facts. A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles. The acceptability of &amp;quot;Nora convened the * The authors were supported respectively under A</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>A Abeille</author>
<author>A Joshi</author>
</authors>
<title>Parsing strategies with `lexicalized&apos; grammars: Application to Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of COLING-88,</booktitle>
<location>Budapest,</location>
<contexts>
<context position="1245" citStr="Schabes et al., 1988" startWordPosition="176" endWordPosition="179">ounds of 0(n5). For a common special case that was known to allow 0(n3) parsing (Eisner, 1997), we present an 0(n3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language—in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;euk, 1988; Pollard and Sag, 1994). Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments. &amp;quot;Convene&amp;quot; requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP&apos;s head (e.g., &amp;quot;meeting&amp;quot;). We use the general term bilexical for a grammar that records such facts. A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles. The acceptability of &amp;quot;Nora convened the * The auth</context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Y. Schabes, A. Abeille, and A. Joshi. 1988. Parsing strategies with `lexicalized&apos; grammars: Application to Tree Adjoining Grammars. In Proceedings of COLING-88, Budapest, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Stochastic lexicalized treeadjoining grammars.</title>
<date>1992</date>
<booktitle>In Proc. of the 1.4th COLING,</booktitle>
<pages>426--432</pages>
<location>Nantes, France,</location>
<marker>Schabes, 1992</marker>
<rawString>Yves Schabes. 1992. Stochastic lexicalized treeadjoining grammars. In Proc. of the 1.4th COLING, pages 426-432, Nantes, France, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions. Computing Surveys,</title>
<date>1980</date>
<pages>12--4</pages>
<contexts>
<context position="5391" citStr="Wetherell, 1980" startWordPosition="887" endWordPosition="888">lexive and transitive closure of written and define L(G) accordingly. We write a 8 =* a-0 for a derivation in which only /3 is rewritten. 3 Bilexical context-free grammars We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT. This formalism closely resembles stochastic grammatical formalisms that are used in several existing natural language processing systems (see §1). We will specify a nonstochastic version, noting that probabilities or other weights may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980). (See §4 for brief discussion.) Suppose G = (VN, VT , P,T[$]) is a CFG in CNF.3 We say that G is bilexical if there exists a set of &amp;quot;delexicalized nonterminals&amp;quot; VD such that VN = {A[a] : A E VD, a E VT} and every production in P has one of the following forms: 2Production S c is also allowed in a CNF grammar if S never appears on the right side of any production. However, S e is not allowed in our bilexical CFGs. 3We have a more general definition that drops the restriction to CNF, but do not give it here. • A[a] B [b] C[a] (1) • A[a] C[a] B[b] (2) • A[a] —&gt; a (3) Thus every nonterminal is le</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>C. S. Wetherell. 1980. Probabilistic languages: A review and some open questions. Computing Surveys, 12(4):361-379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3. Information and Control,</title>
<date>1967</date>
<pages>10--2</pages>
<contexts>
<context position="7318" citStr="Younger, 1967" startWordPosition="1228" endWordPosition="1229">les, but meanwhile omit the similar productions • VP[eat] V[eat] NP[puzzles] • VP[solve] -4 V[solve] NP[goat] • VP[sleep] V[sleep] NP[goati • NP[goat] DET[two] N[goat] since puzzles are not edible, a goat is not solvable, &amp;quot;sleep&amp;quot; is intransitive, and &amp;quot;goat&amp;quot; cannot take plural determiners. (A stochastic version of the grammar could implement &amp;quot;soft preferences&amp;quot; by allowing the rules in the second group but assigning them various low probabilities.) The cost of this expressiveness is a very large grammar. Standard context-free parsing algorithms are inefficient in such a case. The CKY algorithm (Younger, 1967; Aho and Ullman, 1972) is time 0(n3- IPI), where in the worst case I P1 = IVNI3 (one ignores unary productions). For a bilexical grammar, the worst case is IPI = VD I 3 &apos; I VT12, which is large for a large vocabulary VT. We may improve the analysis somewhat by observing that when parsing d1 • • • dn, the CKY algorithm only considers nonterminals of the form A[di]; by restricting to the relevant productions we obtain 0(n3 • IVDI3 • min(n, IVTI)2)• 458 We observe that in practical applications we always have n &lt; IVTI• Let us then restrict our analysis to the (infinite) set of input instances of</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189-208, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>