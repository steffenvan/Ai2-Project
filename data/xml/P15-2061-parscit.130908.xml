<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015726">
<title confidence="0.993481">
Seed-Based Event Trigger Labeling:
How far can event descriptions get us?
</title>
<author confidence="0.990851">
Ofer Bronstein1, Ido Dagan1, Qi Li2, Heng Ji2, Anette Frank3,4
</author>
<affiliation confidence="0.88505475">
1 Computer Science Department, Bar-Ilan University
2 Department of Computer Science, Rensselaer Polytechnic Institute
3 Department of Computational Linguistics, Heidelberg University
4Research Training Group AIPHES, Dept. of Computational Linguistics, Heidelberg University
</affiliation>
<email confidence="0.98932">
oferbr@gmail.com dagan@cs.biu.ac.il
{liq7,jih}@rpi.edu frank@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.993829" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837555555556">
The task of event trigger labeling is typi-
cally addressed in the standard supervised
setting: triggers for each target event type
are annotated as training data, based on
annotation guidelines. We propose an al-
ternative approach, which takes the exam-
ple trigger terms mentioned in the guide-
lines as seeds, and then applies an event-
independent similarity-based classifier for
trigger labeling. This way we can skip
manual annotation for new event types,
while requiring only minimal annotated
training data for few example events at
system setup. Our method is evaluated on
the ACE-2005 dataset, achieving 5.7% F1
improvement over a state-of-the-art super-
vised system which uses the full training
data.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984233333334">
Event trigger labeling is the task of identifying the
main word tokens that express mentions of pre-
specified event types in running text. For example,
in “20 people were wounded in Tuesday’s airport
blast”, “wounded” is a trigger of an Injure event
and “blast” is a trigger of an Attack. The task
both detects trigger tokens and classifies them to
appropriate event types. While this task is often
a component within the broader event extraction
task, labeling both triggers and arguments, this pa-
per focuses on trigger labeling.
Most state-of-the-art event trigger labeling ap-
proaches (Ji and Grishman, 2008; Liao and Grish-
man, 2010b; Hong et al., 2011; Li et al., 2013)
follow the standard supervised learning paradigm.
For each event type, experts first write annotation
guidelines. Then, annotators follow them to label
event triggers in a large dataset. Finally, a classi-
fier is trained over the annotated triggers to label
the target events.
The supervised paradigm requires major human
efforts both in producing high-quality guidelines
and in dataset annotation for each new event type.
Given the rich information embedded in the guide-
lines, we raise in this paper the following research
question: how well can we perform by leverag-
ing only the lexical knowledge already available
in quality guidelines for new event types, without
requiring annotated training data for them?
To address this question, we propose a seed-
based approach for the trigger labeling task (Sec-
tion 2). Given the description for a new event type,
which contains some examples of triggers, we first
collect these triggers into a list of seeds. Then,
at the labeling phase, we consider each text token
as a candidate for a trigger and assess its similar-
ity to the seed list. In the above example, given
seeds such as “explosion” and “fire” for the Attack
event type, we identify that the candidate token
“blast” is a hyponym of “explosion” and synonym
of “fire” and infer that “blast” is a likely Attack
trigger.
In our method, such similarity indicators are en-
coded as a small set of event-independent clas-
sification features, based on lexical matches and
external resources like WordNet. Using event-
independent features allows us to train the system
only once, at system setup phase, requiring anno-
tated triggers in a training set for just a few pre-
selected event types. Then, whenever a new event
type is introduced for labeling, we only need to
collect a seed list for it from its description, and
provide it as input to the system.
We developed a seed-based system (Section 3),
based on a state-of-the-art fully-supervised event
extraction system (Li et al., 2013). When evalu-
ated on the ACE-2005 dataset,1 our system outper-
forms the fully-supervised one (Section 4), even
though we don’t utilize any annotated triggers of
the test events during the labeling phase, and only
</bodyText>
<footnote confidence="0.995419">
1http://projects.ldc.upenn.edu/ace
</footnote>
<page confidence="0.655341">
372
</page>
<note confidence="0.321028666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999792">
Figure 1: Flow of the seed-based approach
</figureCaption>
<bodyText confidence="0.999949428571429">
use the seed triggers appearing in the ACE anno-
tation guidelines. This result contributes to the
broader line of research on avoiding or reducing
annotation cost in information extraction (Section
5). In particular, it suggests the potential utility of
the seed-based approach in scenarios where man-
ual annotation per each new event is too costly.
</bodyText>
<sectionHeader confidence="0.941397" genericHeader="general terms">
2 Seed-Based Problem Setup
</sectionHeader>
<bodyText confidence="0.999688153846154">
This section describes our setup, as graphically il-
lustrated in Figure 1.
Similarly to the supervised setting, our ap-
proach assumes that whenever a new event type is
defined as target, an informative event description
should be written for it. As a prominent example,
we consider Section 5 of the ACE-2005 event an-
notation guidelines,2 which provides a description
for each event type. The description includes a
short verbal specification plus several illustrating
example sentences with marked triggers, spanning
on average less than a page per event type.
As event descriptions specify the intended event
scope, they inherently include representative ex-
amples for event triggers. For instance, the ACE-
2005 guidelines include: “MEET Events include
talks, summits, conferences, meetings, visits,... ”,
followed by an example: “Bush and Putin met this
week... ”. We thus collect triggers mentioned in
each event description into a seed list for the event
type, which is provided as input to our trigger la-
beling method. Triggers from the above quoted
sentences are hence included in the Meet seed list,
shown in Figure 1.
As mentioned in the Introduction, our method
(Section 3) is based on event-independent features
</bodyText>
<footnote confidence="0.887852">
2https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/
files/english-events-guidelines-v5.4.3.pdf
</footnote>
<bodyText confidence="0.99987625">
that identify similarities between a candidate trig-
ger and a given seed list. To train such generic fea-
tures, our training requires several arbitrary train-
ing event types, with a small amount of annotated
triggers, from which it learns weights for the fea-
tures. In our evaluation (Section 4) we use 5 train-
ing event types, with a total of 30 annotated trig-
ger mentions (compared to roughly 5000 used by
the baseline fully-supervised system). In this set-
ting, the training phase is required only once dur-
ing system setup, while no further training is re-
quired for each new target event type.
In summary, our setup requires: (1) a seed list
per target event type; (2) a small number of anno-
tated triggers for few training event types, along
with their seed lists (at system setup).
</bodyText>
<sectionHeader confidence="0.99045" genericHeader="keywords">
3 Method
</sectionHeader>
<bodyText confidence="0.999993416666667">
This section describes the method we designed
to implement the seed-based approach. To as-
sess our approach, we compare it (Section 4) with
the common fully-supervised approach, which re-
quires annotated triggers for each target event
type. Therefore, we implemented our system by
adapting the state-of-the-art fully-supervised event
extraction system of Li et al. (2013), modifying
mechanisms relevant for features and for trigger
labels, as described below. Hence the systems are
comparable with respect to using the same pre-
processing and machine learning infrastructure.
</bodyText>
<subsectionHeader confidence="0.99992">
3.1 The Fully-Supervised System
</subsectionHeader>
<bodyText confidence="0.999912631578947">
The event extraction system of Li et al. (2013) la-
bels triggers and their arguments for a set of target
event types L, for which annotated training docu-
ments are provided. The system utilizes a struc-
tured perceptron with beam search (Collins and
Roark, 2004; Huang et al., 2012). To label trig-
gers, the system scans each sentence x, and cre-
ates candidate assignments y, that for each token
xi assign each possible label yi E L U {⊥} (⊥
meaning xi is not a trigger at all). The score of an
assignment (xi7 yi) is calculated as w · f , where f
is the binary feature vector calculated for (xi7 yi),
and w is the learned feature weight vector.
The classifier’s features capture various proper-
ties of xi and its context, such as its unigram and
its containing bigrams. These features are highly
lexicalized, resulting in a very large feature space.
Additionally, each feature is replicated and paired
with each label yi, allowing the system to learn
</bodyText>
<table confidence="0.99040975">
Few training event types
Die
• Seed List: die, kill, dead, ...
• Annotated triggers in corpus
(small amount, e.g. 10)
 e.g.“Jackson died in 2009...”
Attack
• Seed List: explosion, fire, stab...
• Annotated triggers in corpus
(small amount, e.g. 15)
(generic – once at system setup)
Training with
similarity features
Generic Model
Training
Meet
• Seed List: meet, talks, summit,
conference, meeting, visit...
• (No annotated triggers)
Apply Model:
compare seeds with
tokens in test documents
using similarity features
Labeled Trigger Mentions
of Meet
Elect
Injure
Trigger Labeling
(per new target event type)
373
Feature Description
Same Do the candidate token and a seed share the
Lemma same lemma?
Synonym Is a seed a WN synonym of the candidate token?
Hypernym Is a seed a WN hypernym or instance-hypernym
of the candidate token?
Similarity Does one of these WN relations hold between a
Relations seed and a candidate token? Synonym, Hyper-
nym, Instance Hypernym, Part Holonym, Mem-
ber Holonym, Substance Meronym, Entailment
</table>
<tableCaption confidence="0.993338">
Table 1: Similarity features using WordNet (WN).
</tableCaption>
<bodyText confidence="0.978997166666666">
For the last two features we allow up to 2 levels
of transitivity (e.g. hypernym of hypernym), and
consider also derivations of candidate tokens.
different weights for different labels, e.g., feature
(Unigram:“visited”, Meet) will have a different
weight than (Unigram:“visited”, Attack).
</bodyText>
<subsectionHeader confidence="0.999791">
3.2 The Seed-Based System
</subsectionHeader>
<bodyText confidence="0.976578787878788">
To implement the seed-based approach for trigger
labeling, we adapt only the trigger classification
part in the Li et al. (2013) fully-supervised sys-
tem, ignoring arguments. Given a set of new target
event types T we classify every test sentence once
for each event type t E T . Hence, when classi-
fying a sentence for t, the labeling of each token
xi is binary, where yi E IT, 1} marks whether
xi is a trigger of type t (T) or not (1). For in-
stance xi=“visited” labeled as T when classifying
for t=Meet, means xi is labeled as a Meet trigger.
To score the binary label assignment (xi, yi), we
use a small set of features that assess the similar-
ity between xi and t’s given seed list.
We implement our approach with a basic set
of binary features (Table 1), which are turned on
if similarity is found for at least one seed in the
list. We use a single knowledge resource (Word-
Net (Fellbaum, 1998)) for expansion.3 As in the
fully-supervised system, each feature is replicated
for each label in IT, 1}, learning separately how
well a feature can predict a trigger (T) and a
non-trigger (1). As labels are event-independent,
features are event-independent as well, and their
weights can be learned generically (Figure 1).
Since we label each token independently for
each event type t, multiple labels may be assigned
to the same token. If a single-label setting is re-
quired, standard techniques can be applied, such
as choosing a single random label, or the highest
scoring one.
3This could be potentially extended, e.g. with paraphrase
databases, like (Ganitkevitch et al., 2013).
</bodyText>
<sectionHeader confidence="0.996593" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.993891">
4.1 Setting
</subsectionHeader>
<bodyText confidence="0.986560755102041">
We evaluate our seed-based approach (Section 2)
in comparison to the fully-supervised approach
implemented by Li et al. (2013) (Section 3). To
maintain comparability, we use the ACE-2005
documents with the same split as in (Ji and Grish-
man, 2008; Liao and Grishman, 2010b; Li et al.,
2013) to 40 test documents and 559 training doc-
uments. However, some evaluation settings dif-
fer: Li et al. (2013) train a multi-class model for
all 33 ACE-2005 event types, and classify all to-
kens in the test documents into these event types.
Our approach, on the other hand, trains an event-
independent binary classifier, while testing on new
event types that are different from those utilized
for training. We next describe how this setup is
addressed in our evaluation.
Per-Event Classification To label the test doc-
uments to all 33 event types, we classify each to-
ken in the test documents once for each test event
type.4
Training Event Types When we label for a test
event type t, we use a model that was trained on
different pre-selected training event types. Since
we need to label for all event types, we cannot use
the same model for testing them all, since then the
event types used to train this model could not be
tested. Thus, for each t we use a model trained
on 5 randomly chosen training event types, differ-
ent than t.5 Additionally, to avoid a bias caused
by a particular random choice, we build 10 differ-
ent models, each time choosing a different set of 5
training event types. Then, we label the test docu-
ments for t 10 times, once by each model. When
measuring performance we compute the average
of these 10 runs for each t, as well as the variance
within these runs.
Annotated Triggers Training event types re-
quire annotated triggers from the training docu-
ments. To maintain consistency between differ-
ent sets of training event types, we use a fixed to-
tal of 30 annotated trigger tokens for each set of
4To maintain comparability with the single-label classifi-
cation results of Li et al. (2013), we randomly choose a sin-
gle label for our classification in the few (7) cases where it
yielded two labels for the same token.
5Li et al. (2013) internally split the training documents to
“train” and “dev”. Accordingly, our training event types are
split to 3 “train” events and 2 “dev” events (with annotations
taken from the “train” and “dev” documents respectively).
</bodyText>
<page confidence="0.99598">
374
</page>
<table confidence="0.9809402">
Micro -Avg. (%) Var
Prec Rec Fl Avg
Seed-Based 80.6 67.1 73.2 0.04
Li et al. (2013) 73.7 62.3 67.5 -
Ji and Grishman (2008) 67.6 53.5 59.7 -
</table>
<tableCaption confidence="0.996124">
Table 2: Seed-based performance compared to
</tableCaption>
<bodyText confidence="0.9702300625">
fully-supervised systems, plus average F1 vari-
ance (%) over the 10 test runs per test event type.
training event types. The amounts of 5 training
event types and 30 annotated triggers were chosen
to demonstrate that such small amounts, requiring
little manual effort at system setup, yield high per-
formance (larger training didn’t improve results,
possibly due to the small number of features).
Seed Lists To build the seed lists for all event
types, we manually extracted all triggers men-
tioned in Section 5 of the ACE-2005 guidelines,
as described in Section 2.6 This resulted in lists of
4.2 seeds per event type on average, which is fairly
small. For comparison, each event type has an av-
erage of 46 distinct trigger terms in the training
corpus used by the fully-supervised method.
</bodyText>
<subsectionHeader confidence="0.647627">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.99986952631579">
Table 2 shows our system’s precision, recall and
F1,7 and the average variance of F1 within the 10
runs of each test event type. The very low variance
indicates that the system’s performance does not
depend much on the choice of training event types.
We compare our system’s performance to the
published trigger classification results of the base-
line system of (Li et al., 2013) (its globally op-
timized run, when labeling both triggers and ar-
guments). We also compare to the sentence-level
system in (Ji and Grishman, 2008) which uses the
same dataset split. Our system outperforms the
fully-supervised baseline by 5.7% F1, which is
statistically significant (two-tailed Wilcoxon test,
p &lt; 0.05). This shows that there is no per-
formance hit for the seed-based method on this
dataset, even though it does not require any anno-
tated data for new tested events, thus saving costly
annotation efforts.
</bodyText>
<footnote confidence="0.9939625">
6Our seed lists are publicly available for download at:
https://goo.gl/sErDW9
7We report micro-average as typical for this task. Macro-
average results are a few points lower for our system and for
the system of Li et al. (2013), maintaining similar relative
difference.
</footnote>
<sectionHeader confidence="0.997441" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999973583333333">
Our work contributes to the broader research di-
rection of reducing annotation for information ex-
traction. One such IE paradigm, including Pre-
emptive IE (Shinyama and Sekine, 2006), On-
demand IE (Sekine, 2006; Sekine and Oda, 2007)
and Open IE (Etzioni et al., 2005; Banko et
al., 2007; Banko et al., 2008), focuses on un-
supervised relation and event discovery. We, on
the other hand, follow the same goal as fully-
supervised systems in targeting pre-specified event
types, but aim at minimal annotation cost.
Bootstrapping methods (such as (Yangarber et
al., 2000; Agichtein and Gravano, 2000; Riloff,
1996; Greenwood and Stevenson, 2006; Liao
and Grishman, 2010a; Stevenson and Greenwood,
2005; Huang and Riloff, 2012)) have been widely
applied in IE. Most work started from a small
set of seed patterns, and repeatedly expanded
them from unlabeled corpora. Relying on unla-
beled data, bootstrapping methods are scalable but
tend to produce worse results (Liao and Grish-
man, 2010a) than supervised models due to se-
mantic drift (Curran et al., 2007). Our method can
be seen complementary to bootstrapping frame-
works, since we exploit manually crafted linguis-
tic resources which are more accurate but may not
cover all domains and languages.
Our approach is perhaps closest to (Roth et al.,
2009). They addressed a different IE task – re-
lation extraction, by recognizing entailment be-
tween candidate relation mentions and seed pat-
terns. While they exploited a supervised recogniz-
ing textual entailment (RTE) system, we show that
using only simple WordNet-based similarity fea-
tures and minimal training yields relatively high
performance in event trigger labeling.
</bodyText>
<sectionHeader confidence="0.998633" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998879166666667">
In this paper we show that by utilizing the in-
formation embedded in annotation guidelines and
lexical resources, we can skip manual annotation
for new event types. As we match performance of
a state-of-the-art fully-supervised system over the
ACE-2005 benchmark (and even surpass it), we
offer our approach as an appealing way of reduc-
ing annotation effort while preserving result qual-
ity. Future research may explore additional fea-
tures and knowledge resources, investigate alter-
native approaches for creating effective seed lists,
and extend our approach to argument labeling.
</bodyText>
<page confidence="0.998525">
375
</page>
<sectionHeader confidence="0.998332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999427666666667">
This work was partially supported by the Euro-
pean Commission (project EXCITEMENT, FP7
ICT-287923), and the U.S. DARPA DEFT Pro-
gram No. FA8750-13-2-0041, ARL NS-CTA
No. W911NF-09-2-0053, NSF CAREER IIS-
1523198.
</bodyText>
<sectionHeader confidence="0.998193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999421381578948">
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: extracting relations from large plain-text col-
lections. In Proc. Fifth ACM International Confer-
ence on Digital Libraries, pages 85–94.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
for the web. In Proc. IJCAI, pages 2670–2676.
M. Banko, O. Etzioni, and T. Center. 2008. The trade-
offs between open and traditional relation extraction.
In Proc. ACL, pages 28–36.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111–118.
James R Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proc. PACLING, pages 172–
180.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the web: An experimental study. Artificial Intelli-
gence, 165:91–134.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proc. NAACL-HLT, pages 758–764.
Mark A. Greenwood and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation ex-
traction patterns. In Proc. Workshop on Information
Extraction Beyond The Document, pages 29–35.
Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proc. ACL, pages 1127–1136.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. In Proc. ACL,
pages 286–295.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
NAACL, pages 142–151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Proc.
ACL, pages 254–262.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73–82.
Shasha Liao and Ralph Grishman. 2010a. Filtered
ranking for bootstrapping in event extraction. In
Proc. COLING, pages 680–688.
Shasha Liao and Ralph Grishman. 2010b. Using doc-
ument level cross-event inference to improve event
extraction. In Proc. ACL, pages 789–797.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proc. AAAI,
pages 1044–1049.
Dan Roth, Mark Sammons, and V. G. Vinod Vydis-
waran. 2009. A framework for entailed relation
recognition. In Proc. ACL-IJCNLP Short Papers,
pages 57–60.
Satoshi Sekine and Akira Oda. 2007. System demon-
stration of on-demand information extraction. In
Proc. ACL Demo and Poster Sessions, pages 17–20.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proc. COLING-ACL Poster Sessions, pages
731–738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proc. NAACL, pages 304–311.
Mark Stevenson and Mark A. Greenwood. 2005. A
semantic approach to IE pattern induction. In Proc.
ACL, pages 379–386.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proc. COLING, pages 940–946.
</reference>
<page confidence="0.99911">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.396098">
<title confidence="0.9372275">Seed-Based Event Trigger How far can event descriptions get us?</title>
<author confidence="0.939056">Ido Qi Heng Anette</author>
<affiliation confidence="0.85129375">1Computer Science Department, Bar-Ilan 2Department of Computer Science, Rensselaer Polytechnic 3Department of Computational Linguistics, Heidelberg Training Group AIPHES, Dept. of Computational Linguistics, Heidelberg University</affiliation>
<email confidence="0.921747">oferbr@gmail.comdagan@cs.biu.ac.ilfrank@cl.uni-heidelberg.de</email>
<abstract confidence="0.998243578947368">The task of event trigger labeling is typically addressed in the standard supervised triggers for event type are annotated as training data, based on annotation guidelines. We propose an alternative approach, which takes the example trigger terms mentioned in the guidelines as seeds, and then applies an eventindependent similarity-based classifier for trigger labeling. This way we can skip manual annotation for new event types, while requiring only minimal annotated training data for few example events at system setup. Our method is evaluated on ACE-2005 dataset, achieving 5.7% improvement over a state-of-the-art supervised system which uses the full training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proc. Fifth ACM International Conference on Digital Libraries,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="16584" citStr="Agichtein and Gravano, 2000" startWordPosition="2701" endWordPosition="2704"> relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accura</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: extracting relations from large plain-text collections. In Proc. Fifth ACM International Conference on Digital Libraries, pages 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction for the web. In</title>
<date>2007</date>
<booktitle>Proc. IJCAI,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="16277" citStr="Banko et al., 2007" startWordPosition="2653" endWordPosition="2656">ted events, thus saving costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We report micro-average as typical for this task. Macroaverage results are a few points lower for our system and for the system of Li et al. (2013), maintaining similar relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data,</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction for the web. In Proc. IJCAI, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>O Etzioni</author>
<author>T Center</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="16298" citStr="Banko et al., 2008" startWordPosition="2657" endWordPosition="2660">ing costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We report micro-average as typical for this task. Macroaverage results are a few points lower for our system and for the system of Li et al. (2013), maintaining similar relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping method</context>
</contexts>
<marker>Banko, Etzioni, Center, 2008</marker>
<rawString>M. Banko, O. Etzioni, and T. Center. 2008. The tradeoffs between open and traditional relation extraction. In Proc. ACL, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="7774" citStr="Collins and Roark, 2004" startWordPosition="1207" endWordPosition="1210">Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of Li et al. (2013), modifying mechanisms relevant for features and for trigger labels, as described below. Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure. 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi E L U {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi7 yi) is calculated as w · f , where f is the binary feature vector calculated for (xi7 yi), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additionally,</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. ACL, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Tara Murphy</author>
<author>Bernhard Scholz</author>
</authors>
<title>Minimising semantic drift with mutual exclusion bootstrapping.</title>
<date>2007</date>
<booktitle>In Proc. PACLING,</booktitle>
<pages>172--180</pages>
<contexts>
<context position="17040" citStr="Curran et al., 2007" startWordPosition="2774" endWordPosition="2777">tems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009). They addressed a different IE task – relation extraction, by recognizing entailment between candidate relation mentions and seed patterns. While they exploited a supervised recognizing textual entailment (RTE) system, we show that using only simple WordNet-based similarity features and minimal training yields relatively high performance in event trig</context>
</contexts>
<marker>Curran, Murphy, Scholz, 2007</marker>
<rawString>James R Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with mutual exclusion bootstrapping. In Proc. PACLING, pages 172– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>165--91</pages>
<contexts>
<context position="16257" citStr="Etzioni et al., 2005" startWordPosition="2649" endWordPosition="2652">tated data for new tested events, thus saving costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We report micro-average as typical for this task. Macroaverage results are a few points lower for our system and for the system of Li et al. (2013), maintaining similar relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relyin</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165:91–134.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proc. NAACL-HLT,</booktitle>
<pages>758--764</pages>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proc. NAACL-HLT, pages 758–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
<author>Mark Stevenson</author>
</authors>
<title>Improving semi-supervised acquisition of relation extraction patterns.</title>
<date>2006</date>
<booktitle>In Proc. Workshop on Information Extraction Beyond The Document,</booktitle>
<pages>29--35</pages>
<contexts>
<context position="16629" citStr="Greenwood and Stevenson, 2006" startWordPosition="2707" endWordPosition="2710">rk contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and language</context>
</contexts>
<marker>Greenwood, Stevenson, 2006</marker>
<rawString>Mark A. Greenwood and Mark Stevenson. 2006. Improving semi-supervised acquisition of relation extraction patterns. In Proc. Workshop on Information Extraction Beyond The Document, pages 29–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hong</author>
<author>Jianfeng Zhang</author>
<author>Bin Ma</author>
<author>Jian-Min Yao</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Using cross-entity inference to improve event extraction.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1127--1136</pages>
<contexts>
<context position="1872" citStr="Hong et al., 2011" startWordPosition="273" endWordPosition="276">f identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. The supervised paradigm requires major human efforts both in producing high-quality guidelines and in dataset annotation for each new event type. Given the rich information embedded in the guidelines, we raise in this paper the following research question: how well can we perform by leveraging only the l</context>
</contexts>
<marker>Hong, Zhang, Ma, Yao, Zhou, Zhu, 2011</marker>
<rawString>Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using cross-entity inference to improve event extraction. In Proc. ACL, pages 1127–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Bootstrapped training of event extraction classifiers.</title>
<date>2012</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>286--295</pages>
<contexts>
<context position="16711" citStr="Huang and Riloff, 2012" startWordPosition="2719" endWordPosition="2722">extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009). They addressed a differ</context>
</contexts>
<marker>Huang, Riloff, 2012</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2012. Bootstrapped training of event extraction classifiers. In Proc. ACL, pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="7795" citStr="Huang et al., 2012" startWordPosition="1211" endWordPosition="1214"> our system by adapting the state-of-the-art fully-supervised event extraction system of Li et al. (2013), modifying mechanisms relevant for features and for trigger labels, as described below. Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure. 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi E L U {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi7 yi) is calculated as w · f , where f is the binary feature vector calculated for (xi7 yi), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additionally, each feature is repl</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proc. NAACL, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>254--262</pages>
<contexts>
<context position="1827" citStr="Ji and Grishman, 2008" startWordPosition="264" endWordPosition="267">Introduction Event trigger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. The supervised paradigm requires major human efforts both in producing high-quality guidelines and in dataset annotation for each new event type. Given the rich information embedded in the guidelines, we raise in this paper the following research question: how</context>
<context position="11701" citStr="Ji and Grishman, 2008" startWordPosition="1866" endWordPosition="1870">label each token independently for each event type t, multiple labels may be assigned to the same token. If a single-label setting is required, standard techniques can be applied, such as choosing a single random label, or the highest scoring one. 3This could be potentially extended, e.g. with paraphrase databases, like (Ganitkevitch et al., 2013). 4 Evaluation 4.1 Setting We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by Li et al. (2013) (Section 3). To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b; Li et al., 2013) to 40 test documents and 559 training documents. However, some evaluation settings differ: Li et al. (2013) train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types. Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training. We next describe how this setup is addressed in our evaluation. Per-Event Classification To label the test documents to all 33 event types, we classify e</context>
<context position="13952" citStr="Ji and Grishman (2008)" startWordPosition="2267" endWordPosition="2270">d trigger tokens for each set of 4To maintain comparability with the single-label classification results of Li et al. (2013), we randomly choose a single label for our classification in the few (7) cases where it yielded two labels for the same token. 5Li et al. (2013) internally split the training documents to “train” and “dev”. Accordingly, our training event types are split to 3 “train” events and 2 “dev” events (with annotations taken from the “train” and “dev” documents respectively). 374 Micro -Avg. (%) Var Prec Rec Fl Avg Seed-Based 80.6 67.1 73.2 0.04 Li et al. (2013) 73.7 62.3 67.5 - Ji and Grishman (2008) 67.6 53.5 59.7 - Table 2: Seed-based performance compared to fully-supervised systems, plus average F1 variance (%) over the 10 test runs per test event type. training event types. The amounts of 5 training event types and 30 annotated triggers were chosen to demonstrate that such small amounts, requiring little manual effort at system setup, yield high performance (larger training didn’t improve results, possibly due to the small number of features). Seed Lists To build the seed lists for all event types, we manually extracted all triggers mentioned in Section 5 of the ACE-2005 guidelines, a</context>
<context position="15336" citStr="Ji and Grishman, 2008" startWordPosition="2500" endWordPosition="2503"> 46 distinct trigger terms in the training corpus used by the fully-supervised method. 4.2 Results Table 2 shows our system’s precision, recall and F1,7 and the average variance of F1 within the 10 runs of each test event type. The very low variance indicates that the system’s performance does not depend much on the choice of training event types. We compare our system’s performance to the published trigger classification results of the baseline system of (Li et al., 2013) (its globally optimized run, when labeling both triggers and arguments). We also compare to the sentence-level system in (Ji and Grishman, 2008) which uses the same dataset split. Our system outperforms the fully-supervised baseline by 5.7% F1, which is statistically significant (two-tailed Wilcoxon test, p &lt; 0.05). This shows that there is no performance hit for the seed-based method on this dataset, even though it does not require any annotated data for new tested events, thus saving costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We report micro-average as typical for this task. Macroaverage results are a few points lower for our system and for the system of Li et al. (2013)</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proc. ACL, pages 254–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="1890" citStr="Li et al., 2013" startWordPosition="277" endWordPosition="280">ain word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. The supervised paradigm requires major human efforts both in producing high-quality guidelines and in dataset annotation for each new event type. Given the rich information embedded in the guidelines, we raise in this paper the following research question: how well can we perform by leveraging only the lexical knowledge a</context>
<context position="3867" citStr="Li et al., 2013" startWordPosition="603" endWordPosition="606">s are encoded as a small set of event-independent classification features, based on lexical matches and external resources like WordNet. Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types. Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system. We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system (Li et al., 2013). When evaluated on the ACE-2005 dataset,1 our system outperforms the fully-supervised one (Section 4), even though we don’t utilize any annotated triggers of the test events during the labeling phase, and only 1http://projects.ldc.upenn.edu/ace 372 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Flow of the seed-based approach use the seed triggers appearing </context>
<context position="7281" citStr="Li et al. (2013)" startWordPosition="1129" endWordPosition="1132">ning is required for each new target event type. In summary, our setup requires: (1) a seed list per target event type; (2) a small number of annotated triggers for few training event types, along with their seed lists (at system setup). 3 Method This section describes the method we designed to implement the seed-based approach. To assess our approach, we compare it (Section 4) with the common fully-supervised approach, which requires annotated triggers for each target event type. Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of Li et al. (2013), modifying mechanisms relevant for features and for trigger labels, as described below. Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure. 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignmen</context>
<context position="9980" citStr="Li et al. (2013)" startWordPosition="1568" endWordPosition="1571">ed and a candidate token? Synonym, Hypernym, Instance Hypernym, Part Holonym, Member Holonym, Substance Meronym, Entailment Table 1: Similarity features using WordNet (WN). For the last two features we allow up to 2 levels of transitivity (e.g. hypernym of hypernym), and consider also derivations of candidate tokens. different weights for different labels, e.g., feature (Unigram:“visited”, Meet) will have a different weight than (Unigram:“visited”, Attack). 3.2 The Seed-Based System To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the Li et al. (2013) fully-supervised system, ignoring arguments. Given a set of new target event types T we classify every test sentence once for each event type t E T . Hence, when classifying a sentence for t, the labeling of each token xi is binary, where yi E IT, 1} marks whether xi is a trigger of type t (T) or not (1). For instance xi=“visited” labeled as T when classifying for t=Meet, means xi is labeled as a Meet trigger. To score the binary label assignment (xi, yi), we use a small set of features that assess the similarity between xi and t’s given seed list. We implement our approach with a basic set o</context>
<context position="11582" citStr="Li et al. (2013)" startWordPosition="1846" endWordPosition="1849">endent, features are event-independent as well, and their weights can be learned generically (Figure 1). Since we label each token independently for each event type t, multiple labels may be assigned to the same token. If a single-label setting is required, standard techniques can be applied, such as choosing a single random label, or the highest scoring one. 3This could be potentially extended, e.g. with paraphrase databases, like (Ganitkevitch et al., 2013). 4 Evaluation 4.1 Setting We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by Li et al. (2013) (Section 3). To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b; Li et al., 2013) to 40 test documents and 559 training documents. However, some evaluation settings differ: Li et al. (2013) train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types. Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training. We next describe how this setup is</context>
<context position="13454" citStr="Li et al. (2013)" startWordPosition="2179" endWordPosition="2182">ice, we build 10 different models, each time choosing a different set of 5 training event types. Then, we label the test documents for t 10 times, once by each model. When measuring performance we compute the average of these 10 runs for each t, as well as the variance within these runs. Annotated Triggers Training event types require annotated triggers from the training documents. To maintain consistency between different sets of training event types, we use a fixed total of 30 annotated trigger tokens for each set of 4To maintain comparability with the single-label classification results of Li et al. (2013), we randomly choose a single label for our classification in the few (7) cases where it yielded two labels for the same token. 5Li et al. (2013) internally split the training documents to “train” and “dev”. Accordingly, our training event types are split to 3 “train” events and 2 “dev” events (with annotations taken from the “train” and “dev” documents respectively). 374 Micro -Avg. (%) Var Prec Rec Fl Avg Seed-Based 80.6 67.1 73.2 0.04 Li et al. (2013) 73.7 62.3 67.5 - Ji and Grishman (2008) 67.6 53.5 59.7 - Table 2: Seed-based performance compared to fully-supervised systems, plus average F</context>
<context position="15191" citStr="Li et al., 2013" startWordPosition="2476" endWordPosition="2479">2.6 This resulted in lists of 4.2 seeds per event type on average, which is fairly small. For comparison, each event type has an average of 46 distinct trigger terms in the training corpus used by the fully-supervised method. 4.2 Results Table 2 shows our system’s precision, recall and F1,7 and the average variance of F1 within the 10 runs of each test event type. The very low variance indicates that the system’s performance does not depend much on the choice of training event types. We compare our system’s performance to the published trigger classification results of the baseline system of (Li et al., 2013) (its globally optimized run, when labeling both triggers and arguments). We also compare to the sentence-level system in (Ji and Grishman, 2008) which uses the same dataset split. Our system outperforms the fully-supervised baseline by 5.7% F1, which is statistically significant (two-tailed Wilcoxon test, p &lt; 0.05). This shows that there is no performance hit for the seed-based method on this dataset, even though it does not require any annotated data for new tested events, thus saving costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proc. ACL, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Filtered ranking for bootstrapping in event extraction.</title>
<date>2010</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>680--688</pages>
<contexts>
<context position="1852" citStr="Liao and Grishman, 2010" startWordPosition="268" endWordPosition="272">ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. The supervised paradigm requires major human efforts both in producing high-quality guidelines and in dataset annotation for each new event type. Given the rich information embedded in the guidelines, we raise in this paper the following research question: how well can we perform by l</context>
<context position="11726" citStr="Liao and Grishman, 2010" startWordPosition="1871" endWordPosition="1874">ndently for each event type t, multiple labels may be assigned to the same token. If a single-label setting is required, standard techniques can be applied, such as choosing a single random label, or the highest scoring one. 3This could be potentially extended, e.g. with paraphrase databases, like (Ganitkevitch et al., 2013). 4 Evaluation 4.1 Setting We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by Li et al. (2013) (Section 3). To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b; Li et al., 2013) to 40 test documents and 559 training documents. However, some evaluation settings differ: Li et al. (2013) train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types. Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training. We next describe how this setup is addressed in our evaluation. Per-Event Classification To label the test documents to all 33 event types, we classify each token in the test doc</context>
<context position="16654" citStr="Liao and Grishman, 2010" startWordPosition="2711" endWordPosition="2714">esearch direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhap</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010a. Filtered ranking for bootstrapping in event extraction. In Proc. COLING, pages 680–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using document level cross-event inference to improve event extraction.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>789--797</pages>
<contexts>
<context position="1852" citStr="Liao and Grishman, 2010" startWordPosition="268" endWordPosition="272">ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. The supervised paradigm requires major human efforts both in producing high-quality guidelines and in dataset annotation for each new event type. Given the rich information embedded in the guidelines, we raise in this paper the following research question: how well can we perform by l</context>
<context position="11726" citStr="Liao and Grishman, 2010" startWordPosition="1871" endWordPosition="1874">ndently for each event type t, multiple labels may be assigned to the same token. If a single-label setting is required, standard techniques can be applied, such as choosing a single random label, or the highest scoring one. 3This could be potentially extended, e.g. with paraphrase databases, like (Ganitkevitch et al., 2013). 4 Evaluation 4.1 Setting We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by Li et al. (2013) (Section 3). To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b; Li et al., 2013) to 40 test documents and 559 training documents. However, some evaluation settings differ: Li et al. (2013) train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types. Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training. We next describe how this setup is addressed in our evaluation. Per-Event Classification To label the test documents to all 33 event types, we classify each token in the test doc</context>
<context position="16654" citStr="Liao and Grishman, 2010" startWordPosition="2711" endWordPosition="2714">esearch direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhap</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010b. Using document level cross-event inference to improve event extraction. In Proc. ACL, pages 789–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proc. AAAI,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="16598" citStr="Riloff, 1996" startWordPosition="2705" endWordPosition="2706">ed Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proc. AAAI, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Mark Sammons</author>
<author>V G Vinod Vydiswaran</author>
</authors>
<title>A framework for entailed relation recognition.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP Short Papers,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="17286" citStr="Roth et al., 2009" startWordPosition="2814" endWordPosition="2817">nd Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009). They addressed a different IE task – relation extraction, by recognizing entailment between candidate relation mentions and seed patterns. While they exploited a supervised recognizing textual entailment (RTE) system, we show that using only simple WordNet-based similarity features and minimal training yields relatively high performance in event trigger labeling. 6 Conclusions and Future Work In this paper we show that by utilizing the information embedded in annotation guidelines and lexical resources, we can skip manual annotation for new event types. As we match performance of a state-of-</context>
</contexts>
<marker>Roth, Sammons, Vydiswaran, 2009</marker>
<rawString>Dan Roth, Mark Sammons, and V. G. Vinod Vydiswaran. 2009. A framework for entailed relation recognition. In Proc. ACL-IJCNLP Short Papers, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Akira Oda</author>
</authors>
<title>System demonstration of on-demand information extraction.</title>
<date>2007</date>
<booktitle>In Proc. ACL Demo and Poster Sessions,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="16223" citStr="Sekine and Oda, 2007" startWordPosition="2642" endWordPosition="2645">though it does not require any annotated data for new tested events, thus saving costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We report micro-average as typical for this task. Macroaverage results are a few points lower for our system and for the system of Li et al. (2013), maintaining similar relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded t</context>
</contexts>
<marker>Sekine, Oda, 2007</marker>
<rawString>Satoshi Sekine and Akira Oda. 2007. System demonstration of on-demand information extraction. In Proc. ACL Demo and Poster Sessions, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL Poster Sessions,</booktitle>
<pages>731--738</pages>
<contexts>
<context position="16173" citStr="Sekine, 2006" startWordPosition="2635" endWordPosition="2636">e seed-based method on this dataset, even though it does not require any annotated data for new tested events, thus saving costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We report micro-average as typical for this task. Macroaverage results are a few points lower for our system and for the system of Li et al. (2013), maintaining similar relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a sma</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>Satoshi Sekine. 2006. On-demand information extraction. In Proc. COLING-ACL Poster Sessions, pages 731–738.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="16173" citStr="Shinyama and Sekine, 2006" startWordPosition="2633" endWordPosition="2636">ce hit for the seed-based method on this dataset, even though it does not require any annotated data for new tested events, thus saving costly annotation efforts. 6Our seed lists are publicly available for download at: https://goo.gl/sErDW9 7We report micro-average as typical for this task. Macroaverage results are a few points lower for our system and for the system of Li et al. (2013), maintaining similar relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a sma</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proc. NAACL, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Mark A Greenwood</author>
</authors>
<title>A semantic approach to IE pattern induction.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>379--386</pages>
<contexts>
<context position="16686" citStr="Stevenson and Greenwood, 2005" startWordPosition="2715" endWordPosition="2718">ing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009)</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>Mark Stevenson and Mark A. Greenwood. 2005. A semantic approach to IE pattern induction. In Proc. ACL, pages 379–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>940--946</pages>
<contexts>
<context position="16555" citStr="Yangarber et al., 2000" startWordPosition="2697" endWordPosition="2700">13), maintaining similar relative difference. 5 Related Work Our work contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic re</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Automatic acquisition of domain knowledge for information extraction. In Proc. COLING, pages 940–946.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>