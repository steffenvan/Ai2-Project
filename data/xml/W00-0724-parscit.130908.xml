<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000720">
<note confidence="0.842375">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 119-122, Lisbon, Portugal, 2000.
</note>
<title confidence="0.7565485">
A Default First Order Family Weight Determination Procedure
for WPDV Models
</title>
<author confidence="0.654601">
Hans van Halteren
</author>
<affiliation confidence="0.609691">
Dept. of Language and Speech, University of Nijmegen
</affiliation>
<address confidence="0.591793">
P.O. Box 9103, 6500 HD Nijmegen, The Netherlands
</address>
<email confidence="0.838545">
hvh@1et.kun.n1
</email>
<sectionHeader confidence="0.978505" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994175111111111">
Weighted Probability Distribution Voting
(WPDV) is a newly designed machine learning
algorithm, for which research is currently
aimed at the determination of good weighting
schemes. This paper describes a simple yet
effective weight determination procedure, which
leads to models that can produce competitive
results for a number of NLP classification
tasks.
</bodyText>
<sectionHeader confidence="0.946503" genericHeader="method">
1 The WPDV algorithm
</sectionHeader>
<bodyText confidence="0.989522727272727">
Weighted Probability Distribution Voting
(WPDV) is a supervised learning approach to
classification. A case which is to be classified is
represented as a feature-value pair set:
Fcase= Iffi = {f. = v.}}
An estimation of the probabilities of the various
classes for the case in question is then based on
the classes observed with similar feature-value
pair sets in the training data. To be exact, the
probability of class C for Fcaâ€ž is estimated as
a weighted sum over all possible subsets Fsub of
</bodyText>
<figure confidence="0.3771214">
Fcase:
P(C)= N(C)
freq(C I Fsub)
EWE,&amp;quot; freq(Fsub)
F3ubCFcase
</figure>
<bodyText confidence="0.998106615384616">
with the frequencies (freq) measured on the
training data, and N(C) a normalizing factor
such that &gt;P(C) = 1.
In principle, the weight factors WEsub can be
assigned per individual subset. For the time
being, however, they are assigned for groups of
subsets. First of all, it is possible to restrict
the subsets that are taken into account in the
model, using the size of the subset (e.g. Fsub
contains at most 4 elements) and/or its fre-
quency (e.g. Fsub occurs at least twice in the
training material). Subsets which do not fulfil
the chosen criteria are not used. For the sub-
sets that are used, weight factors are not as-
signed per individual subset either, but rather
per &amp;quot;family&amp;quot;, where a family consists of those
subsets which contain the same combination of
feature types (i.e. the same ft).
The two components of a WPDV model, dis-
tributions and weights, are determined sepa-
rately. In this paper, I will use the term training
set for the data on which the distributions are
based and tuning set for the data on the basis of
which the weights are selected. Whether these
two sets should be disjunct or can coincide is
one of the subjects under investigation.
</bodyText>
<sectionHeader confidence="0.895579" genericHeader="method">
2 Family weights
</sectionHeader>
<bodyText confidence="0.978704611111111">
The various family weighting schemes can be
classified according to the type of use they make
of the tuning data. Here, I use a very rough
classification, into weighting scheme orders.
With Oth order weights, no information
whatsoever is used about the data in tuning
set. Examples of such rudimentary weighting
schemes are the use of a weight of k! for all sub-
sets containing k elements, as has been used e.g.
for wordclass tagger combination (van Halteren
et al., To appear), or even a uniform weight for
all subsets.
With 18t order weights, information is used
about the individual feature types, i.e.
WF. ub = H Wfi
{ilffi=vi/EF3.b}
First order weights ignore any possible inter-
action between two or more feature types, but
</bodyText>
<page confidence="0.99848">
119
</page>
<bodyText confidence="0.999921642857143">
have the clear advantage of corresponding to a
reasonably low number of weights, viz, as many
as there are feature types.
With nth order weights, interaction pat-
terns are determined of up to n feature types
and the family weights are adjusted to compen-
sate for the interaction. When n is equal to
the total number of feature types, this corre-
sponds to weight determination per individual
family. nth order weighting generally requires
much larger numbers of weights, which can be
expected to lead to much slower tuning proce-
dures. In this paper, therefore, I focus on first
order weighting.
</bodyText>
<sectionHeader confidence="0.926985" genericHeader="method">
3 First order weight determination
</sectionHeader>
<bodyText confidence="0.9999878">
As argumented in an earlier paper (van Hal-
teren, 2000a), a theory-based feature weight de-
termination would have to take into account
each feature&apos;s decisiveness and reliability. How-
ever, clear definitions of these qualities, and
hence also means to measure them, are as yet
sorely lacking. As a result, a more pragmatic
approach will have to be taken. Reliability is ig-
nored altogether at the moment,1 and decisive-
ness replaced by an entropy-related measure.
</bodyText>
<subsectionHeader confidence="0.991548">
3.1 Initial weights
</subsectionHeader>
<bodyText confidence="0.995831375">
The weight given to each feature type fi should
preferably increase with the amount of informa-
tion it contributes to the classification process.
A measure related to this is Information Gain,
which represents the difference between the en-
tropy of the choice with and without knowledge
of the presence of a feature (cf. Quinlan (1986)).
As do Daelemans et al. (2000), I opt for a fac-
tor proportional to the feature type&apos;s Gain Ra-
tio, a normalising derivative of the Information
Gain value. The weight factors Win are set to
an optimal multiplication constant C times the
measured Gain Ratio for L. C is determined by
calculating the accuracies for various values of
C on the tuning set2 and selecting the C which
yields the highest accuracy.
&apos;It may still be present, though, in the form of the
abovementioned frequency threshold for features.
2If the tuning set coincides with the training set, all
parts of the tuning procedure are done in leave-one-out
mode: in the WPDV implementation, it is possible to
(virtually) remove the information about each individual
instance from the model when that specific instance has
to be classified.
</bodyText>
<subsectionHeader confidence="0.999773">
3.2 Hill-climbing
</subsectionHeader>
<bodyText confidence="0.999924433333333">
Since the initial weight determination is based
on pragmatic rather than theoretical consider-
ations, it is unlikely that the resulting weights
are already the optimal ones. For this reason,
an attempt is made to locate even better weight
vectors in the n-dimensional weight space. The
navigation mechanism used in this search is hill-
climbing. This means that systematic variations
of the currently best vector are investigated. If
the best variation is better than the currently
best vector, that variation is taken as the best
vector and the process is repeated. This repeti-
tion continues until no better vector is found.
In the experiments described here, the varia-
tion consists of multiplication or division of each
individual WI&apos;, by a variable V (i.e. 2n new vec-
tors are tested each time), which is increased if a
better vector is found, and otherwise decreased.
The process is halted as soon as V falls below
some pre-determined threshold.
Hill-climbing, as most other optimaliza-
tion techniques, is vulnerable to overtraining.
To lessen this vulnerability, the WPDV hill-
climbing implementation splits its tuning mate-
rial into several (normally five) parts. A switch
to a new weight vector is only taken if the ac-
curacy increases on the tuning set as a whole
and does not decrease on more than one part,
i.e. some losses are accepted but only if they
are localized.
</bodyText>
<sectionHeader confidence="0.495734" genericHeader="method">
4 Quality of the first order weights
</sectionHeader>
<bodyText confidence="0.996253461538461">
In order to determine the quality of the WPDV
system, using first order weights as described
above, I run a series of experiments, using tasks
introduced by Daelemans et al. (1999):3
The Part-of-speech tagging task (POS) is
to determine a wordclass tag on the basis of dis-
ambiguated tags of two preceding tokens and
undisambiguated tags for the focus and two fol-
lowing tokens.4 5 features with 170-480 values;
169 classes; 837Kcase training; 2x105Kcase test.
The Grapheme-to-phoneme conversion
with stress task (GS) is to determine the pro-
nunciation of an English grapheme, including
</bodyText>
<footnote confidence="0.9594752">
31 only give a rough description of the tasks here. For
the exact details, I refer the reader to Daelemans et al.
(1999).
4For a overall WPDV approach to wordclass tagging,
see van Halteren (2000b).
</footnote>
<page confidence="0.993083">
120
</page>
<tableCaption confidence="0.8943955">
Table 1: Accuracies for the POS task (with the Table 2: Accuracies for the GS task (with the
training set ah tested in leave-one-out mode) training set ah tested in leave-one-out mode)
</tableCaption>
<table confidence="0.928479111111111">
Weighting scheme Test set
ah i j
Comparison
Naive Bayes 96.41 96.24
TiMBL (k=1) 97.83 97.79
Maccent (freq=2;iter=150) 98.07 98.03
Maccent (freq=1;iter=300) 98.13 98.10
WPDV 0th order weights 97.66 97.71 97.63
1
k! 96.86 96.92 96.86
WPDV initial l&apos;t order 98.14 98.16 98.12
tune =- ah (10GR)
tune = i (12GR) 98.14 98.17 98.12
tune = j (11GR) 98.14 98.16 98.13
WPDV with hill-climbing 98.17 98.21 98.15
tune = ah (30 steps)
tune = i (20 steps) 98.15 98.20 98.12
tune = j (20 steps) 98.15 98.18 98.16
</table>
<bodyText confidence="0.971355565217391">
presence of stress, on the basis of the focus
grapheme, three preceding and three following
graphemes. 7 features with 42 values each; 159
classes; 540Kcase training; 2x68Kcase test.
The PP attachment task (PP) is preposi-
tional phrase attachment to either a preceding
verb or a preceding noun, on the basis of the
verb, the noun, the preposition in question and
the head noun of the prepositional complement.
4 features with 3474, 4612, 68 and 5780 values;
2 classes; 19Kcase training; 2x2Kcase test.
The NP chunking task (NP) is the deter-
mination of the position of the focus token in a
base NP chunk (at beginning of chunk, in chunk,
or not in chunk), on the basis of the words and
tags for two preceding tokens, the focus and
one following token, and also the predictions by
three newfirst stage classifiers for the task.5 11
features with 3 (first stage classifiers), 90 (tags)
and 20K (words) values; 3 classes; 201Kcase
training; 2x25Kcase test.6
For each of the tasks, sections a to h of the data
set are used as the training set and sections i
</bodyText>
<footnote confidence="0.90619525">
6For a WPDV approach to a more general chunking
task, see my contribution to the CoNLL shared task,
elsewhere in these proceedings.
6The number of feature combinations for the NP task
is so large that the WPDV model has to be limited. For
the current experiments, I have opted for a maximum
size for Faub of four features and a threshold frequency
of two observations in the training set.
</footnote>
<table confidence="0.987633388888889">
Weighting scheme Test set
ah i j
Comparison
Naive Bayes 50.05 49.98
TiMBL (k=1) 92.25 92.02
Maccent (freq=2;iter=150) 79.41 79.36
Maccent (freq=1;iter=300) 80.43 80.35
WPDV 0th order weights 90.99 90.49 90.25
1
k! 92.77 92.05 91.89
WPDV initial 18t order 93.27 92.74 92.52
tune = ah (30GR)
tune = i (25GR) 93.24 92.76 92.54
tune = j (25GR) 93.24 92.76 92.54
WPDV with hill-climbing 93.29 92.77 92.53
tune = ah (34 steps)
tune = i (28 steps) 93.25 92.79 92.53
tune = j (12 steps) 93.24 92.76 92.54
</table>
<bodyText confidence="0.9997758">
and j as (two separate) test sets. All three are
also used as tuning sets. This allows a compari-
son between tuning on the training set itself and
on a held-out tuning set. For comparison with
some other well-known machine learning algo-
rithms, I complement the WPDV experiments
with accuracy measurements for three other sys-
tems: 1) A system using a Naive Bayes prob-
ability estimation; 2) TiMBL, using memory
based learning and probability estimation based
on the nearest neighbours (Daelemans et al.,
2000),7 for which I use the parameters which
yielded the best results according to Daelemans
et al. (1999); and 3) Maccent, a maximum en-
tropy based system,5 for which I use both the
default parameters, viz, a frequency threshold
of 2 for features to be used and 150 iterations
of improved iterative scaling, and a more am-
bitious parameter setting, viz, a threshold of 1
and 300 iterations.
The results for various WPDV weights, and
the other machine learning techniques are listed
in Tables 1 to 4.9 Except for one case (PP with
tune on j and test on i), the first order weight
WPDV results are all higher than those for the
</bodyText>
<footnote confidence="0.9986282">
7http://ilk.kub.n1/.
8http://www.cs.kuleuven.ac.berldh.
6The accuracy is shown in italics whenever the tuning
set is equal to the test set, i.e. when there is an unfair
advantage.
</footnote>
<page confidence="0.997248">
121
</page>
<tableCaption confidence="0.920043">
Table 3: Accuracies for the PP task (with the Table 4: Accuracies for the NP task (with the
training set ah tested in leave-one-out mode) training set ah tested in leave-one-out mode)
</tableCaption>
<table confidence="0.993461777777778">
Weighting scheme Test set
ah i 1
Comparison
Naive Bayes 82.68 82.64
TiMBL (k=1) 83.43 81.97
Maccent (freq=2;iter=150) 81.00 80.25
Maccent (freq=1;iter=300) 79.41 79.79
WPDV 0th order weights 80.83 82.26 81.46
1
k! 80.76 82.30 81.30
WPDV initial l&apos;t order 82.89 83.64 82.38
tune = ah (21GR)
tune = i (15GR) 82.82 83.81 82.55
tune = j (11GR) 82.60 83.26 82.76
WPDV with hill-climbing 83.10 83.72 82.68
tune = ah (19 steps)
tune = i (18 steps) 82.95 84.06 82.80
tune = j (16 steps) 82.65 83.10 82.93
</table>
<bodyText confidence="0.993564826086957">
comparison systems.1Â° Oth order weights gener-
ally do not reach this level of accuracy.
Hill-climbing with the tuning set equal to the
training set produces the best results overall.
It always leads to an improvement over initial
weights of the accuracies on both test sets, al-
though sometimes very small (GS). Equally im-
portant, the improvement on the test sets is
comparable to that on the tuning/training set.
This is certainly not the case for hill-climbing
with the tuning set equal to the other test set,
which generally does not reach the same level of
accuracy and may even be detrimental (climb-
ing on PP3).
Strangely enough, hill-climbing with the tun-
ing set equal to the test set itself sometimes does
not even yield the best quality for that test set
(POS with test set i and especially NP with j).
This shows that the weight-&gt;accuracy function
does have local maxima, and the increased risk
for smaller data sets to run into a sub-optimal
one is high enough that it happens in at least
two of the eight test set climbs.
</bodyText>
<footnote confidence="0.510748666666667">
10The accuracies for TiMBL are lower than those
found by Daelemans et al. (1999): POS i 97.95, POS,
97.90, GS i 93.75, GS, 93.58, PP t 83.64, Ppi 82.51, NPi
98.38 and NP i 98.25. This is due to the use of eight part
training sets instead of nine. The extreme differences for
the GS task show how much this task depends on indi-
vidual observations rather than on generalizations, which
probably also explains why Naive Bayes and Maximum
Extropy (Maccent) handle this task so badly.
</footnote>
<table confidence="0.8138285">
Weighting scheme Test set
ah i j
Comparison
Naive Bayes 96.52 96.49
TiMBL (k=3) 98.34 98.22
Maccent (freq=2;iter=150) 97.89 97.75
Maccent (freq=1;iter=300) 97.66 97.45
WPDV 0th order weights 97.56 97.77 97.69
1
k! 97.74 97.97 97.87
WPDV initial 13t order 98.19 98.38 98.26
tune = ah (380GR)
tune = i (60GR) 98.14 98.39 98.17
tune -.= j (360GR) 98.19 98.38 98.27
WPDV with hill-climbing 98.36 98.54 98.44
tune = ah (50 steps)
tune = i (34 steps) 98.25 98.57 98.33
tune = j (12 steps) 98.19 98.38 98.27
</table>
<bodyText confidence="0.999817636363636">
In summary, hill-climbing should preferably
be done with the tuning set equal to the training
set. This is not surprising, as the leave-one-
out mechanism allows the training set to behave
as held-out data, while containing eight times
more cases than a test set turned tuning set.
The disadvantage is a much more time-intensive
hill-climbing procedure, but when developing an
actual production model, the weights only have
to be determined once and the results appear to
be worth it most of the time.
</bodyText>
<sectionHeader confidence="0.997798" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879">
W. Daelemans, A. Van den Bosch, and J. Zavrel.
1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, Special issue
on Natural Language Learning, 34:11-41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2000. TiMBL: Tilburg Mem-
ory Based Learner, version 3.0, reference manual.
Tech. Report ILK-00-01, ILK, Tilburg University.
H. van Halteren. 2000a. Weighted Probability Dis-
tribution Voting, an introduction. In Computa-
tional linguistics in the Netherlands, 1999.
H. van Halteren. 2000b. The detection of in-
consistency in manually tagged text. In Proc.
LINC2000.
H. van Halteren, J. Zavrel, and W. Daelemans.
To appear. Improving accuracy in NLP through
combination of machine learning systems. Com-
putational Linguistics.
J.R. Quinlan. 1986. Induction of Decision Trees.
Machine Learning, 1:81-206.
</reference>
<page confidence="0.997491">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000008">
<note confidence="0.890716">of CoNLL-2000 and LLL-2000, 119-122, Lisbon, Portugal, 2000.</note>
<title confidence="0.999461">A Default First Order Family Weight Determination for WPDV Models</title>
<author confidence="0.999911">Hans van_Halteren</author>
<affiliation confidence="0.991168">Dept. of Language and Speech, University of</affiliation>
<address confidence="0.600853">P.O. Box 9103, 6500 HD Nijmegen, The hvh@1et.kun.n1</address>
<abstract confidence="0.998599545454546">Weighted Probability Distribution Voting (WPDV) is a newly designed machine learning algorithm, for which research is currently aimed at the determination of good weighting schemes. This paper describes a simple yet effective weight determination procedure, which leads to models that can produce competitive results for a number of NLP classification tasks. 1 The WPDV algorithm Weighted Probability Distribution Voting (WPDV) is a supervised learning approach to classification. A case which is to be classified is represented as a feature-value pair set: = {f. = v.}} An estimation of the probabilities of the various classes for the case in question is then based on the classes observed with similar feature-value pair sets in the training data. To be exact, the of class C for is estimated as weighted sum over all possible subsets Fcase: P(C)= N(C) freq(CI F3ubCFcase the frequencies on the data, and normalizing factor that 1. principle, the weight factors can be assigned per individual subset. For the time being, however, they are assigned for groups of subsets. First of all, it is possible to restrict the subsets that are taken into account in the using the size of the subset (e.g. at most 4 elements) its fre- (e.g. at least twice in the training material). Subsets which do not fulfil the chosen criteria are not used. For the subsets that are used, weight factors are not assigned per individual subset either, but rather per &amp;quot;family&amp;quot;, where a family consists of those subsets which contain the same combination of feature types (i.e. the same ft). The two components of a WPDV model, distributions and weights, are determined sepa- In this paper, I will use the term the data on which the distributions are and set the data on the basis of the are selected. Whether these two sets should be disjunct or can coincide is one of the subjects under investigation. 2 Family weights The various family weighting schemes can be classified according to the type of use they make the tuning data. Here, a very rough into weighting scheme Oth weights, information whatsoever is used about the data in tuning set. Examples of such rudimentary weighting are the use of a weight of all subcontaining as has been used e.g. for wordclass tagger combination (van Halteren et al., To appear), or even a uniform weight for all subsets. order weights, is used about the individual feature types, i.e. H {ilffi=vi/EF3.b} First order weights ignore any possible interaction between two or more feature types, but 119 have the clear advantage of corresponding to a reasonably low number of weights, viz, as many as there are feature types. order weights, patterns are determined of up to n feature types and the family weights are adjusted to compensate for the interaction. When n is equal to the total number of feature types, this corresponds to weight determination per individual order weighting generally requires much larger numbers of weights, which can be expected to lead to much slower tuning procedures. In this paper, therefore, I focus on first order weighting. 3 First order weight determination As argumented in an earlier paper (van Halteren, 2000a), a theory-based feature weight determination would have to take into account feature&apos;s However, clear definitions of these qualities, and hence also means to measure them, are as yet sorely lacking. As a result, a more pragmatic approach will have to be taken. Reliability is igaltogether at the and decisiveness replaced by an entropy-related measure. 3.1 Initial weights The weight given to each feature type fi should preferably increase with the amount of information it contributes to the classification process. measure related to this is Gain, which represents the difference between the entropy of the choice with and without knowledge of the presence of a feature (cf. Quinlan (1986)). As do Daelemans et al. (2000), I opt for a facproportional to the feature type&apos;s Ranormalising derivative of the Information Gain value. The weight factors Win are set to optimal multiplication constant the Gain Ratio for determined by calculating the accuracies for various values of the tuning and selecting the yields the highest accuracy. &apos;It may still be present, though, in the form of the abovementioned frequency threshold for features. the tuning set coincides with the training set, all parts of the tuning procedure are done in leave-one-out mode: in the WPDV implementation, it is possible to (virtually) remove the information about each individual instance from the model when that specific instance has to be classified. Since the initial weight determination is based on pragmatic rather than theoretical considerations, it is unlikely that the resulting weights are already the optimal ones. For this reason, an attempt is made to locate even better weight vectors in the n-dimensional weight space. The mechanism used in this search is hillmeans that systematic variations of the currently best vector are investigated. If the best variation is better than the currently best vector, that variation is taken as the best vector and the process is repeated. This repetition continues until no better vector is found. In the experiments described here, the variation consists of multiplication or division of each a variable V (i.e. 2n new vectors are tested each time), which is increased if a better vector is found, and otherwise decreased. The process is halted as soon as V falls below some pre-determined threshold. Hill-climbing, as most other optimalization techniques, is vulnerable to overtraining. To lessen this vulnerability, the WPDV hillclimbing implementation splits its tuning material into several (normally five) parts. A switch to a new weight vector is only taken if the accuracy increases on the tuning set as a whole and does not decrease on more than one part, i.e. some losses are accepted but only if they are localized. 4 Quality of the first order weights In order to determine the quality of the WPDV system, using first order weights as described above, I run a series of experiments, using tasks by Daelemans et al. tagging (POS) is to determine a wordclass tag on the basis of disambiguated tags of two preceding tokens and undisambiguated tags for the focus and two fol- 5 features with 170-480 values; 169 classes; 837Kcase training; 2x105Kcase test. conversion stress (GS) is to determine the pronunciation of an English grapheme, including only give a rough description of the tasks here. For the exact details, I refer the reader to Daelemans et al. (1999). a overall WPDV approach to wordclass tagging, see van Halteren (2000b). 120 Table 1: Accuracies for the POS task (with the Table 2: Accuracies for the GS task (with the set in leave-one-out mode) training set in leave-one-out mode) Weighting scheme Test set ah i j</abstract>
<note confidence="0.742421230769231">Comparison Naive Bayes 96.41 96.24 TiMBL (k=1) 97.83 97.79 Maccent (freq=2;iter=150) 98.07 98.03 Maccent (freq=1;iter=300) 98.13 98.10 0thorder weights 1 97.66 97.71 97.63 k! 96.86 96.92 96.86 initial order =- 98.14 98.16 98.12 tune = i (12GR) 98.14 98.17 98.12 = 98.14 98.16 98.13 WPDV with hill-climbing = steps) 98.17 98.21 98.15 tune = i (20 steps) 98.15 98.20 98.12 = steps) 98.15 98.18 98.16</note>
<abstract confidence="0.997651666666666">presence of stress, on the basis of the focus grapheme, three preceding and three following graphemes. 7 features with 42 values each; 159 classes; 540Kcase training; 2x68Kcase test. attachment (PP) is prepositional phrase attachment to either a preceding verb or a preceding noun, on the basis of the verb, the noun, the preposition in question and the head noun of the prepositional complement. 4 features with 3474, 4612, 68 and 5780 values; 2 classes; 19Kcase training; 2x2Kcase test. chunking (NP) is the determination of the position of the focus token in a base NP chunk (at beginning of chunk, in chunk, or not in chunk), on the basis of the words and tags for two preceding tokens, the focus and one following token, and also the predictions by newfirst stage classifiers for the 11 features with 3 (first stage classifiers), 90 (tags) and 20K (words) values; 3 classes; 201Kcase 2x25Kcase each of the tasks, sections a to the data set are used as the training set and sections i a WPDV approach to a more general chunking task, see my contribution to the CoNLL shared task, elsewhere in these proceedings. number of feature combinations for the NP task is so large that the WPDV model has to be limited. For the current experiments, I have opted for a maximum for four features and a threshold frequency of two observations in the training set. Weighting scheme Test set ah i j</abstract>
<note confidence="0.724150692307692">Comparison Naive Bayes 50.05 49.98 TiMBL (k=1) 92.25 92.02 Maccent (freq=2;iter=150) 79.41 79.36 Maccent (freq=1;iter=300) 80.43 80.35 0thorder weights 1 90.99 90.49 90.25 k! 92.77 92.05 91.89 initial order tune = ah (30GR) 93.27 92.74 92.52 tune = i (25GR) 93.24 92.76 92.54 = 93.24 92.76 92.54 WPDV with hill-climbing = steps) 93.29 92.77 92.53 tune = i (28 steps) 93.25 92.79 92.53 = steps) 93.24 92.76 92.54</note>
<abstract confidence="0.997583909090909">(two separate) test sets. All three are also used as tuning sets. This allows a comparison between tuning on the training set itself and on a held-out tuning set. For comparison with some other well-known machine learning algothe WPDV experiments with accuracy measurements for three other syssystem using Naive Bayes probestimation; 2) memory based learning and probability estimation based on the nearest neighbours (Daelemans et al., for which the parameters which yielded the best results according to Daelemans al. (1999); and 3) maximum enbased for which I use both the default parameters, viz, a frequency threshold of 2 for features to be used and 150 iterations of improved iterative scaling, and a more amparameter setting, viz, a threshold of and 300 iterations. The results for various WPDV weights, and the other machine learning techniques are listed Tables 1 to Except for one case (PP with on test on i), the first order weight results are than those for the accuracy is shown in italics whenever the tuning set is equal to the test set, i.e. when there is an unfair advantage. 121 Table 3: Accuracies for the PP task (with the Table 4: Accuracies for the NP task (with the set in leave-one-out mode) training set in leave-one-out mode) Weighting scheme Test set ah i 1</abstract>
<title confidence="0.369108">Comparison</title>
<note confidence="0.938918666666667">Naive Bayes 82.68 82.64 TiMBL (k=1) 83.43 81.97 Maccent (freq=2;iter=150) 81.00 80.25 Maccent (freq=1;iter=300) 79.41 79.79 0thorder weights 1 80.83 82.26 81.46 k! 80.76 82.30 81.30 initial order = 82.89 83.64 82.38 tune = i (15GR) 82.82 83.81 82.55 = 82.60 83.26 82.76 WPDV with hill-climbing = steps) 83.10 83.72 82.68 tune = i (18 steps) 82.95 84.06 82.80 = steps) 82.65 83.10 82.93</note>
<abstract confidence="0.996409970588236">order weights generally do not reach this level of accuracy. Hill-climbing with the tuning set equal to the training set produces the best results overall. It always leads to an improvement over initial weights of the accuracies on both test sets, although sometimes very small (GS). Equally important, the improvement on the test sets is comparable to that on the tuning/training set. This is certainly not the case for hill-climbing with the tuning set equal to the other test set, which generally does not reach the same level of accuracy and may even be detrimental (climbon Strangely enough, hill-climbing with the tuning set equal to the test set itself sometimes does not even yield the best quality for that test set with test set i and especially NP with This shows that the weight-&gt;accuracy function does have local maxima, and the increased risk for smaller data sets to run into a sub-optimal one is high enough that it happens in at least two of the eight test set climbs. accuracies for TiMBL are lower than those found by Daelemans et al. (1999): POS i 97.95, POS, GS i 93.75, GS, 93.58, PP t83.64, Ppi 82.51, 98.38 and NP i 98.25. This is due to the use of eight part training sets instead of nine. The extreme differences for the GS task show how much this task depends on individual observations rather than on generalizations, which probably also explains why Naive Bayes and Maximum Extropy (Maccent) handle this task so badly. Weighting scheme Test set ah i j</abstract>
<note confidence="0.776087923076923">Comparison Naive Bayes 96.52 96.49 TiMBL (k=3) 98.34 98.22 Maccent (freq=2;iter=150) 97.89 97.75 Maccent (freq=1;iter=300) 97.66 97.45 0thorder weights 1 97.56 97.77 97.69 k! 97.74 97.97 97.87 initial order = 98.19 98.38 98.26 tune = i (60GR) 98.14 98.39 98.17 -.= 98.19 98.38 98.27 WPDV with hill-climbing = steps) 98.36 98.54 98.44 tune = i (34 steps) 98.25 98.57 98.33 = steps) 98.19 98.38 98.27</note>
<abstract confidence="0.809860133333333">In summary, hill-climbing should preferably be done with the tuning set equal to the training set. This is not surprising, as the leave-oneout mechanism allows the training set to behave as held-out data, while containing eight times more cases than a test set turned tuning set. The disadvantage is a much more time-intensive hill-climbing procedure, but when developing an actual production model, the weights only have to be determined once and the results appear to be worth it most of the time. References W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in lanlearning. Learning, Special issue</abstract>
<affiliation confidence="0.725526">Natural Language Learning,</affiliation>
<address confidence="0.680908">Daelemans, J. Zavrel, K. Sloot, and</address>
<note confidence="0.76793625">A. Van den Bosch. 2000. TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual. Report ILK, Tilburg University. H. van Halteren. 2000a. Weighted Probability Dis-</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning, Special issue on Natural Language Learning,</booktitle>
<pages>34--11</pages>
<marker>Daelemans, Van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, Special issue on Natural Language Learning, 34:11-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual.</title>
<date>2000</date>
<tech>Tech. Report ILK-00-01,</tech>
<institution>ILK, Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2000</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2000. TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual. Tech. Report ILK-00-01, ILK, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>Weighted Probability Distribution Voting, an introduction.</title>
<date>2000</date>
<booktitle>In Computational linguistics in the Netherlands,</booktitle>
<marker>van Halteren, 2000</marker>
<rawString>H. van Halteren. 2000a. Weighted Probability Distribution Voting, an introduction. In Computational linguistics in the Netherlands, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>The detection of inconsistency in manually tagged text.</title>
<date>2000</date>
<booktitle>In Proc. LINC2000.</booktitle>
<marker>van Halteren, 2000</marker>
<rawString>H. van Halteren. 2000b. The detection of inconsistency in manually tagged text. In Proc. LINC2000.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>To appear. Improving accuracy in NLP through combination of machine learning systems.</title>
<journal>Computational Linguistics.</journal>
<marker>van Halteren, Zavrel, Daelemans, </marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. To appear. Improving accuracy in NLP through combination of machine learning systems. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of Decision Trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--81</pages>
<contexts>
<context position="4590" citStr="Quinlan (1986)" startWordPosition="759" endWordPosition="760">itions of these qualities, and hence also means to measure them, are as yet sorely lacking. As a result, a more pragmatic approach will have to be taken. Reliability is ignored altogether at the moment,1 and decisiveness replaced by an entropy-related measure. 3.1 Initial weights The weight given to each feature type fi should preferably increase with the amount of information it contributes to the classification process. A measure related to this is Information Gain, which represents the difference between the entropy of the choice with and without knowledge of the presence of a feature (cf. Quinlan (1986)). As do Daelemans et al. (2000), I opt for a factor proportional to the feature type&apos;s Gain Ratio, a normalising derivative of the Information Gain value. The weight factors Win are set to an optimal multiplication constant C times the measured Gain Ratio for L. C is determined by calculating the accuracies for various values of C on the tuning set2 and selecting the C which yields the highest accuracy. &apos;It may still be present, though, in the form of the abovementioned frequency threshold for features. 2If the tuning set coincides with the training set, all parts of the tuning procedure are </context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J.R. Quinlan. 1986. Induction of Decision Trees. Machine Learning, 1:81-206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>