<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004951">
<title confidence="0.9599025">
IIITH: A Corpus-Driven Co-occurrence Based Probabilistic Model for
Noun Compound Paraphrasing
</title>
<author confidence="0.995834">
Nitesh Surtani, Arpita Batra, Urmi Ghosh and Soma Paul
</author>
<affiliation confidence="0.745719333333333">
Language Technologies Research Centre
IIIT Hyderabad
Hyderabad, Andhra Pradesh-500032
</affiliation>
<email confidence="0.958808">
{nitesh.surtaniug08, arpita.batra, urmi.ghosh}@students.iiit.ac.in, soma@iiit.ac.in
</email>
<sectionHeader confidence="0.99419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999753884615385">
This paper presents a system for automatically
generating a set of plausible paraphrases for a
given noun compound and rank them in de-
creasing order of their usage represented by
the confidence value provided by the human
annotators. Our system implements a corpus-
driven probabilistic co-occurrence based
model for predicting the paraphrases, that uses
a seed list of paraphrases extracted from cor-
pus to predict other paraphrases based on their
co-occurrences. The corpus study reveals that
the prepositional paraphrases for the noun
compounds are quite frequent and well cov-
ered but the verb paraphrases, on the other
hand, are scarce, revealing the unsuitability of
the model for standalone corpus-driven ap-
proach. Therefore, to predict other paraphras-
es, we adopt a two-fold approach: (i)
Prediction based on Verb-Verb co-
occurrences, in case the seed paraphrases are
greater than threshold; and (ii) Prediction
based on Semantic Relation of NC, otherwise.
The system achieves a comparabale score of
0.23 for the isomorphic system while main-
taining a score of 0.26 for the non-isomorphic
system.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954558139535">
Semeval 2013 Task 4 (Hendrickx et. al., 2013),
“Free Paraphrases of Noun Compounds” is a pa-
raphrase generation task that requires the system to
generate multiple paraphrases for a given noun
compound and rank them to the best approxima-
tion of the human rankings, represented by the cor-
responding confidence value. The task is an
extension of Semeval 2010 Task 9 (Butnariu et al.,
2010), where the participants were asked to rank
the set of given paraphrases for each noun com-
pound. Although the ranking task is quite distinct
from the task of generating paraphrases, however,
we have taken many insights from the systems de-
veloped for the ranking task, and have reported
them appropriately in our system description.
This paper describes a system for generating a
ranked set of paraphrases for a given NC. A pa-
raphrase can be Prepositional, Verb or Verb + Pre-
positional. Since the prepositional paraphrases are
easily available in the corpus while the occurrences
of verb or verb+prep paraphrases is scarce, the task
of paraphrasing becomes significant in finding out
a method for predicting reliable paraphrases with
verbs for a given NC. Our system implements a
model that is based on co-occurrences of the pa-
raphrases and selects those paraphrases that have a
higher probability of co-occurring with a set of
extracted paraphrases which are referred to as Seed
Paraphrases. Keeping the verb-paraphrase scarcity
issue in mind, we develop a two-way model: (i)
Model 1 is used when the seed paraphrases are
considerable in number i.e., greater than the thre-
shold value. In this case, other verb paraphrases are
predicted based on their co-occurrence with the set
of extracted verb paraphrases. (ii) Model 2 is used
when the size of the seed list falls below the thre-
shold value, in which case, we make use of the
prepositional paraphrases to predict the relation of
the noun compound and select verbs that mostly
co-occur with that relation. Our system achieves an
isomorphic score of 0.23 with a non-isomorphic of
0.26 with the human generated paraphrases. The
next section discusses the system.
</bodyText>
<sectionHeader confidence="0.975063" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.976002">
This section of the paper describes each module of
the system in detail. The first module of the system
</bodyText>
<page confidence="0.986175">
153
</page>
<bodyText confidence="0.983600111111111">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 153–157, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
talks about the Seed data extraction using corpus
search. The next module uses the seed data for
predicting more verbs that would be used in pa-
raphrasing. The third module uses these predicted
verbs in template generation for generating NC
Paraphrasing and the generated paraphrases are
ranked in the last module.
</bodyText>
<subsectionHeader confidence="0.998434">
2.1 Seed Data Extraction Module
</subsectionHeader>
<bodyText confidence="0.997462882352941">
We have relied mostly on the Google N-gram Cor-
pus for extracting the seed paraphrases. Google has
publicly released their web data as n-grams, also
known as Web-1T corpus, via the Linguistic Data
Consortium (Brants and Franz, 2006). It contains
sequences of n-terms that occur more than 40 times
on the web. Since the corpus consists of raw data
from the web, certain pre-processing steps are es-
sential before it can be used. We extract a set of
POS templates from the training data, and general-
ize them enough to accommodate the legitimate
paraphrases extracted from the corpus. The follow-
ing templates are used for extracting n-gram data:
Head-Mod I-gram: This template includes both
the head and the modifier in the same regular ex-
pression. A corresponding 5-gram template for a
NC Amateur-Championship is shown in Table 1.
</bodyText>
<table confidence="0.824757666666667">
Head &lt;*&gt; &lt;*&gt; championship conducted for the
&lt;*&gt;Mod amateurs
Head &lt;*&gt;&lt;*&gt; championship for all amateur
Mod &lt;*&gt; players
Head &lt;*&gt;Mod championship where amateur is
&lt;*&gt;&lt;*&gt; competing
</table>
<tableCaption confidence="0.992786">
Table 1: Templates for paraphrase extraction
</tableCaption>
<bodyText confidence="0.999892571428571">
The paraphrases obtained from the above template
are quite useful, but scarce. To overcome the issue
of coverage of verb paraphrases, a loosely coupled
analysis and representation of compounds can be
employed, as suggested by (Li et.al, 2010). We
retrieve the partial triplets from the n-gram corpus
in the form of “Head Para” and “Para Modifier”.
</bodyText>
<equation confidence="0.4019485">
(Head, Para, ?)
(?, Para, Mod)
Head Template: Head &lt;*&gt; &lt;*&gt;
Mod Template: &lt;*&gt; &lt;*&gt; Mod; &lt;*&gt; Mod &lt;*&gt;
</equation>
<bodyText confidence="0.999437333333333">
But the process of generating paraphrases from
head and the modifier n-gram incorporates a huge
amount of noise and produces a lot of irrelevant
paraphrases. Therefore, these partial paraphrases
are not directly used for generating the paraphrases
but are instead used to diagnose the compatibility
of the selected verb with the head and the modifier
of the given NC in Section 2.2.2. We also extract
paraphrases from ANC and BNC corpus.
</bodyText>
<subsectionHeader confidence="0.996691">
2.2 Verb Prediction Module
</subsectionHeader>
<bodyText confidence="0.999753222222222">
This module is the heart of our system. It imple-
ments two models for predicting the verb paraph-
rases: a Verb Co-occurrence model and a Relation
Prediction model. The decision of selection of
model for verb prediction is based on the size of
the seed list. If the number of seed paraphrases is
above the threshold value, the verb co-occurrence
model is used whereas the relation prediction mod-
el is used if it is below the threshold value.
</bodyText>
<subsubsectionHeader confidence="0.517034">
2.2.1 Verb Co-occurrence Model
</subsubsectionHeader>
<bodyText confidence="0.999725736842106">
This model uses the seed paraphrases extracted
from the corpus to predict other verb paraphrases
by computing their co-occurrences. The model
gains insights from the UCD-PN system (Nulty
and Costello, 2010) which tries to identify a more
general paraphrase by computing the co-
occurrence of a paraphrase with other paraphrases.
But the task of generating paraphrases has two sub-
tle but significant differences: (i) The list of seed
verb paraphrases for a given NC is usually small,
with each seed verb having a corresponding proba-
bility of occurrence; and (ii) Not all the seed verbs
have legitimate representation of the noun com-
pound. Our system incorporates these distinctions
in the co-occurrence model discussed below.
Using the training data at hand, we build a Verb-
Verb co-occurrence matrix, a 2-D matrix where
each cell (i,j) represents the probability of occur-
rence of Vj when Vi has already occurred.
</bodyText>
<equation confidence="0.86061">
Count (Vi, V� )
Count(Vi)
</equation>
<bodyText confidence="0.999880857142857">
The verbs used in co-occurrence matrix are stored
in a List A. Now, for a given test NC, the model
extracts the seed list of verb paraphrases (referred
as List B) from the corpus with their corresponding
probabilities. The above model calculates a score
for each verb in List A, by computing its co-
occurrence with the verbs in List B.
</bodyText>
<equation confidence="0.8219065">
scoreQGA(VQ) _ I P(VQlVb) * P(Vb)
beB
</equation>
<table confidence="0.811244333333333">
(Head, Para, Mod)
P(V� IVi) _ P (Vi, V� )
P (Vi)
</table>
<page confidence="0.998651">
154
</page>
<bodyText confidence="0.9996995">
The term P (Vb) in the above equation represents
the relative occurrence of the verb Vb with the giv-
en NC. The relevance of this term becomes evident
in the next model. The verbs achieving higher
score are selected, suggesting a higher probability
of co-occurrence with the seed verbs.
</bodyText>
<subsubsectionHeader confidence="0.850968">
2.2.2 Semantic Relation Prediction Model
</subsubsectionHeader>
<bodyText confidence="0.999690909090909">
This module describes the second model of the
two-way model, and is used by the system when
the verbs extracted from the corpus are less than
the threshold. In this model, we use prepositional
paraphrases, having a pretty good coverage in the
corpus, to predict the semantic relation of the com-
pound which helps us in predicting the other pa-
raphrases. The intuition behind using semantic
class for predicting paraphrases is that they tend to
capture the behavior of the noun compound and
can be represented by general paraphrases.
</bodyText>
<table confidence="0.9992878">
Noun Compound Relation Paraphrase Sel.
Prep Verb
Garden Party Location In, At Held
Community Life Theme Of, In Made
Advertising Agency Purpose For, Of, In Doing
</table>
<tableCaption confidence="0.995124">
Table 2: Occurrence of Prepositional Paraphrases
</tableCaption>
<bodyText confidence="0.996859093023256">
Relation Annotation: Since a supervised ap-
proach is used for identifying the semantic relation
of the noun compound, we manually annotate the
noun compounds with a semantic relation. We tag
each noun compound with one semantic relation
from the set used in (Moldovan et. al. 2004).
Prep-Rel and Verb-Rel Co-occurrence: A Prep-
Rel co-occurrence matrix similar to Verb-Verb co-
occurrence matrix discussed in last subsection.
This 2-D matrix consists of co-occurrence proba-
bilities between the prepositional paraphrases and
the semantic relation of the compound, where each
cell (i,j) represents the probability of occurrence of
preposition Pj with relation Ri. This matrix is used
as a model to identify semantic relation using pre-
positional paraphrases extracted from the corpus.
The Verb-Relation co-occurrence matrix is used to
predict the most co-occurring verbs with the identi-
fied relation. Each cell (i,j) in the matrix represents
the probability of the verb Vj co-occurring with
relation Ri.
Relation Extraction: Research focusing on se-
mantic relation extraction has followed two direc-
tions: (i) Statistical approaches to using very large
corpus (Berland and Charniak (1999); Hearst
(1998)); and (ii) Ontology based approaches using
hierarchical structure of wordnet (Moldovan et. al.,
2004). We employ a statistical model based on the
Preposition-Relation co-occurrence for identifying
the relation. The model is quite similar to the one
used in Section 2.2, but it is here that the model
reveals its actual power. Since two or more rela-
tions can be represented by same set of preposi-
tional paraphrases, as Theme and Purpose in Table
2, it is important to take into account the probabili-
ties with which the extracted prepositions occur in
the corpus. In Table 2, the NC Community Life
(Theme) occurs frequently with preposition „of‟
whereas the NC Advertising Agency (Purpose) is
mostly represented by preposition „for‟ in the cor-
pus. The term P (Pp) in the equation below cap-
tures this phenomenon and classifies these two
NCs in their respective classes.
</bodyText>
<equation confidence="0.996404">
scorerGR(r) = I P(rlPp) * P(Pp)
pGP
</equation>
<bodyText confidence="0.999952941176471">
The relation with the highest score is selected as
the semantic class of the noun compound. A set of
verbs highly co-occurring with that class are se-
lected, and their compatibility with the correspond-
ing noun compound is judged from their
occurrences with the partial head and the modifier
paraphrases as discussed in Section 2.1. The above
classifier performs moderately and classifies a giv-
en NC with 42.5% accuracy. We have also tried
the Wordnet based Semantic Scattering model
(Moldovan et. al., 2004), trained on a set of 400
instances, but achieved an accuracy of 38%, the
reason for which can be attributed to the small
training set. Since the accuracy of identifying the
correct relation is low, we select some paraphrases
from the 2nd most probable relation, as assigned by
the probabilistic classifier.
</bodyText>
<subsectionHeader confidence="0.999905">
2.3 Paraphrase Generator Module
</subsectionHeader>
<bodyText confidence="0.994135333333333">
After predicting a set of verb for a test noun com-
pound, we use the following templates to generate
the paraphrases:
</bodyText>
<listItem confidence="0.734029666666667">
a) Head VP Mod
b) Head VP PP Mod
c) Head [that|which] VP PP Mod
</listItem>
<bodyText confidence="0.490035333333333">
The paraphrases that are extracted from the corpus
are also cleaned using the POS templates extracted
from the training data.
</bodyText>
<page confidence="0.996817">
155
</page>
<subsectionHeader confidence="0.998812">
2.4 Paraphrase Ranker Module
</subsectionHeader>
<bodyText confidence="0.999041">
Motivated by the observations from Nulty and
Costello (2010) that “people tend to use general,
semantically light paraphrases more often than de-
tailed, semantically heavy ones”, we perform rank-
ing of the paraphrases in two steps: (i) Assigning
different weights to different type of paraphrases,
i.e. a light weight prepositional paraphrases achiev-
ing higher score than the verb paraphrases; and (ii)
Ranking a more general paraphrase with the same
category higher. A paraphrase A is more general
that paraphrase B (Nulty and Costello, 2010) if
</bodyText>
<equation confidence="0.904417">
P(A|B) &gt; P(B|A)
</equation>
<bodyText confidence="0.9998755">
For a list of paraphrases A generated for a given
compound, each paraphrase b in that list is scored
using the below eq., where more general paraph-
rase achieves a high score and is ranked higher.
</bodyText>
<equation confidence="0.993703">
score(b) = I P(bla)
acA
</equation>
<bodyText confidence="0.999933">
The seed paraphrases extracted from the corpus are
ranked higher than the predicted paraphrases.
</bodyText>
<sectionHeader confidence="0.991188" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.998044">
This section presents the implementation of the
overall system.
</bodyText>
<equation confidence="0.98917465625">
// Training Phase – Build Co-occurrence Matrices
Verb_Co-occur = 2-D Matrix
Prep-Rel_Co-occur = 2-D Matrix
Verb-Rel_Co-occur = 2-D Matrix
Verb_List = Verb List extracted from training corpus
// Testing – Extract paraphrases with probabilities
Ext_Verb = List of extracted verb paraphrase
VProb = Probability of each Ext_Verb
Ext_Prep = List of extracted prepositional paraphrases
PProb = Probability of each Ext_Prep
Prob_Verb = List // Verbs with their selection score
Prob_Rel = List // Relations with their selection score
Threshold = 3 // Verb threshold for two-way model
if count( Ext_Verb ) &gt; Threshold
Candidate_Verbs = {Verb_List } - { Ext_Verbs }
foreach Candidate_Verbs Vi :
Prob_Verb[Vi] = 0
foreach Ext_Verb Vj :
Prob_Verb[Vi] += Verb_Co-occur [Vi][Vj] *
VProb[Vj]
else
foreach Prep-Rel_Co-occur as rel :
Prob_Rel[rel] = 0
foreach Ext_Prep as prep :
Prob_Rel[rel] += Prep-Rel_Co-occur[rel][prep]
* PProb[prep]
Rel=select highestProb(Prob_Rel)
Prob_Verb = Verb-Rel_Co-occur[Rel]
sort(Prob_Verb)
Verb_Predicted = select top(N)
Paraphrase = generate_paraphrase(verb_predicted)
rank(Paraphrase)
</equation>
<sectionHeader confidence="0.99989" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9998482">
The set of generated paraphrases are evaluated on
two metrics: a) Isomorphic; b) Non-isomorphic. In
the isomorphic setting, the test paraphrase is
matched to the closest reference paraphrases, but
the reference paraphrase is removed from the set
whereas in non-isomorphic setting, the reference
paraphrase which is mapped to a test paraphrase
can still be used for matching other test paraphras-
es. Table 3 presents the scores of the 3 participat-
ing teams who have submitted total of 4 systems.
</bodyText>
<table confidence="0.9962248">
Systems Isomorphic Non-Isomorphic
SFS 0.2313 0.1794
IIITH 0.2309 0.2583
MELODI-Pri 0.1298 0.5484
MELODI-Cont 0.1357 0.536
</table>
<tableCaption confidence="0.999932">
Table 3: Results of the submitted systems
</tableCaption>
<bodyText confidence="0.999945333333333">
Our system achieves an isomorphic score of 0.23,
just below the SFS system maintaining a score of
0.26 for the non-isomorphic system. The two va-
riants of MELODI system get a high score for the
non-isomorphic metric but low scores for isomor-
phic metric as compared to other systems.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988785714286">
We have described a system for automatically ge-
nerating a set of paraphrases for a given noun
compound, based on the co-occurrences of the pa-
raphrases. The system describes an approach for
handling those 38% cases (calculated for optimum
threshold value) of NCs where it is not convenient
to predict the verbs using their co-occurrences with
the seed verbs, because the size of the seed list is
below a threshold value. For other cases, the verb
co-occurrence model is used to predict the verbs
for NC paraphrasing. The optimum value of thre-
shold parameter investigated from experiments is
found to be 3, showing that atleast 3 verb paraph-
rases are necessary to capture the concept of a NC.
</bodyText>
<page confidence="0.998358">
156
</page>
<sectionHeader confidence="0.993888" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774131578947">
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large. In Proceeding of ACL 1999
T. Brants and A. Franz. 2006. Web 1T 5-gram Version1.
Linguistic Data Consortium
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid O S´ eaghdha, Stan Szpakowicz, and Tony-
Veale. 2010. Semeval-2 task 9: The interpreta-tion of
noun compounds using paraphrasing verbs and pre-
positions. In Proceedings of the 5th SIGLEX Work-
shop on Semantic Evaluation
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid O S´eaghdha, Stan Szpakowicz, and Tony-
Veale. 2013. Semeval’13 task 4: Free Paraphrases of
Noun Compounds. In Proceedings of the Internation-
al Workshop on Semantic Evaluation, Atlanta, Geor-
gia
Marti Hearst. 1998. Automated Discovery of Word-Net
relations. In An Electronic Lexical Database and-
Some of its Applications. MIT Press, Cambridge MA
Mark Lauer. 1995. Designing Statistical Language-
Learners: Experiments on Noun Compounds. Ph.D.
Thesis, Macquarie University
Guofu Li, Alejandra Lopez-Fernandez and Tony Veale.
2010. UCD-Goggle: A Hybrid System for Noun
Compound Paraphrasing. In Proceedings of the 5th
International Workshop on Semantic Evaluation
(SemEval-2), Uppsala, Sweden
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on
Computational Lexical Semantics, pages 60–67, Bos-
ton, MA
Paul Nulty and Fintan Costello. 2010. UCD-PN: Select-
ing general paraphrases using conditional probabili-
ty. In Proceedings of the 5th International Workshop
on Semantic Evaluation (SemEval-2), Uppsala, Swe-
den
</reference>
<page confidence="0.997705">
157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401181">
<title confidence="0.9976735">IIITH: A Corpus-Driven Co-occurrence Based Probabilistic Model for Noun Compound Paraphrasing</title>
<author confidence="0.573109">Nitesh Surtani</author>
<author confidence="0.573109">Arpita Batra</author>
<author confidence="0.573109">Urmi Ghosh</author>
<author confidence="0.573109">Soma</author>
<affiliation confidence="0.7930015">Language Technologies Research IIIT</affiliation>
<address confidence="0.858993">Hyderabad, Andhra</address>
<email confidence="0.860815">nitesh.surtaniug08@students.iiit.ac.in,soma@iiit.ac.in</email>
<email confidence="0.860815">arpita.batra@students.iiit.ac.in,soma@iiit.ac.in</email>
<email confidence="0.860815">urmi.ghosh@students.iiit.ac.in,soma@iiit.ac.in</email>
<abstract confidence="0.998383074074074">This paper presents a system for automatically generating a set of plausible paraphrases for a given noun compound and rank them in decreasing order of their usage represented by the confidence value provided by the human annotators. Our system implements a corpusdriven probabilistic co-occurrence based model for predicting the paraphrases, that uses a seed list of paraphrases extracted from corpus to predict other paraphrases based on their co-occurrences. The corpus study reveals that the prepositional paraphrases for the noun compounds are quite frequent and well covered but the verb paraphrases, on the other hand, are scarce, revealing the unsuitability of the model for standalone corpus-driven approach. Therefore, to predict other paraphrases, we adopt a two-fold approach: (i) Prediction based on Verb-Verb cooccurrences, in case the seed paraphrases are greater than threshold; and (ii) Prediction based on Semantic Relation of NC, otherwise. The system achieves a comparabale score of 0.23 for the isomorphic system while maintaining a score of 0.26 for the non-isomorphic system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large.</title>
<date>1999</date>
<booktitle>In Proceeding of ACL</booktitle>
<contexts>
<context position="10302" citStr="Berland and Charniak (1999)" startWordPosition="1647" endWordPosition="1650">pound, where each cell (i,j) represents the probability of occurrence of preposition Pj with relation Ri. This matrix is used as a model to identify semantic relation using prepositional paraphrases extracted from the corpus. The Verb-Relation co-occurrence matrix is used to predict the most co-occurring verbs with the identified relation. Each cell (i,j) in the matrix represents the probability of the verb Vj co-occurring with relation Ri. Relation Extraction: Research focusing on semantic relation extraction has followed two directions: (i) Statistical approaches to using very large corpus (Berland and Charniak (1999); Hearst (1998)); and (ii) Ontology based approaches using hierarchical structure of wordnet (Moldovan et. al., 2004). We employ a statistical model based on the Preposition-Relation co-occurrence for identifying the relation. The model is quite similar to the one used in Section 2.2, but it is here that the model reveals its actual power. Since two or more relations can be represented by same set of prepositional paraphrases, as Theme and Purpose in Table 2, it is important to take into account the probabilities with which the extracted prepositions occur in the corpus. In Table 2, the NC Com</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large. In Proceeding of ACL 1999</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version1. Linguistic Data Consortium</booktitle>
<contexts>
<context position="4496" citStr="Brants and Franz, 2006" startWordPosition="697" endWordPosition="700">013 Association for Computational Linguistics talks about the Seed data extraction using corpus search. The next module uses the seed data for predicting more verbs that would be used in paraphrasing. The third module uses these predicted verbs in template generation for generating NC Paraphrasing and the generated paraphrases are ranked in the last module. 2.1 Seed Data Extraction Module We have relied mostly on the Google N-gram Corpus for extracting the seed paraphrases. Google has publicly released their web data as n-grams, also known as Web-1T corpus, via the Linguistic Data Consortium (Brants and Franz, 2006). It contains sequences of n-terms that occur more than 40 times on the web. Since the corpus consists of raw data from the web, certain pre-processing steps are essential before it can be used. We extract a set of POS templates from the training data, and generalize them enough to accommodate the legitimate paraphrases extracted from the corpus. The following templates are used for extracting n-gram data: Head-Mod I-gram: This template includes both the head and the modifier in the same regular expression. A corresponding 5-gram template for a NC Amateur-Championship is shown in Table 1. Head</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1T 5-gram Version1. Linguistic Data Consortium</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Su Nam Kim</author>
<author>Preslav Nakov</author>
<author>Diarmuid O S´ eaghdha</author>
<author>Stan Szpakowicz</author>
<author>TonyVeale</author>
</authors>
<title>Semeval-2 task 9: The interpreta-tion of noun compounds using paraphrasing verbs and prepositions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation</booktitle>
<contexts>
<context position="1827" citStr="Butnariu et al., 2010" startWordPosition="268" endWordPosition="271">r than threshold; and (ii) Prediction based on Semantic Relation of NC, otherwise. The system achieves a comparabale score of 0.23 for the isomorphic system while maintaining a score of 0.26 for the non-isomorphic system. 1 Introduction Semeval 2013 Task 4 (Hendrickx et. al., 2013), “Free Paraphrases of Noun Compounds” is a paraphrase generation task that requires the system to generate multiple paraphrases for a given noun compound and rank them to the best approximation of the human rankings, represented by the corresponding confidence value. The task is an extension of Semeval 2010 Task 9 (Butnariu et al., 2010), where the participants were asked to rank the set of given paraphrases for each noun compound. Although the ranking task is quite distinct from the task of generating paraphrases, however, we have taken many insights from the systems developed for the ranking task, and have reported them appropriately in our system description. This paper describes a system for generating a ranked set of paraphrases for a given NC. A paraphrase can be Prepositional, Verb or Verb + Prepositional. Since the prepositional paraphrases are easily available in the corpus while the occurrences of verb or verb+prep </context>
</contexts>
<marker>Butnariu, Kim, Nakov, eaghdha, Szpakowicz, TonyVeale, 2010</marker>
<rawString>Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diarmuid O S´ eaghdha, Stan Szpakowicz, and TonyVeale. 2010. Semeval-2 task 9: The interpreta-tion of noun compounds using paraphrasing verbs and prepositions. In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Su Nam Kim</author>
<author>Preslav Nakov</author>
<author>Diarmuid O S´eaghdha</author>
<author>Stan Szpakowicz</author>
<author>TonyVeale</author>
</authors>
<title>Semeval’13 task 4: Free Paraphrases of Noun Compounds.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation,</booktitle>
<location>Atlanta, Georgia</location>
<marker>Butnariu, Kim, Nakov, S´eaghdha, Szpakowicz, TonyVeale, 2013</marker>
<rawString>Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diarmuid O S´eaghdha, Stan Szpakowicz, and TonyVeale. 2013. Semeval’13 task 4: Free Paraphrases of Noun Compounds. In Proceedings of the International Workshop on Semantic Evaluation, Atlanta, Georgia</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automated Discovery of Word-Net relations. In An Electronic Lexical Database andSome of its Applications.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge MA</location>
<contexts>
<context position="10317" citStr="Hearst (1998)" startWordPosition="1651" endWordPosition="1652">represents the probability of occurrence of preposition Pj with relation Ri. This matrix is used as a model to identify semantic relation using prepositional paraphrases extracted from the corpus. The Verb-Relation co-occurrence matrix is used to predict the most co-occurring verbs with the identified relation. Each cell (i,j) in the matrix represents the probability of the verb Vj co-occurring with relation Ri. Relation Extraction: Research focusing on semantic relation extraction has followed two directions: (i) Statistical approaches to using very large corpus (Berland and Charniak (1999); Hearst (1998)); and (ii) Ontology based approaches using hierarchical structure of wordnet (Moldovan et. al., 2004). We employ a statistical model based on the Preposition-Relation co-occurrence for identifying the relation. The model is quite similar to the one used in Section 2.2, but it is here that the model reveals its actual power. Since two or more relations can be represented by same set of prepositional paraphrases, as Theme and Purpose in Table 2, it is important to take into account the probabilities with which the extracted prepositions occur in the corpus. In Table 2, the NC Community Life (Th</context>
</contexts>
<marker>Hearst, 1998</marker>
<rawString>Marti Hearst. 1998. Automated Discovery of Word-Net relations. In An Electronic Lexical Database andSome of its Applications. MIT Press, Cambridge MA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical LanguageLearners: Experiments on Noun Compounds.</title>
<date>1995</date>
<tech>Ph.D. Thesis,</tech>
<institution>Macquarie University</institution>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical LanguageLearners: Experiments on Noun Compounds. Ph.D. Thesis, Macquarie University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guofu Li</author>
<author>Alejandra Lopez-Fernandez</author>
<author>Tony Veale</author>
</authors>
<title>UCD-Goggle: A Hybrid System for Noun Compound Paraphrasing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2),</booktitle>
<location>Uppsala, Sweden</location>
<marker>Li, Lopez-Fernandez, Veale, 2010</marker>
<rawString>Guofu Li, Alejandra Lopez-Fernandez and Tony Veale. 2010. UCD-Goggle: A Hybrid System for Noun Compound Paraphrasing. In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2), Uppsala, Sweden</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
<author>Roxana Girju</author>
</authors>
<title>Models for the Semantic Classification of Noun Phrases.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL-04 Workshop on Computational Lexical Semantics,</booktitle>
<pages>60--67</pages>
<location>Boston, MA</location>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe, and Roxana Girju. 2004. Models for the Semantic Classification of Noun Phrases. In Proceedings of the HLT-NAACL-04 Workshop on Computational Lexical Semantics, pages 60–67, Boston, MA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Nulty</author>
<author>Fintan Costello</author>
</authors>
<title>UCD-PN: Selecting general paraphrases using conditional probability.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2),</booktitle>
<location>Uppsala, Sweden</location>
<contexts>
<context position="6900" citStr="Nulty and Costello, 2010" startWordPosition="1092" endWordPosition="1095">ts two models for predicting the verb paraphrases: a Verb Co-occurrence model and a Relation Prediction model. The decision of selection of model for verb prediction is based on the size of the seed list. If the number of seed paraphrases is above the threshold value, the verb co-occurrence model is used whereas the relation prediction model is used if it is below the threshold value. 2.2.1 Verb Co-occurrence Model This model uses the seed paraphrases extracted from the corpus to predict other verb paraphrases by computing their co-occurrences. The model gains insights from the UCD-PN system (Nulty and Costello, 2010) which tries to identify a more general paraphrase by computing the cooccurrence of a paraphrase with other paraphrases. But the task of generating paraphrases has two subtle but significant differences: (i) The list of seed verb paraphrases for a given NC is usually small, with each seed verb having a corresponding probability of occurrence; and (ii) Not all the seed verbs have legitimate representation of the noun compound. Our system incorporates these distinctions in the co-occurrence model discussed below. Using the training data at hand, we build a VerbVerb co-occurrence matrix, a 2-D ma</context>
<context position="12467" citStr="Nulty and Costello (2010)" startWordPosition="2007" endWordPosition="2010"> small training set. Since the accuracy of identifying the correct relation is low, we select some paraphrases from the 2nd most probable relation, as assigned by the probabilistic classifier. 2.3 Paraphrase Generator Module After predicting a set of verb for a test noun compound, we use the following templates to generate the paraphrases: a) Head VP Mod b) Head VP PP Mod c) Head [that|which] VP PP Mod The paraphrases that are extracted from the corpus are also cleaned using the POS templates extracted from the training data. 155 2.4 Paraphrase Ranker Module Motivated by the observations from Nulty and Costello (2010) that “people tend to use general, semantically light paraphrases more often than detailed, semantically heavy ones”, we perform ranking of the paraphrases in two steps: (i) Assigning different weights to different type of paraphrases, i.e. a light weight prepositional paraphrases achieving higher score than the verb paraphrases; and (ii) Ranking a more general paraphrase with the same category higher. A paraphrase A is more general that paraphrase B (Nulty and Costello, 2010) if P(A|B) &gt; P(B|A) For a list of paraphrases A generated for a given compound, each paraphrase b in that list is score</context>
</contexts>
<marker>Nulty, Costello, 2010</marker>
<rawString>Paul Nulty and Fintan Costello. 2010. UCD-PN: Selecting general paraphrases using conditional probability. In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2), Uppsala, Sweden</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>