<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003779">
<title confidence="0.979739">
A Stacked, Voted, Stacked Model for Named Entity Recognition
</title>
<author confidence="0.994897">
Dekai Wut* and Grace Ngait and Marine Carpuatt
</author>
<affiliation confidence="0.894046">
t Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.99096">
{dekai,marine}@cs.ust.hk
</email>
<author confidence="0.528467">
t Hong Kong Polytechnic University
</author>
<affiliation confidence="0.81565">
Department of Computing
Kowloon
</affiliation>
<address confidence="0.373154">
Hong Kong
</address>
<email confidence="0.990478">
csgngai@polyu.edu.hk
</email>
<sectionHeader confidence="0.998552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999804090909091">
This paper investigates stacking and voting
methods for combining strong classifiers like
boosting, SVM, and TBL, on the named-entity
recognition task. We demonstrate several ef-
fective approaches, culminating in a model that
achieves error rate reductions on the develop-
ment and test sets of 63.6% and 55.0% (En-
glish) and 47.0% and 51.7% (German) over the
CoNLL-2003 standard baseline respectively,
and 19.7% over a strong AdaBoost baseline
model from CoNLL-2002.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912105263158">
We describe multiple stacking and voting methods that
effectively combine strong classifiers such as boosting,
SVM, and TBL, for the named-entity recognition (NER)
task. NER has emerged as an important step for many
natural language applications, including machine trans-
lation, information retrieval and information extraction.
Much of the research in this field was pioneered in
the Message Understanding Conference (MUC) (Sund-
heim, 1995), which performed detailed entity extraction
and identification on English documents. As a result,
most current NER systems which have impressive per-
formances have been specially constructed and tuned for
English MUC-style documents. It is unclear how well
they would perform when applied to another language.
Our system was designed for the CoNLL-2003 shared
task, the goal of which is to identify and classify four
types of named entities: PERSON, LOCATION, ORGA-
NIZATION and MISCELLANEOUS. The task specifi-
cations were that two languages would be involved. We
</bodyText>
<footnote confidence="0.5769785">
*The author would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research in
part through two research grants (RGC 6083/99E and RGC
6256/00E).
</footnote>
<bodyText confidence="0.9998928">
were given about a month to develop our system on the
first language, which was English, but only two weeks to
adapt it to the surprise language, which was German.
Given the goal of the shared task, we designed our
system to achieve a high performance without relying
too heavily on knowledge that is very specific for a par-
ticular language or domain. In the spirit of language-
independence, we avoided using features and information
which would not be easily obtainable for almost any ma-
jor language.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="method">
2 Classification Methods
</sectionHeader>
<bodyText confidence="0.999901333333333">
To carry out the stacking and voting experiments, we con-
structed a number of relatively strong individual compo-
nent models of the following kinds.
</bodyText>
<subsectionHeader confidence="0.993468">
2.1 Boosting
</subsectionHeader>
<bodyText confidence="0.999087">
The main idea behind boosting algorithms is that a set
of many weak classifiers can be effectively combined to
yield a single strong classifier. Each weak classifier is
trained sequentially, increasingly focusing more heavily
on the instances that the previous classifiers found diffi-
cult to classify.
For the boosting framework, our system uses Ada-
Boost.MH (Freund and Schapire, 1997), an n-ary classifi-
cation variant of the original binary AdaBoost algorithm.
It performs well on a number of natural language process-
ing problems, including text categorization (Schapire and
Singer, 2000) and word sense disambiguation (Escudero
et al., 2000). In particular, it has also been demonstrated
that boosting can be used to build language-independent
NER models that perform exceptionally well (Wu et al.,
2002; Carreras et al., 2002).
</bodyText>
<subsectionHeader confidence="0.999347">
2.2 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.9998422">
Support Vector Machines (SVMs) have gained a con-
siderable following in recent years (Boser et al., 1992),
particularly in dealing with high-dimensional spaces
such as commonly found in natural language prob-
lems like text categorization (Joachims, 1998). SVMs
have shown promise when applied to chunking (Kudo
and Matsumoto, 2001) and named entity recognition
(Sassano and Utsuro, 2000; McNamee and Mayfield,
2002), though performance is quite sensitive to param-
eter choices.
</bodyText>
<subsectionHeader confidence="0.994267">
2.3 Transformation-based Learning
</subsectionHeader>
<bodyText confidence="0.999985692307692">
Transformation-based learning (Brill, 1995) (TBL) is a
rule-based machine learning algorithm that was first in-
troduced by Brill and used for part-of-speech tagging.
The central idea of transformation-based learning is to
learn an ordered list of rules which progressively improve
upon the current state of the training set. An initial as-
signment is made based on simple statistics, and then
rules are greedily learned to correct the mistakes, until
no net improvement can be made.
The experiments presented in this paper were per-
formed using the fnTBL toolkit (Ngai and Florian, 2001),
which implements several optimizations in rule learning
to drastically speed up the time needed for training.
</bodyText>
<sectionHeader confidence="0.997803" genericHeader="method">
3 Data Resources
</sectionHeader>
<subsectionHeader confidence="0.999979">
3.1 Preprocessing the Data
</subsectionHeader>
<bodyText confidence="0.999989739130435">
The data that was provided by the CoNLL organizers was
sentence-delimited and tokenized, and hand-annotated
with named entity chunks. The English data was au-
tomatically labeled with part-of-speech and chunk tags
from the memory-based tagger and chunker (Daelemans
et al., 1996), and the German data was labelled with the
decision-tree-based TreeTagger (Schmidt, 1994). We re-
placed the English part-of-speech tags with those gener-
ated by a transformation-based learner (Ngai and Florian,
2001). The chunk tags did not appear to help in either
case and were discarded.
As we did not want to overly rely on characteristics
which were specific to the Indo-European language fam-
ily, we did not perform detailed morphological analysis;
but instead, an approximation was made by simply ex-
tracting the prefixes and suffixes of up to 4 characters
from all the words.
In order to let the system generalize over word types,
we normalized the case information of all the words in
the corpus by converting them to uniform lower case. To
recapture the lost information, each word was annotated
with a tag that specified if it was in all lower case, all
upper case, or was of mixed case.
</bodyText>
<subsectionHeader confidence="0.995704">
3.2 Gazetteers
</subsectionHeader>
<bodyText confidence="0.997083677419355">
Apart from the training and test data, the CoNLL orga-
nizers also provided two lists of named entities, one in
English and one in Dutch. Part of the challenge for this
year’s shared task was to find ways of using this resource
in the system.
To supplement the provided gazetteers, a large col-
lection of names and words was downloaded from var-
ious web sources. This collection was used to compile
a gazetteer of 120k uncategorized English proper names
and a lexicon of 500k common English words. As there
were no supplied gazetteers for German, we also com-
piled a gazetteer of 8000 German names, which were
mostly personal first and last names and geographical lo-
cations, and a lexicon of 32k common German words.
Named entities in the corpus which appeared in the
gazetteers were identified lexically or using a maximum
forward match algorithm similar to that used in Chinese
word segmentation. Once named entities have been iden-
tified in this preprocessing step, each word can then be
annotated with an NE chunk tag corresponding to the out-
put from the system. The learner can view the NE chunk
tag as an additional feature.
The variations in this approach come from resolving
conflicts between different possible type information for
the same NE. The different ways that we dealt with the
problem were: (1) Rank all the NE types by frequency in
the training corpus. In the case of a conflict, default to
the more common NE. (2) Give all the possible NEs to
the boosting learner as a set of possible NE chunk tags.
(3) Discard the NE type information and annotate each
word with a tag indicating whether it is inside an NE.
</bodyText>
<sectionHeader confidence="0.998364" genericHeader="method">
4 Classifier Combination
</sectionHeader>
<bodyText confidence="0.999971">
It is a well-known fact that if several classifiers are avail-
able, they can be combined in various ways to create
a system that outperforms the best individual classifier.
Since we had several classifiers available to us, it was rea-
sonable to investigate combining them in different ways.
</bodyText>
<subsectionHeader confidence="0.994165">
4.1 Stacking
</subsectionHeader>
<bodyText confidence="0.999983">
Like voting, stacking is a learning paradigm that con-
structs a combined model from several classifiers. The
basic concept behind stacking is to train two or more clas-
sifiers sequentially, with each successive classifier incor-
porating the results of the previous ones in some fashion.
</bodyText>
<subsectionHeader confidence="0.807396">
4.1.1 Integration of External Resources
</subsectionHeader>
<bodyText confidence="0.99997725">
As mentioned above, at the most basic level, lexicon
and gazetteer information was integrated into our classi-
fiers by including them as additional features. However
we also experimented with several different ways of in-
corporating this information via stacking—one possible
approach was to view the gazetteers as a separate system
that would produce an output and then implement stack-
ing to combine their outputs.
</bodyText>
<subsubsectionHeader confidence="0.49624">
4.1.2 Division into Subtasks
</subsubsectionHeader>
<bodyText confidence="0.98815045">
One of the most straightforward approaches to stack-
ing can be applied to tasks that are naturally divisible into
hierarchically ordered subtasks. An example approach,
which was taken by several of the participating teams in
the CoNLL-2002 shared task, is to split the NER task into
the identification phase, where named entities are identi-
fied in the text; and the classification phase, where the
identified named entities are categorized into the various
subtypes. Provided that the performance of the individ-
ual classifier is fairly high (otherwise errors made in the
earlier stages could propagate down the chain), this has
the advantage of reducing the complexity of the task for
each individual classifier.
To construct such a system, we trained a stacked Ada-
Boost.MH classifier to perform NE reclassification on
boundaries identified in the base model. The output of the
initial models are postprocessed to remove all NE type
information and then passed to this stacked classifier. As
Table 1 shows, stacking the boosting models yields a sig-
nificant gain in performance.
</bodyText>
<table confidence="0.995157">
English devel. Precision Recall F0=1
(Boost) Base 88.64% 87.68% 88.16
(Boost) Base + Stacked 89.26% 88.29% 88.77
</table>
<tableCaption confidence="0.974558">
Table 1: Improving classification of NE types via stacked
AdaBoost.MH.
</tableCaption>
<subsubsectionHeader confidence="0.771048">
4.1.3 Error Correction
</subsubsectionHeader>
<bodyText confidence="0.999982894736842">
Another approach to stacking that we investigated in
this work involves a closer interaction between the mod-
els. The general overview of this approach is for a given
model to use the output of another trained model as its
initial state, and to improve beyond it. The idea is that
the second model, with a different learning and represen-
tation bias, will be able to move out of the local maxima
that the previous model has settled into.
To accomplish this we introduced Stacked TBL
(STBL), a variant of TBL tuned for this purpose (Wu et
al., 2003). We found TBL to be an appropriate point of
departure since it starts from an initial state of classifi-
cation and learns rules to iteratively correct the current
labeling. We aimed to use STBL to improve the base
model from the preceding section.
STBL proved quite effective; in fact it yielded the best
base model performance among all our models. Table 2
shows the result of stacking STBL on the boosting base
model.
</bodyText>
<subsectionHeader confidence="0.967313">
4.2 Voting
</subsectionHeader>
<bodyText confidence="0.999665">
The simplest approach to combining classifiers is through
voting, which examines the outputs of the various mod-
</bodyText>
<table confidence="0.988735333333333">
English devel. Precision Recall F0=1
(Boost) Base 88.64% 87.68% 88.16
(Boost + STBL) Base 87.83% 88.79% 88.31
</table>
<tableCaption confidence="0.8911015">
Table 2: Improving the above AdaBoost.MH base model,
via Stacked TBL (STBL).
</tableCaption>
<bodyText confidence="0.999501947368421">
els and selects the classifications which have a weight
exceeding some threshold, where the weight is depen-
dent upon the models that proposed this particular clas-
sification. The variations in this approach stem from the
method by which weights are attached to the models. It
is possible to assign varying weights to the models, in ef-
fect giving one model more “say” than the others. In our
system, however, we simply assigned each model equal
weight, and selected classifications which were proposed
by a majority of models.
Voting was thus used to further improve the base
model. Four models chosen for heterogeneity partici-
pated in the voting: two variants of the AdaBoost.MH
model, the SVM model, and the Stacked TBL model.
As before, the stacked AdaBoost.MH reclassifier was
applied to the voted result, yielding a final stacked voted
stacked model.
This model gave the best overall results on the task as
a whole. Table 3 shows the results of our system.
</bodyText>
<table confidence="0.998717166666667">
English devel. Precision Recall F0=1
Boost1 + Stacked 89.26% 88.29% 88.77
Boost2 + Stacked 82.98% 85.62% 84.28
SVM + Stacked 84.41% 85.71% 85.05
Boost + STBL + Stacked 89.09% 88.07% 88.57
Voted + Stacked 90.18% 88.86% 89.51
</table>
<tableCaption confidence="0.977538">
Table 3: Improving classification of NE types, via stacked
voted stacked AdaBoost.MH, STBL, and SVM models.
</tableCaption>
<sectionHeader confidence="0.962308" genericHeader="method">
5 Overall Results
</sectionHeader>
<bodyText confidence="0.993951">
Complete results on the development and test sets, for
both English and German, are shown in Table 4.
</bodyText>
<sectionHeader confidence="0.997195" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.985788729166667">
This paper has presented an overview of our entry to
the CoNLL-2003 shared task. As individual component
models, we constructed strong AdaBoost.MH models,
SVM models, and Stacked TBL models, and provided
them with detailed features on the data.
We then demonstrated several stacking and voting
models that proved capable of improving performance
further. This was non-trivial since the individual compo-
nent models were all quite strong to begin with. Because
of this the vast majority of classifier combination models
we tested actually turned out to degrade performance, or
showed zero improvement. The models presented here
worked well because they were each motivated by de-
tailed analyses.
We did investigate a number of ways in which
gazetteers could be incorporated. The gazetteer supplied
for the shared task was found not to improve perfor-
mance significantly, because our models were already ad-
equately powerful to correctly identify most of the named
entities supplied by the gazetteer. However, minimal ef-
fort to augment the gazetteers did result in a performance
boost. Moreover, performance was further improved by
the inclusion of a common word lexicon not containing
any named entities.
Inspection revealed that some errors found in the
output of the system stemmed from either erroneous
sentence boundaries in the test data, or difficult-to-avoid
inconsistencies in the the gold standard annotations. For
example, in the following:
1.... [ Panamanian ]MISC boxing legend ...
2.... [ U.S. ]LOC collegiate national champion ...
both “Panamanian” and “U.S.” are used as modifiers, but
one is annotated as a MISC-type NE while the other is
considered a LOC-type.
The stacked voted stacked model obtained an improve-
ment of 4.83 F-Measure points on the English devel-
opment set over our best model from the CoNLL-2002
shared task which we took as our baseline, resulting in
a substantial 19.7% error rate reduction. The system
achieves this respectable performance using very little
in the way of outside resources—only a part-of-speech
tagger and some common wordlists—which can be ob-
tained easily for almost any major language. Most fea-
tures we used can also be used for uninflected and non-
Indo-European languages such as Chinese, where the pre-
fixes and suffixes can be replaced by decomposing the
words at the character level. This is in keeping with the
the language-independent spirit of the shared task.
</bodyText>
<sectionHeader confidence="0.999176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993188533333333">
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. 1992. A training al-
gorithm for optimal margin classifiers. In David Haussler, editor, Proceedings
of the 4th Workshop on Computational Learning Theory, pages 144–152, San
Mateo, CA. ACM Press.
Eric Brill. 1995. Transformation-based error-driven learning and natural lan-
guage processing: A case study in part of speech tagging. Computational
Linguistics, 21(4):543–565.
Xavier Carreras, Llu´is M`arquez, and Llu´is Padr´o. 2002. Named entity extrac-
tion using adaboost. In Proceedings of CoNLL-2002, pages 167–170. Taipei,
Taiwan.
Walter Daelemans, Jakub Zavrel, and Peter Berck. 1996. MBT: A memory-based
part of speech tagger-generator.
Gerard Escudero, Llu´is M`arquez, and German Rigau. 2000. Boosting applied to
word sense disambiguation. In European Conference on Machine Learning,
pages 129–141.
</reference>
<table confidence="0.998931166666667">
English devel. Precision Recall F0=1
LOC 91.88% 92.98% 92.42
MISC 91.53% 83.19% 87.16
ORG 86.43% 80.76% 83.50
PER 90.39% 93.49% 91.91
Overall 90.18% 88.86% 89.51
English test Precision Recall F0=1
LOC 86.27% 85.91% 86.09
MISC 75.88% 77.07% 76.47
ORG 78.19% 72.97% 75.49
PER 83.94% 87.26% 85.57
Overall 82.02% 81.39% 81.70
German devel. Precision Recall F0=1
LOC 75.13% 61.39% 67.57
MISC 75.46% 45.05% 56.42
ORG 80.05% 47.86% 59.91
PER 74.19% 61.96% 67.52
Overall 75.92% 54.67% 63.56
German test Precision Recall F0=1
LOC 74.94% 60.97% 67.23
MISC 72.77% 51.04% 60.00
ORG 61.22% 42.69% 50.30
PER 83.68% 73.39% 78.20
Overall 75.20% 59.35% 66.34
</table>
<tableCaption confidence="0.975602">
Table 4: Overall results of stacked voted stacked model
on development and test sets.
</tableCaption>
<reference confidence="0.998377482758621">
Yoav Freund and Rob E. Schapire. 1997. A decision-theoretic generalization of
on-line learning and an application to boosting. In Journal of Computer and
System Sciences, 55(1), pages 119–139.
Thorsten Joachims. 1998. Text categorization with support vector machines:
learning with many relevant features. In Claire N´edellec and C´eline Rou-
veirol, editors, Proceedings of ECML-98, 10th European Conference on Ma-
chine Learning, number 1398, pages 137–142, Chemnitz, DE. Springer Ver-
lag, Heidelberg, DE.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings ofNAACL-2001.
Paul McNamee and James Mayfield. 2002. Entity extraction without language-
specific resources. In Proceedings of CoNLL-2002, pages 183–186. Taipei,
Taiwan.
Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast
lane. In Proceedings ofNAACL’01, pages 40–47, Pittsburgh, PA. ACL.
Manabu Sassano and Takehito Utsuro. 2000. Named entity chunking techniques
in supervised learning for Japanese named entity recognition. In Proceedings
of COLING-2000, pages 705–711.
Rob E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system
for text categorization. In Machine Learning, 39(2/3, pages 135–168.
Helmut Schmidt. 1994. Probabilistic part-of-speech tagging using decision trees.
In International Conference on New Methods in Natural Language Process-
ing, pages 44–49, Manchester, U.K.
Beth Sundheim. 1995. MUC6 named entity task defnition, version 2.1. In Pro-
ceedings of the Sixth Message Understanding Conference (MUC6).
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang.
2002. Boosting for named entity recognition. In Proceedings ofCoNLL-2002,
pages 195–198. Taipei, Taiwan.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. Forthcoming.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.068608">
<title confidence="0.9953">A Stacked, Voted, Stacked Model for Named Entity Recognition</title>
<affiliation confidence="0.961147333333333">Language Technology Department of Computer University of Science and</affiliation>
<address confidence="0.821738">Clear Water Bay, Hong</address>
<author confidence="0.858476">Kong Polytechnic</author>
<affiliation confidence="0.885036">Department of</affiliation>
<address confidence="0.278665">Hong</address>
<email confidence="0.877538">csgngai@polyu.edu.hk</email>
<abstract confidence="0.893812272727273">This paper investigates stacking and voting methods for combining strong classifiers like boosting, SVM, and TBL, on the named-entity recognition task. We demonstrate several effective approaches, culminating in a model that achieves error rate reductions on the development and test sets of 63.6% and 55.0% (English) and 47.0% and 51.7% (German) over the CoNLL-2003 standard baseline respectively, and 19.7% over a strong AdaBoost baseline</abstract>
<note confidence="0.857878">model from CoNLL-2002.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernhard E Boser</author>
<author>Isabelle Guyon</author>
<author>Vladimir Vapnik</author>
</authors>
<title>A training algorithm for optimal margin classifiers. In</title>
<date>1992</date>
<booktitle>Proceedings of the 4th Workshop on Computational Learning Theory,</booktitle>
<pages>144--152</pages>
<editor>David Haussler, editor,</editor>
<publisher>ACM Press.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="3679" citStr="Boser et al., 1992" startWordPosition="560" endWordPosition="563">s AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. It performs well on a number of natural language processing problems, including text categorization (Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transforma</context>
</contexts>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. 1992. A training algorithm for optimal margin classifiers. In David Haussler, editor, Proceedings of the 4th Workshop on Computational Learning Theory, pages 144–152, San Mateo, CA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4127" citStr="Brill, 1995" startWordPosition="622" endWordPosition="623">, 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning t</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´is M`arquez</author>
<author>Llu´is Padr´o</author>
</authors>
<title>Named entity extraction using adaboost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>167--170</pages>
<location>Taipei, Taiwan.</location>
<marker>Carreras, M`arquez, Padr´o, 2002</marker>
<rawString>Xavier Carreras, Llu´is M`arquez, and Llu´is Padr´o. 2002. Named entity extraction using adaboost. In Proceedings of CoNLL-2002, pages 167–170. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
</authors>
<title>MBT: A memory-based part of speech tagger-generator.</title>
<date>1996</date>
<contexts>
<context position="5101" citStr="Daelemans et al., 1996" startWordPosition="772" endWordPosition="775">es are greedily learned to correct the mistakes, until no net improvement can be made. The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. 3 Data Resources 3.1 Preprocessing the Data The data that was provided by the CoNLL organizers was sentence-delimited and tokenized, and hand-annotated with named entity chunks. The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al., 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). The chunk tags did not appear to help in either case and were discarded. As we did not want to overly rely on characteristics which were specific to the Indo-European language family, we did not perform detailed morphological analysis; but instead, an approximation was made by simply extracting the prefixes and suffixes of up to 4 characters from all the words. In order to let the</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, and Peter Berck. 1996. MBT: A memory-based part of speech tagger-generator.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Escudero</author>
<author>Llu´is M`arquez</author>
<author>German Rigau</author>
</authors>
<title>Boosting applied to word sense disambiguation.</title>
<date>2000</date>
<booktitle>In European Conference on Machine Learning,</booktitle>
<pages>129--141</pages>
<marker>Escudero, M`arquez, Rigau, 2000</marker>
<rawString>Gerard Escudero, Llu´is M`arquez, and German Rigau. 2000. Boosting applied to word sense disambiguation. In European Conference on Machine Learning, pages 129–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Rob E Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and an application to boosting.</title>
<date>1997</date>
<journal>In Journal of Computer and System Sciences,</journal>
<volume>55</volume>
<issue>1</issue>
<pages>119--139</pages>
<contexts>
<context position="3101" citStr="Freund and Schapire, 1997" startWordPosition="472" endWordPosition="475">not be easily obtainable for almost any major language. 2 Classification Methods To carry out the stacking and voting experiments, we constructed a number of relatively strong individual component models of the following kinds. 2.1 Boosting The main idea behind boosting algorithms is that a set of many weak classifiers can be effectively combined to yield a single strong classifier. Each weak classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. For the boosting framework, our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. It performs well on a number of natural language processing problems, including text categorization (Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in deal</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Yoav Freund and Rob E. Schapire. 1997. A decision-theoretic generalization of on-line learning and an application to boosting. In Journal of Computer and System Sciences, 55(1), pages 119–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398,</booktitle>
<pages>137--142</pages>
<publisher>Springer Verlag,</publisher>
<location>Chemnitz, DE.</location>
<contexts>
<context position="3827" citStr="Joachims, 1998" startWordPosition="582" endWordPosition="583">tural language processing problems, including text categorization (Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignme</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: learning with many relevant features. In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398, pages 137–142, Chemnitz, DE. Springer Verlag, Heidelberg, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL-2001.</booktitle>
<contexts>
<context position="3904" citStr="Kudo and Matsumoto, 2001" startWordPosition="592" endWordPosition="595">Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings ofNAACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>James Mayfield</author>
</authors>
<title>Entity extraction without languagespecific resources.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>183--186</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3988" citStr="McNamee and Mayfield, 2002" startWordPosition="604" endWordPosition="607">In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. The experiments present</context>
</contexts>
<marker>McNamee, Mayfield, 2002</marker>
<rawString>Paul McNamee and James Mayfield. 2002. Entity extraction without languagespecific resources. In Proceedings of CoNLL-2002, pages 183–186. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL’01,</booktitle>
<pages>40--47</pages>
<publisher>ACL.</publisher>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4668" citStr="Ngai and Florian, 2001" startWordPosition="708" endWordPosition="711">ces. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. 3 Data Resources 3.1 Preprocessing the Data The data that was provided by the CoNLL organizers was sentence-delimited and tokenized, and hand-annotated with named entity chunks. The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al., 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). We replaced the English part-of-speech tags with those generated by a trans</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast lane. In Proceedings ofNAACL’01, pages 40–47, Pittsburgh, PA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
<author>Takehito Utsuro</author>
</authors>
<title>Named entity chunking techniques in supervised learning for Japanese named entity recognition.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000,</booktitle>
<pages>705--711</pages>
<contexts>
<context position="3959" citStr="Sassano and Utsuro, 2000" startWordPosition="600" endWordPosition="603"> (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be </context>
</contexts>
<marker>Sassano, Utsuro, 2000</marker>
<rawString>Manabu Sassano and Takehito Utsuro. 2000. Named entity chunking techniques in supervised learning for Japanese named entity recognition. In Proceedings of COLING-2000, pages 705–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>In Machine Learning,</booktitle>
<volume>39</volume>
<issue>2</issue>
<pages>135--168</pages>
<contexts>
<context position="3305" citStr="Schapire and Singer, 2000" startWordPosition="502" endWordPosition="505">of the following kinds. 2.1 Boosting The main idea behind boosting algorithms is that a set of many weak classifiers can be effectively combined to yield a single strong classifier. Each weak classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. For the boosting framework, our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. It performs well on a number of natural language processing problems, including text categorization (Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) </context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Rob E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. In Machine Learning, 39(2/3, pages 135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmidt</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Natural Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, U.K.</location>
<contexts>
<context position="5191" citStr="Schmidt, 1994" startWordPosition="786" endWordPosition="787">nts presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. 3 Data Resources 3.1 Preprocessing the Data The data that was provided by the CoNLL organizers was sentence-delimited and tokenized, and hand-annotated with named entity chunks. The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al., 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). The chunk tags did not appear to help in either case and were discarded. As we did not want to overly rely on characteristics which were specific to the Indo-European language family, we did not perform detailed morphological analysis; but instead, an approximation was made by simply extracting the prefixes and suffixes of up to 4 characters from all the words. In order to let the system generalize over word types, we normalized the case information of all the words in</context>
</contexts>
<marker>Schmidt, 1994</marker>
<rawString>Helmut Schmidt. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Natural Language Processing, pages 44–49, Manchester, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Sundheim</author>
</authors>
<title>MUC6 named entity task defnition, version 2.1.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC6).</booktitle>
<contexts>
<context position="1294" citStr="Sundheim, 1995" startWordPosition="183" endWordPosition="185">5.0% (English) and 47.0% and 51.7% (German) over the CoNLL-2003 standard baseline respectively, and 19.7% over a strong AdaBoost baseline model from CoNLL-2002. 1 Introduction We describe multiple stacking and voting methods that effectively combine strong classifiers such as boosting, SVM, and TBL, for the named-entity recognition (NER) task. NER has emerged as an important step for many natural language applications, including machine translation, information retrieval and information extraction. Much of the research in this field was pioneered in the Message Understanding Conference (MUC) (Sundheim, 1995), which performed detailed entity extraction and identification on English documents. As a result, most current NER systems which have impressive performances have been specially constructed and tuned for English MUC-style documents. It is unclear how well they would perform when applied to another language. Our system was designed for the CoNLL-2003 shared task, the goal of which is to identify and classify four types of named entities: PERSON, LOCATION, ORGANIZATION and MISCELLANEOUS. The task specifications were that two languages would be involved. We *The author would like to thank the Ho</context>
</contexts>
<marker>Sundheim, 1995</marker>
<rawString>Beth Sundheim. 1995. MUC6 named entity task defnition, version 2.1. In Proceedings of the Sixth Message Understanding Conference (MUC6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Grace Ngai</author>
<author>Marine Carpuat</author>
<author>Jeppe Larsen</author>
<author>Yongsheng Yang</author>
</authors>
<title>Boosting for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings ofCoNLL-2002,</booktitle>
<pages>195--198</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3521" citStr="Wu et al., 2002" startWordPosition="535" endWordPosition="538"> increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. For the boosting framework, our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. It performs well on a number of natural language processing problems, including text categorization (Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill,</context>
</contexts>
<marker>Wu, Ngai, Carpuat, Larsen, Yang, 2002</marker>
<rawString>Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang. 2002. Boosting for named entity recognition. In Proceedings ofCoNLL-2002, pages 195–198. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Grace Ngai</author>
<author>Marine Carpuat</author>
</authors>
<date>2003</date>
<note>Forthcoming.</note>
<contexts>
<context position="10560" citStr="Wu et al., 2003" startWordPosition="1681" endWordPosition="1684">ification of NE types via stacked AdaBoost.MH. 4.1.3 Error Correction Another approach to stacking that we investigated in this work involves a closer interaction between the models. The general overview of this approach is for a given model to use the output of another trained model as its initial state, and to improve beyond it. The idea is that the second model, with a different learning and representation bias, will be able to move out of the local maxima that the previous model has settled into. To accomplish this we introduced Stacked TBL (STBL), a variant of TBL tuned for this purpose (Wu et al., 2003). We found TBL to be an appropriate point of departure since it starts from an initial state of classification and learns rules to iteratively correct the current labeling. We aimed to use STBL to improve the base model from the preceding section. STBL proved quite effective; in fact it yielded the best base model performance among all our models. Table 2 shows the result of stacking STBL on the boosting base model. 4.2 Voting The simplest approach to combining classifiers is through voting, which examines the outputs of the various modEnglish devel. Precision Recall F0=1 (Boost) Base 88.64% 8</context>
</contexts>
<marker>Wu, Ngai, Carpuat, 2003</marker>
<rawString>Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. Forthcoming.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>