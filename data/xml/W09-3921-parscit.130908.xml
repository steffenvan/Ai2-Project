<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028380">
<title confidence="0.977885">
Concept Form Adaptation in Human-Computer Dialog
</title>
<author confidence="0.982432">
Svetlana Stoyanchev and Amanda Stent
</author>
<affiliation confidence="0.968101">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.907222">
Stony Brook, NY 11794-4400, USA
</address>
<email confidence="0.999274">
svetastenchikova@gmail.com, amanda.stent@gmail.com
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999842416666667">
In this work we examine user adaptation
to a dialog system’s choice of realiza-
tion of task-related concepts. We ana-
lyze forms of the time concept in the Let’s
Go! spoken dialog system. We find that
users adapt to the system’s choice of time
form. We also find that user adaptation
is affected by perceived system adapta-
tion. This means that dialog systems can
guide users’ word choice and can adapt
their own recognition models to gain im-
proved ASR accuracy.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999893105263158">
Considerable research has now demonstrated that
human dialog partners exhibit lexical and syntac-
tic convergence; that is, that in a human-human
conversation the participants become more simi-
lar in their use of language over time (Brennan
and Clark, 1996; Lockridge and Brennan, 2002;
Pickering and others, 2000; Reitter et al., 2006).
Several Wizard-of-Oz studies have also shown ev-
idence of convergence in human-computer dia-
log (Branigan and others, 2003; Brennan, 1996;
Gustafson and others, 1997).
In recent work, we examined user adaptation1
to the system’s choice of verb and preposition us-
ing the deployed Let’s Go! spoken dialog system
(Stoyanchev and Stent, 2009a). This was the first
study to look at convergence with real users of a
real dialog system and examined user adaptation
to verbs and prepositions. The study described
in this paper is a follow-on to our previous study.
</bodyText>
<footnote confidence="0.99481">
1In this paper, we use the term adaptation to indicate di-
rectional convergence, e.g. user adaptation to a system. We
make no claims about the psycholinguistic models underly-
ing this adaptation.
</footnote>
<bodyText confidence="0.999969533333334">
Here we look at user adaptation to the system’s
choice of realization of task-related concepts. In
this paper, we: (1) Confirm our previous results
showing that users adapt to the system’s choice of
words, using transcribed data rather than ASR out-
put; (2) Show that adaptation can persist over time;
and (3) Show that user adaptation is strengthened
by perceived system adaptation.
In addition to providing further evidence of con-
vergence in human-computer dialog, our results
have implications for dialog system design. Cur-
rently, much dialog systems research is devoted to
improving ASR accuracy, because this is a signifi-
cant contributor to task success rates and to dialog
length. One way to improve ASR accuracy is to
use targeted language models. Since users adapt to
the system’s choices of realization for task-related
concepts, we can predict the user’s choice of real-
ization and use this to adjust the ASR’s language
model, improving ASR accuracy specifically on
concept words. Another way to improve ASR ac-
curacy is to guide the user into using words that are
likely to be recognized correctly (Hockey and oth-
ers, 2003; Sheeder and Balogh, 2003; Tomko and
Rosenfeld, 2006). Our results imply that if the de-
signer of a dialog system wants to improve ASR
accuracy, system prompts should be designed to
use word choices that are more recognizable; and,
when, possible, to be adaptive to the user’s choice
of form for task-related concepts.
</bodyText>
<sectionHeader confidence="0.994424" genericHeader="introduction">
2 System
</sectionHeader>
<bodyText confidence="0.999783166666667">
We conducted our experiment using the Let’s Go!
deployed telephone-based spoken dialog system
which provides information about bus routes in
Pittsburgh (Raux and others, 2005). Let’s Go! an-
swers the phones at the transit authority’s website
outside of normal business hours. Its users are
</bodyText>
<subsubsectionHeader confidence="0.642132">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 144–147,
</subsubsectionHeader>
<affiliation confidence="0.93824">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999099">
144
</page>
<bodyText confidence="0.5824435">
Welcome to the CMU Let’s Go! Bus Information
System. How may I help you?
I’d like to go to Forbes and Murray.
To Forbes and Murray. Where are you leaving
</bodyText>
<figure confidence="0.848161">
from?
Carnegie Mellon.
From Carnegie Mellon. What time would you like
to leave?
Ten thirty p.m.
Leaving at ten thirty p. m.?
Yes
There are five buses running between Carnegie
Mellon and Forbes and Murray...
</figure>
<figureCaption confidence="0.970235">
Figure 1: Sample dialog with Let’s Go!
</figureCaption>
<table confidence="0.993866111111111">
Time form Example realizations Frequency
TIME four, five, six thirty... 31.1%
APM four a.m., ten p.m., one 43.5%
fifteen p. m.
POD four in the morn- 4.6%
ing/evening/afternoon/
OCLOCK five o’clock 16%
OTHER four o’clock p. m., sixteen 4.8%
hundred hours
</table>
<tableCaption confidence="0.700256666666667">
Table 1: Time forms in users’ utterances and their
relative frequencies in one month of Let’s Go!
2006 data.
</tableCaption>
<bodyText confidence="0.999863857142857">
naive callers from the general population with a
real task to perform. In order to provide bus route
information, Let’s Go! elicits values for several
task-related concepts: an optional bus route num-
ber, a departure place, a destination and a desired
travel time. Each concept is explicitly confirmed.
Figure 1 shows a sample dialog with the system.
In this work we investigate adaptation to the
time concept because it has multiple different re-
alizations, as shown in Table 1. This variability
is not unique to time; however, it is the only task-
related concept in Let’s Go! that is not usually
realized using named entities (which exhibit less
variability).
</bodyText>
<sectionHeader confidence="0.980807" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999961567567567">
In order to study adaptation, we need to identify a
prime, a point in the conversation where one part-
ner introduces a realization. In Let’s Go! the sys-
tem always asks the user to specify a departure
time. The user then typically says a time, which
the system confirms (see Figure 1). We simulate
an ASR error on the user’s response to the sys-
tem’s time request, so that when the system con-
firms the departure time it confirms a time other
than that recognized in the user’s response. To
make the system’s error more realistic, the time
in the simulated error is a time that is phonetically
close to the time (hour and minute) recognized in
the user’s response. The system’s confirmation
prompt is our prime.
The system runs in one of the three condi-
tions: SYS TIME, SYS APM, or SYS POD. In
each condition it uses the corresponding time for-
mat (TIME, APM, or POD as shown in Table 1).
TIME is the most frequent form in the 2006 Let’s
Go! corpus, but it is potentially ambiguous as it
can mean either night or day. APM is the shortest
unambiguous form. POD is longer and has a very
low frequency in the 2006 Let’s Go! corpus.2
We collected approximately 2000 dialogs with
Let’s Go! using this setup. We used the ASR
output to identify dialogs where a time appears
in the ASR output at least twice3. We manually
transcribed 50 dialogs for each experimental con-
dition. Some of these turned out not to contain
mentions of time either before or after the system’s
time confirmation prompt, so we excluded them.
We examine whether the user adapts to the
system’s choice of form for realizing the time
concept, both in the first time-containing post-
confirmation utterance, and in the rest of the dialog
(until the user hangs up or says “New query”).
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9961925">
In this section we first examine user adaptation to
system’s choice of time expression, and then look
at how perceived system adaptation affects user
adaptation.
</bodyText>
<subsectionHeader confidence="0.954666">
4.1 User adaptation to system time form
</subsectionHeader>
<bodyText confidence="0.999122307692308">
If the user adapts to the system’s time form, then
we would expect to see a greater proportion of the
system’s time form in user utterances following
the prime. We compare the proportion of three
time forms (APM, TIME, and POD) in each sys-
tem condition for 1) Unprimed, 2) First After, and
3) All After user’s utterances, as shown in Table 2.
Unprimed utterances are the user’s time specifica-
tion immediately prior to the prime (the system’s
confirmation prompt). First After utterances are
user utterances immediately following the prime.
All After utterances are all user utterances from the
prime until the user either hangs up or says “New
</bodyText>
<footnote confidence="0.997628666666667">
2We would have liked to also include OCLOCK in the
experiment. However, due to resource limitations we had to
choose only three conditions.
3The most frequent user response to the system’s request
to specify a departure time is “Now”; we exclude these from
our experiment.
</footnote>
<figure confidence="0.752211222222222">
SYS:
USR:
SYS:
USR:
SYS:
USR:
SYS:
USR:
SYS:
</figure>
<page confidence="0.840248">
145
</page>
<table confidence="0.999498866666667">
Unprimed
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 25% 42% 8%
SYS TIME 30% 52% 2%
SYS POD 24% 49% 4%
First After
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 49% 29%4 2%
SYS TIME 21% 46 58% 0%
SYS POD 29% 45% 5%
All After
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 63% 19% 46 3%
SYS TIME 21% 46 50% 2%
SYS POD 37% 46 38% 4%
</table>
<tableCaption confidence="0.994335">
Table 2: Proportions of time forms in different
</tableCaption>
<bodyText confidence="0.995727972222222">
system prompt conditions. The highest propor-
tion among system conditions for each time form
is highlighted. Occurrences of time forms other
than the three examined time forms are excluded
from this table. 4 indicates a statistically signif-
icant difference from the highlighted value in the
column (p &lt; .05 with Bonferroni adjustment). 46
indicates a statistically significant difference from
the highlighted value in the column (p &lt; .01 with
Bonferroni adjustment).
query”. To test the statistical significance of our
results we perform inference on proportions for a
large sample.
APM There are no statistically significant differ-
ences in the proportions of Usr:APM4 forms in
Unprimed utterances for the different system con-
ditions. The proportion of Usr:APM forms in
First After utterances is significantly higher in the
SYS APM condition than in the SYS TIME con-
dition (p &lt; .01), although not significantly dif-
ferent than in the SYS POD condition. The pro-
portion of Usr:APM forms in the All After ut-
terances is significantly higher in the SYS APM
condition than in both the SYS TIME and the
SYS POD conditions (p &lt; .01). We conclude that
there is user adaptation to system time form in the
SYS APM condition.
TIME There are no statistically significant dif-
ferences in the proportions of Usr:TIME forms in
Unprimed utterances for the different system con-
ditions. The proportions of Usr:TIME forms in the
First After utterances in the SYS TIME condition
is significantly higher than that in the SYS APM
condition (p &lt; .01), but not significantly higher
than that in the SYS POD condition. The same
is true of Usr:TIME forms in the All After utter-
</bodyText>
<footnote confidence="0.9087655">
4Usr:time-form refers to the occurrence of the time-form
in a user’s utterance.
</footnote>
<table confidence="0.999271333333333">
condition keep adapt switch total
adaptive 81.8% - 18.2% 33
non-adaptive 37.5% 29.1% 35.4% 48
</table>
<tableCaption confidence="0.986588">
Table 3: Proportions of user actions in First After
confirmation utterances
</tableCaption>
<bodyText confidence="0.997766583333333">
ances. We conclude that there is user adaptation to
system time form in the SYS TIME condition.
POD We did not find statistically significant dif-
ferences in Usr:POD forms for the different sys-
tem conditions in either the Unprimed, First After
or All After data. Because this is the long unam-
biguous form, users may have felt that it would
not be recognized or that it would be inefficient to
produce it.
Figures 2 illustrates the effect of user adaptation
on time form for the SYS APM and SYS TIME
conditions.
</bodyText>
<subsectionHeader confidence="0.837946">
4.2 The effect of system adaptation on user
adaptation
</subsectionHeader>
<bodyText confidence="0.9999166">
Sometimes the user happens to use the same form
in their initial specification of time that the system
uses in its confirmation prompt. This gives the il-
lusion that the system is adapting its choice of time
form to the user. We examined whether users’ per-
ception of system adaptation affected user adapta-
tion in First After confirmation utterances.
For this analysis we used only the dialogs in
the SYS APM and SYS TIME conditions since
the POD form is rare in the Unprimed utterances.
We distinguish between three possible user actions
following the system’s confirmation prompt: 1)
keep - use the same form as in the unprimed ut-
terance; 2) adapt – switch to the same form as in
the system’s confirmation prompt; and 3) switch -
switch to a different form than the one used in the
system’s confirmation prompt or in the unprimed
utterance.
Table 3 shows the proportions for each possible
user action. In the adaptive condition users are
twice as likely to keep the time form than in the
non-adaptive condition (81.8% vs. 37.5%). This
difference is statistically significant (p &lt; .001).
In the non-adaptive system condition users who
change time form are slightly more likely to switch
(35.4%) than to adapt (29.1%).
These results suggest that when the system does
not adapt to the user, the user’s choice is unpre-
dictable. However, if the system adapts to the
user, the user is likely to keep the same form. This
</bodyText>
<page confidence="0.99774">
146
</page>
<figureCaption confidence="0.998303">
Figure 2: User Utterances with TIME APM and TIME ONLY.
</figureCaption>
<bodyText confidence="0.9994736">
means that if the system can adapt to the user when
the user chooses a form that is more likely to be
recognized correctly, that provides positive rein-
forcement, making the user more likely to use that
felicitous form in the future. Furthermore, if the
system does adapt to the user then it may be pos-
sible with high accuracy to predict the user’s form
for subsequent utterances, and to use this infor-
mation to improve ASR accuracy for subsequent
utterances (Stoyanchev and Stent, 2009b).
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999962909090909">
In this paper, we analyzed user adaptation to a dia-
log system’s choice of task-related concept forms.
We showed that users do adapt to the system’s
word choices, and that users are more likely to
adapt when the system appears to adapt to them.
This information may help us guide users into
more felicitous word choices, and/or modify the
system to better recognize anticipated user word
choices. In future work we plan to analyze the
effect of ASR adaptation to user word choice on
speech recognition performance in spoken dialog.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999356853658536">
H. Branigan et al. 2003. Syntactic alignment between
computers and people: The role of belief about men-
tal states. In Proceedings of the 25th Annual Confer-
ence of the Cognitive Science Society.
S. Brennan and H. Clark. 1996. Conceptual pacts and
lexical choice in conversation. Journal of Experi-
mental Psychology, 22(6):1482–1493.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of ISSD, pages 41–44.
J. Gustafson et al. 1997. How do system questions
influence lexical choices in user answers? In Pro-
ceedings of Eurospeech.
B. Hockey et al. 2003. Targeted help for spoken dia-
logue systems: intelligent feedback improves naive
users performance. In Proceedings of EACL.
C. Lockridge and S. Brennan. 2002. Addressees’
needs influence speakers’ early syntactic choices.
Psychonomics Bulletin and Review, 9:550–557.
M. Pickering et al. 2000. Activation of syntactic prim-
ing during language production. Journal of Psy-
cholinguistic Research, 29(2):205–216.
A. Raux et al. 2005. Let’s go public! taking a spoken
dialog system to the real world. In Proceedings of
Eurospeech.
E. Reitter, J. Moore, and F. Keller. 2006. Priming of
syntactic rules in task-oriented dialogue and sponta-
neous conversation. In Proceedings of CogSci.
T. Sheeder and J. Balogh. 2003. Say it like you mean
it: Priming for structure in caller responses to a spo-
ken dialog system. International Journal of Speech
and Technology, 6:103–111.
S. Stoyanchev and A. Stent. 2009a. Lexical and syn-
tactic priming and their impact in deployed spoken
dialog systems. In Proceedings of NAACL.
S. Stoyanchev and A. Stent. 2009b. Predicting concept
types in user corrections in dialog. In Proceedings of
the EACL Workshop on the Semantic Representation
of Spoken Language.
S. Tomko and R. Rosenfeld. 2006. Shaping user input
in speech graffiti: a first pass. In Proceedings of
CHI.
</reference>
<page confidence="0.998087">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.152014">
<title confidence="0.992565">Concept Form Adaptation in Human-Computer Dialog</title>
<author confidence="0.688231">Stoyanchev</author>
<affiliation confidence="0.842542">Department of Computer</affiliation>
<author confidence="0.5136305">Stony Brook Stony Brook</author>
<author confidence="0.5136305">NY</author>
<email confidence="0.998733">svetastenchikova@gmail.com,amanda.stent@gmail.com</email>
<abstract confidence="0.968343538461539">In this work we examine user adaptation to a dialog system’s choice of realization of task-related concepts. We anaforms of the time concept in the dialog system. We find that users adapt to the system’s choice of time form. We also find that user adaptation is affected by perceived system adaptation. This means that dialog systems can guide users’ word choice and can adapt their own recognition models to gain improved ASR accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Branigan</author>
</authors>
<title>Syntactic alignment between computers and people: The role of belief about mental states.</title>
<date>2003</date>
<booktitle>In Proceedings of the 25th Annual Conference of the Cognitive Science Society.</booktitle>
<marker>Branigan, 2003</marker>
<rawString>H. Branigan et al. 2003. Syntactic alignment between computers and people: The role of belief about mental states. In Proceedings of the 25th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brennan</author>
<author>H Clark</author>
</authors>
<title>Conceptual pacts and lexical choice in conversation.</title>
<date>1996</date>
<journal>Journal of Experimental Psychology,</journal>
<volume>22</volume>
<issue>6</issue>
<contexts>
<context position="959" citStr="Brennan and Clark, 1996" startWordPosition="144" endWordPosition="147">pts. We analyze forms of the time concept in the Let’s Go! spoken dialog system. We find that users adapt to the system’s choice of time form. We also find that user adaptation is affected by perceived system adaptation. This means that dialog systems can guide users’ word choice and can adapt their own recognition models to gain improved ASR accuracy. 1 Introduction Considerable research has now demonstrated that human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper i</context>
</contexts>
<marker>Brennan, Clark, 1996</marker>
<rawString>S. Brennan and H. Clark. 1996. Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology, 22(6):1482–1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brennan</author>
</authors>
<title>Lexical entrainment in spontaneous dialog.</title>
<date>1996</date>
<booktitle>In Proceedings of ISSD,</booktitle>
<pages>41--44</pages>
<contexts>
<context position="1176" citStr="Brennan, 1996" startWordPosition="178" endWordPosition="179">eans that dialog systems can guide users’ word choice and can adapt their own recognition models to gain improved ASR accuracy. 1 Introduction Considerable research has now demonstrated that human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper is a follow-on to our previous study. 1In this paper, we use the term adaptation to indicate directional convergence, e.g. user adaptation to a system. We make no claims about the psycholinguistic models underlying thi</context>
</contexts>
<marker>Brennan, 1996</marker>
<rawString>S. Brennan. 1996. Lexical entrainment in spontaneous dialog. In Proceedings of ISSD, pages 41–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gustafson</author>
</authors>
<title>How do system questions influence lexical choices in user answers?</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<marker>Gustafson, 1997</marker>
<rawString>J. Gustafson et al. 1997. How do system questions influence lexical choices in user answers? In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hockey</author>
</authors>
<title>Targeted help for spoken dialogue systems: intelligent feedback improves naive users performance.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL.</booktitle>
<marker>Hockey, 2003</marker>
<rawString>B. Hockey et al. 2003. Targeted help for spoken dialogue systems: intelligent feedback improves naive users performance. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lockridge</author>
<author>S Brennan</author>
</authors>
<title>Addressees’ needs influence speakers’ early syntactic choices. Psychonomics Bulletin and Review,</title>
<date>2002</date>
<pages>9--550</pages>
<contexts>
<context position="988" citStr="Lockridge and Brennan, 2002" startWordPosition="148" endWordPosition="151">the time concept in the Let’s Go! spoken dialog system. We find that users adapt to the system’s choice of time form. We also find that user adaptation is affected by perceived system adaptation. This means that dialog systems can guide users’ word choice and can adapt their own recognition models to gain improved ASR accuracy. 1 Introduction Considerable research has now demonstrated that human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper is a follow-on to our previous</context>
</contexts>
<marker>Lockridge, Brennan, 2002</marker>
<rawString>C. Lockridge and S. Brennan. 2002. Addressees’ needs influence speakers’ early syntactic choices. Psychonomics Bulletin and Review, 9:550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pickering</author>
</authors>
<title>Activation of syntactic priming during language production.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>2</issue>
<marker>Pickering, 2000</marker>
<rawString>M. Pickering et al. 2000. Activation of syntactic priming during language production. Journal of Psycholinguistic Research, 29(2):205–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
</authors>
<title>Let’s go public! taking a spoken dialog system to the real world.</title>
<date>2005</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<marker>Raux, 2005</marker>
<rawString>A. Raux et al. 2005. Let’s go public! taking a spoken dialog system to the real world. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reitter</author>
<author>J Moore</author>
<author>F Keller</author>
</authors>
<title>Priming of syntactic rules in task-oriented dialogue and spontaneous conversation.</title>
<date>2006</date>
<booktitle>In Proceedings of CogSci.</booktitle>
<contexts>
<context position="1039" citStr="Reitter et al., 2006" startWordPosition="156" endWordPosition="159">e find that users adapt to the system’s choice of time form. We also find that user adaptation is affected by perceived system adaptation. This means that dialog systems can guide users’ word choice and can adapt their own recognition models to gain improved ASR accuracy. 1 Introduction Considerable research has now demonstrated that human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper is a follow-on to our previous study. 1In this paper, we use the term adaptation </context>
</contexts>
<marker>Reitter, Moore, Keller, 2006</marker>
<rawString>E. Reitter, J. Moore, and F. Keller. 2006. Priming of syntactic rules in task-oriented dialogue and spontaneous conversation. In Proceedings of CogSci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sheeder</author>
<author>J Balogh</author>
</authors>
<title>Say it like you mean it: Priming for structure in caller responses to a spoken dialog system.</title>
<date>2003</date>
<journal>International Journal of Speech and Technology,</journal>
<pages>6--103</pages>
<contexts>
<context position="2946" citStr="Sheeder and Balogh, 2003" startWordPosition="466" endWordPosition="469"> much dialog systems research is devoted to improving ASR accuracy, because this is a significant contributor to task success rates and to dialog length. One way to improve ASR accuracy is to use targeted language models. Since users adapt to the system’s choices of realization for task-related concepts, we can predict the user’s choice of realization and use this to adjust the ASR’s language model, improving ASR accuracy specifically on concept words. Another way to improve ASR accuracy is to guide the user into using words that are likely to be recognized correctly (Hockey and others, 2003; Sheeder and Balogh, 2003; Tomko and Rosenfeld, 2006). Our results imply that if the designer of a dialog system wants to improve ASR accuracy, system prompts should be designed to use word choices that are more recognizable; and, when, possible, to be adaptive to the user’s choice of form for task-related concepts. 2 System We conducted our experiment using the Let’s Go! deployed telephone-based spoken dialog system which provides information about bus routes in Pittsburgh (Raux and others, 2005). Let’s Go! answers the phones at the transit authority’s website outside of normal business hours. Its users are Proceedin</context>
</contexts>
<marker>Sheeder, Balogh, 2003</marker>
<rawString>T. Sheeder and J. Balogh. 2003. Say it like you mean it: Priming for structure in caller responses to a spoken dialog system. International Journal of Speech and Technology, 6:103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Stoyanchev</author>
<author>A Stent</author>
</authors>
<title>Lexical and syntactic priming and their impact in deployed spoken dialog systems.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1376" citStr="Stoyanchev and Stent, 2009" startWordPosition="208" endWordPosition="211">hat human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper is a follow-on to our previous study. 1In this paper, we use the term adaptation to indicate directional convergence, e.g. user adaptation to a system. We make no claims about the psycholinguistic models underlying this adaptation. Here we look at user adaptation to the system’s choice of realization of task-related concepts. In this paper, we: (1) Confirm our previous results showing that users adapt to the system</context>
</contexts>
<marker>Stoyanchev, Stent, 2009</marker>
<rawString>S. Stoyanchev and A. Stent. 2009a. Lexical and syntactic priming and their impact in deployed spoken dialog systems. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Stoyanchev</author>
<author>A Stent</author>
</authors>
<title>Predicting concept types in user corrections in dialog.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL Workshop on the Semantic Representation of Spoken Language.</booktitle>
<contexts>
<context position="1376" citStr="Stoyanchev and Stent, 2009" startWordPosition="208" endWordPosition="211">hat human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper is a follow-on to our previous study. 1In this paper, we use the term adaptation to indicate directional convergence, e.g. user adaptation to a system. We make no claims about the psycholinguistic models underlying this adaptation. Here we look at user adaptation to the system’s choice of realization of task-related concepts. In this paper, we: (1) Confirm our previous results showing that users adapt to the system</context>
</contexts>
<marker>Stoyanchev, Stent, 2009</marker>
<rawString>S. Stoyanchev and A. Stent. 2009b. Predicting concept types in user corrections in dialog. In Proceedings of the EACL Workshop on the Semantic Representation of Spoken Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tomko</author>
<author>R Rosenfeld</author>
</authors>
<title>Shaping user input in speech graffiti: a first pass.</title>
<date>2006</date>
<booktitle>In Proceedings of CHI.</booktitle>
<contexts>
<context position="2974" citStr="Tomko and Rosenfeld, 2006" startWordPosition="470" endWordPosition="473">rch is devoted to improving ASR accuracy, because this is a significant contributor to task success rates and to dialog length. One way to improve ASR accuracy is to use targeted language models. Since users adapt to the system’s choices of realization for task-related concepts, we can predict the user’s choice of realization and use this to adjust the ASR’s language model, improving ASR accuracy specifically on concept words. Another way to improve ASR accuracy is to guide the user into using words that are likely to be recognized correctly (Hockey and others, 2003; Sheeder and Balogh, 2003; Tomko and Rosenfeld, 2006). Our results imply that if the designer of a dialog system wants to improve ASR accuracy, system prompts should be designed to use word choices that are more recognizable; and, when, possible, to be adaptive to the user’s choice of form for task-related concepts. 2 System We conducted our experiment using the Let’s Go! deployed telephone-based spoken dialog system which provides information about bus routes in Pittsburgh (Raux and others, 2005). Let’s Go! answers the phones at the transit authority’s website outside of normal business hours. Its users are Proceedings of SIGDIAL 2009: the 10th</context>
</contexts>
<marker>Tomko, Rosenfeld, 2006</marker>
<rawString>S. Tomko and R. Rosenfeld. 2006. Shaping user input in speech graffiti: a first pass. In Proceedings of CHI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>