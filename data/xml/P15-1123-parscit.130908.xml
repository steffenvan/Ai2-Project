<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001115">
<title confidence="0.994473">
Discourse-sensitive Automatic Identification of Generic Expressions
</title>
<author confidence="0.991763">
Annemarie Friedrich Manfred Pinkal
</author>
<affiliation confidence="0.940282">
Department of Computational Linguistics
Saarland University, Saarbr¨ucken, Germany
</affiliation>
<email confidence="0.984608">
{afried,pinkal}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.993455" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999332368421053">
This paper describes a novel sequence la-
beling method for identifying generic ex-
pressions, which refer to kinds or arbitrary
members of a class, in discourse context.
The automatic recognition of such expres-
sions is important for any natural language
processing task that requires text under-
standing. Prior work has focused on iden-
tifying generic noun phrases; we present
a new corpus in which not only subjects
but also clauses are annotated for generic-
ity according to an annotation scheme mo-
tivated by semantic theory. Our context-
aware approach for automatically identi-
fying generic expressions uses conditional
random fields and outperforms previous
work based on local decisions when evalu-
ated on this corpus and on related data sets
(ACE-2 and ACE-2005).
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.967438571428571">
Distinguishing between statements about particu-
lar individuals or situations and generic sentences
is an important part of human language under-
standing. Consider example (1): sentence (a)
names characteristic attributes of a kind, which are
inherent to every (typical) individual, and sentence
(b) describes a specific individual.
</bodyText>
<listItem confidence="0.9761665">
(1) (a) The modern domestic horse has a life
expectancy of 25 to 30 years. (generic)
(a) Old Billy lived to the age of 62.
(non-generic)
</listItem>
<bodyText confidence="0.999919404255319">
The above example illustrates that generic and
non-generic sentences differ substantially in their
semantic impact and entailment properties. It can
be inferred from sentence (1a) that a typical horse
has a life expectancy of 25 to 30 years, and if we
know that Nelly is a horse, we can infer that its life
expectancy is 25 to 30 years. Sentence (1b) has
no such properties, it only allows inferences about
the particular individual Old Billy.
An automatic classifier that recognizes generic
expressions would be extremely valuable for var-
ious kinds of natural language processing sys-
tems: for text understanding and question answer-
ing systems, through the improvement of textual
entailment methods, and for systems acquiring
machine-readable knowledge from text. Machine-
readable knowledge bases have different repre-
sentations for statements corresponding to generic
knowledge about kinds and knowledge about spe-
cific individuals. The non-generic sentence (1b)
roughly speaking provides ABox content for a
machine-readable knowledge base, i.e., knowl-
edge about particular instances, e.g, “A is an in-
stance of B / has property X”. In contrast, the
generic sentence (1a) feeds the TBox, i.e., knowl-
edge of the form “All B are C / have property X”.
Reiter and Frank (2010) provide a detailed discus-
sion of the relevance of the distinction between
classes and instances for automatic ontology con-
struction.
In this paper, we present a new corpus anno-
tated in a linguistically motivated way for gener-
icity, and a context-sensitive computational model
for labeling sequences of clauses or noun phrases
(NPs) with their genericity status. Both manual
annotation and automatic recognition of generic
expressions are challenging tasks: virtually all NP
types – definites, indefinites and quantified NPs,
full NPs, pronouns, and even proper names (e.g.
species names such as Elephas maximus) – can be
found in generic and non-generic uses depending
on their clausal context.
In this work, we call clauses generic if they pro-
vide a general characterization of entities of a cer-
tain kind, and we call mentions of NPs generic if
they refer to kinds or arbitrary members of a class.
Although genericity on the clause- and NP-level
</bodyText>
<page confidence="0.932135">
1272
</page>
<note confidence="0.975956666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1272–1281,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999601">
are strongly interrelated, the concepts do not al-
ways coincide. As example (2) shows, sentences
describing episodic events can have a generic NP
as their subject. Note that references to species are
kind-referring / generic on the NP level (following
Krifka et al. (1995), seep. 65).
</bodyText>
<listItem confidence="0.985472">
(2) In September 2013 the blobfish was voted the
“World’s Ugliest Animal”. (subject generic,
clause non-generic)
</listItem>
<bodyText confidence="0.999756545454545">
Genericity often cannot be annotated without
paying attention to the wider discourse context.
Clearly, coreference information is needed for the
genericity classification of pronouns. Often, even
genericity of full NPs or entire clauses cannot be
decided in isolation, as illustrated by example (3).
Sentence (b) could be part of a particular narrative
about a tree, or it could be a generic statement.
Only the context given by (a) clarifies that (b) in-
deed makes reference to any year’s new twigs and
is to be interpreted as generic.
</bodyText>
<listItem confidence="0.98582975">
(3) (a) Sugar maples also have a tendency to
color unevenly in fall. (generic)
(b) The recent year’s growth twigs are green
and turn dark brown. (generic)
</listItem>
<bodyText confidence="0.9999713">
In computational linguistics, most research on
detecting genericity has been done in relation to
the ACE corpora (Mitchell et al., 2003; Walker
et al., 2006), focusing on assigning genericity la-
bels to noun phrases (Suh et al., 2006; Reiter and
Frank, 2010), see Section 2. Our work is based
on these approaches, most notably on the work of
Reiter and Frank (2010), and extends upon them
in the following essential ways.
The major contributions of this work are: (1)
We create a new corpus of Wikipedia articles an-
notated with linguistically motivated genericity la-
bels both on the subject- and clause-level (see Sec-
tion 3). The corpus is balanced with respect to
genericity and about 10,000 clauses in size. (2) We
present a discourse-sensitive genericity labeler.
Technically, we use conditional random fields as
a sequence labeling method (Section 4). We train
and evaluate our method on the Wikipedia dataset
and the ACE corpora, evaluating both the tasks of
predicting NP genericity and the task of predicting
clause-level genericity. Our labeler outperforms
the state-of-the-art by a margin of 6.6-11.9% (de-
pending on the data set) in terms of accuracy, at the
same time increasing Fl-score. Much of the per-
formance gain is due to the inclusion of discourse
information. For the discussion of our experimen-
tal results, see Section 5.
In this paper, we do not address the following
two important aspects of genericity. First, habit-
ual sentences form a class of generalizing state-
ments which bear a close relation to generics. As
can be seen in example (4), they describe a char-
acterizing property of either a specific entity or a
class by generalizing over situations instead of or
in addition to entities (Carlson, 2005). We clas-
sify habitual sentences with a generic subject as
generic, and habitual sentences which describe a
specific entity as non-generic, leaving the task of
habituality detection for future work.
</bodyText>
<listItem confidence="0.9999155">
(4) (a) John smokes after dinner.
(b) Gentlemen smoke after dinner.
</listItem>
<bodyText confidence="0.999887625">
Second, generic clauses express regularities
within classes of entities, and thus are similar to
universally quantified sentences in their truth con-
ditions and entailment properties. However, their
truth-conditional interpretation is tricky, since they
express typicality, describe stereotypes and al-
low exceptions, for example Dutchmen are good
sailors is not false even if most Dutchmen do not
sail at all (Carlson, 1977). We concentrate on the
decision of whether a clause is generic or not, and
leave the truth-conditional interpretation for fur-
ther work. For a detailed discussion of the seman-
tics of generics expressions see the comprehensive
survey by Krifka et al. (1995); a short and instruc-
tive overview can be found in the first part of (Re-
iter and Frank, 2010).
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999945533333333">
In this section, we first briefly review previously
developed annotation schemes for genericity. We
then describe work on automatically predicting the
genericity of NPs or different types of clauses.
Annotation. ACE-2 (Mitchell et al., 2003) and
ACE-2005 (Walker et al., 2006) are the two most
notable annotation projects for labeling genericity
of NPs to date. In the ACE-2 corpus, 40106 en-
tity mentions in 520 newswire and broadcast doc-
uments are marked with regard to whether they re-
fer to “any member of the set in question” (GEN,
generic) rather than “some particular, identifiable
member of that set” (SPC, specific/non-generic).
The major drawback of ACE-2 is that genericity is
basically defined as lack of specificity, which leads
</bodyText>
<page confidence="0.977433">
1273
</page>
<bodyText confidence="0.9998584">
to uncertainty and inconsistencies in the annota-
tion process, and to a heterogeneous set of NPs
labeled with GEN, including quantificational NPs
and NPs in modalized, future, conditional, hypo-
thetical, negated, uncertain, and question contexts.
In addition, in both ACE-2 and ACE-2005, pred-
icative and modifier uses of nouns, to which the
genericity distinction is not applicable, also re-
ceive labels (e.g. John seems to be a nice person /
a subway system).
In the updated guidelines of ACE-2005, the la-
bel USP (underspecified) is introduced for non-
generic non-specific reference, including NPs in
the various contexts mentioned above that were
improperly labeled as generic in ACE-2. The
class also contains mentions of an entity whose
identity would be ‘difficult to locate’ (Officials re-
ported ...). Moreover, annotators are asked to mark
truly ambiguous cases that have both a generic
and a non-generic reading as USP. Finally, NEG
(negated) marks negatively quantified entities that
refer to the empty set of the kind mentioned.
While we agree that in general there are under-
specified cases, the guidelines for ACE-2005 mix
other phenomena into the USP class, resulting in
a high confusion between USP and both of the
labels SPC and GEN in the manual annotations
(Friedrich et al., 2015). Data from two annota-
tors is available, and we compute an agreement of
Cohen’s κ = 0.53 over the four labels. The ACE
corpora consist only of news data, and the distribu-
tions of labels are highly skewed towards specific
mentions. For some criticism of the ACE annota-
tion scheme, see also Suh (2006).
Several linguistically motivated annotation
studies targeting genericity of noun phrases bear
similarity to our annotation scheme (Section 3),
but comprise very little data (Poesio, 2004; Herbe-
lot and Copestake, 2009). In the ARRAU corpus
(Poesio and Artstein, 2008), about 24321 mark-
ables are tagged for genericity.
Nedoluzhko (2013) survey the treatment of
genericity phenomena within coreference resolu-
tion research; they find a consistent definition of
genericity to be lacking. Friedrich and Palmer
(2014b) present an annotation scheme for situa-
tion types including generic sentences, which they
find to be infrequent in their corpus consisting of
news, jokes and (fund-raising) letters. Our new
WikiGenerics corpus contains more than 10,000
clauses, approximately half of which are generic.
Automatic Identification of Genericity. Suh et
al. (2006) propose a rule-based approach, which
extracts only bare plurals and singular NPs quanti-
fied with every or any as generic. Reiter and Frank
(2010) use a wide range of syntactic and semantic
features to train a supervised classifier for identi-
fying generic NPs. We compare to their method
(described in detail in Section 5.2) as a highly-
competitive baseline.
Palmer et al. (2007) classify clauses into several
types of situation entities including states, events,
generalizing sentences (habitual utterances refer-
ring to specific individuals) and generic sentences.
They find that using context by using the labels of
preceding clauses as features improves the classi-
fication of clause types, but generic sentences are
extremely sparse in their data set. Our present ap-
proach uses a sequence labeling model that com-
putes the best labeling for an entire sequence.
</bodyText>
<sectionHeader confidence="0.933728" genericHeader="method">
3 WikiGenerics: Data and Annotations
</sectionHeader>
<bodyText confidence="0.9998105">
In order to study generics in a genre other than
news (as in ACE), we turn to an encyclopedia, in
which we expect many generics. We create our
WikiGenerics corpus1 as follows. We aim to cre-
ate a corpus that is balanced in the sense that it
contains many generic and non-generic sentences,
and also generics from many different domains.
We collect 102 texts about animals, organised
crime, ethnic groups, games, sports, medicine,
music, politics, religion, scientific disciplines and
biographies from Wikipedia. For example, some
sentences make statements about a ‘natural’ kind
(Blobfish are typically shorter than 30 cm), others
express definitions such as the rules of a football
game (The offensive team must line up in a legal
formation before they can snap the ball).
Generic clauses have the typical form of a pred-
icative statement about the sentence topic, which
is normally realized as the grammatical subject in
English. Intuitions about NP-level genericity and
its relation to clause-level genericity are quite reli-
able for topic NPs of clauses, which also typically
occur in subject position in English. Since gener-
ics in non-subject positions are less frequent and
hard to interpret (see the discussion of “dependent
generics” by Link (1995)), we decided to annotate
subject NPs only. We are aware that we are miss-
ing relevant cases (e.g. the less preferred reading
</bodyText>
<footnote confidence="0.997716">
1The WikiGenerics corpus is freely available at:
www.coli.uni-saarland.de/projects/sitent
</footnote>
<page confidence="0.992982">
1274
</page>
<bodyText confidence="0.9999741">
of Cats chase mice, which attributes to mice the
property of being chased by cats), but in this work,
we want to study the “easier” subject cases as a
first step.
We use the discourse parser SPADE (Soricut
and Marcu, 2003) to automatically segment the
first 70 sentences of each article into clauses. Each
clause is manually annotated with the following
information (for more details on the annotation
scheme, see (Friedrich et al., 2015)):
</bodyText>
<listItem confidence="0.760515">
• Task NP: whether or not the subject NP of the
clause refers to a class or kind (generic vs. non-
generic);
• Task Cl: whether the clause is generic, defined
as a clause that makes a characterizing state-
ment about a class or kind, or non-generic.
• Task Cl+NP: using the information from Task
NP and Cl above, we automatically derive the
following classification for each clause (com-
pare to the explanation of example (2)).
– GEN gen: generic clause, subject is generic
by definition (The lion is a predatory cat);
– NON-GEN non-gen: non-generic clause
with a non-generic subject ( Simba roared);
– or NON-GEN gen: episodic clause with a
generic subject (Dinosaurs died out).
– GEN non-gen does not exist by definition.
</listItem>
<bodyText confidence="0.99967372">
We construct the gold standard for our experi-
ments via majority voting over the labels given by
three paid annotators, students of computational
linguistics. Annotators were given a written man-
ual and a short training on documents not included
in the corpus. They are given the option to indicate
segmentation errors, e.g. that two segments should
actually be one, or that one segment contains mul-
tiple clauses. In the latter case, we ask them to give
labels for the first clause in the segment. 10240
(86%) of all pre-segmented clauses received labels
for all three tasks from all annotators, who were
allowed to skip clauses that do not contain a finite
verb. Our gold standard includes an additional 115
segments that did not receive a label by one an-
notator but were unanimously labeled by the other
two. The other segments are disregarded in the ex-
periments. Some of them have expletive subjects,
and most others are non-finite verb phrases such as
to-infinites or headlines that consist of only a NP.
Inter-annotator agreement measured as Fleiss’ n
(Fleiss, 1971) on the segments labeled by all three
annotators is 0.70, 0.73 and 0.69 for Task NP, Task
Cl and Task Cl+NP respectively, indicating sub-
stantial agreement (Landis and Koch, 1977).
</bodyText>
<sectionHeader confidence="0.9430925" genericHeader="method">
4 A Sequence Labeling Model for
Genericity
</sectionHeader>
<bodyText confidence="0.999988931034483">
This section describes our method for identifying
generic clauses and NPs in context. We apply the
following methods on each of the three different
prediction tasks NP, Cl and Cl+NP introduced in
Section 3, varying only the type of labels on which
we train and test. In contrast to prior work, our
computational model integrates not only informa-
tion from each local instance, but also informa-
tion about the genericity status of surrounding in-
stances. The final labeling for the sequence of in-
stances of an entire document is optimized with
regard to these two types of information, which,
as we have argued in Section 1, both play a cru-
cial role in determining genericity. The sequences
to be labeled contain all clauses or NPs of a doc-
ument. We also tried labeling sequences for para-
graphs instead of documents, but the performance
was similar. A reason might be that paragraphs are
quite often linked by mentioning the same entities
(Friedrich and Palmer, 2014a).
Computational model. We use linear chain
conditional random fields (Lafferty et al., 2001)
to label sequences of mentions or sequences of
clauses with regard to their genericity. Conditional
random fields (CRFs) are well suited for our label-
ing task as they do not make an independence as-
sumption between the features. CRFs predict the
conditional probability of label sequence y� given
an observation sequence x� as follows:
</bodyText>
<equation confidence="0.991388">
n m
P(y&amp;quot;|x) = Z(x) exp(E E Aifi(yj−1, yj, 9,j))
j=1 i=1
</equation>
<bodyText confidence="0.995639153846154">
Z(x) is a normalization constant, the sum over
the scores of all possible label sequences for an
observation sequence with the length of Y. The
weights Ai of the feature functions are the param-
eters to be learned. They do not depend on the cur-
rent position j in the sequence. The feature func-
tions fi are in general allowed to look at the cur-
rent label yj, the previous label yj−1 and the entire
observation sequence Y. We use a simple instan-
tiation of a linear chain CRF whose feature func-
tions take two forms, fi(yj, xj) and fi(yj−1, yj).
We create a linear chain CRF model using the
CRF++ toolkit2, using all the default parameters.
</bodyText>
<footnote confidence="0.96004">
2https://code.google.com/p/crfpp
</footnote>
<page confidence="0.790894">
1275
</page>
<table confidence="0.997855185185185">
NP-BASED FEATURES
number sg, pl
person 1, 2, 3
countability from Celex, e.g. count
noun type common, proper, pronoun
determiner type def, indef, demon
part-of-speech POS of head
bare plural true, false
WN granularity number of edges to top node
WN sense [0 − 2] WN senses (head+hypernyms)
WN senseTop top sense in hypernym hierarchy
WN lexical filename person, artifact, event, ...
CLAUSE-BASED FEATURES
dependency [0 − 4] dependency relation between
head and governor etc.
tense tense, aspect and voice informa-
tion, e.g. pres perf active
coarseTense pres, past, fut
progressive true, false
perfective true, false
passive true, false
temporal modifier true, false
number of modifiers numeric
part-of-speech POS of head
predicate lemma of head
adjunct-degree positive, comparative, superlative
adjunct-pred lemma of adverbial clauses’ head
</table>
<tableCaption confidence="0.99931">
Table 1: Features. WN=WordNet.
</tableCaption>
<bodyText confidence="0.999127307692308">
Feature functions. We extract the set of features
listed in Table 1 for each instance. This set of fea-
tures is inspired by Reiter and Frank (2010), see
also Section 5.2. In the case of the WikiGener-
ics corpus, the NP features are extracted for the
subject of the clause. We parse the data using the
Stanford parser (Klein and Manning, 2002) and
obtain the subject NPs from the collapsed depen-
dencies. For the ACE data, the NP features are
extracted for all mentions in the gold standard and
the clause features are extracted from the clause
in which the mention appears. Our feature func-
tions fi(yj, xj) are indicator functions combining
the current label and one of the feature values of
the current mention or clause, for example:
f = if (yj = GENERIC and xj.np.person=3)
return 1 else return 0
We create two versions of the CRF model: the
bigram3 model additionally uses indicator func-
tions f(yj−1, yj) for each combination of labels,
thus taking context into account. The unigram
model does not use these feature functions, it is
thus similar to a maximum entropy model (with a
different normalization). Log-linear models work
very well for many NLP tasks, especially if fea-
tures are correlated as it is the case here, so in or-
</bodyText>
<footnote confidence="0.461795">
3Following CRF++ terminology.
</footnote>
<bodyText confidence="0.994808518518519">
der to get a fair estimate of the impact of using
the context (via the transition feature functions),
we give numbers for this ‘unigram’ model in ad-
dition, rather than simply comparing the bigram-
CRF to a Bayesian network, which is used by
Reiter and Frank (2010). Using more complex
feature functions did not result in significant per-
formance gains, so we chose the simplest model.
Note that even though the feature functions only
formulate relationships between adjacent labels in
the sequence, the optimal labeling is computed for
the entire sequence: the choices of labels assigned
to non-adjacent clauses do influence each other.
Two-step Approach for Task Cl+NP. Task
Cl+NP can be regarded as a combination of the
two decisions made in Task NP and Task Cl.
Therefore, we approach Task Cl+NP in two ways.
(a) We train a CRF which directly outputs the
three labels. (b) The two-step approach combines
the output from the labelers trained for Task NP
and Task Cl into one label in a rule-based way.
This leads to the additional class GEN non-gen,
of which no gold instances exist by definition. As
we evaluate in terms of F1-score and accuracy for
the existing classes, items classified into this arti-
ficial class will simply be counted as wrong and
lack from the recall counts.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999865666666667">
This section reports on our experiments, which
we evaluate in terms of precision (P), recall (R)
and F1-measure per class. We compute macro-
</bodyText>
<equation confidence="0.688755">
averages as Pmacro = 1
|c |∗ �|c|
</equation>
<bodyText confidence="0.9746942">
i=1 Pi etc., where |c|
stands for the number of classes. Macro-F1 is the
harmonic mean of macro-average P and R. To re-
port on statistical significance of differences in ac-
curacy, we apply McNemar’s test with p &lt; 0.01.
</bodyText>
<subsectionHeader confidence="0.998146">
5.1 Experimental Settings and Data
</subsectionHeader>
<bodyText confidence="0.999774333333333">
We report results for cross validation (CV). Be-
cause we leverage contextual information by la-
beling sequences of clauses from entire docu-
ments, for all experiments presented in this sec-
tion, if not indicated otherwise, we put all in-
stances of one document into the same fold as
one sequence. Fold sizes differ slightly from each
other, but folds are kept constant for all experi-
ments.
On WikiGenerics, we carry out all three predic-
tion tasks as defined in Section 3. On the ACE cor-
pora, we only conduct Task NP because there are
</bodyText>
<page confidence="0.973513">
1276
</page>
<table confidence="0.999854428571428">
generic F1 non-generic macro-avg accuracy
P R P R F1 P R F1
System
Majority class baseline 0.0 0.0 0.0 86.8 100 92.9 43.4 50.0 46.5 86.8
Person baseline (R&amp;F) 60.4 10.2 17.5 87.9 99.0 93.1 74.2 54.6 62.9 87.2
R&amp;F (BayesNet) 37.7 72.0 49.5 95.0 81.9 88.0 66.4 76.9 71.3 80.6
Reimpl. (BayesNet) 38.1 67.7 48.8 94.4 83.3 88.5 66.3 75.5 70.6 81.2
</table>
<tableCaption confidence="0.834909">
Table 2: Results of reimplemented baseline on ACE-2 (original, unbalanced data set), 40106 instances
(annotated noun phrases). Weka’s stratified 10-fold cross validation, using all features.
</tableCaption>
<bodyText confidence="0.994051538461539">
no labels corresponding to Task Cl or Task Cl+NP.
For the experiments on WikiGenerics, we use
leave-one-document-out CV, i.e., we train on 101
of the 102 documents and test on the remain-
ing document in each fold. The total number of
clauses is 10355. From ACE-2005, we use the
newswire and broadcast news subsections.4 Due
to low frequency, we omit instances of NEG in our
experiments, and apply a three-way classification
task (GEN, SPC, USP). We present results for all
remaining 40106 mentions and for the subset of
18029 subject mentions, each time using 10-fold
CV.
</bodyText>
<subsectionHeader confidence="0.998626">
5.2 Baseline: Local Classifier
</subsectionHeader>
<bodyText confidence="0.9999288">
The system for identifying generic NPs of Reiter
and Frank (2010), henceforth R&amp;F, makes use of
the English ParGram LFG grammar for the XLE
parser (Butt et al., 2002). As this grammar is not
publicly available, we implement a similar system
using exclusively the Stanford CoreNLP toolsuite
(Manning et al., 2014), the Celex database of En-
glish nouns (Baayen et al., 1996) and WordNet
(Fellbaum, 1999). Our system is based on dkpro
(de Castilho and Gurevych, 2014). We extract the
features listed in Table 1 based on the POS tags
and syntactic dependencies assigned by the Stan-
ford parser (Klein and Manning, 2002). We could
not reimplement several tense- and aspect-related
ParGram-specific features. In order to compen-
sate for this, we add an additional feature (tense)
with finer-grained tense and voice information, us-
ing the rules described by Loaiciga et al. (2014).
Other additional features did not improve perfor-
mance, which shows that R&amp;F’s set of features
captures the syntactic-semantic information rele-
vant to genericity classification quite well. There-
fore, we use this feature set also for the sequence
labeling model. Using the same feature set allows
us to attribute any performance gain to the context-
</bodyText>
<footnote confidence="0.569291333333333">
4The rest of the data comprise broadcast conversation,
weblog and forum texts as well as transcribed conversational
telephone, and would require specialized preprocessing.
</footnote>
<bodyText confidence="0.9993414">
awareness of our model rather than the features.
R&amp;F train a Bayesian network using Weka
(Hall et al., 2009). The decisions of this clas-
sifier are local to each clause. They report the
performance of their system on the ACE-2 cor-
pus: Table 2 shows that the performance of our re-
implemented feature set5 is comparable to the sys-
tem of R&amp;F.6 In all other other tables, “BayesNet
R&amp;F” refers to our re-implemented system.
R&amp;F present the “Person baseline” as a sim-
ple informed baseline (see Table 2). We trained
a J48 decision tree on this feature alone, which
confirmed that only second-person mentions (the
generic “you”) are classified as generic, while all
other mentions are classified as non-generic.
</bodyText>
<subsectionHeader confidence="0.86727">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99997380952381">
In this section, we first discuss the results of our
experiments in terms of identifying generic NPs or
clauses. Then we present some additional experi-
ments testing the influence of the different feature
classes and of other discourse-related information.
All tasks, WikiGenerics. The observations de-
scribed in this paragraph are the same for all three
prediction tasks on WikiGenerics. As Tables 3 and
4 show, our CRF models outperform the baseline
system of R&amp;F by a large margin both in terms of
accuracy and F1-score on the WikiGenerics cor-
pus. In Task NP and Task Cl, precision and re-
call are quite balanced (not shown in tables). The
performance of the bigram model is significantly
better than that of the unigram model, increasing
accuracy by about 3%, at the same time increas-
ing F1. In an oracle experiment, we use the pre-
vious gold label instead of the predicted one for
fi(yj−1, yj), and scores increase by up to 6.6%
compared to the unigram model. These results
provide strong empirical evidence for our hypoth-
</bodyText>
<footnote confidence="0.9888852">
5Implementation available at:
www.coli.uni-saarland.de/projects/sitent
6Table 6 in Reiter and Frank’s paper contains some typo-
graphical errors here. We thank Nils Reiter for making avail-
able his ARFF files, so we can provide this updated version.
</footnote>
<page confidence="0.962246">
1277
</page>
<table confidence="0.9994682">
Task NP: Genericity of Subject Task Cl: Genericity of Clause
System generic non-gen. macro-avg acc. generic non-gen. macro-avg acc.
F1 F1 F1 F1 F1 F1
Majority class 71.9 0.0 35.9 56.1 60.3 3.7 35.1 43.7
BayesNet (R&amp;F) 72.6 70.8 72.3 71.7 72.4 74.6 73.7 73.5
CRF (unigram) 79.3 72.6 75.9 76.4* 77.9 77.0 77.4 77.41
CRF (bigram) 81.3 76.3 78.8 79.1* 80.8 80.6 80.7 80.71
- only clause features 79.2 71.6 75.5 76.0 79.3 78.3 78.8 78.8
- only NP features 76.8 70.8 73.8 74.1 70.7 72.6 71.8 71.7
CRF (bigram, gold) 85.0 80.4 82.7 83.0 82.9 82.6 82.8 82.8
</table>
<tableCaption confidence="0.999036">
Table 3: Results on WikiGenerics for Task NP and Task C. *†Difference statistically significant.
</tableCaption>
<table confidence="0.9998025">
Task Cl+NP: Genericity of Clause (three-way)
GEN gen NON-GEN non-gen NON-GEN gen macro-avg accuracy
F1 F1 F1 P R F1
System
Majority class 67.1 0.0 0.0 16.8 33.3 22.4 50.4
BayesNet (R&amp;F) 69.1 69.1 26.1 54.5 58.4 56.4 65.2
CRF (unigram) 78.5 72.6 35.4 67.2 60.0 63.4 74.0*
CRF (bigram) 81.3 76.9 33.4 70.3 61.8 65.8 77.4*
- two-step 80.8 75.8 28.6 61.5 62.3 61.9 73.4
- only clause feat. 79.4 72.6 25.3 67.0 57.2 61.8 74.3
- only NP feat. 72.9 71.4 2.5 53.0 49.9 51.4 70.0
CRF (bigram, gold) 84.0 80.6 39.1 72.8 65.7 69.0 80.6
</table>
<tableCaption confidence="0.999882">
Table 4: Results on WikiGenerics for Task Cl+NP. *Difference statistically significant.
</tableCaption>
<bodyText confidence="0.99978937037037">
esis that using context information is useful for
identifying the genericity of NPs or clauses.
Task Cl+NP, WikiGenerics. In Task Cl+NP
(see Table 4), only about 6% of the instances
have the gold label NON-GEN gen (i.e., a non-
generic sentence with a generic subject), the other
instances are distributed roughly evenly between
the other two labels. The difficulty of Task Cl+NP
thus consists in identifying this infrequent case.
The three-way CRF outperforms the two-step ap-
proach both in terms of accuracy and macro-
average F1-score. The precision-recall tradeoff
differs: for the NON-GEN gen class, P and R
of the CRF are 55.2% and 24.5% and those of
the two-step-approach are 23.8% and 35.9%. The
two-step approach labels more instances as NON-
GEN gen but does so in a less precise way. While
the performance of our model leaves room for im-
provement on Task Cl+NP, especially with regard
to the class NON-GEN gen, it is worth noting
that the computational model captures something
about the nature of this latter class; its instances do
look different in the feature space. The context-
aware CRF using three labels performs best.
Feature set ablation. In this ablation test,
shown in Tables 3 and 4, our best model (CRF bi-
gram) uses either the set of clause-based or the set
of NP-based features at a time. Clause-based fea-
tures are more important than the NP-based fea-
tures for all three classification tasks. An inter-
esting observation is that the NP features alone
are not able to separate the infrequent class NON-
GEN gen from the other two at all, the F1-score
of 2.5 shows that almost all instances of this class
were labeled as one of the other two classes. In
sum, this shows that whether an NP is interpreted
as generic or not strongly depends on how it is
used in the clause.
Task NP, ACE. Both on ACE-2 (see Table 5)
and on ACE-2005 (see Table 6), the CRF outper-
forms the system of Reiter and Frank (2010) in
terms of accuracy, and has a higher F1-score. We
give results also for subjects only as this parallels
the setting of the WikiGenerics experiments (rea-
sons for the restriction to subjects were given in
Section 3). For subjects, the majority class SPC
is less frequent (compare the accuracies of the
two majority class baselines); only 7% of the sub-
jects are marked as GEN, the rest are labeled as
USP. The bigram model does not outperform the
unigram model, but our oracle experiments show
that context information is indeed useful: accuracy
increases significantly and F1 increases consider-
ably, especially for subjects.
</bodyText>
<page confidence="0.962236">
1278
</page>
<table confidence="0.9998365">
generic non-generic macro-avg accuracy
F1 F1 P R F1
System
Majority class 0.0 92.9 43.4 50.0 46.5 86.8
BayesNet (R&amp;F) 47.4 87.9 65.5 74.6 69.8 80.4
CRF (unigrams) 49.1 93.5 75.5 68.7 71.3 88.5*
CRF (bigrams) 51.0 93.7 76.5 69.8 72.4 88.9
CRF (bigram, gold) 57.6 94.4 79.8 73.4 76.0 90.1*
</table>
<tableCaption confidence="0.957634">
Table 5: Results on ACE-2 for Task NP, 10-fold CV, folds contain complete documents. *Difference
statistically significant.
</tableCaption>
<table confidence="0.9994006">
macro-avg accuracy
P R F1
System
all 18029 annotated mentions
Majority class 27.0 33.3 29.9 81.1
BayesNet (R&amp;F) 50.8 57.2 53.8 74.5
CRF (unigram) 61.6 51.8 55.1 83.2*
CRF (bigram) 60.6 51.7 54.8 83.0
CRF (bigram, gold) 63.9 54.9 58.2 83.9*
5670 subject mentions
Majority class 25.0 33.3 28.6 75.1
BayesNet (R&amp;F) 51.5 53.9 52.7 72.5
CRF (unigram) 58.0 51.3 53.6 77.7*
CRF (bigram) 58.3 51.3 53.7 77.8
CRF (bigram, gold) 62.4 56.1 58.6 79.6*
</table>
<tableCaption confidence="0.999291">
Table 6: Results on ACE-2005 (bn+nw),
</tableCaption>
<bodyText confidence="0.993079391304348">
Task NP, 10-fold CV, 3 classes: SPC, GEN, USP.
*Difference statistically significant.
We identify two reasons for the fact that when
evaluating on the ACE corpora, oracle information
is needed to show the benefit of using bigram fea-
ture functions: (a) The frequency of GEN men-
tions in the ACE corpora is low – news contains
only little generic information, so the context in-
formation is harder to leverage. (b) The ACE an-
notation guidelines contain some vagueness (see
Section 3); this makes it harder for an automatic
system to learn about regularities.
Higher-order Markov models. Another re-
search question is whether models incorporating
not only the previous label, but more preceding la-
bels would perform even better. We turn to the
Mallet toolkit (McCallum, 2002), whose CRF im-
plementation allows for using higher-order mod-
els.7 For example, an order-2 model considers the
two previous labels. We use L1-regularization dur-
ing training. Figure 1 shows that the optimum is
reached for order-1 (bigram) models for each of
the classification tasks for accuracy, the same ten-
</bodyText>
<footnote confidence="0.9972642">
7The CRF++ toolkit, which we use in all other exper-
iments, does not allow for higher-order models. We use
CRF++ in the main experiments as it comes with a concise
documentation; this helps to make our experiments easily
replicable.
</footnote>
<figureCaption confidence="0.9619245">
Figure 1: Labeling results for CRF models of var-
ious orders on WikiGenerics corpus.
</figureCaption>
<bodyText confidence="0.99988953125">
dencies were observed for Fi-score (not shown). It
seems sufficient to use bigram feature functions;
note that as explained in Section 4, the bigram
model does not mean that only adjacent clauses
influence each other – context is actually wider.
Using coreference information. In our approx-
imately balanced WikiGenerics corpus, 54% of
all pronouns are marked as generic and 46% are
marked as non-generic, which shows that there is
no preference for pronouns to occur with either
class. Some of the features (countability, noun
type, determiner type, bare plural, and the Word-
Net related features) are not informative when ap-
plied to personal or relative pronouns. Sometimes,
it is not even possible to determine number with-
out referring to the antecedent (e.g., in the case
of the relative pronoun ‘who’). We conduct the
following experiment: we automatically resolve
coreference using the Stanford coreference reso-
lution system (Raghunathan et al., 2010). We re-
place the NP features of each pronominal instance
with the features of the first link of the coreference
chain. We did not obtain a significant performance
gain. One reason is that this change of features
only applies to about 13% of the data. We observe
that any positive changes in the classification go
along with some negative changes which were of-
ten due to coreference resolution errors. One dif-
ficult step in manually annotating, and hence also
in automatically resolving coreference is to deter-
mine whether a NP is generic or not (Nedoluzhko,
2013). The task of identifying generic NPs and
</bodyText>
<figure confidence="0.984739272727273">
0 (unigram)1 (bigram) 2 Markov order
Task NP
Task Cl
Task Cl+NP
Accuracy %
82
80
78
76
74
72
</figure>
<page confidence="0.991293">
1279
</page>
<bodyText confidence="0.9997965">
coreference resolution are intertwined. We plan to
manually annotate at least part of our corpus with
coreference information in order to test to what ex-
tent the classification of the pronouns’ genericity
status can profit from including antecedent infor-
mation.
</bodyText>
<sectionHeader confidence="0.998617" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999992871794872">
We have presented a novel method for labeling se-
quences of clauses or their subjects with regard
to their genericity, showing that genericity should
be treated as a discourse-sensitive phenomenon.
Our experiments prove that context information
improves automatic labeling results, and that our
model outperforms previous approaches by a large
margin.
The major contributions of this work include the
study of genericity both on the NP- and clause-
level, and the study of the interaction of these two
levels. Our results of Task Cl+NP show that our
model indeed captures the three different types
of clauses resulting from the combination of NP-
level and clause-level genericity.
During the development of our annotation
scheme, we found that it is beneficial to focus
on genericity, disentangling it from the issue of
specificity. Our work provides a step forward to
finding reliable ways to apply semantic theories
of genericity in practice, and we also provide a
new state-of-the-art system for automatically la-
beling generic expressions. This in turn lays foun-
dations for natural language processing tasks re-
quiring text understanding.
Future Work. Our present approach for anno-
tating and automatically classifying targets the
subjects of each clause. We have not attempted
to tackle the task of classifying the genericity sta-
tus of other dependents, as they are even harder
to classify than subjects, and a concise annotation
scheme has to be worked out in order achieve an
acceptable inter-annotator agreement on this task.
Another related distinction is the one between ha-
bitual, stative and episodic sentences (Mathew and
Katz, 2009), which applies to both what we call
generic and non-generic sentences. No large cor-
pora exist to date, but studying the interaction of
these phenomena is on our research agenda.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999712333333333">
We thank the anonymous reviewers, Alexis
Palmer, Nils Reiter and Melissa Peate Sørensen
for their helpful comments related to this work,
and our annotators Christine Bocionek and Kleo-
Isidora Mavridou. This research was supported
in part by the Cluster of Excellence “Multimodal
Computing and Interaction” of the German Excel-
lence Initiative (DFG), and the first author is sup-
ported by an IBM PhD Fellowship.
</bodyText>
<sectionHeader confidence="0.998498" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99711054054054">
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. CELEX2. Philadelphia: Linguistic
Data Consortium.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15, pages 1–7. Association for
Computational Linguistics.
Gregory Norman Carlson. 1977. Reference to kinds in
English. Ph.D. thesis.
Gregory N. Carlson. 2005. Generics, Habituals and
Iteratives. In Alex Barber, editor, Encyclopedia of
Language and Linguistics. Elsevier.
Richard Eckart de Castilho and Iryna Gurevych. 2014.
A broad-coverage collection of portable NLP com-
ponents for building shareable analysis pipelines.
In Proceedings of the Workshop on Open In-
frastructures and Analysis Frameworks for HLT
(OIAF4HLT) at COLING, pages 1–11.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Annemarie Friedrich and Alexis Palmer. 2014a. Cen-
tering Theory in natural text: a large-scale corpus
study. In Proceedings of KONVENS 2014. Univer-
sit¨atsbibliothek Hildesheim.
Annemarie Friedrich and Alexis Palmer. 2014b. Situ-
ation entity annotation. In Proceedings of the Lin-
guistic Annotation Workshop VIII, page 149.
Annemarie Friedrich, Alexis Palmer, Melissa Peate
Srensen, and Manfred Pinkal. 2015. Annotating
genericity: a survey, a scheme, and a corpus. In Pro-
ceedings of the 9th Linguistic Annotation Workshop
(LAW IX), Denver, Colorado, US.
</reference>
<page confidence="0.72338">
1280
</page>
<reference confidence="0.99995706">
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.
Aurelie Herbelot and Ann Copestake. 2009. Anno-
tating genericity: How do humans decide? (A case
study in ontology extraction). Studies in Generative
Grammar 101, page 103.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in neural information
processing systems, pages 3–10.
Manfred Krifka, Francis Jeffrey Pelletier, Gregory N.
Carlson, Alice ter Meulen, Godehard Link, and Gen-
naro Chierchia. 1995. Genericity: An Introduction.
The Generic Book, pages 1–124.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Label-
ing Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, ICML ’01, pages 282–289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.
Godehard Link. 1995. Generic information and depen-
dent generics. The Generic Book, pages 358–382.
Sharid Loaiciga, Thomas Meyer, and Andrei Popescu-
Belis. 2014. English-French Verb Phrase Align-
ment in Europarl. In Proceedings of LREC 2014.
Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Thomas A. Mathew and E. Graham Katz. 2009. Super-
vised Categorization of Habitual and Episodic Sen-
tences. In Sixth Midwest Computational Linguistics
Colloquium, Bloomington, Indiana: Indiana Univer-
sity.
Andrew K McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstein, Lisa Ferro, and
Beth Sundheim. 2003. ACE-2 Version 1.0
LDC2003T11. Philadelphia: Linguistic Data Con-
sortium.
Anna Nedoluzhko. 2013. Generic noun phrases and
annotation of coreference and bridging relations in
the Prague Dependency Treebank. In Proceedings
of the 7th Linguistic Annotation Workshop and In-
teroperability with Discourse, pages 103–111.
Alexis Palmer, Elias Ponvert, Jason Baldridge, and
Carlota Smith. 2007. A sequencing model for sit-
uation entity classification. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, page 896.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
Annotation in the ARRAU Corpus. In LREC.
Massimo Poesio. 2004. Discourse annotation and se-
mantic annotation in the GNOME corpus. In Pro-
ceedings of the 2004 ACL Workshop on Discourse
Annotation, pages 72–79. Association for Computa-
tional Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492–501.
Association for Computational Linguistics.
Nils Reiter and Anette Frank. 2010. Identifying
Generic Noun Phrases. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 40–49, Uppsala, Sweden,
July. Association for Computational Linguistics.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149–156. Association
for Computational Linguistics.
Sangweon Suh, Harry Halpin, and Ewan Klein.
2006. Extracting common sense knowledge from
wikipedia. In Proceedings of the Workshop on Web
Content Mining with Human Language Technolo-
gies at ISWC, volume 6.
Sangweon Suh. 2006. Extracting Generic Statements
for the Semantic Web. Master’s thesis, University of
Edinburgh.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilingual
Training Corpus LDC2006T06. Philadelphia: Lin-
guistic Data Consortium.
</reference>
<page confidence="0.991262">
1281
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511150">
<title confidence="0.999227">Discourse-sensitive Automatic Identification of Generic Expressions</title>
<author confidence="0.999498">Annemarie Friedrich Manfred Pinkal</author>
<affiliation confidence="0.961104">Department of Computational Linguistics Saarland University, Saarbr¨ucken, Germany</affiliation>
<abstract confidence="0.9738631">This paper describes a novel sequence labeling method for identifying generic expressions, which refer to kinds or arbitrary members of a class, in discourse context. The automatic recognition of such expressions is important for any natural language processing task that requires text understanding. Prior work has focused on identifying generic noun phrases; we present a new corpus in which not only subjects but also clauses are annotated for genericity according to an annotation scheme motivated by semantic theory. Our contextaware approach for automatically identifying generic expressions uses conditional random fields and outperforms previous work based on local decisions when evaluated on this corpus and on related data sets (ACE-2 and ACE-2005).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald R Baayen</author>
<author>Richard Piepenbrock</author>
<author>Leon Gulikers</author>
</authors>
<date>1996</date>
<booktitle>CELEX2. Philadelphia: Linguistic Data Consortium.</booktitle>
<contexts>
<context position="23862" citStr="Baayen et al., 1996" startWordPosition="3868" endWordPosition="3871">ances of NEG in our experiments, and apply a three-way classification task (GEN, SPC, USP). We present results for all remaining 40106 mentions and for the subset of 18029 subject mentions, each time using 10-fold CV. 5.2 Baseline: Local Classifier The system for identifying generic NPs of Reiter and Frank (2010), henceforth R&amp;F, makes use of the English ParGram LFG grammar for the XLE parser (Butt et al., 2002). As this grammar is not publicly available, we implement a similar system using exclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect-related ParGram-specific features. In order to compensate for this, we add an additional feature (tense) with finer-grained tense and voice information, using the rules described by Loaiciga et al. (2014). Other additional features did not improve performance, which shows that R&amp;F’s set of features captu</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1996</marker>
<rawString>Harald R. Baayen, Richard Piepenbrock, and Leon Gulikers. 1996. CELEX2. Philadelphia: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Helge Dyvik</author>
<author>Tracy Holloway King</author>
<author>Hiroshi Masuichi</author>
<author>Christian Rohrer</author>
</authors>
<title>The parallel grammar project.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 workshop on Grammar engineering and evaluation-Volume 15,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23657" citStr="Butt et al., 2002" startWordPosition="3835" endWordPosition="3838">102 documents and test on the remaining document in each fold. The total number of clauses is 10355. From ACE-2005, we use the newswire and broadcast news subsections.4 Due to low frequency, we omit instances of NEG in our experiments, and apply a three-way classification task (GEN, SPC, USP). We present results for all remaining 40106 mentions and for the subset of 18029 subject mentions, each time using 10-fold CV. 5.2 Baseline: Local Classifier The system for identifying generic NPs of Reiter and Frank (2010), henceforth R&amp;F, makes use of the English ParGram LFG grammar for the XLE parser (Butt et al., 2002). As this grammar is not publicly available, we implement a similar system using exclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect-related ParGram-specific features. In order to compensate for this, we add an additional feature (te</context>
</contexts>
<marker>Butt, Dyvik, King, Masuichi, Rohrer, 2002</marker>
<rawString>Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and Christian Rohrer. 2002. The parallel grammar project. In Proceedings of the 2002 workshop on Grammar engineering and evaluation-Volume 15, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Norman Carlson</author>
</authors>
<date>1977</date>
<note>Reference to kinds in English. Ph.D. thesis.</note>
<contexts>
<context position="7465" citStr="Carlson, 1977" startWordPosition="1163" endWordPosition="1164">tual sentences which describe a specific entity as non-generic, leaving the task of habituality detection for future work. (4) (a) John smokes after dinner. (b) Gentlemen smoke after dinner. Second, generic clauses express regularities within classes of entities, and thus are similar to universally quantified sentences in their truth conditions and entailment properties. However, their truth-conditional interpretation is tricky, since they express typicality, describe stereotypes and allow exceptions, for example Dutchmen are good sailors is not false even if most Dutchmen do not sail at all (Carlson, 1977). We concentrate on the decision of whether a clause is generic or not, and leave the truth-conditional interpretation for further work. For a detailed discussion of the semantics of generics expressions see the comprehensive survey by Krifka et al. (1995); a short and instructive overview can be found in the first part of (Reiter and Frank, 2010). 2 Related Work In this section, we first briefly review previously developed annotation schemes for genericity. We then describe work on automatically predicting the genericity of NPs or different types of clauses. Annotation. ACE-2 (Mitchell et al.</context>
</contexts>
<marker>Carlson, 1977</marker>
<rawString>Gregory Norman Carlson. 1977. Reference to kinds in English. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory N Carlson</author>
</authors>
<title>Generics, Habituals and Iteratives.</title>
<date>2005</date>
<booktitle>Encyclopedia of Language and Linguistics.</booktitle>
<editor>In Alex Barber, editor,</editor>
<publisher>Elsevier.</publisher>
<contexts>
<context position="6775" citStr="Carlson, 2005" startWordPosition="1061" endWordPosition="1062">epending on the data set) in terms of accuracy, at the same time increasing Fl-score. Much of the performance gain is due to the inclusion of discourse information. For the discussion of our experimental results, see Section 5. In this paper, we do not address the following two important aspects of genericity. First, habitual sentences form a class of generalizing statements which bear a close relation to generics. As can be seen in example (4), they describe a characterizing property of either a specific entity or a class by generalizing over situations instead of or in addition to entities (Carlson, 2005). We classify habitual sentences with a generic subject as generic, and habitual sentences which describe a specific entity as non-generic, leaving the task of habituality detection for future work. (4) (a) John smokes after dinner. (b) Gentlemen smoke after dinner. Second, generic clauses express regularities within classes of entities, and thus are similar to universally quantified sentences in their truth conditions and entailment properties. However, their truth-conditional interpretation is tricky, since they express typicality, describe stereotypes and allow exceptions, for example Dutch</context>
</contexts>
<marker>Carlson, 2005</marker>
<rawString>Gregory N. Carlson. 2005. Generics, Habituals and Iteratives. In Alex Barber, editor, Encyclopedia of Language and Linguistics. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
</authors>
<title>A broad-coverage collection of portable NLP components for building shareable analysis pipelines.</title>
<date>2014</date>
<booktitle>In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING,</booktitle>
<pages>1--11</pages>
<marker>de Castilho, Gurevych, 2014</marker>
<rawString>Richard Eckart de Castilho and Iryna Gurevych. 2014. A broad-coverage collection of portable NLP components for building shareable analysis pipelines. In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1999</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="23891" citStr="Fellbaum, 1999" startWordPosition="3874" endWordPosition="3875">nd apply a three-way classification task (GEN, SPC, USP). We present results for all remaining 40106 mentions and for the subset of 18029 subject mentions, each time using 10-fold CV. 5.2 Baseline: Local Classifier The system for identifying generic NPs of Reiter and Frank (2010), henceforth R&amp;F, makes use of the English ParGram LFG grammar for the XLE parser (Butt et al., 2002). As this grammar is not publicly available, we implement a similar system using exclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect-related ParGram-specific features. In order to compensate for this, we add an additional feature (tense) with finer-grained tense and voice information, using the rules described by Loaiciga et al. (2014). Other additional features did not improve performance, which shows that R&amp;F’s set of features captures the syntactic-semantic in</context>
</contexts>
<marker>Fellbaum, 1999</marker>
<rawString>Christiane Fellbaum. 1999. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="15624" citStr="Fleiss, 1971" startWordPosition="2481" endWordPosition="2482">ls for the first clause in the segment. 10240 (86%) of all pre-segmented clauses received labels for all three tasks from all annotators, who were allowed to skip clauses that do not contain a finite verb. Our gold standard includes an additional 115 segments that did not receive a label by one annotator but were unanimously labeled by the other two. The other segments are disregarded in the experiments. Some of them have expletive subjects, and most others are non-finite verb phrases such as to-infinites or headlines that consist of only a NP. Inter-annotator agreement measured as Fleiss’ n (Fleiss, 1971) on the segments labeled by all three annotators is 0.70, 0.73 and 0.69 for Task NP, Task Cl and Task Cl+NP respectively, indicating substantial agreement (Landis and Koch, 1977). 4 A Sequence Labeling Model for Genericity This section describes our method for identifying generic clauses and NPs in context. We apply the following methods on each of the three different prediction tasks NP, Cl and Cl+NP introduced in Section 3, varying only the type of labels on which we train and test. In contrast to prior work, our computational model integrates not only information from each local instance, b</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annemarie Friedrich</author>
<author>Alexis Palmer</author>
</authors>
<title>Centering Theory in natural text: a large-scale corpus study.</title>
<date>2014</date>
<booktitle>In Proceedings of KONVENS 2014. Universit¨atsbibliothek</booktitle>
<location>Hildesheim.</location>
<contexts>
<context position="10678" citStr="Friedrich and Palmer (2014" startWordPosition="1678" endWordPosition="1681">s are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NPs quantified with every or any as generic. Reiter and Frank (2010) use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic</context>
<context position="16809" citStr="Friedrich and Palmer, 2014" startWordPosition="2680" endWordPosition="2683">y information from each local instance, but also information about the genericity status of surrounding instances. The final labeling for the sequence of instances of an entire document is optimized with regard to these two types of information, which, as we have argued in Section 1, both play a crucial role in determining genericity. The sequences to be labeled contain all clauses or NPs of a document. We also tried labeling sequences for paragraphs instead of documents, but the performance was similar. A reason might be that paragraphs are quite often linked by mentioning the same entities (Friedrich and Palmer, 2014a). Computational model. We use linear chain conditional random fields (Lafferty et al., 2001) to label sequences of mentions or sequences of clauses with regard to their genericity. Conditional random fields (CRFs) are well suited for our labeling task as they do not make an independence assumption between the features. CRFs predict the conditional probability of label sequence y� given an observation sequence x� as follows: n m P(y&amp;quot;|x) = Z(x) exp(E E Aifi(yj−1, yj, 9,j)) j=1 i=1 Z(x) is a normalization constant, the sum over the scores of all possible label sequences for an observation seque</context>
</contexts>
<marker>Friedrich, Palmer, 2014</marker>
<rawString>Annemarie Friedrich and Alexis Palmer. 2014a. Centering Theory in natural text: a large-scale corpus study. In Proceedings of KONVENS 2014. Universit¨atsbibliothek Hildesheim.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annemarie Friedrich</author>
<author>Alexis Palmer</author>
</authors>
<title>Situation entity annotation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop VIII,</booktitle>
<pages>149</pages>
<contexts>
<context position="10678" citStr="Friedrich and Palmer (2014" startWordPosition="1678" endWordPosition="1681">s are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NPs quantified with every or any as generic. Reiter and Frank (2010) use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic</context>
<context position="16809" citStr="Friedrich and Palmer, 2014" startWordPosition="2680" endWordPosition="2683">y information from each local instance, but also information about the genericity status of surrounding instances. The final labeling for the sequence of instances of an entire document is optimized with regard to these two types of information, which, as we have argued in Section 1, both play a crucial role in determining genericity. The sequences to be labeled contain all clauses or NPs of a document. We also tried labeling sequences for paragraphs instead of documents, but the performance was similar. A reason might be that paragraphs are quite often linked by mentioning the same entities (Friedrich and Palmer, 2014a). Computational model. We use linear chain conditional random fields (Lafferty et al., 2001) to label sequences of mentions or sequences of clauses with regard to their genericity. Conditional random fields (CRFs) are well suited for our labeling task as they do not make an independence assumption between the features. CRFs predict the conditional probability of label sequence y� given an observation sequence x� as follows: n m P(y&amp;quot;|x) = Z(x) exp(E E Aifi(yj−1, yj, 9,j)) j=1 i=1 Z(x) is a normalization constant, the sum over the scores of all possible label sequences for an observation seque</context>
</contexts>
<marker>Friedrich, Palmer, 2014</marker>
<rawString>Annemarie Friedrich and Alexis Palmer. 2014b. Situation entity annotation. In Proceedings of the Linguistic Annotation Workshop VIII, page 149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annemarie Friedrich</author>
<author>Alexis Palmer</author>
<author>Melissa Peate Srensen</author>
<author>Manfred Pinkal</author>
</authors>
<title>Annotating genericity: a survey, a scheme, and a corpus.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th Linguistic Annotation Workshop (LAW IX),</booktitle>
<location>Denver, Colorado, US.</location>
<contexts>
<context position="9868" citStr="Friedrich et al., 2015" startWordPosition="1551" endWordPosition="1554">neric in ACE-2. The class also contains mentions of an entity whose identity would be ‘difficult to locate’ (Officials reported ...). Moreover, annotators are asked to mark truly ambiguous cases that have both a generic and a non-generic reading as USP. Finally, NEG (negated) marks negatively quantified entities that refer to the empty set of the kind mentioned. While we agree that in general there are underspecified cases, the guidelines for ACE-2005 mix other phenomena into the USP class, resulting in a high confusion between USP and both of the labels SPC and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagge</context>
<context position="13833" citStr="Friedrich et al., 2015" startWordPosition="2175" endWordPosition="2178">Ps only. We are aware that we are missing relevant cases (e.g. the less preferred reading 1The WikiGenerics corpus is freely available at: www.coli.uni-saarland.de/projects/sitent 1274 of Cats chase mice, which attributes to mice the property of being chased by cats), but in this work, we want to study the “easier” subject cases as a first step. We use the discourse parser SPADE (Soricut and Marcu, 2003) to automatically segment the first 70 sentences of each article into clauses. Each clause is manually annotated with the following information (for more details on the annotation scheme, see (Friedrich et al., 2015)): • Task NP: whether or not the subject NP of the clause refers to a class or kind (generic vs. nongeneric); • Task Cl: whether the clause is generic, defined as a clause that makes a characterizing statement about a class or kind, or non-generic. • Task Cl+NP: using the information from Task NP and Cl above, we automatically derive the following classification for each clause (compare to the explanation of example (2)). – GEN gen: generic clause, subject is generic by definition (The lion is a predatory cat); – NON-GEN non-gen: non-generic clause with a non-generic subject ( Simba roared); –</context>
</contexts>
<marker>Friedrich, Palmer, Srensen, Pinkal, 2015</marker>
<rawString>Annemarie Friedrich, Alexis Palmer, Melissa Peate Srensen, and Manfred Pinkal. 2015. Annotating genericity: a survey, a scheme, and a corpus. In Proceedings of the 9th Linguistic Annotation Workshop (LAW IX), Denver, Colorado, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="24989" citStr="Hall et al., 2009" startWordPosition="4044" endWordPosition="4047">ditional features did not improve performance, which shows that R&amp;F’s set of features captures the syntactic-semantic information relevant to genericity classification quite well. Therefore, we use this feature set also for the sequence labeling model. Using the same feature set allows us to attribute any performance gain to the context4The rest of the data comprise broadcast conversation, weblog and forum texts as well as transcribed conversational telephone, and would require specialized preprocessing. awareness of our model rather than the features. R&amp;F train a Bayesian network using Weka (Hall et al., 2009). The decisions of this classifier are local to each clause. They report the performance of their system on the ACE-2 corpus: Table 2 shows that the performance of our reimplemented feature set5 is comparable to the system of R&amp;F.6 In all other other tables, “BayesNet R&amp;F” refers to our re-implemented system. R&amp;F present the “Person baseline” as a simple informed baseline (see Table 2). We trained a J48 decision tree on this feature alone, which confirmed that only second-person mentions (the generic “you”) are classified as generic, while all other mentions are classified as non-generic. 5.3 </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurelie Herbelot</author>
<author>Ann Copestake</author>
</authors>
<title>Annotating genericity: How do humans decide? (A case study in ontology extraction).</title>
<date>2009</date>
<booktitle>Studies in Generative Grammar 101,</booktitle>
<pages>103</pages>
<contexts>
<context position="10386" citStr="Herbelot and Copestake, 2009" startWordPosition="1634" endWordPosition="1638">igh confusion between USP and both of the labels SPC and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identifica</context>
</contexts>
<marker>Herbelot, Copestake, 2009</marker>
<rawString>Aurelie Herbelot and Ann Copestake. 2009. Annotating genericity: How do humans decide? (A case study in ontology extraction). Studies in Generative Grammar 101, page 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>3--10</pages>
<contexts>
<context position="19179" citStr="Klein and Manning, 2002" startWordPosition="3069" endWordPosition="3072">alse perfective true, false passive true, false temporal modifier true, false number of modifiers numeric part-of-speech POS of head predicate lemma of head adjunct-degree positive, comparative, superlative adjunct-pred lemma of adverbial clauses’ head Table 1: Features. WN=WordNet. Feature functions. We extract the set of features listed in Table 1 for each instance. This set of features is inspired by Reiter and Frank (2010), see also Section 5.2. In the case of the WikiGenerics corpus, the NP features are extracted for the subject of the clause. We parse the data using the Stanford parser (Klein and Manning, 2002) and obtain the subject NPs from the collapsed dependencies. For the ACE data, the NP features are extracted for all mentions in the gold standard and the clause features are extracted from the clause in which the mention appears. Our feature functions fi(yj, xj) are indicator functions combining the current label and one of the feature values of the current mention or clause, for example: f = if (yj = GENERIC and xj.np.person=3) return 1 else return 0 We create two versions of the CRF model: the bigram3 model additionally uses indicator functions f(yj−1, yj) for each combination of labels, th</context>
<context position="24104" citStr="Klein and Manning, 2002" startWordPosition="3909" endWordPosition="3912">Classifier The system for identifying generic NPs of Reiter and Frank (2010), henceforth R&amp;F, makes use of the English ParGram LFG grammar for the XLE parser (Butt et al., 2002). As this grammar is not publicly available, we implement a similar system using exclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect-related ParGram-specific features. In order to compensate for this, we add an additional feature (tense) with finer-grained tense and voice information, using the rules described by Loaiciga et al. (2014). Other additional features did not improve performance, which shows that R&amp;F’s set of features captures the syntactic-semantic information relevant to genericity classification quite well. Therefore, we use this feature set also for the sequence labeling model. Using the same feature set allows us to attribute any performance gain to the co</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Advances in neural information processing systems, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Krifka</author>
<author>Francis Jeffrey Pelletier</author>
<author>Gregory N Carlson</author>
</authors>
<title>Alice ter Meulen, Godehard Link, and Gennaro Chierchia.</title>
<date>1995</date>
<pages>1--124</pages>
<contexts>
<context position="4232" citStr="Krifka et al. (1995)" startWordPosition="642" endWordPosition="645">or arbitrary members of a class. Although genericity on the clause- and NP-level 1272 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1272–1281, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics are strongly interrelated, the concepts do not always coincide. As example (2) shows, sentences describing episodic events can have a generic NP as their subject. Note that references to species are kind-referring / generic on the NP level (following Krifka et al. (1995), seep. 65). (2) In September 2013 the blobfish was voted the “World’s Ugliest Animal”. (subject generic, clause non-generic) Genericity often cannot be annotated without paying attention to the wider discourse context. Clearly, coreference information is needed for the genericity classification of pronouns. Often, even genericity of full NPs or entire clauses cannot be decided in isolation, as illustrated by example (3). Sentence (b) could be part of a particular narrative about a tree, or it could be a generic statement. Only the context given by (a) clarifies that (b) indeed makes reference</context>
<context position="7721" citStr="Krifka et al. (1995)" startWordPosition="1203" endWordPosition="1206"> of entities, and thus are similar to universally quantified sentences in their truth conditions and entailment properties. However, their truth-conditional interpretation is tricky, since they express typicality, describe stereotypes and allow exceptions, for example Dutchmen are good sailors is not false even if most Dutchmen do not sail at all (Carlson, 1977). We concentrate on the decision of whether a clause is generic or not, and leave the truth-conditional interpretation for further work. For a detailed discussion of the semantics of generics expressions see the comprehensive survey by Krifka et al. (1995); a short and instructive overview can be found in the first part of (Reiter and Frank, 2010). 2 Related Work In this section, we first briefly review previously developed annotation schemes for genericity. We then describe work on automatically predicting the genericity of NPs or different types of clauses. Annotation. ACE-2 (Mitchell et al., 2003) and ACE-2005 (Walker et al., 2006) are the two most notable annotation projects for labeling genericity of NPs to date. In the ACE-2 corpus, 40106 entity mentions in 520 newswire and broadcast documents are marked with regard to whether they refer </context>
</contexts>
<marker>Krifka, Pelletier, Carlson, 1995</marker>
<rawString>Manfred Krifka, Francis Jeffrey Pelletier, Gregory N. Carlson, Alice ter Meulen, Godehard Link, and Gennaro Chierchia. 1995. Genericity: An Introduction. The Generic Book, pages 1–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="16903" citStr="Lafferty et al., 2001" startWordPosition="2693" endWordPosition="2696">nding instances. The final labeling for the sequence of instances of an entire document is optimized with regard to these two types of information, which, as we have argued in Section 1, both play a crucial role in determining genericity. The sequences to be labeled contain all clauses or NPs of a document. We also tried labeling sequences for paragraphs instead of documents, but the performance was similar. A reason might be that paragraphs are quite often linked by mentioning the same entities (Friedrich and Palmer, 2014a). Computational model. We use linear chain conditional random fields (Lafferty et al., 2001) to label sequences of mentions or sequences of clauses with regard to their genericity. Conditional random fields (CRFs) are well suited for our labeling task as they do not make an independence assumption between the features. CRFs predict the conditional probability of label sequence y� given an observation sequence x� as follows: n m P(y&amp;quot;|x) = Z(x) exp(E E Aifi(yj−1, yj, 9,j)) j=1 i=1 Z(x) is a normalization constant, the sum over the scores of all possible label sequences for an observation sequence with the length of Y. The weights Ai of the feature functions are the parameters to be lea</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data. biometrics,</title>
<date>1977</date>
<pages>159--174</pages>
<contexts>
<context position="15802" citStr="Landis and Koch, 1977" startWordPosition="2509" endWordPosition="2512">s that do not contain a finite verb. Our gold standard includes an additional 115 segments that did not receive a label by one annotator but were unanimously labeled by the other two. The other segments are disregarded in the experiments. Some of them have expletive subjects, and most others are non-finite verb phrases such as to-infinites or headlines that consist of only a NP. Inter-annotator agreement measured as Fleiss’ n (Fleiss, 1971) on the segments labeled by all three annotators is 0.70, 0.73 and 0.69 for Task NP, Task Cl and Task Cl+NP respectively, indicating substantial agreement (Landis and Koch, 1977). 4 A Sequence Labeling Model for Genericity This section describes our method for identifying generic clauses and NPs in context. We apply the following methods on each of the three different prediction tasks NP, Cl and Cl+NP introduced in Section 3, varying only the type of labels on which we train and test. In contrast to prior work, our computational model integrates not only information from each local instance, but also information about the genericity status of surrounding instances. The final labeling for the sequence of instances of an entire document is optimized with regard to these</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Godehard Link</author>
</authors>
<title>Generic information and dependent generics. The Generic Book,</title>
<date>1995</date>
<pages>358--382</pages>
<contexts>
<context position="13175" citStr="Link (1995)" startWordPosition="2072" endWordPosition="2073">nitions such as the rules of a football game (The offensive team must line up in a legal formation before they can snap the ball). Generic clauses have the typical form of a predicative statement about the sentence topic, which is normally realized as the grammatical subject in English. Intuitions about NP-level genericity and its relation to clause-level genericity are quite reliable for topic NPs of clauses, which also typically occur in subject position in English. Since generics in non-subject positions are less frequent and hard to interpret (see the discussion of “dependent generics” by Link (1995)), we decided to annotate subject NPs only. We are aware that we are missing relevant cases (e.g. the less preferred reading 1The WikiGenerics corpus is freely available at: www.coli.uni-saarland.de/projects/sitent 1274 of Cats chase mice, which attributes to mice the property of being chased by cats), but in this work, we want to study the “easier” subject cases as a first step. We use the discourse parser SPADE (Soricut and Marcu, 2003) to automatically segment the first 70 sentences of each article into clauses. Each clause is manually annotated with the following information (for more deta</context>
</contexts>
<marker>Link, 1995</marker>
<rawString>Godehard Link. 1995. Generic information and dependent generics. The Generic Book, pages 358–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharid Loaiciga</author>
<author>Thomas Meyer</author>
<author>Andrei PopescuBelis</author>
</authors>
<title>English-French Verb Phrase Alignment in Europarl.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="24361" citStr="Loaiciga et al. (2014)" startWordPosition="3948" endWordPosition="3951">xclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect-related ParGram-specific features. In order to compensate for this, we add an additional feature (tense) with finer-grained tense and voice information, using the rules described by Loaiciga et al. (2014). Other additional features did not improve performance, which shows that R&amp;F’s set of features captures the syntactic-semantic information relevant to genericity classification quite well. Therefore, we use this feature set also for the sequence labeling model. Using the same feature set allows us to attribute any performance gain to the context4The rest of the data comprise broadcast conversation, weblog and forum texts as well as transcribed conversational telephone, and would require specialized preprocessing. awareness of our model rather than the features. R&amp;F train a Bayesian network us</context>
</contexts>
<marker>Loaiciga, Meyer, PopescuBelis, 2014</marker>
<rawString>Sharid Loaiciga, Thomas Meyer, and Andrei PopescuBelis. 2014. English-French Verb Phrase Alignment in Europarl. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="23803" citStr="Manning et al., 2014" startWordPosition="3857" endWordPosition="3860">adcast news subsections.4 Due to low frequency, we omit instances of NEG in our experiments, and apply a three-way classification task (GEN, SPC, USP). We present results for all remaining 40106 mentions and for the subset of 18029 subject mentions, each time using 10-fold CV. 5.2 Baseline: Local Classifier The system for identifying generic NPs of Reiter and Frank (2010), henceforth R&amp;F, makes use of the English ParGram LFG grammar for the XLE parser (Butt et al., 2002). As this grammar is not publicly available, we implement a similar system using exclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect-related ParGram-specific features. In order to compensate for this, we add an additional feature (tense) with finer-grained tense and voice information, using the rules described by Loaiciga et al. (2014). Other additional features did not improv</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas A Mathew</author>
<author>E Graham Katz</author>
</authors>
<title>Supervised Categorization of Habitual and Episodic Sentences.</title>
<date>2009</date>
<booktitle>In Sixth Midwest Computational Linguistics Colloquium,</booktitle>
<location>Bloomington, Indiana: Indiana University.</location>
<contexts>
<context position="36546" citStr="Mathew and Katz, 2009" startWordPosition="5971" endWordPosition="5974">g generic expressions. This in turn lays foundations for natural language processing tasks requiring text understanding. Future Work. Our present approach for annotating and automatically classifying targets the subjects of each clause. We have not attempted to tackle the task of classifying the genericity status of other dependents, as they are even harder to classify than subjects, and a concise annotation scheme has to be worked out in order achieve an acceptable inter-annotator agreement on this task. Another related distinction is the one between habitual, stative and episodic sentences (Mathew and Katz, 2009), which applies to both what we call generic and non-generic sentences. No large corpora exist to date, but studying the interaction of these phenomena is on our research agenda. Acknowledgments We thank the anonymous reviewers, Alexis Palmer, Nils Reiter and Melissa Peate Sørensen for their helpful comments related to this work, and our annotators Christine Bocionek and KleoIsidora Mavridou. This research was supported in part by the Cluster of Excellence “Multimodal Computing and Interaction” of the German Excellence Initiative (DFG), and the first author is supported by an IBM PhD Fellowshi</context>
</contexts>
<marker>Mathew, Katz, 2009</marker>
<rawString>Thomas A. Mathew and E. Graham Katz. 2009. Supervised Categorization of Habitual and Episodic Sentences. In Sixth Midwest Computational Linguistics Colloquium, Bloomington, Indiana: Indiana University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<contexts>
<context position="32352" citStr="McCallum, 2002" startWordPosition="5298" endWordPosition="5299"> corpora, oracle information is needed to show the benefit of using bigram feature functions: (a) The frequency of GEN mentions in the ACE corpora is low – news contains only little generic information, so the context information is harder to leverage. (b) The ACE annotation guidelines contain some vagueness (see Section 3); this makes it harder for an automatic system to learn about regularities. Higher-order Markov models. Another research question is whether models incorporating not only the previous label, but more preceding labels would perform even better. We turn to the Mallet toolkit (McCallum, 2002), whose CRF implementation allows for using higher-order models.7 For example, an order-2 model considers the two previous labels. We use L1-regularization during training. Figure 1 shows that the optimum is reached for order-1 (bigram) models for each of the classification tasks for accuracy, the same ten7The CRF++ toolkit, which we use in all other experiments, does not allow for higher-order models. We use CRF++ in the main experiments as it comes with a concise documentation; this helps to make our experiments easily replicable. Figure 1: Labeling results for CRF models of various orders o</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K McCallum. 2002. MALLET: A Machine Learning for Language Toolkit.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexis Mitchell</author>
<author>Stephanie Strassel</author>
<author>Mark Przybocki</author>
<author>JK Davis</author>
</authors>
<location>George Doddington, Ralph Grishman, Adam Meyers, Ada Brunstein, Lisa Ferro, and</location>
<marker>Mitchell, Strassel, Przybocki, Davis, </marker>
<rawString>Alexis Mitchell, Stephanie Strassel, Mark Przybocki, JK Davis, George Doddington, Ralph Grishman, Adam Meyers, Ada Brunstein, Lisa Ferro, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Sundheim</author>
</authors>
<date>2003</date>
<booktitle>ACE-2 Version 1.0 LDC2003T11. Philadelphia: Linguistic Data Consortium.</booktitle>
<marker>Sundheim, 2003</marker>
<rawString>Beth Sundheim. 2003. ACE-2 Version 1.0 LDC2003T11. Philadelphia: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Nedoluzhko</author>
</authors>
<title>Generic noun phrases and annotation of coreference and bridging relations in the Prague Dependency Treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>103--111</pages>
<contexts>
<context position="10503" citStr="Nedoluzhko (2013)" startWordPosition="1655" endWordPosition="1656">notators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NP</context>
<context position="34493" citStr="Nedoluzhko, 2013" startWordPosition="5646" endWordPosition="5647">reference resolution system (Raghunathan et al., 2010). We replace the NP features of each pronominal instance with the features of the first link of the coreference chain. We did not obtain a significant performance gain. One reason is that this change of features only applies to about 13% of the data. We observe that any positive changes in the classification go along with some negative changes which were often due to coreference resolution errors. One difficult step in manually annotating, and hence also in automatically resolving coreference is to determine whether a NP is generic or not (Nedoluzhko, 2013). The task of identifying generic NPs and 0 (unigram)1 (bigram) 2 Markov order Task NP Task Cl Task Cl+NP Accuracy % 82 80 78 76 74 72 1279 coreference resolution are intertwined. We plan to manually annotate at least part of our corpus with coreference information in order to test to what extent the classification of the pronouns’ genericity status can profit from including antecedent information. 6 Conclusion We have presented a novel method for labeling sequences of clauses or their subjects with regard to their genericity, showing that genericity should be treated as a discourse-sensitive </context>
</contexts>
<marker>Nedoluzhko, 2013</marker>
<rawString>Anna Nedoluzhko. 2013. Generic noun phrases and annotation of coreference and bridging relations in the Prague Dependency Treebank. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Palmer</author>
<author>Elias Ponvert</author>
<author>Jason Baldridge</author>
<author>Carlota Smith</author>
</authors>
<title>A sequencing model for situation entity classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>896</pages>
<contexts>
<context position="11401" citStr="Palmer et al. (2007)" startWordPosition="1791" endWordPosition="1794">infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NPs quantified with every or any as generic. Reiter and Frank (2010) use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic NPs. We compare to their method (described in detail in Section 5.2) as a highlycompetitive baseline. Palmer et al. (2007) classify clauses into several types of situation entities including states, events, generalizing sentences (habitual utterances referring to specific individuals) and generic sentences. They find that using context by using the labels of preceding clauses as features improves the classification of clause types, but generic sentences are extremely sparse in their data set. Our present approach uses a sequence labeling model that computes the best labeling for an entire sequence. 3 WikiGenerics: Data and Annotations In order to study generics in a genre other than news (as in ACE), we turn to a</context>
</contexts>
<marker>Palmer, Ponvert, Baldridge, Smith, 2007</marker>
<rawString>Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith. 2007. A sequencing model for situation entity classification. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, page 896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Ron Artstein</author>
</authors>
<title>Anaphoric Annotation in the ARRAU Corpus.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="10435" citStr="Poesio and Artstein, 2008" startWordPosition="1643" endWordPosition="1646"> and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a r</context>
</contexts>
<marker>Poesio, Artstein, 2008</marker>
<rawString>Massimo Poesio and Ron Artstein. 2008. Anaphoric Annotation in the ARRAU Corpus. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>Discourse annotation and semantic annotation in the GNOME corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACL Workshop on Discourse Annotation,</booktitle>
<pages>72--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10355" citStr="Poesio, 2004" startWordPosition="1632" endWordPosition="1633">sulting in a high confusion between USP and both of the labels SPC and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which ar</context>
</contexts>
<marker>Poesio, 2004</marker>
<rawString>Massimo Poesio. 2004. Discourse annotation and semantic annotation in the GNOME corpus. In Proceedings of the 2004 ACL Workshop on Discourse Annotation, pages 72–79. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>492--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33930" citStr="Raghunathan et al., 2010" startWordPosition="5548" endWordPosition="5551">% of all pronouns are marked as generic and 46% are marked as non-generic, which shows that there is no preference for pronouns to occur with either class. Some of the features (countability, noun type, determiner type, bare plural, and the WordNet related features) are not informative when applied to personal or relative pronouns. Sometimes, it is not even possible to determine number without referring to the antecedent (e.g., in the case of the relative pronoun ‘who’). We conduct the following experiment: we automatically resolve coreference using the Stanford coreference resolution system (Raghunathan et al., 2010). We replace the NP features of each pronominal instance with the features of the first link of the coreference chain. We did not obtain a significant performance gain. One reason is that this change of features only applies to about 13% of the data. We observe that any positive changes in the classification go along with some negative changes which were often due to coreference resolution errors. One difficult step in manually annotating, and hence also in automatically resolving coreference is to determine whether a NP is generic or not (Nedoluzhko, 2013). The task of identifying generic NPs</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492–501. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Reiter</author>
<author>Anette Frank</author>
</authors>
<title>Identifying Generic Noun Phrases.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2745" citStr="Reiter and Frank (2010)" startWordPosition="411" endWordPosition="414">hrough the improvement of textual entailment methods, and for systems acquiring machine-readable knowledge from text. Machinereadable knowledge bases have different representations for statements corresponding to generic knowledge about kinds and knowledge about specific individuals. The non-generic sentence (1b) roughly speaking provides ABox content for a machine-readable knowledge base, i.e., knowledge about particular instances, e.g, “A is an instance of B / has property X”. In contrast, the generic sentence (1a) feeds the TBox, i.e., knowledge of the form “All B are C / have property X”. Reiter and Frank (2010) provide a detailed discussion of the relevance of the distinction between classes and instances for automatic ontology construction. In this paper, we present a new corpus annotated in a linguistically motivated way for genericity, and a context-sensitive computational model for labeling sequences of clauses or noun phrases (NPs) with their genericity status. Both manual annotation and automatic recognition of generic expressions are challenging tasks: virtually all NP types – definites, indefinites and quantified NPs, full NPs, pronouns, and even proper names (e.g. species names such as Elep</context>
<context position="5306" citStr="Reiter and Frank, 2010" startWordPosition="816" endWordPosition="819">t of a particular narrative about a tree, or it could be a generic statement. Only the context given by (a) clarifies that (b) indeed makes reference to any year’s new twigs and is to be interpreted as generic. (3) (a) Sugar maples also have a tendency to color unevenly in fall. (generic) (b) The recent year’s growth twigs are green and turn dark brown. (generic) In computational linguistics, most research on detecting genericity has been done in relation to the ACE corpora (Mitchell et al., 2003; Walker et al., 2006), focusing on assigning genericity labels to noun phrases (Suh et al., 2006; Reiter and Frank, 2010), see Section 2. Our work is based on these approaches, most notably on the work of Reiter and Frank (2010), and extends upon them in the following essential ways. The major contributions of this work are: (1) We create a new corpus of Wikipedia articles annotated with linguistically motivated genericity labels both on the subject- and clause-level (see Section 3). The corpus is balanced with respect to genericity and about 10,000 clauses in size. (2) We present a discourse-sensitive genericity labeler. Technically, we use conditional random fields as a sequence labeling method (Section 4). We</context>
<context position="7814" citStr="Reiter and Frank, 2010" startWordPosition="1221" endWordPosition="1225">itions and entailment properties. However, their truth-conditional interpretation is tricky, since they express typicality, describe stereotypes and allow exceptions, for example Dutchmen are good sailors is not false even if most Dutchmen do not sail at all (Carlson, 1977). We concentrate on the decision of whether a clause is generic or not, and leave the truth-conditional interpretation for further work. For a detailed discussion of the semantics of generics expressions see the comprehensive survey by Krifka et al. (1995); a short and instructive overview can be found in the first part of (Reiter and Frank, 2010). 2 Related Work In this section, we first briefly review previously developed annotation schemes for genericity. We then describe work on automatically predicting the genericity of NPs or different types of clauses. Annotation. ACE-2 (Mitchell et al., 2003) and ACE-2005 (Walker et al., 2006) are the two most notable annotation projects for labeling genericity of NPs to date. In the ACE-2 corpus, 40106 entity mentions in 520 newswire and broadcast documents are marked with regard to whether they refer to “any member of the set in question” (GEN, generic) rather than “some particular, identifia</context>
<context position="11169" citStr="Reiter and Frank (2010)" startWordPosition="1752" endWordPosition="1755">na within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NPs quantified with every or any as generic. Reiter and Frank (2010) use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic NPs. We compare to their method (described in detail in Section 5.2) as a highlycompetitive baseline. Palmer et al. (2007) classify clauses into several types of situation entities including states, events, generalizing sentences (habitual utterances referring to specific individuals) and generic sentences. They find that using context by using the labels of preceding clauses as features improves the classification of clause types, but generic sentences are extremely sparse in their da</context>
<context position="18985" citStr="Reiter and Frank (2010)" startWordPosition="3034" endWordPosition="3037">D FEATURES dependency [0 − 4] dependency relation between head and governor etc. tense tense, aspect and voice information, e.g. pres perf active coarseTense pres, past, fut progressive true, false perfective true, false passive true, false temporal modifier true, false number of modifiers numeric part-of-speech POS of head predicate lemma of head adjunct-degree positive, comparative, superlative adjunct-pred lemma of adverbial clauses’ head Table 1: Features. WN=WordNet. Feature functions. We extract the set of features listed in Table 1 for each instance. This set of features is inspired by Reiter and Frank (2010), see also Section 5.2. In the case of the WikiGenerics corpus, the NP features are extracted for the subject of the clause. We parse the data using the Stanford parser (Klein and Manning, 2002) and obtain the subject NPs from the collapsed dependencies. For the ACE data, the NP features are extracted for all mentions in the gold standard and the clause features are extracted from the clause in which the mention appears. Our feature functions fi(yj, xj) are indicator functions combining the current label and one of the feature values of the current mention or clause, for example: f = if (yj = </context>
<context position="20362" citStr="Reiter and Frank (2010)" startWordPosition="3273" endWordPosition="3276">, yj) for each combination of labels, thus taking context into account. The unigram model does not use these feature functions, it is thus similar to a maximum entropy model (with a different normalization). Log-linear models work very well for many NLP tasks, especially if features are correlated as it is the case here, so in or3Following CRF++ terminology. der to get a fair estimate of the impact of using the context (via the transition feature functions), we give numbers for this ‘unigram’ model in addition, rather than simply comparing the bigramCRF to a Bayesian network, which is used by Reiter and Frank (2010). Using more complex feature functions did not result in significant performance gains, so we chose the simplest model. Note that even though the feature functions only formulate relationships between adjacent labels in the sequence, the optimal labeling is computed for the entire sequence: the choices of labels assigned to non-adjacent clauses do influence each other. Two-step Approach for Task Cl+NP. Task Cl+NP can be regarded as a combination of the two decisions made in Task NP and Task Cl. Therefore, we approach Task Cl+NP in two ways. (a) We train a CRF which directly outputs the three l</context>
<context position="23556" citStr="Reiter and Frank (2010)" startWordPosition="3817" endWordPosition="3820">l+NP. For the experiments on WikiGenerics, we use leave-one-document-out CV, i.e., we train on 101 of the 102 documents and test on the remaining document in each fold. The total number of clauses is 10355. From ACE-2005, we use the newswire and broadcast news subsections.4 Due to low frequency, we omit instances of NEG in our experiments, and apply a three-way classification task (GEN, SPC, USP). We present results for all remaining 40106 mentions and for the subset of 18029 subject mentions, each time using 10-fold CV. 5.2 Baseline: Local Classifier The system for identifying generic NPs of Reiter and Frank (2010), henceforth R&amp;F, makes use of the English ParGram LFG grammar for the XLE parser (Butt et al., 2002). As this grammar is not publicly available, we implement a similar system using exclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect</context>
<context position="30062" citStr="Reiter and Frank (2010)" startWordPosition="4918" endWordPosition="4921">res at a time. Clause-based features are more important than the NP-based features for all three classification tasks. An interesting observation is that the NP features alone are not able to separate the infrequent class NONGEN gen from the other two at all, the F1-score of 2.5 shows that almost all instances of this class were labeled as one of the other two classes. In sum, this shows that whether an NP is interpreted as generic or not strongly depends on how it is used in the clause. Task NP, ACE. Both on ACE-2 (see Table 5) and on ACE-2005 (see Table 6), the CRF outperforms the system of Reiter and Frank (2010) in terms of accuracy, and has a higher F1-score. We give results also for subjects only as this parallels the setting of the WikiGenerics experiments (reasons for the restriction to subjects were given in Section 3). For subjects, the majority class SPC is less frequent (compare the accuracies of the two majority class baselines); only 7% of the subjects are marked as GEN, the rest are labeled as USP. The bigram model does not outperform the unigram model, but our oracle experiments show that context information is indeed useful: accuracy increases significantly and F1 increases considerably,</context>
</contexts>
<marker>Reiter, Frank, 2010</marker>
<rawString>Nils Reiter and Anette Frank. 2010. Identifying Generic Noun Phrases. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 40–49, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>149--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13617" citStr="Soricut and Marcu, 2003" startWordPosition="2142" endWordPosition="2145">cally occur in subject position in English. Since generics in non-subject positions are less frequent and hard to interpret (see the discussion of “dependent generics” by Link (1995)), we decided to annotate subject NPs only. We are aware that we are missing relevant cases (e.g. the less preferred reading 1The WikiGenerics corpus is freely available at: www.coli.uni-saarland.de/projects/sitent 1274 of Cats chase mice, which attributes to mice the property of being chased by cats), but in this work, we want to study the “easier” subject cases as a first step. We use the discourse parser SPADE (Soricut and Marcu, 2003) to automatically segment the first 70 sentences of each article into clauses. Each clause is manually annotated with the following information (for more details on the annotation scheme, see (Friedrich et al., 2015)): • Task NP: whether or not the subject NP of the clause refers to a class or kind (generic vs. nongeneric); • Task Cl: whether the clause is generic, defined as a clause that makes a characterizing statement about a class or kind, or non-generic. • Task Cl+NP: using the information from Task NP and Cl above, we automatically derive the following classification for each clause (co</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 149–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangweon Suh</author>
<author>Harry Halpin</author>
<author>Ewan Klein</author>
</authors>
<title>Extracting common sense knowledge from wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at ISWC,</booktitle>
<volume>6</volume>
<contexts>
<context position="5281" citStr="Suh et al., 2006" startWordPosition="812" endWordPosition="815">e (b) could be part of a particular narrative about a tree, or it could be a generic statement. Only the context given by (a) clarifies that (b) indeed makes reference to any year’s new twigs and is to be interpreted as generic. (3) (a) Sugar maples also have a tendency to color unevenly in fall. (generic) (b) The recent year’s growth twigs are green and turn dark brown. (generic) In computational linguistics, most research on detecting genericity has been done in relation to the ACE corpora (Mitchell et al., 2003; Walker et al., 2006), focusing on assigning genericity labels to noun phrases (Suh et al., 2006; Reiter and Frank, 2010), see Section 2. Our work is based on these approaches, most notably on the work of Reiter and Frank (2010), and extends upon them in the following essential ways. The major contributions of this work are: (1) We create a new corpus of Wikipedia articles annotated with linguistically motivated genericity labels both on the subject- and clause-level (see Section 3). The corpus is balanced with respect to genericity and about 10,000 clauses in size. (2) We present a discourse-sensitive genericity labeler. Technically, we use conditional random fields as a sequence labeli</context>
<context position="11023" citStr="Suh et al. (2006)" startWordPosition="1728" endWordPosition="1731">s (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NPs quantified with every or any as generic. Reiter and Frank (2010) use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic NPs. We compare to their method (described in detail in Section 5.2) as a highlycompetitive baseline. Palmer et al. (2007) classify clauses into several types of situation entities including states, events, generalizing sentences (habitual utterances referring to specific individuals) and generic sentences. They find that using context by usi</context>
</contexts>
<marker>Suh, Halpin, Klein, 2006</marker>
<rawString>Sangweon Suh, Harry Halpin, and Ewan Klein. 2006. Extracting common sense knowledge from wikipedia. In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at ISWC, volume 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangweon Suh</author>
</authors>
<title>Extracting Generic Statements for the Semantic Web. Master’s thesis,</title>
<date>2006</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="10167" citStr="Suh (2006)" startWordPosition="1607" endWordPosition="1608">s that refer to the empty set of the kind mentioned. While we agree that in general there are underspecified cases, the guidelines for ACE-2005 mix other phenomena into the USP class, resulting in a high confusion between USP and both of the labels SPC and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which th</context>
</contexts>
<marker>Suh, 2006</marker>
<rawString>Sangweon Suh. 2006. Extracting Generic Statements for the Semantic Web. Master’s thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Walker</author>
<author>Stephanie Strassel</author>
<author>Julie Medero</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2006</date>
<booktitle>ACE 2005 Multilingual Training Corpus LDC2006T06. Philadelphia: Linguistic Data Consortium.</booktitle>
<contexts>
<context position="5206" citStr="Walker et al., 2006" startWordPosition="799" endWordPosition="802"> clauses cannot be decided in isolation, as illustrated by example (3). Sentence (b) could be part of a particular narrative about a tree, or it could be a generic statement. Only the context given by (a) clarifies that (b) indeed makes reference to any year’s new twigs and is to be interpreted as generic. (3) (a) Sugar maples also have a tendency to color unevenly in fall. (generic) (b) The recent year’s growth twigs are green and turn dark brown. (generic) In computational linguistics, most research on detecting genericity has been done in relation to the ACE corpora (Mitchell et al., 2003; Walker et al., 2006), focusing on assigning genericity labels to noun phrases (Suh et al., 2006; Reiter and Frank, 2010), see Section 2. Our work is based on these approaches, most notably on the work of Reiter and Frank (2010), and extends upon them in the following essential ways. The major contributions of this work are: (1) We create a new corpus of Wikipedia articles annotated with linguistically motivated genericity labels both on the subject- and clause-level (see Section 3). The corpus is balanced with respect to genericity and about 10,000 clauses in size. (2) We present a discourse-sensitive genericity </context>
<context position="8107" citStr="Walker et al., 2006" startWordPosition="1266" endWordPosition="1269">e decision of whether a clause is generic or not, and leave the truth-conditional interpretation for further work. For a detailed discussion of the semantics of generics expressions see the comprehensive survey by Krifka et al. (1995); a short and instructive overview can be found in the first part of (Reiter and Frank, 2010). 2 Related Work In this section, we first briefly review previously developed annotation schemes for genericity. We then describe work on automatically predicting the genericity of NPs or different types of clauses. Annotation. ACE-2 (Mitchell et al., 2003) and ACE-2005 (Walker et al., 2006) are the two most notable annotation projects for labeling genericity of NPs to date. In the ACE-2 corpus, 40106 entity mentions in 520 newswire and broadcast documents are marked with regard to whether they refer to “any member of the set in question” (GEN, generic) rather than “some particular, identifiable member of that set” (SPC, specific/non-generic). The major drawback of ACE-2 is that genericity is basically defined as lack of specificity, which leads 1273 to uncertainty and inconsistencies in the annotation process, and to a heterogeneous set of NPs labeled with GEN, including quantif</context>
</contexts>
<marker>Walker, Strassel, Medero, Maeda, 2006</marker>
<rawString>Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. ACE 2005 Multilingual Training Corpus LDC2006T06. Philadelphia: Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>