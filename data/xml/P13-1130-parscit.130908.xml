<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019635">
<title confidence="0.982313">
Language Acquisition and Probabilistic Models: keeping it simple
</title>
<author confidence="0.999038">
Aline Villavicencio♣, Marco Idiart♥ Robert Berwick♦, Igor Malioutov♠
</author>
<affiliation confidence="0.965315">
♣Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
♥Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
♦LIDS, Dept. of EECS, Massachusetts Institute of Technology (USA)
</affiliation>
<address confidence="0.788269">
♠ CSAIL, Dept. of EECS, Massachusetts Institute of Technology (USA)
</address>
<email confidence="0.994302">
avillavicencio@inf.ufrgs.br, marco.idiart@if.ufrgs.br
berwick@csail.mit.edu, igorm@mit.edu
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999814785714286">
Hierarchical Bayesian Models (HBMs)
have been used with some success
to capture empirically observed pat-
terns of under- and overgeneralization
in child language acquisition. How-
ever, as is well known, HBMs are
“ideal” learning systems, assuming ac-
cess to unlimited computational re-
sources that may not be available
to child language learners. Conse-
quently, it remains crucial to carefully
assess the use of HBMs along with al-
ternative, possibly simpler, candidate
models. This paper presents such
an evaluation for a language acquisi-
tion domain where explicit HBMs have
been proposed: the acquisition of En-
glish dative constructions. In particu-
lar, we present a detailed, empirically-
grounded model-selection compari-
son of HBMs vs. a simpler alternative
based on clustering along with max-
imum likelihood estimation that we
call linear competition learning (LCL).
Our results demonstrate that LCL can
match HBM model performance with-
out incurring on the high computa-
tional costs associated with HBMs.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986340909091">
In recent years, with advances in probability
and estimation theory, there has been much
interest in Bayesian models (BMs) (Chater,
Tenenbaum, and Yuille, 2006; Jones and
Love, 2011) and their application to child lan-
guage acquisition with its challenging com-
bination of structured information and in-
complete knowledge, (Perfors, Tenenbaum,
and Wonnacott, 2010; Hsu and Chater, 2010;
Parisien, Fazly, and Stevenson, 2008; Parisien
and Stevenson, 2010) as they offer several ad-
vantages in this domain. They can readily
handle the evident noise and ambiguity of ac-
quisition input, while at the same time pro-
viding efficiency via priors that mirror known
pre-existing language biases. Further, hierar-
chical Bayesian Models (HBMs) can combine
distinct abstraction levels of linguistic knowl-
edge, from variation at the level of individ-
ual lexical items, to cross-item variation, using
hyper-parameters to capture observed pat-
terns of both under- and over-generalization
as in the acquisition of e.g. dative alterna-
tions in English (Hsu and Chater, 2010; Per-
fors, Tenenbaum, and Wonnacott, 2010), and
verb frames in a controlled artificial language
(Wonnacott, Newport, and Tanenhaus, 2008).
HBMs can thus be viewed as providing a
“rational” upper bound on language learn-
ability, yielding optimal models that account
for observed data while minimizing any re-
quired prior information. In addition, the
clustering implicit in HBM modeling intro-
duces additional parameters that can be tuned
to specific data patterns. However, this comes
at a well-known price: HBMs generally are
also ideal learning systems, known to be
computationally infeasible (Kwisthout, Ware-
ham, and van Rooij, 2011). Approximations
proposed to ensure computational tractabil-
ity, like reducing the number of classes that
need to be learned may also be linguisti-
cally and cognitively implausible. For in-
stance, in terms of verb learning, this could
</bodyText>
<page confidence="0.973865">
1321
</page>
<note confidence="0.9140755">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321–1330,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999952275">
take the form of reducing the number of sub-
categorization frames to the relevant subset,
as in (Perfors, Tenenbaum, and Wonnacott,
2010), where only 2 frames are considered for
‘take’, when in fact it is listed in 6 frames
by Levin (1993). Finally, comparison of vari-
ous Bayesian models of the same task is rare
(Jones and Love, 2011) and Bayesian infer-
ence generally can be demonstrated as sim-
ply one class of regularization or smooth-
ing techniques among many others; given the
problem at hand, there may well be other,
equally compelling regularization methods
for dealing with the bias-variance dilemma
(e.g., SVMs (Shalizi, 2009)). Consequently, the
relevance of HBMs for cognitively accurate ac-
counts of human learning remains uncertain
and needs to be carefully assessed.
Here we argue that the strengths of HBMs
for a given task must be evaluated in light of
their computational and cognitive costs, and
compared to other viable alternatives. The fo-
cus should be on finding the simplest statis-
tical models consistent with a given behav-
ior, particularly one that aligns with known
cognitive limitations. In the case of many
language acquisition tasks this behavior often
takes the form of overgeneralization, but with
eventual convergence to some target language
given exposure to more data.
In particular, in this paper we consider how
children acquire English dative verb construc-
tions, comparing HBMs to a simpler alterna-
tive, a linear competition learning (LCL) al-
gorithm that models the behavior of a given
verb as the linear competition between the ev-
idence for that verb, and the average behav-
ior of verbs belonging to its same class. The
results show that combining simple cluster-
ing methods along with ordinary maximum
likelihood estimation yields a result compara-
ble to HBM performance, providing an alter-
native account of the same facts, without the
computational costs incurred by HBM models
that must rely, for example, on Markov Chain
Monte Carlo (MCMC) methods for numeri-
cally integrating complex likelihood integrals,
or on Chinese Restaurant Process (CRP) for
producing partitions.
In terms of Marr&apos;s hierarchy (Marr, 1982)
learning verb alternations is an abstract com-
putational problem (Marr&apos;s type I), solvable
by many type II methods combining repre-
sentations (models, viz. HBMs or LCLs) with
particular algorithms. The HBM convention
of adopting ideal learning amounts to invok-
ing unbounded algorithmic resources, solv-
ability in principle, even though in practice
such methods, even approximate ones, are
provably NP-hard (cf. (Kwisthout, Wareham,
and van Rooij, 2011)). Assuming cognitive
plausibility as a desideratum, we therefore ex-
amine whether HBMs can also be approxi-
mated by another type II method (LCLs) that
does not demand such intensive computa-
tion. Any algorithm that approximates an
HBM can be viewed as implementing a some-
what different underlying model; if it repli-
cates HBM prediction performance but is sim-
pler and less computationally complex then
we assume it is preferable.
This paper is organized as follows: we start
with a discussion of formalizations of lan-
guage acquisition tasks, §2. We present our
experimental framework for the dative acqui-
sition task, formalizing a range of learning
models from simple MLE methods to HBM
techniques, §3, and a computational evalua-
tion of each model, §4. We finish with conclu-
sions and possibilities for future work, §5.
</bodyText>
<sectionHeader confidence="0.902242" genericHeader="method">
2 Evidence in Language Acquisition
</sectionHeader>
<bodyText confidence="0.999601">
A familiar problem for language acquisition is
how children learn which verbs participate in
so-called dative alternations, exemplified by
the child-produced sentences 1 to 3, from the
Brown (1973) corpus in CHILDES (MacWhin-
ney,1995).
</bodyText>
<listItem confidence="0.9993598">
1. you took me three scrambled eggs (a direct object da-
tive (DOD) from Adam at age 3;6)
2. Mommy can you fix dis for me ? (a prepositional da-
tive (PD) from Adam at age 4;7)
3. *Mommy, fix me my tiger (from Adam at age 5;2)
</listItem>
<bodyText confidence="0.99992975">
Examples like these show that children gen-
eralize their use of verbs. For example, in sen-
tence (1), the child Adam uses take as a DOD
before any recorded occurrence of a similar
use of take in adult speech to Adam. Such
verbs alternate because they can also occur
with a prepositional form, as in sentence (2).
However, sometimes a child’s use of verbs like
</bodyText>
<page confidence="0.990292">
1322
</page>
<bodyText confidence="0.999976711864407">
these amounts to an overgeneralization – that
is, their productive use of a verb in a pattern
that does not occur in the adult grammar, as in
sentence (3), above. Faced with these two verb
frames the task for the learner is to decide for a
particular verb if it is a non-alternating DOD
only verb, a PD only verb, or an alternating
verb that allows both forms.
This ambiguity raises an important learn-
ability question, conventionally known as
Baker’s paradox (Baker, 1979). On the as-
sumption that children only receive positive
examples of verb forms, then it is not clear
how they might recover from the overgener-
alization exhibited in sentence (3) above, be-
cause they will never receive positive sen-
tences from adults like (3), using fix in a DOD
form. As has long been noted, if negative ex-
amples were systematically available to learn-
ers, then this problem would be solved, since
the child would be given evidence that the
DOD form is not possible in the adult gram-
mar. However, although parental correction
could be considered to be a source of negative
evidence, it is neither systematic nor generally
available to all children (Marcus, 1993). Even
when it does occur, all careful studies have in-
dicated that it seems mostly concerned with
semantic appropriateness rather than syntax.
In the cases where it is related to syntax, it
is often difficult to determine what the cor-
rection refers to in the utterance and besides
children seem to be oblivious to the correction
(Brown and Hanlon, 1970; Ingram, 1989).
One alternative solution to Baker’s paradox
that has been widely discussed at least since
Chomsky (1981) is the use of indirect negative
evidence. On the indirect negative evidence
model, if a verb is not found where it would
be expected to occur, the learner may con-
clude it is not part of the adult grammar. Cru-
cially, the indirect evidence model is inher-
ently statistical. Different formalizations of in-
direct negative evidence have been incorpo-
rated in several computational learning mod-
els for learning e.g. grammars (Briscoe, 1997;
Villavicencio, 2002; Kwiatkowski et al., 2010);
dative verbs (Perfors, Tenenbaum, and Won-
nacott, 2010; Hsu and Chater, 2010); and mul-
tiword verbs (Nematzadeh, Fazly, and Steven-
son, 2013). Since a number of closely related
models can all implement the indirect nega-
tive evidence approach, the decision of which
one to choose for a given task may not be en-
tirely clear. In this paper we compare a range
of statistical models consistent with a certain
behavior: early overgeneralization, with even-
tual convergence to the correct target on the
basis of exposure to more data.
</bodyText>
<sectionHeader confidence="0.987781" genericHeader="method">
3 Materials and Methods
</sectionHeader>
<subsectionHeader confidence="0.999523">
3.1 Dative Corpora
</subsectionHeader>
<bodyText confidence="0.999975575">
To emulate a child language acquisition en-
vironment we use naturalistic longitudinal
child-directed data, from the Brown corpus in
CHILDES, for one child (Adam) for a subset
of 19 verbs in the DOD and PD verb frames,
figure 1. This dataset was originally reported
in Perfors, Tenenbaum, and Wonnacott (2010),
and longitudinal and incremental aspects to
acquisition are approximated by dividing the
data available into 5 incremental epochs (E1 to
E5 in the figures), where at the final epoch the
learner has seen the full corpus.
Model comparison requires a gold standard
database for acquisition, reporting which
frames have been learned for which verbs at
each stage, and how likely a child is of mak-
ing creative uses of a particular verb in a new
frame. An independent gold standard with
developmental information (e.g. Gropen et
al. (1989)) would clearly be ideal. Absent
this, a first step is demonstrating that sim-
pler alternative models can replicate HBM
performance on their own terms. Therefore,
the gold standard we use for evaluation is
the classification predicted by Perfors, Tenen-
baum, and Wonnacott (2010). The evaluations
reported in our analysis take into account in-
trinsic characteristics of each model in rela-
tion to the likelihoods of the verbs, to deter-
mine the extent to which the models go be-
yond the data they were exposed to, discussed
in section 2. Further, since it has been ar-
gued that very low frequency verbs may not
yet be firmly placed in a child’s lexicon (Yang,
2010; Gropen et al., 1989), at each epoch we
also impose a low-frequency threshold of 5
occurrences, considering only verbs that the
learner has seen at least 5 times. This use of a
low-frequency threshold for learning has ex-
tensive support in the literature for learning
</bodyText>
<page confidence="0.898215">
1323
</page>
<bodyText confidence="0.911136222222222">
of all kinds in both human and non-human
animals, e.g. (Gallistel, 2002). A cut-off fre-
quency in this range has also commonly been
used in NLP tasks like POS tagging (Ratna-
parkhi,1999).
context of verb learning smoothing could be
based on several principles:
• an (innate) expectation as to how verbs in
general should behave;
</bodyText>
<subsectionHeader confidence="0.999926">
3.2 The learners
</subsectionHeader>
<bodyText confidence="0.999813666666667">
We selected a set of representative statistical
models that are capable in principle of solv-
ing this classification task, ranging from what
is perhaps the simplest possible, a simple bi-
nomial, all the way to multi-level hierarchical
Bayesian approaches.
A Binomial distribution serves as the sim-
plest model for capturing the behavior of a
verb occurring in either DOD or PD frame.
Representing the probability of DOD as θ, af-
ter n occurrences of the verb the probability
that y of them are DOD is:
</bodyText>
<equation confidence="0.988902333333333">
(n)
p( y |θ, n) = θy (1- θ)n-y (1)
y
</equation>
<bodyText confidence="0.9974422">
Considering that p(y |θ, n) is the likelihood
in a Bayesian framework, the simplest and the
most intuitive estimator of θ, given y in n verb
occurrences, is the Maximum Likelihood Esti-
mator (MLE):
</bodyText>
<equation confidence="0.9793575">
θMLE = y (2)
n
</equation>
<bodyText confidence="0.999592238095238">
θMLE is viable as a learning model in the sense
that its accuracy increases as the amount of ev-
idence for a verb grows (n → oo), reflecting
the incremental, on-line character of language
learning. However, one well known limita-
tion of MLE is that it assigns zero probability
mass to unseen events. Ruling out events on
the grounds that they did not occur in a finite
data set early in learning may be too strong –
though it should be noted that this is simply
one (overly strong) version of the indirect neg-
ative evidence position.
Again as is familiar, to overcome zero
count problem, models adopt one or another
method of smoothing to assign a small prob-
ability mass to unseen events. In a Bayesian
formulation, this amounts to assigning non-
zero probability mass to some set of priors;
smoothing also captures the notion of gener-
alization, making predictions about data that
has never been seen by the learner. In the
</bodyText>
<listItem confidence="0.783913666666667">
• an acquired class-based expectation of
the behavior of a verb, based on its associ-
ation to similar but more frequent verbs.
</listItem>
<bodyText confidence="0.999980173913044">
The former can be readily implemented
in terms of prior probability estimates. As
we discuss below, class-based estimates arise
from one or another clustering method, and
can produce more accurate estimates for less
frequent verbs based on patterns already
learned for more frequent verbs in the same
class; see (Perfors, Tenenbaum, and Wonna-
cott, 2010). In this case, smoothing is a side-
effect of the behavior of a class as a whole.
When learning begins, the prior probability
is the only source of information for a learner
and, as such, dominates the value of the poste-
rior probability. However, in the large sample
limit, it is the likelihood that dominates the
posterior distribution regardless of the prior.
In Hierarchical Bayesian Models both effects
are naturally incorporated. The prior distri-
bution is structured as a chain of distributions
of parameters and hyper-parameters, and the
data may be divided into classes that share
some of the hyper-parameters, as defined be-
low for the case of a three levels model:
</bodyText>
<equation confidence="0.999818333333333">
λ - Exponential(1)
µ - Exponential(1)
αk - Exponential(λ)
βk - Beta(µ, µ)
θik - Beta(αkβk, αk(1 — βk))
yi|ni - Binomial(θik)
</equation>
<bodyText confidence="0.999422222222222">
The indices refer to the possible hierarchies
among the hyper-parameters. λ and µ are in
the top, and they are shared by all verbs. Then
there are classes of different αk, βk, and the
probabilities for the DOD frame for the dif-
ferent verbs (θik) are drawn according to the
classes k assigned to them. An estimate for
(θik) for a given configuration of clusters is
given by
</bodyText>
<page confidence="0.992424">
1324
</page>
<figureCaption confidence="0.9999905">
Figure 1: Verb tokens per epoch (E1 to E5)
Figure 2: Verb tokens ≥ 5 per epoch (E1 to E5)
</figureCaption>
<bodyText confidence="0.998931125">
where P(Y) is the evidence of the data,
the unnormalized posterior for the hyper-
parameters is
and the likelihood for α and 0 is
The Hierarchical Bayesian Model prediction
for Bi is the average of the estimate BikHBM over
all possible partitions of the verbs in the task.
To simplify the notation we can write
</bodyText>
<equation confidence="0.9726595">
BHBM = E ry+ α0l
L n + α J (3)
</equation>
<bodyText confidence="0.999951054054054">
where in the expression E[... ] are included
the integrals described above and the average
of all possible class partitions. Due to this
complexity, in practice even small data sets re-
quire the use of MCMC methods, and statisti-
cal models for partitions, like CRP (Gelman et
al., 2003; Perfors, Tenenbaum, and Wonnacott,
2010). This complexity also calls into question
the cognitive fidelity of such approaches.
Eq.3 is particularly interesting because by
fixing α and 0 (instead of averaging over them)
it is possible to deduce simpler (and classical)
models: MLE corresponds to α = 0; the so
called “add-one” smoothing (referred in this
paper as L1) corresponds to α = 2 and 0 = 1/2.
From Eq.3 it is also clear that if α and 0 (or
their distributions) are unchanged, as the evi-
dence of a verb grows (n → oo), the HBM esti-
mate approaches MLE&apos;s, (BHBM → BMLE). On
the other hand, when α &gt;&gt; n, BHBM ∼ 0, so
that 0 can be interpreted as a prior value for B
in the low frequency limit.
Following this reasoning, we propose an
alternative approach, a linear competition
learner (LCL), that explicitly models the be-
havior of a given verb as the linear competi-
tion between the evidence for the verb, and
the average behavior of verbs of the same
class. As clustering is defined independently
from parameter estimation, the advantages of
the proposed approach are twofold. First, it
is computationally much simpler, not requir-
ing approximations by Monte Carlo meth-
ods. Second, differently from HBMs where
the same attributes are used for clustering and
parameter estimation (in this case the DOD
and PD counts for each verb), in LCL cluster-
</bodyText>
<page confidence="0.961323">
1325
</page>
<bodyText confidence="0.9990382">
ing may be done using more general contexts
that employ a variety of linguistic and envi-
ronmental attributes.
For LCL the prior and class-based informa-
tion are incorporated as:
</bodyText>
<equation confidence="0.999878">
yi + αCβC
θLCL = (4)
ni + αC
</equation>
<bodyText confidence="0.9998618">
where αC and βC are defined via justifiable
heuristic expressions dependent solely on the
statistics of the class attributed to each verb i.
The strength of the prior (αC) is a mono-
tonic function of the number of elements (mC)
in the class C, excluding the target verb vi.
To approximate the gold standard behavior of
the HBM for this task (Perfors, Tenenbaum,
and Wonnacott, 2010) we chose the following
function for αC:
</bodyText>
<equation confidence="0.99865">
αC = mC3/2(1 − mC−1/5) + 0.1 (5)
</equation>
<bodyText confidence="0.999896833333333">
with the strength of the prior for the LCL
model depending on the number of verbs in
the class, not on their frequency. Eq.5 was
chosen as a good fit to HBMs, without incur-
ring their complexity. The powers are simple
fractions, not arbitrary numbers. A best fit
was not attempted due to the lack of assess-
ment of how accurate HBMs are on real data.
The prior value (βC) is a smoothed estima-
tion of the probability of DOD in a given class,
combining the evidence for all verbs in that
class:
</bodyText>
<equation confidence="0.997226">
YC + 1/2
βC = (6)
NC + 1
</equation>
<bodyText confidence="0.987858714285714">
in this case YC is the number of DOD occur-
rences in the class, and NC the total number
of verb occurrences in the class, in both cases
excluding the target verb vi.
The interpretation of these parameters is
as follows: βC is the estimate of θ in the ab-
sence of any data for a verb; and αC controls
the crossover between this estimate and MLE,
with a large αC requiring a larger sample (ni)
to overcome the bias given by βC.
For comparative purposes, in this paper we
examine alternative models for (a) probability
estimation and (b) clustering. The models are
the following:
</bodyText>
<listItem confidence="0.9991802">
• two models without clusters: MLE and
L1;
• two models where clusters are performed
independently: LCL and MLEαβ; and
• the full HBM described before.
</listItem>
<bodyText confidence="0.96877956">
MLEαβ corresponds to replacing α, β in eq.3
by their maximal likelihood values calculated
from P({yi, ni}iEk|α, β) described before.
For models without clustering, estimation
is based solely on the observed behavior of
verbs. With clustering, same-cluster verbs
share some parameters, influencing one an-
other. HBMs place distributions over pos-
sible clusters, with estimation derived from
averages over distributions. In HBMs, clus-
tering and probability estimation are calcu-
lated jointly. In the other models these two
estimates are calculated separately, permit-
ting ’plug-and-play’ use of external cluster-
ing methods, like X-means (Pelleg and Moore,
2000)1. However, to further assess the impact
of cluster assignment on alternative model
performance, we also used the clusters that
maximize the evidence of the HBM for the
DOD and PD counts of the target verbs, and
we refer to these as Maximum Evidence (ME)
clusters. In MWE clusters, verbs are separated
into 3 classes: one if they have counts for both
frames; another for only the DOD frame; and
a final for only the PD frame.
</bodyText>
<sectionHeader confidence="0.999355" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999940647058824">
The learning task consists of estimating the
probability that a given verb occurs in a partic-
ular frame, using previous occurrences as the
basis for this estimation. In this context, over-
generalization can be viewed as the model’s
predictions that a given verb seen only in one
frame (say, a PD) can also occur in the other
(say, a DOD) as well, and it decreases as the
learner receives more data. In one extreme
we have MLE, which does not overgeneralize,
and in the other the L1 model, which assigns
uniform probability for all unseen cases. The
other 3 models fall somewhere in between,
overgeneralizing beyond the observed data,
using the prior and class-based smoothing to
assign some (low) probability mass to an un-
seen verb-frame pair. The relevant models’
</bodyText>
<footnote confidence="0.97929075">
1Other clustering algorithms were also used; here
we report X-means results as representative of these
models. X-means is available from http://www.cs.
waikato.ac.nz/ml/weka/
</footnote>
<page confidence="0.994634">
1326
</page>
<bodyText confidence="0.999764846153846">
predictions for each of the target verbs in the
DOD frame, given the full corpus, are in fig-
ure 3. In either end of the figure are the verbs
that were attested in only one of the frames
(PD only at the left-hand end, and DOD only
at the right-hand end). For these verbs, LCL
and HBM exhibit similar behavior. When the
low-frequency threshold is applied, MLEαp,
HBM and LCL work equally well, figure 4.
decrease significantly and so does the degree
of overgeneralization in the models’ predic-
tions, as shown in the 3 lighter bars in the fig-
ure.
</bodyText>
<figureCaption confidence="0.884696">
Figure 4: Probability of verbs in DOD frame,
Low Frequency Threshold.
</figureCaption>
<bodyText confidence="0.999965125">
To examine how overgeneralization pro-
gresses during the course of learning as the
models were exposed to increasing amounts
of data, we used the corpus divided by cumu-
lative epochs, as described in §3.1. For each
epoch, verbs seen in only one of the frames
were divided in 5 frequency bins, and the
models were assessed as to how much over-
generalization they displayed for each of these
verbs. Following Perfors, Tenenbaum, and
Wonnacott (2010) overgeneralization is calcu-
lated as the absolute difference between the
models predicted B and BMLE, for each of the
epochs, figure 5, and for comparative pur-
poses their alternating/non-alternating clas-
sification is also adopted. For non-alternating
verbs, overgeneralization reflects the degree
of smoothing of each model. As expected, the
more frequent a verb is, the more confident
the model is in the indirect negative evidence
it has for that verb, and the less it overgeneral-
izes, shown in the lighter bars in all epochs. In
addition, the overall effect of larger amounts
of data are indicated by a reduction in over-
generalization epoch by epoch. The effects of
class-based smoothing can be assessed com-
paring L1, a model without clustering which
displays a constant degree of overgeneraliza-
tion regardless of the epoch, while HBM uses
a distribution over clusters and the other mod-
els X-means. If a low-frequency threshold is
applied, the differences between the models
</bodyText>
<figureCaption confidence="0.538658">
Figure 5: Overgeneralization, per epoch, per
frequency bin, where 0.5 corresponds to the
maximum overgeneralization.
</figureCaption>
<bodyText confidence="0.99908576">
While the models differ somewhat in their
predictions, the quantitative differences need
to be assessed more carefully. To compare
the models and provide an overall difference
measure, we use the predictions of the more
complex model, HBM, as a baseline and then
calculate the difference between its predic-
tions and those of the other models. We
used three different measures for comparing
models, one for their standard difference; one
that prioritizes agreement for high frequency
verbs; and one that focuses more on low fre-
quency verbs.
The first measure, denoted Di�erence, cap-
tures a direct comparison between two mod-
els, M1 and M2 as the average prediction dif-
ference among the verbs, and is defined as:
This measure treats all differences uniformly,
regardless of whether they relate to high or
low frequency verbs in the learning sample
(e.g. for bring with 150 counts and serve with
only 1 have the same weight). To focus on high
frequency verbs, we also define the Weighted
Di�erence between two models as:
Here we expect Dn &lt; D since models tend to
</bodyText>
<page confidence="0.995577">
1327
</page>
<figureCaption confidence="0.999823">
Figure 3: Probability of verbs in DOD frame.
</figureCaption>
<bodyText confidence="0.9998903">
agree as the amount of evidence for each verb
increases. Conversely, our third measure, de-
noted Inverted, prioritizes the agreement be-
tween two models on low frequency verbs, de-
fined as follows:
D1/n captures the degree of similarity in over-
generalization between two models. The re-
sults of applying these three difference mea-
sures are shown in figure 6 for the relevant
models, where grey is for D(M1,M2), black
for Dn(M1, M2) and white for D1/n(M1, M2).
Given the probabilistic nature of Monte Carlo
methods, there is also a variation between dif-
ferent runs of the HBM model (HBM to HBM-
2), and this indicates that models that per-
form within these bounds can be considered
to be equivalent (e.g. HBMs and ME-MLEαp
for Weighted Difference, and the HBMs and
X-MLEαp for the Inverted Difference).
Comparing the prediction agreement, the
strong influence of clustering is clear: the
models that have compatible clusters have
similar performances. For instance, all the
models that adopt the ME clusters for the
data perform closest to HBMs. Moreover, the
weighted differences tend to be smaller than
0.01 and around 0.02 for the inverted differ-
ences. The results for these measures become
even closer in most cases when the low fre-
quency threshold is adopted, figure 7, as the
</bodyText>
<figureCaption confidence="0.951587333333333">
Figure 6: Model Comparisons.
Figure 7: Model Comparison - Low Frequency
Threshold.
</figureCaption>
<figure confidence="0.993435076923077">
1
0.9
0.8
MLE
L1
HBM
LCL
MLE
L1
HBM
LCL
0.50 5 10 15 20 25 30 35 40 45 50
number of examples
</figure>
<figureCaption confidence="0.990952">
Figure 8: DOD probability evolution for mod-
els with increase in evidence
</figureCaption>
<bodyText confidence="0.998067142857143">
evidence reduces the influence of the prior.
To examine the decay of overgeneralization
with the increase in evidence for these mod-
els, two simulated scenarios are defined for a
single generic verb: one where the evidence
for DOD amounts to 75% of the data (dashed
lines) and in the other to 100% (solid lines),
figures 9 and 8. Unsurprisingly, the perfor-
mance of the models is dependent on the
amount of evidence available. This is a con-
sequence of the decrease in the influence of
the priors as the sample size increases in a rate
of 1/N, as shown in figure 9 for the decrease
in overgeneralization. Ultimately it is the ev-
</bodyText>
<figure confidence="0.9155892">
DOD probability
0.7
0.6
1328
number of examples
</figure>
<figureCaption confidence="0.9743365">
Figure 9: Overgeneralization reduction with
increase in evidence
</figureCaption>
<bodyText confidence="0.999817692307692">
idence that dominates the posterior probabil-
ity. Although the Bayesian model exhibits fast
convergence, after 10 examples, the simpler
model L1 is only approximately 3% below the
Bayesian model in performance for scenario 1
and is still 90% accurate in scenario 2, figure 8.
These results suggest that while these mod-
els all differ slightly in the degree of overgen-
eralization for low frequency data and noise,
these differences are small, and as evidence
reaches approximately 10 examples per verb,
the overall performance for all models ap-
proaches that of MLE.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999953542857143">
HBMs have been successfully used for a
number of language acquisition tasks captur-
ing both patterns of under- and overgeneral-
ization found in child language acquisition.
Their (hyper)parameters provide robustness
for dealing with low frequency events, noise,
and uncertainty and a good fit to the data,
but this fidelity comes at the cost of complex
computation. Here we have examined HBMs
against computationally simpler approaches
to dative alternation acquisition, which imple-
ment the indirect negative approach. We also
advanced several measures for model com-
parison in order to quantify their agreement
to assist in the task of model selection. The re-
sults show that the proposed LCL model, in
particular, that combines class-based smooth-
ing with maximum likelihood estimation, ob-
tains results comparable to those of HBMs,
in a much simpler framework. Moreover,
when a cognitively-viable frequency thresh-
old is adopted, differences in the performance
of all models decrease, and quite rapidly ap-
proach the performance of MLE.
In this paper we used standard clustering
techniques grounded solely on verb counts to
enable comparison with previous work. How-
ever, a variety of additional linguistic and dis-
tributional features could be used for cluster-
ing verbs into more semantically motivated
classes, using a larger number of frames and
verbs. This will be examined in future work.
We also plan to investigate the use of cluster-
ing methods more targeted to language tasks
(Sun and Korhonen, 2009).
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999969833333333">
We would like to thank the support of
projects CAPES/COFECUB 707/11, CNPq
482520/2012-4, 478222/2011-4, 312184/2012-
3, 551964/2011-1 and 312077/2012-2. We also
want to thank Amy Perfors for kindly sharing
the input data.
</bodyText>
<sectionHeader confidence="0.999041" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996968952380953">
Baker, Carl L. 1979. Syntactic Theory and the Pro-
jection Problem. Linguistic Inquiry, 10(4):533–
581.
Briscoe, Ted. 1997. Co-evolution of language and
the language acquisition device. In Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 418–427.
Morgan Kaufmann.
Brown, Roger. 1973. A first language: Ehe early
stages. Harvard University Press, Cambridge,
Massachusetts.
Brown, Roger and Camille Hanlon. 1970. Deriva-
tional complexity and the order of acquisition of
child’s speech. In J. Hays, editor, Cognition and
the Development of Language. NY: John Wiley.
Chater, Nick, Joshua B. Tenenbaum, and Alan
Yuille. 2006. Probabilistic models of cogni-
tion: where next? Trends in Cognitive Sciences,
10(7):292 – 293.
Chomsky, Noam. 1981. Lectures on government and
binding. Mouton de Gruyter.
</reference>
<figure confidence="0.996787076923077">
100
10−1
10−2
10−3
10−4
100 101 102
L1
HBM
LCL
L1
HBM
LCL
overgeneralization
</figure>
<page confidence="0.97217">
1329
</page>
<reference confidence="0.999847298969072">
Gallistel, Charles R. 2002. Frequency, contin-
gency, and the information processing theory of
conditioning. In P.Sedlmeier and T. Betsch, ed-
itors, Frequency processing and cognition. Oxford
University Press, pages 153–171.
Gelman, Andrew, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2003. Bayesian Data Analy-
sis, Second Edition (Chapman &amp; Hall/CRC Texts in
Statistical Science). Chapman and Hall/CRC, 2
edition.
Gropen, Jess, Steve Pinker, Michael Hollander,
Richard Goldberg, and Ronald Wilson. 1989.
The learnability and acquisition of the dative al-
ternation in English. Language, 65(2):203–257.
Hsu, Anne S. and Nick Chater. 2010. The logi-
cal problem of language acquisition: A proba-
bilistic perspective. Cognitive Science, 34(6):972–
1016.
Ingram, David. 1989. First Language Acquisition:
Method, Description and Explanation. Cambridge
University Press.
Jones, Matt and Bradley C. Love. 2011. Bayesian
Fundamentalism or Enlightenment? On the ex-
planatory status and theoretical contributions
of Bayesian models of cognition. Behavioral and
Brain Sciences, 34(04):169–188.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon
Goldwater, and Mark Steedman. 2010. Induc-
ing probabilistic CCG grammars from logical
form with higher-order unification. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1223–1233.
Kwisthout, Johan, Todd Wareham, and Iris van
Rooij. 2011. Bayesian intractability is not an
ailment that approximation can cure. Cognitive
Science, 35(5):779–1007.
Levin, B. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
MacWhinney, Brian. 1995. The CHILDES project:
tools for analyzing talk. Hillsdale, NJ: Lawrence
Erlbaum Associates, second edition.
Marcus, Gary F. 1993. Negative evidence in lan-
guage acquisition. Cognition, 46:53–85.
Marr, D. 1982. Vision. San Francisco, CA: W. H.
Freeman.
Nematzadeh, Aida, Afsaneh Fazly, and Suzanne
Stevenson. 2013. Child acquisition of multi-
word verbs: A computational investigation. In
A. Villavicencio, T. Poibeau, A. Korhonen, and
A. Alishahi, editors, Cognitive Aspects of Com-
putational Language Acquisition. Springer, pages
235–256.
Parisien, Christopher, Afsaneh Fazly, and Suzanne
Stevenson. 2008. An incremental bayesian
model for learning syntactic categories. In Pro-
ceedings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL &apos;08, pages
89–96, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Parisien, Christopher and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the 32nd
Annual Conference of the Cognitive Science Society.
Pelleg, Dan and Andrew Moore. 2000. X-means:
Extending k-means with efficient estimation of
the number of clusters. In Proceedings of the
Seventeenth International Conference on Machine
Learning, pages 727–734, San Francisco. Morgan
Kaufmann.
Perfors, Amy, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, nega-
tive evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
(37):607–642.
Ratnaparkhi, Adwait. 1999. Learning to parse nat-
ural language with maximum entropy models.
Machine Learning, pages 151–175.
Shalizi, Cosma R. 2009. Dynamics of bayesian
updating with dependent data and misspeci-
fied models. ElectroCosmanic Journal of Statistics,
3:1039–1074.
Sun, Lin and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired se-
lectional preferences. In EMNLP, pages 638–
647.
Villavicencio, Aline. 2002. The Acquisition of a
Uni�cation-Based Generalised Categorial Grammar.
Ph.D. thesis, Computer Laboratory, University
of Cambridge.
Wonnacott, Elizabeth, Elissa L. Newport, and
Michael K. Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cogni-
tive Psychology, 56:165–209.
Yang, Charles. 2010. Three factors in language
variation. Lingua, 120:1160–1177.
</reference>
<page confidence="0.989932">
1330
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.794933">
<title confidence="0.999358">Language Acquisition and Probabilistic Models: keeping it simple</title>
<author confidence="0.9579745">Marco Robert Igor of Informatics</author>
<author confidence="0.9579745">Federal University of Rio Grande do Sul</author>
<affiliation confidence="0.995202333333333">of Physics, Federal University of Rio Grande do Sul (Brazil) ♦LIDS, Dept. of EECS, Massachusetts Institute of Technology (USA) Dept. of EECS, Massachusetts Institute of Technology (USA)</affiliation>
<email confidence="0.958993">avillavicencio@inf.ufrgs.br,marco.idiart@if.ufrgs.brberwick@csail.mit.edu,igorm@mit.edu</email>
<abstract confidence="0.998467">Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of underand overgeneralization in child language acquisition. However, as is well known, HBMs are “ideal” learning systems, assuming access to unlimited computational resources that may not be available to child language learners. Consequently, it remains crucial to carefully assess the use of HBMs along with alternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisition domain where explicit HBMs have been proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carl L Baker</author>
</authors>
<date>1979</date>
<booktitle>Syntactic Theory and the Projection Problem. Linguistic Inquiry,</booktitle>
<volume>10</volume>
<issue>4</issue>
<pages>581</pages>
<contexts>
<context position="8376" citStr="Baker, 1979" startWordPosition="1310" endWordPosition="1311">bs alternate because they can also occur with a prepositional form, as in sentence (2). However, sometimes a child’s use of verbs like 1322 these amounts to an overgeneralization – that is, their productive use of a verb in a pattern that does not occur in the adult grammar, as in sentence (3), above. Faced with these two verb frames the task for the learner is to decide for a particular verb if it is a non-alternating DOD only verb, a PD only verb, or an alternating verb that allows both forms. This ambiguity raises an important learnability question, conventionally known as Baker’s paradox (Baker, 1979). On the assumption that children only receive positive examples of verb forms, then it is not clear how they might recover from the overgeneralization exhibited in sentence (3) above, because they will never receive positive sentences from adults like (3), using fix in a DOD form. As has long been noted, if negative examples were systematically available to learners, then this problem would be solved, since the child would be given evidence that the DOD form is not possible in the adult grammar. However, although parental correction could be considered to be a source of negative evidence, it </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, Carl L. 1979. Syntactic Theory and the Projection Problem. Linguistic Inquiry, 10(4):533– 581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
</authors>
<title>Co-evolution of language and the language acquisition device.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>418--427</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="9957" citStr="Briscoe, 1997" startWordPosition="1576" endWordPosition="1577">em to be oblivious to the correction (Brown and Hanlon, 1970; Ingram, 1989). One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, and Stevenson, 2013). Since a number of closely related models can all implement the indirect negative evidence approach, the decision of which one to choose for a given task may not be entirely clear. In this paper we compare a range of statistical models consistent with a certain behavior: early overgeneralization, with eventual convergence to the correct target on the basis of exposure to more data. 3 Materials and Methods 3.</context>
</contexts>
<marker>Briscoe, 1997</marker>
<rawString>Briscoe, Ted. 1997. Co-evolution of language and the language acquisition device. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL), pages 418–427. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
</authors>
<title>A first language: Ehe early stages.</title>
<date>1973</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="7280" citStr="Brown (1973)" startWordPosition="1108" endWordPosition="1109">r is organized as follows: we start with a discussion of formalizations of language acquisition tasks, §2. We present our experimental framework for the dative acquisition task, formalizing a range of learning models from simple MLE methods to HBM techniques, §3, and a computational evaluation of each model, §4. We finish with conclusions and possibilities for future work, §5. 2 Evidence in Language Acquisition A familiar problem for language acquisition is how children learn which verbs participate in so-called dative alternations, exemplified by the child-produced sentences 1 to 3, from the Brown (1973) corpus in CHILDES (MacWhinney,1995). 1. you took me three scrambled eggs (a direct object dative (DOD) from Adam at age 3;6) 2. Mommy can you fix dis for me ? (a prepositional dative (PD) from Adam at age 4;7) 3. *Mommy, fix me my tiger (from Adam at age 5;2) Examples like these show that children generalize their use of verbs. For example, in sentence (1), the child Adam uses take as a DOD before any recorded occurrence of a similar use of take in adult speech to Adam. Such verbs alternate because they can also occur with a prepositional form, as in sentence (2). However, sometimes a child’s</context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Brown, Roger. 1973. A first language: Ehe early stages. Harvard University Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
<author>Camille Hanlon</author>
</authors>
<title>Derivational complexity and the order of acquisition of child’s speech.</title>
<date>1970</date>
<booktitle>Cognition and the Development of Language.</booktitle>
<editor>In J. Hays, editor,</editor>
<publisher>NY: John Wiley.</publisher>
<contexts>
<context position="9404" citStr="Brown and Hanlon, 1970" startWordPosition="1484" endWordPosition="1487">since the child would be given evidence that the DOD form is not possible in the adult grammar. However, although parental correction could be considered to be a source of negative evidence, it is neither systematic nor generally available to all children (Marcus, 1993). Even when it does occur, all careful studies have indicated that it seems mostly concerned with semantic appropriateness rather than syntax. In the cases where it is related to syntax, it is often difficult to determine what the correction refers to in the utterance and besides children seem to be oblivious to the correction (Brown and Hanlon, 1970; Ingram, 1989). One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010</context>
</contexts>
<marker>Brown, Hanlon, 1970</marker>
<rawString>Brown, Roger and Camille Hanlon. 1970. Derivational complexity and the order of acquisition of child’s speech. In J. Hays, editor, Cognition and the Development of Language. NY: John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Chater</author>
<author>Joshua B Tenenbaum</author>
<author>Alan Yuille</author>
</authors>
<title>Probabilistic models of cognition: where next? Trends in Cognitive Sciences,</title>
<date>2006</date>
<volume>10</volume>
<issue>7</issue>
<pages>293</pages>
<contexts>
<context position="1689" citStr="Chater, Tenenbaum, and Yuille, 2006" startWordPosition="236" endWordPosition="240">domain where explicit HBMs have been proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs. 1 Introduction In recent years, with advances in probability and estimation theory, there has been much interest in Bayesian models (BMs) (Chater, Tenenbaum, and Yuille, 2006; Jones and Love, 2011) and their application to child language acquisition with its challenging combination of structured information and incomplete knowledge, (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010; Parisien, Fazly, and Stevenson, 2008; Parisien and Stevenson, 2010) as they offer several advantages in this domain. They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction</context>
</contexts>
<marker>Chater, Tenenbaum, Yuille, 2006</marker>
<rawString>Chater, Nick, Joshua B. Tenenbaum, and Alan Yuille. 2006. Probabilistic models of cognition: where next? Trends in Cognitive Sciences, 10(7):292 – 293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Lectures on government and binding. Mouton de Gruyter.</title>
<date>1981</date>
<contexts>
<context position="9525" citStr="Chomsky (1981)" startWordPosition="1504" endWordPosition="1505">tion could be considered to be a source of negative evidence, it is neither systematic nor generally available to all children (Marcus, 1993). Even when it does occur, all careful studies have indicated that it seems mostly concerned with semantic appropriateness rather than syntax. In the cases where it is related to syntax, it is often difficult to determine what the correction refers to in the utterance and besides children seem to be oblivious to the correction (Brown and Hanlon, 1970; Ingram, 1989). One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, </context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, Noam. 1981. Lectures on government and binding. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles R Gallistel</author>
</authors>
<title>Frequency, contingency, and the information processing theory of conditioning. In</title>
<date>2002</date>
<pages>153--171</pages>
<editor>P.Sedlmeier and T. Betsch, editors,</editor>
<publisher>Oxford University Press,</publisher>
<contexts>
<context position="12418" citStr="Gallistel, 2002" startWordPosition="1986" endWordPosition="1987">ion to the likelihoods of the verbs, to determine the extent to which the models go beyond the data they were exposed to, discussed in section 2. Further, since it has been argued that very low frequency verbs may not yet be firmly placed in a child’s lexicon (Yang, 2010; Gropen et al., 1989), at each epoch we also impose a low-frequency threshold of 5 occurrences, considering only verbs that the learner has seen at least 5 times. This use of a low-frequency threshold for learning has extensive support in the literature for learning 1323 of all kinds in both human and non-human animals, e.g. (Gallistel, 2002). A cut-off frequency in this range has also commonly been used in NLP tasks like POS tagging (Ratnaparkhi,1999). context of verb learning smoothing could be based on several principles: • an (innate) expectation as to how verbs in general should behave; 3.2 The learners We selected a set of representative statistical models that are capable in principle of solving this classification task, ranging from what is perhaps the simplest possible, a simple binomial, all the way to multi-level hierarchical Bayesian approaches. A Binomial distribution serves as the simplest model for capturing the beh</context>
</contexts>
<marker>Gallistel, 2002</marker>
<rawString>Gallistel, Charles R. 2002. Frequency, contingency, and the information processing theory of conditioning. In P.Sedlmeier and T. Betsch, editors, Frequency processing and cognition. Oxford University Press, pages 153–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gelman</author>
<author>John B Carlin</author>
<author>Hal S Stern</author>
<author>Donald B Rubin</author>
</authors>
<title>Bayesian Data Analysis,</title>
<date>2003</date>
<journal>Second Edition (Chapman &amp; Hall/CRC Texts in Statistical Science). Chapman and Hall/CRC,</journal>
<volume>2</volume>
<pages>edition.</pages>
<contexts>
<context position="16724" citStr="Gelman et al., 2003" startWordPosition="2743" endWordPosition="2746">e P(Y) is the evidence of the data, the unnormalized posterior for the hyperparameters is and the likelihood for α and 0 is The Hierarchical Bayesian Model prediction for Bi is the average of the estimate BikHBM over all possible partitions of the verbs in the task. To simplify the notation we can write BHBM = E ry+ α0l L n + α J (3) where in the expression E[... ] are included the integrals described above and the average of all possible class partitions. Due to this complexity, in practice even small data sets require the use of MCMC methods, and statistical models for partitions, like CRP (Gelman et al., 2003; Perfors, Tenenbaum, and Wonnacott, 2010). This complexity also calls into question the cognitive fidelity of such approaches. Eq.3 is particularly interesting because by fixing α and 0 (instead of averaging over them) it is possible to deduce simpler (and classical) models: MLE corresponds to α = 0; the so called “add-one” smoothing (referred in this paper as L1) corresponds to α = 2 and 0 = 1/2. From Eq.3 it is also clear that if α and 0 (or their distributions) are unchanged, as the evidence of a verb grows (n → oo), the HBM estimate approaches MLE&apos;s, (BHBM → BMLE). On the other hand, when</context>
</contexts>
<marker>Gelman, Carlin, Stern, Rubin, 2003</marker>
<rawString>Gelman, Andrew, John B. Carlin, Hal S. Stern, and Donald B. Rubin. 2003. Bayesian Data Analysis, Second Edition (Chapman &amp; Hall/CRC Texts in Statistical Science). Chapman and Hall/CRC, 2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jess Gropen</author>
<author>Steve Pinker</author>
<author>Michael Hollander</author>
<author>Richard Goldberg</author>
<author>Ronald Wilson</author>
</authors>
<title>The learnability and acquisition of the dative alternation in English.</title>
<date>1989</date>
<journal>Language,</journal>
<volume>65</volume>
<issue>2</issue>
<contexts>
<context position="11416" citStr="Gropen et al. (1989)" startWordPosition="1813" endWordPosition="1816">e 1. This dataset was originally reported in Perfors, Tenenbaum, and Wonnacott (2010), and longitudinal and incremental aspects to acquisition are approximated by dividing the data available into 5 incremental epochs (E1 to E5 in the figures), where at the final epoch the learner has seen the full corpus. Model comparison requires a gold standard database for acquisition, reporting which frames have been learned for which verbs at each stage, and how likely a child is of making creative uses of a particular verb in a new frame. An independent gold standard with developmental information (e.g. Gropen et al. (1989)) would clearly be ideal. Absent this, a first step is demonstrating that simpler alternative models can replicate HBM performance on their own terms. Therefore, the gold standard we use for evaluation is the classification predicted by Perfors, Tenenbaum, and Wonnacott (2010). The evaluations reported in our analysis take into account intrinsic characteristics of each model in relation to the likelihoods of the verbs, to determine the extent to which the models go beyond the data they were exposed to, discussed in section 2. Further, since it has been argued that very low frequency verbs may </context>
</contexts>
<marker>Gropen, Pinker, Hollander, Goldberg, Wilson, 1989</marker>
<rawString>Gropen, Jess, Steve Pinker, Michael Hollander, Richard Goldberg, and Ronald Wilson. 1989. The learnability and acquisition of the dative alternation in English. Language, 65(2):203–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne S Hsu</author>
<author>Nick Chater</author>
</authors>
<title>The logical problem of language acquisition: A probabilistic perspective.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>6</issue>
<pages>1016</pages>
<contexts>
<context position="1912" citStr="Hsu and Chater, 2010" startWordPosition="270" endWordPosition="273">ng with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs. 1 Introduction In recent years, with advances in probability and estimation theory, there has been much interest in Bayesian models (BMs) (Chater, Tenenbaum, and Yuille, 2006; Jones and Love, 2011) and their application to child language acquisition with its challenging combination of structured information and incomplete knowledge, (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010; Parisien, Fazly, and Stevenson, 2008; Parisien and Stevenson, 2010) as they offer several advantages in this domain. They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic knowledge, from variation at the level of individual lexical items, to cross-item variation, using hyper-parameters to capture observed patterns of both under- and over-generalization as in the acquisi</context>
<context position="10083" citStr="Hsu and Chater, 2010" startWordPosition="1592" endWordPosition="1595">ox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, and Stevenson, 2013). Since a number of closely related models can all implement the indirect negative evidence approach, the decision of which one to choose for a given task may not be entirely clear. In this paper we compare a range of statistical models consistent with a certain behavior: early overgeneralization, with eventual convergence to the correct target on the basis of exposure to more data. 3 Materials and Methods 3.1 Dative Corpora To emulate a child language acquisition environment we use naturalistic longitudinal child-directed data, fro</context>
</contexts>
<marker>Hsu, Chater, 2010</marker>
<rawString>Hsu, Anne S. and Nick Chater. 2010. The logical problem of language acquisition: A probabilistic perspective. Cognitive Science, 34(6):972– 1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ingram</author>
</authors>
<title>First Language Acquisition: Method, Description and Explanation.</title>
<date>1989</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9419" citStr="Ingram, 1989" startWordPosition="1488" endWordPosition="1489"> given evidence that the DOD form is not possible in the adult grammar. However, although parental correction could be considered to be a source of negative evidence, it is neither systematic nor generally available to all children (Marcus, 1993). Even when it does occur, all careful studies have indicated that it seems mostly concerned with semantic appropriateness rather than syntax. In the cases where it is related to syntax, it is often difficult to determine what the correction refers to in the utterance and besides children seem to be oblivious to the correction (Brown and Hanlon, 1970; Ingram, 1989). One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs</context>
</contexts>
<marker>Ingram, 1989</marker>
<rawString>Ingram, David. 1989. First Language Acquisition: Method, Description and Explanation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Jones</author>
<author>Bradley C Love</author>
</authors>
<title>Bayesian Fundamentalism or Enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition.</title>
<date>2011</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>34</volume>
<issue>04</issue>
<contexts>
<context position="1712" citStr="Jones and Love, 2011" startWordPosition="241" endWordPosition="244">proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs. 1 Introduction In recent years, with advances in probability and estimation theory, there has been much interest in Bayesian models (BMs) (Chater, Tenenbaum, and Yuille, 2006; Jones and Love, 2011) and their application to child language acquisition with its challenging combination of structured information and incomplete knowledge, (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010; Parisien, Fazly, and Stevenson, 2008; Parisien and Stevenson, 2010) as they offer several advantages in this domain. They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic k</context>
<context position="3970" citStr="Jones and Love, 2011" startWordPosition="587" endWordPosition="590">guistically and cognitively implausible. For instance, in terms of verb learning, this could 1321 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321–1330, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics take the form of reducing the number of subcategorization frames to the relevant subset, as in (Perfors, Tenenbaum, and Wonnacott, 2010), where only 2 frames are considered for ‘take’, when in fact it is listed in 6 frames by Levin (1993). Finally, comparison of various Bayesian models of the same task is rare (Jones and Love, 2011) and Bayesian inference generally can be demonstrated as simply one class of regularization or smoothing techniques among many others; given the problem at hand, there may well be other, equally compelling regularization methods for dealing with the bias-variance dilemma (e.g., SVMs (Shalizi, 2009)). Consequently, the relevance of HBMs for cognitively accurate accounts of human learning remains uncertain and needs to be carefully assessed. Here we argue that the strengths of HBMs for a given task must be evaluated in light of their computational and cognitive costs, and compared to other viabl</context>
</contexts>
<marker>Jones, Love, 2011</marker>
<rawString>Jones, Matt and Bradley C. Love. 2011. Bayesian Fundamentalism or Enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition. Behavioral and Brain Sciences, 34(04):169–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="10005" citStr="Kwiatkowski et al., 2010" startWordPosition="1580" endWordPosition="1583"> (Brown and Hanlon, 1970; Ingram, 1989). One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, and Stevenson, 2013). Since a number of closely related models can all implement the indirect negative evidence approach, the decision of which one to choose for a given task may not be entirely clear. In this paper we compare a range of statistical models consistent with a certain behavior: early overgeneralization, with eventual convergence to the correct target on the basis of exposure to more data. 3 Materials and Methods 3.1 Dative Corpora To emulate a child language acq</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Kwiatkowski, Tom, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1223–1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Kwisthout</author>
<author>Todd Wareham</author>
<author>Iris van Rooij</author>
</authors>
<title>Bayesian intractability is not an ailment that approximation can cure.</title>
<date>2011</date>
<journal>Cognitive Science,</journal>
<volume>35</volume>
<issue>5</issue>
<marker>Kwisthout, Wareham, van Rooij, 2011</marker>
<rawString>Kwisthout, Johan, Todd Wareham, and Iris van Rooij. 2011. Bayesian intractability is not an ailment that approximation can cure. Cognitive Science, 35(5):779–1007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="3874" citStr="Levin (1993)" startWordPosition="572" endWordPosition="573">actability, like reducing the number of classes that need to be learned may also be linguistically and cognitively implausible. For instance, in terms of verb learning, this could 1321 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321–1330, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics take the form of reducing the number of subcategorization frames to the relevant subset, as in (Perfors, Tenenbaum, and Wonnacott, 2010), where only 2 frames are considered for ‘take’, when in fact it is listed in 6 frames by Levin (1993). Finally, comparison of various Bayesian models of the same task is rare (Jones and Love, 2011) and Bayesian inference generally can be demonstrated as simply one class of regularization or smoothing techniques among many others; given the problem at hand, there may well be other, equally compelling regularization methods for dealing with the bias-variance dilemma (e.g., SVMs (Shalizi, 2009)). Consequently, the relevance of HBMs for cognitively accurate accounts of human learning remains uncertain and needs to be carefully assessed. Here we argue that the strengths of HBMs for a given task mu</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES project: tools for analyzing talk. Hillsdale, NJ: Lawrence Erlbaum Associates,</title>
<date>1995</date>
<note>second edition.</note>
<marker>MacWhinney, 1995</marker>
<rawString>MacWhinney, Brian. 1995. The CHILDES project: tools for analyzing talk. Hillsdale, NJ: Lawrence Erlbaum Associates, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary F Marcus</author>
</authors>
<title>Negative evidence in language acquisition.</title>
<date>1993</date>
<journal>Cognition,</journal>
<pages>46--53</pages>
<contexts>
<context position="9052" citStr="Marcus, 1993" startWordPosition="1426" endWordPosition="1427">s of verb forms, then it is not clear how they might recover from the overgeneralization exhibited in sentence (3) above, because they will never receive positive sentences from adults like (3), using fix in a DOD form. As has long been noted, if negative examples were systematically available to learners, then this problem would be solved, since the child would be given evidence that the DOD form is not possible in the adult grammar. However, although parental correction could be considered to be a source of negative evidence, it is neither systematic nor generally available to all children (Marcus, 1993). Even when it does occur, all careful studies have indicated that it seems mostly concerned with semantic appropriateness rather than syntax. In the cases where it is related to syntax, it is often difficult to determine what the correction refers to in the utterance and besides children seem to be oblivious to the correction (Brown and Hanlon, 1970; Ingram, 1989). One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be ex</context>
</contexts>
<marker>Marcus, 1993</marker>
<rawString>Marcus, Gary F. 1993. Negative evidence in language acquisition. Cognition, 46:53–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marr</author>
</authors>
<date>1982</date>
<location>Vision. San Francisco, CA:</location>
<contexts>
<context position="5782" citStr="Marr, 1982" startWordPosition="875" endWordPosition="876">r competition between the evidence for that verb, and the average behavior of verbs belonging to its same class. The results show that combining simple clustering methods along with ordinary maximum likelihood estimation yields a result comparable to HBM performance, providing an alternative account of the same facts, without the computational costs incurred by HBM models that must rely, for example, on Markov Chain Monte Carlo (MCMC) methods for numerically integrating complex likelihood integrals, or on Chinese Restaurant Process (CRP) for producing partitions. In terms of Marr&apos;s hierarchy (Marr, 1982) learning verb alternations is an abstract computational problem (Marr&apos;s type I), solvable by many type II methods combining representations (models, viz. HBMs or LCLs) with particular algorithms. The HBM convention of adopting ideal learning amounts to invoking unbounded algorithmic resources, solvability in principle, even though in practice such methods, even approximate ones, are provably NP-hard (cf. (Kwisthout, Wareham, and van Rooij, 2011)). Assuming cognitive plausibility as a desideratum, we therefore examine whether HBMs can also be approximated by another type II method (LCLs) that </context>
</contexts>
<marker>Marr, 1982</marker>
<rawString>Marr, D. 1982. Vision. San Francisco, CA: W. H. Freeman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aida Nematzadeh</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Child acquisition of multiword verbs: A computational investigation.</title>
<date>2013</date>
<booktitle>Cognitive Aspects of Computational Language Acquisition.</booktitle>
<pages>235--256</pages>
<editor>In A. Villavicencio, T. Poibeau, A. Korhonen, and A. Alishahi, editors,</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="10144" citStr="Nematzadeh, Fazly, and Stevenson, 2013" startWordPosition="1600" endWordPosition="1605">ince Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, and Stevenson, 2013). Since a number of closely related models can all implement the indirect negative evidence approach, the decision of which one to choose for a given task may not be entirely clear. In this paper we compare a range of statistical models consistent with a certain behavior: early overgeneralization, with eventual convergence to the correct target on the basis of exposure to more data. 3 Materials and Methods 3.1 Dative Corpora To emulate a child language acquisition environment we use naturalistic longitudinal child-directed data, from the Brown corpus in CHILDES, for one child (Adam) for a sub</context>
</contexts>
<marker>Nematzadeh, Fazly, Stevenson, 2013</marker>
<rawString>Nematzadeh, Aida, Afsaneh Fazly, and Suzanne Stevenson. 2013. Child acquisition of multiword verbs: A computational investigation. In A. Villavicencio, T. Poibeau, A. Korhonen, and A. Alishahi, editors, Cognitive Aspects of Computational Language Acquisition. Springer, pages 235–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Parisien</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An incremental bayesian model for learning syntactic categories.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL &apos;08,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1950" citStr="Parisien, Fazly, and Stevenson, 2008" startWordPosition="274" endWordPosition="278">hood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs. 1 Introduction In recent years, with advances in probability and estimation theory, there has been much interest in Bayesian models (BMs) (Chater, Tenenbaum, and Yuille, 2006; Jones and Love, 2011) and their application to child language acquisition with its challenging combination of structured information and incomplete knowledge, (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010; Parisien, Fazly, and Stevenson, 2008; Parisien and Stevenson, 2010) as they offer several advantages in this domain. They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic knowledge, from variation at the level of individual lexical items, to cross-item variation, using hyper-parameters to capture observed patterns of both under- and over-generalization as in the acquisition of e.g. dative alternations in En</context>
</contexts>
<marker>Parisien, Fazly, Stevenson, 2008</marker>
<rawString>Parisien, Christopher, Afsaneh Fazly, and Suzanne Stevenson. 2008. An incremental bayesian model for learning syntactic categories. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL &apos;08, pages 89–96, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Parisien</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Learning verb alternations in a usagebased bayesian model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="1981" citStr="Parisien and Stevenson, 2010" startWordPosition="279" endWordPosition="282">mpetition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs. 1 Introduction In recent years, with advances in probability and estimation theory, there has been much interest in Bayesian models (BMs) (Chater, Tenenbaum, and Yuille, 2006; Jones and Love, 2011) and their application to child language acquisition with its challenging combination of structured information and incomplete knowledge, (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010; Parisien, Fazly, and Stevenson, 2008; Parisien and Stevenson, 2010) as they offer several advantages in this domain. They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic knowledge, from variation at the level of individual lexical items, to cross-item variation, using hyper-parameters to capture observed patterns of both under- and over-generalization as in the acquisition of e.g. dative alternations in English (Hsu and Chater, 2010; Pe</context>
</contexts>
<marker>Parisien, Stevenson, 2010</marker>
<rawString>Parisien, Christopher and Suzanne Stevenson. 2010. Learning verb alternations in a usagebased bayesian model. In Proceedings of the 32nd Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Pelleg</author>
<author>Andrew Moore</author>
</authors>
<title>X-means: Extending k-means with efficient estimation of the number of clusters.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>727--734</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="20637" citStr="Pelleg and Moore, 2000" startWordPosition="3426" endWordPosition="3429">, β in eq.3 by their maximal likelihood values calculated from P({yi, ni}iEk|α, β) described before. For models without clustering, estimation is based solely on the observed behavior of verbs. With clustering, same-cluster verbs share some parameters, influencing one another. HBMs place distributions over possible clusters, with estimation derived from averages over distributions. In HBMs, clustering and probability estimation are calculated jointly. In the other models these two estimates are calculated separately, permitting ’plug-and-play’ use of external clustering methods, like X-means (Pelleg and Moore, 2000)1. However, to further assess the impact of cluster assignment on alternative model performance, we also used the clusters that maximize the evidence of the HBM for the DOD and PD counts of the target verbs, and we refer to these as Maximum Evidence (ME) clusters. In MWE clusters, verbs are separated into 3 classes: one if they have counts for both frames; another for only the DOD frame; and a final for only the PD frame. 4 Evaluation The learning task consists of estimating the probability that a given verb occurs in a particular frame, using previous occurrences as the basis for this estimat</context>
</contexts>
<marker>Pelleg, Moore, 2000</marker>
<rawString>Pelleg, Dan and Andrew Moore. 2000. X-means: Extending k-means with efficient estimation of the number of clusters. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 727–734, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Perfors</author>
<author>Joshua B Tenenbaum</author>
<author>Elizabeth Wonnacott</author>
</authors>
<title>Variability, negative evidence, and the acquisition of verb argument constructions.</title>
<date>2010</date>
<journal>Journal of Child Language,</journal>
<pages>37--607</pages>
<contexts>
<context position="1890" citStr="Perfors, Tenenbaum, and Wonnacott, 2010" startWordPosition="265" endWordPosition="269">mpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs. 1 Introduction In recent years, with advances in probability and estimation theory, there has been much interest in Bayesian models (BMs) (Chater, Tenenbaum, and Yuille, 2006; Jones and Love, 2011) and their application to child language acquisition with its challenging combination of structured information and incomplete knowledge, (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010; Parisien, Fazly, and Stevenson, 2008; Parisien and Stevenson, 2010) as they offer several advantages in this domain. They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic knowledge, from variation at the level of individual lexical items, to cross-item variation, using hyper-parameters to capture observed patterns of both under- and over-generaliza</context>
<context position="3771" citStr="Perfors, Tenenbaum, and Wonnacott, 2010" startWordPosition="549" endWordPosition="553">own to be computationally infeasible (Kwisthout, Wareham, and van Rooij, 2011). Approximations proposed to ensure computational tractability, like reducing the number of classes that need to be learned may also be linguistically and cognitively implausible. For instance, in terms of verb learning, this could 1321 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321–1330, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics take the form of reducing the number of subcategorization frames to the relevant subset, as in (Perfors, Tenenbaum, and Wonnacott, 2010), where only 2 frames are considered for ‘take’, when in fact it is listed in 6 frames by Levin (1993). Finally, comparison of various Bayesian models of the same task is rare (Jones and Love, 2011) and Bayesian inference generally can be demonstrated as simply one class of regularization or smoothing techniques among many others; given the problem at hand, there may well be other, equally compelling regularization methods for dealing with the bias-variance dilemma (e.g., SVMs (Shalizi, 2009)). Consequently, the relevance of HBMs for cognitively accurate accounts of human learning remains unc</context>
<context position="10060" citStr="Perfors, Tenenbaum, and Wonnacott, 2010" startWordPosition="1586" endWordPosition="1591">One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, and Stevenson, 2013). Since a number of closely related models can all implement the indirect negative evidence approach, the decision of which one to choose for a given task may not be entirely clear. In this paper we compare a range of statistical models consistent with a certain behavior: early overgeneralization, with eventual convergence to the correct target on the basis of exposure to more data. 3 Materials and Methods 3.1 Dative Corpora To emulate a child language acquisition environment we use naturalistic longitudinal c</context>
<context position="14835" citStr="Perfors, Tenenbaum, and Wonnacott, 2010" startWordPosition="2403" endWordPosition="2408"> mass to some set of priors; smoothing also captures the notion of generalization, making predictions about data that has never been seen by the learner. In the • an acquired class-based expectation of the behavior of a verb, based on its association to similar but more frequent verbs. The former can be readily implemented in terms of prior probability estimates. As we discuss below, class-based estimates arise from one or another clustering method, and can produce more accurate estimates for less frequent verbs based on patterns already learned for more frequent verbs in the same class; see (Perfors, Tenenbaum, and Wonnacott, 2010). In this case, smoothing is a sideeffect of the behavior of a class as a whole. When learning begins, the prior probability is the only source of information for a learner and, as such, dominates the value of the posterior probability. However, in the large sample limit, it is the likelihood that dominates the posterior distribution regardless of the prior. In Hierarchical Bayesian Models both effects are naturally incorporated. The prior distribution is structured as a chain of distributions of parameters and hyper-parameters, and the data may be divided into classes that share some of the </context>
<context position="16765" citStr="Perfors, Tenenbaum, and Wonnacott, 2010" startWordPosition="2747" endWordPosition="2751">e of the data, the unnormalized posterior for the hyperparameters is and the likelihood for α and 0 is The Hierarchical Bayesian Model prediction for Bi is the average of the estimate BikHBM over all possible partitions of the verbs in the task. To simplify the notation we can write BHBM = E ry+ α0l L n + α J (3) where in the expression E[... ] are included the integrals described above and the average of all possible class partitions. Due to this complexity, in practice even small data sets require the use of MCMC methods, and statistical models for partitions, like CRP (Gelman et al., 2003; Perfors, Tenenbaum, and Wonnacott, 2010). This complexity also calls into question the cognitive fidelity of such approaches. Eq.3 is particularly interesting because by fixing α and 0 (instead of averaging over them) it is possible to deduce simpler (and classical) models: MLE corresponds to α = 0; the so called “add-one” smoothing (referred in this paper as L1) corresponds to α = 2 and 0 = 1/2. From Eq.3 it is also clear that if α and 0 (or their distributions) are unchanged, as the evidence of a verb grows (n → oo), the HBM estimate approaches MLE&apos;s, (BHBM → BMLE). On the other hand, when α &gt;&gt; n, BHBM ∼ 0, so that 0 can be inter</context>
<context position="18664" citStr="Perfors, Tenenbaum, and Wonnacott, 2010" startWordPosition="3082" endWordPosition="3086">DOD and PD counts for each verb), in LCL cluster1325 ing may be done using more general contexts that employ a variety of linguistic and environmental attributes. For LCL the prior and class-based information are incorporated as: yi + αCβC θLCL = (4) ni + αC where αC and βC are defined via justifiable heuristic expressions dependent solely on the statistics of the class attributed to each verb i. The strength of the prior (αC) is a monotonic function of the number of elements (mC) in the class C, excluding the target verb vi. To approximate the gold standard behavior of the HBM for this task (Perfors, Tenenbaum, and Wonnacott, 2010) we chose the following function for αC: αC = mC3/2(1 − mC−1/5) + 0.1 (5) with the strength of the prior for the LCL model depending on the number of verbs in the class, not on their frequency. Eq.5 was chosen as a good fit to HBMs, without incurring their complexity. The powers are simple fractions, not arbitrary numbers. A best fit was not attempted due to the lack of assessment of how accurate HBMs are on real data. The prior value (βC) is a smoothed estimation of the probability of DOD in a given class, combining the evidence for all verbs in that class: YC + 1/2 βC = (6) NC + 1 in this c</context>
</contexts>
<marker>Perfors, Tenenbaum, Wonnacott, 2010</marker>
<rawString>Perfors, Amy, Joshua B. Tenenbaum, and Elizabeth Wonnacott. 2010. Variability, negative evidence, and the acquisition of verb argument constructions. Journal of Child Language, (37):607–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>151--175</pages>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Ratnaparkhi, Adwait. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, pages 151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosma R Shalizi</author>
</authors>
<title>Dynamics of bayesian updating with dependent data and misspecified models.</title>
<date>2009</date>
<journal>ElectroCosmanic Journal of Statistics,</journal>
<pages>3--1039</pages>
<contexts>
<context position="4269" citStr="Shalizi, 2009" startWordPosition="635" endWordPosition="636">ducing the number of subcategorization frames to the relevant subset, as in (Perfors, Tenenbaum, and Wonnacott, 2010), where only 2 frames are considered for ‘take’, when in fact it is listed in 6 frames by Levin (1993). Finally, comparison of various Bayesian models of the same task is rare (Jones and Love, 2011) and Bayesian inference generally can be demonstrated as simply one class of regularization or smoothing techniques among many others; given the problem at hand, there may well be other, equally compelling regularization methods for dealing with the bias-variance dilemma (e.g., SVMs (Shalizi, 2009)). Consequently, the relevance of HBMs for cognitively accurate accounts of human learning remains uncertain and needs to be carefully assessed. Here we argue that the strengths of HBMs for a given task must be evaluated in light of their computational and cognitive costs, and compared to other viable alternatives. The focus should be on finding the simplest statistical models consistent with a given behavior, particularly one that aligns with known cognitive limitations. In the case of many language acquisition tasks this behavior often takes the form of overgeneralization, but with eventual </context>
</contexts>
<marker>Shalizi, 2009</marker>
<rawString>Shalizi, Cosma R. 2009. Dynamics of bayesian updating with dependent data and misspecified models. ElectroCosmanic Journal of Statistics, 3:1039–1074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>638--647</pages>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Sun, Lin and Anna Korhonen. 2009. Improving verb clustering with automatically acquired selectional preferences. In EMNLP, pages 638– 647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>The Acquisition of a Uni�cation-Based Generalised Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Laboratory, University of Cambridge.</institution>
<contexts>
<context position="9978" citStr="Villavicencio, 2002" startWordPosition="1578" endWordPosition="1579">ous to the correction (Brown and Hanlon, 1970; Ingram, 1989). One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, and Stevenson, 2013). Since a number of closely related models can all implement the indirect negative evidence approach, the decision of which one to choose for a given task may not be entirely clear. In this paper we compare a range of statistical models consistent with a certain behavior: early overgeneralization, with eventual convergence to the correct target on the basis of exposure to more data. 3 Materials and Methods 3.1 Dative Corpora To e</context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Villavicencio, Aline. 2002. The Acquisition of a Uni�cation-Based Generalised Categorial Grammar. Ph.D. thesis, Computer Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Wonnacott</author>
<author>Elissa L Newport</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Acquiring and processing verb argument structure: Distributional learning in a miniature language. Cognitive Psychology,</title>
<date>2008</date>
<pages>56--165</pages>
<contexts>
<context position="2713" citStr="Wonnacott, Newport, and Tanenhaus, 2008" startWordPosition="390" endWordPosition="394">nd ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic knowledge, from variation at the level of individual lexical items, to cross-item variation, using hyper-parameters to capture observed patterns of both under- and over-generalization as in the acquisition of e.g. dative alternations in English (Hsu and Chater, 2010; Perfors, Tenenbaum, and Wonnacott, 2010), and verb frames in a controlled artificial language (Wonnacott, Newport, and Tanenhaus, 2008). HBMs can thus be viewed as providing a “rational” upper bound on language learnability, yielding optimal models that account for observed data while minimizing any required prior information. In addition, the clustering implicit in HBM modeling introduces additional parameters that can be tuned to specific data patterns. However, this comes at a well-known price: HBMs generally are also ideal learning systems, known to be computationally infeasible (Kwisthout, Wareham, and van Rooij, 2011). Approximations proposed to ensure computational tractability, like reducing the number of classes tha</context>
</contexts>
<marker>Wonnacott, Newport, Tanenhaus, 2008</marker>
<rawString>Wonnacott, Elizabeth, Elissa L. Newport, and Michael K. Tanenhaus. 2008. Acquiring and processing verb argument structure: Distributional learning in a miniature language. Cognitive Psychology, 56:165–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Yang</author>
</authors>
<title>Three factors in language variation.</title>
<date>2010</date>
<journal>Lingua,</journal>
<pages>120--1160</pages>
<contexts>
<context position="12073" citStr="Yang, 2010" startWordPosition="1928" endWordPosition="1929">step is demonstrating that simpler alternative models can replicate HBM performance on their own terms. Therefore, the gold standard we use for evaluation is the classification predicted by Perfors, Tenenbaum, and Wonnacott (2010). The evaluations reported in our analysis take into account intrinsic characteristics of each model in relation to the likelihoods of the verbs, to determine the extent to which the models go beyond the data they were exposed to, discussed in section 2. Further, since it has been argued that very low frequency verbs may not yet be firmly placed in a child’s lexicon (Yang, 2010; Gropen et al., 1989), at each epoch we also impose a low-frequency threshold of 5 occurrences, considering only verbs that the learner has seen at least 5 times. This use of a low-frequency threshold for learning has extensive support in the literature for learning 1323 of all kinds in both human and non-human animals, e.g. (Gallistel, 2002). A cut-off frequency in this range has also commonly been used in NLP tasks like POS tagging (Ratnaparkhi,1999). context of verb learning smoothing could be based on several principles: • an (innate) expectation as to how verbs in general should behave; </context>
</contexts>
<marker>Yang, 2010</marker>
<rawString>Yang, Charles. 2010. Three factors in language variation. Lingua, 120:1160–1177.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>