<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000202">
<title confidence="0.892609">
Analyzing the Reading Comprehension Task
</title>
<author confidence="0.631057">
Amit Bagga
</author>
<affiliation confidence="0.3904265">
GE Corporate Research and Development
1 Research Circle
</affiliation>
<address confidence="0.563301">
Niskayuna, NY 12309
</address>
<email confidence="0.990586">
bagga@crcl.ge.cort
</email>
<sectionHeader confidence="0.995169" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987969">
In this paper we describe a method for analyzing
the reading comprehension task. First, we describe
a method of classifying facts (information) into cat-
egories or levels; where each level signifies a different
degree of difficulty of extracting a fact from a piece
of text containing it. We then proceed to show how
one can use this model the analyze the complexity
of the reading comprehension task. Finally, we ana-
lyze five different reading comprehension tasks and
present results from this analysis.
</bodyText>
<sectionHeader confidence="0.997864" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999872789473684">
Recently there has been a spate of activity for build-
ing question-answering systems (QA systems) driven
largely by the recently organized QA track at the
Eighth Text Retrieval Conference (TREC-8) (Har-
man, 1999). This increase in research activity has
also fueled research in a related area: building Read-
ing Comprehension systems (Hirschman and others,
1999). But while a number of successful systems
have been developed for each of these tasks, little,
if any, work has been done on analyzing the com-
plexities of the tasks themselves. In this paper we
describe a method of classifying facts (information)
into categories or levels; where each level signifies
a different degree of difficulty of extracting a fact
from a piece of text containing it. We then proceed
to show how one can use this model the analyze the
complexity of the reading comprehension task. Fi-
nally, we analyze five different reading comprehen-
sion tasks and present results from this analysis.
</bodyText>
<sectionHeader confidence="0.9755845" genericHeader="introduction">
2 The Complexity of Extracting a
Fact From Text
</sectionHeader>
<bodyText confidence="0.999990394736842">
Any text document is a collection of facts (infor-
mation). These facts may be explicitly or implicitly
stated in the text. In addition, there are &amp;quot;easy&amp;quot; facts
which may be found in a single sentence (example:
the name of a city) as well as &amp;quot;difficult&amp;quot; facts which
are spread across several sentences (example: the
reason for a particular event).
For a computer system to be able to process text
documents in applications like information extrac-
tion (IE), question answering, and reading compre-
hension, it has to have the ability to extract facts
from text. Obviously, the performance of the system
will depend upon the type of fact it has to extract:
explicit or implicit, easy or difficult, etc. (by no
means is this list complete). In addition, the perfor-
mance of such systems varies greatly depending on
various additional factors including known vocabu-
lary, sentence length, the amount of training, quality
of parsing, etc. Despite the great variations in the
performances of such systems, it has been hypothe-
sized that there are facts that are simply harder to
extract than others (Hirschman, 1992).
In this section we describe a method for estimat-
ing the complexity of extracting a fact from text.
The proposed model was initially used to analyze the
information extraction task (Bagga and Bierrnann,
1997). In addition to verifying Hirschman&apos;s hypoth-
esis, the model also provided us with a framework
for analyzing and understanding the performance of
several IE systems (Bagga and Biermann, 1998). We
have also proposed using this model to analyze the
complexity of the QA task which is related to both
the IE, and the reading comprehension tasks (Bagga
et al., 1999). The remainder of this section describes
the model in detail, and provides a sample applica-
tion of the model to an IE task. In the following
section, we discuss how this model can be used to
analyze the reading comprehension task.
</bodyText>
<sectionHeader confidence="0.447299" genericHeader="method">
2.1 Definitions
Network:
</sectionHeader>
<bodyText confidence="0.9999305">
A network consists of a collection of nodes intercon-
nected by an accompanying set of arcs. Each node
denotes an object and each arc represents a binary
relation between the objects. (Hendrix, 1979)
</bodyText>
<sectionHeader confidence="0.86159" genericHeader="method">
A Partial Network:
</sectionHeader>
<bodyText confidence="0.999819666666667">
A partial network is a collection of nodes intercon-
nected by an accompanying set of arcs where the
collection of nodes is a subset of a collection of nodes
forming a network, and the accompanying set of arcs
is a subset of the set of arcs accompanying the set
of nodes which form the network.
</bodyText>
<page confidence="0.993777">
35
</page>
<figure confidence="0.951117">
have claimed
</figure>
<figureCaption confidence="0.999968">
Figure 1: A Sample Network
</figureCaption>
<bodyText confidence="0.927464714285714">
Figure 1 shows a sample network for the following
piece of text:
&amp;quot;The Extraditables,&amp;quot; or the Armed Branch
of the Medellin Cartel have claimed respon-
sibility for the murder of two employees of
Bogota&apos;s daily El Espectador on Nov 15.
The murders took place in Medellin.
</bodyText>
<subsectionHeader confidence="0.998599">
2.2 The Level of A Fact
</subsectionHeader>
<bodyText confidence="0.9992415">
The level of a fact, F, in a piece of text is defined
by the following algorithm:
</bodyText>
<listItem confidence="0.994571">
1. Build a network, S, for the piece of text.
2. Identify the nodes that are relevant to the fact,
</listItem>
<bodyText confidence="0.972331625">
F. Suppose {xi, , , xr,) are the nodes rel-
evant to F. Let s be the partial network con-
sisting of the set of nodes {1,x2,.,,x} inter-
connected by the set of arcs ft&apos;, t2,
We define the levet of the fact, F, with respect to
the network, S to be equal to k, the number of
arcs linking the nodes which comprise the fact
Fins.
</bodyText>
<subsectionHeader confidence="0.888639">
2.2.1 Observations
</subsectionHeader>
<bodyText confidence="0.997683">
Given the definition of the level of a fact, the follow-
ing observations can be made:
</bodyText>
<listItem confidence="0.995605454545455">
• The level of a fact is related to the concept
of &amp;quot;semantic vicinity&amp;quot; defined by Schubert et.
al. (Schubert and others, 1979). The semantic
vicinity of a node in a network consists of the
nodes and the arcs reachable from that node by
traversing a small number of arcs. The funda-
mental assumption used here is that &amp;quot;the knowl-
edge required to perform an intellectual task
generally lies in the semantic vicinity of the con-
cepts involved in the task&amp;quot; (Schubert and oth-
ers, 1979).
</listItem>
<bodyText confidence="0.9916085">
The level of a fact is equal to the number of
arcs that one needs to traverse to reach all the
concepts (nodes) which comprise the fact of in-
terest.
</bodyText>
<listItem confidence="0.986878545454546">
• A level-0 fact consists of a single node (i.e. no
transitions) in a network.
• A level-k fact is a union of k level-1 facts.
• Conjunctions/disjunctions increase the level of
a fact.
• A higher level fact is likely to be harder to ex-
tract than a lower level fact.
• A fact appearing at one level in a piece of text
may appear at some other level in the same
piece of text.
• The level of a fact in a piece of text depends
</listItem>
<bodyText confidence="0.9827763">
on the granularity of the network constructed
for that piece of text. Therefore, the level of a
fact with respect to a network built at the word
level (i.e. words represent objects and the re-
lationships between the objects) will be greater
than the level of a fact with respect to a network
built at the phrase level (i.e. noun groups repre-
sent objects while verb groups and preposition
groups represent the relationships between the
objects).
</bodyText>
<subsectionHeader confidence="0.767064">
2.2.2 Examples
</subsectionHeader>
<bodyText confidence="0.992912">
Let S be the network shown in Figure 1. S has been
built at the phrase level.
</bodyText>
<listItem confidence="0.966084">
• The city mentioned, in S, is an example of a
level-0 fact because the &amp;quot;city&amp;quot; fact consists only
of one node &amp;quot;Medellin.&amp;quot;
• The type of attack, in 5, is an example of a
level-1 fact.
</listItem>
<page confidence="0.991236">
36
</page>
<bodyText confidence="0.9865048">
We define the type of attack in the network to be
an attack designator such as &amp;quot;murder,&amp;quot; &amp;quot;bomb-
ing,&amp;quot; or &amp;quot;assassination&amp;quot; with one modifier giv-
ing the victim, perpetrator, date, location, or
other information.
In this case the type of attack fact is composed
of the &amp;quot;the murder&amp;quot; and the &amp;quot;two employees&amp;quot;
nodes and their connector. This makes the type
of attack a level-1 fact.
The type of attack could appear as a level-0 fact
as in &amp;quot;the Medellin bombing&amp;quot; (assuming that
the network is built at the phrase level) because
in this case both the attack designator (bomb-
ing) and the modifier (Medellin) occur in the
same node. The type of attack fact occurs as a
level-2 fact in the following sentence (once again
assuming that the network is built at the phrase
level): &amp;quot;10 people were killed in the offensive
which included several bombings.&amp;quot; In this case
there is no direct connector between the attack
designator (several bombings) and its modifier
(10 people). They are connected by the inter-
mediatory &amp;quot;the offensive&amp;quot; node; thereby making
the type of attack a level-2 fact. The type of at-
tack can also appear at higher levels.
</bodyText>
<listItem confidence="0.8186585">
• In S, the date of the murder of the two employ-
ees is an example of a level-2 fact.
</listItem>
<bodyText confidence="0.999839916666667">
This is because the attack designator (the mur-
der) along with its modifier (two employees) ac-
count for one level and the arc to &amp;quot;Nov 15&amp;quot; ac-
counts for the second level.
The date of the attack, in this case, is not a
level-1 fact (because of the two nodes &amp;quot;the mur-
der&amp;quot; and &amp;quot;Nov 15&amp;quot;) because the phrase &amp;quot;the
murder on Nov 15&amp;quot; does not tell one that an at-
tack actually took place. The article could have
been talking about a seminar on murders that
took place on Nov 15 and not about the murder
of two employees which took place then.
</bodyText>
<listItem confidence="0.8184335">
• In 8, the location of the murder of the two em-
ployees is an example of a level-2 fact.
</listItem>
<bodyText confidence="0.997510454545455">
The exact same argument as the date of the
murder of the two employees applies here.
• The complete information, in S, about the vic-
tims is an example of a level-2 fact because to
know that two employees of Bogota&apos;s Daily El
Espectador were victims, one has to know that
they were murdered. The attack designator (the
murder) with its modifier (two employees) ac-
counts for one level, while the connector be-
tween &amp;quot;two employees&amp;quot; and &amp;quot;Bogota&apos;s Daily El
Espectador&amp;quot; accounts for the other.
</bodyText>
<subsectionHeader confidence="0.998294">
2.3 Building the Networks
</subsectionHeader>
<bodyText confidence="0.998955">
As mentioned earlier, the level of a fact for a piece
of text depends on the network constructed for the
text. Since there is no unique network corresponding
to a piece of text, care has to be taken so that the
networks are built consistently.
We used the following algorithm to build the net-
works:
</bodyText>
<listItem confidence="0.987993181818182">
1. Every article was broken up into a non-
overlapping sequence of noun groups (NGs),
verb groups (VGs), and preposition groups
(PGs). The rules employed to identify the NGs,
VGs, and PGs were almost the same as the ones
employed by SRI&apos;s FASTUS system&apos;.
2. The nodes of the network consisted of the NGs
while the transitions between the nodes con-
sisted of the VGs and the PCs.
3. Identification of coreferent nodes and preposi-
tional phrase attachments were done manually.
</listItem>
<bodyText confidence="0.999767666666667">
The networks are built based largely upon the syn-
tactic structure of the text contained in the articles.
However, there is some semantics encoded into the
networks because identification of coreferent nodes
and preposition phrase attachments are done manu-
ally.
Obviously, if one were to employ a different al-
gorithm for building the networks, one would get
different numbers for the level of a fact, But, if the
algorithm were employed consistently across all the
facts of interest and across all articles in a domain,
the numbers on the level of a fact would be consis-
tently different and one would still be able to analyze
the relative complexity of extracting that fact from
a piece of text in the domain.
</bodyText>
<listItem confidence="0.394471333333333">
3 Example: Analyzing the
Complexity of an Information
Extraction Task
</listItem>
<bodyText confidence="0.999512307692308">
In order to validate our model of complexity we ap-
plied it to the Information Extraction (IE) task,
or the Message Understanding task (DAR, 1991),
(DAR, 1992), (ARP, 1993), (DAR, 1995), (DAR,
1998). The goal of an LE task is to extract pre-
specified facts from text and fill in predefined tem-
plates containing labeled slots.
We analyzed the complexity of the task used
for the Fourth Message Understanding Conference
(MUC-4) (DAR, 1992). In this task, the partici-
pants were asked to extract the following facts from
articles describing terrorist activities in Latin Amer-
ica:
</bodyText>
<listItem confidence="0.999127666666667">
• The type of attack.
• The date of the attack.
• The location of the attack.
</listItem>
<footnote confidence="0.9890235">
1We wish to thank Jerry Hobbs of SRI for providing us
with the rules of their partial parser.
</footnote>
<page confidence="0.995571">
37
</page>
<figure confidence="0.999688694444444">
Number of Facts
110
105
100
95
90
85
80
75
70
65
60
55
50
45
40
35
30
25
20
15
10
5
/11
Ii
0 1 2
Mack Fact -e-- -
Date Fact
Location Fact - --
Victim Fact -,x -
Perpetrator Fact -A--
&apos;y •
S&apos;SS.t:-, .......
.. ... ..&amp;quot;1 ..... I I ----
3 5 6 7 8 9 10 11 12 13 14 15
Levels
</figure>
<figureCaption confidence="0.958171">
Figure 2: MUC-4: Level Distribution of Each of the Five Facts
</figureCaption>
<figure confidence="0.997685384615384">
50
45
40
35
30
25
20
15
10
5
0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Levels
</figure>
<figureCaption confidence="0.973734">
Figure 3: MUC-4: Level Distribution of the Five
Facts Combined
</figureCaption>
<listItem confidence="0.9915215">
• The victim (including damage to property).
• The perpetrator(s) (including suspects).
</listItem>
<bodyText confidence="0.999840464285714">
We analyzed a set of 100 articles from the MUC-4
domain each of which reported one or more terror-
ist attacks. Figure 2 shows the level distribution for
each of the five facts. A closer analysis of the figure
shows that the &amp;quot;type of attack&amp;quot; fact is the easiest to
extract while the &amp;quot;perpetrator&amp;quot; fact is the hardest
(the curve peaks at level-2 for this fact). In addition,
Figure 3 shows the level distribution of the five facts
combined. This figure gives some indication of the
complexity of the MUC-4 task because it shows that
almost 50% of the MUC-4 facts occur at level-1. The
expected level of the five facts in the MUC-4 domain
was 1.74 (this is simply the weighted average of the
level distributions of the facts). We define this num-
ber to be the Task Complexity for the MUC-4 task.
Therefore, the MUC-4 task can now be compared to,
say, the MUC-5 task by comparing their Task Com-
plexities. In fact, we computed the Task Complexity
of the MUC-5 task and discovered that it was equal
to 2.5. In comparison, an analysis, using more &amp;quot;su-
perficial&amp;quot; features, done by Beth Sundheim, shows
that the nature of the MUC-5 EJV task is approx-
imately twice as hard as the nature of the MUC-4
task (Sundheim, 1993). The features used in the
study included vocabulary size, the average number
of words per sentence, and the average number of
sentences per article. More details about this anal-
ysis can be found in (Bagga and Bierrnann, 1998).
</bodyText>
<sectionHeader confidence="0.990092" genericHeader="method">
4 Analyzing the Reading
Comprehension Task
</sectionHeader>
<bodyText confidence="0.998941818181818">
The reading comprehension task differs from the QA
task in the following way: while the goal of the QA
task is to find answers for a set of questions from a
collection, of documents, the goal of the reading com-
prehension task is to find answers to a set of ques-
tions from a single related document. Since the QA
task involves extracting answers from a collection of
documents, the complexity of this task depends on
the expected level of occurrence of the answers of
the questions. While it is theoretically possible to
compute the average level of any fact in the entire
</bodyText>
<figure confidence="0.94081475">
Percentage of Facts
.............
MIX 4:5 Facts -0— _
4 4 4 4 4
</figure>
<page confidence="0.993638">
38
</page>
<table confidence="0.989809">
Test # of avg # of avg # of # of avg # of avg # of
sentences levels/sent corefs/sent questions levels/answer corefs/answer
Basic 9 4.11 2.33 8 3.75 2.25
Basic-Interm 13 2.69 2.39 6 3.33 2.50
Intermediate 56 3.50 2.55 9 4.44 3.33
Interm-Adv 17 6.47 1.00 6 7.83 1.33
Advanced 27 6.93 2.08 10 8.20 2.90
</table>
<figureCaption confidence="0.99949">
Figure 4: Summary of Results
</figureCaption>
<bodyText confidence="0.9999">
document collection, it is not humanly possible to
analyze every document in such large collections to
compute this. For example, the TREC collection
used for the QA track is approximately 5GB. How-
ever, since the reading comprehension task involves
extracting the answers from a single document, it is
possible to analyze the document itself in addition
to computing the level of the occurrence of each an-
swer. Therefore, the results presented in this paper
will provide both these values.
</bodyText>
<subsectionHeader confidence="0.994598">
4.1 Analysis and Results
</subsectionHeader>
<bodyText confidence="0.999815176470588">
We analyzed a set of five reading comprehension
tests offered by the English Language Center at
the University of Victoria in Canada 2 . These
five tests are listed in increasing order of diffi-
culty and are classified by the Center as: Ba-
sic, Basic-Intermediate, Intermediate, Intermediate-
Advanced, and Advanced. For each of these tests, we
calculated the level number of each sentence in the
text, and the level number of the sentences contain-
ing the answers to each question for every test. In
addition, we also calculated the number of corefer-
ences present in each sentence in the texts, and the
corresponding number in the sentences containing
each answer. It should be noted that we were forced
to calculate the level number of the sentences con-
taining the answer as opposed to calculating the level
number of the answer itself because several ques-
tions had only true/false answers. Since there was
no way to compute the level numbers of true/false
answers, we decided to calculate the level numbers of
the sentences containing the answers in order to be
consistent. For true/false answers this implied an-
alyzing all the sentences which help determine the
truth value of the question.
Figure 4 shows for each text, the number of sen-
tences in the text, the average level number of a sen-
tence, the average number of coreferences per sen-
tence, the number of questions corresponding to the
test, the average level number of each answer, and
the average number of coreferences per answer.
The results shown in Figure 4 are consistent with
the model. The figure shows that as the difficulty
level of the tests increase, so do the corresponding
level numbers per sentence, and the answers. One
</bodyText>
<footnote confidence="0.781079">
2http://web2.uvcs.uvic.ca/elc/studyzone/index.htm
</footnote>
<bodyText confidence="0.9999541875">
conclusion that we can draw from the numbers is
that the Basic-Intermediate test, based upon the
analysis, is slightly more easy than the Basic test.
We will address this issue in the next section.
The numbers of coreferences, surprisingly, do no
increase with the difficulty of the tests. However,
a closer look at the types of coreference shows that
while most of the coreferences in the first two tests
(Basic, and Basic-Intermediate) are simple pronom-
inal coreferences (he, she, it, etc.), the coreferences
used in the last two tests (Intermediate-Advanced,
and Advanced) require more knowledge to process.
Some examples include marijuana coreferent with
the drug, hemp with the pant, etc. Not being able
to capture the complexity of the coreferences is one,
among several, shortcomings of this model.
</bodyText>
<subsectionHeader confidence="0.97116">
4.2 A Comparison with Qanda
</subsectionHeader>
<bodyText confidence="0.99061864">
MITRE 3 ran its Qanda reading comprehension sys-
tem on the five tests analyzed in the previous sec-
tion. However, instead of producing a single answer
for each question, Qanda produces a list of answers
listed in decreasing order of confidence. The rest of
this section describes an evaluation of Qanda&apos;s per-
formance on the five tests and a comparison with
the analysis done in the previous section.
In order to evaluate Qanda&apos;s performance on the
five tests we decided to use the Mean Reciprocal
Answer Rank (MRAR) technique which was used
for evaluating question-answering systems at TREC-
8 (Singhal, 1999). For each answer, this techniques
assigns a score between 0 and 1 depending on its
rank in the list of answers output. The score for
answer, i, is computed as:
Scorei = rank of answers
If no correct answer is found in the list, a score of
0 is assigned. Therefore, MRAR for a reading com-
prehension test is the sum of the scores for answers
corresponding to each question for that test.
Figure 5 summarizes Qanda&apos;s results for the five
tests. The figure shows, for each test, the number of
questions, the cumulative MRAR for all answers for
the test, and the average MRAR per answer.
</bodyText>
<footnote confidence="0.9830955">
3We would like to thank Marc Light and Eric Breck for
their help with running Qanda on our data.
</footnote>
<page confidence="0.698055">
1
39
</page>
<table confidence="0.975943571428571">
Test # of MRAR for avg MRAR
questions all answers per answer
Basic 8 2.933 0.367
Basic-Interm 6 3.360 0.560
Intermediate 9 2029. 0.226
Interm-Adv 6 1.008 0.168
Advanced 10 7.833 0.783
</table>
<figureCaption confidence="0.998967">
Figure 5: Summary of Qanda&apos;s Results
</figureCaption>
<bodyText confidence="0.9999649375">
The results from Qanda are more or less consis-
tent with the analysis done earlier. Except for the
Advanced test, the average Mean Reciprocal Answer
Rank is consistent with the average number of levels
per sentence (from Figure 4). It should be pointed
out that the system performed significantly better on
the Basic-Intermediate Test compared to the Basic
test consistent with the numbers in Figure 4. How-
ever, contrary to expectation, Qanda performed ex-
ceedingly well on the Advanced test answering 7 out
of the 10 questions with answers whose rank is 1 (i.e.
the first answer among the list of possible answers
for each question is the correct one). We are cur-
rently consulting the developers of the system for
conducting an analysis of the performance on this
test in more detail.
</bodyText>
<sectionHeader confidence="0.996476" genericHeader="method">
5 Shortcomings
</sectionHeader>
<bodyText confidence="0.998805826923077">
This measure is just the beginning of a search for
useful complexity measures. Although the measure
is a big step up from the measures used earlier, it has
a number of shortcomings. The main shortcoming is
the ambiguity regarding the selection of nodes from
the network regarding the fact of interest. Consider
the following sentence: &amp;quot;This is a report from the
Straits of Taiwan. .... Yesterday, China test
fired a missile.&amp;quot; Suppose we are interested in the
location of the launch of the missile. The ambiguity
here arises from the fact that the article does not
explicitly mention that the missile was launched in
the Straits of Taiwan. The decision to infer that
fact from the information present depends upon the
person building the network.
In addition, the measure does not account for the
following factors (the list is not complete):
coreference: If the extraction of a fact requires the
resolution of several coreferences, it is clearly
more difficult than an extraction which does
not. In addition, the degree of difficulty of re-
solving coreferences itself varies from simple ex-
act matches, and pronominal coreferences, to
ones that require external world knowledge.
frequency of answers: The frequency of occur-
rence of facts in a collection of documents has
an impact on the performance of systems.
occurrence of multiple (similar) facts;
Clearly, if several similar facts are present
in the same article, the systems will find it
harder to extract the correct fact.
vocabulary size: Unknown words present some
problems to systems making it harder for them
to perform well.
On the other hand, no measure can take into ac-
count all possible features in natural language. Con-
sider the following example. In an article, suppose
one initially encounters a series of statements that
obliquely imply that the following statement is false.
Then the statement is given: &amp;quot;Bill Clinton visited
Taiwan last week.&amp;quot; Processing such discourse re-
quires an ability to perfectly understand the initial
series of statements before the truth value of the last
statement can be properly evaluated. Such complete
understanding is beyond the state of the art and is
likely to remain so for many years.
Despite these shortcomings, the current measure
does quantify complexity on one very important di-
mension, namely the number of clauses (or phrases)
required to specify a fact. For the short term it
appears to be the best available vehicle for under-
standing the complexity of extracting a fact.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9985886">
In this paper we have described a model that can be
used to analyze the complexity of a reading compre-
hension task. The model has been used to analyze
five different reading comprehension tests, and the
paper presents the results from the analysis.
</bodyText>
<sectionHeader confidence="0.998819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990839333333333">
ARPA. 1993. Fifth Message Understanding Confer-
ence (MUC-5), San Mateo, August. Morgan Kauf-
mann Publishers, Inc.
Amit Bagga and Alan W. Bien-flaw&apos;. 1997. Ana-
lyzing the Complexity of a Domain With Respect
To An Information Extraction Task. In Tenth In-
ternational Conference on Research on Computa-
tional Linguistics (ROCLING X), pages 175-194,
August.
Amit Bagga and Alan W. Aierniann. 1998. Ana-
lyzing the Performance of Message Understand-
ing Systems. Journal of Computational Linguis-
</reference>
<page confidence="0.972">
40
</page>
<reference confidence="0.995099705882353">
tics and Chinese Language Processing, 3(1):1-26,
February.
Amit Bagga, Wlodek Zadrozny, and James Puste-
jai/sky. 1999. Semantics and Complexity of Ques-
tion Answering Systems: Towards a Moore&apos;s Law
for Natural Language Engineering. In 1999 AAAI
Fall Symposium Series on Question Answering
Systems, pages 1-10, November.
DARPA. 1991. Third Message Understanding Con-
ference (MUC-3), San Mateo, May. Morgan Kauf-
mann Publishers, Inc.
DARPA. 1992. Fourth Message Understanding
Conference (MUC-4), San Mateo, June. Morgan
Kaufmann Publishers, Inc.
DARPA: TIPSTER Text Program. 1995. Sixth
Message Understanding Conference (MUC-6),
San Mateo, November. Morgan Kaufmann Pub-
lishers, Inc.
DARPA: TIPSTER Text Program. 1998. Seventh
Message Understanding Conference (MUC-
7). http://www.muc.saic.com/proceedings-
muc_7_toc.httnl, April.
D. K. Harman, editor. 1999. Eighth Text RE-
trieval Conference (TREC-8). National Institute
of Standards and Technology (MST), U.S. De-
partment of Commerce, National Technical Infor-
mation Service, November,
Gary G. Hendrix, 1979. Encoding Knowledge in
Partitioned Networks. In Nicholas V. Findler, edi-
tor, Associative Networks, pages 51-92. Academic
Press, New York.
Lynette Hirschman et al. 1999. Deep Read: A Read-
ing Comprehension System. In 37th Annual Meet-
ing of the Association of Computational Linguis-
tics, pages 325-332, June,
Lynette Hirschman. 1992. An Adjunct Test for
Discourse Processing in MUC-4. In Fourth Mes-
sage Understanding Conference (MUC-4) (DAR,
1992), pages 67-77,
Lenhart K. Schubert et al. 1979. The Structure and
Organization of a Semantic Net for Comprehen-
sion and Inference. In Nicholas V. Findler, editor,
Associative Networks, pages 121-175. Academic
Press, New York.
Amit Singhal. 1999. Question Answering Track at
TREC-8. http://www. research. att. com/- sin ghal/
ga-track-spec.txt, November.
Beth M. Sundheim. 1993. Tipster/MUC-5 Informa-
tion Extraction System Evaluation. In Fifth Mes-
sage Understanding Conference (MUC-5) (ARP,
1993), pages 27-44.
</reference>
<page confidence="0.999447">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999349">Analyzing the Reading Comprehension Task</title>
<author confidence="0.790287">Amit</author>
<affiliation confidence="0.9208175">GE Corporate Research and 1 Research</affiliation>
<address confidence="0.989612">Niskayuna, NY</address>
<email confidence="0.981528">bagga@crcl.ge.cort</email>
<abstract confidence="0.998618565891472">In this paper we describe a method for analyzing the reading comprehension task. First, we describe a method of classifying facts (information) into categories or levels; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it. We then proceed to show how one can use this model the analyze the complexity of the reading comprehension task. Finally, we analyze five different reading comprehension tasks and present results from this analysis. Recently there has been a spate of activity for building question-answering systems (QA systems) driven largely by the recently organized QA track at the Eighth Text Retrieval Conference (TREC-8) (Harman, 1999). This increase in research activity has also fueled research in a related area: building Reading Comprehension systems (Hirschman and others, 1999). But while a number of successful systems have been developed for each of these tasks, little, if any, work has been done on analyzing the complexities of the tasks themselves. In this paper we describe a method of classifying facts (information) into categories or levels; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it. We then proceed to show how one can use this model the analyze the complexity of the reading comprehension task. Finally, we analyze five different reading comprehension tasks and present results from this analysis. 2 The Complexity of Extracting a Fact From Text Any text document is a collection of facts (information). These facts may be explicitly or implicitly stated in the text. In addition, there are &amp;quot;easy&amp;quot; facts which may be found in a single sentence (example: the name of a city) as well as &amp;quot;difficult&amp;quot; facts which are spread across several sentences (example: the reason for a particular event). For a computer system to be able to process text documents in applications like information extraction (IE), question answering, and reading comprehension, it has to have the ability to extract facts from text. Obviously, the performance of the system will depend upon the type of fact it has to extract: explicit or implicit, easy or difficult, etc. (by no means is this list complete). In addition, the performance of such systems varies greatly depending on various additional factors including known vocabulary, sentence length, the amount of training, quality of parsing, etc. Despite the great variations in the performances of such systems, it has been hypothesized that there are facts that are simply harder to extract than others (Hirschman, 1992). In this section we describe a method for estimating the complexity of extracting a fact from text. The proposed model was initially used to analyze the information extraction task (Bagga and Bierrnann, 1997). In addition to verifying Hirschman&apos;s hypoththe model provided us a framework for analyzing and understanding the performance of several IE systems (Bagga and Biermann, 1998). We have also proposed using this model to analyze the complexity of the QA task which is related to both the IE, and the reading comprehension tasks (Bagga et al., 1999). The remainder of this section describes the model in detail, and provides a sample application of the model to an IE task. In the following section, we discuss how this model can be used to analyze the reading comprehension task. 2.1 Definitions Network: of a collection of nodes interconnected by an accompanying set of arcs. Each node denotes an object and each arc represents a binary relation between the objects. (Hendrix, 1979) A Partial Network: network a collection of nodes interconnected by an accompanying set of arcs where the collection of nodes is a subset of a collection of nodes a and the accompanying set of is a subset of the set of arcs accompanying the set of nodes which form the network. 35 have claimed Figure 1: A Sample Network a sample network for the following piece of text: &amp;quot;The Extraditables,&amp;quot; or the Armed Branch of the Medellin Cartel have claimed responsibility for the murder of two employees of Bogota&apos;s daily El Espectador on Nov 15. The murders took place in Medellin. 2.2 The Level of A Fact level of a fact, a piece of text is defined by the following algorithm: Build a network, the piece of text. 2. Identify the nodes that are relevant to the fact, , , the nodes the partial network consisting of the set of nodes {1,x2,.,,x} interconnected by the set of arcs ft&apos;, t2, define the the fact, with respect to network, be equal to k, the number of arcs linking the nodes which comprise the fact 2.2.1 Observations the definition of the level of the following observations can be made: • The level of a fact is related to the concept of &amp;quot;semantic vicinity&amp;quot; defined by Schubert et. (Schubert and others, 1979). The a node in a network consists of the nodes and the arcs reachable from that node by traversing a small number of arcs. The fundaused here is that &amp;quot;the knowlto intellectual task generally lies in the semantic vicinity of the concepts involved in the task&amp;quot; (Schubert and oth- The level of a fact is equal to the number of arcs that one needs to traverse to reach all the concepts (nodes) which comprise the fact of interest. • A level-0 fact consists of a single node (i.e. no transitions) in a network. A level-k fact is a facts. • Conjunctions/disjunctions increase the level of a fact. • A higher level fact is likely to be harder to extract than a lower level fact. A fact appearing at one level in a piece of may appear at some other level in the same piece of text. • The level of a fact in a piece of text depends on the granularity of the network constructed for that piece of text. Therefore, the level of a fact with respect to a network built at the word level (i.e. words represent objects and the relationships between the objects) will be greater than the level of a fact with respect to a network built at the phrase level (i.e. noun groups represent objects while verb groups and preposition groups represent the relationships between the objects). 2.2.2 Examples the network shown in Figure 1. been built at the phrase level. The mentioned, in an example of a level-0 fact because the &amp;quot;city&amp;quot; fact consists only of one node &amp;quot;Medellin.&amp;quot; The of attack, in 5, is an example of a 36 define the type attack the network to be an attack designator such as &amp;quot;murder,&amp;quot; &amp;quot;bombing,&amp;quot; or &amp;quot;assassination&amp;quot; with one modifier giving the victim, perpetrator, date, location, or other information. In this case the type of attack fact is composed of the &amp;quot;the murder&amp;quot; and the &amp;quot;two employees&amp;quot; nodes and their connector. This makes the type of attack a level-1 fact. The type of attack could appear as a level-0 fact as in &amp;quot;the Medellin bombing&amp;quot; (assuming that the network is built at the phrase level) because in this case both the attack designator (bombing) and the modifier (Medellin) occur in the same node. The type of attack fact occurs as a level-2 fact in the following sentence (once again assuming that the network is built at the phrase level): &amp;quot;10 people were killed in the offensive which included several bombings.&amp;quot; In this case there is no direct connector between the attack designator (several bombings) and its modifier (10 people). They are connected by the intermediatory &amp;quot;the offensive&amp;quot; node; thereby making the type of attack a level-2 fact. The type of attack can also appear at higher levels. In date of the murder of the two employees is an example of a level-2 fact. This is because the attack designator (the murder) along with its modifier (two employees) account for one level and the arc to &amp;quot;Nov 15&amp;quot; accounts for the second level. The date of the attack, in this case, is not a level-1 fact (because of the two nodes &amp;quot;the murder&amp;quot; and &amp;quot;Nov 15&amp;quot;) because the phrase &amp;quot;the murder on Nov 15&amp;quot; does not tell one that an attack actually took place. The article could have been talking about a seminar on murders that took place on Nov 15 and not about the murder of two employees which took place then. • In 8, the location of the murder of the two employees is an example of a level-2 fact. The exact same argument as the date of the murder of the two employees applies here. The complete information, in the victims is an example of a level-2 fact because to know that two employees of Bogota&apos;s Daily El Espectador were victims, one has to know that they were murdered. The attack designator (the murder) with its modifier (two employees) accounts for one level, while the connector between &amp;quot;two employees&amp;quot; and &amp;quot;Bogota&apos;s Daily El Espectador&amp;quot; accounts for the other. 2.3 Building the Networks As mentioned earlier, the level of a fact for a piece of text depends on the network constructed for the text. Since there is no unique network corresponding to a piece of text, care has to be taken so that the networks are built consistently. We used the following algorithm to build the networks: 1. Every article was broken up into a nonoverlapping sequence of noun groups (NGs), verb groups (VGs), and preposition groups (PGs). The rules employed to identify the NGs, VGs, and PGs were almost the same as the ones by SRI&apos;s FASTUS 2. The nodes of the network consisted of the NGs while the transitions between the nodes consisted of the VGs and the PCs. 3. Identification of coreferent nodes and prepositional phrase attachments were done manually. The networks are built based largely upon the syntactic structure of the text contained in the articles. However, there is some semantics encoded into the networks because identification of coreferent nodes and preposition phrase attachments are done manually. Obviously, if one were to employ a different alfor the networks, one would get different numbers for the level of a fact, But, if the algorithm were employed consistently across all the facts of interest and across all articles in a domain, the numbers on the level of a fact would be consistently different and one would still be able to analyze the relative complexity of extracting that fact from a piece of text in the domain. 3 Example: Analyzing the Complexity of an Information Extraction Task In order to validate our model of complexity we applied it to the Information Extraction (IE) task, or the Message Understanding task (DAR, 1991), (DAR, 1992), (ARP, 1993), (DAR, 1995), (DAR, 1998). The goal of an LE task is to extract prespecified facts from text and fill in predefined templates containing labeled slots. We analyzed the complexity of the task used for the Fourth Message Understanding Conference (MUC-4) (DAR, 1992). In this task, the participants were asked to extract the following facts from articles describing terrorist activities in Latin America: • The type of attack. • The date of the attack. • The location of the attack. wish to thank Jerry Hobbs of SRI for providing us with the rules of their partial parser.</abstract>
<note confidence="0.87783962962963">37 Number of Facts 110 105 100 95 90 85 80 75 70 65 60 55 50 45 40 35 30 25 20 15 10 5 /11 Ii 0 1 2</note>
<title confidence="0.357615666666667">Mack Fact -e-- - Location Fact - -- Fact</title>
<abstract confidence="0.91301025">Fact &apos;y • ....... ... ..&amp;quot;1..... I</abstract>
<phone confidence="0.621314">5 6 8 9 10 11 12 13 14 15</phone>
<note confidence="0.884805785714286">Levels Figure 2: MUC-4: Level Distribution of Each of the Five Facts 50 45 40 35 30 25 20 15 10 5 0 1 3 4 5 6 7 8 9 10 11 12 13 14 15</note>
<title confidence="0.505638666666667">Levels 3: Distribution of the Five Facts Combined</title>
<abstract confidence="0.983091194630873">The victim (including damage to property). • The perpetrator(s) (including suspects). We analyzed a set of 100 articles from the MUC-4 domain each of which reported one or more terrorist attacks. Figure 2 shows the level distribution for each of the five facts. A closer analysis of the figure shows that the &amp;quot;type of attack&amp;quot; fact is the easiest to extract while the &amp;quot;perpetrator&amp;quot; fact is the hardest (the curve peaks at level-2 for this fact). In addition, Figure 3 shows the level distribution of the five facts combined. This figure gives some indication of the complexity of the MUC-4 task because it shows that almost 50% of the MUC-4 facts occur at level-1. The expected level of the five facts in the MUC-4 domain was 1.74 (this is simply the weighted average of the level distributions of the facts). We define this numto be the Complexity the MUC-4 task. Therefore, the MUC-4 task can now be compared to, the MUC-5 task by comparing their Comfact, we computed the Task Complexity of the MUC-5 task and discovered that it was equal to 2.5. In comparison, an analysis, using more &amp;quot;sufeatures, done by Beth Sundheim, that the nature of the MUC-5 EJV task is approximately twice as hard as the nature of the MUC-4 task (Sundheim, 1993). The features used in the included vocabulary average number of words per sentence, and the average number of sentences per article. More details about this analysis can be found in (Bagga and Bierrnann, 1998). 4 Analyzing the Reading Comprehension Task The reading comprehension task differs from the QA task in the following way: while the goal of the QA task is to find answers for a set of questions from a of documents, goal of the reading comprehension task is to find answers to a set of quesfrom a related document. the QA task involves extracting answers from a collection of documents, the complexity of this task depends on the expected level of occurrence of the answers of the questions. While it is theoretically possible to compute the average level of any fact in the entire Percentage of Facts ............. MIX 4:5 Facts -0— _ 44 4 38 Test avg # of avg # of # of avg # of avg # of sentences levels/sent corefs/sent questions levels/answer corefs/answer Basic 9 4.11 2.33 8 3.75 2.25 Basic-Interm 13 2.69 2.39 6 3.33 2.50 Intermediate 56 3.50 2.55 9 4.44 3.33 Interm-Adv 17 6.47 1.00 6 7.83 1.33 Advanced 27 6.93 2.08 10 8.20 2.90 Figure 4: Summary of Results document collection, it is not humanly possible to analyze every document in such large collections to compute this. For example, the TREC collection used for the QA track is approximately 5GB. However, since the reading comprehension task involves extracting the answers from a single document, it is possible to analyze the document itself in addition to computing the level of the occurrence of each answer. Therefore, the results presented in this paper will provide both these values. 4.1 Analysis and Results We analyzed a set of five reading comprehension tests offered by the English Language Center at University of Victoria in Canada 2. five tests are listed in increasing order of difficulty and are classified by the Center as: Basic, Basic-Intermediate, Intermediate, Intermediate- Advanced, and Advanced. For each of these tests, we calculated the level number of each sentence in the text, and the level number of the sentences containing the answers to each question for every test. In addition, we also calculated the number of coreferences present in each sentence in the texts, and the corresponding number in the sentences containing each answer. It should be noted that we were forced to calculate the level number of the sentences containing the answer as opposed to calculating the level number of the answer itself because several questions had only true/false answers. Since there was no way to compute the level numbers of true/false answers, we decided to calculate the level numbers of the sentences containing the answers in order to be consistent. For true/false answers this implied analyzing all the sentences which help determine the truth value of the question. 4 shows each text, the number of sentences in the text, the average level number of a sentence, the average number of coreferences per sentence, the number of questions corresponding to the test, the average level number of each answer, and the average number of coreferences per answer. The results shown in Figure 4 are consistent with the model. The figure shows that as the difficulty level of the tests increase, so do the corresponding level numbers per sentence, and the answers. One conclusion that we can draw from the numbers is that the Basic-Intermediate test, based upon the analysis, is slightly more easy than the Basic test. We will address this issue in the next section. The numbers of coreferences, surprisingly, do no increase with the difficulty of the tests. However, a closer look at the types of coreference shows that while most of the coreferences in the first two tests (Basic, and Basic-Intermediate) are simple pronominal coreferences (he, she, it, etc.), the coreferences used in the last two tests (Intermediate-Advanced, and Advanced) require more knowledge to process. Some examples include marijuana coreferent with drug, with pant, Not being able to capture the complexity of the coreferences is one, among several, shortcomings of this model. A with Qanda 3ran its Qanda reading comprehension system on the five tests analyzed in the previous section. However, instead of producing a single answer for each question, Qanda produces a list of answers listed in decreasing order of confidence. The rest of this section describes an evaluation of Qanda&apos;s performance on the five tests and a comparison with the analysis done in the previous section. In order to evaluate Qanda&apos;s performance on the five tests we decided to use the Mean Reciprocal Answer Rank (MRAR) technique which was used for evaluating question-answering systems at TREC- 8 (Singhal, 1999). For each answer, this techniques assigns a score between 0 and 1 depending on its rank in the list of answers output. The score for computed as: Scorei = rank of answers If no correct answer is found in the list, a score of 0 is assigned. Therefore, MRAR for a reading comprehension test is the sum of the scores for answers corresponding to each question for that test. Figure 5 summarizes Qanda&apos;s results for the five tests. The figure shows, for each test, the number of the MRAR for all answers the test, and the average MRAR per answer. would like to thank Marc Light and Eric Breck for their help with running Qanda on our data. 1 39 Test # of MRAR for avg MRAR questions all answers per answer</abstract>
<note confidence="0.777232444444445">Basic 8 2.933 0.367 Basic-Interm 6 3.360 0.560 Intermediate 9 2029. 0.226 Interm-Adv 6 1.008 0.168 Advanced 10 7.833 0.783 Figure 5: Summary of Qanda&apos;s Results The results from Qanda are more or less consistent with the analysis done earlier. Except for the Advanced test, the average Mean Reciprocal Answer</note>
<abstract confidence="0.991777861111111">Rank is consistent with the average number of levels per sentence (from Figure 4). It should be pointed out that the system performed significantly better on the Basic-Intermediate Test compared to the Basic test consistent with the numbers in Figure 4. However, contrary to expectation, Qanda performed exceedingly well on the Advanced test answering 7 out of the 10 questions with answers whose rank is 1 (i.e. the first answer among the list of possible answers for each question is the correct one). We are currently consulting the developers of the system for conducting an analysis of the performance on this test in more detail. 5 Shortcomings This measure is just the beginning of a search for useful complexity measures. Although the measure is a big step up from the measures used earlier, it has a number of shortcomings. The main shortcoming is the ambiguity regarding the selection of nodes from the network regarding the fact of interest. Consider the following sentence: &amp;quot;This is a report from the Straits of Taiwan. .... Yesterday, China fired a missile.&amp;quot; Suppose we are interested in the location of the launch of the missile. The ambiguity here arises from the fact that the article does not explicitly mention that the missile was launched in the Straits of Taiwan. The decision to infer that fact from the information present depends upon the person building the network. In addition, the measure does not account for the following factors (the list is not complete): coreference: If the extraction of a fact requires the resolution of several coreferences, it is clearly more difficult than an extraction which does not. In addition, the degree of difficulty of resolving coreferences itself varies from simple exact matches, and pronominal coreferences, to ones that require external world knowledge. of answers: The of occurof facts in of documents has an impact on the performance of systems. occurrence of multiple (similar) facts; Clearly, if several similar facts are present in the same article, the systems will find it harder to extract the correct fact. size: words present some problems to systems making it harder for them to perform well. On the other hand, no measure can take into account all possible features in natural language. Consider the following example. In an article, suppose one initially encounters a series of statements that obliquely imply that the following statement is false. Then the statement is given: &amp;quot;Bill Clinton visited Taiwan last week.&amp;quot; Processing such discourse requires an ability to perfectly understand the initial series of statements before the truth value of the last statement can be properly evaluated. Such complete understanding is beyond the state of the art and is likely to remain so for many years. Despite these shortcomings, the current measure complexity on one very important dimension, namely the number of clauses (or phrases) required to specify a fact. For the short term it appears to be the best available vehicle for understanding the complexity of extracting a fact. 6 Conclusions In this paper we have described a model that can be used to analyze the complexity of a reading comprehension task. The model has been used to analyze five different reading comprehension tests, and the paper presents the results from the analysis.</abstract>
<note confidence="0.885350222222222">References 1993. Message Understanding Confer- (MUC-5), Mateo, August. Morgan Kaufmann Publishers, Inc. Amit Bagga and Alan W. Bien-flaw&apos;. 1997. Analyzing the Complexity of a Domain With Respect An Information Extraction Task. In International Conference on Research on Computa- Linguistics (ROCLING X), August. Bagga and W. Aierniann. 1998. Ana- Performance of Message Understand- Systems. Computational Linguis- 40 Language Processing, February. Amit Bagga, Wlodek Zadrozny, and James Pustejai/sky. 1999. Semantics and Complexity of Ques-</note>
<title confidence="0.703380333333333">tion Answering Systems: Towards a Moore&apos;s Law Natural Language Engineering. In AAAI Fall Symposium Series on Question Answering</title>
<address confidence="0.631361">1-10, November.</address>
<note confidence="0.896563">1991. Message Understanding Con- (MUC-3), Mateo, May. Morgan Kaufmann Publishers, Inc. 1992. Message Understanding (MUC-4), Mateo, June. Morgan Kaufmann Publishers, Inc. TIPSTER Text Program. 1995.</note>
<title confidence="0.92917">Understanding</title>
<author confidence="0.92803">Morgan Kaufmann Pub-</author>
<affiliation confidence="0.793035">lishers, Inc.</affiliation>
<note confidence="0.742742129032258">TIPSTER Text Program. 1998. Understanding Conference (MUCmuc_7_toc.httnl, April. K. Harman, editor. 1999. Text RE- Conference (TREC-8). Institute of Standards and Technology (MST), U.S. Department of Commerce, National Technical Information Service, November, Gary G. Hendrix, 1979. Encoding Knowledge in Partitioned Networks. In Nicholas V. Findler, edi- Networks, 51-92. Academic Press, New York. Lynette Hirschman et al. 1999. Deep Read: A Read- System. In Annual Meeting of the Association of Computational Linguis- 325-332, June, Lynette Hirschman. 1992. An Adjunct Test for Processing in MUC-4. In Mes- Understanding Conference (MUC-4) 1992), pages 67-77, Lenhart K. Schubert et al. 1979. The Structure and Organization of a Semantic Net for Comprehension and Inference. In Nicholas V. Findler, editor, Networks, Press, New York. Amit Singhal. 1999. Question Answering Track at com/sin ghal/ Beth M. Sundheim. 1993. Tipster/MUC-5 Informa- Extraction System Evaluation. In Mes- Understanding Conference (MUC-5) 1993), pages 27-44.</note>
<intro confidence="0.507057">41</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ARPA</author>
</authors>
<title>Fifth Message Understanding Conference (MUC-5),</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, August.</location>
<marker>ARPA, 1993</marker>
<rawString>ARPA. 1993. Fifth Message Understanding Conference (MUC-5), San Mateo, August. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Alan W Bien-flaw&apos;</author>
</authors>
<title>Analyzing the Complexity of a Domain With Respect To An Information Extraction Task.</title>
<date>1997</date>
<booktitle>In Tenth International Conference on Research on Computational Linguistics (ROCLING X),</booktitle>
<pages>175--194</pages>
<marker>Bagga, Bien-flaw&apos;, 1997</marker>
<rawString>Amit Bagga and Alan W. Bien-flaw&apos;. 1997. Analyzing the Complexity of a Domain With Respect To An Information Extraction Task. In Tenth International Conference on Research on Computational Linguistics (ROCLING X), pages 175-194, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Alan W Aierniann</author>
</authors>
<title>Analyzing the Performance of Message Understanding Systems.</title>
<date>1998</date>
<journal>Journal of Computational Linguistics and Chinese Language Processing,</journal>
<pages>3--1</pages>
<marker>Bagga, Aierniann, 1998</marker>
<rawString>Amit Bagga and Alan W. Aierniann. 1998. Analyzing the Performance of Message Understanding Systems. Journal of Computational Linguistics and Chinese Language Processing, 3(1):1-26, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Wlodek Zadrozny</author>
<author>James Pustejaisky</author>
</authors>
<title>Semantics and Complexity of Question Answering Systems: Towards a Moore&apos;s Law for Natural Language Engineering. In</title>
<date>1999</date>
<booktitle>AAAI Fall Symposium Series on Question Answering Systems,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="3362" citStr="Bagga et al., 1999" startWordPosition="543" endWordPosition="546"> harder to extract than others (Hirschman, 1992). In this section we describe a method for estimating the complexity of extracting a fact from text. The proposed model was initially used to analyze the information extraction task (Bagga and Bierrnann, 1997). In addition to verifying Hirschman&apos;s hypothesis, the model also provided us with a framework for analyzing and understanding the performance of several IE systems (Bagga and Biermann, 1998). We have also proposed using this model to analyze the complexity of the QA task which is related to both the IE, and the reading comprehension tasks (Bagga et al., 1999). The remainder of this section describes the model in detail, and provides a sample application of the model to an IE task. In the following section, we discuss how this model can be used to analyze the reading comprehension task. 2.1 Definitions Network: A network consists of a collection of nodes interconnected by an accompanying set of arcs. Each node denotes an object and each arc represents a binary relation between the objects. (Hendrix, 1979) A Partial Network: A partial network is a collection of nodes interconnected by an accompanying set of arcs where the collection of nodes is a su</context>
</contexts>
<marker>Bagga, Zadrozny, Pustejaisky, 1999</marker>
<rawString>Amit Bagga, Wlodek Zadrozny, and James Pustejai/sky. 1999. Semantics and Complexity of Question Answering Systems: Towards a Moore&apos;s Law for Natural Language Engineering. In 1999 AAAI Fall Symposium Series on Question Answering Systems, pages 1-10, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Third Message Understanding Conference (MUC-3),</title>
<date>1991</date>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo,</location>
<marker>DARPA, 1991</marker>
<rawString>DARPA. 1991. Third Message Understanding Conference (MUC-3), San Mateo, May. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Fourth Message Understanding Conference (MUC-4),</title>
<date>1992</date>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo,</location>
<marker>DARPA, 1992</marker>
<rawString>DARPA. 1992. Fourth Message Understanding Conference (MUC-4), San Mateo, June. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA TIPSTER Text Program</author>
</authors>
<date>1995</date>
<booktitle>Sixth Message Understanding Conference (MUC-6),</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo,</location>
<marker>Program, 1995</marker>
<rawString>DARPA: TIPSTER Text Program. 1995. Sixth Message Understanding Conference (MUC-6), San Mateo, November. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA TIPSTER Text Program</author>
</authors>
<date>1998</date>
<booktitle>Seventh Message Understanding Conference (MUC7). http://www.muc.saic.com/proceedingsmuc_7_toc.httnl,</booktitle>
<marker>Program, 1998</marker>
<rawString>DARPA: TIPSTER Text Program. 1998. Seventh Message Understanding Conference (MUC7). http://www.muc.saic.com/proceedingsmuc_7_toc.httnl, April.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Eighth Text REtrieval Conference (TREC-8). National Institute of Standards and Technology (MST), U.S. Department of Commerce, National Technical Information Service,</booktitle>
<editor>D. K. Harman, editor.</editor>
<publisher>November,</publisher>
<marker>1999</marker>
<rawString>D. K. Harman, editor. 1999. Eighth Text REtrieval Conference (TREC-8). National Institute of Standards and Technology (MST), U.S. Department of Commerce, National Technical Information Service, November,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary G Hendrix</author>
</authors>
<title>Encoding Knowledge in Partitioned Networks.</title>
<date>1979</date>
<booktitle>Associative Networks,</booktitle>
<pages>51--92</pages>
<editor>In Nicholas V. Findler, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="3816" citStr="Hendrix, 1979" startWordPosition="622" endWordPosition="623">lso proposed using this model to analyze the complexity of the QA task which is related to both the IE, and the reading comprehension tasks (Bagga et al., 1999). The remainder of this section describes the model in detail, and provides a sample application of the model to an IE task. In the following section, we discuss how this model can be used to analyze the reading comprehension task. 2.1 Definitions Network: A network consists of a collection of nodes interconnected by an accompanying set of arcs. Each node denotes an object and each arc represents a binary relation between the objects. (Hendrix, 1979) A Partial Network: A partial network is a collection of nodes interconnected by an accompanying set of arcs where the collection of nodes is a subset of a collection of nodes forming a network, and the accompanying set of arcs is a subset of the set of arcs accompanying the set of nodes which form the network. 35 have claimed Figure 1: A Sample Network Figure 1 shows a sample network for the following piece of text: &amp;quot;The Extraditables,&amp;quot; or the Armed Branch of the Medellin Cartel have claimed responsibility for the murder of two employees of Bogota&apos;s daily El Espectador on Nov 15. The murders </context>
</contexts>
<marker>Hendrix, 1979</marker>
<rawString>Gary G. Hendrix, 1979. Encoding Knowledge in Partitioned Networks. In Nicholas V. Findler, editor, Associative Networks, pages 51-92. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>Deep Read: A Reading Comprehension System.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>325--332</pages>
<marker>Hirschman, 1999</marker>
<rawString>Lynette Hirschman et al. 1999. Deep Read: A Reading Comprehension System. In 37th Annual Meeting of the Association of Computational Linguistics, pages 325-332, June,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>An Adjunct Test for Discourse Processing in MUC-4.</title>
<date>1992</date>
<booktitle>In Fourth Message Understanding Conference (MUC-4)</booktitle>
<pages>67--77</pages>
<location>DAR,</location>
<contexts>
<context position="2791" citStr="Hirschman, 1992" startWordPosition="451" endWordPosition="452">comprehension, it has to have the ability to extract facts from text. Obviously, the performance of the system will depend upon the type of fact it has to extract: explicit or implicit, easy or difficult, etc. (by no means is this list complete). In addition, the performance of such systems varies greatly depending on various additional factors including known vocabulary, sentence length, the amount of training, quality of parsing, etc. Despite the great variations in the performances of such systems, it has been hypothesized that there are facts that are simply harder to extract than others (Hirschman, 1992). In this section we describe a method for estimating the complexity of extracting a fact from text. The proposed model was initially used to analyze the information extraction task (Bagga and Bierrnann, 1997). In addition to verifying Hirschman&apos;s hypothesis, the model also provided us with a framework for analyzing and understanding the performance of several IE systems (Bagga and Biermann, 1998). We have also proposed using this model to analyze the complexity of the QA task which is related to both the IE, and the reading comprehension tasks (Bagga et al., 1999). The remainder of this secti</context>
</contexts>
<marker>Hirschman, 1992</marker>
<rawString>Lynette Hirschman. 1992. An Adjunct Test for Discourse Processing in MUC-4. In Fourth Message Understanding Conference (MUC-4) (DAR, 1992), pages 67-77,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart K Schubert</author>
</authors>
<title>The Structure and Organization of a Semantic Net for Comprehension and Inference.</title>
<date>1979</date>
<booktitle>Associative Networks,</booktitle>
<pages>121--175</pages>
<editor>In Nicholas V. Findler, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Schubert, 1979</marker>
<rawString>Lenhart K. Schubert et al. 1979. The Structure and Organization of a Semantic Net for Comprehension and Inference. In Nicholas V. Findler, editor, Associative Networks, pages 121-175. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
</authors>
<title>Question Answering Track at TREC-8.</title>
<date>1999</date>
<note>http://www. research. att. com/- sin ghal/ ga-track-spec.txt,</note>
<contexts>
<context position="18187" citStr="Singhal, 1999" startWordPosition="3206" endWordPosition="3207">th Qanda MITRE 3 ran its Qanda reading comprehension system on the five tests analyzed in the previous section. However, instead of producing a single answer for each question, Qanda produces a list of answers listed in decreasing order of confidence. The rest of this section describes an evaluation of Qanda&apos;s performance on the five tests and a comparison with the analysis done in the previous section. In order to evaluate Qanda&apos;s performance on the five tests we decided to use the Mean Reciprocal Answer Rank (MRAR) technique which was used for evaluating question-answering systems at TREC8 (Singhal, 1999). For each answer, this techniques assigns a score between 0 and 1 depending on its rank in the list of answers output. The score for answer, i, is computed as: Scorei = rank of answers If no correct answer is found in the list, a score of 0 is assigned. Therefore, MRAR for a reading comprehension test is the sum of the scores for answers corresponding to each question for that test. Figure 5 summarizes Qanda&apos;s results for the five tests. The figure shows, for each test, the number of questions, the cumulative MRAR for all answers for the test, and the average MRAR per answer. 3We would like t</context>
</contexts>
<marker>Singhal, 1999</marker>
<rawString>Amit Singhal. 1999. Question Answering Track at TREC-8. http://www. research. att. com/- sin ghal/ ga-track-spec.txt, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth M Sundheim</author>
</authors>
<title>Tipster/MUC-5 Information Extraction System Evaluation.</title>
<date>1993</date>
<booktitle>In Fifth Message Understanding Conference (MUC-5) (ARP,</booktitle>
<pages>27--44</pages>
<contexts>
<context position="13252" citStr="Sundheim, 1993" startWordPosition="2376" endWordPosition="2377">vel of the five facts in the MUC-4 domain was 1.74 (this is simply the weighted average of the level distributions of the facts). We define this number to be the Task Complexity for the MUC-4 task. Therefore, the MUC-4 task can now be compared to, say, the MUC-5 task by comparing their Task Complexities. In fact, we computed the Task Complexity of the MUC-5 task and discovered that it was equal to 2.5. In comparison, an analysis, using more &amp;quot;superficial&amp;quot; features, done by Beth Sundheim, shows that the nature of the MUC-5 EJV task is approximately twice as hard as the nature of the MUC-4 task (Sundheim, 1993). The features used in the study included vocabulary size, the average number of words per sentence, and the average number of sentences per article. More details about this analysis can be found in (Bagga and Bierrnann, 1998). 4 Analyzing the Reading Comprehension Task The reading comprehension task differs from the QA task in the following way: while the goal of the QA task is to find answers for a set of questions from a collection, of documents, the goal of the reading comprehension task is to find answers to a set of questions from a single related document. Since the QA task involves ext</context>
</contexts>
<marker>Sundheim, 1993</marker>
<rawString>Beth M. Sundheim. 1993. Tipster/MUC-5 Information Extraction System Evaluation. In Fifth Message Understanding Conference (MUC-5) (ARP, 1993), pages 27-44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>