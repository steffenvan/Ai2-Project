<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012714">
<note confidence="0.473367">
REPRESENTATION OF TEXTS FOR INFORMATION RETRIEVAL
</note>
<author confidence="0.57202">
N.J. Belkin, B.G. Michell, and D.G. Kuehner
</author>
<affiliation confidence="0.82609">
University of Western Ontario
</affiliation>
<bodyText confidence="0.995230910447761">
The representation of whole texts is a major concern of
the field known as information retrieval (IR), an impor-
tant aspect of which might more precisely be called
&apos;document retrieval&apos; (DR). The DR situation, with which
we will be concerned, is, in general, the following:
a. A user, recognizing an information need, presents to
an IR mechanism (i.e., a collection of texts, with a
set of associated activities for representing, stor-
ing, matching, etc.) a request, based upon that need
hoping that the mechanism will be able to satisfy
that need.
b. The task of the IR mechanism is to present the user
with the text(s) that it judges to be most likely to
satisfy the user&apos;s need, based upon the request.
c. The user examines the text(s) and her/his need is
satisfied completely or partially or not at all.
The user&apos;s judgement as to the contribution of each
text in satisfying the need establishes that text&apos;s
usefulness or relevance to the need.
Several characteristics of the problem which DR attempts
to solve make current IR systems rather different from,
say, question-answering systems. One is that the needs
which people bring to the system require, in general,
responses consisting of documents about the topic or
problem rather than specific data, facts, or inferences.
Another is that these needs are typically not precisely
specifiable, being expressions of an anomaly in the
user&apos;s state of knowledge. A third is that this is an
essentially probabilistic, rather than deterministic
situation, and is likely to remain so. And finally,
the corpus of documents in many such systems is in the
order of millions (of, say, journal articles or ab-
stracts), and the potential needs are, within rather
broad subject constraints, unpredictable. The DR situ-
ation thus puts certain constraints upon text represen-
tation and relaxes others. The major relaxation is
that it may not be necessary in such systems to produce
representations which are capable of inference. A con-
straint, on the other hand, is that it is necessary to
have representations which can indicate problems that a
user cannot her/himself specify, and a matching system
whose strategy is to predict which documents might re-
solve specific anomalies. This strategy can, however,
be based on probability of resolution, rather than cer-
tainty. Finally, because of the large amount of data,.
it is desirable that the representation techniques be
reasonably simple computationally.
Appropriate text representations, given these con-
straints, must necessarily be of whole texts, and prob-
ably ought to be themselves whole, unitary structures,
rather than lists of atomic elements, each treated sep-
arately. They must be capable of representing problems,
or needs, as well as expository texts, and they ought
to allow for some sort of pattern matching. An obvious
general schema within these requirements is a labelled
associative network.
Our approach to this general problem is strictly prob-
lem-oriented. We begin with a representation scheme
which we realize is oversimplified, but which stands
within the constraints, and test whether it can be pro-
gressively modified in response to observed deficien-
cies, until either the desired level of performance in
solving the problem is reached, or the approach is shown
to be unworkable. We report here on some linguistical-
ly-derived modifications to a very simple, but neverthe-
less psychologically and linguistically based word-co-
occurrence analysis of text [1] (figure 1).
</bodyText>
<figure confidence="0.991282857142857">
POSITION RANK (r)
Adjacent 1
Same Sentence 2
Adjacent Sentences 3
FOR EACH CO-OCCURRENCE OF EACH WORD PAIR (w1,142)
1
SCORE = 1 + r X 100
</figure>
<figureCaption confidence="0.905436">
FOR ALL CO-OCCURRENCES OF EACH WORD PAIR IN TEXT
ASSOCIATION STRENGTH = SUM (SCORES)
Figure 1. Word Association Algorithm
</figureCaption>
<bodyText confidence="0.999914777777778">
The original analysis was applied to two kinds of texts:
abstracts of articles representing documents stored by
the system, and a set of &apos;problem statements represent-
ing users&apos; information needs -- their anomalous states
of knowledge -- when they approach the system. The
analysis produced graph-like structures, or association
maps, of the abstracts and problem statements which were
evaluated by the authors of the texts (Figure 2)
(Figure 3).
</bodyText>
<sectionHeader confidence="0.9918415" genericHeader="abstract">
CLUSTERING LARGE FILES OF DOCUMENTS
USING THE SINGLE-LINK METHOD
</sectionHeader>
<bodyText confidence="0.999812357142857">
A method for clustering large files of documents
using a clustering algorithm which takes 0(n==2)
operations (single-link) is proposed. This
method is tested on a file of 11,613 documents
derived from an operational system. One prop-
erty of the generated cluster hierarchy (hier-
archy connection percentage) is examined and
it indicates that the hierarchy is similar to
those from other test collections. A comparison
of clustering times with other methods shows
that large files can be clustered by single-
link in a time at least comparable to various
heuristic algorithms which theoretically require
fewer operations.
</bodyText>
<figureCaption confidence="0.997231">
Figure 2. Sample Abstract Analyzed
</figureCaption>
<bodyText confidence="0.999906083333333">
In general, the representations were seen as being ac-
curate reflections of the author&apos;s state of knowledge
or problem; however, the majority of respondents also
felt that some concepts were too strongly or weakly
connected, and that important concepts were omitted
(Table 1).
We think that at least some of these problems arise
because the algorithm takes no account of discourse
structure. But because the evaluations indicated that
the algorithm produces reasonable representations, we
have decided to amend the analytic structure, rather
than abandon it completely.
</bodyText>
<page confidence="0.984022">
147
</page>
<figure confidence="0.8678815">
LINK
7 Strong Associations
= Medium Associations
â€” = Weak Associations
</figure>
<figureCaption confidence="0.988691">
Figure 3. Association Map for Sample Abstract
</figureCaption>
<tableCaption confidence="0.488345">
Table 1. Abstract Representation Evaluation
</tableCaption>
<table confidence="0.980801882352941">
Question % YES % NO % NO
INTERM. RESP.
1. ACCURATE 48.0 29.6 22.0 N=30
REFLECTION?
2.(a) CONCEPTS TOO 63.0 37.0 N=30
STRONGLY
CONNECTED?
(b) CONCEPTS TOO 96.3 3.7 N=30
WEAKLY
CONNECTED?
3. CONCEPTS 88.9 11.1 N=30
OMITTED?
4. IF NO OR 64.3 7.1 21.4 7.1 N=14
&apos;INTERN&apos; to
No. 1, WAS
ABSTRACT
ACCURATE?
</table>
<bodyText confidence="0.893107756097561">
Our current modifications to the analysis consist pri-
marily of methods for translating facts about discourse
structure into rough equivalents within the word-co-
occurrence paradigm. We choose this strategy, rather
than attempting a complete and theoretically adequate
discourse analysis, in order to incorporate insights
about discourse without violating the cost . volume
constraints typical of DR systems. The modit-cations
are designed to recognize such aspects of discourse
structure as establishment of topic; setting of context;
summarizing; concept foregrounding; and stylistic vari-
ation. Textual characteristics which correspond with
these aspects include discourse-initial and discourse-
final sentences; title words in the text; equivalence
relations; and foregrounding devices (Figure 4).
1. Repeat first and last sentences of the text.
These sentences may include the more important con-
cepts, and thus should be more heavily weighted.
2. Repeat first sentence of paragraph after the last
sentence.
To integrate these sentences more fully into the
overall structure.
3. Make the title the first and last sentence of the
text, or overweight the score for each co-occurrence
containing a title word.
Concepts in the title are likely to be the most im-
portant in the text, yet are unlikely to be used
often in the abstract.
4. Hyphenate phrases in the input text (phrases chosen
algorithmically) and then either: a. Use the phrase
only as a unit equivalent to a single word in the
co-occurrence analysis; or b. use any co-occurrence
with either member of the phrase as a co-occurrence
with the phrase, rather than the individual word.
This is to control for conceptual units, as opposed
to conceptual relations.
5. Modify original definition of adjacency, which
counted stop-list words, to one which ignores stop-
list words. This is to correct for the distortion
caused by the distribution of function words in the
recognition of multi-word concepts.
</bodyText>
<figureCaption confidence="0.95868">
Figure 4. Modifications to Text Analysis Program
</figureCaption>
<bodyText confidence="0.999962">
We have written alternative systems for each of the pro-
posed modifications. In this experiment the original
corpus of thirty abstracts (but not the problem state-
ments) is submitted to all versions of the analysis pro-
grams and the results compared to the evaluations of the
original analysis and to one another. From the compar-
isons can be determined: the extent to which discourse
theory can be translated into these terms; and the rela-
tive effectiveness of the various modifications in im-
proving the original representations.
</bodyText>
<sectionHeader confidence="0.939146" genericHeader="introduction">
Reference
</sectionHeader>
<reference confidence="0.973302">
1. Belkin, N.J., Brooks, N.M., and oddy, R.N. 1979.
Representation and classification of knowledge and
information for use in interactive information re-
trieval. In Human Aspects of Information Science.
Oslo: Norwegian Library School.
</reference>
<figure confidence="0.823828">
DOCUM
SINGL
</figure>
<page confidence="0.943571">
148
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000853">
<title confidence="0.997161">REPRESENTATION OF TEXTS FOR INFORMATION RETRIEVAL</title>
<author confidence="0.999597">N J Belkin</author>
<author confidence="0.999597">B G Michell</author>
<author confidence="0.999597">D G Kuehner</author>
<affiliation confidence="0.998531">University of Western Ontario</affiliation>
<abstract confidence="0.999171014925373">The representation of whole texts is a major concern of the field known as information retrieval (IR), an important aspect of which might more precisely be called &apos;document retrieval&apos; (DR). The DR situation, with which we will be concerned, is, in general, the following: a. A user, recognizing an information need, presents to an IR mechanism (i.e., a collection of texts, with a set of associated activities for representing, storing, matching, etc.) a request, based upon that need hoping that the mechanism will be able to satisfy that need. b. The task of the IR mechanism is to present the user with the text(s) that it judges to be most likely to satisfy the user&apos;s need, based upon the request. c. The user examines the text(s) and her/his need is satisfied completely or partially or not at all. The user&apos;s judgement as to the contribution of each text in satisfying the need establishes that text&apos;s usefulness or relevance to the need. Several characteristics of the problem which DR attempts to solve make current IR systems rather different from, say, question-answering systems. One is that the needs which people bring to the system require, in general, responses consisting of documents about the topic or problem rather than specific data, facts, or inferences. Another is that these needs are typically not precisely specifiable, being expressions of an anomaly in the user&apos;s state of knowledge. A third is that this is an essentially probabilistic, rather than deterministic situation, and is likely to remain so. And finally, the corpus of documents in many such systems is in the order of millions (of, say, journal articles or abstracts), and the potential needs are, within rather broad subject constraints, unpredictable. The DR situation thus puts certain constraints upon text representation and relaxes others. The major relaxation is that it may not be necessary in such systems to produce representations which are capable of inference. A constraint, on the other hand, is that it is necessary to have representations which can indicate problems that a user cannot her/himself specify, and a matching system whose strategy is to predict which documents might resolve specific anomalies. This strategy can, however, be based on probability of resolution, rather than certainty. Finally, because of the large amount of data,. it is desirable that the representation techniques be reasonably simple computationally. Appropriate text representations, given these constraints, must necessarily be of whole texts, and probably ought to be themselves whole, unitary structures, rather than lists of atomic elements, each treated separately. They must be capable of representing problems, or needs, as well as expository texts, and they ought allow for some sort of pattern An general schema within these requirements is a labelled associative network. Our approach to this general problem is strictly problem-oriented. We begin with a representation scheme which we realize is oversimplified, but which stands within the constraints, and test whether it can be progressively modified in response to observed deficiencies, until either the desired level of performance in solving the problem is reached, or the approach is shown to be unworkable. We report here on some linguisticalmodifications to a very simple, but nevertheless psychologically and linguistically based word-cooccurrence analysis of text [1] (figure 1).</abstract>
<email confidence="0.449353">(r)</email>
<note confidence="0.904109833333333">Adjacent 1 Same Sentence 2 Sentences EACH CO-OCCURRENCE OF EACH WORD (w1,142) 1 SCORE = 1 + r X 100</note>
<title confidence="0.787562">FOR ALL CO-OCCURRENCES OF EACH WORD PAIR IN TEXT ASSOCIATION STRENGTH = SUM (SCORES)</title>
<abstract confidence="0.9371436">Figure 1. Word Association Algorithm original analysis to two kinds of texts: abstracts of articles representing documents stored by the system, and a set of &apos;problem statements representing users&apos; information needs -their anomalous states of knowledge -when they approach the system. The analysis produced graph-like structures, or association maps, of the abstracts and problem statements which were evaluated by the authors of the texts (Figure 2) (Figure 3).</abstract>
<title confidence="0.599349666666667">LARGE DOCUMENTS USING THE SINGLE-LINK METHOD A method for clustering large files of documents</title>
<abstract confidence="0.974226571428571">a clustering algorithm which takes operations (single-link) is proposed. This method is tested on a file of 11,613 documents derived from an operational system. One property of the generated cluster hierarchy (hierarchy connection percentage) is examined and it indicates that the hierarchy is similar to from other test collections. of clustering times with other methods shows that large files can be clustered by singlelink in a time at least comparable to various algorithms require fewer operations. Figure 2. Sample Abstract Analyzed general, the representations were seen as accurate reflections of the author&apos;s state of knowledge or problem; however, the majority of respondents also felt that some concepts were too strongly or weakly connected, and that important concepts were omitted (Table 1). We think that at least some of these problems arise because the algorithm takes no account of discourse structure. But because the evaluations indicated that the algorithm produces reasonable representations, we have decided to amend the analytic structure, rather than abandon it completely. 147 LINK 7Strong Associations = Medium Associations â€” = Weak Associations Figure 3. Association Map for Sample Abstract Table 1. Abstract Representation Evaluation Question % YES % NO % NO INTERM. RESP.</abstract>
<note confidence="0.764183625">1. ACCURATE 48.0 29.6 22.0 N=30 REFLECTION? 2.(a) CONCEPTS TOO 63.0 37.0 N=30 STRONGLY CONNECTED? (b) CONCEPTS TOO WEAKLY CONNECTED? 96.3 3.7 N=30 3. CONCEPTS OMITTED? 88.9 11.1 N=30 4. IF NO OR &apos;INTERN&apos; to 64.3 7.1 21.4 7.1 N=14 No. 1, WAS</note>
<abstract confidence="0.999522203703704">ACCURATE? Our current modifications to the analysis consist primarily of methods for translating facts about discourse structure into rough equivalents within the word-cooccurrence paradigm. We choose this strategy, rather than attempting a complete and theoretically adequate discourse analysis, in order to incorporate insights about discourse without violating the cost . volume constraints typical of DR systems. The modit-cations are designed to recognize such aspects of discourse structure as establishment of topic; setting of context; summarizing; concept foregrounding; and stylistic variation. Textual characteristics which correspond with these aspects include discourse-initial and discoursefinal sentences; title words in the text; equivalence relations; and foregrounding devices (Figure 4). 1. Repeat first and last sentences of the text. These sentences may include the more important concepts, and thus should be more heavily weighted. 2. Repeat first sentence of paragraph after the last sentence. To integrate these sentences more fully into the overall structure. 3. Make the title the first and last sentence of the text, or overweight the score for each co-occurrence containing a title word. in the title are likely to be the most important in the text, yet are unlikely to be used often in the abstract. 4. Hyphenate phrases in the input text (phrases chosen algorithmically) and then either: a. Use the phrase only as a unit equivalent to a single word in the co-occurrence analysis; or b. use any co-occurrence with either member of the phrase as a co-occurrence with the phrase, rather than the individual word. This is to control for conceptual units, as opposed to conceptual relations. Modify original definition of adjacency, counted stop-list words, to one which ignores stoplist words. This is to correct for the distortion caused by the distribution of function words in the recognition of multi-word concepts. Figure 4. Modifications to Text Analysis Program We have written alternative systems for each of the proposed modifications. In this experiment the original corpus of thirty abstracts (but not the problem statements) is submitted to all versions of the analysis programs and the results compared to the evaluations of the original analysis and to one another. From the comparisons can be determined: the extent to which discourse translated into these terms; and the relative effectiveness of the various modifications in improving the original representations.</abstract>
<note confidence="0.275798111111111">Reference Belkin, N.J., Brooks, N.M., and 1979. Representation and classification of knowledge and information for use in interactive information re- In Aspects of Information Science. Oslo: Norwegian Library School. DOCUM SINGL 148</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N J Belkin</author>
<author>N M Brooks</author>
<author>R N oddy</author>
</authors>
<title>Representation and classification of knowledge and information for use in interactive information retrieval.</title>
<date>1979</date>
<booktitle>In Human Aspects of Information Science.</booktitle>
<publisher>Norwegian Library School.</publisher>
<location>Oslo:</location>
<contexts>
<context position="3586" citStr="[1]" startWordPosition="571" endWordPosition="571">is a labelled associative network. Our approach to this general problem is strictly problem-oriented. We begin with a representation scheme which we realize is oversimplified, but which stands within the constraints, and test whether it can be progressively modified in response to observed deficiencies, until either the desired level of performance in solving the problem is reached, or the approach is shown to be unworkable. We report here on some linguistically-derived modifications to a very simple, but nevertheless psychologically and linguistically based word-cooccurrence analysis of text [1] (figure 1). POSITION RANK (r) Adjacent 1 Same Sentence 2 Adjacent Sentences 3 FOR EACH CO-OCCURRENCE OF EACH WORD PAIR (w1,142) 1 SCORE = 1 + r X 100 FOR ALL CO-OCCURRENCES OF EACH WORD PAIR IN TEXT ASSOCIATION STRENGTH = SUM (SCORES) Figure 1. Word Association Algorithm The original analysis was applied to two kinds of texts: abstracts of articles representing documents stored by the system, and a set of &apos;problem statements representing users&apos; information needs -- their anomalous states of knowledge -- when they approach the system. The analysis produced graph-like structures, or association</context>
</contexts>
<marker>1.</marker>
<rawString>Belkin, N.J., Brooks, N.M., and oddy, R.N. 1979. Representation and classification of knowledge and information for use in interactive information retrieval. In Human Aspects of Information Science. Oslo: Norwegian Library School.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>