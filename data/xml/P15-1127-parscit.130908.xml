<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000175">
<title confidence="0.99248">
Scalable Semantic Parsing with Partial Ontologies
</title>
<author confidence="0.997059">
Eunsol Choi Tom Kwiatkowski† Luke Zettlemoyer
</author>
<affiliation confidence="0.9945545">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<email confidence="0.994544">
eunsol@cs.washington.edu,tomkwiat@google.com, lsz@cs.washington.edu
</email>
<sectionHeader confidence="0.993844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911894736842">
We consider the problem of building scal-
able semantic parsers for Freebase, and
present a new approach for learning to do
partial analyses that ground as much of the
input text as possible without requiring that
all content words be mapped to Freebase
concepts. We study this problem on two
newly introduced large-scale noun phrase
datasets, and present a new semantic pars-
ing model and semi-supervised learning
approach for reasoning with partial onto-
logical support. Experiments demonstrate
strong performance on two tasks: refer-
ring expression resolution and entity at-
tribute extraction. In both cases, the par-
tial analyses allow us to improve precision
over strong baselines, while parsing many
phrases that would be ignored by existing
techniques.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798615384615">
Recently, significant progress has been made in
learning semantic parsers for large knowledge
bases (KBs) such as Freebase (FB) (Cai and Yates,
2013; Berant et al., 2013; Kwiatkowski et al.,
2013; Reddy et al., 2014). Although these methods
can build general purpose meaning representations,
they are typically evaluated on question answering
tasks and are designed to only parse questions that
have complete ontological coverage, in the sense
that there exists a logical form that can be executed
against Freebase to get the correct answer.1 In this
paper, we instead consider the problem of learning
semantic parsers for open domain text containing
</bodyText>
<footnote confidence="0.8997462">
†Now at Google, NY.
1To ensure all questions are answerable, the data is man-
ually filtered. For example, the WebQuestions dataset intro-
duced by Berant et al. (2013) contains only the 7% of the
originally gathered questions.
</footnote>
<figureCaption confidence="0.9738525">
Figure 1: Example noun phrases from Wikipedia
category labels and appositives in newswire text.
</figureCaption>
<bodyText confidence="0.996849833333333">
concepts that may or may not be representable us-
ing the Freebase ontology.
Even very large knowledge bases have two types
of incompleteness that provide challenges for se-
mantic parsing algorithms. They (1) have partial
ontologies that cannot represent the meaning of
many English phrases and (2) are typically miss-
ing many facts. For example, consider the phrases
in Figure 1. They include subjective or otherwise
unmodeled phrases such as “relaxed” and “quake-
hit.” Freebase, despite being large-scale, contains
a limited set of concepts that cannot represent the
meaning of these phrases. They also refer to enti-
ties that may be missing key facts. For example, a
recent study (West et al., 2014) showed that over
70% of people in FB have no birth place, and 99%
have no ethnicity. In our work, we introduce a new
semantic parsing approach that explicitly models
ontological incompleteness and is robust to miss-
ing facts, with the goal of recovering as much of a
sentence’s meaning as the ontology supports. We
argue that this will enable the application of se-
mantic parsers to a range of new tasks, such as
information extraction (IE), where phrases rarely
have full ontological support and new facts must
be added to the KB.
Because existing semantic parsing datasets have
been filtered to limit incompleteness, we introduce
two new corpora that pair complex noun phrases
with one or more entities that they describe. The
</bodyText>
<figure confidence="0.547506">
Haitian human rights activists
Art museums and galleries in New York
School buildings completed in 1897
Olympic gymnasts of Norway
the capital of quake-hit Sichuan Province
a major coal producing province
the relaxed seaside capital of Mozambique
Wikipedia
Appos.
</figure>
<page confidence="0.938175">
1311
</page>
<note confidence="0.981773666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1311–1320,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.9974765">
(a) Wikipedia category
(b) Appos
</figure>
<equation confidence="0.9713495625">
x : Symphonic Poems by Jean Sibelius
e : {The Bard, Finlandia,Pohjola’s Daughter, En Saga, Spring Song, Tapiola... }
l0 : Ax.Symphonic(x) n Poems(x) n by(JeanSibelius, x)
y : Ax.composition.form(x, Symphonicpoems) n composer(JeanSibelius, x)
x : Defunct Korean football clubs
e : { Goyang KB Kookmin Bank FC,Hallelujah FC, Kyungsung FC }
l0 : Ax.defunct(x) n korean(x) n football(x) n clubs(x)
y : Ax.OpenType[defunct](x) n OpenRel(x,KOREA) n football clubs(x))
x : a driving force behind the project
e : Germany
l0 : Ax.driving(x) n force(x) n behind(x, theproject)
y : Ax.OpenType[driving force](x) n OpenRel[behind](x, OpenEntity[the project])
x : an EU outpost in the Mediterranean
e : Malta
l0 : Ax.outpost(x) n EU(x) n in(x, theMediterranean)
y : Ax.OpenRel(x,EU) n OpenType[outpost](x) n contained by(x,MediterraneanSea)
</equation>
<figureCaption confidence="0.971236">
Figure 2: Examples of noun phrases x, from the Wikipedia category and apposition datasets, paired with
the set of entities e they describe, their underspecified logical form l0, and their final logical form y.
</figureCaption>
<bodyText confidence="0.990818413043479">
first new dataset contains 365,000 Wikipedia cate-
gory labels (Figure 1, top), each paired with the list
of the associated Wikipedia entity pages. The sec-
ond has 67,000 noun phrases paired with a single
named entity, extracted from the appositive con-
structions in KBP 2009 newswire text (Figure 1,
bottom).2 This new data is both large scale, and
unique in the focus on noun phrases. Noun phrases
contain a number of challenging compositional
phenomena, including implicit relations and noun-
noun modifiers (e.g. see Gerber and Chai (2010)).
To better model text with only partial ontologi-
cal support, we present a new semantic parser that
builds logical forms with concepts from a target
ontology and open concepts that are introduced
when there is no appropriate concept match in
the target ontology. Figure 2 shows examples of
the meanings that we extract. Only the first of
these examples can be fully represented using Free-
base, all other examples require explicit modeling
of open concepts. To build these logical forms,
we follow recent work for Combinatory Categori-
cal Grammar (CCG) semantic parsing with Free-
base (Kwiatkowski et al., 2013), extended to model
when open concepts should be used. We develop
a two-stage learning algorithm: we first compute
broad coverage lexical statistics over all of the data,
which are then incorporated as features in a full
parsing model. The parsing model is tuned on a
hand-labeled data set with gold analyses.
Experiments demonstrate the benefits of the new
approach. It significantly outperforms strong base-
2All new data is available on the authors’ websites.
lines on both a referring expression resolution task,
where much like in the QA setting we directly eval-
uate if we recover the correct logical form for each
input noun phrase, and on entity attribute extrac-
tion, where individual facts are extracted from the
groundable part of the logical form. We also see
that modeling incompleteness significantly boosts
precision; we are able to more effectively deter-
mine which words should not be mapped to KB
concepts. When run on all of the Wikipedia cat-
egory data, we estimate that the learned model
would discover 12 million new facts that could be
added to Freebase with 72% precision.
</bodyText>
<sectionHeader confidence="0.997966" genericHeader="introduction">
2 Overview
</sectionHeader>
<subsectionHeader confidence="0.977827">
Semantic Parsing with Open Concepts Our
</subsectionHeader>
<bodyText confidence="0.997564058823529">
goal is to learn to map noun phrase referring ex-
pressions x to logical forms y that describe their
meaning. In this work, y is built using both con-
cepts from a knowledge base K and open concepts
that lie outside of the scope of K. For example,
in Figure 2 the phrase “Defunct Korean football
clubs” is modeled using a logical form y that con-
tains the K concept football clubs(x) as well
as the open concepts OpenType[defunct](x).
In this paper we describe a new method for learn-
ing the mapping from x to y from corpora of refer-
ring expression noun phrases, paired with a sets of
entities a that these referring expressions describe.
Figure 2 shows examples of these data drawn from
two sources.
Tasks We introduce two new datasets (Sec. 3)
that pair referring noun phrases x with one or more
</bodyText>
<page confidence="0.991331">
1312
</page>
<bodyText confidence="0.999935695652174">
entities e that they describe. These data support
evaluation for two tasks: referring expression reso-
lution and information extraction.
In referring expression resolution, the parser is
given x and is used to predict the referring expres-
sion logical form y that describes e. Since the
majority of our data cannot be fully modeled with
Freebase, we evaluate each y against a hand labeled
gold standard instead of trying to extract e from K.
The entity attribute extraction task also involves
mapping phrases x to logical forms y, with the
goal of adding new facts to the knowledge base K.
To do this, we assume each x is additionally paired
with an set of entities e. We also define an entity
attribute to be a literal in y that uses only concepts
from K. Finally, we extract, for each entity in
e, all of the attributes listed in y. For example,
the first logical form y in Figure 2 has two
entity attributes: composer(JeanSibelius, x)
and composition.form(x, Symphonic poems)
which can be added to K for the entities
{TheBard,Finlandia}.
Model and Learning Our approach extends
the two-stage semantic parser introduced by
Kwiatkowski et al (2013). We use CCG to build
domain-independent logical forms l0 and then in-
troduce a new method for reasoning about how
to map this intermediary representation onto both
open concepts and K concepts (Sec. 4).
To learn this model, we assume access to data
with two different types of annotations. The first
contains noun phrase descriptions x and described
entity sets a (as in Figure 2), which can be eas-
ily gathered at scale with no manual data labeling
effort. However, this data, in general, has signif-
icant amount of knowledge base incompleteness;
many described concepts and entity attributes will
be missing from K (see Sec. 3 for more details).
Therefore, to support effective learning, we will
also use a small hand-labeled dataset containing
x, e, a gold logical form y, an intermediary CCG
logical form l0, and a mapping from words in x to
constants in K and open concepts. Our full learning
approach (Sec. 5) estimates a linear model on the
small labeled dataset, with broad coverage features
derived from the larger dataset.
</bodyText>
<sectionHeader confidence="0.997586" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999490081632653">
We gathered two new datasets that pair complex
noun phrases with one or more Freebase entities.
The Wikipedia category dataset contains
365,504 Wikipedia category names paired with the
list of entities in that category. 3 Table 1 shows the
details of this dataset and examples are given in
Figure 2. For each development and test data, we
randomly select 500 categories consisted of 3-10
words and describing fewer than 100 entities.
The apposition dataset is a large set of com-
plex noun phrases paired with named entities, ex-
tracted from appositive constructions such as “Gus-
tav Bayer, a former Olympic gymnast for Norway.”
For this example, we extract the entity “Gustav
Bayer” and pair it with the noun phrase “a former
Olympic gymnast for Norway.” To identify apposi-
tive constructions, we ran the Stanford dependency
parser on the newswire section of the KBP 2009
source corpus,4 and selected noun phrases com-
posed of 3 to 10 words, starting with an article, and
paired with a named entity that is in Freebase.
This procedure of identifying complex entity de-
scriptions allows for information extraction from
a wide range of sources. However, it is also noisy
and challenging. The dependency parser makes er-
rors, for example “the next day against the United
States, Spain” is falsely detected as an apposition.
Furthermore, addressing context and co-reference
is often necessary. For example, “Puerto Montt, a
city south of the capital” or “the company’s par-
ent, Shenhua Group” requires reference resolution.
We gathered 67 thousand appositions, which will
be released to support future work, and randomly
selected 300 for testing.
Measuring Incompleteness To study the
amount of incompleteness in this data, we hand
labeled logical forms for 500 Wikipedia categories
in the development set. Examples of annotations
are given in the rows labeled y in Figure 2. We
use these to measure the schema and fact coverage
of Freebase. Many of the entities in this dataset
do not have the Freebase attributes described by
the category phrases. When a concept is not in
Freebase, we annotate it as OpenType or OpenRel,
as shown in Figure 2. On average, each Wikipedia
category name describes 2.58 Freebase attributes,
and 0.39 concepts that cannot be mapped to FB.
Overall, 27.2% of the phrases contain concepts
that do not exist in the Freebase schema.
</bodyText>
<footnote confidence="0.99733125">
3Compiled by the YAGO project, available at: www.mpi-
inf.mpg.de/departments/databases-and-information-
systems/research/yago-naga/yago/downloads/
4http://www.nist.gov/tac/2009/
</footnote>
<page confidence="0.847276">
1313
</page>
<table confidence="0.997701857142857">
entire set dev test
# categories 365,504 500 500
# words per category 4.1 4.4 4.3
# unique words 84,996 1,100 1,063
# entities per category 19.9 19.1 18.7
# entities 2,813,631 9,511 9,281
# entity-category pairs 7,292,326 9,549 9,331
</table>
<tableCaption confidence="0.993269">
Table 1: Wikipedia category data statistics.
</tableCaption>
<table confidence="0.99917375">
entire set test set
# appositions 66,924 300
# unique words 25,472 817
# words per apposition 5.73 5.93
</table>
<tableCaption confidence="0.998874">
Table 2: Appositive data statistics.
</tableCaption>
<bodyText confidence="0.999369">
Each category may have multiple correct logical
forms. For example, “Hotels” can be mapped to:
hotel(x), accomodation.type(x, hotel), or
building function(x, hotel). There are also
genuine ambiguities in meaning. For example,
“People from Bordeaux” can be interpreted as
</bodyText>
<equation confidence="0.659675">
people(x) ∧ place lived(x,Bordeaux) or
people(x) ∧ place of birth(x, Bordeaux).
</equation>
<bodyText confidence="0.999424388888889">
We made a best effort attempt to gather as many
correct logical forms as possible, finding on
average 1.8 logical forms per noun phrase. There
were 97 unique binary relations, and 247 unique
unary attributes in the annotation.
Given these logical forms, we also measured
factual coverage. For the 72.8% of phrases that
can be completely represented using Freebase, we
executed the logical forms and compared the result
to the labeled entity set. In total, 56% of the queries
returned no entities and those that did return results
have on average 15% overlap with the Wikipedia
entity set. We also measured how often attributes
from the labeled logical forms were assigned to the
Wikipedia entities in FB, finding that only 33.6%
were present. Given this rate, we estimate that it is
possible to add 12 million new facts into FB from
the 7 million entity-category pairs.
</bodyText>
<sectionHeader confidence="0.94363" genericHeader="method">
4 Mapping Text to Meaning
</sectionHeader>
<bodyText confidence="0.980938886363636">
We adopt a two-stage semantic parsing ap-
proach (Kwiatkowski et al., 2013). We first use
a CCG parser to define a set CCG(x) of possible
logical forms l0. Then we will choose the logical
form l0 that closely matches the linguistic struc-
ture of the input text x, according to a learned
linear model, and use an ontological match step
that defines a set of transformations ONT(l0, K) to
map this meaning to a Freebase query y. Figure 2
shows examples of x, l0 and y. In this section we
describe our approach with the more detailed ex-
ample derivation in Figure 3. We also describe the
parameterization of a linear model that scores each
derivation.
CCG parsing We use a CCG (Steedman, 1996)
semantic parser (Kwiatkowski et al., 2013) to gen-
erate an underspecified logical form l0. Figure 3a
shows an example parse. The constants Former,
Municipalities, in, Brandenburgh in l0 are not
tied to the target knowledge base, causing the logi-
cal form to be underspecified. They can be replaced
with Freebase constants in the later ontology match-
ing step.
Ontological Matching The ontological match
step has structural match and constant match com-
ponents. Structural match operators can collapse
or expand sub-expressions in the logical forms
to match equivalent typed concepts in the target
knowledge base. We adopt existing structural
match operators (Kwiatkowski et al., 2013) and
refer readers to that work for details.
Constant match operators replace underspeci-
fied constants in the underspecified logical form
l0 with concepts from the target knowledge base.
There are four constant match operations used in
Figure 3. The first two constant matches, shown be-
low, match underspecified constants with constants
of the same type from Freebase.
in → location.containedby
Brandenburgh → BRANDENBURGH
However, because we are modeling the semantics
of phrases that are not covered by the Freebase
schema, we also require the following two constant
matches:
</bodyText>
<equation confidence="0.9953795">
Former(x) → OpenType
municipalities(x) → OpenRel(x, Municipality)
</equation>
<bodyText confidence="0.999824666666667">
Here, the word ‘former’ has been associated with
a placeholder typing predicate since Freebase has
no way of expressing end dates of administrative
divisions. There is also no Freebase type repre-
senting the concept ‘municipalities.’ However, this
word is associated with an entity in Freebase. Since
there is no suitable linking predicate for the entity
Municipality, we introduce a placeholder link-
ing predicate OpenRel in the step from l2 → l3.
Our constant match operators can also introduce
placeholder entities OpenEntity when there is no
good match in Freebase.
</bodyText>
<page confidence="0.966301">
1314
</page>
<figure confidence="0.903441">
(a) CCG parse builds an underspecified semantic representation of the sentence.
Former municipalities in Brandenburgh
N/N N N\N/NP NP
AfAx.f(x) ∧ former(x) Ax.municipalities(x) AfAxAy.f(y) ∧ in(y, x) Brandenburg
&gt; &gt;
N N\N
Ax.former(x) ∧ municipalities(x) AfAy.f(y) ∧ in(y, Brandenburg)
N
l0 = Ax.former(x) ∧ municipalities(x) ∧ in(x, Brandenburg)
&lt;
(b) Constant matches replace underspecified constants with Freebase concepts
</figure>
<equation confidence="0.9969786">
l0 = Ax.former(x) ∧ municipalities(x) ∧ in(x, Brandenburg)
l1 = Ax.former(x) ∧ municipalities(x) ∧ in(x, Brandenburg)
l2 = Ax.former(x) ∧ municipalities(x) ∧ location.containedby(x, Brandenburg)
l3 = Ax.former(x) ∧ OpenRel(x, Municipality) ∧ location.containedby(x, Brandenburg)
l4 = Ax.OpenType(x) ∧ OpenRel(x, Municipality) ∧ location.containedby(x, Brandenburg)
</equation>
<figureCaption confidence="0.98286">
Figure 3: Derivation of the analysis for “Former municipalities in Brandenburgh”. This analysis contains
a placeholder type and a placeholder relation as described in Section 4.
</figureCaption>
<bodyText confidence="0.99985575">
We also allow the creation of typing predicates
from matched entities through the introduction of
linking predicates. For example, there is no native
type associated with the word ‘actor’ in Freebase.
Instead we create a typing predicate by matching
the word to a Freebase entity Actor using Free-
base API and allowing the introduction of linked
predicates such as person.profession :
</bodyText>
<equation confidence="0.97653">
actor(x) → person.profession(x, Actor)
</equation>
<bodyText confidence="0.999848">
Scoring Full Parses Our goal in this paper is to
learn a function from the phrase x to the correct
analysis y. We score each parse using a linear
model with features that signal attributes of the
underspecified parse φp and those that signal at-
tributes of the ontological match φont. Since the
model factors over the two stages of parser, we split
the prediction problem similarly. First, we select
the maximum scoring underspecified logical form:
</bodyText>
<equation confidence="0.997846">
l* = arg max (θp · φp(l))
lECCG(x)
</equation>
<bodyText confidence="0.930121">
and then we select the highest scoring Freebase
analysis y* that can be built from l*:
</bodyText>
<equation confidence="0.993713">
y* = arg max (θont · φont(r))
rEONT(l∗,1C)
</equation>
<bodyText confidence="0.99967">
We describe an approach to learning the parameter
vectors θp and θont below.
</bodyText>
<sectionHeader confidence="0.996116" genericHeader="method">
5 Learning
</sectionHeader>
<bodyText confidence="0.9283063">
We introduce a learning approach that first collates
aggregate statistics from the 7 million Wikipedia
entity-category pairs and existing facts in FB, and
then uses a small labeled training set to tune the
weights for features that incorporate these statistics.
. . . . . . military.conflict)
military.conflict)
time.event.loc(x,
. . . . . . time.event.start time
BattleOfGrunwald military conflict.commanders
</bodyText>
<figure confidence="0.909633583333333">
Wikipedia Category
Wars involving the Grand Duchy of Lituania
Entity Attribute
BattleOfGrunwald type(x,
GollubWar type(x,
BattleOfGrunwald
Grunwald)
Entity Relation
BattleOfGrunwald military conflict.combatants
GollubWar
in Figure 4 the category
involving the
</figure>
<subsectionHeader confidence="0.401657">
Grand Duchy of
</subsectionHeader>
<bodyText confidence="0.891137166666667">
is associated with
the relation military
and the attribute
multiple times, because they are present in many of
the
entities. For each of the sub-phrases
in the category name we count these associations
over the entire Wikipedia category set.
We use these counts to calculate Pointwise
Mutual Information (PMI) between words and
Freebase attributes or relations. We choose PMI to
avoid overcompensating common words, attributes,
or relations. For example, the word
is seen
with the incorrect analysis type(x, time.event)
more frequently than the correct analysis
However, PMI
penalizes the attribute
</bodyText>
<equation confidence="0.96413525">
‘Wars
Lithuania’
conflict.combatants
type(x, military.conflict)
category’s
‘Wars’
type(x,military.conflict).
type(x,time.event) for
</equation>
<figureCaption confidence="0.996996">
Figure 4: Labeled entities are associated with at-
</figureCaption>
<bodyText confidence="0.853248714285714">
tributes an
d relations.
Broad Coverage Lexical Statistics Each
Wikipedia category is associated with a number
of entities, most of which exist in FB. We use
these entities to extract relations and attributes in
FB associated with that category. For example,
</bodyText>
<page confidence="0.930799">
1315
</page>
<bodyText confidence="0.9998275">
its popularity and the correct analysis is preferred.
As PMI has a tendency to emphasize rare counts,
we chose PMI squared, which takes the squared
value of the co-occurence count (PMI2(a, b) =
</bodyText>
<equation confidence="0.981746">
log count(a∧b)2 ) as a feature.
count(a)∗count(b) ,
</equation>
<bodyText confidence="0.999880896551724">
Structural KB Statistics Existing semantic
parsers typically make use of type constraints
to limit the space of possible logical forms.
These strong type constraints are not fea-
sible when the knowledge base is incom-
plete. For example, in Freebase the relation
military conflict.combatants expects an en-
tity of type military conflict.combatant as
its object. However, many countries that have been
involved in wars are not assigned this type.
We instead calculate type overlap statistics for
all Freebase entities, to find likely missing types.
For example, including the fact that the object of
military conflict.combatants is very often
of type location.country.
Learning from Labeled Data We train each
half of the prediction problem separately, as de-
fined in Section 4, using the labeled training data
introduced in Section 3. We use structured max-
margin perceptrons to learn feature weights for
both the underspecified parse and the ontological
match step following (Kwiatkowski et al., 2013).
The aggregate statistics collected from 7 million
category-entity pairs produce very useful lexical
features. We integrate these statistics into our linear
model by summing their values for each derivation
and treating them as a feature. All of the other fea-
tures described in Section 6 are not word specific
and are therefore far less sparse.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="method">
6 Features
</sectionHeader>
<bodyText confidence="0.999580225806452">
We include a number of features that enable soft
type checking on the output logical form, described
first below, along with other features that measure
different aspects of the analysis.
Coherency features For example, con-
sider the phrase “The UK home city of
the Queen,” with Freebase logical form
y = Ax.home(QEII, x) ∧ in(x, UK) ∧ city(x).
Each of the relations has expected types for
their argument: the relation (home) expects
a subject of type (person) and an object
of type (location). Each type in Freebase
lives in a hierarchy, so the type city implies
{location, administrative division,... }.
The next four features test agreement of these
types on different parts of the output logical form.
Relation arguments trigger a feature if their
type is in the set of types expected by the relation.
QEII is a person so this feature is triggered for the
relation-argument application in home(QEII, x).
Relation-relation pairs can share variable argu-
ments. For example, the variable x is the object
of (home) and the subject of (in). Each relation
expects a set of types of x. We have features to
signal if: these sets are disjoint; one set subsumes
the other; and the PMI between the highest level
expected type (described in Section 5) if the sets
are disjoint. In the example given here, the type
(location) expected by (in) subsumes the type
(city) expected by (home) so the second feature
fires. We treat types such as city(x) as unary
relations and include them in this feature set.
Type domain measures compatibility among do-
mains in Freebase. Freebase is split into high-level
domains and some of these are relevant, such as
‘football’ and ‘sports’. We identify those by count-
ing their co-occurrences. This becomes an indica-
tor feature that signals their co-occurrence in y.
Named entity type features test if the entity e
that we are extracting attributes for have Freebase
type “person”, “location” or “organization”. If it
does, we have a feature indicating if y defines a
set of the same type. This features is not used in
the referring expression task presented in Section 7
since we cannot assume access to the entities that
are described.
CCG parse feature signals which lexical items
were used in the CCG parse. Another feature fires
if capitalized words map to named entities.
String similarity features signal exact string
match, stemmed string match, and length weighted
string edit distance between a phrase in the sen-
tence and the name of the Freebase element it was
matched on. We also use the Freebase search API
to generate scores for phrase, entity pairs and in-
clude the log of this score as a features.
Lexical PMI feature includes the lexical Point-
wise Mutual Information described in Section 5.
Freebase constant features signal the use of
linking predicates, as defined in Section 4, and
the log frequency count of the Freebase attributes
across all entities in the Wikipedia category set.
</bodyText>
<page confidence="0.982145">
1316
</page>
<bodyText confidence="0.997733666666667">
Other features indicate the use of OpenRel,
OpenEntity, OpenType in y and count repetitions
of Freebase concepts in y.
</bodyText>
<sectionHeader confidence="0.996136" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999796911111111">
Knowledge base We use the Jan. 26, 2014 Free-
base dump. After pruning binary predicates taking
numeric values, it contains 9351 binary predicates,
2754 unary predicates, and 1.2 billion assertions.
Pruning and Feature Initialization We per-
form beam search at each semantic parsing stage,
using the Freebase search API to determine candi-
date named entities (10 per phrase), binary predi-
cates (300 per phrase), and unary predicates (500
per phrase). The ontology matching stage consid-
ers the highest scored underspecified parse.
The features are initialized to prefer well-typed
logical forms. Type checking features are initially
set to -2 for mismatch. Features signalling incom-
patible topic domains and repetition are initialized
as -10. All other initial feature weights are set to 1.
Datasets and Annotation We evaluate on the
Wikipedia category and appositive datasets intro-
duced in Sec. 3. On the Wikipedia development
data, we annotated 500 logical forms, underspeci-
fied logical forms and constant mappings for ontol-
ogy matching. The Wikipedia test data is composed
of 500 unseen categories. We did not train on the
appositive dataset, as it contains challenges such
as co-reference and parsing errors as described in
Sec. 3. Instead, we chose 300 randomly selected ex-
amples for evaluation, and ran on the model trained
on the Wikipedia development data.
Evaluation Metrics We report five-fold cross
validation for development but ran the final model
once on the test data, manually scoring the output.
For evaluation on the referring expression resolu-
tion performance (as defined in Sec. 2), we include
accuracy for the final logical form (Exact Match).
We also evaluate precision and recall for predicting
individual literals in this logical form on the devel-
opment set. To control for missing facts, we did
not evaluate the set of returned entities.
To evaluate entity attribute extraction perfor-
mance (as defined in Sec. 2), we identified three
classes of predictions. Extractions can be correct,
benign, or false. Correct attributes are actually
described in the phrase, benign extraction may
not have been described but are still true, and
false extractions are not true. For example, if
</bodyText>
<table confidence="0.999790666666667">
System Exact Partial Match
Match P R F1
KCAZ13 1.4 9.6 6.3 7.0
IE Baseline 6.8 37.0 23.3 28.6
NoPMI 11.0 23.7 20.8 21.6
NoOpenSchema 13.7 35.8 30.0 31.1
NoTyping 9.6 37.6 29.3 31.8
Our Approach 15.9 39.3 33.5 35.1
with Gold NE 20.8 46.6 40.5 42.3
</table>
<tableCaption confidence="0.9986655">
Table 3: Referring expression resolution perfor-
mance on the development set on gold references.
</tableCaption>
<table confidence="0.9998502">
Data System Exact Match Accuracy
Wikipedia IE Baseline 21.8%
Our Approach 28.4%
Appos IE Baseline 0.0%
Our Approach 4.7%
</table>
<tableCaption confidence="0.96675">
Table 4: Manual evaluation for referring expression
resolution on the test sets.
</tableCaption>
<bodyText confidence="0.999810294117647">
the phrase “the capital of the communist-ruled
nation” is mapped to the pair of attributes
capital of administrative division(x) ,
location(x), the first is correct and the second is
benign. Other incorrect facts would be false.
On the development set, we report precision and
recall against the union of the FB attributes in our
annotations without adjusting for benign extrac-
tions or the fact that the annotations are not com-
plete. For the test sets, we computed precision
(P) where benign extractions are considered to be
wrong, as well as an adjusted precision metric (P*)
where benign extractions are counted as correct. As
we do not have full test set annotations, we cannot
report recall. Finally, we report the average number
of facts extracted per noun phrase (fact #).
Comparison Systems We compare performance
to a number of ablated versions of the full system,
where we have removed the open-constant ontology
matching operators (NoOpenSchema), the PMI fea-
tures (NoPMI), or the type checking features (No-
Typing). For the referring expression resolution
task, we excluded the named entity type feature, as
this assumes typing information about the entity
we are extracting attributes for.
We report results without the PMI features and
the open schema matching operators (KCAZ13),
which is a reimplementation of a recent Freebase
QA model (Kwiatkowski et al., 2013). We also
learn with gold named entity linking (Gold NE).
For the entity attribute extraction, we built a su-
pervised learning baseline that combines the output
of two discrete SVMs, one for predicting unary re-
lations and one for binary relations. Each classifier
</bodyText>
<page confidence="0.981699">
1317
</page>
<table confidence="0.998365375">
System Top n P R F1 fact #
IE Baseline - 37.3 26.5 30.6 1.6
1 44.2 32.8 37.7 1.9
2 36.9 38.0 37.5 2.6
3 30.7 42.7 35.7 3.6
Our Approach 4 27.0 44.7 33.6 4.2
5 23.7 47.2 31.6 5.1
10 15.9 52.0 24.3 8.5
</table>
<tableCaption confidence="0.9004165">
Table 5: Entity attribute extraction performance on
the Wikipedia category development set.
</tableCaption>
<table confidence="0.9998766">
Data System P P* fact #
Wikipedia IE Baseline 56.7 58.7 1.6
Our Approach 61.2 72.6 2.0
IE Baseline 4.9 13.9 1.3
Appos Our Approach 33.2 61.4 0.9
</table>
<tableCaption confidence="0.979729">
Table 6: Manual evaluation for entity attribute ex-
traction on the test sets.
</tableCaption>
<bodyText confidence="0.999791846153846">
is trained using the annotated Wikipedia categories.
This dataset contains hundreds of unary and bi-
nary relations, which the IE baseline can predict.
Each classifier is further anchored on a specific
word, and includes n-gram and POS context fea-
tures around that word, following features from
Mintz et al (2009). To predict binary relations, we
used named entities as anchors. For unary attributes
we anchored on all possible nouns and adjectives.
The final logical form includes the best relation
predicted by each classifier. We use the Stanford
CoreNLP5 toolkit for tokenization, named entity
recognition, and part-of-speech tagging.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.9999313125">
Tables 3 and 4 show performance on the referring
expression resolution task. Tables 5 and 6 show
performance on the extraction task. Reported pre-
cision is lower on the labeled development set than
on the test set, where predicted logical forms are
manually evaluated. This reflects the fact that, de-
spite our best attempts, the development set labels
are incomplete, as discussed in Section 3.
Referring expression resolution The systems
retrieve the full meaning with 28.4% accuracy on
the Wikipedia test set, and 15.9% on the develop-
ment set. The gold named entity input improves
performance by modest amounts. This suggests
that the errors stem from ontology mismatches, as
we will describe in more detail later in the qualita-
tive analysis. We also see that all of the ablations
</bodyText>
<footnote confidence="0.756288">
5http://nlp.stanford.edu/software/corenlp.html
</footnote>
<bodyText confidence="0.9998828">
hurt performance, and that the KCAZ13 model per-
forms extremely poorly. The independent classifier
baseline performs well at the sub-clause level, but
fails to form a full logical form of the referring
expression. Partial grounding and broad-coverage
data statistics are essential for this problem.
Entity attribute extraction In the two test sets,
the approach achieves high benign precision lev-
els (P*) of 72.6 and 61.4. However, the apposi-
tives data is significantly more challenging, and the
model misses many of the true facts that could be
extracted. Many errors comes in the early stages of
the pipeline, which can be attributed at least in part
to both (1) the higher levels of noise in the input
data (see Section 3), and (2) the fact that the CCG
parser was developed on the Wikipedia category la-
bels. While the IE baseline performs reasonably on
the Wikipedia test data, its performance degrades
significantly on appositions. As it is trained to pre-
dict pre-determined relations, it does not generalize
to different domains.
For the development set, Table 5 also shows the
precision-recall trade off for the set of Freebase
attributes that appear in the top-n predicted logical
forms. Precision drops quickly but recall can be
improved significantly, showing that the model can
produce many of the labeled facts.
Qualitative evaluation We sampled 100 errors
from the Wikipedia test set for qualitative analy-
sis. 10% came from entity linking. About 30%
come from choosing a superset or subset of the
desired meaning, for example by mapping “novel”
to book. About 10% of the errors are from do-
main ambiguity, such as mapping “stage actor” to
film.film actor. 10% of the cases are from spu-
rious string similarity, such as mapping “Hungarian
expatriates“ to nationality(x, Hungary). 15%
of the failures were due to incorrect underspecified
logical forms and, finally, about 10% of the errors
were because the typing features encouraged com-
pound nouns to be split into separate attributes. On
the apposition dataset, 65% of errors stems from
parsing, either in apposition detection or CCG pars-
ing. Better modeling the complex attachment deci-
sions for the noun phrases in the apposition dataset
remains an area for future work.
One advantage of our approach, especially in
comparison to classifier based models like the IE
baseline, is the ability to predict previously unseen
relations. Counting only the correctly predicted
</bodyText>
<page confidence="0.974019">
1318
</page>
<bodyText confidence="0.999957222222222">
triples, we see that over 40% of the unique rela-
tions we predict is not in the development set; our
model learns to generalize based on the learned
PMI features and other lexical cues.
Finally, our approach extracted 2.0 entity at-
tributes per Wikipedia phrase and 0.9 per appo-
sition on average. This matches our intuition that
the apposition dataset contains many more words
that cannot be modeled with concepts in Freebase.
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="related work">
9 Related Work
</sectionHeader>
<bodyText confidence="0.99998">
Recent work has begun to study the problem of
knowledge base incompleteness and reasoning with
open concepts. Joshi et al. (2014) describes an
approach for mapping short search queries to a
single Freebase relation, that benefits from model-
ing schema incompleteness. Additionally, Krish-
namurthy et al. (2012; 2014) present a semantic
parser that builds partial meaning representations
with Freebase for information extraction applica-
tions. This is similar in spirit to the approach we
present here, however they focus on a small, fixed,
set of binary relations while we aim to represent as
much of the text as possible using the entire Free-
base ontology. Krishnamurthy and Mitchell (2015)
have also studied semantic parsing with open con-
cepts via matrix factorization. They use Freebase
entities but do not include Freebase concepts.
The problem of building complete sentence anal-
yses using all of the Freebase ontology has re-
cently received attention within the context of ques-
tion answering systems (Cai and Yates, 2013;
Kwiatkowski et al., 2013; Berant et al., 2013; Be-
rant and Liang, 2014; Reddy et al., 2014). Since
they do not model KB incompleteness, these mod-
els will not work well on data that cannot be fully
modeled by Freebase. In section 7, we report re-
sults using one of these systems to provide a refer-
ence point for our approach. There has also been
other work on Freebase question answering (Yao
and Van Durme, 2014; Bordes et al., 2014; Wang
et al., 2014) that directly searches the facts in the
KB to find answers without explicitly modeling
compositional semantic structure. Therefore, these
methods will suffer when facts are missing.
The syntactic and semantic structure of noun
phrases has been extensively studied. For example,
work on NomBank (Meyers et al., 2004; Gerber
and Chai, 2010) focus on the challenge of modeling
implicit arguments introduced by nominal predi-
cates. In a manual study, we discovered that the
65% of our noun phrases contain implicit relations.
We build on insights from Vadas and Curran (2008),
who studied how to model the syntactic structure
of noun phrases in CCGBank. While we are, to the
best of our knowledge, the first to study compound
noun phrases for semantic parsing to knowledge-
bases, semantic parsers for noun phrase referring
expressions have been built for visual referring ex-
pression (FitzGerald et al., 2013).
There has been little work on IE from compound
noun phrases. Most existing IE algorithms extract
a single relation, usually represented as a verb that
holds between a pair of named entities, for exam-
ple with supervised learning techniques (Freitag,
1998) or via distant supervision (Mintz et al., 2009;
Riedel et al., 2013; Hoffmann et al., 2011). We aim
to go beyond relations between entity pairs, and to
retrieve full semantics of noun phrases, extracting
unary and binary relations for a single entity. A
notable exception to this trend is the ReNoun sys-
tem (Yahya et al., 2014) which models noun phrase
structure for open information extraction. They
report that 97% of the attributes in Freebase are
commonly expressed as noun phrases. However,
unlike our work, they considered open information
extraction and did not ground the extractions in an
external KB.
</bodyText>
<sectionHeader confidence="0.993664" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999978692307692">
In this paper, we present a semantic parsing ap-
proach with knowledge base incompleteness, ap-
plied to the problem of information extraction from
noun phrases. When run on all of the Wikipedia
category data, the approach would extract up to 12
million new Freebase facts at 72% precision.
There is significant potential for improving the
parsing models, as well as better optimizing the
precision recall trade-off for the extracted facts. It
would also be interesting to gather data with com-
positional phenomena, such as negation and dis-
junction, and study its impact on the performance
of the semantic parser.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999263875">
The research was supported in part by DARPA, un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), as well
as the NSF (IIS-1115966, IIS-1252835) and a gift
from Google, Inc. The authors thank Xiao Ling
for providing the apposition data, and Yoav Artzi,
Nicholas FitzGerald, Mike Lewis, Sameer Singh
and Mark Yatskar for discussion and comments.
</bodyText>
<page confidence="0.994434">
1319
</page>
<sectionHeader confidence="0.995875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999811044247788">
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the Empir-
ical Methods in Natural Language Processing.
Antoine Bordes, Jason Weston, and Sumit Chopra.
2014. Question answering with subgraph embed-
dings. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, Doha, Qatar. Association for Computational
Linguistics.
Qingqing Cai and Alexandar Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Dayne Freitag. 1998. Toward general-purpose learn-
ing for information extraction. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics.
Matthew Gerber and Joyce Y Chai. 2010. Beyond
nombank: a study of implicit arguments for nomi-
nal predicates. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics.
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Dan Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the Con-
ference of the Association of Computational Linguis-
tics.
Mandar Joshi, Uma Sawat, and Soumen Chakrabarti.
2014. Knowledge graph and corpus driven segmen-
tation and answer inference for telegraphic entity-
seeking queries. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Jayant Krishnamurthy and Tom M Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. Association
for Computational Linguistics.
Jayant Krishnamurthy and Tom M. Mitchell. 2014.
Joint syntactic and semantic parsing with combi-
natory categorial grammar. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Jayant Krishnamurthy and Tom Mitchell. 2015. Learn-
ing a compositional semantics for freebase with an
opne predicate vocabulary. Transactions of the As-
sociation for Computational Linguistics.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating noun ar-
gument structure for nombank. In LREC, volume 4.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.
Siva Reddy, Mirella Lapata, and Mark Steedman. 2014.
Large-scale semantic parsing without question-
answer pairs. Transactions of the Association for
Computational Linguistics, 2.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics.
Mark Steedman. 1996. Surface Structure and Interpre-
tation. The MIT Press.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with ccg. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics.
Zhenghao Wang, Shengquan Yan, Huaming Wang,
and Xuedong Huang. 2014. An overview of mi-
crosoft deep qa system on stanford webquestions
benchmark. Technical Report MSR-TR-2014-121,
September.
Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In Proceedings of World Wide Web
Conference.
Mohamed Yahya, Steven Euijong Whang, Rahul
Gupta, and Alon Halevy. 2014. Renoun: Fact ex-
traction for nominal attributes. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
</reference>
<page confidence="0.989975">
1320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975404">
<title confidence="0.999978">Scalable Semantic Parsing with Partial Ontologies</title>
<author confidence="0.999012">Choi Tom</author>
<affiliation confidence="0.999906">Computer Science &amp; University of Washington</affiliation>
<email confidence="0.999809">eunsol@cs.washington.edu,tomkwiat@google.com,lsz@cs.washington.edu</email>
<abstract confidence="0.99883275">We consider the problem of building scalable semantic parsers for Freebase, and present a new approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts. We study this problem on two newly introduced large-scale noun phrase datasets, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="35876" citStr="Berant and Liang, 2014" startWordPosition="5725" endWordPosition="5729">he approach we present here, however they focus on a small, fixed, set of binary relations while we aim to represent as much of the text as possible using the entire Freebase ontology. Krishnamurthy and Mitchell (2015) have also studied semantic parsing with open concepts via matrix factorization. They use Freebase entities but do not include Freebase concepts. The problem of building complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntactic and semantic structure</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1166" citStr="Berant et al., 2013" startWordPosition="164" endWordPosition="167">e noun phrase datasets, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. 1 Introduction Recently, significant progress has been made in learning semantic parsers for large knowledge bases (KBs) such as Freebase (FB) (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Reddy et al., 2014). Although these methods can build general purpose meaning representations, they are typically evaluated on question answering tasks and are designed to only parse questions that have complete ontological coverage, in the sense that there exists a logical form that can be executed against Freebase to get the correct answer.1 In this paper, we instead consider the problem of learning semantic parsers for open domain text containing †Now at Google, NY. 1To ensure all questions are answerable, the data is manually filtered. For example, the WebQuesti</context>
<context position="35852" citStr="Berant et al., 2013" startWordPosition="5721" endWordPosition="5724">imilar in spirit to the approach we present here, however they focus on a small, fixed, set of binary relations while we aim to represent as much of the text as possible using the entire Freebase ontology. Krishnamurthy and Mitchell (2015) have also studied semantic parsing with open concepts via matrix factorization. They use Freebase entities but do not include Freebase concepts. The problem of building complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntacti</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Sumit Chopra</author>
</authors>
<title>Question answering with subgraph embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar.</location>
<contexts>
<context position="36237" citStr="Bordes et al., 2014" startWordPosition="5793" endWordPosition="5796">The problem of building complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntactic and semantic structure of noun phrases has been extensively studied. For example, work on NomBank (Meyers et al., 2004; Gerber and Chai, 2010) focus on the challenge of modeling implicit arguments introduced by nominal predicates. In a manual study, we discovered that the 65% of our noun phrases contain implicit relations. We build on insights from Vadas and Curran (2008), who stu</context>
</contexts>
<marker>Bordes, Weston, Chopra, 2014</marker>
<rawString>Antoine Bordes, Jason Weston, and Sumit Chopra. 2014. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexandar Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1145" citStr="Cai and Yates, 2013" startWordPosition="160" endWordPosition="163">introduced large-scale noun phrase datasets, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. 1 Introduction Recently, significant progress has been made in learning semantic parsers for large knowledge bases (KBs) such as Freebase (FB) (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Reddy et al., 2014). Although these methods can build general purpose meaning representations, they are typically evaluated on question answering tasks and are designed to only parse questions that have complete ontological coverage, in the sense that there exists a logical form that can be executed against Freebase to get the correct answer.1 In this paper, we instead consider the problem of learning semantic parsers for open domain text containing †Now at Google, NY. 1To ensure all questions are answerable, the data is manually filtered. For e</context>
<context position="35805" citStr="Cai and Yates, 2013" startWordPosition="5713" endWordPosition="5716"> information extraction applications. This is similar in spirit to the approach we present here, however they focus on a small, fixed, set of binary relations while we aim to represent as much of the text as possible using the entire Freebase ontology. Krishnamurthy and Mitchell (2015) have also studied semantic parsing with open concepts via matrix factorization. They use Freebase entities but do not include Freebase concepts. The problem of building complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods w</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexandar Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas FitzGerald</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Learning distributions over logical forms for referring expression generation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="37160" citStr="FitzGerald et al., 2013" startWordPosition="5940" endWordPosition="5943">work on NomBank (Meyers et al., 2004; Gerber and Chai, 2010) focus on the challenge of modeling implicit arguments introduced by nominal predicates. In a manual study, we discovered that the 65% of our noun phrases contain implicit relations. We build on insights from Vadas and Curran (2008), who studied how to model the syntactic structure of noun phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has been little work on IE from compound noun phrases. Most existing IE algorithms extract a single relation, usually represented as a verb that holds between a pair of named entities, for example with supervised learning techniques (Freitag, 1998) or via distant supervision (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011). We aim to go beyond relations between entity pairs, and to retrieve full semantics of noun phrases, extracting unary and binary relations for a single entity. A notable exception to this trend is the ReNoun system (Yahya et al., 2014) which models no</context>
</contexts>
<marker>FitzGerald, Artzi, Zettlemoyer, 2013</marker>
<rawString>Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer. 2013. Learning distributions over logical forms for referring expression generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward general-purpose learning for information extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="37416" citStr="Freitag, 1998" startWordPosition="5983" endWordPosition="5984">om Vadas and Curran (2008), who studied how to model the syntactic structure of noun phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has been little work on IE from compound noun phrases. Most existing IE algorithms extract a single relation, usually represented as a verb that holds between a pair of named entities, for example with supervised learning techniques (Freitag, 1998) or via distant supervision (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011). We aim to go beyond relations between entity pairs, and to retrieve full semantics of noun phrases, extracting unary and binary relations for a single entity. A notable exception to this trend is the ReNoun system (Yahya et al., 2014) which models noun phrase structure for open information extraction. They report that 97% of the attributes in Freebase are commonly expressed as noun phrases. However, unlike our work, they considered open information extraction and did not ground the extractions in an e</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Toward general-purpose learning for information extraction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Y Chai</author>
</authors>
<title>Beyond nombank: a study of implicit arguments for nominal predicates.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5532" citStr="Gerber and Chai (2010)" startWordPosition="848" endWordPosition="851">they describe, their underspecified logical form l0, and their final logical form y. first new dataset contains 365,000 Wikipedia category labels (Figure 1, top), each paired with the list of the associated Wikipedia entity pages. The second has 67,000 noun phrases paired with a single named entity, extracted from the appositive constructions in KBP 2009 newswire text (Figure 1, bottom).2 This new data is both large scale, and unique in the focus on noun phrases. Noun phrases contain a number of challenging compositional phenomena, including implicit relations and nounnoun modifiers (e.g. see Gerber and Chai (2010)). To better model text with only partial ontological support, we present a new semantic parser that builds logical forms with concepts from a target ontology and open concepts that are introduced when there is no appropriate concept match in the target ontology. Figure 2 shows examples of the meanings that we extract. Only the first of these examples can be fully represented using Freebase, all other examples require explicit modeling of open concepts. To build these logical forms, we follow recent work for Combinatory Categorical Grammar (CCG) semantic parsing with Freebase (Kwiatkowski et a</context>
<context position="36596" citStr="Gerber and Chai, 2010" startWordPosition="5848" endWordPosition="5851">on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntactic and semantic structure of noun phrases has been extensively studied. For example, work on NomBank (Meyers et al., 2004; Gerber and Chai, 2010) focus on the challenge of modeling implicit arguments introduced by nominal predicates. In a manual study, we discovered that the 65% of our noun phrases contain implicit relations. We build on insights from Vadas and Curran (2008), who studied how to model the syntactic structure of noun phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has been little work on IE f</context>
</contexts>
<marker>Gerber, Chai, 2010</marker>
<rawString>Matthew Gerber and Joyce Y Chai. 2010. Beyond nombank: a study of implicit arguments for nominal predicates. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Dan Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="37508" citStr="Hoffmann et al., 2011" startWordPosition="5997" endWordPosition="6000"> phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has been little work on IE from compound noun phrases. Most existing IE algorithms extract a single relation, usually represented as a verb that holds between a pair of named entities, for example with supervised learning techniques (Freitag, 1998) or via distant supervision (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011). We aim to go beyond relations between entity pairs, and to retrieve full semantics of noun phrases, extracting unary and binary relations for a single entity. A notable exception to this trend is the ReNoun system (Yahya et al., 2014) which models noun phrase structure for open information extraction. They report that 97% of the attributes in Freebase are commonly expressed as noun phrases. However, unlike our work, they considered open information extraction and did not ground the extractions in an external KB. 10 Conclusion In this paper, we present a semantic parsing approach with knowled</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Dan Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the Conference of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mandar Joshi</author>
<author>Uma Sawat</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Knowledge graph and corpus driven segmentation and answer inference for telegraphic entityseeking queries.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34913" citStr="Joshi et al. (2014)" startWordPosition="5573" endWordPosition="5576">tions. Counting only the correctly predicted 1318 triples, we see that over 40% of the unique relations we predict is not in the development set; our model learns to generalize based on the learned PMI features and other lexical cues. Finally, our approach extracted 2.0 entity attributes per Wikipedia phrase and 0.9 per apposition on average. This matches our intuition that the apposition dataset contains many more words that cannot be modeled with concepts in Freebase. 9 Related Work Recent work has begun to study the problem of knowledge base incompleteness and reasoning with open concepts. Joshi et al. (2014) describes an approach for mapping short search queries to a single Freebase relation, that benefits from modeling schema incompleteness. Additionally, Krishnamurthy et al. (2012; 2014) present a semantic parser that builds partial meaning representations with Freebase for information extraction applications. This is similar in spirit to the approach we present here, however they focus on a small, fixed, set of binary relations while we aim to represent as much of the text as possible using the entire Freebase ontology. Krishnamurthy and Mitchell (2015) have also studied semantic parsing with </context>
</contexts>
<marker>Joshi, Sawat, Chakrabarti, 2014</marker>
<rawString>Mandar Joshi, Uma Sawat, and Soumen Chakrabarti. 2014. Knowledge graph and corpus driven segmentation and answer inference for telegraphic entityseeking queries. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom M Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Joint syntactic and semantic parsing with combinatory categorial grammar.</title>
<date>2014</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<marker>Krishnamurthy, Mitchell, 2014</marker>
<rawString>Jayant Krishnamurthy and Tom M. Mitchell. 2014. Joint syntactic and semantic parsing with combinatory categorial grammar. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning a compositional semantics for freebase with an opne predicate vocabulary. Transactions of the Association for Computational Linguistics.</title>
<date>2015</date>
<contexts>
<context position="35472" citStr="Krishnamurthy and Mitchell (2015)" startWordPosition="5660" endWordPosition="5663"> base incompleteness and reasoning with open concepts. Joshi et al. (2014) describes an approach for mapping short search queries to a single Freebase relation, that benefits from modeling schema incompleteness. Additionally, Krishnamurthy et al. (2012; 2014) present a semantic parser that builds partial meaning representations with Freebase for information extraction applications. This is similar in spirit to the approach we present here, however they focus on a small, fixed, set of binary relations while we aim to represent as much of the text as possible using the entire Freebase ontology. Krishnamurthy and Mitchell (2015) have also studied semantic parsing with open concepts via matrix factorization. They use Freebase entities but do not include Freebase concepts. The problem of building complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of the</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2015</marker>
<rawString>Jayant Krishnamurthy and Tom Mitchell. 2015. Learning a compositional semantics for freebase with an opne predicate vocabulary. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1192" citStr="Kwiatkowski et al., 2013" startWordPosition="168" endWordPosition="171">s, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. 1 Introduction Recently, significant progress has been made in learning semantic parsers for large knowledge bases (KBs) such as Freebase (FB) (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Reddy et al., 2014). Although these methods can build general purpose meaning representations, they are typically evaluated on question answering tasks and are designed to only parse questions that have complete ontological coverage, in the sense that there exists a logical form that can be executed against Freebase to get the correct answer.1 In this paper, we instead consider the problem of learning semantic parsers for open domain text containing †Now at Google, NY. 1To ensure all questions are answerable, the data is manually filtered. For example, the WebQuestions dataset introduced by </context>
<context position="6141" citStr="Kwiatkowski et al., 2013" startWordPosition="947" endWordPosition="950"> and Chai (2010)). To better model text with only partial ontological support, we present a new semantic parser that builds logical forms with concepts from a target ontology and open concepts that are introduced when there is no appropriate concept match in the target ontology. Figure 2 shows examples of the meanings that we extract. Only the first of these examples can be fully represented using Freebase, all other examples require explicit modeling of open concepts. To build these logical forms, we follow recent work for Combinatory Categorical Grammar (CCG) semantic parsing with Freebase (Kwiatkowski et al., 2013), extended to model when open concepts should be used. We develop a two-stage learning algorithm: we first compute broad coverage lexical statistics over all of the data, which are then incorporated as features in a full parsing model. The parsing model is tuned on a hand-labeled data set with gold analyses. Experiments demonstrate the benefits of the new approach. It significantly outperforms strong base2All new data is available on the authors’ websites. lines on both a referring expression resolution task, where much like in the QA setting we directly evaluate if we recover the correct logi</context>
<context position="9227" citStr="Kwiatkowski et al (2013)" startWordPosition="1475" endWordPosition="1478">ms y, with the goal of adding new facts to the knowledge base K. To do this, we assume each x is additionally paired with an set of entities e. We also define an entity attribute to be a literal in y that uses only concepts from K. Finally, we extract, for each entity in e, all of the attributes listed in y. For example, the first logical form y in Figure 2 has two entity attributes: composer(JeanSibelius, x) and composition.form(x, Symphonic poems) which can be added to K for the entities {TheBard,Finlandia}. Model and Learning Our approach extends the two-stage semantic parser introduced by Kwiatkowski et al (2013). We use CCG to build domain-independent logical forms l0 and then introduce a new method for reasoning about how to map this intermediary representation onto both open concepts and K concepts (Sec. 4). To learn this model, we assume access to data with two different types of annotations. The first contains noun phrase descriptions x and described entity sets a (as in Figure 2), which can be easily gathered at scale with no manual data labeling effort. However, this data, in general, has significant amount of knowledge base incompleteness; many described concepts and entity attributes will be </context>
<context position="14531" citStr="Kwiatkowski et al., 2013" startWordPosition="2330" endWordPosition="2333">nted using Freebase, we executed the logical forms and compared the result to the labeled entity set. In total, 56% of the queries returned no entities and those that did return results have on average 15% overlap with the Wikipedia entity set. We also measured how often attributes from the labeled logical forms were assigned to the Wikipedia entities in FB, finding that only 33.6% were present. Given this rate, we estimate that it is possible to add 12 million new facts into FB from the 7 million entity-category pairs. 4 Mapping Text to Meaning We adopt a two-stage semantic parsing approach (Kwiatkowski et al., 2013). We first use a CCG parser to define a set CCG(x) of possible logical forms l0. Then we will choose the logical form l0 that closely matches the linguistic structure of the input text x, according to a learned linear model, and use an ontological match step that defines a set of transformations ONT(l0, K) to map this meaning to a Freebase query y. Figure 2 shows examples of x, l0 and y. In this section we describe our approach with the more detailed example derivation in Figure 3. We also describe the parameterization of a linear model that scores each derivation. CCG parsing We use a CCG (St</context>
<context position="15825" citStr="Kwiatkowski et al., 2013" startWordPosition="2546" endWordPosition="2549">an underspecified logical form l0. Figure 3a shows an example parse. The constants Former, Municipalities, in, Brandenburgh in l0 are not tied to the target knowledge base, causing the logical form to be underspecified. They can be replaced with Freebase constants in the later ontology matching step. Ontological Matching The ontological match step has structural match and constant match components. Structural match operators can collapse or expand sub-expressions in the logical forms to match equivalent typed concepts in the target knowledge base. We adopt existing structural match operators (Kwiatkowski et al., 2013) and refer readers to that work for details. Constant match operators replace underspecified constants in the underspecified logical form l0 with concepts from the target knowledge base. There are four constant match operations used in Figure 3. The first two constant matches, shown below, match underspecified constants with constants of the same type from Freebase. in → location.containedby Brandenburgh → BRANDENBURGH However, because we are modeling the semantics of phrases that are not covered by the Freebase schema, we also require the following two constant matches: Former(x) → OpenType m</context>
<context position="22106" citStr="Kwiatkowski et al., 2013" startWordPosition="3482" endWordPosition="3485"> countries that have been involved in wars are not assigned this type. We instead calculate type overlap statistics for all Freebase entities, to find likely missing types. For example, including the fact that the object of military conflict.combatants is very often of type location.country. Learning from Labeled Data We train each half of the prediction problem separately, as defined in Section 4, using the labeled training data introduced in Section 3. We use structured maxmargin perceptrons to learn feature weights for both the underspecified parse and the ontological match step following (Kwiatkowski et al., 2013). The aggregate statistics collected from 7 million category-entity pairs produce very useful lexical features. We integrate these statistics into our linear model by summing their values for each derivation and treating them as a feature. All of the other features described in Section 6 are not word specific and are therefore far less sparse. 6 Features We include a number of features that enable soft type checking on the output logical form, described first below, along with other features that measure different aspects of the analysis. Coherency features For example, consider the phrase “Th</context>
<context position="29651" citStr="Kwiatkowski et al., 2013" startWordPosition="4711" endWordPosition="4714">d per noun phrase (fact #). Comparison Systems We compare performance to a number of ablated versions of the full system, where we have removed the open-constant ontology matching operators (NoOpenSchema), the PMI features (NoPMI), or the type checking features (NoTyping). For the referring expression resolution task, we excluded the named entity type feature, as this assumes typing information about the entity we are extracting attributes for. We report results without the PMI features and the open schema matching operators (KCAZ13), which is a reimplementation of a recent Freebase QA model (Kwiatkowski et al., 2013). We also learn with gold named entity linking (Gold NE). For the entity attribute extraction, we built a supervised learning baseline that combines the output of two discrete SVMs, one for predicting unary relations and one for binary relations. Each classifier 1317 System Top n P R F1 fact # IE Baseline - 37.3 26.5 30.6 1.6 1 44.2 32.8 37.7 1.9 2 36.9 38.0 37.5 2.6 3 30.7 42.7 35.7 3.6 Our Approach 4 27.0 44.7 33.6 4.2 5 23.7 47.2 31.6 5.1 10 15.9 52.0 24.3 8.5 Table 5: Entity attribute extraction performance on the Wikipedia category development set. Data System P P* fact # Wikipedia IE Bas</context>
<context position="35831" citStr="Kwiatkowski et al., 2013" startWordPosition="5717" endWordPosition="5720">on applications. This is similar in spirit to the approach we present here, however they focus on a small, fixed, set of binary relations while we aim to represent as much of the text as possible using the entire Freebase ontology. Krishnamurthy and Mitchell (2015) have also studied semantic parsing with open concepts via matrix factorization. They use Freebase entities but do not include Freebase concepts. The problem of building complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are </context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>Annotating noun argument structure for nombank.</title>
<date>2004</date>
<booktitle>In LREC,</booktitle>
<volume>4</volume>
<contexts>
<context position="36572" citStr="Meyers et al., 2004" startWordPosition="5844" endWordPosition="5847">s will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntactic and semantic structure of noun phrases has been extensively studied. For example, work on NomBank (Meyers et al., 2004; Gerber and Chai, 2010) focus on the challenge of modeling implicit arguments introduced by nominal predicates. In a manual study, we discovered that the 65% of our noun phrases contain implicit relations. We build on insights from Vadas and Curran (2008), who studied how to model the syntactic structure of noun phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. Annotating noun argument structure for nombank. In LREC, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30744" citStr="Mintz et al (2009)" startWordPosition="4905" endWordPosition="4908">: Entity attribute extraction performance on the Wikipedia category development set. Data System P P* fact # Wikipedia IE Baseline 56.7 58.7 1.6 Our Approach 61.2 72.6 2.0 IE Baseline 4.9 13.9 1.3 Appos Our Approach 33.2 61.4 0.9 Table 6: Manual evaluation for entity attribute extraction on the test sets. is trained using the annotated Wikipedia categories. This dataset contains hundreds of unary and binary relations, which the IE baseline can predict. Each classifier is further anchored on a specific word, and includes n-gram and POS context features around that word, following features from Mintz et al (2009). To predict binary relations, we used named entities as anchors. For unary attributes we anchored on all possible nouns and adjectives. The final logical form includes the best relation predicted by each classifier. We use the Stanford CoreNLP5 toolkit for tokenization, named entity recognition, and part-of-speech tagging. 8 Results Tables 3 and 4 show performance on the referring expression resolution task. Tables 5 and 6 show performance on the extraction task. Reported precision is lower on the labeled development set than on the test set, where predicted logical forms are manually evaluat</context>
<context position="37463" citStr="Mintz et al., 2009" startWordPosition="5989" endWordPosition="5992"> to model the syntactic structure of noun phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has been little work on IE from compound noun phrases. Most existing IE algorithms extract a single relation, usually represented as a verb that holds between a pair of named entities, for example with supervised learning techniques (Freitag, 1998) or via distant supervision (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011). We aim to go beyond relations between entity pairs, and to retrieve full semantics of noun phrases, extracting unary and binary relations for a single entity. A notable exception to this trend is the ReNoun system (Yahya et al., 2014) which models noun phrase structure for open information extraction. They report that 97% of the attributes in Freebase are commonly expressed as noun phrases. However, unlike our work, they considered open information extraction and did not ground the extractions in an external KB. 10 Conclusion In this paper, we pre</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Mirella Lapata</author>
<author>Mark Steedman</author>
</authors>
<title>Large-scale semantic parsing without questionanswer pairs.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<contexts>
<context position="1213" citStr="Reddy et al., 2014" startWordPosition="172" endWordPosition="175">tic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. 1 Introduction Recently, significant progress has been made in learning semantic parsers for large knowledge bases (KBs) such as Freebase (FB) (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Reddy et al., 2014). Although these methods can build general purpose meaning representations, they are typically evaluated on question answering tasks and are designed to only parse questions that have complete ontological coverage, in the sense that there exists a logical form that can be executed against Freebase to get the correct answer.1 In this paper, we instead consider the problem of learning semantic parsers for open domain text containing †Now at Google, NY. 1To ensure all questions are answerable, the data is manually filtered. For example, the WebQuestions dataset introduced by Berant et al. (2013) </context>
<context position="35897" citStr="Reddy et al., 2014" startWordPosition="5730" endWordPosition="5733">ere, however they focus on a small, fixed, set of binary relations while we aim to represent as much of the text as possible using the entire Freebase ontology. Krishnamurthy and Mitchell (2015) have also studied semantic parsing with open concepts via matrix factorization. They use Freebase entities but do not include Freebase concepts. The problem of building complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntactic and semantic structure of noun phrases has </context>
</contexts>
<marker>Reddy, Lapata, Steedman, 2014</marker>
<rawString>Siva Reddy, Mirella Lapata, and Mark Steedman. 2014. Large-scale semantic parsing without questionanswer pairs. Transactions of the Association for Computational Linguistics, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Benjamin M Marlin</author>
<author>Andrew McCallum</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="37484" citStr="Riedel et al., 2013" startWordPosition="5993" endWordPosition="5996">tic structure of noun phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has been little work on IE from compound noun phrases. Most existing IE algorithms extract a single relation, usually represented as a verb that holds between a pair of named entities, for example with supervised learning techniques (Freitag, 1998) or via distant supervision (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011). We aim to go beyond relations between entity pairs, and to retrieve full semantics of noun phrases, extracting unary and binary relations for a single entity. A notable exception to this trend is the ReNoun system (Yahya et al., 2014) which models noun phrase structure for open information extraction. They report that 97% of the attributes in Freebase are commonly expressed as noun phrases. However, unlike our work, they considered open information extraction and did not ground the extractions in an external KB. 10 Conclusion In this paper, we present a semantic parsi</context>
</contexts>
<marker>Riedel, Yao, Marlin, McCallum, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and Andrew McCallum. 2013. Relation extraction with matrix factorization and universal schemas. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="15144" citStr="Steedman, 1996" startWordPosition="2444" endWordPosition="2445">3). We first use a CCG parser to define a set CCG(x) of possible logical forms l0. Then we will choose the logical form l0 that closely matches the linguistic structure of the input text x, according to a learned linear model, and use an ontological match step that defines a set of transformations ONT(l0, K) to map this meaning to a Freebase query y. Figure 2 shows examples of x, l0 and y. In this section we describe our approach with the more detailed example derivation in Figure 3. We also describe the parameterization of a linear model that scores each derivation. CCG parsing We use a CCG (Steedman, 1996) semantic parser (Kwiatkowski et al., 2013) to generate an underspecified logical form l0. Figure 3a shows an example parse. The constants Former, Municipalities, in, Brandenburgh in l0 are not tied to the target knowledge base, causing the logical form to be underspecified. They can be replaced with Freebase constants in the later ontology matching step. Ontological Matching The ontological match step has structural match and constant match components. Structural match operators can collapse or expand sub-expressions in the logical forms to match equivalent typed concepts in the target knowle</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Mark Steedman. 1996. Surface Structure and Interpretation. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Parsing noun phrase structure with ccg.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36828" citStr="Vadas and Curran (2008)" startWordPosition="5886" endWordPosition="5889">Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntactic and semantic structure of noun phrases has been extensively studied. For example, work on NomBank (Meyers et al., 2004; Gerber and Chai, 2010) focus on the challenge of modeling implicit arguments introduced by nominal predicates. In a manual study, we discovered that the 65% of our noun phrases contain implicit relations. We build on insights from Vadas and Curran (2008), who studied how to model the syntactic structure of noun phrases in CCGBank. While we are, to the best of our knowledge, the first to study compound noun phrases for semantic parsing to knowledgebases, semantic parsers for noun phrase referring expressions have been built for visual referring expression (FitzGerald et al., 2013). There has been little work on IE from compound noun phrases. Most existing IE algorithms extract a single relation, usually represented as a verb that holds between a pair of named entities, for example with supervised learning techniques (Freitag, 1998) or via dist</context>
</contexts>
<marker>Vadas, Curran, 2008</marker>
<rawString>David Vadas and James R. Curran. 2008. Parsing noun phrase structure with ccg. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghao Wang</author>
<author>Shengquan Yan</author>
<author>Huaming Wang</author>
<author>Xuedong Huang</author>
</authors>
<title>An overview of microsoft deep qa system on stanford webquestions benchmark.</title>
<date>2014</date>
<tech>Technical Report MSR-TR-2014-121,</tech>
<contexts>
<context position="36257" citStr="Wang et al., 2014" startWordPosition="5797" endWordPosition="5800">ng complete sentence analyses using all of the Freebase ontology has recently received attention within the context of question answering systems (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Since they do not model KB incompleteness, these models will not work well on data that cannot be fully modeled by Freebase. In section 7, we report results using one of these systems to provide a reference point for our approach. There has also been other work on Freebase question answering (Yao and Van Durme, 2014; Bordes et al., 2014; Wang et al., 2014) that directly searches the facts in the KB to find answers without explicitly modeling compositional semantic structure. Therefore, these methods will suffer when facts are missing. The syntactic and semantic structure of noun phrases has been extensively studied. For example, work on NomBank (Meyers et al., 2004; Gerber and Chai, 2010) focus on the challenge of modeling implicit arguments introduced by nominal predicates. In a manual study, we discovered that the 65% of our noun phrases contain implicit relations. We build on insights from Vadas and Curran (2008), who studied how to model th</context>
</contexts>
<marker>Wang, Yan, Wang, Huang, 2014</marker>
<rawString>Zhenghao Wang, Shengquan Yan, Huaming Wang, and Xuedong Huang. 2014. An overview of microsoft deep qa system on stanford webquestions benchmark. Technical Report MSR-TR-2014-121, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert West</author>
<author>Evgeniy Gabrilovich</author>
<author>Kevin Murphy</author>
<author>Shaohua Sun</author>
<author>Rahul Gupta</author>
<author>Dekang Lin</author>
</authors>
<title>Knowledge base completion via search-based question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of World Wide Web Conference.</booktitle>
<contexts>
<context position="2664" citStr="West et al., 2014" startWordPosition="402" endWordPosition="405">n very large knowledge bases have two types of incompleteness that provide challenges for semantic parsing algorithms. They (1) have partial ontologies that cannot represent the meaning of many English phrases and (2) are typically missing many facts. For example, consider the phrases in Figure 1. They include subjective or otherwise unmodeled phrases such as “relaxed” and “quakehit.” Freebase, despite being large-scale, contains a limited set of concepts that cannot represent the meaning of these phrases. They also refer to entities that may be missing key facts. For example, a recent study (West et al., 2014) showed that over 70% of people in FB have no birth place, and 99% have no ethnicity. In our work, we introduce a new semantic parsing approach that explicitly models ontological incompleteness and is robust to missing facts, with the goal of recovering as much of a sentence’s meaning as the ontology supports. We argue that this will enable the application of semantic parsers to a range of new tasks, such as information extraction (IE), where phrases rarely have full ontological support and new facts must be added to the KB. Because existing semantic parsing datasets have been filtered to limi</context>
</contexts>
<marker>West, Gabrilovich, Murphy, Sun, Gupta, Lin, 2014</marker>
<rawString>Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge base completion via search-based question answering. In Proceedings of World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Steven Euijong Whang</author>
<author>Rahul Gupta</author>
<author>Alon Halevy</author>
</authors>
<title>Renoun: Fact extraction for nominal attributes.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="37744" citStr="Yahya et al., 2014" startWordPosition="6038" endWordPosition="6041">xpression (FitzGerald et al., 2013). There has been little work on IE from compound noun phrases. Most existing IE algorithms extract a single relation, usually represented as a verb that holds between a pair of named entities, for example with supervised learning techniques (Freitag, 1998) or via distant supervision (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011). We aim to go beyond relations between entity pairs, and to retrieve full semantics of noun phrases, extracting unary and binary relations for a single entity. A notable exception to this trend is the ReNoun system (Yahya et al., 2014) which models noun phrase structure for open information extraction. They report that 97% of the attributes in Freebase are commonly expressed as noun phrases. However, unlike our work, they considered open information extraction and did not ground the extractions in an external KB. 10 Conclusion In this paper, we present a semantic parsing approach with knowledge base incompleteness, applied to the problem of information extraction from noun phrases. When run on all of the Wikipedia category data, the approach would extract up to 12 million new Freebase facts at 72% precision. There is signif</context>
</contexts>
<marker>Yahya, Whang, Gupta, Halevy, 2014</marker>
<rawString>Mohamed Yahya, Steven Euijong Whang, Rahul Gupta, and Alon Halevy. 2014. Renoun: Fact extraction for nominal attributes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>