<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9992595">
Preservation of Recognizability for
Synchronous Tree Substitution Grammars
</title>
<author confidence="0.996422">
Zolt´an F¨ul¨op Andreas Maletti Heiko Vogler
</author>
<affiliation confidence="0.996815666666667">
Department of Computer Science Departament de Filologies Rom`aniques Faculty of Computer Science
University of Szeged Universitat Rovira i Virgili Technische Universit¨at Dresden
Szeged, Hungary Tarragona, Spain Dresden, Germany
</affiliation>
<sectionHeader confidence="0.979884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982818181818">
We consider synchronous tree substitution
grammars (STSG). With the help of a
characterization of the expressive power
of STSG in terms of weighted tree bimor-
phisms, we show that both the forward and
the backward application of an STSG pre-
serve recognizability of weighted tree lan-
guages in all reasonable cases. As a con-
sequence, both the domain and the range
of an STSG without chain rules are recog-
nizable weighted tree languages.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938626865672">
The syntax-based approach to statistical machine
translation (Yamada and Knight, 2001) becomes
more and more competitive in machine transla-
tion, which is a subfield of natural language pro-
cessing (NLP). In this approach the full parse trees
of the involved sentences are available to the trans-
lation model, which can base its decisions on this
rich structure. In the competing phrase-based ap-
proach (Koehn et al., 2003) the translation model
only has access to the linear sentence structure.
There are two major classes of syntax-based
translation models: tree transducers and synchro-
nous grammars. Examples in the former class
are the top-down tree transducer (Rounds, 1970;
Thatcher, 1970), the extended top-down tree trans-
ducer (Arnold and Dauchet, 1982; Galley et al.,
2004; Knight and Graehl, 2005; Graehl et al.,
2008; Maletti et al., 2009), and the extended
multi bottom-up tree transducer (Lilin, 1981; En-
gelfriet et al., 2009; Maletti, 2010). The lat-
ter class contains the syntax-directed transduc-
tions of Lewis II and Stearns (1968), the gen-
eralized syntax-directed transductions (Aho and
Ullman, 1969), the synchronous tree substitu-
tion grammar (STSG) by Schabes (1990) and the
synchronous tree adjoining grammar (STAG) by
Abeill´e et al. (1990) and Shieber and Schabes
(1990). The first bridge between those two classes
were established in (Martin and Vere, 1970). Fur-
ther comparisons can be found in (Shieber, 2004)
for STSG and in (Shieber, 2006) for STAG.
One of the main challenges in NLP is the am-
biguity that is inherent in natural languages. For
instance, the sentence “I saw the man with the
telescope” has several different meanings. Some
of them can be distinguished by the parse tree,
so that probabilistic parsers (Nederhof and Satta,
2006) for natural languages can (partially) achieve
the disambiguation. Such a parser returns a set
of parse trees for each input sentence, and in
addition, each returned parse tree is assigned a
likelihood. Thus, the result can be seen as a
mapping from parse trees to probabilities where
the impossible parses are assigned the probabil-
ity 0. Such mappings are called weighted tree lan-
guages, of which some can be finitely represented
by weighted regular tree grammars (Alexandrakis
and Bozapalidis, 1987). Those weighted tree
languages are recognizable and there exist algo-
rithms (Huang and Chiang, 2005) that efficiently
extract the k-best parse trees (i.e., those with the
highest probability) for further processing.
In this paper we consider synchronized tree sub-
stitution grammars (STSG). To overcome a techni-
cal difficulty we add (grammar) nonterminals to
them. Since an STSG often uses the nontermi-
nals of a context-free grammar as terminal sym-
bols (i.e., its derived trees contain both termi-
nal and nonterminal symbols of the context-free
grammar), we call the newly added (grammar)
nonterminals of the STSG states. Substitution does
no longer take place at synchronized nonterminals
(of the context-free grammar) but at synchronized
states (one for the input and one for the output
side). The states themselves will not appear in the
final derived trees, which yields that it is sufficient
to assume that only identical states are synchro-
</bodyText>
<page confidence="0.822269">
1
</page>
<note confidence="0.9830325">
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 1–9,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999948630136986">
nized. Under those conventions a rule of an STSG
has the form q —* (s, t, V, a) where q is a state,
a E R&gt;0 is the rule weight, s is an input tree that
can contain states at the leaves, and t is an output
tree that can also contain states. Finally, the syn-
chronization is defined by V , which is a bijection
between the state-labeled leaves of s and t. We
require that V only relates identical states.
The rules of an STSG are applied in a step-wise
manner. Here we use a derivation relation to define
the semantics of an STSG. It can be understood as
the synchronization of the derivation relations of
two regular tree grammars (G´ecseg and Steinby,
1984; G´ecseg and Steinby, 1997) where the syn-
chronization is done on nonterminals (or states) in
the spirit of syntax-directed transductions (Lewis
II and Stearns, 1968). Thus each sentential form
is a pair of (nonterminal-) connected trees.
An STSG !9 computes a mapping Tq, called
its weighted tree transformation, that assigns a
weight to each pair of input and output trees,
where both the input and output tree may not con-
tain any state. This transformation is obtained as
follows: We start with two copies of the initial
state that are synchronized. Given a connected tree
pair (�, O, we can apply the rule q —* (s, t, V, a)
to each pair of synchronized states q. Such an ap-
plication replaces the selected state q in � by s and
the corresponding state q in ( by t. All the re-
maining synchronized states and the synchronized
states of V remain synchronized. The result is
a new connected tree pair. This step charges the
weight a. The weights of successive applications
(or steps) are multiplied to obtain the weight of the
derivation. The weighted tree transformation Tq
assigns to each pair of trees the sum of all weights
of derivations that derive that pair.
Shieber (2004) showed that for every classical
unweighted STSG there exists an equivalent bi-
morphism (Arnold and Dauchet, 1982). The con-
verse result only holds up to deterministic rela-
belings (G´ecseg and Steinby, 1984; G´ecseg and
Steinby, 1997), which remove the state informa-
tion from the input and output tree. It is this dif-
ference that motivates us to add states to STSG.
We generalize the result of Shieber (2004) and
prove that every weighted tree transformation that
is computable by an STSG can also be computed
by a weighted bimorphism and vice versa.
Given an STSG and a recognizable weighted
tree language cp of input trees, we investigate un-
der which conditions the weighted tree language
obtained by applying !9 to cp is again recognizable.
In other words, we investigate under which condi-
tions the forward application of !9 preserves rec-
ognizability. The same question is investigated for
backward application, which is the corresponding
operation given a recognizable weighted tree lan-
guage of output trees. Since STSG are symmet-
ric (i.e., input and output can be exchanged), the
results for backward application can be obtained
easily from the results for forward application.
Our main result is that forward application pre-
serves recognizability if the STSG !9 is output-
productive, which means that each rule of !9 con-
tains at least one output symbol that is not a state.
Dually, backward application preserves recogniz-
ability if !9 is input-productive, which is the anal-
ogous property for the input side. In fact, those re-
sults hold for weights taken from an arbitrary com-
mutative semiring (Hebisch and Weinert, 1998;
Golan, 1999), but we present the results only for
probabilities.
</bodyText>
<sectionHeader confidence="0.952125" genericHeader="introduction">
2 Preliminary definitions
</sectionHeader>
<bodyText confidence="0.991323066666667">
In this contribution we will work with ranked
trees. Each symbol that occurs in such a tree
has a fixed rank that determines the number of
children of nodes with this label. Formally, let
E be a ranked alphabet, which is a finite set E
together with a mapping rkE : E —* N that asso-
ciates a rank rkE(a) with every a E E. We let
Ek = {Q E E  |rkE(a) = k} be the set contain-
ing all symbols in E that have rank k. A E-tree
indexed by a set Q is a tree with nodes labeled by
elements of E U Q, where the nodes labeled by
some a E E have exactly rkE(a) children and the
nodes with labels of Q have no children. Formally,
the set TE(Q) of (term representations of) E-trees
indexed by a set Q is the smallest set T such that
</bodyText>
<listItem confidence="0.992002">
• Q C T and
• Q(t1, ... , tk) E T for every Q E Ek and
t1,...,tk E T.
</listItem>
<bodyText confidence="0.99902875">
We generally write a instead of a() for all a E E0.
We frequently work with the set pos(t) of po-
sitions of a E-tree t, which is defined as fol-
lows. If t E Q, then pos(t) = {E}, and if
</bodyText>
<equation confidence="0.9951315">
t = Q(t1, ... , tk), then
pos(t) = {E} U {iw  |1 G i G k, w E pos(tz)} .
</equation>
<bodyText confidence="0.8491895">
Thus, each position is a finite (possibly empty) se-
quence of natural numbers. Clearly, each position
</bodyText>
<page confidence="0.984898">
2
</page>
<bodyText confidence="0.999009">
designates a node of the tree, and vice versa. Thus
we identify nodes with positions. As usual, a leaf
is a node that has no children. The set of all leaves
of t is denoted by lv(t). Clearly, lv(t) ⊆ pos(t).
The label of a position w ∈ pos(t) is denoted
by t(w). Moreover, for every A ⊆ E ∪ Q, let
</bodyText>
<equation confidence="0.805712">
posA(t) = {w ∈ pos(t)  |t(w) ∈ A} and
lvA(t) = posA(t) ∩ lv(t) be the sets of po-
</equation>
<bodyText confidence="0.986031269230769">
sitions and leaves that are labeled with an ele-
ment of A, respectively. Let t ∈ TE(Q) and
w1, ... , wk ∈ lvQ(t) be k (pairwise) different
leaves. We write t[w1 ← t1, ... , wk ← tk] or just
t[wi ← ti  |1 ≤ i ≤ k] with t1, ... , tk ∈ TE(Q)
for the tree obtained from t by replacing, for every
1 ≤ i ≤ k, the leaf wi with the tree ti.
For the rest of this paper, let E and A be two
arbitrary ranked alphabets. To avoid consistency
issues, we assume that a symbol σ that occurs in
both E and A has the same rank in E and A; i.e.,
rkE(σ) = rkA(σ). A deterministic relabeling is
a mapping r: E → A such that r(σ) ∈ Ak for
every σ ∈ Ek. For a tree s ∈ TE, the relabeled
tree r(s) ∈ TA is such that pos(r(s)) = pos(s)
and (r(s))(w) = r(s(w)) for every w ∈ pos(s).
The class of tree transformations computed by de-
terministic relabelings is denoted by dREL.
A tree language (over E) is a subset of TE. Cor-
respondingly, a weighted tree language (over E)
is a mapping ϕ: TE → R≥0. A weighted tree
transformation (over E and A) is a mapping
τ : TE × TA → R≥0. Its inverse is the weighted
tree transformation τ−1 : TA × TE → R≥0, which
is defined by τ−1(t, s) = τ(s, t) for every t ∈ TA
and s ∈ TE.
</bodyText>
<sectionHeader confidence="0.9140415" genericHeader="method">
3 Synchronous tree substitution
grammars with states
</sectionHeader>
<bodyText confidence="0.99888015">
Let Q be a finite set of states with a distinguished
initial state qS ∈ Q. A connected tree pair is a
tuple (s, t, V, a) where s ∈ TE(Q), t ∈ TA(Q),
and a ∈ R≥0. Moreover, V : lvQ(s) → lvQ(t) is
a bijective mapping such that s(u) = t(v) for ev-
ery (u, v) ∈ V . We will often identify V with its
graph. Intuitively, a connected tree pair (s, t, V, a)
is a pair of trees (s, t) with a weight a such that
each node labeled by a state in s has a correspond-
ing node in t, and vice versa. Such a connected
tree pair (s, t, V, a) is input-productive and output-
productive if s ∈/ Q and t ∈/ Q, respectively. Let
Conn denote the set of all connected tree pairs that
use the index set Q. Moreover, let Connp ⊆ Conn
contain all connected tree pairs that are input- or
output-productive.
A synchronous tree substitution grammar G
(with states) over E, A, and Q (for short: STSG),
is a finite set of rules of the form q → (s, t, V, a)
where q ∈ Q and (s, t, V, a) ∈ Connp. We call
a rule q → (s, t, V, a) a q-rule, of which q and
(s, t, V, a) are the left-hand and right-hand side,
respectively, and a is its weight. The STSG G is
input-productive (respectively, output-productive)
if each of its rules is so. To simplify the following
development, we assume (without loss of general-
ity) that two different q-rules differ on more than
just their weight.&apos;
To make sure that we do not account essentially
the same derivation twice, we have to use a deter-
ministic derivation mode. Since the choice is im-
material, we use the leftmost derivation mode for
the output component t of a connected tree pair
(s, t, V, a). For every (s, t, V, a) ∈ Conn such
that V =6 ∅, the leftmost output position is the
pair (w, w0) ∈ V , where w0 is the leftmost (i.e.,
the lexicographically smallest) position of lvQ(t).
Next we define derivations. The derivation re-
lation induced by G is the binary relation ⇒G
over Conn such that
</bodyText>
<equation confidence="0.93329425">
ξ = (s1, t1, V1, a1) ⇒G (s2, t2, V2, a2) = ζ
if and only if the leftmost output position of ξ is
(w, w0) ∈ V1 and there exists a rule
s1(w) → (s, t, V, a) ∈ G
</equation>
<bodyText confidence="0.727114">
such that
</bodyText>
<listItem confidence="0.982669">
• s2 = s1[w ← s] and t2 = t1[w0 ← t],
• V2 = (V1 \ {(w, w0)}) ∪ V 0 where
V 0 = {(ww1, w0w2)  |(w1, w2) ∈ V }, and
• a2 = a1 · a.
</listItem>
<bodyText confidence="0.967855">
A sequence D = (ξ1, ... , ξn) ∈ Connn is a
derivation of (s, t, V, a) ∈ Conn from q ∈ Q if
</bodyText>
<listItem confidence="0.625392333333333">
•ξ1 = (q, q, {(ε, ε)}, 1),
•ξn = (s, t, V, a), and
• ξi ⇒G ξi+1 for every 1 ≤ i ≤ n − 1.
</listItem>
<bodyText confidence="0.430455">
The set of all such derivations is denoted by
D (s, t, V, a).
</bodyText>
<figure confidence="0.881415265306122">
For every q ∈ Q, s ∈ TE(Q), t ∈ TA(Q), and
bijection V : lvQ(s) → lvQ(t), let
�
τG(s,t,V ) =
�
a∈R&gt;p,D∈D&amp;quot;(s,t,V,a)
&apos;Formally, q — (s, t, V, a) E G and q --+ (s, t, V, b) E G
implies a = b.
a .
3
⇒g,lo
σ
σ α 324−1
�
α α
σ
α 36−1
�
σ
α σ
σ
o o
o o
σ
α σ
α α
⇒g,lo
σ
σ
e o
o e
o �1 o ⇒g,lo
6−1
⇒g,lo
σ
α σ
σ
α 108−1
�
σ
⇒g,lo
o α
α o
σ
18−1
�
e α
σ
α e
</figure>
<figureCaption confidence="0.999973">
Figure 1: Example derivation with the STSG G of Example 1.
</figureCaption>
<bodyText confidence="0.5507875">
Finally, the weighted tree transformation com-
puted by G is the weighted tree transformation
</bodyText>
<equation confidence="0.833527">
τq : TΣ × TΔ → R&gt;0 with τq(s, t) = τ&apos;S
q (s, t, ∅)
</equation>
<bodyText confidence="0.703799">
for every s ∈ TΣ and t ∈ TΔ. As usual, we
call two STSG equivalent if they compute the same
weighted tree transformation. We observe that
every STSG is essentially a linear, nondeleting
weighted extended top-down (or bottom-up) tree
transducer (Arnold and Dauchet, 1982; Graehl et
al., 2008; Engelfriet et al., 2009) without (both-
sided) epsilon rules, and vice versa.
</bodyText>
<equation confidence="0.788409666666667">
Example 1. Let us consider the STSG G over
E = A = {σ, α} and Q = {e, o} where qS = o,
rk(σ) = 2, and rk(α) = 0. The STSG G consists
</equation>
<bodyText confidence="0.925605">
of the following rules where V = {(1, 2), (2, 1)}
and id = {(1,1), (2, 2)}:
</bodyText>
<equation confidence="0.905096">
o → (σ(o, e), σ(e, o), V,1/3) (ρ1)
o → (σ(e, o), σ(o, e), V,1/6) (ρ2)
o → (σ(e, o), σ(e, o), id, 1/6) (ρ3)
o → (α, α, ∅,1/3) (ρ4)
e → (σ(e, e), σ(e, e), V,1/2) (ρ5)
e → (σ(o, o), σ(o, o), V,1/2) (ρ6)
Figure 1 shows a derivation induced by G. It can
easily be checked that τq(s, t) = 1
6,3,2,3,3 where
s = σ(σ(α, α), α) and t = σ(α, σ(α, α)). More-
</equation>
<bodyText confidence="0.8944717">
over, τq(s, s) = τq(s, t). If τ&apos;q(s, t, ∅) =6 0 with
q ∈ {e, o}, then s and t have the same number
of α-labeled leaves. This number is odd if q = o,
otherwise it is even. Moreover, at every position
w ∈ pos(s), the left and right subtrees s1 and s2
are interchanged in s and t (due to V in the rules
ρ1, ρ2, ρ5, ρ6) except if s1 and s2 contain an even
and odd number, respectively, of α-labeled leaves.
In the latter case, the subtrees can be interchanged
or left unchanged (both with probability 1/6).
</bodyText>
<sectionHeader confidence="0.981834" genericHeader="method">
4 Recognizable weighted tree languages
</sectionHeader>
<bodyText confidence="0.999844583333333">
Next, we recall weighted regular tree grammars
(Alexandrakis and Bozapalidis, 1987). To keep
the presentation simple, we identify WRTG with
particular STSG, in which the input and the out-
put components are identical. More precisely, a
weighted regular tree grammar over E and Q (for
short: WRTG) is an STSG G over E, E, and Q
where each rule has the form q → (s, s, id, a)
where id is the suitable (partial) identity mapping.
It follows that s ∈/ Q, which yields that we do not
have chain rules. In the rest of this paper, we will
specify a rule q → (s, s, id, a) of a WRTG sim-
ply by q a → s. For every q ∈ Q, we define the
weighted tree language ϕ&apos;q : TΣ(Q) → R&gt;0 gen-
erated by G from q by ϕ&apos;q(s) = τ&apos;q(s, s, idlvQ(s))
for every s ∈ TΣ(Q), where idlvQ(s) is the iden-
tity on lvQ(s). Moreover, the weighted tree lan-
guage ϕq : TΣ → R&gt;0 generated by G is defined
by ϕq(s) = ϕ&apos;Sq (s) for every s ∈ TΣ.
A weighted tree language ϕ: TΣ → R&gt;0 is
recognizable if there exists a WRTG G such that
ϕ = ϕq. We note that our notion of recognizabil-
ity coincides with the classical one (Alexandrakis
and Bozapalidis, 1987; F¨ul¨op and Vogler, 2009).
</bodyText>
<equation confidence="0.7530198">
Example 2. We consider the WRTG K over the in-
put alphabet E = {σ, α} and P = {p, q} with
qS = q, rk(σ) = 2, and rk(α) = 0. The WRTG K
contains the following rules:
q →0.4 σ(p, α) q →0.6 α p →1 σ(α, q) (ν1–ν3)
</equation>
<bodyText confidence="0.888381">
Let s ∈ TΣ be such that ϕK(s) =6 0. Then s is a
thin tree with zig-zag shape; i.e., there exists n ≥ 1
such that pos(s) contains exactly the positions:
</bodyText>
<listItem confidence="0.99414975">
• (12)2 for every 0 ≤ i ≤ bn�1
2 c, and
• (12)21, (12)22, and (12)211 for every integer
0 ≤ i ≤ bn�3
</listItem>
<sectionHeader confidence="0.35034" genericHeader="method">
2 c.
</sectionHeader>
<bodyText confidence="0.81694">
The integer n can be understood as the length of
a derivation that derives s from q. Some example
</bodyText>
<page confidence="0.988307">
4
</page>
<figureCaption confidence="0.811389333333333">
Figure 2: Example trees and their weight in ϕG
where G is the WRTG of Example 2.
trees with their weights are displayed in Figure 2.
</figureCaption>
<bodyText confidence="0.733203125">
Proposition 3. For every WRTG G there is an
equivalent WRTG G0 in normal form, in which the
right-hand side of every rule contains exactly one
symbol of E.
Proof. We can obtain the statement by a trivial ex-
tension to the weighted case of the approach used
in Lemma II.3.4 of (G´ecseg and Steinby, 1984)
and Section 6 of (G´ecseg and Steinby, 1997).
</bodyText>
<sectionHeader confidence="0.98472" genericHeader="method">
5 STSG and weighted bimorphisms
</sectionHeader>
<bodyText confidence="0.999963538461538">
In this section, we characterize the expressive
power of STSG in terms of weighted bimorphisms.
This will provide a conceptually clear pattern for
the construction in our main result (see Theo-
rem 6) concerning the closure of recognizable
weighted tree languages under forward and back-
ward application. For this we first recall tree ho-
momorphisms. Let F and E be two ranked al-
phabets. Moreover, let h: F → TE × (N∗)∗
be a mapping such that h(γ) = (s, u1, ... , uk)
for every γ ∈ Fk where s ∈ TE and all leaves
u1,... , uk ∈ lv(s) are pairwise different. The
mapping h induces the (linear and complete) tree
</bodyText>
<equation confidence="0.63531525">
e
for every γ ∈ Fk and d1, ... , dke(di) for ev-
Tr with
h(γ) = (s, u1, ... , uk) and di = h
</equation>
<bodyText confidence="0.872360583333333">
ery 1 ≤ i ≤ k. Moreover, every (linear and
complete) tree homomorphism is induced in this
way. In the rest of this paper we will not distin-
guish between h and eh and simply write h instead
e
of h. The homomorphism h is order-preserving
if u1 &lt; · · · &lt; uk for every γ ∈ Fk where
h(γ) = (s, u1, ... , uk). Finally, we note that
every τ ∈ dREL can be computed by a order-
preserving tree homomorphism.
A weighted bimorphism B over E and A con-
sists of a WRTG K over F and P and two tree ho-
</bodyText>
<figureCaption confidence="0.9858935">
Figure 3: Illustration of the semantics of the bi-
morphism B.
</figureCaption>
<equation confidence="0.813349375">
momorphisms
hin: Tr → TE and hout : Tr → To .
The bimorphism B computes the weighted tree
transformation τB : TE × To → R≥o with
XτB(s, t) = ϕK(d)
d∈h�1
in (�)∩h�1
out(t)
</equation>
<bodyText confidence="0.874629">
for every s ∈ TE and t ∈ To.
Without loss of generality, we assume that ev-
ery bimorphism B is presented by an WRTG K in
normal form and an order-preserving output ho-
momorphism hout. Next, we prepare the relation
between STSG and weighted bimorphisms. Let
G be an STSG over E, A, and Q. Moreover, let
B be a weighted bimorphism over E and A con-
sisting of (i) K over F and P in normal form,
(ii) hin, and (iii) order-preserving hout. We say
that G and B are related if Q = P and there
is a bijection θ: G → K such that, for every
rule ρ ∈ G with ρ = (q → (s, t, V, a)) and
θ(ρ) = (p � γ(p1, ... ,pk)) we have
</bodyText>
<listItem confidence="0.996008333333333">
• p = q,
•
hin(γ) = (s, u1, ... , uk),
• hout(γ) = (t, v1, ... , vk),
• V = {(u1, v1), ... , (uk, vk)}, and
• s(ui) = pi = t(vi) for every 1 ≤ i ≤ k.
Let G and B be related. The following three easy
statements can be used to prove that G and B are
equivalent:
1. For every derivation D ∈ D&apos;(s, t, ∅, a) with
q ∈ Q, s ∈ TE, t ∈ To, a ∈ R≥o, there exists
d ∈ Tr and a derivation D0 ∈ DK(d, d, ∅, a)
such that hin(d) = s and hout(d) = t.
2. For every d ∈ Tr and D0 ∈ D�K(d, d, ∅, a)
with q ∈ Q and a ∈ R≥o, there exists a
derivation D ∈ D�G(hin(d), hout(d), ∅, a).
3. The mentioned correspondence on deriva-
tions is a bijection.
</listItem>
<bodyText confidence="0.930437">
Given an STSG G, we can easily construct a
weighted bimorphism B such that G and B are re-
lated, and vice versa. Hence, STSG and weighted
</bodyText>
<figure confidence="0.993452181818182">
σ
σ
σ
α
α
α σ
α
σ
σ
α α
α
α α
weight: 0.6 weight: 0.24 weight: 0.096
homomorphism
e
h: Tr → TE, which is defined by
ed1,...,uk ← edkl
h(γ(d1, ... , dk)) = s[u1 ←
Tr ϕ)C R&gt;o
(hin, hout)
τB
Tr × To
</figure>
<page confidence="0.9454">
5
</page>
<bodyText confidence="0.999640090909091">
bimorphisms are equally expressive, which gener-
alizes the corresponding characterization result in
the unweighted case by Shieber (2004), which we
will state after the introduction of STSG↓.
Classical synchronous tree substitution gram-
mars (STSG↓) do not have states. An STSG↓ can
be seen as an STSG by considering every substitu-
tion site (i.e., each pair of synchronised nontermi-
nals) as a state.2 We illustrate this by means of an
example here. Let us consider the STSG↓ G with
the following rules:
</bodyText>
<listItem confidence="0.999758666666667">
• (S(α, B↓), S(D↓, β)) with weight 0.2
• (B(γ, B↓), D(δ, D↓)) with weight 0.3
• (B(α), D(β)) with weight 0.4.
</listItem>
<bodyText confidence="0.938456571428571">
The substitution sites are marked with ↓. Any
rule with root A can be applied to a substitution
site A↓. An equivalent STSG G0 has the rules:
hS, Si → (S(α, hB, Di), S(hB, Di, β), V, 0.2)
hB, Di → (B(γ, hB, Di), D(δ, hB, Di), V 0, 0.3)
hB, Di → (B(α), D(β), ∅, 0.4) ,
where V = {(2,1)} and V 0 = {(2, 2)}. It is easy
to see that G and G0 are equivalent.
Let E = {γ, γ0, γ00, α, β} where γ, γ0, γ00 ∈ E1
and α, β ∈ E0 (and γ0 =6 γ00 and α =6 β). We write
γm(t) with t ∈ TE for the tree γ(· · · γ(t) · · · ) con-
taining m occurrences of γ above t. STSG↓ have a
certain locality property, which yields that STSG↓
cannot compute transformations like
</bodyText>
<equation confidence="0.894005">
{ 1 if s = γ0(γm(α)) = t
or s = γ00(γm(β)) = t
0 otherwise
</equation>
<bodyText confidence="0.999531857142857">
for every s, t ∈ TE. The non-local feature is the
correspondence between the symbols γ0 and α (in
the first alternative) and the symbols γ00 and β (in
the second alternative). An STSG that computes τ
is presented in Figure 4.
Theorem 4. Let τ be a weighted tree transforma-
tion. Then the following are equivalent.
</bodyText>
<listItem confidence="0.9376235">
1. τ is computable by an STSG.
2. τ is computable by a weighted bimorphism.
3. There exists a STSG↓ G and deterministic re-
labelings r1 and r2 such that
</listItem>
<equation confidence="0.92981375">
τ(s, t) = E τG(s0, t0) .
s�∈r−1
1 (s),t�∈r−1
2 (t)
</equation>
<footnote confidence="0.884418">
2To avoid a severe expressivity restriction, several initial
states are allowed for an STSGI.
</footnote>
<bodyText confidence="0.999546666666667">
The inverse of an STSG computable weighted
tree transformation can be computed by an STSG.
Formally, the inverse of the STSG G is the STSG
</bodyText>
<equation confidence="0.932358">
G−1 = {(t, s, V −1, a)  |(s, t, V, a) ∈ G}
</equation>
<bodyText confidence="0.836114">
where V −1 is the inverse of V . Then τG−1 = τ−1
</bodyText>
<equation confidence="0.647006">
G .
</equation>
<sectionHeader confidence="0.92736" genericHeader="evaluation">
6 Forward and backward application
</sectionHeader>
<bodyText confidence="0.999697444444445">
Let us start this section with the definition of the
concepts of forward and backward application of a
weighted tree transformation τ : TE × To → R≥0
to weighted tree languages ϕ: TE → R≥0 and
ψ: To → R≥0. We will give general definitions
first and deal with the potentially infinite sums
later. The forward application of τ to ϕ is the
weighted tree language τ(ϕ): To → R≥0, which
is defined for every t ∈ To by
</bodyText>
<equation confidence="0.9972365">
(τ(ϕ))(t) = E ϕ(s) · τ(s, t) . (1)
s∈TE
</equation>
<bodyText confidence="0.999540666666667">
Dually, the backward application of τ to ψ is
the weighted tree language τ−1(ψ): TE → R≥0,
which is defined for every s ∈ TE by
</bodyText>
<equation confidence="0.9900665">
(τ−1(ψ))(s) = E τ(s, t) · ψ(t) . (2)
t∈TA
</equation>
<bodyText confidence="0.93015904">
In general, the sums in Equations (1) and (2) can
be infinite. Let us recall the important property
that makes them finite in our theorems.
Proposition 5. For every input-productive (resp.,
output-productive) STSG G and every tree s ∈ TE
(resp., t ∈ To), there exist only finitely many
trees t ∈ To (respectively, s ∈ TE) such that
τG(s, t) =6 0.
Proof sketch. If G is input-productive, then each
derivation step creates at least one input symbol.
Consequently, any derivation for the input tree s
can contain at most as many steps as there are
nodes (or positions) in s. Clearly, there are only
finitely many such derivations, which proves the
statement. Dually, we can obtain the statement for
output-productive STSG.
In the following, we will consider forward ap-
plications τG(ϕ) where G is an output-productive
STSG and ϕ is recognizable, which yields that (1)
is well-defined by Proposition 5. Similarly, we
consider backward applications τ−1
G (ψ) where G
is input-productive and ψ is recognizable, which
again yields that (2) is well-defined by Proposi-
tion 5. The question is whether τG(ϕ) and τ−1
</bodyText>
<equation confidence="0.761924">
G (ψ)
τ(s, t) =
6
γi
q0 →
q1 q1
— 1
γi
γ
q1 →
q1
γ
q1
1 —
γii
q0 →
γii
1 —
q2 q2
q2 q2 q2 → β —1 β
1
—α
γ q1 → α
γ
q2 →
— 1
</equation>
<figureCaption confidence="0.995196">
Figure 4: STSG computing the weighted tree transformation τ with initial state q0.
</figureCaption>
<bodyText confidence="0.997043875">
are again recognizable. To avoid confusion, we
occasionally use angled parentheses as in hp, qi
instead of standard parentheses as in (p, q). More-
over, for ease of presentation, we identify the ini-
tial state qS with hqS, qSi.
Theorem 6. Let G be an STSG over Σ, Δ, and Q.
Moreover, let ϕ: TE → R≥0 and ψ: TA → R≥0
be recognizable weighted tree languages.
</bodyText>
<listItem confidence="0.998908">
1. If G is output-productive, then τG(ϕ) is rec-
ognizable.
2. If G is input-productive, then τ−1
</listItem>
<equation confidence="0.964478">
G (ψ) is rec-
ognizable.
</equation>
<bodyText confidence="0.998702785714286">
Proof. For the first item, let K be a WRTG over
Σ and P such that ϕ = ϕK. Without loss of gen-
erality, we suppose that K is in normal form.
Intuitively, we take each rule q → (s, t, V, a)
of G and run the WRTG K with every start state p
on the input side s of the rule. In this way, we
obtain a weight b. The WRTG will reach the state
leaves of s in certain states, which we then trans-
fer to the linked states in t to obtain t0. Finally, we
remove the input side and obtain a rule hp, qi →ab t0
for the WRTG L that represents the forward ap-
plication. We note that the same rule of L might
be constructed several times. If this happens, then
we replace the several copies by one rule whose
weight is the sum of the weights of all copies.
As already mentioned the initial state is hqS, qSi.
Clearly, this approach is inspired (and made rea-
sonable) by the bimorphism characterization. We
can take the HADAMARD product of the WRTG of
the bimorphism with the inverse image of ϕK un-
der its input homomorphism. Then we can simply
project to the output side. Our construction per-
forms those three steps at once. The whole process
is illustrated in Figure 5.
Formally, we construct the WRTG L over Δ and
P ×Q with the following rules. Let p ∈ P, q ∈ Q,
and t0 ∈ TA(P × Q). Then hp, qi →c t0 is a rule
in L0, where
</bodyText>
<equation confidence="0.992339666666667">
c = � ab .
(q→(s,t,V,a))∈G
V ={(u1,v1),...,(uk,vk)}
p1,...,pk∈P
t&apos;=t[vi←hpi,t(vi)i|1≤i≤k]
b=ϕ-&apos; �(s[ui←pi|1≤i≤k])
</equation>
<bodyText confidence="0.998040428571429">
This might create infinitely many rules in L0, but
clearly only finitely many will have a weight dif-
ferent from 0. Thus, we can obtain the finite rule
set L by removing all rules with weight 0.
The main statement to prove is the following:
for every t ∈ TA(Q) with lvQ(t) = {v1, ... , vk},
p,p1,...,pk ∈ P, and q ∈ Q
</bodyText>
<equation confidence="0.99681325">
ϕp K(s0) · τq G(s, t, V ) = ϕhp,qi
L (t0) ,
s∈TΣ(Q)
u1,...,uk∈lvQ(s)
</equation>
<bodyText confidence="0.994667">
where
</bodyText>
<listItem confidence="0.96618425">
• V = {(u1, v1), ... , (uk, vk)},
• s0 = s[ui ← pi  |1 ≤ i ≤ k], and
• t0 = t[vi ← hpi, t(vi)i  |1 ≤ i ≤ k].
In particular, for t ∈ TA we obtain
</listItem>
<equation confidence="0.981088666666667">
� ϕpK(s) · τqG(s, t, ∅) = ϕhp,qi
s∈TΣ L (t) ,
which yields
(τG(ϕK))(t) = �
s∈TΣ
�=
s∈TΣ
= ϕL (t) = ϕL(t) .
hqS,qSi
</equation>
<bodyText confidence="0.973301">
In the second item G is input-productive. Then
G−1 is output-productive and τ−1
</bodyText>
<equation confidence="0.999082333333333">
G (ψ) = τG−1(ψ).
Hence the first statement proves that τ−1
G (ψ) is
</equation>
<bodyText confidence="0.9231504">
recognizable.
Example 7. As an illustration of the construction
in Theorem 6, let us apply the STSG G of Exam-
ple 1 to the WRTG K over Σ and P = {p, qS, qα}
and the following rules:
</bodyText>
<equation confidence="0.9416655">
2
qS → σ(p, qα)
5 qS
p → σ(qα,qS)
1 qα → α .
1
</equation>
<bodyText confidence="0.9983985">
In fact, K is in normal form and is equivalent to
the WRTG of Example 2. Using the construction
in the proof of Theorem 6 we obtain the WRTG L
over Σ and P × Q with Q = {e, o}. We will only
</bodyText>
<figure confidence="0.612750285714286">
�
ϕK(s) · τG(s, t)
ϕqSK (s) · τqS
G (s, t,∅)
3
→ α
5
</figure>
<page confidence="0.526001">
7
</page>
<figure confidence="0.999495103448276">
σ
α σ
hq, oi hqα, oi
σ
σ
o o
o o
hq, oi
↓
σ
2
σ α 1
36 � �5
hqα, oi hq, oi
σ
α σ
o o
5
1 2
36
−−−→
hq, oi
o
↓
σ
α
α σ
� 36
1
</figure>
<figureCaption confidence="0.99781575">
Figure 5: Illustration of the construction in the proof of Theorem 6 using the WRTG K of Example 7:
some example rule (left), run of K on the input side of the rule (middle), and resulting rule (right).
Figure 6: WRTG constructed in Example 7. We
renamed the states and calculated the weights.
</figureCaption>
<bodyText confidence="0.979724333333333">
show rules of L that contribute to ϕL. To the right
of each rule we indicate from which state of K and
which rule of G the rule was constructed.
</bodyText>
<figure confidence="0.964772222222222">
1·2
−→ σ(hqα, oi, hp, ei) 6 5qS, ρ2
1·2
−→ σ(hp, ei, hqα, oi) 6 5qS, ρ3
1·
−→ α
3 5 qS, ρ4
1·1
3−→ α qα, ρ4
</figure>
<bodyText confidence="0.995011777777778">
the inverse of τ. Moreover, we can express the
domain dom(τ) of τ as the backward applica-
tion τ−1(1) where 1 is the weighted tree language
that assigns the weight 1 to each tree. Note that 1
is recognizable for every ranked alphabet.
We note that the sums in Equations (3) and (4)
might be infinite, but for input-productive (re-
spectively, output-productive) STSG G the do-
main dom(τq) (respectively, the range range(τq))
are well-defined by Proposition 5. Using those ob-
servations and Theorem 6 we can obtain the fol-
lowing statement.
Corollary 8. Let G be an STSG. If G is input-
productive, then dom(τq) is recognizable. More-
over, if G is output-productive, then range(τq) is
recognizable.
Proof. These statements follow directly from The-
orem 6 with the help of the observation that
</bodyText>
<figure confidence="0.944076379310345">
dom(τq) = τ−1
q (1) and range(τq) = τq(1).
σ
σ
1
q2 −→ α
3 q3
1
1
1
q1
−→ 15
−→ 15
5
−→ α
q2 q3 q1
q3 q2 q1
σ
1
−→ 5
q1 q2
hqS, oi
hqS, oi
hqS, oi
hqα, oi
3
hp, ei 1·2 Conclusion
−→ σ(hqS,oi, hqα,oi) p, ρ6
2 5
</figure>
<bodyText confidence="0.980659333333333">
The initial state of L is hqS, oi. It is easy to see that
every t ∈ TE such that ϕL(t) =6 0 is thin, which
means that |pos(t) ∩ N&apos; |≤ 2 for every n ∈ N.
</bodyText>
<sectionHeader confidence="0.967758" genericHeader="conclusions">
7 Domain and range
</sectionHeader>
<bodyText confidence="0.993431833333333">
Finally, let us consider the domain and range of a
weighted tree transformation τ : TE ×To → R&gt;0.
Again, we first give general definitions and deal
with the infinite sums that might occur in them
later. The domain dom(τ) of τ and the range
range(τ) of τ are defined by
</bodyText>
<equation confidence="0.9710155">
(dom(τ))(s) = � τ(s, u) (3)
uETΔ
(range(τ))(t) = � τ(u, t) (4)
uETΣ
</equation>
<bodyText confidence="0.999938461538462">
for every s ∈ TE and t ∈ To. Obviously,
the domain dom(τ) is the range range(τ−1) of
We showed that every output-productive STSG
preserves recognizability under forward applica-
tion. Dually, every input-productive STSG pre-
serves recognizability under backward applica-
tion. We presented direct and effective construc-
tions for these operations. Special cases of those
constructions can be used to compute the domain
of an input-productive STSG and the range of an
output-productive STSG. Finally, we presented a
characterization of the power of STSG in terms of
weighted bimorphisms.
</bodyText>
<sectionHeader confidence="0.99794" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.870108166666667">
ZOLT ´AN F ¨UL¨OP and HEIKO VOGLER were finan-
cially supported by the T´AMOP-4.2.2/08/1/2008-
0008 program of the Hungarian National Devel-
opment Agency. ANDREAS MALETTI was finan-
cially supported by the Ministerio de Educaci´on y
Ciencia (MEC) grant JDCI-2007-760.
</bodyText>
<page confidence="0.997279">
8
</page>
<sectionHeader confidence="0.993865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907563218391">
Anne Abeill´e, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized TAGs for machine trans-
lation. In Proc. 13th CoLing, volume 3, pages 1–6.
University of Helsinki, Finland.
Alfred V. Aho and Jeffrey D. Ullman. 1969. Transla-
tions on a context-free grammar. In Proc. 1st STOC,
pages 93–112. ACM.
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene’s theorem.
Inf. Process. Lett., 24(1):1–4.
Andr´e Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d’arbres. Theoret. Comput. Sci.,
20(1):33–93.
Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009. Extended multi bottom-up tree transducers
— composition and decomposition. Acta Inform.,
46(8):561–590.
Zolt´an Fil¨op and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9, pages 313–403.
Springer.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. HLT-NAACL 2004, pages 273–280. ACL.
Ferenc G´ecseg and Magnus Steinby. 1984. Tree Au-
tomata. Akad´emiai Kiad´o, Budapest, Hungary.
Ferenc G´ecseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, chapter 1,
pages 1–68. Springer.
Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391–427.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings
— Algebraic Theory and Applications in Computer
Science. World Scientific.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. 9th IWPT, pages 53–64. ACL.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natural
language processing. In Proc. 6th CICLing, volume
3406 of LNCS, pages 1–24. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 48–54. ACL.
Philip M. Lewis II and Richard Edwin Stearns. 1968.
Syntax-directed transductions. J. ACM, 15(3):465–
488.
Eric Lilin. 1981. Propri´et´es de clˆoture d’une extension
de transducteurs d’arbres d´eterministes. In Proc.
6th CAAP, volume 112 of LNCS, pages 280–289.
Springer.
Andreas Maletti, Jonathan Graehl, Mark Hopkins,
and Kevin Knight. 2009. The power of ex-
tended top-down tree transducers. SIAMJ. Comput.,
39(2):410–430.
Andreas Maletti. 2010. Why synchronous tree substi-
tution grammars? In Proc. HLT-NAACL 2010. ACL.
to appear.
David F. Martin and Steven A. Vere. 1970. On syntax-
directed transduction and tree transducers. In Proc.
2nd STOC, pages 129–135. ACM.
Mark-Jan Nederhof and Giorgio Satta. 2006. Proba-
bilistic parsing strategies. J. ACM, 53(3):406–436.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257–287.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. 13th
CoLing, pages 253–258. ACL.
Stuart M. Shieber. 2004. Synchronous grammars as
tree transducers. In Proc. TAG+7, pages 88–95. Si-
mon Fraser University.
Stuart M. Shieber. 2006. Unifying synchronous tree
adjoining grammars and tree transducers via bimor-
phisms. In Proc. 11th EACL, pages 377–384. ACL.
James W. Thatcher. 1970. Generalized sequential
machine maps. J. Comput. System Sci., 4(4):339–
367.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. 39th
ACL, pages 523–530. ACL.
</reference>
<page confidence="0.997109">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974725">
<title confidence="0.9990425">Preservation of Recognizability Synchronous Tree Substitution Grammars</title>
<author confidence="0.999121">Zolt´an F¨ul¨op Andreas Maletti Heiko Vogler</author>
<affiliation confidence="0.999875">Department of Computer Science Departament de Filologies Rom`aniques Faculty of Computer Science University of Szeged Universitat Rovira i Virgili Technische Universit¨at Dresden</affiliation>
<address confidence="0.995784">Szeged, Hungary Tarragona, Spain Dresden, Germany</address>
<abstract confidence="0.9984825">We consider synchronous tree substitution With the help of a characterization of the expressive power terms of weighted tree bimorphisms, we show that both the forward and backward application of an preserve recognizability of weighted tree languages in all reasonable cases. As a consequence, both the domain and the range an chain rules are recognizable weighted tree languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using lexicalized TAGs for machine translation.</title>
<date>1990</date>
<booktitle>In Proc. 13th CoLing,</booktitle>
<volume>3</volume>
<pages>1--6</pages>
<institution>University of Helsinki,</institution>
<marker>Abeill´e, Schabes, Joshi, 1990</marker>
<rawString>Anne Abeill´e, Yves Schabes, and Aravind K. Joshi. 1990. Using lexicalized TAGs for machine translation. In Proc. 13th CoLing, volume 3, pages 1–6. University of Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Translations on a context-free grammar.</title>
<date>1969</date>
<booktitle>In Proc. 1st STOC,</booktitle>
<pages>93--112</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1919" citStr="Aho and Ullman, 1969" startWordPosition="287" endWordPosition="290"> There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural languages. For instance, the sentence “I saw the man with the telescope” has several different meanings. Some of them can be distinguished by the parse tree, so t</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1969. Translations on a context-free grammar. In Proc. 1st STOC, pages 93–112. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Athanasios Alexandrakis</author>
<author>Symeon Bozapalidis</author>
</authors>
<title>Weighted grammars and Kleene’s theorem.</title>
<date>1987</date>
<journal>Inf. Process. Lett.,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="3072" citStr="Alexandrakis and Bozapalidis, 1987" startWordPosition="475" endWordPosition="478">eral different meanings. Some of them can be distinguished by the parse tree, so that probabilistic parsers (Nederhof and Satta, 2006) for natural languages can (partially) achieve the disambiguation. Such a parser returns a set of parse trees for each input sentence, and in addition, each returned parse tree is assigned a likelihood. Thus, the result can be seen as a mapping from parse trees to probabilities where the impossible parses are assigned the probability 0. Such mappings are called weighted tree languages, of which some can be finitely represented by weighted regular tree grammars (Alexandrakis and Bozapalidis, 1987). Those weighted tree languages are recognizable and there exist algorithms (Huang and Chiang, 2005) that efficiently extract the k-best parse trees (i.e., those with the highest probability) for further processing. In this paper we consider synchronized tree substitution grammars (STSG). To overcome a technical difficulty we add (grammar) nonterminals to them. Since an STSG often uses the nonterminals of a context-free grammar as terminal symbols (i.e., its derived trees contain both terminal and nonterminal symbols of the context-free grammar), we call the newly added (grammar) nonterminals </context>
<context position="15061" citStr="Alexandrakis and Bozapalidis, 1987" startWordPosition="2849" endWordPosition="2852"> α)). Moreover, τq(s, s) = τq(s, t). If τ&apos;q(s, t, ∅) =6 0 with q ∈ {e, o}, then s and t have the same number of α-labeled leaves. This number is odd if q = o, otherwise it is even. Moreover, at every position w ∈ pos(s), the left and right subtrees s1 and s2 are interchanged in s and t (due to V in the rules ρ1, ρ2, ρ5, ρ6) except if s1 and s2 contain an even and odd number, respectively, of α-labeled leaves. In the latter case, the subtrees can be interchanged or left unchanged (both with probability 1/6). 4 Recognizable weighted tree languages Next, we recall weighted regular tree grammars (Alexandrakis and Bozapalidis, 1987). To keep the presentation simple, we identify WRTG with particular STSG, in which the input and the output components are identical. More precisely, a weighted regular tree grammar over E and Q (for short: WRTG) is an STSG G over E, E, and Q where each rule has the form q → (s, s, id, a) where id is the suitable (partial) identity mapping. It follows that s ∈/ Q, which yields that we do not have chain rules. In the rest of this paper, we will specify a rule q → (s, s, id, a) of a WRTG simply by q a → s. For every q ∈ Q, we define the weighted tree language ϕ&apos;q : TΣ(Q) → R&gt;0 generated by G fro</context>
</contexts>
<marker>Alexandrakis, Bozapalidis, 1987</marker>
<rawString>Athanasios Alexandrakis and Symeon Bozapalidis. 1987. Weighted grammars and Kleene’s theorem. Inf. Process. Lett., 24(1):1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Arnold</author>
<author>Max Dauchet</author>
</authors>
<title>Morphismes et bimorphismes d’arbres.</title>
<date>1982</date>
<journal>Theoret. Comput. Sci.,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1563" citStr="Arnold and Dauchet, 1982" startWordPosition="231" endWordPosition="234">translation, which is a subfield of natural language processing (NLP). In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Mar</context>
<context position="6163" citStr="Arnold and Dauchet, 1982" startWordPosition="1001" endWordPosition="1004">Such an application replaces the selected state q in � by s and the corresponding state q in ( by t. All the remaining synchronized states and the synchronized states of V remain synchronized. The result is a new connected tree pair. This step charges the weight a. The weights of successive applications (or steps) are multiplied to obtain the weight of the derivation. The weighted tree transformation Tq assigns to each pair of trees the sum of all weights of derivations that derive that pair. Shieber (2004) showed that for every classical unweighted STSG there exists an equivalent bimorphism (Arnold and Dauchet, 1982). The converse result only holds up to deterministic relabelings (G´ecseg and Steinby, 1984; G´ecseg and Steinby, 1997), which remove the state information from the input and output tree. It is this difference that motivates us to add states to STSG. We generalize the result of Shieber (2004) and prove that every weighted tree transformation that is computable by an STSG can also be computed by a weighted bimorphism and vice versa. Given an STSG and a recognizable weighted tree language cp of input trees, we investigate under which conditions the weighted tree language obtained by applying !9 </context>
<context position="13782" citStr="Arnold and Dauchet, 1982" startWordPosition="2584" endWordPosition="2587">,lo σ σ α 324−1 � α α σ α 36−1 � σ α σ σ o o o o σ α σ α α ⇒g,lo σ σ e o o e o �1 o ⇒g,lo 6−1 ⇒g,lo σ α σ σ α 108−1 � σ ⇒g,lo o α α o σ 18−1 � e α σ α e Figure 1: Example derivation with the STSG G of Example 1. Finally, the weighted tree transformation computed by G is the weighted tree transformation τq : TΣ × TΔ → R&gt;0 with τq(s, t) = τ&apos;S q (s, t, ∅) for every s ∈ TΣ and t ∈ TΔ. As usual, we call two STSG equivalent if they compute the same weighted tree transformation. We observe that every STSG is essentially a linear, nondeleting weighted extended top-down (or bottom-up) tree transducer (Arnold and Dauchet, 1982; Graehl et al., 2008; Engelfriet et al., 2009) without (bothsided) epsilon rules, and vice versa. Example 1. Let us consider the STSG G over E = A = {σ, α} and Q = {e, o} where qS = o, rk(σ) = 2, and rk(α) = 0. The STSG G consists of the following rules where V = {(1, 2), (2, 1)} and id = {(1,1), (2, 2)}: o → (σ(o, e), σ(e, o), V,1/3) (ρ1) o → (σ(e, o), σ(o, e), V,1/6) (ρ2) o → (σ(e, o), σ(e, o), id, 1/6) (ρ3) o → (α, α, ∅,1/3) (ρ4) e → (σ(e, e), σ(e, e), V,1/2) (ρ5) e → (σ(o, o), σ(o, o), V,1/2) (ρ6) Figure 1 shows a derivation induced by G. It can easily be checked that τq(s, t) = 1 6,3,2,3</context>
</contexts>
<marker>Arnold, Dauchet, 1982</marker>
<rawString>Andr´e Arnold and Max Dauchet. 1982. Morphismes et bimorphismes d’arbres. Theoret. Comput. Sci., 20(1):33–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost Engelfriet</author>
<author>Eric Lilin</author>
<author>Andreas Maletti</author>
</authors>
<title>Extended multi bottom-up tree transducers — composition and decomposition.</title>
<date>2009</date>
<journal>Acta Inform.,</journal>
<volume>46</volume>
<issue>8</issue>
<contexts>
<context position="1741" citStr="Engelfriet et al., 2009" startWordPosition="260" endWordPosition="264">h can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inhe</context>
<context position="13829" citStr="Engelfriet et al., 2009" startWordPosition="2592" endWordPosition="2595">o σ α σ α α ⇒g,lo σ σ e o o e o �1 o ⇒g,lo 6−1 ⇒g,lo σ α σ σ α 108−1 � σ ⇒g,lo o α α o σ 18−1 � e α σ α e Figure 1: Example derivation with the STSG G of Example 1. Finally, the weighted tree transformation computed by G is the weighted tree transformation τq : TΣ × TΔ → R&gt;0 with τq(s, t) = τ&apos;S q (s, t, ∅) for every s ∈ TΣ and t ∈ TΔ. As usual, we call two STSG equivalent if they compute the same weighted tree transformation. We observe that every STSG is essentially a linear, nondeleting weighted extended top-down (or bottom-up) tree transducer (Arnold and Dauchet, 1982; Graehl et al., 2008; Engelfriet et al., 2009) without (bothsided) epsilon rules, and vice versa. Example 1. Let us consider the STSG G over E = A = {σ, α} and Q = {e, o} where qS = o, rk(σ) = 2, and rk(α) = 0. The STSG G consists of the following rules where V = {(1, 2), (2, 1)} and id = {(1,1), (2, 2)}: o → (σ(o, e), σ(e, o), V,1/3) (ρ1) o → (σ(e, o), σ(o, e), V,1/6) (ρ2) o → (σ(e, o), σ(e, o), id, 1/6) (ρ3) o → (α, α, ∅,1/3) (ρ4) e → (σ(e, e), σ(e, e), V,1/2) (ρ5) e → (σ(o, o), σ(o, o), V,1/2) (ρ6) Figure 1 shows a derivation induced by G. It can easily be checked that τq(s, t) = 1 6,3,2,3,3 where s = σ(σ(α, α), α) and t = σ(α, σ(α, α)</context>
</contexts>
<marker>Engelfriet, Lilin, Maletti, 2009</marker>
<rawString>Joost Engelfriet, Eric Lilin, and Andreas Maletti. 2009. Extended multi bottom-up tree transducers — composition and decomposition. Acta Inform., 46(8):561–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an Fil¨op</author>
<author>Heiko Vogler</author>
</authors>
<title>Weighted tree automata and tree transducers.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, chapter 9,</booktitle>
<pages>313--403</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer.</publisher>
<marker>Fil¨op, Vogler, 2009</marker>
<rawString>Zolt´an Fil¨op and Heiko Vogler. 2009. Weighted tree automata and tree transducers. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 9, pages 313–403. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL 2004,</booktitle>
<pages>273--280</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1584" citStr="Galley et al., 2004" startWordPosition="235" endWordPosition="238">bfield of natural language processing (NLP). In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL 2004, pages 273–280. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc G´ecseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree Automata. Akad´emiai Kiad´o,</title>
<date>1984</date>
<location>Budapest, Hungary.</location>
<marker>G´ecseg, Steinby, 1984</marker>
<rawString>Ferenc G´ecseg and Magnus Steinby. 1984. Tree Automata. Akad´emiai Kiad´o, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc G´ecseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree languages.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, chapter 1,</booktitle>
<pages>1--68</pages>
<publisher>Springer.</publisher>
<marker>G´ecseg, Steinby, 1997</marker>
<rawString>Ferenc G´ecseg and Magnus Steinby. 1997. Tree languages. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, chapter 1, pages 1–68. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan S Golan</author>
</authors>
<title>Semirings and their Applications.</title>
<date>1999</date>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="7686" citStr="Golan, 1999" startWordPosition="1254" endWordPosition="1255">e symmetric (i.e., input and output can be exchanged), the results for backward application can be obtained easily from the results for forward application. Our main result is that forward application preserves recognizability if the STSG !9 is outputproductive, which means that each rule of !9 contains at least one output symbol that is not a state. Dually, backward application preserves recognizability if !9 is input-productive, which is the analogous property for the input side. In fact, those results hold for weights taken from an arbitrary commutative semiring (Hebisch and Weinert, 1998; Golan, 1999), but we present the results only for probabilities. 2 Preliminary definitions In this contribution we will work with ranked trees. Each symbol that occurs in such a tree has a fixed rank that determines the number of children of nodes with this label. Formally, let E be a ranked alphabet, which is a finite set E together with a mapping rkE : E —* N that associates a rank rkE(a) with every a E E. We let Ek = {Q E E |rkE(a) = k} be the set containing all symbols in E that have rank k. A E-tree indexed by a set Q is a tree with nodes labeled by elements of E U Q, where the nodes labeled by some </context>
</contexts>
<marker>Golan, 1999</marker>
<rawString>Jonathan S. Golan. 1999. Semirings and their Applications. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="1630" citStr="Graehl et al., 2008" startWordPosition="243" endWordPosition="246">n this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, </context>
<context position="13803" citStr="Graehl et al., 2008" startWordPosition="2588" endWordPosition="2591">36−1 � σ α σ σ o o o o σ α σ α α ⇒g,lo σ σ e o o e o �1 o ⇒g,lo 6−1 ⇒g,lo σ α σ σ α 108−1 � σ ⇒g,lo o α α o σ 18−1 � e α σ α e Figure 1: Example derivation with the STSG G of Example 1. Finally, the weighted tree transformation computed by G is the weighted tree transformation τq : TΣ × TΔ → R&gt;0 with τq(s, t) = τ&apos;S q (s, t, ∅) for every s ∈ TΣ and t ∈ TΔ. As usual, we call two STSG equivalent if they compute the same weighted tree transformation. We observe that every STSG is essentially a linear, nondeleting weighted extended top-down (or bottom-up) tree transducer (Arnold and Dauchet, 1982; Graehl et al., 2008; Engelfriet et al., 2009) without (bothsided) epsilon rules, and vice versa. Example 1. Let us consider the STSG G over E = A = {σ, α} and Q = {e, o} where qS = o, rk(σ) = 2, and rk(α) = 0. The STSG G consists of the following rules where V = {(1, 2), (2, 1)} and id = {(1,1), (2, 2)}: o → (σ(o, e), σ(e, o), V,1/3) (ρ1) o → (σ(e, o), σ(o, e), V,1/6) (ρ2) o → (σ(e, o), σ(e, o), id, 1/6) (ρ3) o → (α, α, ∅,1/3) (ρ4) e → (σ(e, e), σ(e, e), V,1/2) (ρ5) e → (σ(o, o), σ(o, o), V,1/2) (ρ6) Figure 1 shows a derivation induced by G. It can easily be checked that τq(s, t) = 1 6,3,2,3,3 where s = σ(σ(α, α</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training tree transducers. Computational Linguistics, 34(3):391–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hebisch</author>
<author>Hanns J Weinert</author>
</authors>
<date>1998</date>
<booktitle>Semirings — Algebraic Theory and Applications in Computer Science. World Scientific.</booktitle>
<contexts>
<context position="7672" citStr="Hebisch and Weinert, 1998" startWordPosition="1250" endWordPosition="1253">output trees. Since STSG are symmetric (i.e., input and output can be exchanged), the results for backward application can be obtained easily from the results for forward application. Our main result is that forward application preserves recognizability if the STSG !9 is outputproductive, which means that each rule of !9 contains at least one output symbol that is not a state. Dually, backward application preserves recognizability if !9 is input-productive, which is the analogous property for the input side. In fact, those results hold for weights taken from an arbitrary commutative semiring (Hebisch and Weinert, 1998; Golan, 1999), but we present the results only for probabilities. 2 Preliminary definitions In this contribution we will work with ranked trees. Each symbol that occurs in such a tree has a fixed rank that determines the number of children of nodes with this label. Formally, let E be a ranked alphabet, which is a finite set E together with a mapping rkE : E —* N that associates a rank rkE(a) with every a E E. We let Ek = {Q E E |rkE(a) = k} be the set containing all symbols in E that have rank k. A E-tree indexed by a set Q is a tree with nodes labeled by elements of E U Q, where the nodes la</context>
</contexts>
<marker>Hebisch, Weinert, 1998</marker>
<rawString>Udo Hebisch and Hanns J. Weinert. 1998. Semirings — Algebraic Theory and Applications in Computer Science. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. 9th IWPT,</booktitle>
<pages>53--64</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3172" citStr="Huang and Chiang, 2005" startWordPosition="490" endWordPosition="493">erhof and Satta, 2006) for natural languages can (partially) achieve the disambiguation. Such a parser returns a set of parse trees for each input sentence, and in addition, each returned parse tree is assigned a likelihood. Thus, the result can be seen as a mapping from parse trees to probabilities where the impossible parses are assigned the probability 0. Such mappings are called weighted tree languages, of which some can be finitely represented by weighted regular tree grammars (Alexandrakis and Bozapalidis, 1987). Those weighted tree languages are recognizable and there exist algorithms (Huang and Chiang, 2005) that efficiently extract the k-best parse trees (i.e., those with the highest probability) for further processing. In this paper we consider synchronized tree substitution grammars (STSG). To overcome a technical difficulty we add (grammar) nonterminals to them. Since an STSG often uses the nonterminals of a context-free grammar as terminal symbols (i.e., its derived trees contain both terminal and nonterminal symbols of the context-free grammar), we call the newly added (grammar) nonterminals of the STSG states. Substitution does no longer take place at synchronized nonterminals (of the cont</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. 9th IWPT, pages 53–64. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>An overview of probabilistic tree transducers for natural language processing.</title>
<date>2005</date>
<booktitle>In Proc. 6th CICLing,</booktitle>
<volume>3406</volume>
<pages>1--24</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1609" citStr="Knight and Graehl, 2005" startWordPosition="239" endWordPosition="242">guage processing (NLP). In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can b</context>
</contexts>
<marker>Knight, Graehl, 2005</marker>
<rawString>Kevin Knight and Jonathan Graehl. 2005. An overview of probabilistic tree transducers for natural language processing. In Proc. 6th CICLing, volume 3406 of LNCS, pages 1–24. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL 2003,</booktitle>
<pages>48--54</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1226" citStr="Koehn et al., 2003" startWordPosition="181" endWordPosition="184">gnizability of weighted tree languages in all reasonable cases. As a consequence, both the domain and the range of an STSG without chain rules are recognizable weighted tree languages. 1 Introduction The syntax-based approach to statistical machine translation (Yamada and Knight, 2001) becomes more and more competitive in machine translation, which is a subfield of natural language processing (NLP). In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL 2003, pages 48–54. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M Lewis</author>
<author>Richard Edwin Stearns</author>
</authors>
<title>Syntax-directed transductions.</title>
<date>1968</date>
<journal>J. ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>488</pages>
<marker>Lewis, Stearns, 1968</marker>
<rawString>Philip M. Lewis II and Richard Edwin Stearns. 1968. Syntax-directed transductions. J. ACM, 15(3):465– 488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Lilin</author>
</authors>
<title>Propri´et´es de clˆoture d’une extension de transducteurs d’arbres d´eterministes.</title>
<date>1981</date>
<booktitle>In Proc. 6th CAAP,</booktitle>
<volume>112</volume>
<pages>280--289</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1716" citStr="Lilin, 1981" startWordPosition="258" endWordPosition="259">n model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is t</context>
</contexts>
<marker>Lilin, 1981</marker>
<rawString>Eric Lilin. 1981. Propri´et´es de clˆoture d’une extension de transducteurs d’arbres d´eterministes. In Proc. 6th CAAP, volume 112 of LNCS, pages 280–289. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
<author>Jonathan Graehl</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
</authors>
<title>The power of extended top-down tree transducers.</title>
<date>2009</date>
<journal>SIAMJ. Comput.,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1653" citStr="Maletti et al., 2009" startWordPosition="247" endWordPosition="250">ull parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (</context>
</contexts>
<marker>Maletti, Graehl, Hopkins, Knight, 2009</marker>
<rawString>Andreas Maletti, Jonathan Graehl, Mark Hopkins, and Kevin Knight. 2009. The power of extended top-down tree transducers. SIAMJ. Comput., 39(2):410–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Why synchronous tree substitution grammars?</title>
<date>2010</date>
<booktitle>In Proc. HLT-NAACL 2010. ACL.</booktitle>
<note>to appear.</note>
<contexts>
<context position="1757" citStr="Maletti, 2010" startWordPosition="265" endWordPosition="266">on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural </context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010. Why synchronous tree substitution grammars? In Proc. HLT-NAACL 2010. ACL. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David F Martin</author>
<author>Steven A Vere</author>
</authors>
<title>On syntaxdirected transduction and tree transducers.</title>
<date>1970</date>
<booktitle>In Proc. 2nd STOC,</booktitle>
<pages>129--135</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2182" citStr="Martin and Vere, 1970" startWordPosition="328" endWordPosition="331">982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural languages. For instance, the sentence “I saw the man with the telescope” has several different meanings. Some of them can be distinguished by the parse tree, so that probabilistic parsers (Nederhof and Satta, 2006) for natural languages can (partially) achieve the disambiguation. Such a parser returns a set of parse trees for each input sentence, and in addition, each returned parse tree is assigned a likelihood. Thus, th</context>
</contexts>
<marker>Martin, Vere, 1970</marker>
<rawString>David F. Martin and Steven A. Vere. 1970. On syntaxdirected transduction and tree transducers. In Proc. 2nd STOC, pages 129–135. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>Probabilistic parsing strategies.</title>
<date>2006</date>
<journal>J. ACM,</journal>
<volume>53</volume>
<issue>3</issue>
<contexts>
<context position="2571" citStr="Nederhof and Satta, 2006" startWordPosition="395" endWordPosition="398">titution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural languages. For instance, the sentence “I saw the man with the telescope” has several different meanings. Some of them can be distinguished by the parse tree, so that probabilistic parsers (Nederhof and Satta, 2006) for natural languages can (partially) achieve the disambiguation. Such a parser returns a set of parse trees for each input sentence, and in addition, each returned parse tree is assigned a likelihood. Thus, the result can be seen as a mapping from parse trees to probabilities where the impossible parses are assigned the probability 0. Such mappings are called weighted tree languages, of which some can be finitely represented by weighted regular tree grammars (Alexandrakis and Bozapalidis, 1987). Those weighted tree languages are recognizable and there exist algorithms (Huang and Chiang, 2005</context>
</contexts>
<marker>Nederhof, Satta, 2006</marker>
<rawString>Mark-Jan Nederhof and Giorgio Satta. 2006. Probabilistic parsing strategies. J. ACM, 53(3):406–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Rounds</author>
</authors>
<title>Mappings and grammars on trees.</title>
<date>1970</date>
<booktitle>Math. Systems Theory,</booktitle>
<pages>4--3</pages>
<contexts>
<context position="1481" citStr="Rounds, 1970" startWordPosition="221" endWordPosition="222">Yamada and Knight, 2001) becomes more and more competitive in machine translation, which is a subfield of natural language processing (NLP). In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and S</context>
</contexts>
<marker>Rounds, 1970</marker>
<rawString>William C. Rounds. 1970. Mappings and grammars on trees. Math. Systems Theory, 4(3):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and computational aspects of lexicalized grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1987" citStr="Schabes (1990)" startWordPosition="299" endWordPosition="300">ducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural languages. For instance, the sentence “I saw the man with the telescope” has several different meanings. Some of them can be distinguished by the parse tree, so that probabilistic parsers (Nederhof and Satta, 2006) for natural lan</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and computational aspects of lexicalized grammars. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree-adjoining grammars.</title>
<date>1990</date>
<booktitle>In Proc. 13th CoLing,</booktitle>
<pages>253--258</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2094" citStr="Shieber and Schabes (1990)" startWordPosition="314" endWordPosition="317">(Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural languages. For instance, the sentence “I saw the man with the telescope” has several different meanings. Some of them can be distinguished by the parse tree, so that probabilistic parsers (Nederhof and Satta, 2006) for natural languages can (partially) achieve the disambiguation. Such a parser returns a set of parse trees for each inpu</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart M. Shieber and Yves Schabes. 1990. Synchronous tree-adjoining grammars. In Proc. 13th CoLing, pages 253–258. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Synchronous grammars as tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. TAG+7,</booktitle>
<pages>88--95</pages>
<publisher>Simon Fraser University.</publisher>
<contexts>
<context position="2235" citStr="Shieber, 2004" startWordPosition="339" endWordPosition="340">al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural languages. For instance, the sentence “I saw the man with the telescope” has several different meanings. Some of them can be distinguished by the parse tree, so that probabilistic parsers (Nederhof and Satta, 2006) for natural languages can (partially) achieve the disambiguation. Such a parser returns a set of parse trees for each input sentence, and in addition, each returned parse tree is assigned a likelihood. Thus, the result can be seen as a mapping from parse trees to</context>
<context position="6050" citStr="Shieber (2004)" startWordPosition="986" endWordPosition="987">ected tree pair (�, O, we can apply the rule q —* (s, t, V, a) to each pair of synchronized states q. Such an application replaces the selected state q in � by s and the corresponding state q in ( by t. All the remaining synchronized states and the synchronized states of V remain synchronized. The result is a new connected tree pair. This step charges the weight a. The weights of successive applications (or steps) are multiplied to obtain the weight of the derivation. The weighted tree transformation Tq assigns to each pair of trees the sum of all weights of derivations that derive that pair. Shieber (2004) showed that for every classical unweighted STSG there exists an equivalent bimorphism (Arnold and Dauchet, 1982). The converse result only holds up to deterministic relabelings (G´ecseg and Steinby, 1984; G´ecseg and Steinby, 1997), which remove the state information from the input and output tree. It is this difference that motivates us to add states to STSG. We generalize the result of Shieber (2004) and prove that every weighted tree transformation that is computable by an STSG can also be computed by a weighted bimorphism and vice versa. Given an STSG and a recognizable weighted tree lang</context>
<context position="20307" citStr="Shieber (2004)" startWordPosition="3965" endWordPosition="3966">th q ∈ Q and a ∈ R≥o, there exists a derivation D ∈ D�G(hin(d), hout(d), ∅, a). 3. The mentioned correspondence on derivations is a bijection. Given an STSG G, we can easily construct a weighted bimorphism B such that G and B are related, and vice versa. Hence, STSG and weighted σ σ σ α α α σ α σ σ α α α α α weight: 0.6 weight: 0.24 weight: 0.096 homomorphism e h: Tr → TE, which is defined by ed1,...,uk ← edkl h(γ(d1, ... , dk)) = s[u1 ← Tr ϕ)C R&gt;o (hin, hout) τB Tr × To 5 bimorphisms are equally expressive, which generalizes the corresponding characterization result in the unweighted case by Shieber (2004), which we will state after the introduction of STSG↓. Classical synchronous tree substitution grammars (STSG↓) do not have states. An STSG↓ can be seen as an STSG by considering every substitution site (i.e., each pair of synchronised nonterminals) as a state.2 We illustrate this by means of an example here. Let us consider the STSG↓ G with the following rules: • (S(α, B↓), S(D↓, β)) with weight 0.2 • (B(γ, B↓), D(δ, D↓)) with weight 0.3 • (B(α), D(β)) with weight 0.4. The substitution sites are marked with ↓. Any rule with root A can be applied to a substitution site A↓. An equivalent STSG G</context>
</contexts>
<marker>Shieber, 2004</marker>
<rawString>Stuart M. Shieber. 2004. Synchronous grammars as tree transducers. In Proc. TAG+7, pages 88–95. Simon Fraser University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Unifying synchronous tree adjoining grammars and tree transducers via bimorphisms. In</title>
<date>2006</date>
<booktitle>Proc. 11th EACL,</booktitle>
<pages>377--384</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2267" citStr="Shieber, 2006" startWordPosition="345" endWordPosition="346">, and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). The first bridge between those two classes were established in (Martin and Vere, 1970). Further comparisons can be found in (Shieber, 2004) for STSG and in (Shieber, 2006) for STAG. One of the main challenges in NLP is the ambiguity that is inherent in natural languages. For instance, the sentence “I saw the man with the telescope” has several different meanings. Some of them can be distinguished by the parse tree, so that probabilistic parsers (Nederhof and Satta, 2006) for natural languages can (partially) achieve the disambiguation. Such a parser returns a set of parse trees for each input sentence, and in addition, each returned parse tree is assigned a likelihood. Thus, the result can be seen as a mapping from parse trees to probabilities where the impossi</context>
</contexts>
<marker>Shieber, 2006</marker>
<rawString>Stuart M. Shieber. 2006. Unifying synchronous tree adjoining grammars and tree transducers via bimorphisms. In Proc. 11th EACL, pages 377–384. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Thatcher</author>
</authors>
<title>Generalized sequential machine maps.</title>
<date>1970</date>
<journal>J. Comput. System Sci.,</journal>
<volume>4</volume>
<issue>4</issue>
<pages>367</pages>
<contexts>
<context position="1498" citStr="Thatcher, 1970" startWordPosition="223" endWordPosition="224">ght, 2001) becomes more and more competitive in machine translation, which is a subfield of natural language processing (NLP). In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, 1970), the extended top-down tree transducer (Arnold and Dauchet, 1982; Galley et al., 2004; Knight and Graehl, 2005; Graehl et al., 2008; Maletti et al., 2009), and the extended multi bottom-up tree transducer (Lilin, 1981; Engelfriet et al., 2009; Maletti, 2010). The latter class contains the syntax-directed transductions of Lewis II and Stearns (1968), the generalized syntax-directed transductions (Aho and Ullman, 1969), the synchronous tree substitution grammar (STSG) by Schabes (1990) and the synchronous tree adjoining grammar (STAG) by Abeill´e et al. (1990) and Shieber and Schabes (1990). Th</context>
</contexts>
<marker>Thatcher, 1970</marker>
<rawString>James W. Thatcher. 1970. Generalized sequential machine maps. J. Comput. System Sci., 4(4):339– 367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. 39th ACL,</booktitle>
<pages>523--530</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="893" citStr="Yamada and Knight, 2001" startWordPosition="125" endWordPosition="128">chnische Universit¨at Dresden Szeged, Hungary Tarragona, Spain Dresden, Germany Abstract We consider synchronous tree substitution grammars (STSG). With the help of a characterization of the expressive power of STSG in terms of weighted tree bimorphisms, we show that both the forward and the backward application of an STSG preserve recognizability of weighted tree languages in all reasonable cases. As a consequence, both the domain and the range of an STSG without chain rules are recognizable weighted tree languages. 1 Introduction The syntax-based approach to statistical machine translation (Yamada and Knight, 2001) becomes more and more competitive in machine translation, which is a subfield of natural language processing (NLP). In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure. In the competing phrase-based approach (Koehn et al., 2003) the translation model only has access to the linear sentence structure. There are two major classes of syntax-based translation models: tree transducers and synchronous grammars. Examples in the former class are the top-down tree transducer (Rounds, 1970; Thatcher, </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proc. 39th ACL, pages 523–530. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>