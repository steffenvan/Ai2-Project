<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000623">
<title confidence="0.971617">
Are Semantically Coherent Topic Models Useful for Ad Hoc Information
Retrieval?
</title>
<author confidence="0.999419">
Romain Deveaud Eric SanJuan
</author>
<affiliation confidence="0.8303935">
University of Avignon - LIA
Avignon, France
</affiliation>
<keyword confidence="0.2220835">
romain.deveaud@univ-avignon.fr
eric.sanjuan@univ-avignon.fr
</keyword>
<sectionHeader confidence="0.981102" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966066666667">
The current topic modeling approaches for
Information Retrieval do not allow to ex-
plicitly model query-oriented latent top-
ics. More, the semantic coherence of the
topics has never been considered in this
field. We propose a model-based feedback
approach that learns Latent Dirichlet Al-
location topic models on the top-ranked
pseudo-relevant feedback, and we mea-
sure the semantic coherence of those top-
ics. We perform a first experimental eval-
uation using two major TREC test collec-
tions. Results show that retrieval perfor-
mances tend to be better when using topics
with higher semantic coherence.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997622695652174">
Representing documents as mixtures of “topics”
has always been a challenge and an objective for
researchers working in text-related fields. Based
on the words used within a document, topic mod-
els learn topic level relations by assuming that the
document covers a small set of concepts. Learn-
ing the topics from a document collection can help
to extract high level semantic information, and
help humans to understand the meaning of doc-
uments. Latent Semantic Indexing (Deerwester
et al., 1990) (LSI), probabilistic Latent Seman-
tic Analysis (Hofmann, 2001) (pLSA) and Latent
Dirichlet Allocation (Blei et al., 2003) (LDA) are
the most famous approaches that tried to tackle
this problem throughout the years. Topics pro-
duced by these methods are generally fancy and
appealing, and often correlate well with human
concepts. This is one of the reasons of the inten-
sive use of topic models (and especially LDA) in
current research in Natural Language Processing
(NLP) related areas.
One main problem in ad hoc Information Re-
trieval (IR) is the difficulty for users to translate a
</bodyText>
<author confidence="0.593747">
Patrice Bellot
</author>
<affiliation confidence="0.670794">
Aix-Marseille University - LSIS
</affiliation>
<address confidence="0.63423">
Marseille, France
</address>
<email confidence="0.939187">
patrice.bellot@lsis.org
</email>
<bodyText confidence="0.999969414634147">
complex information need into a keyword query.
The most popular and effective approach to over-
come this problem is to improve the representa-
tion of the query by adding query-related “con-
cepts”. This approach mostly relies on pseudo-
relevance feedback, where these so-called “con-
cepts” are the most frequent words occurring in the
top-ranked documents retrieved by the retrieval
system (Lavrenko and Croft, 2001). From that
perspective, topic models seem attractive in the
sense that they can provide a descriptive and intu-
itive representation of concepts. But how can we
quantify the usefulness of these topics with respect
to an IR system? Recently, researchers developed
measures which evaluate the semantic coherence
of topic models (Newman et al., 2010; Mimno et
al., 2011; Stevens et al., 2012). We adopt their
view of semantic coherence and apply one of these
measures to query-oriented topics.
Several studies concentrated on improving the
quality of document ranking using topic models,
especially probabilistic ones. The approach by
Wei and Croft (2006) was the first to leverage
LDA topics to improve the estimate of document
language models and achieved good empirical re-
sults. Following this pioneering work, several
studies explored the use of pLSA and LDA un-
der different experimental settings (Park and Ra-
mamohanarao, 2009; Yi and Allan, 2009; Andrze-
jewski and Buttler, 2011; Lu et al., 2011). The re-
ported results suggest that the words and the prob-
ability distributions learned by probabilistic topic
models are effective for query expansion. The
main drawback of these approaches is that topics
are learned on the whole target document collec-
tion prior to retrieval, thus leading to a static top-
ical representation of the collection. Depending
on the query and on its specificity, topics may ei-
ther be too coarse or too fine to accurately rep-
resent the latent concepts of the query. Recently,
Ye et al. (2011) proposed a method which uses
</bodyText>
<page confidence="0.963148">
148
</page>
<bodyText confidence="0.8742035">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 148–152,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
LDA and learns topics directly on a limited set
of documents. While this approach is a first step
towards modeling query-oriented topics, it lacks
some theoretic principles and only aims to heuris-
tically construct a “best” topic (from all learned
topics) before expanding the query with its most
probable words. More, none of the aforemen-
tioned works studied the semantic coherence of
those generated topics. We tackle these issues by
making the following contributions:
</bodyText>
<listItem confidence="0.997034909090909">
• we introduce Topic-Driven Relevance Mod-
els, a model-based feedback approach (Zhai
and Lafferty, 2001) for integrating topic mod-
els into relevance models by learning topics
on pseudo-relevant feedback documents (as
opposed to the entire document collection),
• we explore the coherence of those generated
topics using the queries of two major and
well-established TREC test collections,
• we evaluate the effects coherent topics have
on ad hoc IR using the same test collections.
</listItem>
<sectionHeader confidence="0.805052" genericHeader="method">
2 Topic-Driven Relevance Models
</sectionHeader>
<subsectionHeader confidence="0.905107">
2.1 Relevance Models
</subsectionHeader>
<bodyText confidence="0.999969083333334">
The goal of relevance models is to improve
the representation of a query Q by selecting
terms from a set of initially retrieved docu-
ments (Lavrenko and Croft, 2001). As the concen-
tration of relevant documents is usually higher in
the top ranks of the ranking list, this is constituted
by a number N of top-ranked documents. Rele-
vance models usually perform better when com-
bined with the original query model (or maxi-
mum likelihood estimate). Let ˜θQ be this maxi-
mum likelihood query estimate and ˆθQ a relevance
model, the updated new query model is given by:
</bodyText>
<equation confidence="0.999728">
P(w|θQ) = λ P(w|˜θQ) + (1 − λ)P(w|ˆθQ) (1)
</equation>
<bodyText confidence="0.999700285714286">
where λ E [0, 1] is a parameter that controls the
tradeoff between the original query model and the
relevance model. One of the most robust variants
of the relevance models is computed as follows:
often referred to as model-based feedback (Zhai
and Lafferty, 2001). We assume P(θD) to be uni-
form, resulting in an estimated relevance model
based on a sum of document models weighted
by the query likelihood score. The final, inter-
polated, estimate expressed in equation (1) is of-
ten referred in the literature as RM3. We tackle
the null probabilities problem by smoothing the
document language model using the well-known
Dirichlet smoothing (Zhai and Lafferty, 2004).
</bodyText>
<subsectionHeader confidence="0.997968">
2.2 LDA-based Feedback Model
</subsectionHeader>
<bodyText confidence="0.999983096774194">
The estimation of the feedback model ˆθQ consti-
tutes the first contribution of this work. We pro-
pose to explicitly model the latent topics (or con-
cepts) that exist behind an information need, and
to use them to improve the query representation.
We consider O as the set of pseudo-relevant feed-
back documents from which the latent concepts
would be extracted. The retrieval algorithm used
to obtain these documents can be of any kind, the
important point is that O is a reduced collection
that contains the top documents ranked by an au-
tomatic and state-of-the-art retrieval process.
Instead of viewing O as a set of document lan-
guage models that are likely to contain topical in-
formation about the query, we take a probabilistic
topic modeling approach. We specifically focus
on Latent Dirichlet Allocation (LDA), since it is
currently one of the most representative. In LDA,
each topic multinomial distribution φk is gener-
ated by a conjugate Dirichlet prior with parame-
ter β, while each document multinomial distribu-
tion θd is generated by a conjugate Dirichlet prior
with parameter α. In other words, θd,k is the prob-
ability of topic k occurring in document D (i.e.
P(k|D)). Respectively, φk,w is the probability of
word w belonging to topic k (i.e. P(w|k)). We use
variational inference implemented in the LDA-C
software1 to overcome intractability issues (Blei et
al., 2003; Griffiths and Steyvers, 2004). Under this
setting, we compute the topic-driven estimation of
the query model using the following equation:
</bodyText>
<equation confidence="0.9785526">
P(w |ˆθQ) a X ( P(θD)P(w|θD)
BDEΘ
P(w |X Y P(t|θD) Y �
ˆθQ) a P(θD)P(w|θD) PTM(w|D) P(t|θD) (3)
BDEΘ tEQ tEQ
</equation>
<bodyText confidence="0.991285666666667">
(2)
where O is a set of pseudo-relevant feedback doc-
uments and θD is the language model of document
D. This notion of estimating a query model is
where PTM(w|D) is the probability of word w
occurring in document D using the previously
</bodyText>
<footnote confidence="0.973498">
1www.cs.princeton.edu/˜blei/lda-c
</footnote>
<page confidence="0.990403">
149
</page>
<figure confidence="0.995439181818182">
WT10g Robust04
Coherence
Coherence
9.49.6 9.8 10.0 10.2
9.4 9.6 9.8 10.0 10.2
Number of topics
3 5 10 15 20
Number of topics
3 5 10 15 20
5 10 20 30 40 50 5 10 20 30 40 50
Number of feedback documents Number of feedback documents
</figure>
<figureCaption confidence="0.958746">
Figure 1: Semantic coherence of the topic models for different values of K, in function of the number
N of feedback documents.
</figureCaption>
<bodyText confidence="0.747572333333333">
learned multinomial distributions. Let To be a
topic model learned on the O set of feedback doc-
uments, this probability is given by:
</bodyText>
<equation confidence="0.995589">
PTM(w|D) = � Ok,w · BD,k (4)
kETΘ
</equation>
<bodyText confidence="0.9612842">
High probabilities are thus given to words that are
important in topic k, when k is an important topic
in document D. In the remainder of this paper, we
refer to this general approach as TDRM for Topic-
Driven Relevance Models.
</bodyText>
<subsectionHeader confidence="0.9883855">
2.3 Measuring the coherence of
query-oriented topics
</subsectionHeader>
<bodyText confidence="0.999337761904762">
TDRM relies on two important parameters: the
number of topics K that we want to learn, and
the number of feedback documents N from which
LDA learns the topics. Varying these two param-
eters can help to capture more information and to
model finer topics, but how about their global se-
mantic coherence?
Term similarities measured in restricted do-
mains was the first step for evaluating seman-
tic coherence (Gliozzo et al., 2007), and was a
first basis for the development of several topic
coherence evaluation measures (Newman et al.,
2010). Computing the Pointwise Mutual Informa-
tion (PMI) of all word pairs over Wikipedia was
found to be an effective metric using news and
books corpora. Recently, Stevens et al. (2012)
used (among others) an aggregate version of this
metric to evaluate large amounts of topic models.
We use this method to evaluate the coherence of
query-oriented topics. Specifically, the coherence
of a topic model ToK composed of K topics is:
</bodyText>
<equation confidence="0.9996615">
P(w, w&apos;) + c
log P(w)P(wl) (5)
</equation>
<bodyText confidence="0.9997558">
where probabilities of word occurrences and co-
occurrences are estimated using an external refer-
ence corpus. Following Newman et al. (2010), we
use Wikipedia to compute PMI and set c = 1 as
in (Stevens et al., 2012).
</bodyText>
<sectionHeader confidence="0.999724" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999299">
3.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999995">
We performed our evaluation using two main
TREC2 collections: Robust04 and WT10g. Ro-
bust04 is composed 528,155 of news articles com-
ing from three newspapers and the FBIS. It sup-
ported the TREC 2004 Robust track, from which
we used the 250 query topics (numbers: 301-450,
601-700). The WT10g collection is composed of
1,692,096 web pages, and supported the TREC
Web track for four years (2001-2004). We focus
on the 2000 and 2001 ad-hoc query topics (num-
bers: 451-550). We used the open-source index-
ing and retrieval system Indri3 to run our exper-
iments. We indexed the two collections with the
exact same parameters: tokens were stemmed with
the well-known light Krovetz stemmer and stop-
words were removed using the standard English
stoplist embedded with Indri (417 words).
</bodyText>
<subsectionHeader confidence="0.999496">
3.2 Semantic coherence evaluation
</subsectionHeader>
<bodyText confidence="0.9584995">
Most coherent topics are composed of rare words
that do not often occur in the reference corpus, but
</bodyText>
<figure confidence="0.9478533125">
2trec.nist.gov
3lemurproject.org/indri.php
1 K E
c(ToK) = K (w,w&apos;)Eki
Z=1
150
WT10g Robust04
Number of topics
● 3
● ● 5
●
10
15
20
● RM3
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Number of topics
● 3
● 5
10
15
20
● ● RM3
●
MAP
0.260 0.265 0.270 0.275 0.280 0.285 0.290
MAP
0.200 0.205 0.210 0.215 0.220
5 10 20 30 40 50
Number of feedback documents
5 10 20 30 40 50
Number of feedback documents
</figure>
<figureCaption confidence="0.998172">
Figure 2: Retrieval performance in terms of Mean Average Precision (MAP) of the TDRM approach.
</figureCaption>
<bodyText confidence="0.997661321428571">
Each line represent a different number of topics K, and the performance are reported in function the
number N of feedback documents. The black, plain line represents the RM3 baseline.
co-occur at lot together. We see on Figure 1 that
very coherent topics are identified in the top 5 and
10 feedback documents for the WT10g collection,
suggesting that closely related documents are re-
trieved in the top ranks. Results are quite different
on the Robust04 collection, where topic models
with 20 topics on 5 documents are the least co-
herent. However, when looking at the Robust04
documents, we see that they are on average almost
twice smaller than the WT10g web pages. We hy-
pothesize that the heterogeneous nature of the web
allows to model very different topics covering sev-
eral aspects of the query, while news articles are
contributions focused on a single subject.
Overall, the more coherent topic models contain
a reasonable amount of topics (10-15), thus allow-
ing to fit with variable amounts of documents. The
attentive reader will notice that the topic coher-
ence scores are very high compared to those pre-
viously reported in the literature (Stevens et al.,
2012). The TDRM approach captures topics that
are centered around a specific information need,
often with a limited vocabulary, which favors word
co-occurrence. On the other hand, topics learned
on entire collections are coarser than ours, which
leads to lower coherence scores.
</bodyText>
<subsectionHeader confidence="0.999693">
3.3 Document retrieval results
</subsectionHeader>
<bodyText confidence="0.99986603030303">
Since TDRM is based on Relevance Mod-
els (Lavrenko and Croft, 2001), we take the RM3
approach presented in Section 2.1 as baseline. The
λ parameter is common between RM3 and TDRM
and is determined for each query using leave-
one-query-out cross-validation (that is: learn the
best parameter setting for all queries but one, and
evaluate the held-out query using the previously
learned parameter).
We report ad hoc document retrieval perfor-
mances in Figure 2. We noticed in the previous
section that the most coherent topic models were
modeled using 5 feedback documents and 20 top-
ics for the WT10g collection, and this parame-
ter combination also achieves the best retrieval re-
sults. Overall, using 10, 15 or 20 topics allow it
to achieve high and similar performance from 5 to
20 documents. We observe than using 20 topics
for the Robust04 collection consistently achieves
the highest results, with the topic model coherence
growing as the number of feedback documents in-
creases. Although topics coming from news ar-
ticles may be limited, they benefit from the rich
vocabulary of professional writers who are trained
to avoid repetition. Their use of synonyms allows
TDRM to model deep topics, with a comprehen-
sive description of query aspects. Since synonyms
are less likely to co-occur in encyclopedic articles
like Wikipedia, we think that, in our case, the se-
mantic coherence measure could be more accurate
using other textual resources. This measure seems
however to be effective when dealing with hetero-
geneously structured documents.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="conclusions">
4 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.99968">
Overall, modeling query-oriented topic models
and estimating the feedback query model using
these topics greatly improves ad hoc Information
Retrieval, compared to state-of-the-art relevance
models. While semantically coherent topic mod-
</bodyText>
<page confidence="0.995263">
151
</page>
<bodyText confidence="0.999561166666667">
els do not seem to be effective in the context of a
news articles search task, they are a good indica-
tor of effectiveness in the context of web search.
Measuring the semantic coherence of query top-
ics could help predict query effectiveness or even
choose the best query-representative topic model.
</bodyText>
<sectionHeader confidence="0.997391" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98974325">
This work was supported by the French Agency
for Scientific Research (Agence Nationale de
la Recherche) under CAAS project (ANR 2010
CORD 001 02).
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999014">
David Andrzejewski and David Buttler. 2011. Latent
Topic Feedback for Information Retrieval. In Pro-
ceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ’11, pages 600–608.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.
Alfio Massimiliano Gliozzo, Marco Pennacchiotti, and
Patrick Pantel. 2007. The Domain Restriction Hy-
pothesis: Relating Term Similarity and Semantic
Consistency. In Human Language Technologies:
The 2007 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 131–138.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101 Suppl.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42:177–196.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-Based Language Models. In Proceedings
of the 24th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’01, pages 120–127.
Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011.
Investigating task performance of probabilistic topic
models: an empirical study of PLSA and LDA. In-
formation Retrieval, 14:178–203.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing Semantic Coherence in Topic Models.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 262–272.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic Evaluation of
Topic Coherence. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 100–108.
Laurence A. Park and Kotagiri Ramamohanarao. 2009.
The Sensitivity of Latent Dirichlet Allocation for In-
formation Retrieval. In Proceedings of the Euro-
pean Conference on Machine Learning and Knowl-
edge Discovery in Databases, ECML PKDD ’09,
pages 176–188.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Buttler. 2012. Exploring Topic
Coherence over Many Models and Many Topics.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 952–961.
Xing Wei and W. Bruce Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’06, pages 178–185.
Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin.
2011. Finding a Good Query-Related Topic for
Boosting Pseudo-Relevance Feedback. JASIST,
62(4):748–760.
Xing Yi and James Allan. 2009. A Comparative Study
of Utilizing Topic Models for Information Retrieval.
In Proceedings of the 31th European Conference on
IR Research on Advances in Information Retrieval,
ECIR ’09, pages 29–41. Springer-Verlag.
Chengxiang Zhai and John Lafferty. 2001. Model-
based Feedback in the Language Modeling Ap-
proach to Information Retrieval. In Proceedings
of the Tenth International Conference on Informa-
tion and Knowledge Management, CIKM ’01, pages
403–410.
Chengxiang Zhai and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models Applied
to Information Retrieval. ACM Transactions on In-
formation Systems, 22(2):179–214.
</reference>
<page confidence="0.998132">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888077">
<title confidence="0.996976">Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</title>
<author confidence="0.999877">Romain Deveaud Eric SanJuan</author>
<affiliation confidence="0.999863">University of Avignon - LIA</affiliation>
<address confidence="0.95806">Avignon, France</address>
<email confidence="0.9718065">romain.deveaud@univ-avignon.freric.sanjuan@univ-avignon.fr</email>
<abstract confidence="0.999058">The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval performances tend to be better when using topics with higher semantic coherence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>David Buttler</author>
</authors>
<title>Latent Topic Feedback for Information Retrieval.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’11,</booktitle>
<pages>600--608</pages>
<contexts>
<context position="3385" citStr="Andrzejewski and Buttler, 2011" startWordPosition="517" endWordPosition="521">imno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al., 2011). The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection. Depending on the query and on its specificity, topics may either be too coarse or too fine to accurately represent the latent concepts of the query. Recently, Ye et al. (2011) proposed a method which uses 148 Proceedings of the 51st Annu</context>
</contexts>
<marker>Andrzejewski, Buttler, 2011</marker>
<rawString>David Andrzejewski and David Buttler. 2011. Latent Topic Feedback for Information Retrieval. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’11, pages 600–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1446" citStr="Blei et al., 2003" startWordPosition="213" endWordPosition="216">tion Representing documents as mixtures of “topics” has always been a challenge and an objective for researchers working in text-related fields. Based on the words used within a document, topic models learn topic level relations by assuming that the document covers a small set of concepts. Learning the topics from a document collection can help to extract high level semantic information, and help humans to understand the meaning of documents. Latent Semantic Indexing (Deerwester et al., 1990) (LSI), probabilistic Latent Semantic Analysis (Hofmann, 2001) (pLSA) and Latent Dirichlet Allocation (Blei et al., 2003) (LDA) are the most famous approaches that tried to tackle this problem throughout the years. Topics produced by these methods are generally fancy and appealing, and often correlate well with human concepts. This is one of the reasons of the intensive use of topic models (and especially LDA) in current research in Natural Language Processing (NLP) related areas. One main problem in ad hoc Information Retrieval (IR) is the difficulty for users to translate a Patrice Bellot Aix-Marseille University - LSIS Marseille, France patrice.bellot@lsis.org complex information need into a keyword query. Th</context>
<context position="7825" citStr="Blei et al., 2003" startWordPosition="1245" endWordPosition="1248">cifically focus on Latent Dirichlet Allocation (LDA), since it is currently one of the most representative. In LDA, each topic multinomial distribution φk is generated by a conjugate Dirichlet prior with parameter β, while each document multinomial distribution θd is generated by a conjugate Dirichlet prior with parameter α. In other words, θd,k is the probability of topic k occurring in document D (i.e. P(k|D)). Respectively, φk,w is the probability of word w belonging to topic k (i.e. P(w|k)). We use variational inference implemented in the LDA-C software1 to overcome intractability issues (Blei et al., 2003; Griffiths and Steyvers, 2004). Under this setting, we compute the topic-driven estimation of the query model using the following equation: P(w |ˆθQ) a X ( P(θD)P(w|θD) BDEΘ P(w |X Y P(t|θD) Y � ˆθQ) a P(θD)P(w|θD) PTM(w|D) P(t|θD) (3) BDEΘ tEQ tEQ (2) where O is a set of pseudo-relevant feedback documents and θD is the language model of document D. This notion of estimating a query model is where PTM(w|D) is the probability of word w occurring in document D using the previously 1www.cs.princeton.edu/˜blei/lda-c 149 WT10g Robust04 Coherence Coherence 9.49.6 9.8 10.0 10.2 9.4 9.6 9.8 10.0 10.2</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1325" citStr="Deerwester et al., 1990" startWordPosition="196" endWordPosition="199">ctions. Results show that retrieval performances tend to be better when using topics with higher semantic coherence. 1 Introduction Representing documents as mixtures of “topics” has always been a challenge and an objective for researchers working in text-related fields. Based on the words used within a document, topic models learn topic level relations by assuming that the document covers a small set of concepts. Learning the topics from a document collection can help to extract high level semantic information, and help humans to understand the meaning of documents. Latent Semantic Indexing (Deerwester et al., 1990) (LSI), probabilistic Latent Semantic Analysis (Hofmann, 2001) (pLSA) and Latent Dirichlet Allocation (Blei et al., 2003) (LDA) are the most famous approaches that tried to tackle this problem throughout the years. Topics produced by these methods are generally fancy and appealing, and often correlate well with human concepts. This is one of the reasons of the intensive use of topic models (and especially LDA) in current research in Natural Language Processing (NLP) related areas. One main problem in ad hoc Information Retrieval (IR) is the difficulty for users to translate a Patrice Bellot Ai</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Massimiliano Gliozzo</author>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>The Domain Restriction Hypothesis: Relating Term Similarity and Semantic Consistency. In Human Language Technologies: The</title>
<date>2007</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>131--138</pages>
<contexts>
<context position="9575" citStr="Gliozzo et al., 2007" startWordPosition="1555" endWordPosition="1558">s an important topic in document D. In the remainder of this paper, we refer to this general approach as TDRM for TopicDriven Relevance Models. 2.3 Measuring the coherence of query-oriented topics TDRM relies on two important parameters: the number of topics K that we want to learn, and the number of feedback documents N from which LDA learns the topics. Varying these two parameters can help to capture more information and to model finer topics, but how about their global semantic coherence? Term similarities measured in restricted domains was the first step for evaluating semantic coherence (Gliozzo et al., 2007), and was a first basis for the development of several topic coherence evaluation measures (Newman et al., 2010). Computing the Pointwise Mutual Information (PMI) of all word pairs over Wikipedia was found to be an effective metric using news and books corpora. Recently, Stevens et al. (2012) used (among others) an aggregate version of this metric to evaluate large amounts of topic models. We use this method to evaluate the coherence of query-oriented topics. Specifically, the coherence of a topic model ToK composed of K topics is: P(w, w&apos;) + c log P(w)P(wl) (5) where probabilities of word occ</context>
</contexts>
<marker>Gliozzo, Pennacchiotti, Pantel, 2007</marker>
<rawString>Alfio Massimiliano Gliozzo, Marco Pennacchiotti, and Patrick Pantel. 2007. The Domain Restriction Hypothesis: Relating Term Similarity and Semantic Consistency. In Human Language Technologies: The 2007 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, 101 Suppl.</booktitle>
<contexts>
<context position="7856" citStr="Griffiths and Steyvers, 2004" startWordPosition="1249" endWordPosition="1252">Latent Dirichlet Allocation (LDA), since it is currently one of the most representative. In LDA, each topic multinomial distribution φk is generated by a conjugate Dirichlet prior with parameter β, while each document multinomial distribution θd is generated by a conjugate Dirichlet prior with parameter α. In other words, θd,k is the probability of topic k occurring in document D (i.e. P(k|D)). Respectively, φk,w is the probability of word w belonging to topic k (i.e. P(w|k)). We use variational inference implemented in the LDA-C software1 to overcome intractability issues (Blei et al., 2003; Griffiths and Steyvers, 2004). Under this setting, we compute the topic-driven estimation of the query model using the following equation: P(w |ˆθQ) a X ( P(θD)P(w|θD) BDEΘ P(w |X Y P(t|θD) Y � ˆθQ) a P(θD)P(w|θD) PTM(w|D) P(t|θD) (3) BDEΘ tEQ tEQ (2) where O is a set of pseudo-relevant feedback documents and θD is the language model of document D. This notion of estimating a query model is where PTM(w|D) is the probability of word w occurring in document D using the previously 1www.cs.princeton.edu/˜blei/lda-c 149 WT10g Robust04 Coherence Coherence 9.49.6 9.8 10.0 10.2 9.4 9.6 9.8 10.0 10.2 Number of topics 3 5 10 15 20 </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101 Suppl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised Learning by Probabilistic Latent Semantic Analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--177</pages>
<contexts>
<context position="1387" citStr="Hofmann, 2001" startWordPosition="206" endWordPosition="207">using topics with higher semantic coherence. 1 Introduction Representing documents as mixtures of “topics” has always been a challenge and an objective for researchers working in text-related fields. Based on the words used within a document, topic models learn topic level relations by assuming that the document covers a small set of concepts. Learning the topics from a document collection can help to extract high level semantic information, and help humans to understand the meaning of documents. Latent Semantic Indexing (Deerwester et al., 1990) (LSI), probabilistic Latent Semantic Analysis (Hofmann, 2001) (pLSA) and Latent Dirichlet Allocation (Blei et al., 2003) (LDA) are the most famous approaches that tried to tackle this problem throughout the years. Topics produced by these methods are generally fancy and appealing, and often correlate well with human concepts. This is one of the reasons of the intensive use of topic models (and especially LDA) in current research in Natural Language Processing (NLP) related areas. One main problem in ad hoc Information Retrieval (IR) is the difficulty for users to translate a Patrice Bellot Aix-Marseille University - LSIS Marseille, France patrice.bellot</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learning, 42:177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance-Based Language Models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="2407" citStr="Lavrenko and Croft, 2001" startWordPosition="364" endWordPosition="367">cessing (NLP) related areas. One main problem in ad hoc Information Retrieval (IR) is the difficulty for users to translate a Patrice Bellot Aix-Marseille University - LSIS Marseille, France patrice.bellot@lsis.org complex information need into a keyword query. The most popular and effective approach to overcome this problem is to improve the representation of the query by adding query-related “concepts”. This approach mostly relies on pseudorelevance feedback, where these so-called “concepts” are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (Lavrenko and Croft, 2001). From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. But how can we quantify the usefulness of these topics with respect to an IR system? Recently, researchers developed measures which evaluate the semantic coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially proba</context>
<context position="5313" citStr="Lavrenko and Croft, 2001" startWordPosition="824" endWordPosition="827">oach (Zhai and Lafferty, 2001) for integrating topic models into relevance models by learning topics on pseudo-relevant feedback documents (as opposed to the entire document collection), • we explore the coherence of those generated topics using the queries of two major and well-established TREC test collections, • we evaluate the effects coherent topics have on ad hoc IR using the same test collections. 2 Topic-Driven Relevance Models 2.1 Relevance Models The goal of relevance models is to improve the representation of a query Q by selecting terms from a set of initially retrieved documents (Lavrenko and Croft, 2001). As the concentration of relevant documents is usually higher in the top ranks of the ranking list, this is constituted by a number N of top-ranked documents. Relevance models usually perform better when combined with the original query model (or maximum likelihood estimate). Let ˜θQ be this maximum likelihood query estimate and ˆθQ a relevance model, the updated new query model is given by: P(w|θQ) = λ P(w|˜θQ) + (1 − λ)P(w|ˆθQ) (1) where λ E [0, 1] is a parameter that controls the tradeoff between the original query model and the relevance model. One of the most robust variants of the relev</context>
<context position="13327" citStr="Lavrenko and Croft, 2001" startWordPosition="2203" endWordPosition="2206"> a reasonable amount of topics (10-15), thus allowing to fit with variable amounts of documents. The attentive reader will notice that the topic coherence scores are very high compared to those previously reported in the literature (Stevens et al., 2012). The TDRM approach captures topics that are centered around a specific information need, often with a limited vocabulary, which favors word co-occurrence. On the other hand, topics learned on entire collections are coarser than ours, which leads to lower coherence scores. 3.3 Document retrieval results Since TDRM is based on Relevance Models (Lavrenko and Croft, 2001), we take the RM3 approach presented in Section 2.1 as baseline. The λ parameter is common between RM3 and TDRM and is determined for each query using leaveone-query-out cross-validation (that is: learn the best parameter setting for all queries but one, and evaluate the held-out query using the previously learned parameter). We report ad hoc document retrieval performances in Figure 2. We noticed in the previous section that the most coherent topic models were modeled using 5 feedback documents and 20 topics for the WT10g collection, and this parameter combination also achieves the best retri</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance-Based Language Models. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval,</title>
<date>2011</date>
<pages>14--178</pages>
<contexts>
<context position="3403" citStr="Lu et al., 2011" startWordPosition="522" endWordPosition="525">., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al., 2011). The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection. Depending on the query and on its specificity, topics may either be too coarse or too fine to accurately represent the latent concepts of the query. Recently, Ye et al. (2011) proposed a method which uses 148 Proceedings of the 51st Annual Meeting of the </context>
</contexts>
<marker>Lu, Mei, Zhai, 2011</marker>
<rawString>Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011. Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval, 14:178–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing Semantic Coherence in Topic Models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>262--272</pages>
<contexts>
<context position="2772" citStr="Mimno et al., 2011" startWordPosition="422" endWordPosition="425"> by adding query-related “concepts”. This approach mostly relies on pseudorelevance feedback, where these so-called “concepts” are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (Lavrenko and Croft, 2001). From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. But how can we quantify the usefulness of these topics with respect to an IR system? Recently, researchers developed measures which evaluate the semantic coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and </context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing Semantic Coherence in Topic Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 262–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic Evaluation of Topic Coherence.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="2752" citStr="Newman et al., 2010" startWordPosition="418" endWordPosition="421">entation of the query by adding query-related “concepts”. This approach mostly relies on pseudorelevance feedback, where these so-called “concepts” are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (Lavrenko and Croft, 2001). From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. But how can we quantify the usefulness of these topics with respect to an IR system? Recently, researchers developed measures which evaluate the semantic coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 200</context>
<context position="9687" citStr="Newman et al., 2010" startWordPosition="1573" endWordPosition="1576"> TopicDriven Relevance Models. 2.3 Measuring the coherence of query-oriented topics TDRM relies on two important parameters: the number of topics K that we want to learn, and the number of feedback documents N from which LDA learns the topics. Varying these two parameters can help to capture more information and to model finer topics, but how about their global semantic coherence? Term similarities measured in restricted domains was the first step for evaluating semantic coherence (Gliozzo et al., 2007), and was a first basis for the development of several topic coherence evaluation measures (Newman et al., 2010). Computing the Pointwise Mutual Information (PMI) of all word pairs over Wikipedia was found to be an effective metric using news and books corpora. Recently, Stevens et al. (2012) used (among others) an aggregate version of this metric to evaluate large amounts of topic models. We use this method to evaluate the coherence of query-oriented topics. Specifically, the coherence of a topic model ToK composed of K topics is: P(w, w&apos;) + c log P(w)P(wl) (5) where probabilities of word occurrences and cooccurrences are estimated using an external reference corpus. Following Newman et al. (2010), we </context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic Evaluation of Topic Coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence A Park</author>
<author>Kotagiri Ramamohanarao</author>
</authors>
<title>The Sensitivity of Latent Dirichlet Allocation for Information Retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD ’09,</booktitle>
<pages>176--188</pages>
<contexts>
<context position="3333" citStr="Park and Ramamohanarao, 2009" startWordPosition="508" endWordPosition="512"> coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al., 2011). The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection. Depending on the query and on its specificity, topics may either be too coarse or too fine to accurately represent the latent concepts of the query. Recently, Ye et al. (2011) proposed </context>
</contexts>
<marker>Park, Ramamohanarao, 2009</marker>
<rawString>Laurence A. Park and Kotagiri Ramamohanarao. 2009. The Sensitivity of Latent Dirichlet Allocation for Information Retrieval. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD ’09, pages 176–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Stevens</author>
<author>Philip Kegelmeyer</author>
<author>David Andrzejewski</author>
<author>David Buttler</author>
</authors>
<title>Exploring Topic Coherence over Many Models and Many Topics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>952--961</pages>
<contexts>
<context position="2795" citStr="Stevens et al., 2012" startWordPosition="426" endWordPosition="429">ated “concepts”. This approach mostly relies on pseudorelevance feedback, where these so-called “concepts” are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (Lavrenko and Croft, 2001). From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. But how can we quantify the usefulness of these topics with respect to an IR system? Recently, researchers developed measures which evaluate the semantic coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al</context>
<context position="9868" citStr="Stevens et al. (2012)" startWordPosition="1603" endWordPosition="1606">umber of feedback documents N from which LDA learns the topics. Varying these two parameters can help to capture more information and to model finer topics, but how about their global semantic coherence? Term similarities measured in restricted domains was the first step for evaluating semantic coherence (Gliozzo et al., 2007), and was a first basis for the development of several topic coherence evaluation measures (Newman et al., 2010). Computing the Pointwise Mutual Information (PMI) of all word pairs over Wikipedia was found to be an effective metric using news and books corpora. Recently, Stevens et al. (2012) used (among others) an aggregate version of this metric to evaluate large amounts of topic models. We use this method to evaluate the coherence of query-oriented topics. Specifically, the coherence of a topic model ToK composed of K topics is: P(w, w&apos;) + c log P(w)P(wl) (5) where probabilities of word occurrences and cooccurrences are estimated using an external reference corpus. Following Newman et al. (2010), we use Wikipedia to compute PMI and set c = 1 as in (Stevens et al., 2012). 3 Evaluation 3.1 Experimental setup We performed our evaluation using two main TREC2 collections: Robust04 a</context>
<context position="12956" citStr="Stevens et al., 2012" startWordPosition="2146" endWordPosition="2149"> when looking at the Robust04 documents, we see that they are on average almost twice smaller than the WT10g web pages. We hypothesize that the heterogeneous nature of the web allows to model very different topics covering several aspects of the query, while news articles are contributions focused on a single subject. Overall, the more coherent topic models contain a reasonable amount of topics (10-15), thus allowing to fit with variable amounts of documents. The attentive reader will notice that the topic coherence scores are very high compared to those previously reported in the literature (Stevens et al., 2012). The TDRM approach captures topics that are centered around a specific information need, often with a limited vocabulary, which favors word co-occurrence. On the other hand, topics learned on entire collections are coarser than ours, which leads to lower coherence scores. 3.3 Document retrieval results Since TDRM is based on Relevance Models (Lavrenko and Croft, 2001), we take the RM3 approach presented in Section 2.1 as baseline. The λ parameter is common between RM3 and TDRM and is determined for each query using leaveone-query-out cross-validation (that is: learn the best parameter setting</context>
</contexts>
<marker>Stevens, Kegelmeyer, Andrzejewski, Buttler, 2012</marker>
<rawString>Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring Topic Coherence over Many Models and Many Topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 952–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Wei</author>
<author>W Bruce Croft</author>
</authors>
<title>LDA-based Document Models for Ad-hoc Retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’06,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="3058" citStr="Wei and Croft (2006)" startWordPosition="465" endWordPosition="468">models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. But how can we quantify the usefulness of these topics with respect to an IR system? Recently, researchers developed measures which evaluate the semantic coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al., 2011). The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. The main drawback of these approaches is that topics are learned on the whole target document collecti</context>
</contexts>
<marker>Wei, Croft, 2006</marker>
<rawString>Xing Wei and W. Bruce Croft. 2006. LDA-based Document Models for Ad-hoc Retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’06, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Ye</author>
<author>Jimmy Xiangji Huang</author>
<author>Hongfei Lin</author>
</authors>
<title>Finding a Good Query-Related Topic for Boosting Pseudo-Relevance Feedback.</title>
<date>2011</date>
<journal>JASIST,</journal>
<volume>62</volume>
<issue>4</issue>
<contexts>
<context position="3923" citStr="Ye et al. (2011)" startWordPosition="610" endWordPosition="613">rk and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al., 2011). The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection. Depending on the query and on its specificity, topics may either be too coarse or too fine to accurately represent the latent concepts of the query. Recently, Ye et al. (2011) proposed a method which uses 148 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 148–152, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics LDA and learns topics directly on a limited set of documents. While this approach is a first step towards modeling query-oriented topics, it lacks some theoretic principles and only aims to heuristically construct a “best” topic (from all learned topics) before expanding the query with its most probable words. More, none of the aforementioned works studied the semantic coheren</context>
</contexts>
<marker>Ye, Huang, Lin, 2011</marker>
<rawString>Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin. 2011. Finding a Good Query-Related Topic for Boosting Pseudo-Relevance Feedback. JASIST, 62(4):748–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Yi</author>
<author>James Allan</author>
</authors>
<title>A Comparative Study of Utilizing Topic Models for Information Retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09,</booktitle>
<pages>29--41</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3353" citStr="Yi and Allan, 2009" startWordPosition="513" endWordPosition="516">wman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones. The approach by Wei and Croft (2006) was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results. Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings (Park and Ramamohanarao, 2009; Yi and Allan, 2009; Andrzejewski and Buttler, 2011; Lu et al., 2011). The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion. The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection. Depending on the query and on its specificity, topics may either be too coarse or too fine to accurately represent the latent concepts of the query. Recently, Ye et al. (2011) proposed a method which uses </context>
</contexts>
<marker>Yi, Allan, 2009</marker>
<rawString>Xing Yi and James Allan. 2009. A Comparative Study of Utilizing Topic Models for Information Retrieval. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09, pages 29–41. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>Modelbased Feedback in the Language Modeling Approach to Information Retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth International Conference on Information and Knowledge Management, CIKM ’01,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="4718" citStr="Zhai and Lafferty, 2001" startWordPosition="729" endWordPosition="732"> c�2013 Association for Computational Linguistics LDA and learns topics directly on a limited set of documents. While this approach is a first step towards modeling query-oriented topics, it lacks some theoretic principles and only aims to heuristically construct a “best” topic (from all learned topics) before expanding the query with its most probable words. More, none of the aforementioned works studied the semantic coherence of those generated topics. We tackle these issues by making the following contributions: • we introduce Topic-Driven Relevance Models, a model-based feedback approach (Zhai and Lafferty, 2001) for integrating topic models into relevance models by learning topics on pseudo-relevant feedback documents (as opposed to the entire document collection), • we explore the coherence of those generated topics using the queries of two major and well-established TREC test collections, • we evaluate the effects coherent topics have on ad hoc IR using the same test collections. 2 Topic-Driven Relevance Models 2.1 Relevance Models The goal of relevance models is to improve the representation of a query Q by selecting terms from a set of initially retrieved documents (Lavrenko and Croft, 2001). As </context>
<context position="6016" citStr="Zhai and Lafferty, 2001" startWordPosition="947" endWordPosition="950"> of the ranking list, this is constituted by a number N of top-ranked documents. Relevance models usually perform better when combined with the original query model (or maximum likelihood estimate). Let ˜θQ be this maximum likelihood query estimate and ˆθQ a relevance model, the updated new query model is given by: P(w|θQ) = λ P(w|˜θQ) + (1 − λ)P(w|ˆθQ) (1) where λ E [0, 1] is a parameter that controls the tradeoff between the original query model and the relevance model. One of the most robust variants of the relevance models is computed as follows: often referred to as model-based feedback (Zhai and Lafferty, 2001). We assume P(θD) to be uniform, resulting in an estimated relevance model based on a sum of document models weighted by the query likelihood score. The final, interpolated, estimate expressed in equation (1) is often referred in the literature as RM3. We tackle the null probabilities problem by smoothing the document language model using the well-known Dirichlet smoothing (Zhai and Lafferty, 2004). 2.2 LDA-based Feedback Model The estimation of the feedback model ˆθQ constitutes the first contribution of this work. We propose to explicitly model the latent topics (or concepts) that exist behi</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001. Modelbased Feedback in the Language Modeling Approach to Information Retrieval. In Proceedings of the Tenth International Conference on Information and Knowledge Management, CIKM ’01, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A Study of Smoothing Methods for Language Models Applied to Information Retrieval.</title>
<date>2004</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="6417" citStr="Zhai and Lafferty, 2004" startWordPosition="1012" endWordPosition="1015">rols the tradeoff between the original query model and the relevance model. One of the most robust variants of the relevance models is computed as follows: often referred to as model-based feedback (Zhai and Lafferty, 2001). We assume P(θD) to be uniform, resulting in an estimated relevance model based on a sum of document models weighted by the query likelihood score. The final, interpolated, estimate expressed in equation (1) is often referred in the literature as RM3. We tackle the null probabilities problem by smoothing the document language model using the well-known Dirichlet smoothing (Zhai and Lafferty, 2004). 2.2 LDA-based Feedback Model The estimation of the feedback model ˆθQ constitutes the first contribution of this work. We propose to explicitly model the latent topics (or concepts) that exist behind an information need, and to use them to improve the query representation. We consider O as the set of pseudo-relevant feedback documents from which the latent concepts would be extracted. The retrieval algorithm used to obtain these documents can be of any kind, the important point is that O is a reduced collection that contains the top documents ranked by an automatic and state-of-the-art retri</context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Transactions on Information Systems, 22(2):179–214.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>