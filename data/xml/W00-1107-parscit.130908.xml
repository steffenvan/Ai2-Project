<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.987958">
REXTOR: A System for Generating Relations
from Natural Language
</title>
<note confidence="0.854794666666667">
Boris Katz and Jimmy Lin
MIT Artificial Intelligence Laboratory
200 Technology Square
</note>
<address confidence="0.824745">
Cambridge, MA 02139 USA
</address>
<email confidence="0.997222">
lboris, jimmylinl@ai.mit.edu
</email>
<sectionHeader confidence="0.997271" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995948">
This paper argues that a finite-state
language model with a ternary expres-
sion representation is currently the most
practical and suitable bridge between
natural language processing and infor-
mation retrieval. Despite the theoreti-
cal computational inadequacies of finite-
state grammars, they are very cost ef-
fective (in time and space requirements)
and adequate for practical purposes.
The ternary expressions that we use
are not only linguistically-motivated, but
also amenable to rapid large-scale index-
ing. REXTOR (Relations EXtracTOR) is
an implementation of this model; in one
uniform framework, the system provides
two separate grammars for extracting
arbitrary patterns of text and building
ternary expressions from them. These
content representational structures serve
as the input to our ternary expressions
indexer. This approach to natural lan-
guage information retrieval promises to
significantly raise the performance of
current systems.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889363636363">
Traditional information retrieval (111,) has been
built on the &amp;quot;bag-of-words&amp;quot; assumption, which
equates the weighted component keywords of a
document with its semantic content. Obviously,
a document is much more than the sum of its in-
dividual keywords. Although keywords may offer
some indication of &amp;quot;meaning,&amp;quot; they alone cannot
capture the richness and expressiveness of natu-
ral language. Consider the following sets of sen-
tences/phrases that have similar word content,
but (dramatically) different meanings:&apos;
</bodyText>
<figure confidence="0.904244428571429">
&apos;Examples taken from (Loper, 2000)
(1) The big man ate the dog.
(1&apos;) The big dog ate the man.
(2) The meaning of life
(2&apos;) A meaningful life
(3) The bank of the river
(3&apos;) The bank near the river
</figure>
<bodyText confidence="0.999235695652174">
Due to the inability of keywords to capture the
&amp;quot;meaning&amp;quot; of documents, a traditional informa-
tion retrieval system (i.e., one using the bag-of-
words paradigm) will suffer from poor precision in
response to a user query accurately and precisely
formulated in natural language.
The application of natural language process-
ing (NLP) terhniques to information retrieval
promises to generate representational structures
that better capture the semantic content of docu-
ments. In particular, syntactic analysis can high-
light the relationships between various terms and
phrases in a sentence, which will allow us to distin-
guish between the example pairs given above and
answer queries with higher precision than tradi-
tional IR systems.
However, a syntactically-informed representa-
tional structure faces the problem of linguistic
variations, the phenomenon in which similar se-
mantic content may be expressed in different sur-
face forms. Consider the following sets of sen-
tences that express the same meaning using dif-
ferent constructions:
</bodyText>
<listItem confidence="0.5825062">
(4) What is Bill Gates&apos; net worth?
(4&apos;)What is the net worth of Bill Gates?
(5) John gave the book to Mary.
(5&apos;) John gave Mary the book.
(5&amp;quot;) Mary was given the book by John.
</listItem>
<bodyText confidence="0.670079875">
(6) The president surprised the country with
his actions.
(6&apos;) The president&apos;s actions surprised the
country.
(7) Over 22 million people live in Taiwan.
(7&apos;) The population of Taiwan is 22 million.
An effective linguistically-motivated informa-
tion retrieval system must not only handle rel-
</bodyText>
<page confidence="0.998974">
67
</page>
<bodyText confidence="0.998504813953489">
atively simple syntactic variations (e.g., (4) and
(5)), alternate realization of verb arguments (e.g.,
(6) and (6&apos;)), but also more complicated semantic
variations (e.g., (7) and (7&apos;)). This can be ac-
complished by linguistic normalization, a process
by which linguistic variants that contain the same
semantic content are mapped onto the same rep-
resentational structure.
The precision of information retrieval systems
can be dramatically improved if they index not
only single terms, but normalized representational
structures derived from language. However, the
optimal structure of this representation and the
efficient generation of these structures remains an
open research problem.
This paper argues that, for the purposes of
information retrieval systems, the most suitable
representational structure of document content is
ternary expressions (compared to, for example,
keywords, trees or case frames). Ternary (three-
place) expressions may be thought of as typed
binary relations (e.g., subject-relation-object) or
two-place predicates (e.g., transitive verbs like
&apos;hit&apos;); they are linguistically-motivated and effi-
cient to index. Also, for information retrieval, a
finite-state grammar is the most practical and cost
effective method by which to extract these ternary
expressions from documents. Combined together,
a finite-state language model and ternary expres-
sion representation provide a convenient and pow-
erful framework for integrating natural language
processing with information retrieval.
REXTOR (Relations EXtracTOR) is a docu-
ment content analysis system designed to unify
and generalize many previous natural language
information retrieval techniques into one single
framework. The system provides two separate
grammars: one for extracting arbitrary entities
from documents, and the other for building re-
lations from the extracted items. REXTOR also
provides a playground and testbed for future ex-
perimentation in linguistically-motivated indexing
schemes.
</bodyText>
<sectionHeader confidence="0.975923" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.995727222222222">
We believe that, for humans, natural language is
the best mechanism for information access. It is
intuitive, easy to use, rapidly deployable, and re-
quires no specialized training,
The REXTOR System builds on the experi-
ence of START (SynTactic Analysis using Re-
versible Transformations), a natural language sys-
tem available for question answering on the World
Wide Web.2 Since December, 1993, when it first
</bodyText>
<footnote confidence="0.738383">
2http://ww.aiamit.edu/projects/infolab
</footnote>
<bodyText confidence="0.998666">
came online, START has engaged in millions of ex-
changes with hundreds of thousands of people all
over the world, supplying users with knowledge
regarding geography, weather, movies, and many
many other areas. Despite the successes of START
in serving actual users, its domain of knowledge is
relatively small and expanding its knowledge base
is a time-consuming task. The goal of REXTOR is
to overcome this bottleneck and to provide a gen-
eral framework for natural-language information
retrieval. REXTOR not only draws its inspiration
from START (in providing question answering ca-
pabilities), but also borrows a simplified form of
its representational structures (Katz, 1980; Katz,
1990).
The START System (Katz, 1990; Katz, 1997)
analyzes English text and builds a knowledge base
from information found in the text. The knowl-
edge is expressed in the form of embedded ternary
expressions (T-expressions) subject-relation-
object triples where the subject and object can
themselves be ternary expressions. For exam-
ple, &amp;quot;The population of Zimbabwe is 11, 044, 147&amp;quot;
would be represented as two ternary expressions:
</bodyText>
<sectionHeader confidence="0.9587615" genericHeader="method">
[POPULATION-1 IS 11044147]
[POPULATION-1 RELATED-TO ZIMBABWE]
</sectionHeader>
<bodyText confidence="0.9998895">
Experience from START has shown that a robust
full-text natural language question-answering sys-
tem cannot be realistically expected any time
soon. Numerous problems such as intersenten-
tial reference, paraphrasing, summarization, com-
mon sense implication, and many more, will take
a long time to solve satisfactorily. In order to by-
pass intractable complexities of language, START
uses computer-analyzable natural language anno-
tations, which consist of simplified English sen-
tences and phrases, to describe various informa-
tion segments (which may be text, images, or even
video and other multimedia content). These nat-
ural language annotations serve as metadata and
inform START regarding the type of questions that
a particular information segment is capable of an-
swering (Katz, 1997). By performing retrieval on
natural language annotations, the system is able
to provide knowledge that it may not be able to
analyze itself (either language that is too com-
plex or non-textual segments). Because these an-
notations must be manually generated, expand-
ing START&apos;S knowledge base is relatively time-
intensive.
REXTOR attempts to eliminate the need for hu-
man involvement during content analysis, and also
aims to serve as the foundation of a natural lan-
guage information retrieval system. Ultimately,
</bodyText>
<page confidence="0.99889">
68
</page>
<bodyText confidence="0.99981075">
we hope that REXTOR will serve as a stepping
stone towards a comprehensive system capable of
providing users with &amp;quot;just the right information&amp;quot;
to queries posed in natural language.
</bodyText>
<sectionHeader confidence="0.998005" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.999973228915663">
The concept of indexing more than simple key-
words is not new; the idea of indexing (parts
of) phrases, for example, is more than a decade
old (Fagan, 1987). Arampatzis (1998) introduced
the phrase retrieval hypothesis, which asserted
that phrases are a better indication of document
content than keywords. Several researchers have
also explored different techniques of linguistic nor-
malization for information retrieval (Strzalkowski
et al., 1996; Zhai et al., 1996; Arampatzis et
al., 2000). The performance improvements were
neither negligible nor dramatic, but despite the
lack of any significant breakthroughs, the au-
thors affirmed the potential value of linguistically-
motivated indexing schemes and the advantages
they offer over traditional IR.
Previous research in linguistically motivated
information retrieval concentrated primarily on
noun phrases and their attached prepositional
phrases. Techniques that involve head/modifier
relations have been tried, e.g., indexing adjec-
tive/noun and noun/right adjunct pairs (which
normalizes variants such as &amp;quot;information re-
trieval&amp;quot; and &amp;quot;retrieval of information&amp;quot;). How-
ever, there has been little experimentation with
other types of linguistic relations, e.g., apposi-
tives, predicate nominatives (i.e., the is-a rela-
tion), predicate adjectives (i.e., the has-property
relation), etc. Furthermore, indexing of word
pairs and phrases in many previous systems was
accomplished by converting those representations
into lexical items and atomic terms, indexed in
the same manner as single words. The treat-
ment of these representational structures using a
restrictive bag-of-words paradigm limits the type
of queries that may be formulated. For example,
treating adjective/noun pairs ([adj., noun]) as lex-
ical atoms renders it impossible to find the equiv-
alent of &amp;quot;all big things,&amp;quot; corresponding to the pair
[big, *].
The extraction of these relations from docu-
ments has been relatively inefficient and unsys-
tematic. One approach is to first parse the
document using a full-text parser, and then ex-
tract interesting relations from the resulting parse
tree (Fagan, 1987; Grislunan and Sterling, 1993;
Loper, 2000). This approach is slow and inefficient
because full-text parsing is very time-intensive.
Due to current limitations of computational tech-
nology, only a small fraction of the information
gathered by a full parser can be efficiently indexed.
For the most part, relations that can be effec-
tively utilized for information retrieval purposes
only occupy a few nodes of a (possibly dense)
parse tree; thus, most of the knowledge gathered
by the parser is thrown away. Also, extracting
non-linguistic relations from parse trees is very
difficult; many interesting relations (from an IR
point of view) have no linguistic foundation, e.g.,
adjacent word pairs. The other approach to ex-
tracting relations from text is to build simple fil-
ters for every new relation. This approach is un-
systematic, and does not allow for rapid addition
of new relations to a system.
The REXTOR System utilizes an integrated
model to systematically extract arbitrary textual
patterns and relations (ternary expressions) from
documents. The concept of coupling structure-
building actions with parsing originated with aug-
mented transition networks (ATNs)(Thorne et al.,
1968; Woods, 1970). Similarly, PLNLP (Heidorn,
1972; Jensen et al., 1993) is a programming lan-
guage for writing phrase structure rules that in-
clude specific conditions under which the rule can
be applied. These rules may also be augmented
by structure-building actions that are to be taken
when the rule is applied. However, these sys-
tems that attempt full-text parsing are less effi-
cient for information retrieval applications due to
the long time necessary to generate full linguistic
parse trees. REXTOR was designed with a simple
language model and an equally simple, yet expres-
sive, representation of &amp;quot;meaning.&amp;quot;
</bodyText>
<sectionHeader confidence="0.9341345" genericHeader="method">
4 Bridging Natural Language and
Information Retrieval
</sectionHeader>
<bodyText confidence="0.999992857142857">
In order to bridge the gap between natural lan-
guage and information retrieval, natural language
text must be distilled into a representational
structure that is amenable to fast, large-scale in-
dexing. We argue that a finite-state model of nat-
ural language with ternary expressions is currently
the most suitable combination for this task.
</bodyText>
<subsectionHeader confidence="0.999531">
4.1 Finite-State Language Model
</subsectionHeader>
<bodyText confidence="0.999962555555556">
Despite its limitations, a finite-state grammar
seems to provide the best natural language model
for information retrieval purposes. One of the
most notable computational inadequacies of the
finite-state model is the absence of a pushdown
mechanism to suspend the processing of a con-
stituent at a given level while using the same
grammar to process an embedded constituent
(Woods, 1970). Due to this inadequacy, certain
</bodyText>
<page confidence="0.996206">
69
</page>
<bodyText confidence="0.998980940298508">
English constructions, such as center embedding,
cannot be described by any finite-state gram-
mar (Chomsky, 1959a; Chomsky, 1959b). How-
ever, Church (1980) demonstrated that the finite-
state language model is adequate to describe a
performance model of language (i.e., constrained
by memory, attention, and other realistic limi-
tations) that approximates competence (i.e., lan-
guage ability under optimal conditions without re-
source constraints). Many phenomena that can-
not be handled by finite-state grammars are awk-
ward from a psycholinguistic point of view, and
hence rarely seen. More recently, Pereira and
Wright (1991) developed formal methods of ap-
proximating context-free grammars with finite-
state grairunars.3 Thus, for practical purposes,
computationally simple finite-state grammars can
be utilized to adequately model natural language.
Empirically, the effectiveness of the finite-
state language model has been demonstrated in
the Message Understanding Conferences (MUCs),
which evaluated information extraction (IE) sys-
tems on a variety of domain-specific tasks. The
conferences have shown that superficial parsing
using finite-state grammars performs better than
deep parsing using context-free grammars (at least
under the current constraints of technology). The
NYU team switched over from a system that per-
formed full parsing (PROTEUS) in MUC-5 (Gr-
ishman and Sterling, 1993) to a regular expres-
sion matching parser in MUC-6 (Grishman, 1995).
Full parsing was slow and error-prone, and the
process of building a full syntactic analysis in-
volved relatively unconstrained search which con-
sumed large amounts of both time and space. The
longer debug-cycles that resulted from this trans-
lated into fewer iterations with which to tune the
system within a given amount of time. Further-
more, the complexity of a full context-free gram-
mar contributed to maintenance problems; com-
plex interactions within the grammar prevented
rapid updating of the system to handle new con-
structions.
Finite-state grammars have been used to ex-
tract entities such as proper nouns, names, lo-
cations, etc., with relatively high precision. To
a lesser extent, these grammars have proven to
be effective in identifying syntactic constructions
such as noun phrases and verb phrases. FASTUS
(Hobbs et al., 1996), the most notable of these sys-
tems, is modeled after cascaded, nondeterministic
finite-state automata. The finite-state transduc-
ers are &amp;quot;cascaded&amp;quot; in that they are arranged in
3However, these approximations overgenerate, al-
though in predictable, systematic ways.
series; each one maps the output structures from
the previous transducer into structures that com-
prise the input to the next transducer.
There are many similarities between informa-
tion extraction and building effective representa-
tional structures for information retrieval. Both
tasks involve identifying entities (e.g., phrases)
and the relationships between those entities.
Thus, the application of proven information ex-
traction techniques (i.e., finite-state technology)
to information retrieval offers promise in raising
the performance of IR systems.
</bodyText>
<subsectionHeader confidence="0.996223">
4.2 Ternary Expressions
</subsectionHeader>
<bodyText confidence="0.994017195121951">
Ternary (three-place) expressions currently ap-
pear to be the most suitable representational
structure for meaning extracted from text. They
may be intuitively viewed as subject-relation-
object triples, and can easily express many types
of relations, e.g., subject-verb-object relations,
possession relations, etc. From a syntactic point of
view, ternary expressions may be viewed as typed
binary relations. Given the binary branching hy-
pothesis of linguistic theory, ternary expressions
are theoretically capable of expressing any arbi-
trary tree — thus, ternary expressions are com-
patible with linguistic theory. From a semantic
point of view, ternary expressions may be viewed
as two-place predicates, and can be manipulated
using predicate logic. Finally, ternary expressions
are highly amenable to rapid large-scale indexing,
which is a necessary prerequisite of information re-
trieval systems. Although other representational
structures (e.g., trees or case frames) may be bet-
ter adapted for some purposes, they are much
more difficult to index and retrieve efficiently due
to their size and complexity.
In fact, indexing linguistic tree structures has
been attempted (Smeaton et al., 1994), with very
disappointing results: precision actually decreased
due to the inability to handle variations in tree
structure (i.e., the same semantic content could
be expressed using different syntactic structures),
and to the poor quality of the full-text natural lan-
guage parser, which was also rather slow. Despite
recent advances, full-text natural language parsers
are still relatively error-prone; indexing incorrect
parse trees is a source of performance degrada-
tion. Furthermore, matching trees and sub-trees is
a computationally intensive task, especially since
full linguistic parse trees may be relatively deep.
Relations are easier to match because they are
typically much simpler than parse trees. For ex-
ample, the tree
[[shiny happy people I [of [Wonderland]]]
</bodyText>
<page confidence="0.991095">
70
</page>
<bodyText confidence="0.993222090909091">
may be &amp;quot;flattened&amp;quot; into three relations:
&lt; shiny describes people &gt;
&lt; happy describes people &gt;
&lt; people related-to Wonderland &gt;
Indexing case frames has also been attempted
(Croft and Lewis, 1987; Loper, 2000), but with
limited success. Full semantic analysis is still
an open research problem, especially in the gen-
eral domain. Since full semantic analysis can-
not be performed without full-text parsing, case
frame analysis inherits the unreliability of current
parsers. Furthermore, semantic analysis requires
extensive knowledge in the lexicon, which is ex-
tremely time-intensive to construct. Finally, due
to the complex structure of case frames, they are
more difficult to store and index than ternary ex-
pressions.
Since ternary expressions are merely three-place
relations, they may be indexed and retrieved much
in the same way as rows within the table of a rela-
tional database:4 hence, well-known optimizations
for databases may be applied for extremely high
performance.
Previous linguistically-motivated indexing
schemes may easily be reformulated using ternary
expressions. For example, indexing adjacent
word pairs consists of indexing adjacent words
with the adjacent relation. In fact, all pairs
(e.g., adjective-noun, head-modifier) can be
reformulated as ternary expressions by assigning
a type to the pair. This finer granularity allows
the capture of more intricate relations between
words in a document.
</bodyText>
<sectionHeader confidence="0.99542" genericHeader="method">
5 The REXTOR System
</sectionHeader>
<bodyText confidence="0.983335235294118">
Using its finite-state language model, the REXTOR
System generates a set of ternary expressions
that correspond to content of a part-of-speech-
tagged input document. Currently, the Brill Tag-
ger (Brill, 1992) (with minor postprocessing) is
used for the part-of-speech (POS) tagging. The
relations construction process consists of two dis-
tinct processes, each guided by its own externally
specified grammar file. Extraction rules are ap-
plied to match arbitrary patterns of text, based
either on one of thirty-nine POS tags or on exact
words. Whenever an item is extracted, a corre-
sponding relation rule is triggered, which handles
the actual generation of the ternary expressions
(relations).
41n fact, our first implementation of a ternary ex-
pressions indexer used a SQL database.
</bodyText>
<subsectionHeader confidence="0.95261">
5.1 Extraction Rules
</subsectionHeader>
<bodyText confidence="0.99952847368421">
Extraction rules are used to extract arbitrary pat-
terns of text according to a grammar specification.
The REXTOR grammar is written as regular ex-
pression rules, which are computationally equiv-
alent to finite-state automata.5 Writing gram-
mar rules in this fashion allows for perspicuity,
the property whereby permitted types of construc-
tions are readily apparent from the rules. Such
a human-readable formulation simplifies mainte-
nance of the grammar.
The extraction stage of the REXTOR System
performs a no-lookallead left-to-right scan of ev-
ery input sentence, identifies the longest match-
ing pattern (from any grammar rule), reduces the
input sequence based on the matched rule, and
continues with the next unmatched word. If a
word cannot be included in any grammar rule, it
is skipped.
An extraction rule takes the following form:
</bodyText>
<equation confidence="0.754219">
EntityType := template;
</equation>
<bodyText confidence="0.999943333333333">
The rule can be read as Entity-Type is defined
as template. A successful match of the pattern in
template signifies a successfully extracted entity.
The template consists of a series of legal tokens,
which are shown in Table 1. In addition, token
modifiers (also in Table 1) can alter the meaning
of the immediately preceding token. Tokens sur-
rounded by curly braces (0) are saved as bound
variables, which can be later utilized to build re-
lations (ternary expressions). These variables are
referenced numerically starting at zero (e.g., the
0th bound variable).
</bodyText>
<subsectionHeader confidence="0.981723">
5.2 Relation Rules
</subsectionHeader>
<bodyText confidence="0.981786333333334">
A relation rule is triggered by the successful ex-
traction of a particular entity (Ent ityType). The
relations grammar directs the construction of the
actual ternary expression. A relation rule takes
the following form:
EntityType :=&gt; &lt;atoml atom2 atom3&gt; ;
The Ent ityType is the trigger for the relation,
i.e., the rule is applied whenever a string of that
type is extracted. The right hand side of the re-
lation rule is the ternary expression to be gener-
ated, which is a triple composed of three atoms.
Valid atoms are shown in Table 2. They are either
string literals or they manipulate the bound vari-
ables saved from the extraction process in some
manner.
</bodyText>
<footnote confidence="0.959161333333333">
5For an algorithm converting regular expressions
to nondeterministic finite-state automata, please refer
to (Aho et al., 1988), Chapter 3.
</footnote>
<page confidence="0.99012">
71
</page>
<table confidence="0.991690416666667">
Token Description
POS This matches any word tagged as the part-of-speech POS.
POS [string] This matches a specific word (string) of a specific part-of-speech (POS).
EntityType This matches any extracted string of type EntityType.
(token° I tokeni I . . • ) This expression matches any one of the alternative tokens given within
the parentheses. Matches are attempted in the order in which they are
written, e.g., the first token is tried first.
Description
This modifier matches zero or more occurrences of the previous token.
This modifier matches zero or one occurrence of the previous token.
This modifier matches one or more occurrences of the previous token.
Token Modifier
</table>
<tableCaption confidence="0.999941">
Table 1: Valid tokens and token modifiers for extraction rules.
</tableCaption>
<figure confidence="0.969930571428572">
Modifier
En]
{n}
[1] ,EntityTypei [31 , • • •
(alternativei I alternative2I . • •)
&apos;string&apos;
Description
</figure>
<figureCaption confidence="0.512539166666667">
Evaluates to the nth bound variable of the trigger EntityType,
interpreted as a string.
Evaluates to the nth bound variable of the trigger EntityType,
interpreted as a list of strings. The extraction rule token inside
the bound variable is stripped of its outermost * or +, and the
bound variable is broken into a list according to this pattern. For
example, {JJX*} is interpreted as a list of JJX, or adjectives.
This expression extracts a bound variable nested inside other
bound variables. The ith bound variable of trigger EntityType
is extracted; if this item is of type EntityTypel, then the jth
bound variable is extracted (the expression returns false if the
entity types do not match); each comma separated unit is inter-
preted in this manner, up to an arbitrary depth.
This compound expression evaluates to the disjunction of an
arbitrary number of valid atoms (as defined in this table). Each
alternative is evaluated in a left to right order; the disjunction
evaluates to the first alternative that returns a non-empty string.
A literal string.
</figureCaption>
<tableCaption confidence="0.992139">
Table 2: Valid atoms for the relation rules.
</tableCaption>
<table confidence="0.8242304">
Extraction Rules: NounGroup : (PRPZ I DT)? {JJX*} { (NNPX I NNX I NNPS I NNS)+};
PrepositioaalPhrase : = IN {NounGroup};
ComplerNounGroup : = INounGroupHPrepositionalPhrasel ;
Relation Rules: NounGroup :&gt; &lt;{0} &apos; describes &apos; [1] &gt; ;
ComplerNounGroup : =&gt;
</table>
<figure confidence="0.975038">
&lt;[0] ,NounGroup [1]
&apos; related-to &apos;
[1] , Prepos it ioaalPhrase [0] ,NounGroup [1] &gt; ;
</figure>
<figureCaption confidence="0.999857">
Figure 1: Example of relation and extraction rules. (PRPZ is the part-of-speech tag for possessive pronouns,
</figureCaption>
<bodyText confidence="0.861003">
DT for determiners, JJX for adjectives, JJR for comparative adjectives, JJS for superlative adjectives, NNX for
singular or mass nouns, NNS for plural nouns, NNPX for singular proper nouns, NNPS for plural proper nouns, IN
for prepositions.)
</bodyText>
<page confidence="0.994751">
72
</page>
<subsectionHeader confidence="0.689342">
5.3 Examples
</subsectionHeader>
<bodyText confidence="0.9908864">
A few extraction and relation rules are given
in Figure 1. The first extraction rule defines
a NounGroup as a sequence consisting of: an
optional possessive pronoun or determiner, any
number of adjectives, one or more nouns (of
any type). Also, the sequence of adjectives is
saved as the 0th bound variable, and the se-
quence of nouns is saved as the 1st bound vari-
able. The rules for Preposit ionalPhrase and
ComplexNounGroup can be interpreted similarly.
Consider the following noun phrase:
the big, bad wolf of the dark forest
REXTOR recognizes two NounGroups in the
above phrase: the big, bad wolf and the dark for-
est. The corresponding relation rule triggers, and
generates the following relations:
&lt; (big, bad) describes wolf &gt;
&lt; (dark) describes forest &gt;
Note that the first bound variable in NounGroup
is interpreted as a list; thus, the above two re-
lations expand into three distinct relations when
completely enumerated:
&lt; big describes wolf &gt;
&lt; bad describes wolf &gt;
&lt; dark describes forest &gt;
The ability to interpret bound variables as a list
of strings allows for easy manipulation of repeated
structure, like textual lists or enumerations.
In addition, the entire noun phrase the big, bad
wolf of the dark forest will be recognized as a
ComplexNounGroup. This will result in the fol-
lowing relation:
&lt; wolf related-to forest &gt;
The relation rule associated with
ComplexNounGroup involves extracting nested
bound variables. The first atom evaluates to
the lth bound variable (a NounGroup) inside
the 0th bound variable inside the trigger item
ComplexNounGroup. The third atom is similarly
evaluated.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999762833333333">
Informal analysis of documents using REXTOR re-
veals that it can potentially serve as an effective
framework for extracting &amp;quot;meaning&amp;quot; from docu-
ments. In particular, the system is capable of
identifying the following types of linguistic con-
structions and generating relations from them:
</bodyText>
<listItem confidence="0.96719625">
• Simple sentences can be extracted by
noting a simple NounGroup VerbGroup
NounGroup pattern. From this, subject-verb-
object (SVO) relations can be derived.
• Predicative nominatives can be recog-
nized by identifying the &amp;quot;be&amp;quot; verb and the
NounGroup directly following it. These con-
structions may be useful in establishing onto-
logical hierarchies, i.e., is-a trees.
• Predicative adjectives can be recognized
by the &amp;quot;be&amp;quot; verb and a succession of one or
more adjectives (or adjectival phrase). They
may provide addition information regarding
the attributes of entities, e.g., has-property.
• Appositives are characteristically offset by
commas and usually contain a single noun
phrase; thus, they can be recognized rela-
tively easily. Common in prose, appositives
offer a wealth of additional information re-
garding various entities, e.g., location of sites,
age or position of people, etc.
• Prepositional phrases are relatively easy
to extract, and may supply valuable relations
that increase the precision of information re-
</listItem>
<bodyText confidence="0.979679761904762">
trieval systems. Ternary expressions allow
for a better representation of prepositional
phrases (compared to pairs) because they al-
low the preposition to more specifically de-
termine the type of relation (thus, examples
like &amp;quot;boat by the water&amp;quot; and &amp;quot;boat under
the water,&amp;quot; which have completely different
meanings, may be indexed separately and dis-
tinctly).
However, the prepositional phrase attach-
ment problem (in the general-domain case)
is still an open research topic, and thus poses
some problems to content analysis. Regard-
less, for the purposes of information retrieval,
it may be acceptable to err on the side of over-
generation in considering attachment, i.e.,
enumerate all possible relations. This will
no doubt generate a large number of (pos-
sibly incorrect) relations, and more research
is required to determine effective methods of
controlling this explosion.
</bodyText>
<listItem confidence="0.7760515">
• Relative clauses of some types can be iden-
tified by a finite-state language model. They
may supply additional useful SVO relations
for indexing purposes.
</listItem>
<bodyText confidence="0.99989725">
We believe that future breakthroughs in natu-
ral language information retrieval will occur in the
generation of meaningful relations. Although the
finite-state language model of REXTOR is powerful
</bodyText>
<page confidence="0.997364">
73
</page>
<bodyText confidence="0.999891285714286">
enough to extract many linguistically interesting
constructions, the approach is not fundamentally
new. What differentiates our system from pre-
vious work such as FASTUS (Hobbs et al., 1996)
is that REXTOR not only provides a mechanism
for extraction, but also introduces the paradigm
of ternary expressions to capture document con-
tent for information retrieval. The relations view
of natural language documents is highly amenable
to integration with information retrieval systems.
Through a relations representation, REXTOR is
able to distinguish the subtle differences in mean-
ing between the pairs of sentences and phrases
given in the introduction:
</bodyText>
<figure confidence="0.899493642857143">
(1) The man ate the dog.
&lt; man is-subject-of eat &gt;
&lt; dog is-object-of eat &gt;
(1&apos;) The dog ate the man.
&lt; man is-object-of eat &gt;
&lt; dog is-subject-of eat &gt;
(2) The meaning of life
&lt; meaning possessive-relation life &gt;
(2&apos;) A meaningful life
&lt; meaningful describes life &gt;
(3) The bank of the river
&lt; bank possessive-relation river &gt;
(3&apos;) The bank near the river
&lt; bank near-relation river &gt;
</figure>
<bodyText confidence="0.994465083333333">
The ability to extract subject-verb-object re-
lations, e.g., (1) and (1&apos;), allows an IR system
to distinguish between two very different state-
ments. Similarly, REXTOR can differentiate be-
tween prepositional phrases (2) and adjectival
modification (2&apos;). Although the system does not
have any notion of semantics (e.g., word sense),
syntax may offer crucial clues to meaning in cases
such as (3) and (3&apos;).
Similarly, REXTOR is capable of performing lin-
guistic normalization at the syntactic and mor-
phological levels. Consider these sets of examples
</bodyText>
<listItem confidence="0.854594428571429">
originally presented in the introduction:
(4) What is Bill Gates&apos; net worth?
(4&apos;) What is the net worth of Bill Gates?
&lt; &amp;quot;net worth&amp;quot; related-to &amp;quot;Bill Gates&amp;quot; &gt;
(5) John gave the book to Mary.
(5&apos;) John gave Mary the book.
(5&amp;quot;) Mary was given the book by John.
</listItem>
<table confidence="0.93799225">
&lt; John is-subject-of give &gt;
&lt; book is-direct-object-of give &gt;
&lt; Mary is-indirect-object-of give &gt;
(6) The president surprised the country with
his actions.
&lt; president is-subject-of surprise &gt;
&lt; country is-object-of surprise &gt;
&lt; surprise with actions &gt;
(6&apos;) The president&apos;s actions surprised his
country.
&lt; actions related-to president &gt;
&lt; actions is-subject-of surprise &gt;
&lt; country is-object-of surprise &gt;
(7) Over 22 million people live in Taiwan.
&lt; &amp;quot;22 million&amp;quot; is-quantity-of people &gt;
&lt; people is-subject-of live &gt;
&lt; live in Taiwan &gt;
(7&apos;) The population of Taiwan is 22 million.
&lt; population is &amp;quot;22 million&amp;quot; &gt;
&lt; population related-to Taiwan &gt;
</table>
<bodyText confidence="0.994403837209302">
With relations, different surface forms of ex-
pressing the &amp;quot;possession relation&amp;quot; may be nor-
malized into the same structure, e.g., (4) and
(4&apos;). Similarly, alternative surface realization of
the same verb-headed relation can be recognized
and equated with each other by writing different
extraction rules that generate the same relations,
e.g., (5), (5&apos;), and (5&amp;quot;). The process of normal-
ization will hopefully lead to greater recall in in-
formation retrieval systems. Note that (6) and
(6&apos;) demonstrate a limitation of REXTOR, namely
its inability to deal with alternative realizations of
verb arguments. Also, the system does not have
any notion of semantics, and thus is unable to
equate two sentences that have the same meaning,
e.g., (7) and (7&apos;). Although it is certainly possible
to manually encode such semantic knowledge as
extraction and relation rules, this solution is far
from elegant.
A potential solution to this semantic variations
problem is to borrow the solution employed by
START. A ternary expression representation of
natural language mimics its syntactic organiza-
tion, and hence sentences that differ in surface
form but are close in meaning will not map into
the same structure. In order to solve this problem,
START deploys &amp;quot;S-rules&amp;quot; (Katz and Levin, 1988),
which are reversible syntactic/semantic transfor-
mational rules that render explicit the relationship
between alternate realizations of the same mean-
ing. For example, a buy expression is semantically
equivalent to a sell expression, except the subject
and indirect objects are exchanged. Because many
verbs can undergo the same alternations, they can
in fact be grouped into verb classes, and hence
governed by the same S-rules. Thus, S-rules can
be viewed as metarules applied over ternary ex-
pressions. A similar technique for handling both
syntactic and semantic variations can be found in
(Grislunan, 1995; Jacquemin et al., 1997). Both
utilize metarules (e.g., for passive/active transfor-
mation) applied over textual patterns in order to
generate and handle variations.
</bodyText>
<page confidence="0.995475">
74
</page>
<bodyText confidence="0.986357652173913">
Below we present a concrete example of how
REXTOR could potentially improve the perfor-
mance of existing keyword search engines dramat-
ically. We indexed an electronic version of the
Worldbook Encyclopedia at the sentence level us-
ing the following two techniques:
1. A simple inverted keyword index. All stop-
words are thrown out, and all content words
are stemmed. Retrieval was performed by
matching content words in the query with
content words in the encyclopedia articles.
2. A ternary expressions index using the rela-
tions generated by REXTOR. The grammar
was written to extract possessive relations,
description relations (adjective-noun modifi-
cation), prepositional relations, subject-verb
relations, and verb-object relations. Re-
trieval was performed by matching ternary
expressions from the query (extracted using a
separate grammar) with ternary expressions
extracted from the encyclopedia articles.
The following shows the results of the keyword
search engine:
</bodyText>
<reference confidence="0.896987388888889">
Question: What do frogs eat?
Answer:
(R1) Adult frogs eat mainly insects and
other small animals, including earthworms,
minnows, and spiders.
(R2) Bow-fins eat mainly other fish, frogs,
and crayfish.
(R3) Most cobras eat many kinds of ani-
mals, such as frogs, fishes, birds, and various
small mammals
(R4) One group of South American frogs
feeds mainly on other frogs.
(R5) Cranes eat a variety of foods, including
frogs, fishes, birds, and various small mam-
mals.
(R6) Frogs eat many other animals, includ-
ing spiders, flies, and worms.
(R7) ...
</reference>
<bodyText confidence="0.9984789">
After removing stopwords from the query, our
simple keyword search engine returned 33 results
that contain the keywords frog and eat. How-
ever, only (R1), (R4), and (R6) correctly answer
the user query; the other results answer the ques-
tion &amp;quot;What eats frogs?&amp;quot; or otherwise coinciden-
tally contain those two terms. (Apparently, our
poor frog has more predators than prey.) A bag-
of-words approach fundamentally cannot differen-
tiate between a query in which the frog is in the
subject position and a query in which the frog is in
the object position. However, by parsing subject-
verb-object relations using REXTOR, a ternary ex-
pressions indexer can effectively filter out irrele-
vant results, returning the three correct responses.
While indexing relations may potentially lower re-
call, due to unanticipated constructions, it has a
tremendous potential in increasing precision.
Furthermore, consider the following queries, in
which REXTOR would outperform traditional key-
</bodyText>
<listItem confidence="0.860845888888889">
word engines:
(8) How many South Koreans were recently
allowed to visit their North Korean rela-
tives?
(9) Where did John see Mary?
(10) Regarding what issue did the president
of Russia criticize China?
(11) Are electronics the biggest export from
Japan to the United States?
</listItem>
<bodyText confidence="0.999887235294118">
A traditional search engine using the bag-of-
words approach would suffer from poor precision
when faced with the above queries. Many verbs
take arguments of the same semantic type, and in
most of these sentences, reordering the verb argu-
ments drastically alters their meaning. For exam-
ple, a keyword search engine would not be able to
distinguish between a question regarding South
Koreans visiting North Korea and North Kore-
ans visiting South Korea (8) because both queries
have the same keyword content. Similarly, the
keyword approach would be unable to determine
who did the seeing (9), or who did the criticiz-
ing (10). Modification relations also pose difficul-
ties to the bag-of-words paradigm, e.g., was it the
North Korean or South Korean relatives (8)? Was
it the president of Russia or the president of China
(10)? Furthermore, there are some constructions
whose meaning critically depends on relations be-
tween the entities, e.g., (11), because &amp;quot;from X to
Y&amp;quot; and &amp;quot;from Y to X&amp;quot; usually differ in meaning.
The current version of REXTOR is merely a pro-
totype; thus, we have made minimal attempts
to optimize its processing speed. On a Pentium
III 933 MHz Linux system with 512 megabytes
of RAM,&apos; analyzing a sentence in the Worldbook
Encyclopedia required 0.0378 seconds on average.
This translates into a content analysis rate of
roughly 340 words a second, or approximately 11.4
megabytes of text per hour. Although the system
composed of REXTOR and the ternary expressions
indexer is slower than the simple keyword indexer,
we believe that the potential to dramatically in-
crease precision offsets the longer processing time.
</bodyText>
<footnote confidence="0.9024535">
6However, REXTOR is not a memory-intensive sys-
tem; RAM utilization during trial runs was rather low.
</footnote>
<page confidence="0.998481">
75
</page>
<bodyText confidence="0.999845214285714">
This paper presents only the first stage of an
linguistically-motivated information retrieval sys-
tem. Although we have presented the results of
a preliminary investigation into the effectiveness
of this approach, we cannot draw any conclu-
sions until more comprehensive tests have been
conducted. However, many prior techniques used
in natural language information retrieval (e.g.,
head/modifier pairs) can be expressed within the
REXTOR framework, and furthermore the system
provides a playground for experimenting with new
techniques. Thus, we believe that our approach
shows great promise in moving towards higher
performance information retrieval systems.
</bodyText>
<sectionHeader confidence="0.998373" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999975333333333">
This paper presented a scheme for integrating nat-
ural language processing and information retrieval
by adopting a finite-state model of language and
a ternary expression representation of document
content. We provided justification for our lan-
guage model and representational structures in
both linguistic and empirical terms. REXTOR is
an implementation of our ideas — it not only in-
tegrates many previous natural language indexing
techniques, but also provides a sufficiently gen-
eral framework for much future experimentation.
Although we have not yet conducted comprehen-
sive tests, the extraction of &amp;quot;meaning&amp;quot; from doc-
uments using REXTOR promises to better fulfill
users&apos; information needs.
</bodyText>
<sectionHeader confidence="0.998854" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999015">
We would like to thank Sue Felslain for her insight-
ful comments in reviewing drafts of this paper.
</bodyText>
<sectionHeader confidence="0.99927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998620776119403">
Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman.
1988. Compilers - Principles, Techniques, and
Tools. Addison-Wesley.
Avi Arampatzis, Th.P. van der Weide, C.H.A.
Koster, and P. van Bommel. 1998. Phrase-
based information retrieval. Information Pro-
cessing and Management, 34(6):693-707, De-
cember.
Avi Araznpatzis, Th.P. van der Weide, C.H.A.
Koster, and P. van Bommel. 2000. An
evaluation of linguistically-motivated indexing
schemes. In Proceedings of BCS-IRSG 2000
Colloquium on IR Research.
Eric Brill. 1992. A simple rule-based part of
speech tagger. In Proceedings of the Third Con-
ference on Applied Natural Language Process-
ing.
Noam Chomsky. 1959a. A note on phrase
structure grammars. Information and Control,
2:393-395.
Noam Chomsky. 1959b. On certain formal prop-
erties of grammars Information and Control,
2:137-167.
Kenneth W. Church. 1980. On memory limita-
tions in natural language processing. Technical
Report TR-245, MIT Laboratory for Computer
Science.
Bruce Croft and David D. Lewis. 1987. An ap-
proach to natural language processing for doc-
ument retrieval. In Proceedings of the 10th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval (SIGIR-87).
Joel L. Fagan. 1987. Experiments in Auto-
matic Phrase Indexing for Document Retrieval:
A Comparisons of Syntactic and Non-Syntactic
Methods. Ph.D. thesis, Cornell University.
Ralph Grishman and John Sterling. 1993. New
York University: Description of the PROTEUS
system as used for MUC-5. In Proceedings of the
5th Message Understanding Conference (MUC-
5).
Ralph Grishman. 1995. The NYU system for
MUC-6 or where&apos;s the syntax. In Proceedings
of the 6th Message Understanding Conference
(MUC-6).
George E. Heidom. 1972. Natural lan-
guage inputs to a simulation programming sys-
tems. Technical Report NPS-55HD72101A,
Naval Postgraduate School.
Jerry R. Hobbs, Douglas Appelt, John Bear,
David Israel, Megumi Kameyama, Mark Stickel,
and Mabry Tyson. 1996. FASTUS: A cascaded
finite-state transducer for extracting informa-
tion from natural-language text. In Roche and
Schabes, editors, Finite State Devices for Nat-
ural Language Processing. MIT Press.
Christian Jacquemin, Judith L. Kla.vans, and Eve-
lyne Tzoukermann. 1997. Expansion of multi-
word terms for indexing and retrieval using
morphology and syntax. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics (ACL&apos;97).
Karen Jensen, George E. Heidorn, and Stephen D.
Richardson, editors. 1993. Natural Language
Processing: The PLNLP Approach. Kluwer
Academic Publishers.
</reference>
<page confidence="0.841926">
76
</page>
<reference confidence="0.999261833333333">
Boris Katz and Beth Levin. 1988. Exploiting lex-
ical regularities in designing natural language
systems. In Proceedings of the 12th Interna-
tional Conference on Computational Linguistics
(COLING &apos;88).
Boris Katz. 1980. A three-step procedure for lan-
guage generation. Technical Report 599, MIT
Artificial Intelligence Laboratory.
Boris Katz. 1990. Using English for indexing and
retrieving. In P.H. Winston and S.A. Shellard,
editors, Artificial Intelligence at MIT: Expand-
ing Frontiers, volume 1. MIT Press.
Boris Katz. 1997. Annotating the World Wide
Web using natural language. In Proceedings of
the 5th RIA0 Conference on Computer Assisted
Information Searching on the Internet (RIAO
&apos;97).
Edward Loper. 2000. Applying semantic rela-
tion extraction to information retrieval. Mas-
ter&apos;s thesis, Massachusetts Institute of Technol-
ogy.
Fernando Pereira and Rebecca Wright. 1991.
Finite-state approximation of phrase structure
grammars. In Proceedings of the 29th Meeting
of the ACL.
Alan F. Smeaton, Ruairi O&apos;Donnell, and Fergus
Kelledy. 1994. Indexing structures derived
from syntax in TREC-3: System description.
In Proceedings of the 3rd Text REtrieval Con-
ference (TREC-3).
Tomek Strzalkowski, Louise Guthrie, Jussi Karl-
gren, Jim Leistensnider, Fang Lin, Jose Perez-
Carballo, Troy Straszheim, Jin Wang, and Jon
Wilding. 1996. Natural language information
retrieval: TREC-5 report. In Proceedings of the
5th Text REtrieval Conference (TREC-5).
J. Thorne, P. Bratley, and H. Dewar. 1968. The
syntactic analysis of English by machine. In
Donald Michie, editor, Machine Intelligence 3.
Edinburgh University Press.
William A. Woods. 1970. Transition network
grammars for natural language analysis. Com-
munications of the ACM, 13(10).
Chengxiang Zhai, Xiang Tong, Natasa Milic-
Frayling, and David A. Evans. 1996. Evalu-
ation of syntactic phrase indexing — CLARIT
NLP track report. In Proceedings of the 5th
Text REtrieval Conference (TREC-5).
</reference>
<page confidence="0.99911">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946589">
<title confidence="0.996933">REXTOR: A System for Generating from Natural Language</title>
<author confidence="0.999446">Boris Katz</author>
<author confidence="0.999446">Jimmy</author>
<affiliation confidence="0.996159">MIT Artificial Intelligence</affiliation>
<address confidence="0.979358">200 Technology Cambridge, MA 02139</address>
<email confidence="0.999935">lboris,jimmylinl@ai.mit.edu</email>
<abstract confidence="0.999893961538462">This paper argues that a finite-state language model with a ternary expression representation is currently the most practical and suitable bridge between natural language processing and information retrieval. Despite the theoretical computational inadequacies of finitestate grammars, they are very cost effective (in time and space requirements) and adequate for practical purposes. The ternary expressions that we use are not only linguistically-motivated, but also amenable to rapid large-scale index- EXtracTOR) is an implementation of this model; in one uniform framework, the system provides two separate grammars for extracting arbitrary patterns of text and building ternary expressions from them. These content representational structures serve as the input to our ternary expressions indexer. This approach to natural language information retrieval promises to significantly raise the performance of current systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Question: What do frogs eat? Answer:</title>
<marker></marker>
<rawString>Question: What do frogs eat? Answer:</rawString>
</citation>
<citation valid="false">
<title>(R1) Adult frogs eat mainly insects and other small animals, including earthworms, minnows, and spiders.</title>
<marker></marker>
<rawString>(R1) Adult frogs eat mainly insects and other small animals, including earthworms, minnows, and spiders.</rawString>
</citation>
<citation valid="false">
<title>(R2) Bow-fins eat mainly other fish, frogs, and crayfish. (R3) Most cobras eat many kinds of animals, such as frogs, fishes, birds, and various small mammals</title>
<marker></marker>
<rawString>(R2) Bow-fins eat mainly other fish, frogs, and crayfish. (R3) Most cobras eat many kinds of animals, such as frogs, fishes, birds, and various small mammals</rawString>
</citation>
<citation valid="false">
<title>(R4) One group of South American frogs feeds mainly on other frogs. (R5) Cranes eat a variety of foods, including frogs, fishes, birds, and various small mammals.</title>
<marker></marker>
<rawString>(R4) One group of South American frogs feeds mainly on other frogs. (R5) Cranes eat a variety of foods, including frogs, fishes, birds, and various small mammals.</rawString>
</citation>
<citation valid="false">
<title>(R6) Frogs eat many other animals, including spiders, flies, and worms.</title>
<tech>(R7) ...</tech>
<marker></marker>
<rawString>(R6) Frogs eat many other animals, including spiders, flies, and worms. (R7) ...</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Ravi Sethi</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1988</date>
<booktitle>Compilers - Principles, Techniques, and Tools.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="22809" citStr="Aho et al., 1988" startWordPosition="3414" endWordPosition="3417">pression. A relation rule takes the following form: EntityType :=&gt; &lt;atoml atom2 atom3&gt; ; The Ent ityType is the trigger for the relation, i.e., the rule is applied whenever a string of that type is extracted. The right hand side of the relation rule is the ternary expression to be generated, which is a triple composed of three atoms. Valid atoms are shown in Table 2. They are either string literals or they manipulate the bound variables saved from the extraction process in some manner. 5For an algorithm converting regular expressions to nondeterministic finite-state automata, please refer to (Aho et al., 1988), Chapter 3. 71 Token Description POS This matches any word tagged as the part-of-speech POS. POS [string] This matches a specific word (string) of a specific part-of-speech (POS). EntityType This matches any extracted string of type EntityType. (token° I tokeni I . . • ) This expression matches any one of the alternative tokens given within the parentheses. Matches are attempted in the order in which they are written, e.g., the first token is tried first. Description This modifier matches zero or more occurrences of the previous token. This modifier matches zero or one occurrence of the previ</context>
</contexts>
<marker>Aho, Sethi, Ullman, 1988</marker>
<rawString>Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 1988. Compilers - Principles, Techniques, and Tools. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avi Arampatzis</author>
<author>Th P van der Weide</author>
<author>C H A Koster</author>
<author>P van Bommel</author>
</authors>
<date>1998</date>
<booktitle>Phrasebased information retrieval. Information Processing and Management,</booktitle>
<pages>34--6</pages>
<marker>Arampatzis, van der Weide, Koster, van Bommel, 1998</marker>
<rawString>Avi Arampatzis, Th.P. van der Weide, C.H.A. Koster, and P. van Bommel. 1998. Phrasebased information retrieval. Information Processing and Management, 34(6):693-707, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avi Araznpatzis</author>
<author>Th P van der Weide</author>
<author>C H A Koster</author>
<author>P van Bommel</author>
</authors>
<title>An evaluation of linguistically-motivated indexing schemes.</title>
<date>2000</date>
<booktitle>In Proceedings of BCS-IRSG 2000 Colloquium on IR Research.</booktitle>
<marker>Araznpatzis, van der Weide, Koster, van Bommel, 2000</marker>
<rawString>Avi Araznpatzis, Th.P. van der Weide, C.H.A. Koster, and P. van Bommel. 2000. An evaluation of linguistically-motivated indexing schemes. In Proceedings of BCS-IRSG 2000 Colloquium on IR Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="19990" citStr="Brill, 1992" startWordPosition="2964" endWordPosition="2965">sily be reformulated using ternary expressions. For example, indexing adjacent word pairs consists of indexing adjacent words with the adjacent relation. In fact, all pairs (e.g., adjective-noun, head-modifier) can be reformulated as ternary expressions by assigning a type to the pair. This finer granularity allows the capture of more intricate relations between words in a document. 5 The REXTOR System Using its finite-state language model, the REXTOR System generates a set of ternary expressions that correspond to content of a part-of-speechtagged input document. Currently, the Brill Tagger (Brill, 1992) (with minor postprocessing) is used for the part-of-speech (POS) tagging. The relations construction process consists of two distinct processes, each guided by its own externally specified grammar file. Extraction rules are applied to match arbitrary patterns of text, based either on one of thirty-nine POS tags or on exact words. Whenever an item is extracted, a corresponding relation rule is triggered, which handles the actual generation of the ternary expressions (relations). 41n fact, our first implementation of a ternary expressions indexer used a SQL database. 5.1 Extraction Rules Extrac</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>A note on phrase structure grammars.</title>
<date>1959</date>
<journal>Information and Control,</journal>
<pages>2--393</pages>
<contexts>
<context position="13351" citStr="Chomsky, 1959" startWordPosition="1991" endWordPosition="1992">y the most suitable combination for this task. 4.1 Finite-State Language Model Despite its limitations, a finite-state grammar seems to provide the best natural language model for information retrieval purposes. One of the most notable computational inadequacies of the finite-state model is the absence of a pushdown mechanism to suspend the processing of a constituent at a given level while using the same grammar to process an embedded constituent (Woods, 1970). Due to this inadequacy, certain 69 English constructions, such as center embedding, cannot be described by any finite-state grammar (Chomsky, 1959a; Chomsky, 1959b). However, Church (1980) demonstrated that the finitestate language model is adequate to describe a performance model of language (i.e., constrained by memory, attention, and other realistic limitations) that approximates competence (i.e., language ability under optimal conditions without resource constraints). Many phenomena that cannot be handled by finite-state grammars are awkward from a psycholinguistic point of view, and hence rarely seen. More recently, Pereira and Wright (1991) developed formal methods of approximating context-free grammars with finitestate grairunars</context>
</contexts>
<marker>Chomsky, 1959</marker>
<rawString>Noam Chomsky. 1959a. A note on phrase structure grammars. Information and Control, 2:393-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>On certain formal properties of grammars Information and Control,</title>
<date>1959</date>
<pages>2--137</pages>
<contexts>
<context position="13351" citStr="Chomsky, 1959" startWordPosition="1991" endWordPosition="1992">y the most suitable combination for this task. 4.1 Finite-State Language Model Despite its limitations, a finite-state grammar seems to provide the best natural language model for information retrieval purposes. One of the most notable computational inadequacies of the finite-state model is the absence of a pushdown mechanism to suspend the processing of a constituent at a given level while using the same grammar to process an embedded constituent (Woods, 1970). Due to this inadequacy, certain 69 English constructions, such as center embedding, cannot be described by any finite-state grammar (Chomsky, 1959a; Chomsky, 1959b). However, Church (1980) demonstrated that the finitestate language model is adequate to describe a performance model of language (i.e., constrained by memory, attention, and other realistic limitations) that approximates competence (i.e., language ability under optimal conditions without resource constraints). Many phenomena that cannot be handled by finite-state grammars are awkward from a psycholinguistic point of view, and hence rarely seen. More recently, Pereira and Wright (1991) developed formal methods of approximating context-free grammars with finitestate grairunars</context>
</contexts>
<marker>Chomsky, 1959</marker>
<rawString>Noam Chomsky. 1959b. On certain formal properties of grammars Information and Control, 2:137-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>On memory limitations in natural language processing.</title>
<date>1980</date>
<tech>Technical Report TR-245,</tech>
<institution>MIT Laboratory for Computer Science.</institution>
<contexts>
<context position="13393" citStr="Church (1980)" startWordPosition="1997" endWordPosition="1998">ask. 4.1 Finite-State Language Model Despite its limitations, a finite-state grammar seems to provide the best natural language model for information retrieval purposes. One of the most notable computational inadequacies of the finite-state model is the absence of a pushdown mechanism to suspend the processing of a constituent at a given level while using the same grammar to process an embedded constituent (Woods, 1970). Due to this inadequacy, certain 69 English constructions, such as center embedding, cannot be described by any finite-state grammar (Chomsky, 1959a; Chomsky, 1959b). However, Church (1980) demonstrated that the finitestate language model is adequate to describe a performance model of language (i.e., constrained by memory, attention, and other realistic limitations) that approximates competence (i.e., language ability under optimal conditions without resource constraints). Many phenomena that cannot be handled by finite-state grammars are awkward from a psycholinguistic point of view, and hence rarely seen. More recently, Pereira and Wright (1991) developed formal methods of approximating context-free grammars with finitestate grairunars.3 Thus, for practical purposes, computati</context>
</contexts>
<marker>Church, 1980</marker>
<rawString>Kenneth W. Church. 1980. On memory limitations in natural language processing. Technical Report TR-245, MIT Laboratory for Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Croft</author>
<author>David D Lewis</author>
</authors>
<title>An approach to natural language processing for document retrieval.</title>
<date>1987</date>
<booktitle>In Proceedings of the 10th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-87).</booktitle>
<contexts>
<context position="18537" citStr="Croft and Lewis, 1987" startWordPosition="2748" endWordPosition="2751"> language parsers are still relatively error-prone; indexing incorrect parse trees is a source of performance degradation. Furthermore, matching trees and sub-trees is a computationally intensive task, especially since full linguistic parse trees may be relatively deep. Relations are easier to match because they are typically much simpler than parse trees. For example, the tree [[shiny happy people I [of [Wonderland]]] 70 may be &amp;quot;flattened&amp;quot; into three relations: &lt; shiny describes people &gt; &lt; happy describes people &gt; &lt; people related-to Wonderland &gt; Indexing case frames has also been attempted (Croft and Lewis, 1987; Loper, 2000), but with limited success. Full semantic analysis is still an open research problem, especially in the general domain. Since full semantic analysis cannot be performed without full-text parsing, case frame analysis inherits the unreliability of current parsers. Furthermore, semantic analysis requires extensive knowledge in the lexicon, which is extremely time-intensive to construct. Finally, due to the complex structure of case frames, they are more difficult to store and index than ternary expressions. Since ternary expressions are merely three-place relations, they may be inde</context>
</contexts>
<marker>Croft, Lewis, 1987</marker>
<rawString>Bruce Croft and David D. Lewis. 1987. An approach to natural language processing for document retrieval. In Proceedings of the 10th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-87).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel L Fagan</author>
</authors>
<title>Experiments in Automatic Phrase Indexing for Document Retrieval: A Comparisons of Syntactic and Non-Syntactic Methods.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="8635" citStr="Fagan, 1987" startWordPosition="1288" endWordPosition="1289">ted, expanding START&apos;S knowledge base is relatively timeintensive. REXTOR attempts to eliminate the need for human involvement during content analysis, and also aims to serve as the foundation of a natural language information retrieval system. Ultimately, 68 we hope that REXTOR will serve as a stepping stone towards a comprehensive system capable of providing users with &amp;quot;just the right information&amp;quot; to queries posed in natural language. 3 Previous Work The concept of indexing more than simple keywords is not new; the idea of indexing (parts of) phrases, for example, is more than a decade old (Fagan, 1987). Arampatzis (1998) introduced the phrase retrieval hypothesis, which asserted that phrases are a better indication of document content than keywords. Several researchers have also explored different techniques of linguistic normalization for information retrieval (Strzalkowski et al., 1996; Zhai et al., 1996; Arampatzis et al., 2000). The performance improvements were neither negligible nor dramatic, but despite the lack of any significant breakthroughs, the authors affirmed the potential value of linguisticallymotivated indexing schemes and the advantages they offer over traditional IR. Prev</context>
<context position="10592" citStr="Fagan, 1987" startWordPosition="1566" endWordPosition="1567">xed in the same manner as single words. The treatment of these representational structures using a restrictive bag-of-words paradigm limits the type of queries that may be formulated. For example, treating adjective/noun pairs ([adj., noun]) as lexical atoms renders it impossible to find the equivalent of &amp;quot;all big things,&amp;quot; corresponding to the pair [big, *]. The extraction of these relations from documents has been relatively inefficient and unsystematic. One approach is to first parse the document using a full-text parser, and then extract interesting relations from the resulting parse tree (Fagan, 1987; Grislunan and Sterling, 1993; Loper, 2000). This approach is slow and inefficient because full-text parsing is very time-intensive. Due to current limitations of computational technology, only a small fraction of the information gathered by a full parser can be efficiently indexed. For the most part, relations that can be effectively utilized for information retrieval purposes only occupy a few nodes of a (possibly dense) parse tree; thus, most of the knowledge gathered by the parser is thrown away. Also, extracting non-linguistic relations from parse trees is very difficult; many interestin</context>
</contexts>
<marker>Fagan, 1987</marker>
<rawString>Joel L. Fagan. 1987. Experiments in Automatic Phrase Indexing for Document Retrieval: A Comparisons of Syntactic and Non-Syntactic Methods. Ph.D. thesis, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>New York University: Description of the PROTEUS system as used for MUC-5.</title>
<date>1993</date>
<booktitle>In Proceedings of the 5th Message Understanding Conference (MUC5).</booktitle>
<contexts>
<context position="14625" citStr="Grishman and Sterling, 1993" startWordPosition="2169" endWordPosition="2173">ly simple finite-state grammars can be utilized to adequately model natural language. Empirically, the effectiveness of the finitestate language model has been demonstrated in the Message Understanding Conferences (MUCs), which evaluated information extraction (IE) systems on a variety of domain-specific tasks. The conferences have shown that superficial parsing using finite-state grammars performs better than deep parsing using context-free grammars (at least under the current constraints of technology). The NYU team switched over from a system that performed full parsing (PROTEUS) in MUC-5 (Grishman and Sterling, 1993) to a regular expression matching parser in MUC-6 (Grishman, 1995). Full parsing was slow and error-prone, and the process of building a full syntactic analysis involved relatively unconstrained search which consumed large amounts of both time and space. The longer debug-cycles that resulted from this translated into fewer iterations with which to tune the system within a given amount of time. Furthermore, the complexity of a full context-free grammar contributed to maintenance problems; complex interactions within the grammar prevented rapid updating of the system to handle new constructions.</context>
</contexts>
<marker>Grishman, Sterling, 1993</marker>
<rawString>Ralph Grishman and John Sterling. 1993. New York University: Description of the PROTEUS system as used for MUC-5. In Proceedings of the 5th Message Understanding Conference (MUC5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>The NYU system for MUC-6 or where&apos;s the syntax.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6).</booktitle>
<contexts>
<context position="14691" citStr="Grishman, 1995" startWordPosition="2183" endWordPosition="2184">guage. Empirically, the effectiveness of the finitestate language model has been demonstrated in the Message Understanding Conferences (MUCs), which evaluated information extraction (IE) systems on a variety of domain-specific tasks. The conferences have shown that superficial parsing using finite-state grammars performs better than deep parsing using context-free grammars (at least under the current constraints of technology). The NYU team switched over from a system that performed full parsing (PROTEUS) in MUC-5 (Grishman and Sterling, 1993) to a regular expression matching parser in MUC-6 (Grishman, 1995). Full parsing was slow and error-prone, and the process of building a full syntactic analysis involved relatively unconstrained search which consumed large amounts of both time and space. The longer debug-cycles that resulted from this translated into fewer iterations with which to tune the system within a given amount of time. Furthermore, the complexity of a full context-free grammar contributed to maintenance problems; complex interactions within the grammar prevented rapid updating of the system to handle new constructions. Finite-state grammars have been used to extract entities such as </context>
</contexts>
<marker>Grishman, 1995</marker>
<rawString>Ralph Grishman. 1995. The NYU system for MUC-6 or where&apos;s the syntax. In Proceedings of the 6th Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Heidom</author>
</authors>
<title>Natural language inputs to a simulation programming systems.</title>
<date>1972</date>
<tech>Technical Report NPS-55HD72101A,</tech>
<institution>Naval Postgraduate School.</institution>
<marker>Heidom, 1972</marker>
<rawString>George E. Heidom. 1972. Natural language inputs to a simulation programming systems. Technical Report NPS-55HD72101A, Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Douglas Appelt</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Megumi Kameyama</author>
<author>Mark Stickel</author>
<author>Mabry Tyson</author>
</authors>
<title>FASTUS: A cascaded finite-state transducer for extracting information from natural-language text.</title>
<date>1996</date>
<booktitle>In Roche and Schabes, editors, Finite State Devices for Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="15529" citStr="Hobbs et al., 1996" startWordPosition="2314" endWordPosition="2317">esulted from this translated into fewer iterations with which to tune the system within a given amount of time. Furthermore, the complexity of a full context-free grammar contributed to maintenance problems; complex interactions within the grammar prevented rapid updating of the system to handle new constructions. Finite-state grammars have been used to extract entities such as proper nouns, names, locations, etc., with relatively high precision. To a lesser extent, these grammars have proven to be effective in identifying syntactic constructions such as noun phrases and verb phrases. FASTUS (Hobbs et al., 1996), the most notable of these systems, is modeled after cascaded, nondeterministic finite-state automata. The finite-state transducers are &amp;quot;cascaded&amp;quot; in that they are arranged in 3However, these approximations overgenerate, although in predictable, systematic ways. series; each one maps the output structures from the previous transducer into structures that comprise the input to the next transducer. There are many similarities between information extraction and building effective representational structures for information retrieval. Both tasks involve identifying entities (e.g., phrases) and th</context>
<context position="29823" citStr="Hobbs et al., 1996" startWordPosition="4527" endWordPosition="4530"> is required to determine effective methods of controlling this explosion. • Relative clauses of some types can be identified by a finite-state language model. They may supply additional useful SVO relations for indexing purposes. We believe that future breakthroughs in natural language information retrieval will occur in the generation of meaningful relations. Although the finite-state language model of REXTOR is powerful 73 enough to extract many linguistically interesting constructions, the approach is not fundamentally new. What differentiates our system from previous work such as FASTUS (Hobbs et al., 1996) is that REXTOR not only provides a mechanism for extraction, but also introduces the paradigm of ternary expressions to capture document content for information retrieval. The relations view of natural language documents is highly amenable to integration with information retrieval systems. Through a relations representation, REXTOR is able to distinguish the subtle differences in meaning between the pairs of sentences and phrases given in the introduction: (1) The man ate the dog. &lt; man is-subject-of eat &gt; &lt; dog is-object-of eat &gt; (1&apos;) The dog ate the man. &lt; man is-object-of eat &gt; &lt; dog is-su</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Israel, Kameyama, Stickel, Tyson, 1996</marker>
<rawString>Jerry R. Hobbs, Douglas Appelt, John Bear, David Israel, Megumi Kameyama, Mark Stickel, and Mabry Tyson. 1996. FASTUS: A cascaded finite-state transducer for extracting information from natural-language text. In Roche and Schabes, editors, Finite State Devices for Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
<author>Judith L Kla vans</author>
<author>Evelyne Tzoukermann</author>
</authors>
<title>Expansion of multiword terms for indexing and retrieval using morphology and syntax.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL&apos;97).</booktitle>
<contexts>
<context position="34034" citStr="Jacquemin et al., 1997" startWordPosition="5203" endWordPosition="5206"> reversible syntactic/semantic transformational rules that render explicit the relationship between alternate realizations of the same meaning. For example, a buy expression is semantically equivalent to a sell expression, except the subject and indirect objects are exchanged. Because many verbs can undergo the same alternations, they can in fact be grouped into verb classes, and hence governed by the same S-rules. Thus, S-rules can be viewed as metarules applied over ternary expressions. A similar technique for handling both syntactic and semantic variations can be found in (Grislunan, 1995; Jacquemin et al., 1997). Both utilize metarules (e.g., for passive/active transformation) applied over textual patterns in order to generate and handle variations. 74 Below we present a concrete example of how REXTOR could potentially improve the performance of existing keyword search engines dramatically. We indexed an electronic version of the Worldbook Encyclopedia at the sentence level using the following two techniques: 1. A simple inverted keyword index. All stopwords are thrown out, and all content words are stemmed. Retrieval was performed by matching content words in the query with content words in the ency</context>
</contexts>
<marker>Jacquemin, vans, Tzoukermann, 1997</marker>
<rawString>Christian Jacquemin, Judith L. Kla.vans, and Evelyne Tzoukermann. 1997. Expansion of multiword terms for indexing and retrieval using morphology and syntax. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL&apos;97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Jensen</author>
<author>George E Heidorn</author>
<author>Stephen D Richardson</author>
<author>editors</author>
</authors>
<date>1993</date>
<booktitle>Natural Language Processing: The PLNLP Approach.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="11848" citStr="Jensen et al., 1993" startWordPosition="1756" endWordPosition="1759">w) have no linguistic foundation, e.g., adjacent word pairs. The other approach to extracting relations from text is to build simple filters for every new relation. This approach is unsystematic, and does not allow for rapid addition of new relations to a system. The REXTOR System utilizes an integrated model to systematically extract arbitrary textual patterns and relations (ternary expressions) from documents. The concept of coupling structurebuilding actions with parsing originated with augmented transition networks (ATNs)(Thorne et al., 1968; Woods, 1970). Similarly, PLNLP (Heidorn, 1972; Jensen et al., 1993) is a programming language for writing phrase structure rules that include specific conditions under which the rule can be applied. These rules may also be augmented by structure-building actions that are to be taken when the rule is applied. However, these systems that attempt full-text parsing are less efficient for information retrieval applications due to the long time necessary to generate full linguistic parse trees. REXTOR was designed with a simple language model and an equally simple, yet expressive, representation of &amp;quot;meaning.&amp;quot; 4 Bridging Natural Language and Information Retrieval In</context>
</contexts>
<marker>Jensen, Heidorn, Richardson, editors, 1993</marker>
<rawString>Karen Jensen, George E. Heidorn, and Stephen D. Richardson, editors. 1993. Natural Language Processing: The PLNLP Approach. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Beth Levin</author>
</authors>
<title>Exploiting lexical regularities in designing natural language systems.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (COLING &apos;88).</booktitle>
<contexts>
<context position="33400" citStr="Katz and Levin, 1988" startWordPosition="5107" endWordPosition="5110">, and thus is unable to equate two sentences that have the same meaning, e.g., (7) and (7&apos;). Although it is certainly possible to manually encode such semantic knowledge as extraction and relation rules, this solution is far from elegant. A potential solution to this semantic variations problem is to borrow the solution employed by START. A ternary expression representation of natural language mimics its syntactic organization, and hence sentences that differ in surface form but are close in meaning will not map into the same structure. In order to solve this problem, START deploys &amp;quot;S-rules&amp;quot; (Katz and Levin, 1988), which are reversible syntactic/semantic transformational rules that render explicit the relationship between alternate realizations of the same meaning. For example, a buy expression is semantically equivalent to a sell expression, except the subject and indirect objects are exchanged. Because many verbs can undergo the same alternations, they can in fact be grouped into verb classes, and hence governed by the same S-rules. Thus, S-rules can be viewed as metarules applied over ternary expressions. A similar technique for handling both syntactic and semantic variations can be found in (Grislu</context>
</contexts>
<marker>Katz, Levin, 1988</marker>
<rawString>Boris Katz and Beth Levin. 1988. Exploiting lexical regularities in designing natural language systems. In Proceedings of the 12th International Conference on Computational Linguistics (COLING &apos;88).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
</authors>
<title>A three-step procedure for language generation.</title>
<date>1980</date>
<tech>Technical Report 599,</tech>
<institution>MIT Artificial Intelligence Laboratory.</institution>
<contexts>
<context position="6488" citStr="Katz, 1980" startWordPosition="957" endWordPosition="958">reds of thousands of people all over the world, supplying users with knowledge regarding geography, weather, movies, and many many other areas. Despite the successes of START in serving actual users, its domain of knowledge is relatively small and expanding its knowledge base is a time-consuming task. The goal of REXTOR is to overcome this bottleneck and to provide a general framework for natural-language information retrieval. REXTOR not only draws its inspiration from START (in providing question answering capabilities), but also borrows a simplified form of its representational structures (Katz, 1980; Katz, 1990). The START System (Katz, 1990; Katz, 1997) analyzes English text and builds a knowledge base from information found in the text. The knowledge is expressed in the form of embedded ternary expressions (T-expressions) subject-relationobject triples where the subject and object can themselves be ternary expressions. For example, &amp;quot;The population of Zimbabwe is 11, 044, 147&amp;quot; would be represented as two ternary expressions: [POPULATION-1 IS 11044147] [POPULATION-1 RELATED-TO ZIMBABWE] Experience from START has shown that a robust full-text natural language question-answering system can</context>
</contexts>
<marker>Katz, 1980</marker>
<rawString>Boris Katz. 1980. A three-step procedure for language generation. Technical Report 599, MIT Artificial Intelligence Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
</authors>
<title>Using English for indexing and retrieving.</title>
<date>1990</date>
<booktitle>Artificial Intelligence at MIT: Expanding Frontiers,</booktitle>
<volume>1</volume>
<editor>In P.H. Winston and S.A. Shellard, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6501" citStr="Katz, 1990" startWordPosition="959" endWordPosition="960">sands of people all over the world, supplying users with knowledge regarding geography, weather, movies, and many many other areas. Despite the successes of START in serving actual users, its domain of knowledge is relatively small and expanding its knowledge base is a time-consuming task. The goal of REXTOR is to overcome this bottleneck and to provide a general framework for natural-language information retrieval. REXTOR not only draws its inspiration from START (in providing question answering capabilities), but also borrows a simplified form of its representational structures (Katz, 1980; Katz, 1990). The START System (Katz, 1990; Katz, 1997) analyzes English text and builds a knowledge base from information found in the text. The knowledge is expressed in the form of embedded ternary expressions (T-expressions) subject-relationobject triples where the subject and object can themselves be ternary expressions. For example, &amp;quot;The population of Zimbabwe is 11, 044, 147&amp;quot; would be represented as two ternary expressions: [POPULATION-1 IS 11044147] [POPULATION-1 RELATED-TO ZIMBABWE] Experience from START has shown that a robust full-text natural language question-answering system cannot be realis</context>
</contexts>
<marker>Katz, 1990</marker>
<rawString>Boris Katz. 1990. Using English for indexing and retrieving. In P.H. Winston and S.A. Shellard, editors, Artificial Intelligence at MIT: Expanding Frontiers, volume 1. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
</authors>
<title>Annotating the World Wide Web using natural language.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th RIA0 Conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97).</booktitle>
<contexts>
<context position="6544" citStr="Katz, 1997" startWordPosition="966" endWordPosition="967">ng users with knowledge regarding geography, weather, movies, and many many other areas. Despite the successes of START in serving actual users, its domain of knowledge is relatively small and expanding its knowledge base is a time-consuming task. The goal of REXTOR is to overcome this bottleneck and to provide a general framework for natural-language information retrieval. REXTOR not only draws its inspiration from START (in providing question answering capabilities), but also borrows a simplified form of its representational structures (Katz, 1980; Katz, 1990). The START System (Katz, 1990; Katz, 1997) analyzes English text and builds a knowledge base from information found in the text. The knowledge is expressed in the form of embedded ternary expressions (T-expressions) subject-relationobject triples where the subject and object can themselves be ternary expressions. For example, &amp;quot;The population of Zimbabwe is 11, 044, 147&amp;quot; would be represented as two ternary expressions: [POPULATION-1 IS 11044147] [POPULATION-1 RELATED-TO ZIMBABWE] Experience from START has shown that a robust full-text natural language question-answering system cannot be realistically expected any time soon. Numerous pr</context>
<context position="7770" citStr="Katz, 1997" startWordPosition="1146" endWordPosition="1147">intersentential reference, paraphrasing, summarization, common sense implication, and many more, will take a long time to solve satisfactorily. In order to bypass intractable complexities of language, START uses computer-analyzable natural language annotations, which consist of simplified English sentences and phrases, to describe various information segments (which may be text, images, or even video and other multimedia content). These natural language annotations serve as metadata and inform START regarding the type of questions that a particular information segment is capable of answering (Katz, 1997). By performing retrieval on natural language annotations, the system is able to provide knowledge that it may not be able to analyze itself (either language that is too complex or non-textual segments). Because these annotations must be manually generated, expanding START&apos;S knowledge base is relatively timeintensive. REXTOR attempts to eliminate the need for human involvement during content analysis, and also aims to serve as the foundation of a natural language information retrieval system. Ultimately, 68 we hope that REXTOR will serve as a stepping stone towards a comprehensive system capab</context>
</contexts>
<marker>Katz, 1997</marker>
<rawString>Boris Katz. 1997. Annotating the World Wide Web using natural language. In Proceedings of the 5th RIA0 Conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
</authors>
<title>Applying semantic relation extraction to information retrieval. Master&apos;s thesis,</title>
<date>2000</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="1720" citStr="Loper, 2000" startWordPosition="242" endWordPosition="243">ificantly raise the performance of current systems. 1 Introduction Traditional information retrieval (111,) has been built on the &amp;quot;bag-of-words&amp;quot; assumption, which equates the weighted component keywords of a document with its semantic content. Obviously, a document is much more than the sum of its individual keywords. Although keywords may offer some indication of &amp;quot;meaning,&amp;quot; they alone cannot capture the richness and expressiveness of natural language. Consider the following sets of sentences/phrases that have similar word content, but (dramatically) different meanings:&apos; &apos;Examples taken from (Loper, 2000) (1) The big man ate the dog. (1&apos;) The big dog ate the man. (2) The meaning of life (2&apos;) A meaningful life (3) The bank of the river (3&apos;) The bank near the river Due to the inability of keywords to capture the &amp;quot;meaning&amp;quot; of documents, a traditional information retrieval system (i.e., one using the bag-ofwords paradigm) will suffer from poor precision in response to a user query accurately and precisely formulated in natural language. The application of natural language processing (NLP) terhniques to information retrieval promises to generate representational structures that better capture the s</context>
<context position="10636" citStr="Loper, 2000" startWordPosition="1572" endWordPosition="1573"> treatment of these representational structures using a restrictive bag-of-words paradigm limits the type of queries that may be formulated. For example, treating adjective/noun pairs ([adj., noun]) as lexical atoms renders it impossible to find the equivalent of &amp;quot;all big things,&amp;quot; corresponding to the pair [big, *]. The extraction of these relations from documents has been relatively inefficient and unsystematic. One approach is to first parse the document using a full-text parser, and then extract interesting relations from the resulting parse tree (Fagan, 1987; Grislunan and Sterling, 1993; Loper, 2000). This approach is slow and inefficient because full-text parsing is very time-intensive. Due to current limitations of computational technology, only a small fraction of the information gathered by a full parser can be efficiently indexed. For the most part, relations that can be effectively utilized for information retrieval purposes only occupy a few nodes of a (possibly dense) parse tree; thus, most of the knowledge gathered by the parser is thrown away. Also, extracting non-linguistic relations from parse trees is very difficult; many interesting relations (from an IR point of view) have </context>
<context position="18551" citStr="Loper, 2000" startWordPosition="2752" endWordPosition="2753">till relatively error-prone; indexing incorrect parse trees is a source of performance degradation. Furthermore, matching trees and sub-trees is a computationally intensive task, especially since full linguistic parse trees may be relatively deep. Relations are easier to match because they are typically much simpler than parse trees. For example, the tree [[shiny happy people I [of [Wonderland]]] 70 may be &amp;quot;flattened&amp;quot; into three relations: &lt; shiny describes people &gt; &lt; happy describes people &gt; &lt; people related-to Wonderland &gt; Indexing case frames has also been attempted (Croft and Lewis, 1987; Loper, 2000), but with limited success. Full semantic analysis is still an open research problem, especially in the general domain. Since full semantic analysis cannot be performed without full-text parsing, case frame analysis inherits the unreliability of current parsers. Furthermore, semantic analysis requires extensive knowledge in the lexicon, which is extremely time-intensive to construct. Finally, due to the complex structure of case frames, they are more difficult to store and index than ternary expressions. Since ternary expressions are merely three-place relations, they may be indexed and retrie</context>
</contexts>
<marker>Loper, 2000</marker>
<rawString>Edward Loper. 2000. Applying semantic relation extraction to information retrieval. Master&apos;s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Rebecca Wright</author>
</authors>
<title>Finite-state approximation of phrase structure grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Meeting of the ACL.</booktitle>
<contexts>
<context position="13859" citStr="Pereira and Wright (1991)" startWordPosition="2064" endWordPosition="2067">in 69 English constructions, such as center embedding, cannot be described by any finite-state grammar (Chomsky, 1959a; Chomsky, 1959b). However, Church (1980) demonstrated that the finitestate language model is adequate to describe a performance model of language (i.e., constrained by memory, attention, and other realistic limitations) that approximates competence (i.e., language ability under optimal conditions without resource constraints). Many phenomena that cannot be handled by finite-state grammars are awkward from a psycholinguistic point of view, and hence rarely seen. More recently, Pereira and Wright (1991) developed formal methods of approximating context-free grammars with finitestate grairunars.3 Thus, for practical purposes, computationally simple finite-state grammars can be utilized to adequately model natural language. Empirically, the effectiveness of the finitestate language model has been demonstrated in the Message Understanding Conferences (MUCs), which evaluated information extraction (IE) systems on a variety of domain-specific tasks. The conferences have shown that superficial parsing using finite-state grammars performs better than deep parsing using context-free grammars (at lea</context>
</contexts>
<marker>Pereira, Wright, 1991</marker>
<rawString>Fernando Pereira and Rebecca Wright. 1991. Finite-state approximation of phrase structure grammars. In Proceedings of the 29th Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan F Smeaton</author>
<author>Ruairi O&apos;Donnell</author>
<author>Fergus Kelledy</author>
</authors>
<title>Indexing structures derived from syntax in TREC-3: System description.</title>
<date>1994</date>
<booktitle>In Proceedings of the 3rd Text REtrieval Conference (TREC-3).</booktitle>
<contexts>
<context position="17565" citStr="Smeaton et al., 1994" startWordPosition="2602" endWordPosition="2605">s are compatible with linguistic theory. From a semantic point of view, ternary expressions may be viewed as two-place predicates, and can be manipulated using predicate logic. Finally, ternary expressions are highly amenable to rapid large-scale indexing, which is a necessary prerequisite of information retrieval systems. Although other representational structures (e.g., trees or case frames) may be better adapted for some purposes, they are much more difficult to index and retrieve efficiently due to their size and complexity. In fact, indexing linguistic tree structures has been attempted (Smeaton et al., 1994), with very disappointing results: precision actually decreased due to the inability to handle variations in tree structure (i.e., the same semantic content could be expressed using different syntactic structures), and to the poor quality of the full-text natural language parser, which was also rather slow. Despite recent advances, full-text natural language parsers are still relatively error-prone; indexing incorrect parse trees is a source of performance degradation. Furthermore, matching trees and sub-trees is a computationally intensive task, especially since full linguistic parse trees ma</context>
</contexts>
<marker>Smeaton, O&apos;Donnell, Kelledy, 1994</marker>
<rawString>Alan F. Smeaton, Ruairi O&apos;Donnell, and Fergus Kelledy. 1994. Indexing structures derived from syntax in TREC-3: System description. In Proceedings of the 3rd Text REtrieval Conference (TREC-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Louise Guthrie</author>
<author>Jussi Karlgren</author>
<author>Jim Leistensnider</author>
<author>Fang Lin</author>
<author>Jose PerezCarballo</author>
<author>Troy Straszheim</author>
<author>Jin Wang</author>
<author>Jon Wilding</author>
</authors>
<title>Natural language information retrieval: TREC-5 report.</title>
<date>1996</date>
<booktitle>In Proceedings of the 5th Text REtrieval Conference (TREC-5).</booktitle>
<contexts>
<context position="8926" citStr="Strzalkowski et al., 1996" startWordPosition="1324" endWordPosition="1327">R will serve as a stepping stone towards a comprehensive system capable of providing users with &amp;quot;just the right information&amp;quot; to queries posed in natural language. 3 Previous Work The concept of indexing more than simple keywords is not new; the idea of indexing (parts of) phrases, for example, is more than a decade old (Fagan, 1987). Arampatzis (1998) introduced the phrase retrieval hypothesis, which asserted that phrases are a better indication of document content than keywords. Several researchers have also explored different techniques of linguistic normalization for information retrieval (Strzalkowski et al., 1996; Zhai et al., 1996; Arampatzis et al., 2000). The performance improvements were neither negligible nor dramatic, but despite the lack of any significant breakthroughs, the authors affirmed the potential value of linguisticallymotivated indexing schemes and the advantages they offer over traditional IR. Previous research in linguistically motivated information retrieval concentrated primarily on noun phrases and their attached prepositional phrases. Techniques that involve head/modifier relations have been tried, e.g., indexing adjective/noun and noun/right adjunct pairs (which normalizes vari</context>
</contexts>
<marker>Strzalkowski, Guthrie, Karlgren, Leistensnider, Lin, PerezCarballo, Straszheim, Wang, Wilding, 1996</marker>
<rawString>Tomek Strzalkowski, Louise Guthrie, Jussi Karlgren, Jim Leistensnider, Fang Lin, Jose PerezCarballo, Troy Straszheim, Jin Wang, and Jon Wilding. 1996. Natural language information retrieval: TREC-5 report. In Proceedings of the 5th Text REtrieval Conference (TREC-5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Thorne</author>
<author>P Bratley</author>
<author>H Dewar</author>
</authors>
<title>The syntactic analysis of English by machine.</title>
<date>1968</date>
<booktitle>Machine Intelligence 3.</booktitle>
<editor>In Donald Michie, editor,</editor>
<publisher>Edinburgh University Press.</publisher>
<contexts>
<context position="11779" citStr="Thorne et al., 1968" startWordPosition="1746" endWordPosition="1749"> very difficult; many interesting relations (from an IR point of view) have no linguistic foundation, e.g., adjacent word pairs. The other approach to extracting relations from text is to build simple filters for every new relation. This approach is unsystematic, and does not allow for rapid addition of new relations to a system. The REXTOR System utilizes an integrated model to systematically extract arbitrary textual patterns and relations (ternary expressions) from documents. The concept of coupling structurebuilding actions with parsing originated with augmented transition networks (ATNs)(Thorne et al., 1968; Woods, 1970). Similarly, PLNLP (Heidorn, 1972; Jensen et al., 1993) is a programming language for writing phrase structure rules that include specific conditions under which the rule can be applied. These rules may also be augmented by structure-building actions that are to be taken when the rule is applied. However, these systems that attempt full-text parsing are less efficient for information retrieval applications due to the long time necessary to generate full linguistic parse trees. REXTOR was designed with a simple language model and an equally simple, yet expressive, representation o</context>
</contexts>
<marker>Thorne, Bratley, Dewar, 1968</marker>
<rawString>J. Thorne, P. Bratley, and H. Dewar. 1968. The syntactic analysis of English by machine. In Donald Michie, editor, Machine Intelligence 3. Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>10</issue>
<contexts>
<context position="11793" citStr="Woods, 1970" startWordPosition="1750" endWordPosition="1751"> interesting relations (from an IR point of view) have no linguistic foundation, e.g., adjacent word pairs. The other approach to extracting relations from text is to build simple filters for every new relation. This approach is unsystematic, and does not allow for rapid addition of new relations to a system. The REXTOR System utilizes an integrated model to systematically extract arbitrary textual patterns and relations (ternary expressions) from documents. The concept of coupling structurebuilding actions with parsing originated with augmented transition networks (ATNs)(Thorne et al., 1968; Woods, 1970). Similarly, PLNLP (Heidorn, 1972; Jensen et al., 1993) is a programming language for writing phrase structure rules that include specific conditions under which the rule can be applied. These rules may also be augmented by structure-building actions that are to be taken when the rule is applied. However, these systems that attempt full-text parsing are less efficient for information retrieval applications due to the long time necessary to generate full linguistic parse trees. REXTOR was designed with a simple language model and an equally simple, yet expressive, representation of &amp;quot;meaning.&amp;quot; 4</context>
<context position="13203" citStr="Woods, 1970" startWordPosition="1969" endWordPosition="1970">ructure that is amenable to fast, large-scale indexing. We argue that a finite-state model of natural language with ternary expressions is currently the most suitable combination for this task. 4.1 Finite-State Language Model Despite its limitations, a finite-state grammar seems to provide the best natural language model for information retrieval purposes. One of the most notable computational inadequacies of the finite-state model is the absence of a pushdown mechanism to suspend the processing of a constituent at a given level while using the same grammar to process an embedded constituent (Woods, 1970). Due to this inadequacy, certain 69 English constructions, such as center embedding, cannot be described by any finite-state grammar (Chomsky, 1959a; Chomsky, 1959b). However, Church (1980) demonstrated that the finitestate language model is adequate to describe a performance model of language (i.e., constrained by memory, attention, and other realistic limitations) that approximates competence (i.e., language ability under optimal conditions without resource constraints). Many phenomena that cannot be handled by finite-state grammars are awkward from a psycholinguistic point of view, and hen</context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>William A. Woods. 1970. Transition network grammars for natural language analysis. Communications of the ACM, 13(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>Xiang Tong</author>
<author>Natasa MilicFrayling</author>
<author>David A Evans</author>
</authors>
<title>Evaluation of syntactic phrase indexing — CLARIT NLP track report.</title>
<date>1996</date>
<booktitle>In Proceedings of the 5th Text REtrieval Conference (TREC-5).</booktitle>
<contexts>
<context position="8945" citStr="Zhai et al., 1996" startWordPosition="1328" endWordPosition="1331">stone towards a comprehensive system capable of providing users with &amp;quot;just the right information&amp;quot; to queries posed in natural language. 3 Previous Work The concept of indexing more than simple keywords is not new; the idea of indexing (parts of) phrases, for example, is more than a decade old (Fagan, 1987). Arampatzis (1998) introduced the phrase retrieval hypothesis, which asserted that phrases are a better indication of document content than keywords. Several researchers have also explored different techniques of linguistic normalization for information retrieval (Strzalkowski et al., 1996; Zhai et al., 1996; Arampatzis et al., 2000). The performance improvements were neither negligible nor dramatic, but despite the lack of any significant breakthroughs, the authors affirmed the potential value of linguisticallymotivated indexing schemes and the advantages they offer over traditional IR. Previous research in linguistically motivated information retrieval concentrated primarily on noun phrases and their attached prepositional phrases. Techniques that involve head/modifier relations have been tried, e.g., indexing adjective/noun and noun/right adjunct pairs (which normalizes variants such as &amp;quot;infor</context>
</contexts>
<marker>Zhai, Tong, MilicFrayling, Evans, 1996</marker>
<rawString>Chengxiang Zhai, Xiang Tong, Natasa MilicFrayling, and David A. Evans. 1996. Evaluation of syntactic phrase indexing — CLARIT NLP track report. In Proceedings of the 5th Text REtrieval Conference (TREC-5).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>