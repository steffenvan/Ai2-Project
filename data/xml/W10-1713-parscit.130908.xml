<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.712321">
The RALI Machine Translation System for WMT 2010
</title>
<note confidence="0.67921575">
St´ephane Huet, Julien Bourdaillet, Alexandre Patry and Philippe Langlais
RALI - Universit´e de Montr´eal
C.P. 6128, succursale Centre-ville
H3C 3J7, Montr´eal, Qu´ebec, Canada
</note>
<email confidence="0.995868">
{huetstep,bourdaij,patryale,felipe}@iro.umontreal.ca
</email>
<sectionHeader confidence="0.997344" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998484">
We describe our system for the translation
task of WMT 2010. This system, devel-
oped for the English-French and French-
English directions, is based on Moses and
was trained using only the resources sup-
plied for the workshop. We report exper-
iments to enhance it with out-of-domain
parallel corpora sub-sampling, N-best list
post-processing and a French grammatical
checker.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956470588235">
This paper presents the phrase-based machine
translation system developed at RALI in order
to participate in both the French-English and
English-French translation tasks. In these two
tasks, we used all the corpora supplied for the con-
straint data condition apart from the LDC Giga-
word corpora.
We describe its different components in Sec-
tion 2. Section 3 reports our experiments to sub-
sample the available out-of-domain corpora in or-
der to adapt the translation models to the news
domain. Section 4, dedicated to post-processing,
presents how N-best lists are reranked and how the
French 1-best output is corrected by a grammatical
checker. Section 5 studies how the original source
language of news acts upon translation quality. We
conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.980947" genericHeader="method">
2 System Architecture
</sectionHeader>
<subsectionHeader confidence="0.995121">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999980791666667">
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated
French words starting with a capital letter. We
significantly cleaned up the parallel Giga word
corpus (noted as gw hereafter), keeping 18.1 M
of the original 22.5 M sentence pairs. For exam-
ple, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with
capital letters were removed.
Moreover, training material was tokenized with
the tool provided for the workshop and truecased,
meaning that the words occuring after a strong
punctuation mark were lowercased when they be-
longed to a dictionary of common all-lowercased
forms; the others were left unchanged. In order
to reduce the number of words unknown to the
translation models, all numbers were serialized,
i.e. mapped to a special unique token. The origi-
nal numbers are then placed back in the translation
in the same order as they appear in the source sen-
tence. Since translations are mostly monotonic be-
tween French and English, this simple algorithm
works well most of the time.
</bodyText>
<subsectionHeader confidence="0.994641">
2.2 Language Models
</subsectionHeader>
<bodyText confidence="0.999972095238095">
We trained Kneser-Ney discounted 5-gram lan-
guage models (LMs) on each available corpus us-
ing the SRILM toolkit (Stolcke, 2002). These
LMs were combined through linear interpola-
tion: first, an out-of-domain LM was built from
Europarl, UN and gw; then, this model was
combined with the two in-domain LMs trained
on news-commentary and news.shuffled, which
will be referred to as nc and ns in the remainder
of the article. Weights were fixed by optimizing
the perplexity of a development corpus made of
news-test2008 and news-syscomb2009 texts.
In order to reduce the size of the LMs, we
limited the vocabulary of our models to 1 M
words for English and French. The words of
these vocabularies were selected from the com-
putation of the number of their occurences us-
ing the method proposed by Venkataraman and
Wang (2003). The out-of-vocabulary rate mea-
sured on news-test2009 and news-test2010
with a so-built vocabulary varies between 0.6%
</bodyText>
<page confidence="0.991193">
103
</page>
<note confidence="0.45357">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 103–109,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999892625">
and 0.8 % for both English and French, while it
was between 0.4 % and 0.7 % before the vocabu-
lary was pruned.
To train the LM on the 48 M-sentence English
ns corpus, 32 Gb RAM were required and up to
16 Gb RAM, for the other corpora. To reduce the
memory needs during decoding, LMs were pruned
using the SRILM prune option.
</bodyText>
<subsectionHeader confidence="0.999481">
2.3 Alignment and Translation Models
</subsectionHeader>
<bodyText confidence="0.9998966">
All parallel corpora were aligned with
Giza++ (Och and Ney, 2003). Our transla-
tion models are phrase-based models (PBMs)
built with Moses (Koehn et al., 2007) with the
following non-default settings:
</bodyText>
<listItem confidence="0.936711333333333">
• maximum sentence length of 80 words,
• limit on the number of phrase translations
loaded for each phrase fixed to 30.
</listItem>
<bodyText confidence="0.94267875">
Weights of LM, phrase table and lexicalized
reordering model scores were optimized on the
development corpus thanks to the MERT algo-
rithm (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.939271">
2.4 Experiments
</subsectionHeader>
<bodyText confidence="0.999975294117647">
This section reports experiments done on the
news-test2009 corpus for testing various config-
urations. In these first experiments, we trained
LMs and translation models on the Europarl cor-
pus.
Case We tested two methods to handle case. The
first one lowercases all training data and docu-
ments to translate, while the second one normal-
izes all training data and documents into their nat-
ural case. These two methods require a post-
processing recapitalization but this last step is
more basic for the truecase method. Training mod-
els on lowercased material led to a 23.15 % case-
insensitive BLEU and a 21.61 % case-sensitive
BLEU; from truecased corpora, we obtained a
23.24 % case-insensitive BLEU and a 22.13 %
case-sensitive BLEU. As truecasing induces an in-
crease of the two metrics, we built all our mod-
els in truecase. The results shown in the remain-
der of this paper are reported in terms of case-
insensitive BLEU which showed last year a bet-
ter correlation with human judgments than case-
sensitive BLEU for the two languages we con-
sider (Callison-Burch et al., 2009).
Tokenization Two tokenizers were tested: one
provided for the workshop and another we devel-
oped. They differ mainly in the processing of com-
pound words: our in-house tokenizer splits these
words (e.g. percentage-wise is turned into percent-
age - wise), which improves the lexical coverage of
the models trained on the corpus. This feature
does not exist in the WMT tool. However, us-
ing the WMT tokenizer, we measured a 23.24 %
BLEU, while our in-house tokenizer yielded a
lower BLEU of 22.85 %. Follow these results
prompted us to use the WMT tokenizer.
Serialization In order to test the effect of se-
rialization, i.e. the mapping of all numbers to
a special unique token, we measured the BLEU
score obtained by a PBM trained on Europarl for
English-French, when numbers are left unchanged
(Table 1, line 1) or serialized (line 2). These
results exhibit a slight decrease of BLEU when
serialization is performed. Moreover, if BLEU
is computed using a serialized reference (line 3),
which is equivalent to ignoring deserialization er-
rors, a minor gain of BLEU is observed, which
validates our recovering method. Since resorting
to serialization/deserialization yields comparable
performance to a system not using it, while reduc-
ing the model’s size, we chose to use it.
</bodyText>
<table confidence="0.676771">
BLEU
no serialization 23.24
corpus serialization 23.13
corpus and reference serialization 23.27
</table>
<tableCaption confidence="0.929445">
Table 1: BLEU measured for English-French on
news-test2009 when training on Europarl.
</tableCaption>
<bodyText confidence="0.999647533333334">
LM Table 2 reports the perplexity measured on
news-test2009 for French (column 1) and En-
glish (column 3) LMs learned on different cor-
pora and interpolated using the development cor-
pus. We also provide the BLEU score (column 2)
for English-French obtained from translation mod-
els trained on Europarl and nc. As expected, us-
ing in-domain corpora (line 2) for English-French
led to better results than using out-of-domain data
(line 3). The best perplexities and BLEU score
are obtained when LMs trained on all the available
corpora are combined (line 4). The last three lines
exhibit how LMs perform when they are trained on
in-domain corpora without pruning them. While
the gzipped 5-gram LM (last line) obtained in
</bodyText>
<page confidence="0.995085">
104
</page>
<bodyText confidence="0.9998068">
such a manner occupies 1.4 Gb on hard disk, the
gzipped pruned 5-gram LM (line 4) trained using
all corpora occupies 0.9 Gb and yields the same
BLEU score. This last LM was used in all the ex-
periments reported in the subsequent sections.
</bodyText>
<table confidence="0.756507777777778">
corpora Fr En
ppl BLEU ppl
nc 327 22.44 454
nc + ns 125 25.69 166
Europarl + UN + Gw 156 24.91 225
all corpora 113 26.01 151
nc + ns (3g, unpruned) 138 25.32 -
nc + ns (4g, unpruned) 124 25.86 -
nc + ns (5g, unpruned) 120 26.04 -
</table>
<tableCaption confidence="0.58449">
Table 2: LMs perplexities and BLEU scores mea-
sured on news-test2009. Translation models
used here were trained on nc and Europarl.
</tableCaption>
<sectionHeader confidence="0.98184" genericHeader="method">
3 Domain adaptation
</sectionHeader>
<bodyText confidence="0.999992090909091">
As the only news parallel corpus provided for
the workshop contains 85k sentence pairs, we
must resort to other parallel out-of-domain cor-
pora in order to build reliable translation models.
If in-domain and out-of-domain LMs are usually
mixed with the well-studied interpolation tech-
niques, training translation models from data of
different domains has received less attention (Fos-
ter and Kuhn, 2007; Bertoldi and Federico, 2009).
Therefore, there is still no widely accepted tech-
nique for this last purpose.
</bodyText>
<subsectionHeader confidence="0.999853">
3.1 Effects of the training data size
</subsectionHeader>
<bodyText confidence="0.999076625">
We investigated how increasing training data acts
upon BLEU score. Table 3 shows a high increase
of 2.7 points w.r.t. the use of nc alone (line 1)
when building the phrase table and the reordering
model from nc and either the 1.7 M-sentence-pair
Europarl (line 2) or a 1.7M-sentence-pair cor-
pus extracted from the 3 out-of-domain corpora:
Europarl, UN and Gw (line 3). Training a PBM on
merged parallel corpora is not necessarily the best
way to combine data from different domains. We
repeated 20 times nc before adding it to Europarl
so as to have the same amount of out-of-domain
and in-domain material. This method turned out
to be less successful since it led to a minor 0.15
BLEU decrease (line 4) w.r.t. our previous system.
Following the motto “no data is better than more
</bodyText>
<table confidence="0.999584777777778">
corpora En-*Fr Fr-*En
nc 23.29 23.23
nc +Europarl 26.01 -
nc + 1.7 M random pairs 26.02 26.68
20Xnc + Europarl 25.86 -
nc + 8.7 M pairs (part 0) 26.44 27.65
nc + 8.7 M pairs (part 1) 26.68 27.46
nc + 8.7 M pairs (part 2) 26.54 27.50
3 models merged 26.86 27.56
</table>
<tableCaption confidence="0.927087">
Table 3: BLEU (in %) measured on news-
test2009 for English-French and French-English
</tableCaption>
<bodyText confidence="0.963502464285714">
when translations models and lexicalized reorder-
ing models are built using various amount of data
in addition to nc.
data”, a PBM was built using all the parallel cor-
pora at our disposal. Since the overall parallel sen-
tences were too numerous for our computational
resources to be simultaneously used, we randomly
split out-of-domain corpora into 3 parts of 8.7 M
sentence pairs each and then combined them with
nc. PBMs were trained on each of these parts
(lines 5 to 7), which yields respectively 0.5 and
0.8 BLEU gain for English-French and French-
English w.r.t. the use of 1.7 M out-of-domain sen-
tence pairs. The more significant improvement no-
ticed for the French-English direction is probably
explained by the fact that the French language is
morphologically richer than English. The 3 PBMs
were then combined by merging the 3 phrase ta-
bles. To do so, the 5 phrase table scores computed
by Moses were mixed using the geometric average
and a 6th score was added, which counts the num-
ber of phrase tables where the given phrase pair
occurs. We ended up with a phrase table contain-
ing 623 M entries, only 9 % and 4 % of them being
in 2 and 3 tables respectively. The resulting phrase
table led to a slight improvement of BLEU scores
(last line) w.r.t. the previous models, except for the
model trained on part 0 for French-English.
</bodyText>
<subsectionHeader confidence="0.999405">
3.2 Corpus sub-sampling
</subsectionHeader>
<bodyText confidence="0.999526875">
Whereas using all corpora improves translation
quality, it requires a huge amount of memory and
disk space. We investigate in this section ways to
select sentence pairs among large out-of-domain
corpora.
Unknown words The main interest of adding
new training material relies on the finding of
words missing in the phrase table. According to
</bodyText>
<page confidence="0.996755">
105
</page>
<bodyText confidence="0.9993615">
this principle, nc was extended with new sentence
pairs containing an unknown word (Table 4, line 2)
or a word that belongs to our LM vocabulary and
that occurs less than 3 times in the current cor-
pus (line 3). This resulted in adding 400k pairs
in the first case and 950 k in the second one, with
BLEU scores close or even better than those ob-
tained with 1.7 M.
</bodyText>
<table confidence="0.999501846153846">
corpora En-*Fr Fr-*En
nc + 1.7 M random pairs 26.02 26.68
nc + 400k pairs (occ = 1) 25.67 -
nc + 950k pairs (occ = 3) 26.13 -
nc + Joshua sub-sampling 26.98 27.68
nc + IR (1-g q, w/ repet) 25.81 -
nc + IR (1-g q, no repet) 26.56 27.54
nc + IR (1,2-g q, w/ repet) 26.26 -
nc + IR (1,2-g q, no repet) 26.53 -
nc + 8.7 M pairs 26.68 27.65
+ IR score (1g q, no repet) 26.93 27.65
3 large models merged 26.86 27.56
+ IR score (1g q, no repet) 26.98 27.74
</table>
<tableCaption confidence="0.984835">
Table 4: BLEU measured on news-test2009 for
</tableCaption>
<bodyText confidence="0.985952355263158">
English-French and French-English using transla-
tion models trained on nc and a subset of out-of-
domain corpora.
Unknown n-grams We applied the sub-
sampling method available in the Joshua
toolkit (Li et al., 2009). This method adds a
new sentence pair when it contains new n-grams
(with 1 &lt; n &lt; 12) occurring less than 20 times in
the current corpus, which led us to add 1.5 M pairs
for English-French and 1.4 M for French-English.
A significant improvement of BLEU is observed
using this method (0.8 for English-French and
1.0 for French-English) w.r.t. the use of 1.7 M
randomly selected pairs. However, this method
has the major drawback of needing to build a new
phrase table for each document to translate.
Information retrieval Information retrieval
(IR) methods have been used in the past to sub-
sample parallel corpora (Hildebrand et al., 2005;
L¨u et al., 2007). These studies use sentences
belonging to the development and test corpora as
queries to select the k most similar source sen-
tences in an indexed parallel corpus. The retrieved
sentence pairs constitute a training corpus for
the translation models. In order to alleviate the
fact that a new PBM has to be learned for each
new test corpus, we built queries using sentences
contained in the monolingual ns corpus, leading
to the selection of sentence pairs stylistically
close to those in the news domain. The source
sentences of the three out-of-domain corpora
were indexed using Lemur.1 Two types of queries
were built from ns sentences after removing stop
words: the first one is limited to unigrams, the
second one contains both unigrams and bigrams,
with a weight for bigrams twice as high as for
unigrams. The interest of the latter query type is
based on the hypothesis that bigrams are more
domain-dependent than unigrams. Another choice
that needs to be made when using IR methods is
concerning the retention of redundant sentences
in the final corpus.
Lines 5 to 8 of Table 4 show the results obtained
when sentence pairs were gathered up to the size
of Europarl, i.e. 1.7 M pairs. 10 sentences were
retrieved per query in various configurations: with
or without bigrams inside queries, with or without
duplicate sentence pairs in the training corpus. Re-
sults demonstrate the interest of the approach since
the BLEU scores are close to those obtained us-
ing the previous tested method based on n-grams
of the test data. Taking bigrams into account does
not improve results and adding only once new sen-
tences is more relevant than duplicating them.
Since using all data led to even better perfor-
mances (see last line of Table 3), we used infor-
mation provided by the IR method in the PBMs
trained on nc + 8.7M out-of-domain sentence
pairs or taking into account all the training ma-
terial. To this end, we included a new score in
the phrase tables which is fixed to 1 for entries
that are in the phrase table trained on sentences
retrieved with unigram queries without repetition
(see line 6 of Table 4), and 0 otherwise. Therefore,
this score aims at boosting the weight of phrases
that were found in sentences close to the news do-
main. The results reported in the 4 last lines of Ta-
ble 4 show minor but consistent gains when adding
this score. The outputs of the PBMs trained on
all the training corpus and which obtained the best
BLEU scores on news-test2009 were submitted
as contrastive runs. The two first lines of Table 5
report the results on this years’s test data, when
the score related to the retrieved corpus is incor-
porated or not. These results still exhibit a minor
improvement when adding this score.
</bodyText>
<footnote confidence="0.995164">
1www.lemurproject.org
</footnote>
<page confidence="0.986188">
106
</page>
<table confidence="0.999736333333333">
BLEU En—*Fr TER BLEU Fr—*En TER
BLEU-cased BLEU-cased
PBM 27.5 26.5 62.2 27.8 26.9 61.2
+IR score 27.7 26.6 62.1 28.0 27.0 61.0
+N-best list reranking 27.9 26.8 62.1 28.0 27.0 61.2
+grammatical checker 28.0 26.9 62.0 - - -
</table>
<tableCaption confidence="0.996573">
Table 5: Official results of our system on news-test2010.
</tableCaption>
<sectionHeader confidence="0.951565" genericHeader="method">
4 Post-processing
</sectionHeader>
<subsectionHeader confidence="0.999704">
4.1 N-best List Reranking
</subsectionHeader>
<bodyText confidence="0.999991214285714">
Our best PBM enhanced by IR methods was em-
ployed to generate 500-best lists. These lists were
reranked combining the global decoder score with
the length ratio between source and target sen-
tences, and the proportions of source sentence n-
grams that are in the news monolingual corpora
(with 1 &lt; n &lt; 5). Weights of these 7 scores are
optimized via MERT on news-test2009. Lines 2
and 3 of Table 5 provide the results obtained be-
fore and after N-best list reranking. They show a
tiny gain for all metrics for English-French, while
the results remain constant for French-English.
Nevertheless, we decided to use those translations
for the French-English task as our primary run.
</bodyText>
<subsectionHeader confidence="0.998992">
4.2 Grammatical Checker
</subsectionHeader>
<bodyText confidence="0.999906684210527">
PBM outputs contain a significant number of
grammatical errors, even when LMs are trained
on large data sets. We tested the use of a gram-
matical checker for the French language: Antidote
RX distributed by Druide informatique inc.2 This
software was applied in a systematic way on the
first translation generated after N-best reranking.
Thus, as soon as the software suggests one or sev-
eral choices that it considers as more correct than
the original translation, the first proposal is kept.
The checked translation is our first run for English-
French.
Antidote RX changed at least one word in
26 % of the news-test2010 sentences. The most
frequent type of corrections are agreement errors,
like in the following example where the agreement
between the subject nombre (number) is correctly
made with the adjective coup´e (cut), thanks to the
full syntactic parsing of the French sentence.
</bodyText>
<footnote confidence="0.71444975">
Source: [...] the number of revaccinations could then be
cut [...]
Reranking: [...] le nombre de revaccinations pourrait
2www.druide.com
</footnote>
<subsubsectionHeader confidence="0.232934">
alors ˆetre coup´ees [...]
</subsubsectionHeader>
<bodyText confidence="0.796096">
+Grammatical checker: [...] le nombre de revacci-
nations pourrait alors ˆetre coup´e [...]
The example below exhibits a good decision
made by the grammatical checker on the mood of
the French verb ˆetre (to be).
</bodyText>
<construct confidence="0.437561166666667">
Source: It will be a long time before anything else will be
on offer in Iraq.
Reranking: Il faudra beaucoup de temps avant que tout
le reste sera offert en Irak.
+Grammatical checker: Il faudra beaucoup de temps
avant que tout le reste soit offert en Irak.
</construct>
<bodyText confidence="0.9999132">
A last interesting type of corrected errors con-
cerns negation. Antidote has indeed the capacity
to add the French particle ne when it is missing in
the expressions ne ... pas, ne ... plus, aucun ne, per-
sonne ne or rien ne. The results obtained using the
grammatical checker are reported in the last line
of Table 5. The automatic evaluation shows only a
minor improvement but we expect the changes in-
duced by this tool to be more significant for human
annotators.
</bodyText>
<sectionHeader confidence="0.877643" genericHeader="method">
5 Effects of the Original Source
</sectionHeader>
<subsectionHeader confidence="0.807347">
Language of Articles on Translation
</subsectionHeader>
<bodyText confidence="0.999987133333333">
During our experiments, we found that translation
quality is highly variable depending on the origi-
nal source language of the news sentences. This
phenomenon is correlated to the previous work of
Kurokawa et al. (2009) that showed that whether
or not a piece of text is an original or a trans-
lation has an impact on translation performance.
The main reason that explains our observations
is probably that the topics and the vocabulary of
news originally expressed in languages other than
French and English tend to differ more from those
of the training materials used to train PBM mod-
els for these two languages. In order to take into
account this phenomenon, MERT tuning was re-
peated for each original source language, using the
</bodyText>
<page confidence="0.996359">
107
</page>
<bodyText confidence="0.9986768">
same PBM models trained on all parallel corpora
and incorporating an IR score.
Columns 1 and 3 of Table 5 display the BLEU
measured using our previous global MERT op-
timization made on 2553 sentence pairs, while
columns 2 and 4 show the results obtained when
running MERT on subsets of the development ma-
terial, made of around 700 sentence pairs each.
The BLEU measured on the whole 2010 test set
is reported in the last line. As expected, language-
dependent MERT tends to increase the LM weight
for English and French. However, an absolute
0.35 % BLEU decrease is globally observed for
English-French using this approach and a 0.21 %
improvement for French-English.
</bodyText>
<table confidence="0.998717125">
En-*Fr Fr-*En
MERT global lang dep global lang dep
Cz 21.95 21.45 21.84 21.85
En 30.80 29.84 33.73 35.00
Fr 37.59 36.96 31.59 32.62
De 16.60 16.73 17.41 17.76
Es 24.52 24.45 29.25 28.31
total 27.64 27.39 27.99 28.20
</table>
<tableCaption confidence="0.957667">
Table 6: BLEU scores measured on parts of
</tableCaption>
<bodyText confidence="0.7188865">
news-test2010 according to the original source
language.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999981631578947">
This paper presented our statistical machine trans-
lation system developed for the translation task us-
ing Moses. Our submitted runs were generated
from models trained on all the corpora made avail-
able for the workshop, as this method had pro-
vided the best results in our experiments. This
system was enhanced using IR methods which
exploits news monolingual copora, N-best list
reranking and a French grammatical checker.
This was our first participation where such a
huge amount data was involved. Training models
on so many sentences is challenging from an engi-
neering point of view and requires important com-
putational resources and storage capacities. The
time spent in handling voluminous data prevented
us from testing more approaches. We suggest that
the next edition of the workshop could integrate
a task restraining the number of parameters in the
models trained.
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796055555556">
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In 4th EACL Workshop
on Statistical Machine Translation (WMT), Athens,
Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In 2nd ACL Workshop
on Statistical Machine Translation (WMT), Prague,
Czech Republic.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In 10th conference
of the European Association for Machine Transla-
tion (EAMT), Budapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics (ACL), Companion Vol-
ume, Prague, Czech Republic.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and
its impact on machine translation. In 12th Machine
Translation Summit, Ottawa, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Join Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague, Czech Repub-
lic.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41stAnnual Meet-
ing of the Association for Computational Linguistics
(ACL), Sapporo, Japan.
</reference>
<page confidence="0.986788">
108
</page>
<reference confidence="0.998999875">
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing (ICSLP),
Denver, CO, USA.
Arnand Venkataraman and Wen Wang. 2003. Tech-
niques for effective vocabulary selection. In 8th Eu-
ropean Conference on Speech Communication and
Technology (Eurospeech), Geneva, Switzerland.
</reference>
<page confidence="0.998961">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.276116">
<title confidence="0.644102333333333">The RALI Machine Translation System for WMT 2010 St´ephane Huet, Julien Bourdaillet, Alexandre Patry and Philippe RALI - Universit´e de</title>
<note confidence="0.7687215">C.P. 6128, succursale H3C 3J7, Montr´eal, Qu´ebec,</note>
<abstract confidence="0.988530727272727">We describe our system for the translation task of WMT 2010. This system, developed for the English-French and French- English directions, is based on Moses and was trained using only the resources supplied for the workshop. We report experiments to enhance it with out-of-domain parallel corpora sub-sampling, N-best list post-processing and a French grammatical checker.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In 4th EACL Workshop on Statistical Machine Translation (WMT),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="8866" citStr="Bertoldi and Federico, 2009" startWordPosition="1440" endWordPosition="1443">124 25.86 - nc + ns (5g, unpruned) 120 26.04 - Table 2: LMs perplexities and BLEU scores measured on news-test2009. Translation models used here were trained on nc and Europarl. 3 Domain adaptation As the only news parallel corpus provided for the workshop contains 85k sentence pairs, we must resort to other parallel out-of-domain corpora in order to build reliable translation models. If in-domain and out-of-domain LMs are usually mixed with the well-studied interpolation techniques, training translation models from data of different domains has received less attention (Foster and Kuhn, 2007; Bertoldi and Federico, 2009). Therefore, there is still no widely accepted technique for this last purpose. 3.1 Effects of the training data size We investigated how increasing training data acts upon BLEU score. Table 3 shows a high increase of 2.7 points w.r.t. the use of nc alone (line 1) when building the phrase table and the reordering model from nc and either the 1.7 M-sentence-pair Europarl (line 2) or a 1.7M-sentence-pair corpus extracted from the 3 out-of-domain corpora: Europarl, UN and Gw (line 3). Training a PBM on merged parallel corpora is not necessarily the best way to combine data from different domains.</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In 4th EACL Workshop on Statistical Machine Translation (WMT), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation.</title>
<date>2009</date>
<booktitle>In 4th EACL Workshop on Statistical Machine Translation (WMT),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="5646" citStr="Callison-Burch et al., 2009" startWordPosition="902" endWordPosition="905">recapitalization but this last step is more basic for the truecase method. Training models on lowercased material led to a 23.15 % caseinsensitive BLEU and a 21.61 % case-sensitive BLEU; from truecased corpora, we obtained a 23.24 % case-insensitive BLEU and a 22.13 % case-sensitive BLEU. As truecasing induces an increase of the two metrics, we built all our models in truecase. The results shown in the remainder of this paper are reported in terms of caseinsensitive BLEU which showed last year a better correlation with human judgments than casesensitive BLEU for the two languages we consider (Callison-Burch et al., 2009). Tokenization Two tokenizers were tested: one provided for the workshop and another we developed. They differ mainly in the processing of compound words: our in-house tokenizer splits these words (e.g. percentage-wise is turned into percentage - wise), which improves the lexical coverage of the models trained on the corpus. This feature does not exist in the WMT tool. However, using the WMT tokenizer, we measured a 23.24 % BLEU, while our in-house tokenizer yielded a lower BLEU of 22.85 %. Follow these results prompted us to use the WMT tokenizer. Serialization In order to test the effect of </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In 4th EACL Workshop on Statistical Machine Translation (WMT), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In 2nd ACL Workshop on Statistical Machine Translation (WMT),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8836" citStr="Foster and Kuhn, 2007" startWordPosition="1435" endWordPosition="1439">nc + ns (4g, unpruned) 124 25.86 - nc + ns (5g, unpruned) 120 26.04 - Table 2: LMs perplexities and BLEU scores measured on news-test2009. Translation models used here were trained on nc and Europarl. 3 Domain adaptation As the only news parallel corpus provided for the workshop contains 85k sentence pairs, we must resort to other parallel out-of-domain corpora in order to build reliable translation models. If in-domain and out-of-domain LMs are usually mixed with the well-studied interpolation techniques, training translation models from data of different domains has received less attention (Foster and Kuhn, 2007; Bertoldi and Federico, 2009). Therefore, there is still no widely accepted technique for this last purpose. 3.1 Effects of the training data size We investigated how increasing training data acts upon BLEU score. Table 3 shows a high increase of 2.7 points w.r.t. the use of nc alone (line 1) when building the phrase table and the reordering model from nc and either the 1.7 M-sentence-pair Europarl (line 2) or a 1.7M-sentence-pair corpus extracted from the 3 out-of-domain corpora: Europarl, UN and Gw (line 3). Training a PBM on merged parallel corpora is not necessarily the best way to combin</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In 2nd ACL Workshop on Statistical Machine Translation (WMT), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In 10th conference of the European Association for Machine Translation (EAMT),</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="13517" citStr="Hildebrand et al., 2005" startWordPosition="2280" endWordPosition="2283">adds a new sentence pair when it contains new n-grams (with 1 &lt; n &lt; 12) occurring less than 20 times in the current corpus, which led us to add 1.5 M pairs for English-French and 1.4 M for French-English. A significant improvement of BLEU is observed using this method (0.8 for English-French and 1.0 for French-English) w.r.t. the use of 1.7 M randomly selected pairs. However, this method has the major drawback of needing to build a new phrase table for each document to translate. Information retrieval Information retrieval (IR) methods have been used in the past to subsample parallel corpora (Hildebrand et al., 2005; L¨u et al., 2007). These studies use sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constitute a training corpus for the translation models. In order to alleviate the fact that a new PBM has to be learned for each new test corpus, we built queries using sentences contained in the monolingual ns corpus, leading to the selection of sentence pairs stylistically close to those in the news domain. The source sentences of the three out-of-domain corpora were indexed using L</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In 10th conference of the European Association for Machine Translation (EAMT), Budapest, Hungary.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics (ACL), Companion Volume,</booktitle>
<location>Alexandra</location>
<contexts>
<context position="4251" citStr="Koehn et al., 2007" startWordPosition="667" endWordPosition="670">, pages 103–109, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics and 0.8 % for both English and French, while it was between 0.4 % and 0.7 % before the vocabulary was pruned. To train the LM on the 48 M-sentence English ns corpus, 32 Gb RAM were required and up to 16 Gb RAM, for the other corpora. To reduce the memory needs during decoding, LMs were pruned using the SRILM prune option. 2.3 Alignment and Translation Models All parallel corpora were aligned with Giza++ (Och and Ney, 2003). Our translation models are phrase-based models (PBMs) built with Moses (Koehn et al., 2007) with the following non-default settings: • maximum sentence length of 80 words, • limit on the number of phrase translations loaded for each phrase fixed to 30. Weights of LM, phrase table and lexicalized reordering model scores were optimized on the development corpus thanks to the MERT algorithm (Och, 2003). 2.4 Experiments This section reports experiments done on the news-test2009 corpus for testing various configurations. In these first experiments, we trained LMs and translation models on the Europarl corpus. Case We tested two methods to handle case. The first one lowercases all trainin</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In 45th Annual Meeting of the Association for Computational Linguistics (ACL), Companion Volume, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kurokawa</author>
<author>Cyril Goutte</author>
<author>Pierre Isabelle</author>
</authors>
<title>Automatic detection of translated text and its impact on machine translation.</title>
<date>2009</date>
<booktitle>In 12th Machine Translation</booktitle>
<location>Summit, Ottawa, Canada.</location>
<contexts>
<context position="19578" citStr="Kurokawa et al. (2009)" startWordPosition="3310" endWordPosition="3313">n it is missing in the expressions ne ... pas, ne ... plus, aucun ne, personne ne or rien ne. The results obtained using the grammatical checker are reported in the last line of Table 5. The automatic evaluation shows only a minor improvement but we expect the changes induced by this tool to be more significant for human annotators. 5 Effects of the Original Source Language of Articles on Translation During our experiments, we found that translation quality is highly variable depending on the original source language of the news sentences. This phenomenon is correlated to the previous work of Kurokawa et al. (2009) that showed that whether or not a piece of text is an original or a translation has an impact on translation performance. The main reason that explains our observations is probably that the topics and the vocabulary of news originally expressed in languages other than French and English tend to differ more from those of the training materials used to train PBM models for these two languages. In order to take into account this phenomenon, MERT tuning was repeated for each original source language, using the 107 same PBM models trained on all parallel corpora and incorporating an IR score. Colu</context>
</contexts>
<marker>Kurokawa, Goutte, Isabelle, 2009</marker>
<rawString>David Kurokawa, Cyril Goutte, and Pierre Isabelle. 2009. Automatic detection of translated text and its impact on machine translation. In 12th Machine Translation Summit, Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In 4th EACL Workshop on Statistical Machine Translation (WMT),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="12880" citStr="Li et al., 2009" startWordPosition="2171" endWordPosition="2174">7 - nc + 950k pairs (occ = 3) 26.13 - nc + Joshua sub-sampling 26.98 27.68 nc + IR (1-g q, w/ repet) 25.81 - nc + IR (1-g q, no repet) 26.56 27.54 nc + IR (1,2-g q, w/ repet) 26.26 - nc + IR (1,2-g q, no repet) 26.53 - nc + 8.7 M pairs 26.68 27.65 + IR score (1g q, no repet) 26.93 27.65 3 large models merged 26.86 27.56 + IR score (1g q, no repet) 26.98 27.74 Table 4: BLEU measured on news-test2009 for English-French and French-English using translation models trained on nc and a subset of out-ofdomain corpora. Unknown n-grams We applied the subsampling method available in the Joshua toolkit (Li et al., 2009). This method adds a new sentence pair when it contains new n-grams (with 1 &lt; n &lt; 12) occurring less than 20 times in the current corpus, which led us to add 1.5 M pairs for English-French and 1.4 M for French-English. A significant improvement of BLEU is observed using this method (0.8 for English-French and 1.0 for French-English) w.r.t. the use of 1.7 M randomly selected pairs. However, this method has the major drawback of needing to build a new phrase table for each document to translate. Information retrieval Information retrieval (IR) methods have been used in the past to subsample para</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In 4th EACL Workshop on Statistical Machine Translation (WMT), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan L¨u</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Join Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<location>Prague, Czech Republic.</location>
<marker>L¨u, Huang, Liu, 2007</marker>
<rawString>Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Join Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4158" citStr="Och and Ney, 2003" startWordPosition="652" endWordPosition="655">103 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 103–109, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics and 0.8 % for both English and French, while it was between 0.4 % and 0.7 % before the vocabulary was pruned. To train the LM on the 48 M-sentence English ns corpus, 32 Gb RAM were required and up to 16 Gb RAM, for the other corpora. To reduce the memory needs during decoding, LMs were pruned using the SRILM prune option. 2.3 Alignment and Translation Models All parallel corpora were aligned with Giza++ (Och and Ney, 2003). Our translation models are phrase-based models (PBMs) built with Moses (Koehn et al., 2007) with the following non-default settings: • maximum sentence length of 80 words, • limit on the number of phrase translations loaded for each phrase fixed to 30. Weights of LM, phrase table and lexicalized reordering model scores were optimized on the development corpus thanks to the MERT algorithm (Och, 2003). 2.4 Experiments This section reports experiments done on the news-test2009 corpus for testing various configurations. In these first experiments, we trained LMs and translation models on the Eur</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41stAnnual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4562" citStr="Och, 2003" startWordPosition="720" endWordPosition="721">ther corpora. To reduce the memory needs during decoding, LMs were pruned using the SRILM prune option. 2.3 Alignment and Translation Models All parallel corpora were aligned with Giza++ (Och and Ney, 2003). Our translation models are phrase-based models (PBMs) built with Moses (Koehn et al., 2007) with the following non-default settings: • maximum sentence length of 80 words, • limit on the number of phrase translations loaded for each phrase fixed to 30. Weights of LM, phrase table and lexicalized reordering model scores were optimized on the development corpus thanks to the MERT algorithm (Och, 2003). 2.4 Experiments This section reports experiments done on the news-test2009 corpus for testing various configurations. In these first experiments, we trained LMs and translation models on the Europarl corpus. Case We tested two methods to handle case. The first one lowercases all training data and documents to translate, while the second one normalizes all training data and documents into their natural case. These two methods require a postprocessing recapitalization but this last step is more basic for the truecase method. Training models on lowercased material led to a 23.15 % caseinsensiti</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In 41stAnnual Meeting of the Association for Computational Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In 7th International Conference on Spoken Language Processing (ICSLP),</booktitle>
<location>Denver, CO, USA.</location>
<contexts>
<context position="2729" citStr="Stolcke, 2002" startWordPosition="416" endWordPosition="417">belonged to a dictionary of common all-lowercased forms; the others were left unchanged. In order to reduce the number of words unknown to the translation models, all numbers were serialized, i.e. mapped to a special unique token. The original numbers are then placed back in the translation in the same order as they appear in the source sentence. Since translations are mostly monotonic between French and English, this simple algorithm works well most of the time. 2.2 Language Models We trained Kneser-Ney discounted 5-gram language models (LMs) on each available corpus using the SRILM toolkit (Stolcke, 2002). These LMs were combined through linear interpolation: first, an out-of-domain LM was built from Europarl, UN and gw; then, this model was combined with the two in-domain LMs trained on news-commentary and news.shuffled, which will be referred to as nc and ns in the remainder of the article. Weights were fixed by optimizing the perplexity of a development corpus made of news-test2008 and news-syscomb2009 texts. In order to reduce the size of the LMs, we limited the vocabulary of our models to 1 M words for English and French. The words of these vocabularies were selected from the computation </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In 7th International Conference on Spoken Language Processing (ICSLP), Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnand Venkataraman</author>
<author>Wen Wang</author>
</authors>
<title>Techniques for effective vocabulary selection.</title>
<date>2003</date>
<booktitle>In 8th European Conference on Speech Communication and Technology (Eurospeech),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3420" citStr="Venkataraman and Wang (2003)" startWordPosition="531" endWordPosition="534">an out-of-domain LM was built from Europarl, UN and gw; then, this model was combined with the two in-domain LMs trained on news-commentary and news.shuffled, which will be referred to as nc and ns in the remainder of the article. Weights were fixed by optimizing the perplexity of a development corpus made of news-test2008 and news-syscomb2009 texts. In order to reduce the size of the LMs, we limited the vocabulary of our models to 1 M words for English and French. The words of these vocabularies were selected from the computation of the number of their occurences using the method proposed by Venkataraman and Wang (2003). The out-of-vocabulary rate measured on news-test2009 and news-test2010 with a so-built vocabulary varies between 0.6% 103 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 103–109, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics and 0.8 % for both English and French, while it was between 0.4 % and 0.7 % before the vocabulary was pruned. To train the LM on the 48 M-sentence English ns corpus, 32 Gb RAM were required and up to 16 Gb RAM, for the other corpora. To reduce the memory needs during decoding, LMs were p</context>
</contexts>
<marker>Venkataraman, Wang, 2003</marker>
<rawString>Arnand Venkataraman and Wen Wang. 2003. Techniques for effective vocabulary selection. In 8th European Conference on Speech Communication and Technology (Eurospeech), Geneva, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>