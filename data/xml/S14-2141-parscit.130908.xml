<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047611">
<title confidence="0.9967035">
UTexas: Natural Language Semantics using Distributional Semantics and
Probabilistic Logic
</title>
<author confidence="0.99867">
Islam Beltagy∗, Stephen Roller∗, Gemma Boleda†, Katrin Erk†, Raymond J. Mooney∗
</author>
<affiliation confidence="0.984284333333333">
∗ Department of Computer Science
† Department of Linguistics
The University of Texas at Austin
</affiliation>
<email confidence="0.9808205">
{beltagy, roller, mooney}@cs.utexas.edu
gemma.boleda@upf.edu, katrin.erk@mail.utexas.edu
</email>
<sectionHeader confidence="0.99386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998994">
We represent natural language semantics
by combining logical and distributional in-
formation in probabilistic logic. We use
Markov Logic Networks (MLN) for the
RTE task, and Probabilistic Soft Logic
(PSL) for the STS task. The system is
evaluated on the SICK dataset. Our best
system achieves 73% accuracy on the RTE
task, and a Pearson’s correlation of 0.71 on
the STS task.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999847">
Textual Entailment systems based on logical infer-
ence excel in correct reasoning, but are often brit-
tle due to their inability to handle soft logical in-
ferences. Systems based on distributional seman-
tics excel in lexical and soft reasoning, but are un-
able to handle phenomena like negation and quan-
tifiers. We present a system which takes the best
of both approaches by combining distributional se-
mantics with probabilistic logical inference.
Our system builds on our prior work (Belt-
agy et al., 2013; Beltagy et al., 2014a; Beltagy
and Mooney, 2014; Beltagy et al., 2014b). We
use Boxer (Bos, 2008), a wide-coverage semantic
analysis tool to map natural sentences to logical
form. Then, distributional information is encoded
in the form of inference rules. We generate lexical
and phrasal rules, and experiment with symmetric
and asymmetric similarity measures. Finally, we
use probabilistic logic frameworks to perform in-
ference, Markov Logic Networks (MLN) for RTE,
and Probabilistic Soft Logic (PSL) for STS.
</bodyText>
<footnote confidence="0.5531885">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<sectionHeader confidence="0.98851" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.996995">
2.1 Logical Semantics
</subsectionHeader>
<bodyText confidence="0.999938285714286">
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle “graded” aspects of meaning in language
because they are binary by nature.
</bodyText>
<subsectionHeader confidence="0.992713">
2.2 Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.9999798">
Distributional models use statistics of word co-
occurrences to predict semantic similarity of
words and phrases (Turney and Pantel, 2010;
Mitchell and Lapata, 2010), based on the obser-
vation that semantically similar words occur in
similar contexts. Words are represented as vec-
tors in high dimensional spaces generated from
their contexts. Also, it is possible to compute vec-
tor representations for larger phrases composition-
ally from their parts (Mitchell and Lapata, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010). Distributional similarity is usually a
mixture of semantic relations, but particular asym-
metric similarity measures can, to a certain ex-
tent, predict hypernymy and lexical entailment
distributionally (Kotlerman et al., 2010; Lenci and
Benotto, 2012; Roller et al., 2014). Distribu-
tional models capture the graded nature of mean-
ing, but do not adequately capture logical struc-
ture (Grefenstette, 2013).
</bodyText>
<subsectionHeader confidence="0.999277">
2.3 Markov Logic Network
</subsectionHeader>
<bodyText confidence="0.999252571428571">
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
</bodyText>
<page confidence="0.97968">
796
</page>
<note confidence="0.730234">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796–801,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999978833333333">
MLNs define a probability distribution over pos-
sible worlds, where the probability of a world in-
creases exponentially with the total weight of the
logical clauses that it satisfies. A variety of in-
ference methods for MLNs have been developed,
however, computational overhead is still an issue.
</bodyText>
<subsectionHeader confidence="0.997702">
2.4 Probabilistic Soft Logic
</subsectionHeader>
<bodyText confidence="0.999544545454546">
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms (i.e., atoms without vari-
ables) have soft, continuous truth values on the
interval [0, 1] rather than binary truth values as
used in MLNs and most other probabilistic logics.
Given a set of weighted inference rules, and with
the help of Lukasiewicz’s relaxation of the logical
operators, PSL builds a graphical model defining a
probability distribution over the continuous space
of values of the random variables in the model
(Kimmig et al., 2012). Then, PSL’s MPE infer-
ence (Most Probable Explanation) finds the overall
interpretation with the maximum probability given
a set of evidence. This optimization problem is a
second-order cone program (SOCP) (Kimmig et
al., 2012) and can be solved in polynomial time.
</bodyText>
<subsectionHeader confidence="0.99795">
2.5 Recognizing Textual Entailment
</subsectionHeader>
<bodyText confidence="0.9990045">
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or is not related
(Neutral) to another, the hypothesis.
</bodyText>
<subsectionHeader confidence="0.99842">
2.6 Semantic Textual Similarity
</subsectionHeader>
<bodyText confidence="0.999390428571429">
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system’s output and
gold standard scores.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="method">
3 Approach
</sectionHeader>
<subsectionHeader confidence="0.999786">
3.1 Logical Representation
</subsectionHeader>
<bodyText confidence="0.899762857142857">
The first component in the system is Boxer (Bos,
2008), which maps the input sentences into logical
form, in which the predicates are words in the sen-
tence. For example, the sentence “A man is driving
a car” in logical form is:
Ix, y, z. man(x) n agent(y, x) n drive(y) n
patient(y, z) n car(z)
</bodyText>
<subsectionHeader confidence="0.99926">
3.2 Distributional Representation
</subsectionHeader>
<bodyText confidence="0.995312377777778">
Next, distributional information is encoded in
the form of weighted inference rules connecting
words and phrases of the input sentences T and H.
For example, for sentences T: “A man is driving
a car”, and H: “A guy is driving a vehicle”, we
would like to generate rules like bx. man(x) ==&gt;-
guy(x)Iw1, bx.car(x) ==&gt;- vehicle(x)Iw2, where
w1 and w2 are weights indicating the similarity of
the antecedent and consequent of each rule.
Inferences rules are generated as in Beltagy et
al. (2013). Given two input sentences T and H,
for all pairs (a, b), where a and b are words or
phrases of T and H respectively, generate an infer-
ence rule: a —* b I w, where the rule weight w is
a function of sim(−—*a , —*−b ), and sim is a similarity
measure of the distributional vectors —*−a , —*−b . We
experimented with the symmetric similarity mea-
sure cosine, and asym, the supervised, asymmet-
ric similarity measure of Roller et al. (2014).
The asym measure uses the vector difference
(−—*a −
sifier for distinguishing between four different
word relations: hypernymy, cohyponymy, meron-
omy, and no relation. The model is trained us-
ing the noun-noun subset of the BLESS data set
(Baroni and Lenci, 2011). The final similarity
weight is given by the model’s estimated probabil-
ity that the word relationship is either hypernymy
ormeronomy: asym(−—* a , —*− b ) = P(hyper(a, b))+
P(mero(a, b)).
Distributional representations for words are de-
rived by counting co-occurrences in the ukWaC,
WaCkypedia, BNC and Gigaword corpora. We
use the 2000 most frequent content words as ba-
sis dimensions, and count co-occurrences within
a two word context window. The vector space is
weighted using Positive Pointwise Mutual Infor-
mation.
Phrases are defined in terms of Boxer’s output
to be more than one unary atom sharing the same
variable like “a little kid” (little(k) n kid(k)),
or two unary atoms connected by a relation like
“a man is driving” (man(m) n agent(d, m) n
drive(d)). We compute vector representations of
—*−b ) as features in a logistic regression clas-
</bodyText>
<page confidence="0.855965">
797
</page>
<bodyText confidence="0.9999274">
phrases using vector addition across the compo-
nent predicates. We also tried computing phrase
vectors using component-wise vector multiplica-
tion (Mitchell and Lapata, 2010), but found it per-
formed marginally worse than addition.
</bodyText>
<subsectionHeader confidence="0.999526">
3.3 Probabilistic Logical Inference
</subsectionHeader>
<bodyText confidence="0.999781666666667">
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E, RB).
</bodyText>
<subsectionHeader confidence="0.981763">
3.4 Task 1: RTE using MLNs
</subsectionHeader>
<bodyText confidence="0.999801692307692">
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE classification prob-
lem for the relation between T and H can be
split into two inference tasks. The first is test-
ing if T entails H, Pr(H|T, RB). The second
is testing if the negation of the text -,T entails H,
Pr(H|-,T, RB). In case Pr(H|T, RB) is high,
while Pr(H|-,T, RB) is low, this indicates En-
tails. In case it is the other way around, this in-
dicates Contradicts. If both values are close, this
means T does not affect the probability of H and
indicative of Neutral. We train an SVM classifier
with LibSVM’s default parameters to map the two
probabilities to the final decision.
The MLN implementation we use is
Alchemy (Kok et al., 2005). Queries in Alchemy
can only be ground atoms. However, in our
case the query is a complex formula (H). We
extended Alchemy to calculate probabilities of
queries (Beltagy and Mooney, 2014). Probability
of a formula Q given an MLN K equals the ratio
between the partition function Z of the ground
network of K with and without Q added as a hard
rule (Gogate and Domingos, 2011)
</bodyText>
<equation confidence="0.968449333333333">
Z(K U {(Q, 00)�)
P(Q  |K) = (1)
Z(K)
</equation>
<bodyText confidence="0.999939947368421">
We estimate Z of the ground networks using Sam-
pleSearch (Gogate and Dechter, 2011), an ad-
vanced importance sampling algorithm that is suit-
able for ground networks generated by MLNs.
A general problem with MLN inference is
its computational overhead, especially for the
complex logical formulae generated by our ap-
proach. To make inference faster, we reduce the
size of the ground network through an automatic
type-checking technique proposed in Beltagy and
Mooney (2014). For example, consider the ev-
idence ground atom man(M) denoting that the
constant M is of type man. Then, consider an-
other predicate like car(x). In case there are no in-
ference rule connecting man(x) and car(x), then
we know that M which we know is a man cannot
be a car, so we remove the ground atom car(M)
from the ground network. This technique reduces
the size of the ground network dramatically and
makes inference tractable.
Another problem with MLN inference is that
quantifiers sometimes behave in an undesir-
able way, due to the Domain Closure Assump-
tion (Richardson and Domingos, 2006) that MLNs
make. For example, consider the text-hypothesis
pair: “There is a black bird” and “All birds are
black”, which in logic are T : bird(B)nblack(B)
and H : Vx. bird(x) ==&gt;- black(x). Because of
the Domain Closure Assumption, MLNs conclude
that T entails H because H is true for all constants
in the domain (in this example, the single constant
B). We solve this problem by introducing extra
constants and evidence in the domain. In the ex-
ample above, we introduce evidence of a new bird
bird(D), which prevents the hypothesis from be-
ing true. The full details of the technique of deal-
ing with the domain closure is beyond the scope of
this paper.
</bodyText>
<subsectionHeader confidence="0.913293">
3.5 Task 2: STS using PSL
</subsectionHeader>
<bodyText confidence="0.9999708">
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach for computing similarity between struc-
tured objects. We showed in Beltagy et al. (2014a)
how to perform the STS task using PSL. PSL
does not work “out of the box” for STS, be-
cause Lukasiewicz’s equation for the conjunction
is very restrictive. We address this by replacing
Lukasiewicz’s equation for conjunction with an
averaging equation, then change the optimization
problem and grounding technique accordingly.
For each STS pair of sentences 51, 52, we run
PSL twice, once where E = 51, Q = 52 and an-
other where E = 52, Q = 51, and output the two
scores. The final similarity score is produced from
</bodyText>
<page confidence="0.995401">
798
</page>
<bodyText confidence="0.9984365">
an Additive Regression model with WEKA’s de-
fault parameters trained to map the two PSL scores
to the overall similarity score (Friedman, 1999;
Hall et al., 2009).
</bodyText>
<subsectionHeader confidence="0.9588305">
3.6 Task 3: RTE and STS using Vector
Spaces and Keyword Counts
</subsectionHeader>
<bodyText confidence="0.999965342857143">
As a baseline, we also attempt both the RTE and
STS tasks using only vector representations and
unigram counts. This baseline model uses a super-
vised regressor with features based on vector sim-
ilarity and keyword counts. The same input fea-
tures are used for performing RTE and STS, but a
SVM classifier and Additive Regression model is
trained separately for each task. This baseline is
meant to establish whether the task truly requires
the sophisticated logical inference of MLNs and
PSL, or if merely checking for logical keywords
and textual similarity is sufficient.
The first two features are simply the cosine and
asym similarities between the text and hypothesis,
using vector addition of the unigrams to compute
a single vector for the entire sentence.
We also compute vectors for both the text and
hypothesis using vector addition of the mutually
exclusive unigrams (MEUs). The MEUs are de-
fined as the unigrams of the premise and hypoth-
esis with common unigrams removed. For exam-
ple, if the premise is “A dog chased a cat” and the
hypothesis is “A dog watched a mouse”, the MEUs
are “chased cat” and “watched mouse.” We com-
pute vector addition of the MEUs, and compute
similarity using both the cosine and asym mea-
sures. These form two features for the regressor.
The last feature of the model is a keyword
count. We count how many times 13 different
keywords appear in either the text or the hypoth-
esis. These keywords include negation (no, not,
nobody, etc.) and quantifiers (a, the, some, etc.)
The counts of each keyword form the last 13 fea-
tures as input to the regressor. In total, there are
17 features used in this baseline system.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998867166666667">
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014 (Marelli et al.,
2014a; Marelli et al., 2014b). The dataset is
10,000 pairs of sentences, 5000 training and 5000
for testing. Sentences are annotated for both tasks.
</bodyText>
<table confidence="0.9976756">
SICK-RTE SICK-STS
Baseline 70.0 71.1
MLN/PSL + Cosine 72.8 68.6
MLN/PSL + Asym 73.2 68.9
Ensemble 73.2 71.5
</table>
<tableCaption confidence="0.999791">
Table 1: Test RTE accuracy and STS Correlation.
</tableCaption>
<subsectionHeader confidence="0.993919">
4.1 Systems Compared
</subsectionHeader>
<bodyText confidence="0.998206">
We compare multiple configurations of our proba-
bilistic logic system.
</bodyText>
<listItem confidence="0.984626818181818">
• Baseline: Vector- and keyword-only baseline
described in Section 3.6;
• MLN/PSL + Cosine: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using cosine as a similarity measure;
• MLN/PSL + Asym: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using asym as a similarity measure;
• Ensemble: An ensemble method which uses
all of the features in the above methods as in-
puts for the RTE and STS classifiers.
</listItem>
<subsectionHeader confidence="0.901436">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999925769230769">
Table 1 shows our results on the held-out test set
for SemEval 2014 Task 1.
On the RTE task, we see that both the MLN +
Cosine and MLN + Asym models outperformed
the Baseline, indicating that textual entailment re-
quires real inference to handle negation and quan-
tifiers. The MLN + Asym and Ensemble sys-
tems perform identically on RTE, further suggest-
ing that the logical inference subsumes keyword
detection.
The MLN + Asym system outperforms the
MLN + Cosine system, emphasizing the impor-
tance of asymmetric measures for predicting lex-
ical entailment. Intuitively, this makes perfect
sense: dog entails animal, but not vice versa.
In an error analysis performed on a development
set, we found our RTE system was extremely con-
servative: we rarely confused the Entails and Con-
tradicts classes, indicating we correctly predict the
direction of entailment, but frequently misclassify
examples as Neutral. An examination of these ex-
amples showed the errors were mostly due to miss-
ing or weakly-weighted distributional rules.
On STS, our vector space baseline outperforms
both PSL-based systems, but the ensemble outper-
forms any of its components. This is a testament to
</bodyText>
<page confidence="0.996238">
799
</page>
<bodyText confidence="0.999933380952381">
the power of distributional models in their ability
to predict word and sentence similarity. Surpris-
ingly, we see that the PSL + Asym system slightly
outperforms the PSL + Cosine system. This may
indicate that even in STS, some notion of asymme-
try plays a role, or that annotators may have been
biased by simultaneously annotating both tasks.
As with RTE, the major bottleneck of our system
appears to be the knowledge base, which is built
solely using distributional inference rules.
Results also show that our system’s perfor-
mance is close to the baseline system. One of
the reasons behind that could be that sentences are
not exploiting the full power of logical represen-
tations. On RTE for example, most of the con-
tradicting pairs are two similar sentences with one
of them being negated. This way, the existence
of any negation cue in one of the two sentences is
a strong signal for contradiction, which what the
baseline system does without deeply representing
the semantics of the negation.
</bodyText>
<sectionHeader confidence="0.981376" genericHeader="conclusions">
5 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999986235294118">
We showed how to combine logical and distribu-
tional semantics using probabilistic logic, and how
to perform the RTE and STS tasks using it. The
system is tested on the SICK dataset.
The distributional side can be extended in many
directions. We would like to use longer phrases,
more sophisticated compositionality techniques,
and contextualized vectors of word meaning. We
also believe inference rules could be dramatically
improved by integrating from paraphrases collec-
tions like PPDB (Ganitkevitch et al., 2013).
Finally, MLN inference could be made more ef-
ficient by exploiting the similarities between the
two ground networks (the one with Q and the one
without). PLS inference could be enhanced by us-
ing a learned, weighted average of rules, rather
than the simple mean.
</bodyText>
<sectionHeader confidence="0.99497" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998489375">
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Some experiments were run on the Mastodon
Cluster supported by NSF Grant EIA-0303609.
The authors acknowledge the Texas Advanced
Computing Center (TACC)1 for providing grid re-
sources that have contributed to these results. We
thank the anonymous reviewers and the UTexas
</bodyText>
<footnote confidence="0.910888">
1http://www.tacc.utexas.edu
</footnote>
<bodyText confidence="0.922928">
Natural Language and Learning group for their
helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.986163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99899908">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1–10, Edinburgh, UK, July. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy and Raymond J. Mooney. 2014. Ef-
ficient Markov logic inference for natural language
semantics. In Proceedings of AAAI 2014 Workshop
on Statistical Relational AI (StarAI-14).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014a. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014b. Semantic parsing using distributional se-
mantics and probabilistic logic. In Proceedings
of ACL 2014 Workshop on Semantic Parsing (SP-
2014).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
J.H. Friedman. 1999. Stochastic gradient boosting.
Technical report, Stanford University.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Vibhav Gogate and Rina Dechter. 2011. Sample-
search: Importance sampling in presence of deter-
minism. Artificial Intelligence, 175(2):694–729.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In 27th Conference on Un-
certainty in Artificial Intelligence (UAI-11).
</reference>
<page confidence="0.997912">
800
</page>
<bodyText confidence="0.9424131">
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(3):1388–1429.
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373–398.
</bodyText>
<reference confidence="0.999838758064516">
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Stanley Kok, Parag Singla, Matthew Richardson, and
Pedro Domingos. 2005. The Alchemy system
for statistical relational AI. Technical report, De-
partment of Computer Science and Engineering,
University of Washington. http://www.cs.
washington.edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation
of compositional distributional semantic models.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Hrafn Loftsson, Bente
Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), Reykjavik, Ice-
land, may. European Language Resources Associa-
tion (ELRA).
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107–136.
Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of the Twenty
Fifth International Conference on Computational
Linguistics (COLING-14), Dublin, Ireland.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.
</reference>
<page confidence="0.998306">
801
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.202912">
<title confidence="0.9970075">UTexas: Natural Language Semantics using Distributional Semantics and Probabilistic Logic</title>
<author confidence="0.997564">Stephen Gemma Katrin Raymond J</author>
<affiliation confidence="0.826779">of Computer</affiliation>
<abstract confidence="0.406417333333333">of The University of Texas at roller,</abstract>
<email confidence="0.997231">gemma.boleda@upf.edu,katrin.erk@mail.utexas.edu</email>
<abstract confidence="0.988131">We represent natural language semantics by combining logical and distributional information in probabilistic logic. We use Markov Logic Networks (MLN) for the RTE task, and Probabilistic Soft Logic (PSL) for the STS task. The system is evaluated on the SICK dataset. Our best system achieves 73% accuracy on the RTE task, and a Pearson’s correlation of 0.71 on the STS task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of Semantic Evaluation (SemEval-12).</booktitle>
<contexts>
<context position="5595" citStr="Agirre et al., 2012" startWordPosition="838" endWordPosition="841">Explanation) finds the overall interpretation with the maximum probability given a set of evidence. This optimization problem is a second-order cone program (SOCP) (Kimmig et al., 2012) and can be solved in polynomial time. 2.5 Recognizing Textual Entailment Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or is not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 Approach 3.1 Logical Representation The first component in the system is Boxer (Bos, 2008), which maps the input sentences into logical form, in which the predicates are words in the sentence. For example, the sentence “A man is driving a car” in logical form is: Ix, y, z. man(x) n agent(y, x) n drive(y) n patient(y, z) n car(z) 3.2 Distributional Representation Next, distributional information is encoded in the form o</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of Semantic Evaluation (SemEval-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="7328" citStr="Baroni and Lenci, 2011" startWordPosition="1136" endWordPosition="1139"> words or phrases of T and H respectively, generate an inference rule: a —* b I w, where the rule weight w is a function of sim(−—*a , —*−b ), and sim is a similarity measure of the distributional vectors —*−a , —*−b . We experimented with the symmetric similarity measure cosine, and asym, the supervised, asymmetric similarity measure of Roller et al. (2014). The asym measure uses the vector difference (−—*a − sifier for distinguishing between four different word relations: hypernymy, cohyponymy, meronomy, and no relation. The model is trained using the noun-noun subset of the BLESS data set (Baroni and Lenci, 2011). The final similarity weight is given by the model’s estimated probability that the word relationship is either hypernymy ormeronomy: asym(−—* a , —*− b ) = P(hyper(a, b))+ P(mero(a, b)). Distributional representations for words are derived by counting co-occurrences in the ukWaC, WaCkypedia, BNC and Gigaword corpora. We use the 2000 most frequent content words as basis dimensions, and count co-occurrences within a two word context window. The vector space is weighted using Positive Pointwise Mutual Information. Phrases are defined in terms of Boxer’s output to be more than one unary atom sha</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1–10, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-10).</booktitle>
<contexts>
<context position="2890" citStr="Baroni and Zamparelli, 2010" startWordPosition="423" endWordPosition="427">d” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode c</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Raymond J Mooney</author>
</authors>
<title>Efficient Markov logic inference for natural language semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of AAAI 2014 Workshop on Statistical Relational AI (StarAI-14).</booktitle>
<contexts>
<context position="1302" citStr="Beltagy and Mooney, 2014" startWordPosition="193" endWordPosition="196"> task, and a Pearson’s correlation of 0.71 on the STS task. 1 Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by t</context>
<context position="9913" citStr="Beltagy and Mooney, 2014" startWordPosition="1576" endWordPosition="1579">H, Pr(H|-,T, RB). In case Pr(H|T, RB) is high, while Pr(H|-,T, RB) is low, this indicates Entails. In case it is the other way around, this indicates Contradicts. If both values are close, this means T does not affect the probability of H and indicative of Neutral. We train an SVM classifier with LibSVM’s default parameters to map the two probabilities to the final decision. The MLN implementation we use is Alchemy (Kok et al., 2005). Queries in Alchemy can only be ground atoms. However, in our case the query is a complex formula (H). We extended Alchemy to calculate probabilities of queries (Beltagy and Mooney, 2014). Probability of a formula Q given an MLN K equals the ratio between the partition function Z of the ground network of K with and without Q added as a hard rule (Gogate and Domingos, 2011) Z(K U {(Q, 00)�) P(Q |K) = (1) Z(K) We estimate Z of the ground networks using SampleSearch (Gogate and Dechter, 2011), an advanced importance sampling algorithm that is suitable for ground networks generated by MLNs. A general problem with MLN inference is its computational overhead, especially for the complex logical formulae generated by our approach. To make inference faster, we reduce the size of the gr</context>
</contexts>
<marker>Beltagy, Mooney, 2014</marker>
<rawString>Islam Beltagy and Raymond J. Mooney. 2014. Efficient Markov logic inference for natural language semantics. In Proceedings of AAAI 2014 Workshop on Statistical Relational AI (StarAI-14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Montague meets Markov: Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM-13).</booktitle>
<contexts>
<context position="1253" citStr="Beltagy et al., 2013" startWordPosition="184" endWordPosition="188"> best system achieves 73% accuracy on the RTE task, and a Pearson’s correlation of 0.71 on the STS task. 1 Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. P</context>
<context position="6629" citStr="Beltagy et al. (2013)" startWordPosition="1010" endWordPosition="1013">g a car” in logical form is: Ix, y, z. man(x) n agent(y, x) n drive(y) n patient(y, z) n car(z) 3.2 Distributional Representation Next, distributional information is encoded in the form of weighted inference rules connecting words and phrases of the input sentences T and H. For example, for sentences T: “A man is driving a car”, and H: “A guy is driving a vehicle”, we would like to generate rules like bx. man(x) ==&gt;- guy(x)Iw1, bx.car(x) ==&gt;- vehicle(x)Iw2, where w1 and w2 are weights indicating the similarity of the antecedent and consequent of each rule. Inferences rules are generated as in Beltagy et al. (2013). Given two input sentences T and H, for all pairs (a, b), where a and b are words or phrases of T and H respectively, generate an inference rule: a —* b I w, where the rule weight w is a function of sim(−—*a , —*−b ), and sim is a similarity measure of the distributional vectors —*−a , —*−b . We experimented with the symmetric similarity measure cosine, and asym, the supervised, asymmetric similarity measure of Roller et al. (2014). The asym measure uses the vector difference (−—*a − sifier for distinguishing between four different word relations: hypernymy, cohyponymy, meronomy, and no relat</context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. Montague meets Markov: Deep semantics with probabilistic logical form. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Probabilistic soft logic for semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL-14).</booktitle>
<contexts>
<context position="1275" citStr="Beltagy et al., 2014" startWordPosition="189" endWordPosition="192">73% accuracy on the RTE task, and a Pearson’s correlation of 0.71 on the STS task. 1 Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and procee</context>
<context position="12077" citStr="Beltagy et al. (2014" startWordPosition="1955" endWordPosition="1958">entails H because H is true for all constants in the domain (in this example, the single constant B). We solve this problem by introducing extra constants and evidence in the domain. In the example above, we introduce evidence of a new bird bird(D), which prevents the hypothesis from being true. The full details of the technique of dealing with the domain closure is beyond the scope of this paper. 3.5 Task 2: STS using PSL PSL is the probabilistic logic we use for the STS task since it has been shown to be an effective approach for computing similarity between structured objects. We showed in Beltagy et al. (2014a) how to perform the STS task using PSL. PSL does not work “out of the box” for STS, because Lukasiewicz’s equation for the conjunction is very restrictive. We address this by replacing Lukasiewicz’s equation for conjunction with an averaging equation, then change the optimization problem and grounding technique accordingly. For each STS pair of sentences 51, 52, we run PSL twice, once where E = 51, Q = 52 and another where E = 52, Q = 51, and output the two scores. The final similarity score is produced from 798 an Additive Regression model with WEKA’s default parameters trained to map the t</context>
</contexts>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Katrin Erk, and Raymond Mooney. 2014a. Probabilistic soft logic for semantic textual similarity. In Proceedings of Association for Computational Linguistics (ACL-14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Semantic parsing using distributional semantics and probabilistic logic.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014 Workshop on Semantic Parsing (SP2014).</booktitle>
<contexts>
<context position="1275" citStr="Beltagy et al., 2014" startWordPosition="189" endWordPosition="192">73% accuracy on the RTE task, and a Pearson’s correlation of 0.71 on the STS task. 1 Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and procee</context>
<context position="12077" citStr="Beltagy et al. (2014" startWordPosition="1955" endWordPosition="1958">entails H because H is true for all constants in the domain (in this example, the single constant B). We solve this problem by introducing extra constants and evidence in the domain. In the example above, we introduce evidence of a new bird bird(D), which prevents the hypothesis from being true. The full details of the technique of dealing with the domain closure is beyond the scope of this paper. 3.5 Task 2: STS using PSL PSL is the probabilistic logic we use for the STS task since it has been shown to be an effective approach for computing similarity between structured objects. We showed in Beltagy et al. (2014a) how to perform the STS task using PSL. PSL does not work “out of the box” for STS, because Lukasiewicz’s equation for the conjunction is very restrictive. We address this by replacing Lukasiewicz’s equation for conjunction with an averaging equation, then change the optimization problem and grounding technique accordingly. For each STS pair of sentences 51, 52, we run PSL twice, once where E = 51, Q = 52 and another where E = 52, Q = 51, and output the two scores. The final similarity score is produced from 798 an Additive Regression model with WEKA’s default parameters trained to map the t</context>
</contexts>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Katrin Erk, and Raymond Mooney. 2014b. Semantic parsing using distributional semantics and probabilistic logic. In Proceedings of ACL 2014 Workshop on Semantic Parsing (SP2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer.</title>
<date>2008</date>
<booktitle>In Proceedings of Semantics in Text Processing (STEP-08).</booktitle>
<contexts>
<context position="1352" citStr="Bos, 2008" startWordPosition="204" endWordPosition="205">troduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecom</context>
<context position="5863" citStr="Bos, 2008" startWordPosition="880" endWordPosition="881"> Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or is not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 Approach 3.1 Logical Representation The first component in the system is Boxer (Bos, 2008), which maps the input sentences into logical form, in which the predicates are words in the sentence. For example, the sentence “A man is driving a car” in logical form is: Ix, y, z. man(x) n agent(y, x) n drive(y) n patient(y, z) n car(z) 3.2 Distributional Representation Next, distributional information is encoded in the form of weighted inference rules connecting words and phrases of the input sentences T and H. For example, for sentences T: “A man is driving a car”, and H: “A guy is driving a vehicle”, we would like to generate rules like bx. man(x) ==&gt;- guy(x)Iw1, bx.car(x) ==&gt;- vehicle(</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with Boxer. In Proceedings of Semantics in Text Processing (STEP-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Friedman</author>
</authors>
<title>Stochastic gradient boosting.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="12738" citStr="Friedman, 1999" startWordPosition="2073" endWordPosition="2074">oes not work “out of the box” for STS, because Lukasiewicz’s equation for the conjunction is very restrictive. We address this by replacing Lukasiewicz’s equation for conjunction with an averaging equation, then change the optimization problem and grounding technique accordingly. For each STS pair of sentences 51, 52, we run PSL twice, once where E = 51, Q = 52 and another where E = 52, Q = 51, and output the two scores. The final similarity score is produced from 798 an Additive Regression model with WEKA’s default parameters trained to map the two PSL scores to the overall similarity score (Friedman, 1999; Hall et al., 2009). 3.6 Task 3: RTE and STS using Vector Spaces and Keyword Counts As a baseline, we also attempt both the RTE and STS tasks using only vector representations and unigram counts. This baseline model uses a supervised regressor with features based on vector similarity and keyword counts. The same input features are used for performing RTE and STS, but a SVM classifier and Additive Regression model is trained separately for each task. This baseline is meant to establish whether the task truly requires the sophisticated logical inference of MLNs and PSL, or if merely checking fo</context>
</contexts>
<marker>Friedman, 1999</marker>
<rawString>J.H. Friedman. 1999. Stochastic gradient boosting. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13).</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vibhav Gogate</author>
<author>Rina Dechter</author>
</authors>
<title>Samplesearch: Importance sampling in presence of determinism.</title>
<date>2011</date>
<journal>Artificial Intelligence,</journal>
<volume>175</volume>
<issue>2</issue>
<contexts>
<context position="10220" citStr="Gogate and Dechter, 2011" startWordPosition="1635" endWordPosition="1638">default parameters to map the two probabilities to the final decision. The MLN implementation we use is Alchemy (Kok et al., 2005). Queries in Alchemy can only be ground atoms. However, in our case the query is a complex formula (H). We extended Alchemy to calculate probabilities of queries (Beltagy and Mooney, 2014). Probability of a formula Q given an MLN K equals the ratio between the partition function Z of the ground network of K with and without Q added as a hard rule (Gogate and Domingos, 2011) Z(K U {(Q, 00)�) P(Q |K) = (1) Z(K) We estimate Z of the ground networks using SampleSearch (Gogate and Dechter, 2011), an advanced importance sampling algorithm that is suitable for ground networks generated by MLNs. A general problem with MLN inference is its computational overhead, especially for the complex logical formulae generated by our approach. To make inference faster, we reduce the size of the ground network through an automatic type-checking technique proposed in Beltagy and Mooney (2014). For example, consider the evidence ground atom man(M) denoting that the constant M is of type man. Then, consider another predicate like car(x). In case there are no inference rule connecting man(x) and car(x),</context>
</contexts>
<marker>Gogate, Dechter, 2011</marker>
<rawString>Vibhav Gogate and Rina Dechter. 2011. Samplesearch: Importance sampling in presence of determinism. Artificial Intelligence, 175(2):694–729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vibhav Gogate</author>
<author>Pedro Domingos</author>
</authors>
<title>Probabilistic theorem proving.</title>
<date>2011</date>
<booktitle>In 27th Conference on Uncertainty in Artificial Intelligence (UAI-11).</booktitle>
<contexts>
<context position="10101" citStr="Gogate and Domingos, 2011" startWordPosition="1612" endWordPosition="1615">, this means T does not affect the probability of H and indicative of Neutral. We train an SVM classifier with LibSVM’s default parameters to map the two probabilities to the final decision. The MLN implementation we use is Alchemy (Kok et al., 2005). Queries in Alchemy can only be ground atoms. However, in our case the query is a complex formula (H). We extended Alchemy to calculate probabilities of queries (Beltagy and Mooney, 2014). Probability of a formula Q given an MLN K equals the ratio between the partition function Z of the ground network of K with and without Q added as a hard rule (Gogate and Domingos, 2011) Z(K U {(Q, 00)�) P(Q |K) = (1) Z(K) We estimate Z of the ground networks using SampleSearch (Gogate and Dechter, 2011), an advanced importance sampling algorithm that is suitable for ground networks generated by MLNs. A general problem with MLN inference is its computational overhead, especially for the complex logical formulae generated by our approach. To make inference faster, we reduce the size of the ground network through an automatic type-checking technique proposed in Beltagy and Mooney (2014). For example, consider the evidence ground atom man(M) denoting that the constant M is of ty</context>
</contexts>
<marker>Gogate, Domingos, 2011</marker>
<rawString>Vibhav Gogate and Pedro Domingos. 2011. Probabilistic theorem proving. In 27th Conference on Uncertainty in Artificial Intelligence (UAI-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12758" citStr="Hall et al., 2009" startWordPosition="2075" endWordPosition="2078">t of the box” for STS, because Lukasiewicz’s equation for the conjunction is very restrictive. We address this by replacing Lukasiewicz’s equation for conjunction with an averaging equation, then change the optimization problem and grounding technique accordingly. For each STS pair of sentences 51, 52, we run PSL twice, once where E = 51, Q = 52 and another where E = 52, Q = 51, and output the two scores. The final similarity score is produced from 798 an Additive Regression model with WEKA’s default parameters trained to map the two PSL scores to the overall similarity score (Friedman, 1999; Hall et al., 2009). 3.6 Task 3: RTE and STS using Vector Spaces and Keyword Counts As a baseline, we also attempt both the RTE and STS tasks using only vector representations and unigram counts. This baseline model uses a supervised regressor with features based on vector similarity and keyword counts. The same input features are used for performing RTE and STS, but a SVM classifier and Additive Regression model is trained separately for each task. This baseline is meant to establish whether the task truly requires the sophisticated logical inference of MLNs and PSL, or if merely checking for logical keywords a</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<date>1993</date>
<booktitle>From Discourse to Logic.</booktitle>
<publisher>Kluwer.</publisher>
<contexts>
<context position="2112" citStr="Kamp and Reyle, 1993" startWordPosition="308" endWordPosition="311">of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is </context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Kimmig</author>
<author>Stephen H Bach</author>
<author>Matthias Broecheler</author>
<author>Bert Huang</author>
<author>Lise Getoor</author>
</authors>
<title>A short introduction to Probabilistic Soft Logic.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12).</booktitle>
<contexts>
<context position="4228" citStr="Kimmig et al., 2012" startWordPosition="623" endWordPosition="627">em compared to hard logical constraints. 796 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796–801, Dublin, Ireland, August 23-24, 2014. MLNs define a probability distribution over possible worlds, where the probability of a world increases exponentially with the total weight of the logical clauses that it satisfies. A variety of inference methods for MLNs have been developed, however, computational overhead is still an issue. 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is another recently proposed framework for probabilistic logic (Kimmig et al., 2012). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing feature of PSL is that ground atoms (i.e., atoms without variables) have soft, continuous truth values on the interval [0, 1] rather than binary truth values as used in MLNs and most other probabilistic logics. Given a set of weighted inference rules, and with the help of Lukasiewicz’s relaxation of the logical operators, PSL builds a graphical model defining a probability dis</context>
</contexts>
<marker>Kimmig, Bach, Broecheler, Huang, Getoor, 2012</marker>
<rawString>Angelika Kimmig, Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. 2012. A short introduction to Probabilistic Soft Logic. In Proceedings of NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Parag Singla</author>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>The Alchemy system for statistical relational AI.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science and Engineering, University of Washington.</institution>
<note>http://www.cs. washington.edu/ai/alchemy.</note>
<contexts>
<context position="9725" citStr="Kok et al., 2005" startWordPosition="1545" endWordPosition="1548">the relation between T and H can be split into two inference tasks. The first is testing if T entails H, Pr(H|T, RB). The second is testing if the negation of the text -,T entails H, Pr(H|-,T, RB). In case Pr(H|T, RB) is high, while Pr(H|-,T, RB) is low, this indicates Entails. In case it is the other way around, this indicates Contradicts. If both values are close, this means T does not affect the probability of H and indicative of Neutral. We train an SVM classifier with LibSVM’s default parameters to map the two probabilities to the final decision. The MLN implementation we use is Alchemy (Kok et al., 2005). Queries in Alchemy can only be ground atoms. However, in our case the query is a complex formula (H). We extended Alchemy to calculate probabilities of queries (Beltagy and Mooney, 2014). Probability of a formula Q given an MLN K equals the ratio between the partition function Z of the ground network of K with and without Q added as a hard rule (Gogate and Domingos, 2011) Z(K U {(Q, 00)�) P(Q |K) = (1) Z(K) We estimate Z of the ground networks using SampleSearch (Gogate and Dechter, 2011), an advanced importance sampling algorithm that is suitable for ground networks generated by MLNs. A gen</context>
</contexts>
<marker>Kok, Singla, Richardson, Domingos, 2005</marker>
<rawString>Stanley Kok, Parag Singla, Matthew Richardson, and Pedro Domingos. 2005. The Alchemy system for statistical relational AI. Technical report, Department of Computer Science and Engineering, University of Washington. http://www.cs. washington.edu/ai/alchemy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="3115" citStr="Kotlerman et al., 2010" startWordPosition="455" endWordPosition="458">10; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. 796 Proceedings of the 8th International Workshop on Semantic Eval</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the first Joint Conference on Lexical and Computational Semantics (*SEM-12).</booktitle>
<contexts>
<context position="3140" citStr="Lenci and Benotto, 2012" startWordPosition="459" endWordPosition="462"> 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. 796 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pa</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of the first Joint Conference on Lexical and Computational Semantics (*SEM-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="14625" citStr="Marelli et al., 2014" startWordPosition="2395" endWordPosition="2398">ilarity using both the cosine and asym measures. These form two features for the regressor. The last feature of the model is a keyword count. We count how many times 13 different keywords appear in either the text or the hypothesis. These keywords include negation (no, not, nobody, etc.) and quantifiers (a, the, some, etc.) The counts of each keyword form the last 13 features as input to the regressor. In total, there are 17 features used in this baseline system. 4 Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. SICK-RTE SICK-STS Baseline 70.0 71.1 MLN/PSL + Cosine 72.8 68.6 MLN/PSL + Asym 73.2 68.9 Ensemble 73.2 71.5 Table 1: Test RTE accuracy and STS Correlation. 4.1 Systems Compared We compare multiple configurations of our probabilistic logic system. • Baseline: Vector- and keyword-only baseline described in Section 3.6; • MLN/PSL + Cosine: MLN and PSL based methods described in Sections 3.4 and 3.5, using cosine as a similarity measure; • MLN/PSL + Asym:</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014a. SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A sick cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson,</booktitle>
<location>Bente Maegaard, Joseph Mariani, Asuncion Moreno,</location>
<contexts>
<context position="14625" citStr="Marelli et al., 2014" startWordPosition="2395" endWordPosition="2398">ilarity using both the cosine and asym measures. These form two features for the regressor. The last feature of the model is a keyword count. We count how many times 13 different keywords appear in either the text or the hypothesis. These keywords include negation (no, not, nobody, etc.) and quantifiers (a, the, some, etc.) The counts of each keyword form the last 13 features as input to the regressor. In total, there are 17 features used in this baseline system. 4 Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. SICK-RTE SICK-STS Baseline 70.0 71.1 MLN/PSL + Cosine 72.8 68.6 MLN/PSL + Asym 73.2 68.9 Ensemble 73.2 71.5 Table 1: Test RTE accuracy and STS Correlation. 4.1 Systems Compared We compare multiple configurations of our probabilistic logic system. • Baseline: Vector- and keyword-only baseline described in Section 3.6; • MLN/PSL + Cosine: MLN and PSL based methods described in Sections 3.4 and 3.5, using cosine as a similarity measure; • MLN/PSL + Asym:</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014b. A sick cure for the evaluation of compositional distributional semantic models. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL08).</booktitle>
<contexts>
<context position="2833" citStr="Mitchell and Lapata, 2008" startWordPosition="415" endWordPosition="418">, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ we</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of Association for Computational Linguistics (ACL08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="3378" citStr="Richardson and Domingos, 2006" startWordPosition="495" endWordPosition="498">ations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. 796 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796–801, Dublin, Ireland, August 23-24, 2014. MLNs define a probability distribution over possible worlds, where the probability of a world increases exponentially with the total weight of the logical clauses that it satisfies. A vari</context>
<context position="11203" citStr="Richardson and Domingos, 2006" startWordPosition="1798" endWordPosition="1801">oposed in Beltagy and Mooney (2014). For example, consider the evidence ground atom man(M) denoting that the constant M is of type man. Then, consider another predicate like car(x). In case there are no inference rule connecting man(x) and car(x), then we know that M which we know is a man cannot be a car, so we remove the ground atom car(M) from the ground network. This technique reduces the size of the ground network dramatically and makes inference tractable. Another problem with MLN inference is that quantifiers sometimes behave in an undesirable way, due to the Domain Closure Assumption (Richardson and Domingos, 2006) that MLNs make. For example, consider the text-hypothesis pair: “There is a black bird” and “All birds are black”, which in logic are T : bird(B)nblack(B) and H : Vx. bird(x) ==&gt;- black(x). Because of the Domain Closure Assumption, MLNs conclude that T entails H because H is true for all constants in the domain (in this example, the single constant B). We solve this problem by introducing extra constants and evidence in the domain. In the example above, we introduce evidence of a new bird bird(D), which prevents the hypothesis from being true. The full details of the technique of dealing with</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Katrin Erk</author>
<author>Gemma Boleda</author>
</authors>
<title>Inclusive yet selective: Supervised distributional hypernymy detection.</title>
<date>2014</date>
<booktitle>In Proceedings of the Twenty Fifth International Conference on Computational Linguistics (COLING-14),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3162" citStr="Roller et al., 2014" startWordPosition="463" endWordPosition="466">rvation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. 796 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796–801, Dublin, I</context>
<context position="7065" citStr="Roller et al. (2014)" startWordPosition="1094" endWordPosition="1097">r(x) ==&gt;- vehicle(x)Iw2, where w1 and w2 are weights indicating the similarity of the antecedent and consequent of each rule. Inferences rules are generated as in Beltagy et al. (2013). Given two input sentences T and H, for all pairs (a, b), where a and b are words or phrases of T and H respectively, generate an inference rule: a —* b I w, where the rule weight w is a function of sim(−—*a , —*−b ), and sim is a similarity measure of the distributional vectors —*−a , —*−b . We experimented with the symmetric similarity measure cosine, and asym, the supervised, asymmetric similarity measure of Roller et al. (2014). The asym measure uses the vector difference (−—*a − sifier for distinguishing between four different word relations: hypernymy, cohyponymy, meronomy, and no relation. The model is trained using the noun-noun subset of the BLESS data set (Baroni and Lenci, 2011). The final similarity weight is given by the model’s estimated probability that the word relationship is either hypernymy ormeronomy: asym(−—* a , —*− b ) = P(hyper(a, b))+ P(mero(a, b)). Distributional representations for words are derived by counting co-occurrences in the ukWaC, WaCkypedia, BNC and Gigaword corpora. We use the 2000 </context>
</contexts>
<marker>Roller, Erk, Boleda, 2014</marker>
<rawString>Stephen Roller, Katrin Erk, and Gemma Boleda. 2014. Inclusive yet selective: Supervised distributional hypernymy detection. In Proceedings of the Twenty Fifth International Conference on Computational Linguistics (COLING-14), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2495" citStr="Turney and Pantel, 2010" startWordPosition="363" endWordPosition="366">ceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Ko</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>