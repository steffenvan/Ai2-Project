<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004093">
<title confidence="0.992519">
Mapping Source to Target Strings without Alignment by Analogical
Learning: A Case Study with Transliteration
</title>
<author confidence="0.614678">
Philippe Langlais
</author>
<affiliation confidence="0.459747">
RALI / DIRO
Universit´e de Montr´eal
</affiliation>
<address confidence="0.969238">
Montr´eal, Canada, H3C 3J7
</address>
<email confidence="0.998592">
felipe@iro.umontreal.ca
</email>
<sectionHeader confidence="0.993926" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999112923076923">
Analogical learning over strings is a holis-
tic model that has been investigated by a
few authors as a means to map forms of a
source language to forms of a target lan-
guage. In this study, we revisit this learn-
ing paradigm and apply it to the translit-
eration task. We show that alone, it per-
forms worse than a statistical phrase-based
machine translation engine, but the com-
bination of both approaches outperforms
each one taken separately, demonstrating
the usefulness of the information captured
by a so-called formal analogy.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976345454546">
A proportional analogy is a relationship between
four objects, noted [x : y :: z : t], which reads as
“x is to y as z is to t”. While some strategies have
been proposed for handling semantic relationships
(Turney and Littman, 2005; Duc et al., 2011),
we focus in this study on formal proportional
analogies (hereafter formal analogies or simply
analogies), that is, proportional analogies involv-
ing relationships at the graphemic level, such as
[atomkraftwerken : atomkriegen :: kraftwerks :
kriegs] in German.
Analogical learning over strings has been in-
vestigated by several authors. Yvon (1997) ad-
dressed the task of grapheme-to-phoneme conver-
sion, a problem which continues to be studied ac-
tively, see for instance (Bhargava and Kondrak,
2011). Stroppa and Yvon (2005) applied analog-
ical learning to computing morphosyntactic fea-
tures to be associated with a form (lemma, part-
of-speech, and additional features such as number,
gender, case, tense, mood, etc.). The performance
of the analogical engine on the Dutch language
was as good as or better than the one reported in
(van den Bosch and Daelemans, 1993). Lepage
and Denoual (2005) pioneered the application of
analogical learning to Machine Translation. Dif-
ferent variants of the system they proposed have
been tested in a number of evaluation campaigns,
see for instance (Lepage et al., 2009). Langlais
and Patry (2007) investigated the more specific
task of translating unknown words, a problem si-
multaneously studied in (Denoual, 2007).
Analogical learning has been applied to various
other purposes, among which query expansion in
information retrieval (Moreau et al., 2007), clas-
sification of nominal and binary data, and hand-
written character recognition (Miclet et al., 2008).
Formal analogy has also been used for solving
Raven IQ tests (Correa et al., 2012).
In this study, we investigate the relevance
of analogical learning for English proper name
transliteration into Chinese. We compare it to
the statistical phrase-based machine translation
approach (Koehn et al., 2003) initially proposed
for transliteration by Finch and Sumita (2010).
We show that alone, analogical learning underper-
forms the phrase-based approach, but that a com-
bination of both outperforms individual systems.
We describe in section 2 the principle of ana-
logical learning. In section 3, we report on ex-
periments we conducted in applying analogical
learning on the NEWS 2009 English-to-Chinese
transliteration task. Related works are discussed
in section 4. We conclude in section 5 and identify
avenues we believe deserve investigations.
</bodyText>
<sectionHeader confidence="0.880634" genericHeader="introduction">
2 Analogical Learning
</sectionHeader>
<subsectionHeader confidence="0.929669">
2.1 Formal Analogy
</subsectionHeader>
<bodyText confidence="0.998636833333333">
In this study, we use the most general definition
of formal analogy we found, initially described
in (Yvon et al., 2004). It handles a large variety
of relations, including but not limited to affixa-
tion operations (i.e. [capital : anticapitalisme ::
commun : anticommuniste] in French), stem mu-
</bodyText>
<page confidence="0.984639">
684
</page>
<bodyText confidence="0.963571037037037">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 684–689,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
tations (i.e. [lang : l¨ange :: stark : st¨arke] in Ger-
man), and even templatic relations (i.e [KaaTiB :
KuTaaB :: QaaRi’ : QuRaa’] in Arabic).
Informally,1 this definition states that 4 forms
x, y, z and t are in analogical relation iff we can
find a d-factorization (a factorization into d fac-
tors) of each form, such that the ith factors (i ∈
[1, d]) of x and z equal (in ensemble terms) the ith
factors of y and t.
For instance, [this guy drinks : this boat sinks
:: these guys drank : these boats sank] holds be-
cause of the following 4-uple of 5-factorizations,
whose factors are aligned column-wise for clarity,
and where spaces (underlined) are treated as regu-
lar characters (c designates the empty factor):
fx ≡ ( this guy c dr inks )
fy ≡ ( this boat c s inks )
fz ≡ ( these guy s dr ank )
ft ≡ ( these boat s s ank )
This analogy “captures” among other things
that in English, changing this for these implies a
plural mark (s) to the corresponding noun. Note
that analogies can relate arbitrarily distant sub-
strings. For instance the 3rd-person singular mark
of the verbs relates to the first substring this.
</bodyText>
<subsectionHeader confidence="0.999778">
2.2 Analogical Learning
</subsectionHeader>
<bodyText confidence="0.999694">
We now clarify the process of analogical learning.
Let L = {(i(xk), o(xk))k} be a training set (or
memory) gathering pairs of input i(xk) and out-
put o(xk) representations of elements xk. In this
study, the elements we consider are pairs of En-
glish / Chinese proper names in a transliteration
relation. Given an element t for which we only
know i(t), analogical learning works by:
</bodyText>
<listItem confidence="0.997613555555556">
1. building Ei(t) = {(x, y, z) ∈ L3  |[i(x) :
i(y) :: i(z) : i(t)]}, the set of triples in the
training set that stand in analogical propor-
tion with t in the input space,
2. building Eo(t) = {[o(x) : o(y) :: o(z) :
?]  |(x, y, z) ∈ Ei(t)}, the set of solutions to
the output analogical equations obtained,
3. selecting o(t) among the solutions aggre-
gated into Eo(t).
</listItem>
<bodyText confidence="0.9528865">
In this description, we define an analogical
equation as an analogy with one form missing, and
</bodyText>
<footnote confidence="0.9035665">
1We refer the reader to (Stroppa and Yvon, 2005) for a
more technical exposition.
</footnote>
<equation confidence="0.96913215">
we note [x : y :: z : ?] the set of its solutions (i.e.
undoable ∈ [reader: doer:: unreadable : ?]).2
L = { (Schell, 谢尔), (Zemens, 泽门斯), (Zell, 泽尔),
(Schemansky, 谢曼斯基), (Clise, 克莱斯),
(Rovine, 罗 文), (Rovensky, 罗 文 斯 基), ... }
� [Schell: Zell:: Schemansky : Zemansky]
↓ ↓ ↓ ↓
[谢 尔 : 泽 尔 :: 谢 曼 斯 基 : ?] [4 sols] :
曼 斯 泽 基 泽 曼 斯 基 曼 斯 基 泽 . . .
�[Rovine : Rovensky :: Zieman : Zemansky]
↓ ↓ ↓ ↓
[罗 文 : 罗 文 斯 基 :: 齐 曼 : ?] [6 sols]:
斯 基 齐 曼 斯 齐 曼 基 斯 齐 基 曼 ...
� [Stephens: Stephansky :: Zemens : Zemansky]
↓ ↓ ↓ ↓
[斯 蒂 芬 斯 : 斯 蒂 芬 斯 基 :: 泽 门 斯 : ?] [9 sols] :
斯 泽 基 门 泽 基 门 斯 泽 斯 门 基 ...
...
31 solutions: 泽 曼 斯 基 (77) 泽 门 斯 基 (59)
曼 泽 斯 基 (29) 兹 梅 斯 卡 P (20) ...
</equation>
<figureCaption confidence="0.98046">
Figure 1: Excerpt of a transliteration session for
</figureCaption>
<bodyText confidence="0.920170592592593">
the English proper name Zemansky. 31 solutions
have been identified in total (4 by the first equation
reported); the one underlined (actually the most
frequently generated) is the sanctioned one.
Figure 1 illustrates this process on a translit-
eration session for the English proper name
Zemansky. The training corpus L is a set of
pairs of English proper names and their Chi-
nese Transliteration(s). Step 1 identifies analogies
among English proper names: 7 such analogies are
identified, 3 of which are reported (marked with a
� sign). Step 2 projects the English forms in ana-
logical proportion into their known transliteration
(illustrated by a ↓ sign) in order to solve Chinese
analogical equations. Step 3 aggregates the solu-
tions produced during the second step. In the ex-
ample, it consists in sorting the solutions in de-
creasing order of the number of time they have
been generated during step 2 (see next section for
a better strategy).
There are several important points to consider
when deploying the learning procedure shown
above. First, the search stage (step 1) has a time
complexity that can be prohibitive in some appli-
cations of interest. We refer the reader to (Langlais
and Yvon, 2008) for a practical solution to this.
Second, we need a way to solve an analogical
</bodyText>
<footnote confidence="0.474758">
2Analogical equation solvers typically produce several so-
lutions to an equation.
</footnote>
<page confidence="0.992084">
685
</page>
<bodyText confidence="0.999946555555555">
equation. We applied the finite-state machine pro-
cedure described in (Yvon et al., 2004). Suffice it
to say that typically, this solver produces several
solutions to an equation, most of them spurious,3
reinforcing the need for an efficient aggregation
step (step 3). Last, it might happen that the over-
all approach fails at producing a solution, because
no input analogy is identified during step 1, or be-
cause the input analogies identified do not lead to
analogies in the output space. This silence issue is
analyzed in section 3. A detailed account of those
problems and possible solutions are discussed in
(Somers et al., 2009).
We underline that analogies in both source and
target languages are considered independently: the
approach does not attempt to align source and tar-
get substrings, but relies instead on the inductive
bias that input analogies (often) imply output ones.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997562">
3.1 Setting
</subsectionHeader>
<bodyText confidence="0.999942037037037">
The task we study is part of the NEWS evalua-
tion campaign conducted in 2009 (Li et al., 2009).
The dataset consists of 31961 English-Chinese
transliteration examples for training the system
(TRAIN), 2 896 ones for tuning it (DEV), and 2 896
for testing them (TEST).
We compare two different approaches to
transliteration: a statistical phrase-based machine
translation engine — which according to Li et
al. (2009) was popular among participating sys-
tems to NEWS — as well as differently flavored
analogical systems.
We trained (on TRAIN) a phrase-based transla-
tion device with the Moses toolkit (Koehn et al.,
2007), very similarly to (Finch and Sumita, 2010),
that is, considering each character as a word. The
coefficients of the log-linear function optimized by
Moses’ decoder were tuned (with MERT) on DEV.
For the analogical system, we investigated the
use of classifiers trained in a supervised way to
recognize the good solutions generated during
step 2. For this, we first transliterated the DEV
dataset using TRAIN as a memory. Then, we
trained a classifier, taking advantage of the DEV
corpus for the supervision. We tried two types
of learners — support vector machines (Cortes
and Vapnik, 1995) and voted perceptrons (Freund
</bodyText>
<footnote confidence="0.5457345">
3A spurious solution is a string that does not belong to the
language under consideration. See Figure 1 for examples.
</footnote>
<bodyText confidence="0.999746055555555">
and Schapire, 1999)4 — and found the former to
slightly outperform the latter. Finally, we translit-
erated the TEST corpus using both the TRAIN and
DEV corpora as a memory,5 and applied our clas-
sifiers on the solutions generated.
The lack of space prevents us to describe the 61
features we used for characterizing a solution. We
initially considered a set of features which charac-
terizes a solution (frequency, rank in the candidate
list, language model likelihood, etc.), and the pro-
cess that generated the solution (i.e. number of
analogies involved), but no feature that would use
scored pairs of substrings (such as mutual infor-
mation of substrings).6 Thus, we also considered
in a second stage a set of features that we collected
thanks to a n-best list of solutions computed by
Moses (Moses’ score given to a solution, its rank
in the n-best list, etc.).
</bodyText>
<subsectionHeader confidence="0.717667">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.99991184">
We ran the NEWS 2009 official evaluation script7
in order to compute ACC (the accuracy of the
first solution), F1 (the F-measure which gives
partial credits proportional to the longest subse-
quence between the reference transliteration and
the first candidate), and the Mean Reciprocal Rank
(MRR), where 100/MRR roughly indicates the av-
erage rank of the correct solution over the session.
Table 1 reports the results of several transliter-
ation configurations we tested. The first two sys-
tems are pure analogical devices, (M) is the Moses
configuration, (AM1) is a variant discussed further,
(AM2) is the best configuration we tested (a com-
bination of Moses and analogical learning), and
the last two lines show the lowest and highest per-
forming systems among the 18 standard runs reg-
istered at NEWS 2009 (Li et al., 2009). Several
observations have to be made.
First, none of the variants tested outperformed
the best system reported at NEWS 2009. This is
not surprising since we conducted only prelimi-
nary experiments with analogy. Still, we were
pleased to observe that the best configuration we
devised (AM2) would have ranked fourth on this
task.
</bodyText>
<footnote confidence="0.990362">
4We used libSVM (Chang and Lin, 2011) for training
svms, and an in-house package for training voted perceptrons.
5This is fair since there is no training involved. Many
participants to the NEWS campaign did this as well.
6We avoided this in order to keep the classifiers simple to
train.
7http://translit.i2r.a-star.edu.sg/
news2009/evaluation/.
</footnote>
<page confidence="0.996163">
686
</page>
<bodyText confidence="0.99994775">
The ana-freq system is an analogical device
where the aggregation step consists in sorting so-
lutions in decreasing order of frequency. It is
clearly outperformed by the Moses system. The
ana-svma system is an analogical device where
the solutions are selected by the SVM trained on
analogical features only. Learning to recognize
good solutions from spurious ones improves accu-
racy (over A1). Still, we are far from the accuracy
we would observe by using an oracle classifier
(ACC = 81.5). Clearly, further experiments with
better feature engineering must be conducted. It
is noteworthy that the pure analogical devices we
tested (A1 and A2) did not return any solution for
3.7% of the test forms, which explains some loss
in performance compared to the SMT approach,
which always delivers a solution.8
System ana-svma+m (AM1) is an analogical
device where the classifier makes uses of the fea-
tures extracted by Moses. Obviously, those fea-
tures drastically improve accuracy of the classifier.
Configuration (AM2) is a combination which cas-
cades the hybrid device (AM1) with the SMT en-
gine (M). This means that the former system is
trusted whenever it produces a solution, and the
latter one is used as a backup. This configuration
outperforms Moses, which demonstrates the com-
plementarity of the analogical information.
</bodyText>
<table confidence="0.99990125">
Configuration ACC F1 MRR rank
A1 ana-freq 56.6 79.1 63.0 16
A2 ana-svma 58.0 80.0 58.8 15
M moses 66.6 85.9 66.6 6
AM1 ana-svma+m 63.4 82.0 64.1 10
AM2 AM1 + M 68.5 86.9 69.0 4
last NEWS 2009 19.9 60.6 22.9 23
first NEWS 2009 73.1 89.5 81.2 1
</table>
<tableCaption confidence="0.999582">
Table 1: Evaluation of different configurations
</tableCaption>
<bodyText confidence="0.95767">
with the metrics used at NEWS. The last column
indicates the rank of systems as if we had submit-
ted the top 5 configurations to NEWS 2009.
</bodyText>
<sectionHeader confidence="0.999957" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.991379645833333">
Most approaches to transliteration we know rely
on some form of substring alignment. This align-
ment can be learnt explicitly as in (Knight and
8Removing the solutions produced by the SMT engine for
the 3.7% test forms that receive no solution from the analog-
ical devices would result in an accuracy score of 65.0.
Graehl, 1998; Li et al., 2004; Jiampojamarn et al.,
2007), or it can be indirectly modeled as in (Oh et
al., 2009) where transliteration is seen as a tagging
task (that is, labeling each source grapheme with a
target one), and where the model learns correspon-
dences at the substring level. See also the semi-
supervised approach of (Sajjad et al., 2012). Ana-
logical inference differs drastically from those ap-
proaches, since it finds relations in the source ma-
terial and solves target equations independently.
Therefore, no alignment whatsoever is required.
Transliteration by analogical learning has been
attempted by Dandapat et al. (2010) for an
English-to-Hindi transliteration task. They com-
pared various heuristics to speed up analogical
learning, and several combinations of phrase-
based SMT and analogical learning. Our results
confirm the observation they made that combining
an analogical device with SMT leads to gains over
individual systems. Still, their work differs from
the present one in the fact that they considered the
top frequency aggregator (similar to A1), which we
showed to be suboptimal. Also, they used the def-
inition of formal analogy of Lepage (1998), which
is provably less general than the one we used. The
impact of this choice for different language pairs
remains to be investigated.
Aggregating solutions produced by analogical
inference with the help of a classifier has been re-
ported in (Langlais et al., 2009). The authors in-
vestigated an arguably more specific task: translat-
ing medical terms. Another difference is that we
classify solutions produced by analogical learning
(roughly 100 solutions per test form), while they
classified pairs of input/target analogies, whose
number can be rather high, leading to huge and
highly unbalanced learning tasks. The authors re-
port training experiments with millions of exam-
ples and only a few positive ones. In fact, we
initially attempted to recognize fruitful analogical
pairs, but found it especially slow and disappoint-
ing.
</bodyText>
<sectionHeader confidence="0.995846" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999952142857143">
We considered the NEWS 2009 English-to-
Chinese transliteration task for investigating ana-
logical learning, a holistic approach that does not
rely on an alignment or segmentation model. We
have shown that alone, the approach fails to trans-
late 3.7% of the test forms, underperforms the
state-of-the-art SMT engine Moses, while still de-
</bodyText>
<page confidence="0.990468">
687
</page>
<bodyText confidence="0.999956352941177">
livering decent performance. By combining both
approaches, we obtained a system which outper-
forms the individual ones we tested.
We believe analogical inference over strings has
not delivered all his potential yet. In particular,
we have observed that there is a huge room for
improvements in the aggregation step. We have
tested a simple classifier approach, mining a tiny
subset of the features that could be put at use.
More research on this issue is warranted, notably
looking at machine-learned ranking algorithms.
Also, the silence issue we faced could be tack-
led by the notion of analogical dissimilarity intro-
duced by Miclet et al. (2008). The idea of using
near analogies in analogical learning has been suc-
cessfully investigated by the authors on a number
of standard classification testbeds.
</bodyText>
<sectionHeader confidence="0.992249" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999951125">
This work has been founded by the Natural
Sciences and Engineering Research Council of
Canada. We are grateful to Fabrizio Gotti for his
contribution to this work, and to the anonymous
reviewers for their useful comments. We are also
indebted to Min Zhang and Haizhou Li who pro-
vided us with the NEWS 2009 English-Chinese
datasets.
</bodyText>
<sectionHeader confidence="0.998514" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782803149607">
Aditya Bhargava and Grzegorz Kondrak. 2011. How
do you pronounce your name? Improving G2P with
transliterations. In 49th ACL/HLT, pages 399–408,
Portland, USA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Trans. Intell. Syst. Technol., 2(3):1–27, May.
William Fernando Correa, Henri Prade, and Gilles
Richard. 2012. When intelligence is just a matter
of copying. In 20th ECAI, pages 276–281, Mont-
pellier, France.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
Vector Networks. Mach. Learn., 20(3):273–297.
Sandipan Dandapat, Sara Morrissey, Sudip Ku-
mar Naskar, and Harold Somers. 2010. Mitigat-
ing Problems in Analogy-based EBMT with SMT
and vice versa: a Case Study with Named En-
tity Transliteration. In 24th Pacific Asia Confer-
ence on Language Information and Computation
(PACLIC’10), pages 365–372, Sendai, Japan.
´Etienne Denoual. 2007. Analogical translation of
unknown words in a statistical machine translation
framework. In MT Summit XI, pages 135–141,
Copenhagen, Denmark.
Nguyen Tuan Duc, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Cross-Language Latent Relational
Search: Mapping Knowledge across Languages. In
25th AAAI, pages 1237 – 1242, San Francisco, USA.
Andrew Finch and Eiichiro Sumita. 2010. Translit-
eration Using a Phrase-based Statistical Machine
Translation System to Re-score the Output of a Joint
Multigram Model. In 2nd Named Entities Workshop
(NEWS’10), pages 48–52, Uppsala, Sweden.
Y. Freund and R. E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Mach. Learn., 37(3):277–296.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying Many-to-Many Alignments
and Hidden Markov Models to Letter-to-Phoneme
Conversion. In NAACL/HLT’07, pages 372–379.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Comput. Linguist., 24(4):599–612.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
NAACL/HLT’03, pages 48–54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In 45th ACL, pages 177–180. Interactive Poster and
Demonstration Sessions.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating Unknown Words by Analogical Learning. In
EMNLP/CoNLL’07, pages 877–886, Prague, Czech
Republic.
Philippe Langlais and Franc¸ois Yvon. 2008. Scaling
up Analogical Learning. In 22nd COLING, pages
51–54, Manchester, United Kingdom. Poster.
Philippe Langlais, Franc¸ois Yvon, and Pierre Zweigen-
baum. 2009. Improvements in Analogical Learn-
ing: Application to Translating multi-Terms of the
Medical Domain. In 12th EACL, pages 487–495,
Athens, Greece.
Yves Lepage and ´Etienne Denoual. 2005. Purest ever
example-based machine translation: Detailed pre-
sentation and assesment. Mach. Translat, 19:25–
252.
Yves Lepage, Adrien Lardilleux, and Julien Gosme.
2009. The GREYC Translation Memory for the
IWSLT 2009 Evaluation Campaign: one step be-
yond translation memory. In 6th IWSLT, pages 45–
688 49, Tokyo, Japan.
Yves Lepage. 1998. Solving Analogies on Words: an
Algorithm. In COLING/ACL, pages 728–733, Mon-
treal, Canada.
Haizhou Li, Min Zhang, and Jian Su. 2004. A Joint
Source-Channel Model for Machine Transliteration.
In 42nd ACL, pages 159–166, Barcelona, Spain.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of NEWS 2009 Machine
Transliteration Shared Task. In 1st Named Entities
Workshop (NEWS’09): Shared Task on Translitera-
tion, pages 1–18, Singapore.
Laurent Miclet, Sabri Bayroudh, and Arnaud Delhay.
2008. Analogical Dissimilarity: Definitions, Algo-
rithms and two experiments in Machine Learning.
Journal of Artificial Intelligence Research, pages
793–824.
Fabienne Moreau, Vincent Claveau, and Pascale
S´ebillot. 2007. Automatic Morphological Query
Expansion Using Analogy-based Machine Learn-
ing. In 29th European Conference on IR research
(ECIR’07), pages 222–233, Rome, Italy.
Jong-hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine Transliteration using Target-
Language Grapheme and Phoneme: Multi-engine
Transliteration Approach. In 1st Named Entities
Workshop (NEWS’09): Shared Task on Transliter-
ation, pages 36–39, Singapore.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A Statistical Model for Unsupervised and
Semi-supervised Transliteration Mining. In 50th
ACL, pages 469–477, Jeju Island, Korea.
Harold Somers, Sandipan Sandapat, and Sudip Kumar
Naskar. 2009. A Review of EBMT Using Pro-
portional Analogies. In 3rd Workshop on Example-
based Machine Translation, pages 53–60, Dublin,
Ireland.
Nicolas Stroppa and Franc¸ois Yvon. 2005. An Ana-
logical Learner for Morphological Analysis. In 9th
CoNLL, pages 120–127, Ann Arbor, USA.
P.D. Turney and M.L. Littman. 2005. Corpus-based
Learning of Analogies and Semantic Relations. In
Machine Learning, volume 60, pages 251–278.
Antal van den Bosch and Walter Daelemans. 1993.
Data-Oriented Methods for Grapheme-to-Phoneme
Conversion. In 6th EACL, pages 45–53, Utrecht,
Netherlands.
Franc¸ois Yvon, Nicolas Stroppa, Arnaud Delhay, and
Laurent Miclet. 2004. Solving Analogies on Words.
Technical Report D005, ´Ecole Nationale Sup´erieure
des T´el´ecommuncations, Paris, France.
Franc¸ois Yvon. 1997. Paradigmatic Cascades: a Lin-
guistically Sound Model of Pronunciation by Anal-
ogy. In 35th ACL, pages 429–435, Madrid, Spain.
</reference>
<page confidence="0.998877">
689
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894474">
<title confidence="0.9988845">Mapping Source to Target Strings without Alignment by Learning: A Case Study with Transliteration</title>
<author confidence="0.964434">Philippe</author>
<affiliation confidence="0.9652635">RALI / Universit´e de</affiliation>
<address confidence="0.994969">Montr´eal, Canada, H3C</address>
<email confidence="0.996566">felipe@iro.umontreal.ca</email>
<abstract confidence="0.998803857142857">Analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language. In this study, we revisit this learning paradigm and apply it to the transliteration task. We show that alone, it performs worse than a statistical phrase-based machine translation engine, but the combination of both approaches outperforms each one taken separately, demonstrating the usefulness of the information captured by a so-called formal analogy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aditya Bhargava</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>How do you pronounce your name? Improving G2P with transliterations.</title>
<date>2011</date>
<booktitle>In 49th ACL/HLT,</booktitle>
<pages>399--408</pages>
<location>Portland, USA.</location>
<contexts>
<context position="1514" citStr="Bhargava and Kondrak, 2011" startWordPosition="238" endWordPosition="241"> to t”. While some strategies have been proposed for handling semantic relationships (Turney and Littman, 2005; Duc et al., 2011), we focus in this study on formal proportional analogies (hereafter formal analogies or simply analogies), that is, proportional analogies involving relationships at the graphemic level, such as [atomkraftwerken : atomkriegen :: kraftwerks : kriegs] in German. Analogical learning over strings has been investigated by several authors. Yvon (1997) addressed the task of grapheme-to-phoneme conversion, a problem which continues to be studied actively, see for instance (Bhargava and Kondrak, 2011). Stroppa and Yvon (2005) applied analogical learning to computing morphosyntactic features to be associated with a form (lemma, partof-speech, and additional features such as number, gender, case, tense, mood, etc.). The performance of the analogical engine on the Dutch language was as good as or better than the one reported in (van den Bosch and Daelemans, 1993). Lepage and Denoual (2005) pioneered the application of analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al.,</context>
</contexts>
<marker>Bhargava, Kondrak, 2011</marker>
<rawString>Aditya Bhargava and Grzegorz Kondrak. 2011. How do you pronounce your name? Improving G2P with transliterations. In 49th ACL/HLT, pages 399–408, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines.</title>
<date>2011</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="12348" citStr="Chang and Lin, 2011" startWordPosition="2117" endWordPosition="2120">ant discussed further, (AM2) is the best configuration we tested (a combination of Moses and analogical learning), and the last two lines show the lowest and highest performing systems among the 18 standard runs registered at NEWS 2009 (Li et al., 2009). Several observations have to be made. First, none of the variants tested outperformed the best system reported at NEWS 2009. This is not surprising since we conducted only preliminary experiments with analogy. Still, we were pleased to observe that the best configuration we devised (AM2) would have ranked fourth on this task. 4We used libSVM (Chang and Lin, 2011) for training svms, and an in-house package for training voted perceptrons. 5This is fair since there is no training involved. Many participants to the NEWS campaign did this as well. 6We avoided this in order to keep the classifiers simple to train. 7http://translit.i2r.a-star.edu.sg/ news2009/evaluation/. 686 The ana-freq system is an analogical device where the aggregation step consists in sorting solutions in decreasing order of frequency. It is clearly outperformed by the Moses system. The ana-svma system is an analogical device where the solutions are selected by the SVM trained on analo</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Trans. Intell. Syst. Technol., 2(3):1–27, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Fernando Correa</author>
<author>Henri Prade</author>
<author>Gilles Richard</author>
</authors>
<title>When intelligence is just a matter of copying.</title>
<date>2012</date>
<booktitle>In 20th ECAI,</booktitle>
<pages>276--281</pages>
<location>Montpellier, France.</location>
<contexts>
<context position="2593" citStr="Correa et al., 2012" startWordPosition="407" endWordPosition="410">lation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patry (2007) investigated the more specific task of translating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012). In this study, we investigate the relevance of analogical learning for English proper name transliteration into Chinese. We compare it to the statistical phrase-based machine translation approach (Koehn et al., 2003) initially proposed for transliteration by Finch and Sumita (2010). We show that alone, analogical learning underperforms the phrase-based approach, but that a combination of both outperforms individual systems. We describe in section 2 the principle of analogical learning. In section 3, we report on experiments we conducted in applying analogical learning on the NEWS 2009 Englis</context>
</contexts>
<marker>Correa, Prade, Richard, 2012</marker>
<rawString>William Fernando Correa, Henri Prade, and Gilles Richard. 2012. When intelligence is just a matter of copying. In 20th ECAI, pages 276–281, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>SupportVector Networks.</title>
<date>1995</date>
<location>Mach. Learn.,</location>
<contexts>
<context position="10139" citStr="Cortes and Vapnik, 1995" startWordPosition="1750" endWordPosition="1753">oses toolkit (Koehn et al., 2007), very similarly to (Finch and Sumita, 2010), that is, considering each character as a word. The coefficients of the log-linear function optimized by Moses’ decoder were tuned (with MERT) on DEV. For the analogical system, we investigated the use of classifiers trained in a supervised way to recognize the good solutions generated during step 2. For this, we first transliterated the DEV dataset using TRAIN as a memory. Then, we trained a classifier, taking advantage of the DEV corpus for the supervision. We tried two types of learners — support vector machines (Cortes and Vapnik, 1995) and voted perceptrons (Freund 3A spurious solution is a string that does not belong to the language under consideration. See Figure 1 for examples. and Schapire, 1999)4 — and found the former to slightly outperform the latter. Finally, we transliterated the TEST corpus using both the TRAIN and DEV corpora as a memory,5 and applied our classifiers on the solutions generated. The lack of space prevents us to describe the 61 features we used for characterizing a solution. We initially considered a set of features which characterizes a solution (frequency, rank in the candidate list, language mod</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. SupportVector Networks. Mach. Learn., 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandipan Dandapat</author>
<author>Sara Morrissey</author>
<author>Sudip Kumar Naskar</author>
<author>Harold Somers</author>
</authors>
<title>Mitigating Problems in Analogy-based EBMT with SMT and vice versa: a Case Study with Named Entity Transliteration.</title>
<date>2010</date>
<booktitle>In 24th Pacific Asia Conference on Language Information and Computation (PACLIC’10),</booktitle>
<pages>365--372</pages>
<location>Sendai, Japan.</location>
<contexts>
<context position="15379" citStr="Dandapat et al. (2010)" startWordPosition="2618" endWordPosition="2621">et al., 2004; Jiampojamarn et al., 2007), or it can be indirectly modeled as in (Oh et al., 2009) where transliteration is seen as a tagging task (that is, labeling each source grapheme with a target one), and where the model learns correspondences at the substring level. See also the semisupervised approach of (Sajjad et al., 2012). Analogical inference differs drastically from those approaches, since it finds relations in the source material and solves target equations independently. Therefore, no alignment whatsoever is required. Transliteration by analogical learning has been attempted by Dandapat et al. (2010) for an English-to-Hindi transliteration task. They compared various heuristics to speed up analogical learning, and several combinations of phrasebased SMT and analogical learning. Our results confirm the observation they made that combining an analogical device with SMT leads to gains over individual systems. Still, their work differs from the present one in the fact that they considered the top frequency aggregator (similar to A1), which we showed to be suboptimal. Also, they used the definition of formal analogy of Lepage (1998), which is provably less general than the one we used. The imp</context>
</contexts>
<marker>Dandapat, Morrissey, Naskar, Somers, 2010</marker>
<rawString>Sandipan Dandapat, Sara Morrissey, Sudip Kumar Naskar, and Harold Somers. 2010. Mitigating Problems in Analogy-based EBMT with SMT and vice versa: a Case Study with Named Entity Transliteration. In 24th Pacific Asia Conference on Language Information and Computation (PACLIC’10), pages 365–372, Sendai, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Etienne Denoual</author>
</authors>
<title>Analogical translation of unknown words in a statistical machine translation framework.</title>
<date>2007</date>
<booktitle>In MT Summit XI,</booktitle>
<pages>135--141</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2265" citStr="Denoual, 2007" startWordPosition="358" endWordPosition="359">peech, and additional features such as number, gender, case, tense, mood, etc.). The performance of the analogical engine on the Dutch language was as good as or better than the one reported in (van den Bosch and Daelemans, 1993). Lepage and Denoual (2005) pioneered the application of analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patry (2007) investigated the more specific task of translating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012). In this study, we investigate the relevance of analogical learning for English proper name transliteration into Chinese. We compare it to the statistical phrase-based machine translation approach (Koehn et al., 2003) initially proposed for transliteration by Finch and S</context>
</contexts>
<marker>Denoual, 2007</marker>
<rawString>´Etienne Denoual. 2007. Analogical translation of unknown words in a statistical machine translation framework. In MT Summit XI, pages 135–141, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Tuan Duc</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Cross-Language Latent Relational Search: Mapping Knowledge across Languages.</title>
<date>2011</date>
<booktitle>In 25th AAAI,</booktitle>
<pages>1237--1242</pages>
<location>San Francisco, USA.</location>
<contexts>
<context position="1016" citStr="Duc et al., 2011" startWordPosition="164" endWordPosition="167"> this study, we revisit this learning paradigm and apply it to the transliteration task. We show that alone, it performs worse than a statistical phrase-based machine translation engine, but the combination of both approaches outperforms each one taken separately, demonstrating the usefulness of the information captured by a so-called formal analogy. 1 Introduction A proportional analogy is a relationship between four objects, noted [x : y :: z : t], which reads as “x is to y as z is to t”. While some strategies have been proposed for handling semantic relationships (Turney and Littman, 2005; Duc et al., 2011), we focus in this study on formal proportional analogies (hereafter formal analogies or simply analogies), that is, proportional analogies involving relationships at the graphemic level, such as [atomkraftwerken : atomkriegen :: kraftwerks : kriegs] in German. Analogical learning over strings has been investigated by several authors. Yvon (1997) addressed the task of grapheme-to-phoneme conversion, a problem which continues to be studied actively, see for instance (Bhargava and Kondrak, 2011). Stroppa and Yvon (2005) applied analogical learning to computing morphosyntactic features to be asso</context>
</contexts>
<marker>Duc, Bollegala, Ishizuka, 2011</marker>
<rawString>Nguyen Tuan Duc, Danushka Bollegala, and Mitsuru Ishizuka. 2011. Cross-Language Latent Relational Search: Mapping Knowledge across Languages. In 25th AAAI, pages 1237 – 1242, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Transliteration Using a Phrase-based Statistical Machine Translation System to Re-score the Output of a Joint Multigram Model.</title>
<date>2010</date>
<booktitle>In 2nd Named Entities Workshop (NEWS’10),</booktitle>
<pages>48--52</pages>
<location>Uppsala,</location>
<contexts>
<context position="2877" citStr="Finch and Sumita (2010)" startWordPosition="447" endWordPosition="450">oual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012). In this study, we investigate the relevance of analogical learning for English proper name transliteration into Chinese. We compare it to the statistical phrase-based machine translation approach (Koehn et al., 2003) initially proposed for transliteration by Finch and Sumita (2010). We show that alone, analogical learning underperforms the phrase-based approach, but that a combination of both outperforms individual systems. We describe in section 2 the principle of analogical learning. In section 3, we report on experiments we conducted in applying analogical learning on the NEWS 2009 English-to-Chinese transliteration task. Related works are discussed in section 4. We conclude in section 5 and identify avenues we believe deserve investigations. 2 Analogical Learning 2.1 Formal Analogy In this study, we use the most general definition of formal analogy we found, initial</context>
<context position="9592" citStr="Finch and Sumita, 2010" startWordPosition="1661" endWordPosition="1664">S evaluation campaign conducted in 2009 (Li et al., 2009). The dataset consists of 31961 English-Chinese transliteration examples for training the system (TRAIN), 2 896 ones for tuning it (DEV), and 2 896 for testing them (TEST). We compare two different approaches to transliteration: a statistical phrase-based machine translation engine — which according to Li et al. (2009) was popular among participating systems to NEWS — as well as differently flavored analogical systems. We trained (on TRAIN) a phrase-based translation device with the Moses toolkit (Koehn et al., 2007), very similarly to (Finch and Sumita, 2010), that is, considering each character as a word. The coefficients of the log-linear function optimized by Moses’ decoder were tuned (with MERT) on DEV. For the analogical system, we investigated the use of classifiers trained in a supervised way to recognize the good solutions generated during step 2. For this, we first transliterated the DEV dataset using TRAIN as a memory. Then, we trained a classifier, taking advantage of the DEV corpus for the supervision. We tried two types of learners — support vector machines (Cortes and Vapnik, 1995) and voted perceptrons (Freund 3A spurious solution i</context>
</contexts>
<marker>Finch, Sumita, 2010</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2010. Transliteration Using a Phrase-based Statistical Machine Translation System to Re-score the Output of a Joint Multigram Model. In 2nd Named Entities Workshop (NEWS’10), pages 48–52, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<location>Mach. Learn.,</location>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Mach. Learn., 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion. In</title>
<date>2007</date>
<booktitle>NAACL/HLT’07,</booktitle>
<pages>372--379</pages>
<contexts>
<context position="14797" citStr="Jiampojamarn et al., 2007" startWordPosition="2525" endWordPosition="2528">9 19.9 60.6 22.9 23 first NEWS 2009 73.1 89.5 81.2 1 Table 1: Evaluation of different configurations with the metrics used at NEWS. The last column indicates the rank of systems as if we had submitted the top 5 configurations to NEWS 2009. 4 Related Work Most approaches to transliteration we know rely on some form of substring alignment. This alignment can be learnt explicitly as in (Knight and 8Removing the solutions produced by the SMT engine for the 3.7% test forms that receive no solution from the analogical devices would result in an accuracy score of 65.0. Graehl, 1998; Li et al., 2004; Jiampojamarn et al., 2007), or it can be indirectly modeled as in (Oh et al., 2009) where transliteration is seen as a tagging task (that is, labeling each source grapheme with a target one), and where the model learns correspondences at the substring level. See also the semisupervised approach of (Sajjad et al., 2012). Analogical inference differs drastically from those approaches, since it finds relations in the source material and solves target equations independently. Therefore, no alignment whatsoever is required. Transliteration by analogical learning has been attempted by Dandapat et al. (2010) for an English-to</context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion. In NAACL/HLT’07, pages 372–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration. Comput. Linguist.,</journal>
<volume>24</volume>
<issue>4</issue>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Comput. Linguist., 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation. In</title>
<date>2003</date>
<booktitle>NAACL/HLT’03,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2811" citStr="Koehn et al., 2003" startWordPosition="438" endWordPosition="441">lating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012). In this study, we investigate the relevance of analogical learning for English proper name transliteration into Chinese. We compare it to the statistical phrase-based machine translation approach (Koehn et al., 2003) initially proposed for transliteration by Finch and Sumita (2010). We show that alone, analogical learning underperforms the phrase-based approach, but that a combination of both outperforms individual systems. We describe in section 2 the principle of analogical learning. In section 3, we report on experiments we conducted in applying analogical learning on the NEWS 2009 English-to-Chinese transliteration task. Related works are discussed in section 4. We conclude in section 5 and identify avenues we believe deserve investigations. 2 Analogical Learning 2.1 Formal Analogy In this study, we u</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In NAACL/HLT’03, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In 45th ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="9548" citStr="Koehn et al., 2007" startWordPosition="1654" endWordPosition="1657">ing The task we study is part of the NEWS evaluation campaign conducted in 2009 (Li et al., 2009). The dataset consists of 31961 English-Chinese transliteration examples for training the system (TRAIN), 2 896 ones for tuning it (DEV), and 2 896 for testing them (TEST). We compare two different approaches to transliteration: a statistical phrase-based machine translation engine — which according to Li et al. (2009) was popular among participating systems to NEWS — as well as differently flavored analogical systems. We trained (on TRAIN) a phrase-based translation device with the Moses toolkit (Koehn et al., 2007), very similarly to (Finch and Sumita, 2010), that is, considering each character as a word. The coefficients of the log-linear function optimized by Moses’ decoder were tuned (with MERT) on DEV. For the analogical system, we investigated the use of classifiers trained in a supervised way to recognize the good solutions generated during step 2. For this, we first transliterated the DEV dataset using TRAIN as a memory. Then, we trained a classifier, taking advantage of the DEV corpus for the supervision. We tried two types of learners — support vector machines (Cortes and Vapnik, 1995) and vote</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In 45th ACL, pages 177–180. Interactive Poster and Demonstration Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Alexandre Patry</author>
</authors>
<title>Translating Unknown Words by Analogical Learning.</title>
<date>2007</date>
<booktitle>In EMNLP/CoNLL’07,</booktitle>
<pages>877--886</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2147" citStr="Langlais and Patry (2007)" startWordPosition="339" endWordPosition="342">a and Yvon (2005) applied analogical learning to computing morphosyntactic features to be associated with a form (lemma, partof-speech, and additional features such as number, gender, case, tense, mood, etc.). The performance of the analogical engine on the Dutch language was as good as or better than the one reported in (van den Bosch and Daelemans, 1993). Lepage and Denoual (2005) pioneered the application of analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patry (2007) investigated the more specific task of translating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012). In this study, we investigate the relevance of analogical learning for English proper name transliteration into Chinese. We compare it to the statistica</context>
</contexts>
<marker>Langlais, Patry, 2007</marker>
<rawString>Philippe Langlais and Alexandre Patry. 2007. Translating Unknown Words by Analogical Learning. In EMNLP/CoNLL’07, pages 877–886, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Scaling up Analogical Learning.</title>
<date>2008</date>
<booktitle>In 22nd COLING,</booktitle>
<pages>51--54</pages>
<location>Manchester, United Kingdom. Poster.</location>
<contexts>
<context position="7856" citStr="Langlais and Yvon, 2008" startWordPosition="1379" endWordPosition="1382">gical proportion into their known transliteration (illustrated by a ↓ sign) in order to solve Chinese analogical equations. Step 3 aggregates the solutions produced during the second step. In the example, it consists in sorting the solutions in decreasing order of the number of time they have been generated during step 2 (see next section for a better strategy). There are several important points to consider when deploying the learning procedure shown above. First, the search stage (step 1) has a time complexity that can be prohibitive in some applications of interest. We refer the reader to (Langlais and Yvon, 2008) for a practical solution to this. Second, we need a way to solve an analogical 2Analogical equation solvers typically produce several solutions to an equation. 685 equation. We applied the finite-state machine procedure described in (Yvon et al., 2004). Suffice it to say that typically, this solver produces several solutions to an equation, most of them spurious,3 reinforcing the need for an efficient aggregation step (step 3). Last, it might happen that the overall approach fails at producing a solution, because no input analogy is identified during step 1, or because the input analogies ide</context>
</contexts>
<marker>Langlais, Yvon, 2008</marker>
<rawString>Philippe Langlais and Franc¸ois Yvon. 2008. Scaling up Analogical Learning. In 22nd COLING, pages 51–54, Manchester, United Kingdom. Poster.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Franc¸ois Yvon</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Improvements in Analogical Learning: Application to Translating multi-Terms of the Medical Domain.</title>
<date>2009</date>
<booktitle>In 12th EACL,</booktitle>
<pages>487--495</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="16184" citStr="Langlais et al., 2009" startWordPosition="2746" endWordPosition="2749">ur results confirm the observation they made that combining an analogical device with SMT leads to gains over individual systems. Still, their work differs from the present one in the fact that they considered the top frequency aggregator (similar to A1), which we showed to be suboptimal. Also, they used the definition of formal analogy of Lepage (1998), which is provably less general than the one we used. The impact of this choice for different language pairs remains to be investigated. Aggregating solutions produced by analogical inference with the help of a classifier has been reported in (Langlais et al., 2009). The authors investigated an arguably more specific task: translating medical terms. Another difference is that we classify solutions produced by analogical learning (roughly 100 solutions per test form), while they classified pairs of input/target analogies, whose number can be rather high, leading to huge and highly unbalanced learning tasks. The authors report training experiments with millions of examples and only a few positive ones. In fact, we initially attempted to recognize fruitful analogical pairs, but found it especially slow and disappointing. 5 Conclusion We considered the NEWS </context>
</contexts>
<marker>Langlais, Yvon, Zweigenbaum, 2009</marker>
<rawString>Philippe Langlais, Franc¸ois Yvon, and Pierre Zweigenbaum. 2009. Improvements in Analogical Learning: Application to Translating multi-Terms of the Medical Domain. In 12th EACL, pages 487–495, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
<author>´Etienne Denoual</author>
</authors>
<title>Purest ever example-based machine translation: Detailed presentation and assesment.</title>
<date>2005</date>
<pages>252</pages>
<location>Mach. Translat,</location>
<contexts>
<context position="1907" citStr="Lepage and Denoual (2005)" startWordPosition="302" endWordPosition="305">ogical learning over strings has been investigated by several authors. Yvon (1997) addressed the task of grapheme-to-phoneme conversion, a problem which continues to be studied actively, see for instance (Bhargava and Kondrak, 2011). Stroppa and Yvon (2005) applied analogical learning to computing morphosyntactic features to be associated with a form (lemma, partof-speech, and additional features such as number, gender, case, tense, mood, etc.). The performance of the analogical engine on the Dutch language was as good as or better than the one reported in (van den Bosch and Daelemans, 1993). Lepage and Denoual (2005) pioneered the application of analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patry (2007) investigated the more specific task of translating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 200</context>
</contexts>
<marker>Lepage, Denoual, 2005</marker>
<rawString>Yves Lepage and ´Etienne Denoual. 2005. Purest ever example-based machine translation: Detailed presentation and assesment. Mach. Translat, 19:25– 252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
<author>Adrien Lardilleux</author>
<author>Julien Gosme</author>
</authors>
<title>The GREYC Translation Memory for the IWSLT 2009 Evaluation Campaign: one step beyond translation memory.</title>
<date>2009</date>
<booktitle>In 6th IWSLT,</booktitle>
<pages>45--688</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="2120" citStr="Lepage et al., 2009" startWordPosition="335" endWordPosition="338">Kondrak, 2011). Stroppa and Yvon (2005) applied analogical learning to computing morphosyntactic features to be associated with a form (lemma, partof-speech, and additional features such as number, gender, case, tense, mood, etc.). The performance of the analogical engine on the Dutch language was as good as or better than the one reported in (van den Bosch and Daelemans, 1993). Lepage and Denoual (2005) pioneered the application of analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patry (2007) investigated the more specific task of translating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012). In this study, we investigate the relevance of analogical learning for English proper name transliteration into Chinese. We c</context>
</contexts>
<marker>Lepage, Lardilleux, Gosme, 2009</marker>
<rawString>Yves Lepage, Adrien Lardilleux, and Julien Gosme. 2009. The GREYC Translation Memory for the IWSLT 2009 Evaluation Campaign: one step beyond translation memory. In 6th IWSLT, pages 45– 688 49, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
</authors>
<title>Solving Analogies on Words: an Algorithm. In</title>
<date>1998</date>
<booktitle>COLING/ACL,</booktitle>
<pages>728--733</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="15917" citStr="Lepage (1998)" startWordPosition="2704" endWordPosition="2705">eration by analogical learning has been attempted by Dandapat et al. (2010) for an English-to-Hindi transliteration task. They compared various heuristics to speed up analogical learning, and several combinations of phrasebased SMT and analogical learning. Our results confirm the observation they made that combining an analogical device with SMT leads to gains over individual systems. Still, their work differs from the present one in the fact that they considered the top frequency aggregator (similar to A1), which we showed to be suboptimal. Also, they used the definition of formal analogy of Lepage (1998), which is provably less general than the one we used. The impact of this choice for different language pairs remains to be investigated. Aggregating solutions produced by analogical inference with the help of a classifier has been reported in (Langlais et al., 2009). The authors investigated an arguably more specific task: translating medical terms. Another difference is that we classify solutions produced by analogical learning (roughly 100 solutions per test form), while they classified pairs of input/target analogies, whose number can be rather high, leading to huge and highly unbalanced l</context>
</contexts>
<marker>Lepage, 1998</marker>
<rawString>Yves Lepage. 1998. Solving Analogies on Words: an Algorithm. In COLING/ACL, pages 728–733, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A Joint Source-Channel Model for Machine Transliteration.</title>
<date>2004</date>
<booktitle>In 42nd ACL,</booktitle>
<pages>159--166</pages>
<location>Barcelona,</location>
<contexts>
<context position="14769" citStr="Li et al., 2004" startWordPosition="2521" endWordPosition="2524">0 4 last NEWS 2009 19.9 60.6 22.9 23 first NEWS 2009 73.1 89.5 81.2 1 Table 1: Evaluation of different configurations with the metrics used at NEWS. The last column indicates the rank of systems as if we had submitted the top 5 configurations to NEWS 2009. 4 Related Work Most approaches to transliteration we know rely on some form of substring alignment. This alignment can be learnt explicitly as in (Knight and 8Removing the solutions produced by the SMT engine for the 3.7% test forms that receive no solution from the analogical devices would result in an accuracy score of 65.0. Graehl, 1998; Li et al., 2004; Jiampojamarn et al., 2007), or it can be indirectly modeled as in (Oh et al., 2009) where transliteration is seen as a tagging task (that is, labeling each source grapheme with a target one), and where the model learns correspondences at the substring level. See also the semisupervised approach of (Sajjad et al., 2012). Analogical inference differs drastically from those approaches, since it finds relations in the source material and solves target equations independently. Therefore, no alignment whatsoever is required. Transliteration by analogical learning has been attempted by Dandapat et </context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A Joint Source-Channel Model for Machine Transliteration. In 42nd ACL, pages 159–166, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<date>2009</date>
<booktitle>Report of NEWS 2009 Machine Transliteration Shared Task. In 1st Named Entities Workshop (NEWS’09): Shared Task on Transliteration,</booktitle>
<pages>1--18</pages>
<contexts>
<context position="9026" citStr="Li et al., 2009" startWordPosition="1573" endWordPosition="1576">ing step 1, or because the input analogies identified do not lead to analogies in the output space. This silence issue is analyzed in section 3. A detailed account of those problems and possible solutions are discussed in (Somers et al., 2009). We underline that analogies in both source and target languages are considered independently: the approach does not attempt to align source and target substrings, but relies instead on the inductive bias that input analogies (often) imply output ones. 3 Experiments 3.1 Setting The task we study is part of the NEWS evaluation campaign conducted in 2009 (Li et al., 2009). The dataset consists of 31961 English-Chinese transliteration examples for training the system (TRAIN), 2 896 ones for tuning it (DEV), and 2 896 for testing them (TEST). We compare two different approaches to transliteration: a statistical phrase-based machine translation engine — which according to Li et al. (2009) was popular among participating systems to NEWS — as well as differently flavored analogical systems. We trained (on TRAIN) a phrase-based translation device with the Moses toolkit (Koehn et al., 2007), very similarly to (Finch and Sumita, 2010), that is, considering each charac</context>
<context position="11981" citStr="Li et al., 2009" startWordPosition="2057" endWordPosition="2060"> the reference transliteration and the first candidate), and the Mean Reciprocal Rank (MRR), where 100/MRR roughly indicates the average rank of the correct solution over the session. Table 1 reports the results of several transliteration configurations we tested. The first two systems are pure analogical devices, (M) is the Moses configuration, (AM1) is a variant discussed further, (AM2) is the best configuration we tested (a combination of Moses and analogical learning), and the last two lines show the lowest and highest performing systems among the 18 standard runs registered at NEWS 2009 (Li et al., 2009). Several observations have to be made. First, none of the variants tested outperformed the best system reported at NEWS 2009. This is not surprising since we conducted only preliminary experiments with analogy. Still, we were pleased to observe that the best configuration we devised (AM2) would have ranked fourth on this task. 4We used libSVM (Chang and Lin, 2011) for training svms, and an in-house package for training voted perceptrons. 5This is fair since there is no training involved. Many participants to the NEWS campaign did this as well. 6We avoided this in order to keep the classifiers</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A. Kumaran, Vladimir Pervouchine, and Min Zhang. 2009. Report of NEWS 2009 Machine Transliteration Shared Task. In 1st Named Entities Workshop (NEWS’09): Shared Task on Transliteration, pages 1–18, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Miclet</author>
<author>Sabri Bayroudh</author>
<author>Arnaud Delhay</author>
</authors>
<title>Analogical Dissimilarity: Definitions, Algorithms and two experiments in Machine Learning.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>793--824</pages>
<contexts>
<context position="2509" citStr="Miclet et al., 2008" startWordPosition="392" endWordPosition="395">and Denoual (2005) pioneered the application of analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patry (2007) investigated the more specific task of translating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classification of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012). In this study, we investigate the relevance of analogical learning for English proper name transliteration into Chinese. We compare it to the statistical phrase-based machine translation approach (Koehn et al., 2003) initially proposed for transliteration by Finch and Sumita (2010). We show that alone, analogical learning underperforms the phrase-based approach, but that a combination of both outperforms individual systems. We describe in section 2 the principle of analogical learning. In section 3, we report</context>
</contexts>
<marker>Miclet, Bayroudh, Delhay, 2008</marker>
<rawString>Laurent Miclet, Sabri Bayroudh, and Arnaud Delhay. 2008. Analogical Dissimilarity: Definitions, Algorithms and two experiments in Machine Learning. Journal of Artificial Intelligence Research, pages 793–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Moreau</author>
<author>Vincent Claveau</author>
<author>Pascale S´ebillot</author>
</authors>
<title>Automatic Morphological Query Expansion Using Analogy-based Machine Learning.</title>
<date>2007</date>
<booktitle>In 29th European Conference on IR research (ECIR’07),</booktitle>
<pages>222--233</pages>
<location>Rome, Italy.</location>
<marker>Moreau, Claveau, S´ebillot, 2007</marker>
<rawString>Fabienne Moreau, Vincent Claveau, and Pascale S´ebillot. 2007. Automatic Morphological Query Expansion Using Analogy-based Machine Learning. In 29th European Conference on IR research (ECIR’07), pages 222–233, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-hoon Oh</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Machine Transliteration using TargetLanguage Grapheme and Phoneme: Multi-engine Transliteration Approach.</title>
<date>2009</date>
<booktitle>In 1st Named Entities Workshop (NEWS’09): Shared Task on Transliteration,</booktitle>
<pages>36--39</pages>
<contexts>
<context position="14854" citStr="Oh et al., 2009" startWordPosition="2537" endWordPosition="2540">ation of different configurations with the metrics used at NEWS. The last column indicates the rank of systems as if we had submitted the top 5 configurations to NEWS 2009. 4 Related Work Most approaches to transliteration we know rely on some form of substring alignment. This alignment can be learnt explicitly as in (Knight and 8Removing the solutions produced by the SMT engine for the 3.7% test forms that receive no solution from the analogical devices would result in an accuracy score of 65.0. Graehl, 1998; Li et al., 2004; Jiampojamarn et al., 2007), or it can be indirectly modeled as in (Oh et al., 2009) where transliteration is seen as a tagging task (that is, labeling each source grapheme with a target one), and where the model learns correspondences at the substring level. See also the semisupervised approach of (Sajjad et al., 2012). Analogical inference differs drastically from those approaches, since it finds relations in the source material and solves target equations independently. Therefore, no alignment whatsoever is required. Transliteration by analogical learning has been attempted by Dandapat et al. (2010) for an English-to-Hindi transliteration task. They compared various heuris</context>
</contexts>
<marker>Oh, Uchimoto, Torisawa, 2009</marker>
<rawString>Jong-hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Machine Transliteration using TargetLanguage Grapheme and Phoneme: Multi-engine Transliteration Approach. In 1st Named Entities Workshop (NEWS’09): Shared Task on Transliteration, pages 36–39, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sajjad</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining.</title>
<date>2012</date>
<booktitle>In 50th ACL,</booktitle>
<pages>469--477</pages>
<location>Jeju Island,</location>
<contexts>
<context position="15091" citStr="Sajjad et al., 2012" startWordPosition="2577" endWordPosition="2580">ly on some form of substring alignment. This alignment can be learnt explicitly as in (Knight and 8Removing the solutions produced by the SMT engine for the 3.7% test forms that receive no solution from the analogical devices would result in an accuracy score of 65.0. Graehl, 1998; Li et al., 2004; Jiampojamarn et al., 2007), or it can be indirectly modeled as in (Oh et al., 2009) where transliteration is seen as a tagging task (that is, labeling each source grapheme with a target one), and where the model learns correspondences at the substring level. See also the semisupervised approach of (Sajjad et al., 2012). Analogical inference differs drastically from those approaches, since it finds relations in the source material and solves target equations independently. Therefore, no alignment whatsoever is required. Transliteration by analogical learning has been attempted by Dandapat et al. (2010) for an English-to-Hindi transliteration task. They compared various heuristics to speed up analogical learning, and several combinations of phrasebased SMT and analogical learning. Our results confirm the observation they made that combining an analogical device with SMT leads to gains over individual systems.</context>
</contexts>
<marker>Sajjad, Fraser, Schmid, 2012</marker>
<rawString>Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2012. A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining. In 50th ACL, pages 469–477, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
<author>Sandipan Sandapat</author>
<author>Sudip Kumar Naskar</author>
</authors>
<title>A Review of EBMT Using Proportional Analogies.</title>
<date>2009</date>
<booktitle>In 3rd Workshop on Examplebased Machine Translation,</booktitle>
<pages>53--60</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="8653" citStr="Somers et al., 2009" startWordPosition="1511" endWordPosition="1514"> applied the finite-state machine procedure described in (Yvon et al., 2004). Suffice it to say that typically, this solver produces several solutions to an equation, most of them spurious,3 reinforcing the need for an efficient aggregation step (step 3). Last, it might happen that the overall approach fails at producing a solution, because no input analogy is identified during step 1, or because the input analogies identified do not lead to analogies in the output space. This silence issue is analyzed in section 3. A detailed account of those problems and possible solutions are discussed in (Somers et al., 2009). We underline that analogies in both source and target languages are considered independently: the approach does not attempt to align source and target substrings, but relies instead on the inductive bias that input analogies (often) imply output ones. 3 Experiments 3.1 Setting The task we study is part of the NEWS evaluation campaign conducted in 2009 (Li et al., 2009). The dataset consists of 31961 English-Chinese transliteration examples for training the system (TRAIN), 2 896 ones for tuning it (DEV), and 2 896 for testing them (TEST). We compare two different approaches to transliteration</context>
</contexts>
<marker>Somers, Sandapat, Naskar, 2009</marker>
<rawString>Harold Somers, Sandipan Sandapat, and Sudip Kumar Naskar. 2009. A Review of EBMT Using Proportional Analogies. In 3rd Workshop on Examplebased Machine Translation, pages 53–60, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Stroppa</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>An Analogical Learner for Morphological Analysis.</title>
<date>2005</date>
<booktitle>In 9th CoNLL,</booktitle>
<pages>120--127</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1539" citStr="Stroppa and Yvon (2005)" startWordPosition="242" endWordPosition="245"> have been proposed for handling semantic relationships (Turney and Littman, 2005; Duc et al., 2011), we focus in this study on formal proportional analogies (hereafter formal analogies or simply analogies), that is, proportional analogies involving relationships at the graphemic level, such as [atomkraftwerken : atomkriegen :: kraftwerks : kriegs] in German. Analogical learning over strings has been investigated by several authors. Yvon (1997) addressed the task of grapheme-to-phoneme conversion, a problem which continues to be studied actively, see for instance (Bhargava and Kondrak, 2011). Stroppa and Yvon (2005) applied analogical learning to computing morphosyntactic features to be associated with a form (lemma, partof-speech, and additional features such as number, gender, case, tense, mood, etc.). The performance of the analogical engine on the Dutch language was as good as or better than the one reported in (van den Bosch and Daelemans, 1993). Lepage and Denoual (2005) pioneered the application of analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patr</context>
<context position="5908" citStr="Stroppa and Yvon, 2005" startWordPosition="975" endWordPosition="978">names in a transliteration relation. Given an element t for which we only know i(t), analogical learning works by: 1. building Ei(t) = {(x, y, z) ∈ L3 |[i(x) : i(y) :: i(z) : i(t)]}, the set of triples in the training set that stand in analogical proportion with t in the input space, 2. building Eo(t) = {[o(x) : o(y) :: o(z) : ?] |(x, y, z) ∈ Ei(t)}, the set of solutions to the output analogical equations obtained, 3. selecting o(t) among the solutions aggregated into Eo(t). In this description, we define an analogical equation as an analogy with one form missing, and 1We refer the reader to (Stroppa and Yvon, 2005) for a more technical exposition. we note [x : y :: z : ?] the set of its solutions (i.e. undoable ∈ [reader: doer:: unreadable : ?]).2 L = { (Schell, 谢尔), (Zemens, 泽门斯), (Zell, 泽尔), (Schemansky, 谢曼斯基), (Clise, 克莱斯), (Rovine, 罗 文), (Rovensky, 罗 文 斯 基), ... } � [Schell: Zell:: Schemansky : Zemansky] ↓ ↓ ↓ ↓ [谢 尔 : 泽 尔 :: 谢 曼 斯 基 : ?] [4 sols] : 曼 斯 泽 基 泽 曼 斯 基 曼 斯 基 泽 . . . �[Rovine : Rovensky :: Zieman : Zemansky] ↓ ↓ ↓ ↓ [罗 文 : 罗 文 斯 基 :: 齐 曼 : ?] [6 sols]: 斯 基 齐 曼 斯 齐 曼 基 斯 齐 基 曼 ... � [Stephens: Stephansky :: Zemens : Zemansky] ↓ ↓ ↓ ↓ [斯 蒂 芬 斯 : 斯 蒂 芬 斯 基 :: 泽 门 斯 : ?] [9 sols] : 斯 泽 基 门 泽</context>
</contexts>
<marker>Stroppa, Yvon, 2005</marker>
<rawString>Nicolas Stroppa and Franc¸ois Yvon. 2005. An Analogical Learner for Morphological Analysis. In 9th CoNLL, pages 120–127, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>M L Littman</author>
</authors>
<title>Corpus-based Learning of Analogies and Semantic Relations.</title>
<date>2005</date>
<booktitle>In Machine Learning,</booktitle>
<volume>60</volume>
<pages>251--278</pages>
<contexts>
<context position="997" citStr="Turney and Littman, 2005" startWordPosition="160" endWordPosition="163">s of a target language. In this study, we revisit this learning paradigm and apply it to the transliteration task. We show that alone, it performs worse than a statistical phrase-based machine translation engine, but the combination of both approaches outperforms each one taken separately, demonstrating the usefulness of the information captured by a so-called formal analogy. 1 Introduction A proportional analogy is a relationship between four objects, noted [x : y :: z : t], which reads as “x is to y as z is to t”. While some strategies have been proposed for handling semantic relationships (Turney and Littman, 2005; Duc et al., 2011), we focus in this study on formal proportional analogies (hereafter formal analogies or simply analogies), that is, proportional analogies involving relationships at the graphemic level, such as [atomkraftwerken : atomkriegen :: kraftwerks : kriegs] in German. Analogical learning over strings has been investigated by several authors. Yvon (1997) addressed the task of grapheme-to-phoneme conversion, a problem which continues to be studied actively, see for instance (Bhargava and Kondrak, 2011). Stroppa and Yvon (2005) applied analogical learning to computing morphosyntactic </context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>P.D. Turney and M.L. Littman. 2005. Corpus-based Learning of Analogies and Semantic Relations. In Machine Learning, volume 60, pages 251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daelemans</author>
</authors>
<title>Data-Oriented Methods for Grapheme-to-Phoneme Conversion.</title>
<date>1993</date>
<booktitle>In 6th EACL,</booktitle>
<pages>45--53</pages>
<location>Utrecht, Netherlands.</location>
<marker>van den Bosch, Daelemans, 1993</marker>
<rawString>Antal van den Bosch and Walter Daelemans. 1993. Data-Oriented Methods for Grapheme-to-Phoneme Conversion. In 6th EACL, pages 45–53, Utrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Yvon</author>
<author>Nicolas Stroppa</author>
<author>Arnaud Delhay</author>
<author>Laurent Miclet</author>
</authors>
<title>Solving Analogies on Words.</title>
<date>2004</date>
<booktitle>Technical Report D005, ´Ecole Nationale Sup´erieure des T´el´ecommuncations,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="3512" citStr="Yvon et al., 2004" startWordPosition="546" endWordPosition="549">one, analogical learning underperforms the phrase-based approach, but that a combination of both outperforms individual systems. We describe in section 2 the principle of analogical learning. In section 3, we report on experiments we conducted in applying analogical learning on the NEWS 2009 English-to-Chinese transliteration task. Related works are discussed in section 4. We conclude in section 5 and identify avenues we believe deserve investigations. 2 Analogical Learning 2.1 Formal Analogy In this study, we use the most general definition of formal analogy we found, initially described in (Yvon et al., 2004). It handles a large variety of relations, including but not limited to affixation operations (i.e. [capital : anticapitalisme :: commun : anticommuniste] in French), stem mu684 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 684–689, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tations (i.e. [lang : l¨ange :: stark : st¨arke] in German), and even templatic relations (i.e [KaaTiB : KuTaaB :: QaaRi’ : QuRaa’] in Arabic). Informally,1 this definition states that 4 forms x, y, z and t are in analogical relation i</context>
<context position="8109" citStr="Yvon et al., 2004" startWordPosition="1420" endWordPosition="1423">er of the number of time they have been generated during step 2 (see next section for a better strategy). There are several important points to consider when deploying the learning procedure shown above. First, the search stage (step 1) has a time complexity that can be prohibitive in some applications of interest. We refer the reader to (Langlais and Yvon, 2008) for a practical solution to this. Second, we need a way to solve an analogical 2Analogical equation solvers typically produce several solutions to an equation. 685 equation. We applied the finite-state machine procedure described in (Yvon et al., 2004). Suffice it to say that typically, this solver produces several solutions to an equation, most of them spurious,3 reinforcing the need for an efficient aggregation step (step 3). Last, it might happen that the overall approach fails at producing a solution, because no input analogy is identified during step 1, or because the input analogies identified do not lead to analogies in the output space. This silence issue is analyzed in section 3. A detailed account of those problems and possible solutions are discussed in (Somers et al., 2009). We underline that analogies in both source and target </context>
</contexts>
<marker>Yvon, Stroppa, Delhay, Miclet, 2004</marker>
<rawString>Franc¸ois Yvon, Nicolas Stroppa, Arnaud Delhay, and Laurent Miclet. 2004. Solving Analogies on Words. Technical Report D005, ´Ecole Nationale Sup´erieure des T´el´ecommuncations, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Yvon</author>
</authors>
<title>Paradigmatic Cascades: a Linguistically Sound Model of Pronunciation by Analogy.</title>
<date>1997</date>
<booktitle>In 35th ACL,</booktitle>
<pages>429--435</pages>
<location>Madrid,</location>
<contexts>
<context position="1364" citStr="Yvon (1997)" startWordPosition="216" endWordPosition="217"> Introduction A proportional analogy is a relationship between four objects, noted [x : y :: z : t], which reads as “x is to y as z is to t”. While some strategies have been proposed for handling semantic relationships (Turney and Littman, 2005; Duc et al., 2011), we focus in this study on formal proportional analogies (hereafter formal analogies or simply analogies), that is, proportional analogies involving relationships at the graphemic level, such as [atomkraftwerken : atomkriegen :: kraftwerks : kriegs] in German. Analogical learning over strings has been investigated by several authors. Yvon (1997) addressed the task of grapheme-to-phoneme conversion, a problem which continues to be studied actively, see for instance (Bhargava and Kondrak, 2011). Stroppa and Yvon (2005) applied analogical learning to computing morphosyntactic features to be associated with a form (lemma, partof-speech, and additional features such as number, gender, case, tense, mood, etc.). The performance of the analogical engine on the Dutch language was as good as or better than the one reported in (van den Bosch and Daelemans, 1993). Lepage and Denoual (2005) pioneered the application of analogical learning to Mach</context>
</contexts>
<marker>Yvon, 1997</marker>
<rawString>Franc¸ois Yvon. 1997. Paradigmatic Cascades: a Linguistically Sound Model of Pronunciation by Analogy. In 35th ACL, pages 429–435, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>