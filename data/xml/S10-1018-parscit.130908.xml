<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.084286">
<title confidence="0.975285">
SUCRE: A Modular System for Coreference Resolution
</title>
<author confidence="0.989505">
Hamidreza Kobdani and Hinrich Sch¨utze
</author>
<affiliation confidence="0.9964215">
Institute for Natural Language Processing
University of Stuttgart, Germany
</affiliation>
<email confidence="0.982157">
kobdani@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993505" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999713647058824">
This paper presents SUCRE, a new soft-
ware tool for coreference resolution and
its feature engineering. It is able to sep-
arately do noun, pronoun and full coref-
erence resolution. SUCRE introduces a
new approach to the feature engineering
of coreference resolution based on a rela-
tional database model and a regular feature
definition language. SUCRE successfully
participated in SemEval-2010 Task 1 on
Coreference Resolution in Multiple Lan-
guages (Recasens et al., 2010) for gold
and regular closed annotation tracks of six
languages. It obtained the best results in
several categories, including the regular
closed annotation tracks of English and
German.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984510204082">
In this paper, we introduce a new software tool
for coreference resolution. Coreference resolution
is the process of finding discourse entities (mark-
ables) referring to the same real-world entity or
concept. In other words, this process groups the
markables of a document into equivalence classes
(coreference entities) so that all markables in an
entity are coreferent.
There are various publicly available systems
that perform coreference resolution, such as
BART (Versley et al., 2008) and GUITAR (Stein-
berger et al., 2007). A considerable engineering
effort is needed for the full coreference resolution
task, and a significant part of this effort concerns
feature engineering. Thus, a system which is able
to extract the features based on a feature defini-
tion language can help the researcher reduce the
implementation effort needed for feature extrac-
tion. Most methods of coreference resolution, if
providing a baseline, usually use a feature set sim-
ilar to (Soon et al., 2001) or (Ng and Cardie, 2002)
and do the feature extraction in the preprocessing
stage. SUCRE has been developed to provide a
more flexible method for feature engineering of
coreference resolution. It has a novel approach to
model an unstructured text corpus in a structured
framework by using a relational database model
and a regular feature definition language to define
and extract the features. Relational databases are
a well-known technology for structured data mod-
eling and are supported by a wide array of soft-
ware and tools. Converting a text corpus to/from
its equivalent relational database model is straight-
forward in our framework.
A regular language for feature definition is a
very flexible method to extract different features
from text. In addition to features defined di-
rectly in SUCRE, it accepts also externally ex-
tracted/generated features. Its modular architec-
ture makes it possible to use any externally avail-
able classification method too. In addition to link
features (features related to a markable pair), it
is also possible to define other kinds of features:
atomic word and markable features. This ap-
proach to feature engineering is suitable not only
for knowledge-rich but also for knowledge-poor
datasets. It is also language independent. The re-
sults of SUCRE in SemEval-2010 Task 1 show the
promise of our framework.
</bodyText>
<sectionHeader confidence="0.960343" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.9991456">
The architecture of SUCRE has two main parts:
preprocessing and coreference resolution.
In preprocessing the text corpus is converted to
a relational database model. These are the main
functionalities in this stage:
</bodyText>
<listItem confidence="0.998118666666667">
1. Preliminary text conversion
2. Extracting atomic word features
3. Markable detection
</listItem>
<page confidence="0.922448">
92
</page>
<note confidence="0.6324815">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 92–95,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.999604153846154">
Column Characteristic
Word Table
Word-ID Primary Key
Document-ID Foreign Key
Paragraph-ID Foreign Key
Sentence-ID Foreign Key
Word-String Attribute
Word-Feature-0 Attribute
Word-Feature-1 Attribute
... Attribute
Word-Feature-N Attribute
Markable Table
Markable-ID Primary Key
Begin-Word-ID Foreign Key
End-Word-ID Foreign Key
Head-Word-ID Foreign Key
Markable-Feature-0 Attribute
Markable-Feature-1 Attribute
... Attribute
Markable-Feature-N Attribute
Links Table
Link-ID Primary Key
First-Markable-ID Foreign Key
Second-Markable-ID Foreign Key
Coreference-Status Attribute
Status-Confidence-Level Attribute
</table>
<tableCaption confidence="0.999565">
Table 1: Relational Database Model of Text Corpus
</tableCaption>
<sectionHeader confidence="0.564329" genericHeader="method">
4. Extracting atomic markable features
</sectionHeader>
<bodyText confidence="0.994234">
After converting (modeling) the text corpus to
the database, coreference resolution can be per-
formed. Its functional components are:
</bodyText>
<listItem confidence="0.9991842">
1. Relational Database Model of Text Corpus
2. Link Generator
3. Link Feature Extractor
4. Learning (Applicable on Train Data)
5. Decoding (Applicable on Test Data)
</listItem>
<subsectionHeader confidence="0.8114365">
2.1 Relational Database Model of Text
Corpus
</subsectionHeader>
<bodyText confidence="0.999944888888889">
The Relational Database model of text thev cor-
pus is an easy to generate format. Three tables are
needed to have a minimum running system: Word,
Markable and Link.
Table 1 presents the database model of the text
corpus. In the word table, Word-ID is the index
of the word, starting from the beginning of the
corpus. It is used as the primary key to uniquely
identify each token. Document-ID, Paragraph-ID
and Sentence-ID are each counted from the be-
ginning of the corpus, and also act as the foreign
keys pointing to the primary keys of the docu-
ment, paragraph and sentence tables, which are
optional (the system can also work without them).
It is obvious that the raw text as well as any other
format of the corpus can be generated from the
word table. Any word features (Word-Feature-#X
columns) can be defined and will then be added
to the word table in preprocessing. In the mark-
able table, Markable-ID is the primary key. Begin-
Word-ID, End-Word-ID and Head-Word-ID refer
to the word table. Like the word features, the
markable features are not mandatory and in the
preprocessing we can decide which features are
added to the table. In the link table, Link-ID is
the primary key; First-Markable-ID and Second-
Markable-ID refer to the markable table.
</bodyText>
<subsectionHeader confidence="0.999251">
2.2 Link Generator
</subsectionHeader>
<bodyText confidence="0.9999">
For training, the system generates a positive train-
ing instance for each adjacent coreferent markable
pair and negative training instances for a markable
m and all markables disreferent with m that occur
before m (Soon et al., 2001). For decoding it gen-
erates all the possible links inside a window of 100
markables.
</bodyText>
<subsectionHeader confidence="0.998793">
2.3 Link Feature Extractor
</subsectionHeader>
<bodyText confidence="0.999905269230769">
There are two main categories of features in
SUCRE: Atomic Features and Link Features
We first explain atomic features in detail and
then turn to link features and the extraction method
we use.
Atomic Features: The current version of
SUCRE supports the atomic features of words
and markables but in the next versions we are
going to extend it to sentences, paragraphs and
documents. An atomic feature is an attribute. For
example the position of the word in the corpus
is an atomic word feature. Atomic word features
are stored in the columns of the word table called
Word-Feature-X.
In addition to word position in the corpus, doc-
ument number, paragraph number and sentence
number, the following are examples of atomic
word features which can be extracted in prepro-
cessing: Part of speech tag, Grammatical Gen-
der (male, female or neutral), Natural Gender
(male or female), Number (e.g. singular, plural or
both), Semantic Class, Type (e.g. pronoun types:
personal, reflexive, demonstrative ...), Case (e.g.
nominative, accusative, dative or genitive in Ger-
man) and Pronoun Person (first, second or third).
Other possible atomic markable features include:
</bodyText>
<page confidence="0.99456">
93
</page>
<bodyText confidence="0.998948106382979">
number of words in markable, named entity, alias,
syntactic role and semantic class.
For sentences, the following could be extracted:
number of words in the sentence and sentence
type (e.g. simple, compound or complex). For
paragraphs these features are possible: number of
words and number of sentences in the paragraph.
Finally, examples of document features include
document type (e.g. news, article or book), num-
ber of words, sentences and paragraphs in the doc-
ument.
Link Features: Link features are defined over a
pair of markables. For link feature extraction, the
head words of the markables are usually used, but
in some cases the head word may not be a suitable
choice. For example, consider the two markables
the books and a book. In both cases book is the
head word, but to distinguish which markable is
definite and which indefinite, the article must be
taken into account. Now consider the two mark-
ables the university student from Germany and the
university student from France. In this case, the
head words and the first four words of each mark-
able are the same but they can not be coreferent;
this can be detected only by looking at the last
words. Sometimes we need to consider all words
in the two markables, or even define a feature for
a markable as a unit. To cover all such cases
we need a regular feature definition language with
some keywords to select different word combina-
tions of two markables. For this purpose, we de-
fine the following variables. m1 is the first mark-
able in the pair. m1b, m1e and m1h are the first,
last and head words of the first markable in the
pair. m1a refers to all words of the first markable
in the pair. m2, m2b, m2e, m2h and m2a have
the same definitions as above but for the second
markable in the pair.
In addition to the above keywords there are
some other keywords that this paper does not have
enough space to mention (e.g. for accessing the
constant values, syntax relations or roles). The
currently available functions are: exact- and sub-
string matching (in two forms: case-sensitive and
case-insensitive), edit distance, alias, word rela-
tion, markable parse tree path, absolute value.
Two examples of link features are as follows:
</bodyText>
<construct confidence="0.759624">
• (seqmatch(m1a, m2a) &gt; 0)
&amp;&amp; (m1h.f0 == f0.N )
&amp;&amp; (m2h.f0 == f0.N )
</construct>
<bodyText confidence="0.9993246">
means that there is at least one exact match
between the words of the markables and that
the head words of both are nouns (f0 means
Word-Feature-0, which is part of speech in
our system).
</bodyText>
<listItem confidence="0.631891">
• (abs(m2b.stcnum − m1b.stcnum) == 0)
&amp;&amp; (m2h.f3 == f3.reflexive)
</listItem>
<bodyText confidence="0.9995968">
means that two markables are in the same
sentence and that the type of the sec-
ond markable head word is reflexive (f3
means Word-Feature-3, which is morpholog-
ical type in our system).
</bodyText>
<subsectionHeader confidence="0.999045">
2.4 Learning
</subsectionHeader>
<bodyText confidence="0.999925428571429">
There are four classifiers integrated in SUCRE:
Decision-Tree, Naive-Bayes, Support Vector Ma-
chine (Joachims, 2002) and Maximum-Entropy
(Tsuruoka, 2006).
When we compared these classifiers, the best
results, which are reported in Section 3, were
achieved with the Decision-Tree.
</bodyText>
<subsectionHeader confidence="0.995371">
2.5 Decoding
</subsectionHeader>
<bodyText confidence="0.99999">
In decoding, the coreference chains are created.
SUCRE uses best-first clustering for this purpose.
It searches for the best predicted antecedent from
right-to-left starting from the end of the document.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999692727272727">
Table 2 shows the results of SUCRE and the best
competitor system on the test portions of the six
languages from SemEval-2010 Task 1. Four dif-
ferent evaluation metrics were used to rank the
participating systems: MUC (Vilain et al., 1995),
B3 (Bagga and Baldwin, 1998), CEAF (Luo,
2005) and BLANC (Recasens and Hovy, in prep).
SUCRE has the best results in regular closed
annotation track of English and German (for all
metrics). Its results for gold closed annotation
track of both English and German are the best
in MUC and BLANC scoring metrics (MUC: En-
glish +27.1 German +32.5, BLANC: English +9.5
German +9.0) and for CEAF and B3 (CEAF: En-
glish -1.3 German -4.8, B3: English -2.1 German
-4.8); in comparison to the second ranked sys-
tem, the performance is clearly better in the first
case and slightly better in the second. This re-
sult shows that SUCRE has been optimized in a
way that achieves good results on the four different
scoring metrics. We view this good performance
as a demonstration of the strength of SUCRE: our
</bodyText>
<page confidence="0.99852">
94
</page>
<bodyText confidence="0.9962787">
method of feature extraction, definition and tuning
is uniform and can be optimized and applied to all
languages and tracks.
Results of SUCRE show a correlation between
the MUC and BLANC scores (the best MUC
scores of all tracks and the best BLANC scores in
11 tracks of a total 12), in our opinion this correla-
tion is not because of the high similarity between
MUC and BLANC, but it is because of the bal-
anced scores.
</bodyText>
<table confidence="0.99957676">
Language ca de en es it nl
System SUCRE (Gold Annotation)
MD-F1 100 100 100 100 98.4 100
CEAF-F1 68.7 72.9 74.3 69.8 66.0 58.8
MUC-F1 56.2 58.4 60.8 55.3 45.0 69.8
B3-F1 77.0 81.1 82.4 77.4 76.8 67.0
BLANC 63.6 66.4 70.8 64.5 56.9 65.3
System SUCRE (Regular Annotation)
MD-F1 69.7 78.4 80.7 70.3 90.8 42.3
CEAF-F1 47.2 59.9 62.7 52.9 61.3 15.9
MUC-F1 37.3 40.9 52.5 36.3 50.4 29.7
B3-F1 51.1 64.3 67.1 55.6 70.6 11.7
BLANC 54.2 53.6 61.2 51.4 57.7 46.9
System Best Competitor (Gold Annotation)
MD-F1 100 100 100 100 N/A N/A
CEAF-F1 70.5 77.7 75.6 66.6 N/A N/A
MUC-F1 42.5 25.9 33.7 24.7 N/A N/A
B3-F1 79.9 85.9 84.5 78.2 N/A N/A
BLANC 59.7 57.4 61.3 55.6 N/A N/A
System Best Competitor (Regular Annotation)
MD-F1 82.7 59.2 73.9 83.1 55.9 34.7
CEAF-F1 57.1 49.5 57.3 59.3 45.8 17.0
MUC-F1 22.9 15.4 24.6 21.7 42.7 8.3
B3-F1 64.6 50.7 61.3 66.0 46.4 17.0
BLANC 51.0 44.7 49.3 51.4 59.6 32.3
</table>
<tableCaption confidence="0.99153375">
Table 2: Results of SUCRE and the best competitor system.
Bold F1 scores indicate that the result is the best SemEval
result. MD: Markable Detection, ca: Catalan, de: German,
en:English, es: Spanish, it: Italian, nl: Dutch
</tableCaption>
<sectionHeader confidence="0.999017" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999970461538461">
In this paper, we have presented a new modular
system for coreference resolution. In comparison
with the existing systems the most important ad-
vantage of our system is its flexible method of fea-
ture engineering based on relational database and a
regular feature definition language. There are four
classifiers integrated in SUCRE: Decision-Tree,
Naive-Bayes, SVM and Maximum-Entropy. The
system is able to separately do noun, pronoun and
full coreference resolution. The system uses best-
first clustering. It searches for the best predicted
antecedent from right-to-left starting from the end
of the document.
</bodyText>
<sectionHeader confidence="0.996035" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824923076923">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines, Methods, Theory,
and Algorithms. Kluwer/Springer.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In HLT ’05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 25–32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL, pages 104–111.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens, Lluis M`arquez, Emili Sapena,
M.Ant`onia Marti, Mariona Taul´e, V´eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. In
Computational Linguistics, pages 521–544.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jovb, and Karel Jezek. 2007. Two uses of anaphora
resolution in summarization. In Information Pro-
cessing and Management, Special issue on Summa-
rization, pages 1663–1680.
Yoshimasa Tsuruoka. 2006. A simple c++ library
for maximum entropy classification. Tsujii labora-
tory, Department of Computer Science, University of
Tokyo.
Yannick Versley, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
and Xiaofeng Yang. 2008. Bart: A modular toolkit
for coreference resolution. In Proceedings of the
46nd Annual Meeting of the Association for Com-
putational Linguistics, pages 9–12.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC6
’95: Proceedings of the 6th conference on Message
understanding, pages 45–52, Morristown, NJ, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.99908">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308324">
<title confidence="0.999081">SUCRE: A Modular System for Coreference Resolution</title>
<author confidence="0.994149">Hamidreza Kobdani</author>
<author confidence="0.994149">Hinrich Sch¨utze</author>
<affiliation confidence="0.8091895">Institute for Natural Language Processing University of Stuttgart, Germany</affiliation>
<email confidence="0.998856">kobdani@ims.uni-stuttgart.de</email>
<abstract confidence="0.966420777777778">This paper presents SUCRE, a new software tool for coreference resolution and its feature engineering. It is able to separately do noun, pronoun and full coreference resolution. SUCRE introduces a new approach to the feature engineering of coreference resolution based on a relational database model and a regular feature definition language. SUCRE successfully participated in SemEval-2010 Task 1 on Coreference Resolution in Multiple Languages (Recasens et al., 2010) for gold and regular closed annotation tracks of six languages. It obtained the best results in several categories, including the regular closed annotation tracks of English and German.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains. In</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="11007" citStr="Bagga and Baldwin, 1998" startWordPosition="1743" endWordPosition="1746">6). When we compared these classifiers, the best results, which are reported in Section 3, were achieved with the Decision-Tree. 2.5 Decoding In decoding, the coreference chains are created. SUCRE uses best-first clustering for this purpose. It searches for the best predicted antecedent from right-to-left starting from the end of the document. 3 Results Table 2 shows the results of SUCRE and the best competitor system on the test portions of the six languages from SemEval-2010 Task 1. Four different evaluation metrics were used to rank the participating systems: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, in prep). SUCRE has the best results in regular closed annotation track of English and German (for all metrics). Its results for gold closed annotation track of both English and German are the best in MUC and BLANC scoring metrics (MUC: English +27.1 German +32.5, BLANC: English +9.5 German +9.0) and for CEAF and B3 (CEAF: English -1.3 German -4.8, B3: English -2.1 German -4.8); in comparison to the second ranked system, the performance is clearly better in the first case and slightly better in the second. This result shows that SUCRE has been o</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Learning to Classify Text Using Support Vector Machines, Methods, Theory, and Algorithms.</title>
<date>2002</date>
<publisher>Kluwer/Springer.</publisher>
<contexts>
<context position="10348" citStr="Joachims, 2002" startWordPosition="1642" endWordPosition="1643">) &gt; 0) &amp;&amp; (m1h.f0 == f0.N ) &amp;&amp; (m2h.f0 == f0.N ) means that there is at least one exact match between the words of the markables and that the head words of both are nouns (f0 means Word-Feature-0, which is part of speech in our system). • (abs(m2b.stcnum − m1b.stcnum) == 0) &amp;&amp; (m2h.f3 == f3.reflexive) means that two markables are in the same sentence and that the type of the second markable head word is reflexive (f3 means Word-Feature-3, which is morphological type in our system). 2.4 Learning There are four classifiers integrated in SUCRE: Decision-Tree, Naive-Bayes, Support Vector Machine (Joachims, 2002) and Maximum-Entropy (Tsuruoka, 2006). When we compared these classifiers, the best results, which are reported in Section 3, were achieved with the Decision-Tree. 2.5 Decoding In decoding, the coreference chains are created. SUCRE uses best-first clustering for this purpose. It searches for the best predicted antecedent from right-to-left starting from the end of the document. 3 Results Table 2 shows the results of SUCRE and the best competitor system on the test portions of the six languages from SemEval-2010 Task 1. Four different evaluation metrics were used to rank the participating syste</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Learning to Classify Text Using Support Vector Machines, Methods, Theory, and Algorithms. Kluwer/Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11025" citStr="Luo, 2005" startWordPosition="1748" endWordPosition="1749">ifiers, the best results, which are reported in Section 3, were achieved with the Decision-Tree. 2.5 Decoding In decoding, the coreference chains are created. SUCRE uses best-first clustering for this purpose. It searches for the best predicted antecedent from right-to-left starting from the end of the document. 3 Results Table 2 shows the results of SUCRE and the best competitor system on the test portions of the six languages from SemEval-2010 Task 1. Four different evaluation metrics were used to rank the participating systems: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, in prep). SUCRE has the best results in regular closed annotation track of English and German (for all metrics). Its results for gold closed annotation track of both English and German are the best in MUC and BLANC scoring metrics (MUC: English +27.1 German +32.5, BLANC: English +9.5 German +9.0) and for CEAF and B3 (CEAF: English -1.3 German -4.8, B3: English -2.1 German -4.8); in comparison to the second ranked system, the performance is clearly better in the first case and slightly better in the second. This result shows that SUCRE has been optimized in a way </context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 25–32, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1882" citStr="Ng and Cardie, 2002" startWordPosition="280" endWordPosition="283"> publicly available systems that perform coreference resolution, such as BART (Versley et al., 2008) and GUITAR (Steinberger et al., 2007). A considerable engineering effort is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. Thus, a system which is able to extract the features based on a feature definition language can help the researcher reduce the implementation effort needed for feature extraction. Most methods of coreference resolution, if providing a baseline, usually use a feature set similar to (Soon et al., 2001) or (Ng and Cardie, 2002) and do the feature extraction in the preprocessing stage. SUCRE has been developed to provide a more flexible method for feature engineering of coreference resolution. It has a novel approach to model an unstructured text corpus in a structured framework by using a relational database model and a regular feature definition language to define and extract the features. Relational databases are a well-known technology for structured data modeling and are supported by a wide array of software and tools. Converting a text corpus to/from its equivalent relational database model is straightforward i</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the ACL, pages 104–111.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marta</author>
</authors>
<title>Recasens and Eduard Hovy. in prep. BLANC: Implementing the Rand Index for Coreference Evaluation.</title>
<marker>Marta, </marker>
<rawString>Marta Recasens and Eduard Hovy. in prep. BLANC: Implementing the Rand Index for Coreference Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Lluis M`arquez</author>
<author>Emili Sapena</author>
<author>M Ant`onia Marti</author>
<author>Mariona Taul´e</author>
<author>V´eronique Hoste</author>
<author>Massimo Poesio</author>
<author>Yannick Versley</author>
</authors>
<title>SemEval-2010 Task 1: Coreference resolution in multiple languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010),</booktitle>
<location>Uppsala,</location>
<marker>Recasens, M`arquez, Sapena, Marti, Taul´e, Hoste, Poesio, Versley, 2010</marker>
<rawString>Marta Recasens, Lluis M`arquez, Emili Sapena, M.Ant`onia Marti, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley. 2010. SemEval-2010 Task 1: Coreference resolution in multiple languages. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>521--544</pages>
<contexts>
<context position="1857" citStr="Soon et al., 2001" startWordPosition="275" endWordPosition="278">rent. There are various publicly available systems that perform coreference resolution, such as BART (Versley et al., 2008) and GUITAR (Steinberger et al., 2007). A considerable engineering effort is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. Thus, a system which is able to extract the features based on a feature definition language can help the researcher reduce the implementation effort needed for feature extraction. Most methods of coreference resolution, if providing a baseline, usually use a feature set similar to (Soon et al., 2001) or (Ng and Cardie, 2002) and do the feature extraction in the preprocessing stage. SUCRE has been developed to provide a more flexible method for feature engineering of coreference resolution. It has a novel approach to model an unstructured text corpus in a structured framework by using a relational database model and a regular feature definition language to define and extract the features. Relational databases are a well-known technology for structured data modeling and are supported by a wide array of software and tools. Converting a text corpus to/from its equivalent relational database m</context>
<context position="6245" citStr="Soon et al., 2001" startWordPosition="945" endWordPosition="948">e table, Markable-ID is the primary key. BeginWord-ID, End-Word-ID and Head-Word-ID refer to the word table. Like the word features, the markable features are not mandatory and in the preprocessing we can decide which features are added to the table. In the link table, Link-ID is the primary key; First-Markable-ID and SecondMarkable-ID refer to the markable table. 2.2 Link Generator For training, the system generates a positive training instance for each adjacent coreferent markable pair and negative training instances for a markable m and all markables disreferent with m that occur before m (Soon et al., 2001). For decoding it generates all the possible links inside a window of 100 markables. 2.3 Link Feature Extractor There are two main categories of features in SUCRE: Atomic Features and Link Features We first explain atomic features in detail and then turn to link features and the extraction method we use. Atomic Features: The current version of SUCRE supports the atomic features of words and markables but in the next versions we are going to extend it to sentences, paragraphs and documents. An atomic feature is an attribute. For example the position of the word in the corpus is an atomic word f</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. In Computational Linguistics, pages 521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Massimo Poesio</author>
<author>Mijail A Kabadjovb</author>
<author>Karel Jezek</author>
</authors>
<title>Two uses of anaphora resolution in summarization.</title>
<date>2007</date>
<booktitle>In Information Processing and Management, Special issue on Summarization,</booktitle>
<pages>1663--1680</pages>
<contexts>
<context position="1400" citStr="Steinberger et al., 2007" startWordPosition="201" endWordPosition="205">l categories, including the regular closed annotation tracks of English and German. 1 Introduction In this paper, we introduce a new software tool for coreference resolution. Coreference resolution is the process of finding discourse entities (markables) referring to the same real-world entity or concept. In other words, this process groups the markables of a document into equivalence classes (coreference entities) so that all markables in an entity are coreferent. There are various publicly available systems that perform coreference resolution, such as BART (Versley et al., 2008) and GUITAR (Steinberger et al., 2007). A considerable engineering effort is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. Thus, a system which is able to extract the features based on a feature definition language can help the researcher reduce the implementation effort needed for feature extraction. Most methods of coreference resolution, if providing a baseline, usually use a feature set similar to (Soon et al., 2001) or (Ng and Cardie, 2002) and do the feature extraction in the preprocessing stage. SUCRE has been developed to provide a more flexible method </context>
</contexts>
<marker>Steinberger, Poesio, Kabadjovb, Jezek, 2007</marker>
<rawString>Josef Steinberger, Massimo Poesio, Mijail A. Kabadjovb, and Karel Jezek. 2007. Two uses of anaphora resolution in summarization. In Information Processing and Management, Special issue on Summarization, pages 1663–1680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
</authors>
<title>A simple c++ library for maximum entropy classification.</title>
<date>2006</date>
<institution>Tsujii laboratory, Department of Computer Science, University of Tokyo.</institution>
<contexts>
<context position="10385" citStr="Tsuruoka, 2006" startWordPosition="1646" endWordPosition="1647">0 == f0.N ) means that there is at least one exact match between the words of the markables and that the head words of both are nouns (f0 means Word-Feature-0, which is part of speech in our system). • (abs(m2b.stcnum − m1b.stcnum) == 0) &amp;&amp; (m2h.f3 == f3.reflexive) means that two markables are in the same sentence and that the type of the second markable head word is reflexive (f3 means Word-Feature-3, which is morphological type in our system). 2.4 Learning There are four classifiers integrated in SUCRE: Decision-Tree, Naive-Bayes, Support Vector Machine (Joachims, 2002) and Maximum-Entropy (Tsuruoka, 2006). When we compared these classifiers, the best results, which are reported in Section 3, were achieved with the Decision-Tree. 2.5 Decoding In decoding, the coreference chains are created. SUCRE uses best-first clustering for this purpose. It searches for the best predicted antecedent from right-to-left starting from the end of the document. 3 Results Table 2 shows the results of SUCRE and the best competitor system on the test portions of the six languages from SemEval-2010 Task 1. Four different evaluation metrics were used to rank the participating systems: MUC (Vilain et al., 1995), B3 (Ba</context>
</contexts>
<marker>Tsuruoka, 2006</marker>
<rawString>Yoshimasa Tsuruoka. 2006. A simple c++ library for maximum entropy classification. Tsujii laboratory, Department of Computer Science, University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
<author>Vladimir Eidelman</author>
<author>Alan Jern</author>
<author>Jason Smith</author>
<author>Xiaofeng Yang</author>
</authors>
<title>Bart: A modular toolkit for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--12</pages>
<contexts>
<context position="1362" citStr="Versley et al., 2008" startWordPosition="195" endWordPosition="198">btained the best results in several categories, including the regular closed annotation tracks of English and German. 1 Introduction In this paper, we introduce a new software tool for coreference resolution. Coreference resolution is the process of finding discourse entities (markables) referring to the same real-world entity or concept. In other words, this process groups the markables of a document into equivalence classes (coreference entities) so that all markables in an entity are coreferent. There are various publicly available systems that perform coreference resolution, such as BART (Versley et al., 2008) and GUITAR (Steinberger et al., 2007). A considerable engineering effort is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. Thus, a system which is able to extract the features based on a feature definition language can help the researcher reduce the implementation effort needed for feature extraction. Most methods of coreference resolution, if providing a baseline, usually use a feature set similar to (Soon et al., 2001) or (Ng and Cardie, 2002) and do the feature extraction in the preprocessing stage. SUCRE has been develo</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, 2008</marker>
<rawString>Yannick Versley, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, and Xiaofeng Yang. 2008. Bart: A modular toolkit for coreference resolution. In Proceedings of the 46nd Annual Meeting of the Association for Computational Linguistics, pages 9–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In MUC6 ’95: Proceedings of the 6th conference on Message understanding,</booktitle>
<pages>45--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10977" citStr="Vilain et al., 1995" startWordPosition="1738" endWordPosition="1741">mum-Entropy (Tsuruoka, 2006). When we compared these classifiers, the best results, which are reported in Section 3, were achieved with the Decision-Tree. 2.5 Decoding In decoding, the coreference chains are created. SUCRE uses best-first clustering for this purpose. It searches for the best predicted antecedent from right-to-left starting from the end of the document. 3 Results Table 2 shows the results of SUCRE and the best competitor system on the test portions of the six languages from SemEval-2010 Task 1. Four different evaluation metrics were used to rank the participating systems: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, in prep). SUCRE has the best results in regular closed annotation track of English and German (for all metrics). Its results for gold closed annotation track of both English and German are the best in MUC and BLANC scoring metrics (MUC: English +27.1 German +32.5, BLANC: English +9.5 German +9.0) and for CEAF and B3 (CEAF: English -1.3 German -4.8, B3: English -2.1 German -4.8); in comparison to the second ranked system, the performance is clearly better in the first case and slightly better in the second. This resu</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In MUC6 ’95: Proceedings of the 6th conference on Message understanding, pages 45–52, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>