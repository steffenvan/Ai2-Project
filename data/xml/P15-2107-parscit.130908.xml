<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024731">
<title confidence="0.990588">
Annotation and Classification of an Email Importance Corpus
</title>
<author confidence="0.998648">
Fan Zhang
</author>
<affiliation confidence="0.997452">
Computer Science Department
University of Pittsburgh
</affiliation>
<address confidence="0.707007">
Pittsburgh, PA 15260
</address>
<email confidence="0.998958">
zhangfan@cs.pitt.edu
</email>
<sectionHeader confidence="0.993899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993141875">
This paper presents an email importance
corpus annotated through Amazon Me-
chanical Turk (AMT). Annotators anno-
tate the email content type and email im-
portance for three levels of hierarchy (se-
nior manager, middle manager and em-
ployee). Each email is annotated by
5 turkers. Agreement study shows that
the agreed AMT annotations are close to
the expert annotations. The annotated
dataset demonstrates difference in propor-
tions of content type between different lev-
els. An email importance prediction sys-
tem is trained on the dataset and identifies
the unimportant emails at minimum 0.55
precision with only text-based features.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999808857142857">
It is common that people receive tens or hundreds
of emails everyday. Reading and managing all
these emails consume significant time and atten-
tion. Many efforts have been made to address the
email overload problem. There are studies mod-
eling the email importance and the recipients’ ac-
tions in order to help with the user’s interaction
with emails (Dabbish and Kraut, 2006; Dabbish
et al., 2005). Meanwhile, there are NLP studies
on spam message filtering, email intention classi-
fication, and priority email selection to reduce the
number of emails to read (Schneider, 2003; Co-
hen et al., 2004; Jeong et al., 2009; Dredze et
al., 2009). In our project, we intend to build an
email briefing system which extracts and summa-
rizes important email information for the users.
However, we believe there are critical com-
ponents missing from the current research work.
First, to the extent of our knowledge, there is lit-
tle public email corpus with email importance la-
beled. Most of the prior works were either based
</bodyText>
<note confidence="0.74002775">
Kui Xu
Research and Technology Center
Robert Bosch LLC
Palo Alto, CA 94304
</note>
<email confidence="0.902752">
Kui.Xu2@us.bosch.com
</email>
<bodyText confidence="0.999986481481481">
on surveys or private commercial data (Dabbish
and Kraut, 2006; Aberdeen et al., 2010). Second,
little attention has been paid to study the difference
of emails received by people at different levels of
hierarchy. Third, most of the prior works chose
the user’s action to the email (e.g. replies, opens)
as the indicator of email importance. However, we
argue that the user action does not necessarily in-
dicate the importance of the email. For example,
a work-related reminder email can be more impor-
tant than a regular social greeting email. However,
a user is more likely to reply to the later and keep
the information of the former in mind. Specifically
for the goal of our email briefing system, impor-
tance decided upon the user’s action is insufficient.
This paper proposes to annotate email impor-
tance on the Enron email corpus (Klimt and Yang,
2004). Emails are grouped according to the re-
cipient’s levels of hierarchy. The importance of
an email is annotated not only according to the
user’s action but also according to the importance
of the information contained in the email. The
content type of the emails are also annotated for
the email importance study. Section 3 describe the
annotation and analysis of the dataset. Section 4
describes our email importance prediction system
trained on the annotated corpus.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999790916666667">
The most relevant work is the email corpus an-
notated by Dredze et al. (Dredze et al., 2008a;
Dredze et al., 2008b). 2391 emails from inboxes
of 4 volunteers were included. Each volunteer
manually annotated whether their own emails need
to be replied or not. The annotations are reliable
as they come from the emails’ owners. However, it
lacks diversity in the user distribution with only 4
volunteers. Also, whether an email gets response
or not does not always indicate its importance.
While commercial products such as Gmail Priority
Inbox (Aberdeen et al., 2010) has a better cover-
</bodyText>
<page confidence="0.932037">
651
</page>
<bodyText confidence="0.93431925">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 651–656,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
age of users and decides the importance of emails
upon more factors1, it is unlikely to have their data
accessible to public due to user privacy concerns.
The Enron corpus is a public email corpus
widely researched (Klimt and Yang, 2004). Lam-
pert et al. (2010) annotated whether an email con-
tains action request or not based on the agreed an-
notations of three annotators. We followed sim-
ilar ideas and labeled the email importance and
content type with the agreed Amazon Mechanical
Turk annotations. Emails are selected from En-
ron employees at different levels of hierarchy and
their importance are labeled according to the im-
portance of their content. While our corpus can
be less reliable without the annotations from the
emails’ real recipients, it is more diverse and has
better descriptions of email importance.
</bodyText>
<sectionHeader confidence="0.976487" genericHeader="method">
3 Data annotation
</sectionHeader>
<subsectionHeader confidence="0.999524">
3.1 Annotation scheme
</subsectionHeader>
<bodyText confidence="0.966523137931035">
Annotators are required to select the importance of
the email from three levels: Not important, Nor-
mal and Important. Not important emails con-
tain little useful information and require no action
from the recipient. It can be junk emails missed
by the spam filter or social greeting emails that do
not require response from the recipient. Important
emails either contain very important information
to the recipient or contain urgent issues that re-
quire immediate action (e.g. change of meeting
time/place). Normal emails contain less impor-
tant information or contain less urgent issues than
Important emails. For example, emails discussing
about plans of social events after work would typ-
ically be categorized as Normal.
We also annotate the email content type as it
reveals the semantic information contained in the
emails. There are a variety of email content type
definitions (Jabbari et al., 2006; Goldstein et al.,
2006; Dabbish et al., 2005). We choose Dabbish et
al.’s definition for our work. Eight categories are
included: Action Request, Info Request, Info At-
tachment, Status Update, Scheduling, Reminder,
Social, and Other. While an email can contain
more than one type of content, annotators are re-
quired to select one primary type.
1Including user actions and action time, the user actions
not only include the Reply action but also includes actions
such as opens, manual corrections, etc.
</bodyText>
<subsectionHeader confidence="0.999911">
3.2 Annotation with AMT
</subsectionHeader>
<bodyText confidence="0.999991641025641">
Amazon Mechanical Turk is widely used in data
annotation (Lawson et al., 2010; Marge et al.,
2010). It is typically reliable for simple tasks. Ob-
serving the fact that it takes little time for a user to
decide an email’s importance, we choose AMT to
do the annotations and manage to reduce the an-
notation noise through redundant annotation.
Creamer et al. categorized the employees of the
Enron dataset to 4 groups: senior managers, mid-
dle managers, traders and employees2 (Creamer
et al., 2009). We hypothesized that the types of
emails received by different groups were different
and annotated different groups separately. Based
on Creamer et al’s work, we identified 23 senior
managers with a total of 21728 emails, 20 middle
managers with 13779 emails and 17 regular em-
ployees with 12137 emails. The trader group was
not annotated as it was more specific to Enron. For
each group, one batch of 750 assignments (email)
was released. The emails were randomly selected
from all the group members’ received emails (to
or cc’ed). Turkers were presented with all de-
tails available in the Enron dataset, including sub-
ject, sender, recipients, cclist, date and the con-
tent (with history of forwards and replies). Turkers
were required to make their choices as they were
in the position.3 Each assignment was annotated
by 5 turkers at the rate of $0.06 per Turker assign-
ment. The email type and the email importance
are decided according to the majority votes. If an
email has 3 agreed votes or higher, we call this
email agreed. Table 1 demonstrates the average
time per assignment (Time), the effectively hourly
rate (Ehr), the number of emails with message type
agreed (#TypeAgreed), importance agreed (#Im-
poAgreed) and both agreed (#AllAgreed). We find
that #AllAgreed is close to #TypeAgreed, which
indicates a major overlap between the agreed type
annotation and the agreed importance annotation.
</bodyText>
<subsectionHeader confidence="0.99816">
3.3 Data discussion
</subsectionHeader>
<bodyText confidence="0.985306818181818">
In this paper we focus on the AllAgreed emails to
mitigate the effects of annotation noise. Table 2
demonstrates the contingency table of the corpus.
2Senior managers include CEO, presidents, vice presi-
dents, chief risk officer, chief operating officer and manag-
ing directors. The other employees at management level are
categorized to middle managers
3E.g. instruction of the senior manager batch: Imagine
you were the CEO/president/vice president/managing direc-
tor of the company, categorize the emails into the three cate-
gories [Not Important], [Normal], [Important].
</bodyText>
<page confidence="0.987898">
652
</page>
<table confidence="0.996907">
Level Time (s) Ehr ($) #All #TypeAgreed #ImpoAgreed #AllAgreed
Senior (23) 40 5.400 750 589 656 574
Middle (20) 33 6.545 750 556 622 550
Employee (17) 31 6.968 750 593 643 586
</table>
<tableCaption confidence="0.985956">
Table 1: AMT annotation results, notice that #AllAgreed is close to #TypeAgreed
</tableCaption>
<table confidence="0.999978">
Act.Req Info.Req Info Status Schedule Reminder Social Other All
Senior 60 49 255 57 43 4 68 38 574
Not 0 0 0 0 0 0 33 30 63
Normal 38 37 231 51 37 4 35 8 441
Important 22 12 24 6 6 0 0 0 70
Middle 82 53 261 22 49 0 37 46 550
Not 0 0 1 0 0 0 10 32 43
Normal 64 47 247 22 49 0 27 14 470
Important 18 6 13 0 0 0 0 0 37
Employee 61 65 326 22 29 1 52 30 586
Not 0 0 1 0 0 0 8 26 35
Normal 43 62 315 22 27 1 44 4 518
Important 18 3 10 0 2 0 0 0 33
</table>
<tableCaption confidence="0.8215035">
Table 2: Contingency table of content type and importance of AllAgreed emails; bold indicates the
proportions of this category is significantly different between groups (p&lt;0.05)
</tableCaption>
<bodyText confidence="0.999954785714286">
A potential issue of the corpus is that the impor-
tance of the email is not decided by the real email
recipient. To address this concern, we compared
the AllAgreed results with the annotations from
an expert annotator. 50 emails were randomly se-
lected from AllAgreed emails for each level. The
annotator was required to check the background
of each recipient (e.g. the recipient’s position in
the company at the time, his/her department infor-
mation and the projects he/she was involved in if
these information were available online) and judge
the relationship between the email’s contacts be-
fore annotation (e.g. if the contact is a family
member or a close friend of the recipient). Agree-
ment study shows a Kappa score of 0.7970 for the
senior manager level, 0.6420 for the middle man-
ager level and 0.7845 for the employee level. It
demonstrates that the agreed Turker annotations
are as reliable as well-prepared expert annotations.
We first tested whether the content type pro-
portions were significantly different between dif-
ferent levels of hierarchy. Recipients with more
than 20 emails sampled were selected. A vector of
content type proportions was built for each recipi-
ent on his/her sampled emails. Then we applied
multivariate analysis of variance (MANOVA) to
test the difference in the means of the vectors
between levels4. We found that there were sig-
nificant differences in proportions of status up-
date (p=0.042) and social emails (p=0.035). This
agrees with the impression that the senior man-
agers spend more time on project management and
social relationship development. Following the
same approach, we tested whether there were sig-
nificant differences in importance proportions be-
tween levels. However, no significant difference
was found while we can observe a higher portion
of Important emails in the Senior group in Table 2.
In the next section, we further investigate the rela-
tionship between content type and message impor-
tance using the content type as a baseline feature
in email importance prediction.
</bodyText>
<sectionHeader confidence="0.988454" genericHeader="method">
4 Email importance prediction
</sectionHeader>
<bodyText confidence="0.999940833333333">
In this section we present a preliminary study of
automatic email importance prediction. Two base-
lines are compared, including a Majority baseline
where the most frequent class is chosen and a Type
baseline where the only feature used for classifica-
tion is the email content type.
</bodyText>
<footnote confidence="0.99680425">
4We cannot use Chi-square to test the difference between
groups directly on Table 2 as the emails sampled do not sat-
isfy the independence consumption if they come from the
same recipient
</footnote>
<page confidence="0.997112">
653
</page>
<table confidence="0.99988075">
Features Acc Kappa P(U) R(I)
Sr. Mgrs
Majority 76.83 0 0 0
Type 68.78 37.93 58.76 44.81
Text 76.34 26.96 71.83∗ 14.67†
Text+Type 78.43 33.80 75.99∗ 12.13†
Mgrs
Majority 85.45 0 0 0
Type 69.81 32.75 50.47 49.80
Text 87.09 26.64 54.67 4.17†
Text+Type 88.55 36.42 63.80∗ 7.59†
Emp
Majority 88.39 0 0 0
Type 80.34 38.63 40.21 45.12
Text 88.83 30.98 63.83∗ 1.67†
Text+Type 89.16 36.71 72.50∗ 1.67†
</table>
<tableCaption confidence="0.998338">
Table 3: Results of Experiment 1; ∗ indicates sig-
</tableCaption>
<bodyText confidence="0.9111784">
nificantly better than the Type baseline; † indicates
significantly worse than the Type baseline; bold
indicates better than all other methods. With only
text-based features, the system achieves at least
54.67 precision in identifying unimportant emails.
</bodyText>
<table confidence="0.99669125">
Groups Acc Kappa P(U) R(I)
Sr. Mgrs 77.70 19.24 65.22 10.00
Mgrs 83.27 30.03 61.90 2.70
Emp 83.10 33.89 46.94 33.33
</table>
<tableCaption confidence="0.997656">
Table 4: Cross-group results of Experiment 2
</tableCaption>
<subsectionHeader confidence="0.97304">
4.1 Feature extraction
</subsectionHeader>
<bodyText confidence="0.9997676">
While prior works have pointed out that the so-
cial features such as contacting frequency are re-
lated to the user’s action on emails (Lampert et al.,
2010; Dredze et al., 2008a), in this paper we only
focus on features that can be extracted from text.
N-gram features Binary unigram features are
extracted from the email subject and the email
content separately. Stop words are not filtered as
they might also hint the email importance.
Part-of-speech tags According to our observa-
tion, the work-related emails have more content
words than greeting emails. Thus, POS tag fea-
tures are extracted from the email content, includ-
ing the total numbers of POS tags in the text and
the average numbers of tags in each sentence. 5
</bodyText>
<footnote confidence="0.984458">
5The Part-of-speech (POS) tags are tagged with the Stan-
ford CoreNLP toolkit (Manning et al., 2014; Toutanova et al.,
2003), containing 36 POS tags as defined in the Penn Tree-
bank annotation.
</footnote>
<bodyText confidence="0.999828181818182">
Length features We observe that work-related
emails tend to be more succinct than unimpor-
tant emails such as advertisements. Thus, length
features are extracted including the length of the
email subject and email content, and the average
length of sentences in the email content.
Content features Inspired by prior works
(Lampert et al., 2010; Dredze et al., 2008a), fea-
tures that provide hints of the email content are ex-
tracted, including the number of question marks,
date information and capitalized words, etc.
</bodyText>
<subsectionHeader confidence="0.948469">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999946">
We treat our task as a multi-class classification
problem. We test classifications within-level and
cross-level with only text-based features.
Experiment 1 Each level is tested with 10-fold
cross-validation. SVM of the Weka toolkit (Hall et
al., 2009) is chosen as the classifier. To address the
data imbalance problem, the minority classes of
the training data are oversampled with the Weka
SMOTE package (Chawla et al., 2002). The pa-
rameters of SMOTE are decided according to the
class distribution of the training data.
Experiment 2 The classifiers are trained on two
levels and tested on the other level. Again, SVM is
chosen as the model and SMOTE is used to over-
sample the training data.
</bodyText>
<subsectionHeader confidence="0.99662">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.985676052631579">
Kappa6 and accuracy are chosen to evaluate the
overall performance in prediction. For our email
briefing task specifically, precision in unimpor-
tant email prediction P(U) (avoid the false recog-
nition of unimportant emails) and recall in impor-
tant email prediction R(I) (cover as many impor-
tant emails as possible) are evaluated. Paired t-
tests are utilized to compare whether there are sig-
nificant differences in performance (p &lt; 0.05).
As demonstrated in Table 3, the text-based fea-
tures are useful for the prediction of unimportant
email classification but not as useful for the recog-
nition of important emails. It also shows that
the content type is an important indicator of the
email’s importance. While the content type is not
always accessible in real life settings, the results
demonstrate the necessity of extracting semantic
information for email importance prediction. In
Table 4, precision of unimportant email prediction
</bodyText>
<footnote confidence="0.986645">
6The agreement between the system and the majority la-
bels from the Mechanical Turk
</footnote>
<page confidence="0.99819">
654
</page>
<bodyText confidence="0.999892">
is higher on the manager levels but lower on the
employee level. This indicates a potential differ-
ence of email features between the manager levels
and the employee level.
</bodyText>
<sectionHeader confidence="0.758484" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999672304347826">
In this paper we present an email importance cor-
pus collected through AMT. The dataset focuses
on the importance of the information contained in
the email instead of the email recipient’s action.
The content type of the email is also annotated and
we find differences in content type proportions be-
tween different levels of hierarchy. Experiments
demonstrate that the content type is an important
indicator of email importance. The system based
on only text-based features identifies unimportant
emails at minimum 0.5467 precision.
Agreement study shows that the agreed Turker
annotations are as good as annotations of well-
prepared expert annotators. We plan to increase
the size of our dataset through AMT. We expect
the dataset to be helpful for studies on email over-
load problems. Meanwhile, we are aware that the
current corpus lacks social and personal informa-
tion. We believe features regarding such informa-
tion (e.g. the recipient’s email history with the
contact, the recipient’s personal preference in cat-
egorizing emails, etc.) should also be incorporated
for importance prediction.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99840825">
We would like to thank Zhe Feng, Doo Soon Kim,
Lin Zhao, Sai Sudharsanan and other employees
of the Bosch RTC 1.2 group for their helpful feed-
back, Prof. Diane Litman for her instructions to
the first author and all the anonymous reviewers
for their suggestions.
This research is supported by the Research and
Technology Center of Robert Bosch LLC.
</bodyText>
<sectionHeader confidence="0.998569" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999699015151515">
Douglas Aberdeen, Ondrej Pacovsky, and Andrew
Slater. 2010. The learning behind gmail priority
inbox. In LCCC: NIPS 2010 Workshop on Learning
on Cores, Clusters and Clouds.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall,
and W Philip Kegelmeyer. 2002. Smote: synthetic
minority over-sampling technique. Journal of artifi-
cial intelligence research, 16(1):321–357.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
“speech acts”. In Dekang Lin and Dekai Wu, edi-
tors, Proceedings of EMNLP 2004, pages 309–316,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Germ´an Creamer, Ryan Rowe, Shlomo Hershkop, and
Salvatore J Stolfo. 2009. Segmentation and auto-
mated social hierarchy detection through email net-
work analysis. In Advances in Web Mining and Web
Usage Analysis, pages 40–58. Springer.
Laura A Dabbish and Robert E Kraut. 2006. Email
overload at work: an analysis of factors associated
with email strain. In Proceedings of the 2006 20th
anniversary conference on Computer supported co-
operative work, pages 431–440. ACM.
Laura A Dabbish, Robert E Kraut, Susan Fussell, and
Sara Kiesler. 2005. Understanding email use: pre-
dicting action on a message. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 691–700. ACM.
Mark Dredze, Tova Brooks, Josh Carroll, Joshua Ma-
garick, John Blitzer, and Fernando Pereira. 2008a.
Intelligent email: reply and attachment prediction.
In Proceedings of the 13th international conference
on Intelligent user interfaces, pages 321–324. ACM.
Mark Dredze, Hanna M Wallach, Danny Puller, Tova
Brooks, Josh Carroll, Joshua Magarick, John Blitzer,
Fernando Pereira, et al. 2008b. Intelligent email:
Aiding users with ai. In AAAI, pages 1524–1527.
Mark Dredze, Bill N Schilit, and Peter Norvig. 2009.
Suggesting email view filters for triage and search.
In IJCAI, pages 1414–1419.
Jade Goldstein, Andres Kwasinksi, Paul Kingsbury,
Roberta Evans Sabin, and Albert McDowell. 2006.
Annotating subsets of the enron email corpus. In
CEAS. Citeseer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.
Sanaz Jabbari, Ben Allison, David Guthrie, and Louise
Guthrie. 2006. Towards the orwellian nightmare:
separation of business and personal emails. In
Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 407–411. Association
for Computational Linguistics.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3-Volume 3, pages 1250–1259.
Association for Computational Linguistics.
Bryan Klimt and Yiming Yang. 2004. The enron cor-
pus: A new dataset for email classification research.
In Machine learning: ECML 2004, pages 217–226.
Springer.
</reference>
<page confidence="0.986097">
655
</page>
<reference confidence="0.999635142857143">
Andrew Lampert, Robert Dale, and Cecile Paris. 2010.
Detecting emails containing requests for action. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
984–992. Association for Computational Linguis-
tics.
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large
email datasets for named entity recognition with me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 71–79.
Association for Computational Linguistics.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Matthew Marge, Satanjeev Banerjee, and Alexander I
Rudnicky. 2010. Using the amazon mechanical turk
to transcribe and annotate meeting speech for extrac-
tive summarization. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon’s Mechanical Turk, pages
99–107. Association for Computational Linguistics.
Karl-Michael Schneider. 2003. A comparison of event
models for naive bayes anti-spam e-mail filtering.
In Proceedings of the tenth conference on Euro-
pean chapter of the Association for Computational
Linguistics-Volume 1, pages 307–314. Association
for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.99891">
656
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738043">
<title confidence="0.999755">Annotation and Classification of an Email Importance Corpus</title>
<author confidence="0.996108">Fan</author>
<affiliation confidence="0.9997805">Computer Science University of</affiliation>
<address confidence="0.773366">Pittsburgh, PA</address>
<email confidence="0.999393">zhangfan@cs.pitt.edu</email>
<abstract confidence="0.997219647058824">This paper presents an email importance corpus annotated through Amazon Mechanical Turk (AMT). Annotators annotate the email content type and email importance for three levels of hierarchy (senior manager, middle manager and employee). Each email is annotated by 5 turkers. Agreement study shows that the agreed AMT annotations are close to the expert annotations. The annotated dataset demonstrates difference in proportions of content type between different levels. An email importance prediction system is trained on the dataset and identifies the unimportant emails at minimum 0.55 precision with only text-based features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas Aberdeen</author>
<author>Ondrej Pacovsky</author>
<author>Andrew Slater</author>
</authors>
<title>The learning behind gmail priority inbox.</title>
<date>2010</date>
<booktitle>In LCCC: NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds.</booktitle>
<contexts>
<context position="2006" citStr="Aberdeen et al., 2010" startWordPosition="314" endWordPosition="317"> 2003; Cohen et al., 2004; Jeong et al., 2009; Dredze et al., 2009). In our project, we intend to build an email briefing system which extracts and summarizes important email information for the users. However, we believe there are critical components missing from the current research work. First, to the extent of our knowledge, there is little public email corpus with email importance labeled. Most of the prior works were either based Kui Xu Research and Technology Center Robert Bosch LLC Palo Alto, CA 94304 Kui.Xu2@us.bosch.com on surveys or private commercial data (Dabbish and Kraut, 2006; Aberdeen et al., 2010). Second, little attention has been paid to study the difference of emails received by people at different levels of hierarchy. Third, most of the prior works chose the user’s action to the email (e.g. replies, opens) as the indicator of email importance. However, we argue that the user action does not necessarily indicate the importance of the email. For example, a work-related reminder email can be more important than a regular social greeting email. However, a user is more likely to reply to the later and keep the information of the former in mind. Specifically for the goal of our email bri</context>
<context position="3824" citStr="Aberdeen et al., 2010" startWordPosition="617" endWordPosition="620">n system trained on the annotated corpus. 2 Related work The most relevant work is the email corpus annotated by Dredze et al. (Dredze et al., 2008a; Dredze et al., 2008b). 2391 emails from inboxes of 4 volunteers were included. Each volunteer manually annotated whether their own emails need to be replied or not. The annotations are reliable as they come from the emails’ owners. However, it lacks diversity in the user distribution with only 4 volunteers. Also, whether an email gets response or not does not always indicate its importance. While commercial products such as Gmail Priority Inbox (Aberdeen et al., 2010) has a better cover651 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 651–656, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics age of users and decides the importance of emails upon more factors1, it is unlikely to have their data accessible to public due to user privacy concerns. The Enron corpus is a public email corpus widely researched (Klimt and Yang, 2004). Lampert et al. (2010) annotated whether an email contains acti</context>
</contexts>
<marker>Aberdeen, Pacovsky, Slater, 2010</marker>
<rawString>Douglas Aberdeen, Ondrej Pacovsky, and Andrew Slater. 2010. The learning behind gmail priority inbox. In LCCC: NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
<author>Kevin W Bowyer</author>
<author>Lawrence O Hall</author>
<author>W Philip Kegelmeyer</author>
</authors>
<title>Smote: synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>Journal of artificial intelligence research,</journal>
<pages>16--1</pages>
<contexts>
<context position="15056" citStr="Chawla et al., 2002" startWordPosition="2487" endWordPosition="2490">010; Dredze et al., 2008a), features that provide hints of the email content are extracted, including the number of question marks, date information and capitalized words, etc. 4.2 Experiments We treat our task as a multi-class classification problem. We test classifications within-level and cross-level with only text-based features. Experiment 1 Each level is tested with 10-fold cross-validation. SVM of the Weka toolkit (Hall et al., 2009) is chosen as the classifier. To address the data imbalance problem, the minority classes of the training data are oversampled with the Weka SMOTE package (Chawla et al., 2002). The parameters of SMOTE are decided according to the class distribution of the training data. Experiment 2 The classifiers are trained on two levels and tested on the other level. Again, SVM is chosen as the model and SMOTE is used to oversample the training data. 4.3 Evaluation Kappa6 and accuracy are chosen to evaluate the overall performance in prediction. For our email briefing task specifically, precision in unimportant email prediction P(U) (avoid the false recognition of unimportant emails) and recall in important email prediction R(I) (cover as many important emails as possible) are </context>
</contexts>
<marker>Chawla, Bowyer, Hall, Kegelmeyer, 2002</marker>
<rawString>Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. 2002. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16(1):321–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to classify email into “speech acts”.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>309--316</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1409" citStr="Cohen et al., 2004" startWordPosition="214" endWordPosition="218">features. 1 Introduction It is common that people receive tens or hundreds of emails everyday. Reading and managing all these emails consume significant time and attention. Many efforts have been made to address the email overload problem. There are studies modeling the email importance and the recipients’ actions in order to help with the user’s interaction with emails (Dabbish and Kraut, 2006; Dabbish et al., 2005). Meanwhile, there are NLP studies on spam message filtering, email intention classification, and priority email selection to reduce the number of emails to read (Schneider, 2003; Cohen et al., 2004; Jeong et al., 2009; Dredze et al., 2009). In our project, we intend to build an email briefing system which extracts and summarizes important email information for the users. However, we believe there are critical components missing from the current research work. First, to the extent of our knowledge, there is little public email corpus with email importance labeled. Most of the prior works were either based Kui Xu Research and Technology Center Robert Bosch LLC Palo Alto, CA 94304 Kui.Xu2@us.bosch.com on surveys or private commercial data (Dabbish and Kraut, 2006; Aberdeen et al., 2010). S</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. 2004. Learning to classify email into “speech acts”. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 309–316, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Germ´an Creamer</author>
<author>Ryan Rowe</author>
<author>Shlomo Hershkop</author>
<author>Salvatore J Stolfo</author>
</authors>
<title>Segmentation and automated social hierarchy detection through email network analysis.</title>
<date>2009</date>
<booktitle>In Advances in Web Mining and Web Usage Analysis,</booktitle>
<pages>40--58</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6901" citStr="Creamer et al., 2009" startWordPosition="1111" endWordPosition="1114">ns not only include the Reply action but also includes actions such as opens, manual corrections, etc. 3.2 Annotation with AMT Amazon Mechanical Turk is widely used in data annotation (Lawson et al., 2010; Marge et al., 2010). It is typically reliable for simple tasks. Observing the fact that it takes little time for a user to decide an email’s importance, we choose AMT to do the annotations and manage to reduce the annotation noise through redundant annotation. Creamer et al. categorized the employees of the Enron dataset to 4 groups: senior managers, middle managers, traders and employees2 (Creamer et al., 2009). We hypothesized that the types of emails received by different groups were different and annotated different groups separately. Based on Creamer et al’s work, we identified 23 senior managers with a total of 21728 emails, 20 middle managers with 13779 emails and 17 regular employees with 12137 emails. The trader group was not annotated as it was more specific to Enron. For each group, one batch of 750 assignments (email) was released. The emails were randomly selected from all the group members’ received emails (to or cc’ed). Turkers were presented with all details available in the Enron dat</context>
</contexts>
<marker>Creamer, Rowe, Hershkop, Stolfo, 2009</marker>
<rawString>Germ´an Creamer, Ryan Rowe, Shlomo Hershkop, and Salvatore J Stolfo. 2009. Segmentation and automated social hierarchy detection through email network analysis. In Advances in Web Mining and Web Usage Analysis, pages 40–58. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura A Dabbish</author>
<author>Robert E Kraut</author>
</authors>
<title>Email overload at work: an analysis of factors associated with email strain.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work,</booktitle>
<pages>431--440</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1188" citStr="Dabbish and Kraut, 2006" startWordPosition="179" endWordPosition="182">nstrates difference in proportions of content type between different levels. An email importance prediction system is trained on the dataset and identifies the unimportant emails at minimum 0.55 precision with only text-based features. 1 Introduction It is common that people receive tens or hundreds of emails everyday. Reading and managing all these emails consume significant time and attention. Many efforts have been made to address the email overload problem. There are studies modeling the email importance and the recipients’ actions in order to help with the user’s interaction with emails (Dabbish and Kraut, 2006; Dabbish et al., 2005). Meanwhile, there are NLP studies on spam message filtering, email intention classification, and priority email selection to reduce the number of emails to read (Schneider, 2003; Cohen et al., 2004; Jeong et al., 2009; Dredze et al., 2009). In our project, we intend to build an email briefing system which extracts and summarizes important email information for the users. However, we believe there are critical components missing from the current research work. First, to the extent of our knowledge, there is little public email corpus with email importance labeled. Most o</context>
</contexts>
<marker>Dabbish, Kraut, 2006</marker>
<rawString>Laura A Dabbish and Robert E Kraut. 2006. Email overload at work: an analysis of factors associated with email strain. In Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work, pages 431–440. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura A Dabbish</author>
<author>Robert E Kraut</author>
<author>Susan Fussell</author>
<author>Sara Kiesler</author>
</authors>
<title>Understanding email use: predicting action on a message.</title>
<date>2005</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>691--700</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1211" citStr="Dabbish et al., 2005" startWordPosition="183" endWordPosition="186">oportions of content type between different levels. An email importance prediction system is trained on the dataset and identifies the unimportant emails at minimum 0.55 precision with only text-based features. 1 Introduction It is common that people receive tens or hundreds of emails everyday. Reading and managing all these emails consume significant time and attention. Many efforts have been made to address the email overload problem. There are studies modeling the email importance and the recipients’ actions in order to help with the user’s interaction with emails (Dabbish and Kraut, 2006; Dabbish et al., 2005). Meanwhile, there are NLP studies on spam message filtering, email intention classification, and priority email selection to reduce the number of emails to read (Schneider, 2003; Cohen et al., 2004; Jeong et al., 2009; Dredze et al., 2009). In our project, we intend to build an email briefing system which extracts and summarizes important email information for the users. However, we believe there are critical components missing from the current research work. First, to the extent of our knowledge, there is little public email corpus with email importance labeled. Most of the prior works were </context>
<context position="5927" citStr="Dabbish et al., 2005" startWordPosition="951" endWordPosition="954"> recipient. Important emails either contain very important information to the recipient or contain urgent issues that require immediate action (e.g. change of meeting time/place). Normal emails contain less important information or contain less urgent issues than Important emails. For example, emails discussing about plans of social events after work would typically be categorized as Normal. We also annotate the email content type as it reveals the semantic information contained in the emails. There are a variety of email content type definitions (Jabbari et al., 2006; Goldstein et al., 2006; Dabbish et al., 2005). We choose Dabbish et al.’s definition for our work. Eight categories are included: Action Request, Info Request, Info Attachment, Status Update, Scheduling, Reminder, Social, and Other. While an email can contain more than one type of content, annotators are required to select one primary type. 1Including user actions and action time, the user actions not only include the Reply action but also includes actions such as opens, manual corrections, etc. 3.2 Annotation with AMT Amazon Mechanical Turk is widely used in data annotation (Lawson et al., 2010; Marge et al., 2010). It is typically reli</context>
</contexts>
<marker>Dabbish, Kraut, Fussell, Kiesler, 2005</marker>
<rawString>Laura A Dabbish, Robert E Kraut, Susan Fussell, and Sara Kiesler. 2005. Understanding email use: predicting action on a message. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 691–700. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Tova Brooks</author>
<author>Josh Carroll</author>
<author>Joshua Magarick</author>
<author>John Blitzer</author>
<author>Fernando Pereira</author>
</authors>
<title>Intelligent email: reply and attachment prediction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th international conference on Intelligent user interfaces,</booktitle>
<pages>321--324</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3349" citStr="Dredze et al., 2008" startWordPosition="541" endWordPosition="544"> the Enron email corpus (Klimt and Yang, 2004). Emails are grouped according to the recipient’s levels of hierarchy. The importance of an email is annotated not only according to the user’s action but also according to the importance of the information contained in the email. The content type of the emails are also annotated for the email importance study. Section 3 describe the annotation and analysis of the dataset. Section 4 describes our email importance prediction system trained on the annotated corpus. 2 Related work The most relevant work is the email corpus annotated by Dredze et al. (Dredze et al., 2008a; Dredze et al., 2008b). 2391 emails from inboxes of 4 volunteers were included. Each volunteer manually annotated whether their own emails need to be replied or not. The annotations are reliable as they come from the emails’ owners. However, it lacks diversity in the user distribution with only 4 volunteers. Also, whether an email gets response or not does not always indicate its importance. While commercial products such as Gmail Priority Inbox (Aberdeen et al., 2010) has a better cover651 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In</context>
<context position="13359" citStr="Dredze et al., 2008" startWordPosition="2213" endWordPosition="2216">es significantly better than the Type baseline; † indicates significantly worse than the Type baseline; bold indicates better than all other methods. With only text-based features, the system achieves at least 54.67 precision in identifying unimportant emails. Groups Acc Kappa P(U) R(I) Sr. Mgrs 77.70 19.24 65.22 10.00 Mgrs 83.27 30.03 61.90 2.70 Emp 83.10 33.89 46.94 33.33 Table 4: Cross-group results of Experiment 2 4.1 Feature extraction While prior works have pointed out that the social features such as contacting frequency are related to the user’s action on emails (Lampert et al., 2010; Dredze et al., 2008a), in this paper we only focus on features that can be extracted from text. N-gram features Binary unigram features are extracted from the email subject and the email content separately. Stop words are not filtered as they might also hint the email importance. Part-of-speech tags According to our observation, the work-related emails have more content words than greeting emails. Thus, POS tag features are extracted from the email content, including the total numbers of POS tags in the text and the average numbers of tags in each sentence. 5 5The Part-of-speech (POS) tags are tagged with the St</context>
</contexts>
<marker>Dredze, Brooks, Carroll, Magarick, Blitzer, Pereira, 2008</marker>
<rawString>Mark Dredze, Tova Brooks, Josh Carroll, Joshua Magarick, John Blitzer, and Fernando Pereira. 2008a. Intelligent email: reply and attachment prediction. In Proceedings of the 13th international conference on Intelligent user interfaces, pages 321–324. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Hanna M Wallach</author>
<author>Danny Puller</author>
<author>Tova Brooks</author>
<author>Josh Carroll</author>
<author>Joshua Magarick</author>
<author>John Blitzer</author>
<author>Fernando Pereira</author>
</authors>
<title>Intelligent email: Aiding users with ai.</title>
<date>2008</date>
<booktitle>In AAAI,</booktitle>
<pages>1524--1527</pages>
<contexts>
<context position="3349" citStr="Dredze et al., 2008" startWordPosition="541" endWordPosition="544"> the Enron email corpus (Klimt and Yang, 2004). Emails are grouped according to the recipient’s levels of hierarchy. The importance of an email is annotated not only according to the user’s action but also according to the importance of the information contained in the email. The content type of the emails are also annotated for the email importance study. Section 3 describe the annotation and analysis of the dataset. Section 4 describes our email importance prediction system trained on the annotated corpus. 2 Related work The most relevant work is the email corpus annotated by Dredze et al. (Dredze et al., 2008a; Dredze et al., 2008b). 2391 emails from inboxes of 4 volunteers were included. Each volunteer manually annotated whether their own emails need to be replied or not. The annotations are reliable as they come from the emails’ owners. However, it lacks diversity in the user distribution with only 4 volunteers. Also, whether an email gets response or not does not always indicate its importance. While commercial products such as Gmail Priority Inbox (Aberdeen et al., 2010) has a better cover651 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In</context>
<context position="13359" citStr="Dredze et al., 2008" startWordPosition="2213" endWordPosition="2216">es significantly better than the Type baseline; † indicates significantly worse than the Type baseline; bold indicates better than all other methods. With only text-based features, the system achieves at least 54.67 precision in identifying unimportant emails. Groups Acc Kappa P(U) R(I) Sr. Mgrs 77.70 19.24 65.22 10.00 Mgrs 83.27 30.03 61.90 2.70 Emp 83.10 33.89 46.94 33.33 Table 4: Cross-group results of Experiment 2 4.1 Feature extraction While prior works have pointed out that the social features such as contacting frequency are related to the user’s action on emails (Lampert et al., 2010; Dredze et al., 2008a), in this paper we only focus on features that can be extracted from text. N-gram features Binary unigram features are extracted from the email subject and the email content separately. Stop words are not filtered as they might also hint the email importance. Part-of-speech tags According to our observation, the work-related emails have more content words than greeting emails. Thus, POS tag features are extracted from the email content, including the total numbers of POS tags in the text and the average numbers of tags in each sentence. 5 5The Part-of-speech (POS) tags are tagged with the St</context>
</contexts>
<marker>Dredze, Wallach, Puller, Brooks, Carroll, Magarick, Blitzer, Pereira, 2008</marker>
<rawString>Mark Dredze, Hanna M Wallach, Danny Puller, Tova Brooks, Josh Carroll, Joshua Magarick, John Blitzer, Fernando Pereira, et al. 2008b. Intelligent email: Aiding users with ai. In AAAI, pages 1524–1527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Bill N Schilit</author>
<author>Peter Norvig</author>
</authors>
<title>Suggesting email view filters for triage and search.</title>
<date>2009</date>
<booktitle>In IJCAI,</booktitle>
<pages>1414--1419</pages>
<contexts>
<context position="1451" citStr="Dredze et al., 2009" startWordPosition="223" endWordPosition="226">at people receive tens or hundreds of emails everyday. Reading and managing all these emails consume significant time and attention. Many efforts have been made to address the email overload problem. There are studies modeling the email importance and the recipients’ actions in order to help with the user’s interaction with emails (Dabbish and Kraut, 2006; Dabbish et al., 2005). Meanwhile, there are NLP studies on spam message filtering, email intention classification, and priority email selection to reduce the number of emails to read (Schneider, 2003; Cohen et al., 2004; Jeong et al., 2009; Dredze et al., 2009). In our project, we intend to build an email briefing system which extracts and summarizes important email information for the users. However, we believe there are critical components missing from the current research work. First, to the extent of our knowledge, there is little public email corpus with email importance labeled. Most of the prior works were either based Kui Xu Research and Technology Center Robert Bosch LLC Palo Alto, CA 94304 Kui.Xu2@us.bosch.com on surveys or private commercial data (Dabbish and Kraut, 2006; Aberdeen et al., 2010). Second, little attention has been paid to s</context>
</contexts>
<marker>Dredze, Schilit, Norvig, 2009</marker>
<rawString>Mark Dredze, Bill N Schilit, and Peter Norvig. 2009. Suggesting email view filters for triage and search. In IJCAI, pages 1414–1419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Andres Kwasinksi</author>
<author>Paul Kingsbury</author>
<author>Roberta Evans Sabin</author>
<author>Albert McDowell</author>
</authors>
<title>Annotating subsets of the enron email corpus. In CEAS.</title>
<date>2006</date>
<publisher>Citeseer.</publisher>
<contexts>
<context position="5904" citStr="Goldstein et al., 2006" startWordPosition="947" endWordPosition="950">equire response from the recipient. Important emails either contain very important information to the recipient or contain urgent issues that require immediate action (e.g. change of meeting time/place). Normal emails contain less important information or contain less urgent issues than Important emails. For example, emails discussing about plans of social events after work would typically be categorized as Normal. We also annotate the email content type as it reveals the semantic information contained in the emails. There are a variety of email content type definitions (Jabbari et al., 2006; Goldstein et al., 2006; Dabbish et al., 2005). We choose Dabbish et al.’s definition for our work. Eight categories are included: Action Request, Info Request, Info Attachment, Status Update, Scheduling, Reminder, Social, and Other. While an email can contain more than one type of content, annotators are required to select one primary type. 1Including user actions and action time, the user actions not only include the Reply action but also includes actions such as opens, manual corrections, etc. 3.2 Annotation with AMT Amazon Mechanical Turk is widely used in data annotation (Lawson et al., 2010; Marge et al., 2010</context>
</contexts>
<marker>Goldstein, Kwasinksi, Kingsbury, Sabin, McDowell, 2006</marker>
<rawString>Jade Goldstein, Andres Kwasinksi, Paul Kingsbury, Roberta Evans Sabin, and Albert McDowell. 2006. Annotating subsets of the enron email corpus. In CEAS. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="14880" citStr="Hall et al., 2009" startWordPosition="2458" endWordPosition="2461">uding the length of the email subject and email content, and the average length of sentences in the email content. Content features Inspired by prior works (Lampert et al., 2010; Dredze et al., 2008a), features that provide hints of the email content are extracted, including the number of question marks, date information and capitalized words, etc. 4.2 Experiments We treat our task as a multi-class classification problem. We test classifications within-level and cross-level with only text-based features. Experiment 1 Each level is tested with 10-fold cross-validation. SVM of the Weka toolkit (Hall et al., 2009) is chosen as the classifier. To address the data imbalance problem, the minority classes of the training data are oversampled with the Weka SMOTE package (Chawla et al., 2002). The parameters of SMOTE are decided according to the class distribution of the training data. Experiment 2 The classifiers are trained on two levels and tested on the other level. Again, SVM is chosen as the model and SMOTE is used to oversample the training data. 4.3 Evaluation Kappa6 and accuracy are chosen to evaluate the overall performance in prediction. For our email briefing task specifically, precision in unimp</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanaz Jabbari</author>
<author>Ben Allison</author>
<author>David Guthrie</author>
<author>Louise Guthrie</author>
</authors>
<title>Towards the orwellian nightmare: separation of business and personal emails.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>407--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5880" citStr="Jabbari et al., 2006" startWordPosition="943" endWordPosition="946">g emails that do not require response from the recipient. Important emails either contain very important information to the recipient or contain urgent issues that require immediate action (e.g. change of meeting time/place). Normal emails contain less important information or contain less urgent issues than Important emails. For example, emails discussing about plans of social events after work would typically be categorized as Normal. We also annotate the email content type as it reveals the semantic information contained in the emails. There are a variety of email content type definitions (Jabbari et al., 2006; Goldstein et al., 2006; Dabbish et al., 2005). We choose Dabbish et al.’s definition for our work. Eight categories are included: Action Request, Info Request, Info Attachment, Status Update, Scheduling, Reminder, Social, and Other. While an email can contain more than one type of content, annotators are required to select one primary type. 1Including user actions and action time, the user actions not only include the Reply action but also includes actions such as opens, manual corrections, etc. 3.2 Annotation with AMT Amazon Mechanical Turk is widely used in data annotation (Lawson et al., </context>
</contexts>
<marker>Jabbari, Allison, Guthrie, Guthrie, 2006</marker>
<rawString>Sanaz Jabbari, Ben Allison, David Guthrie, and Louise Guthrie. 2006. Towards the orwellian nightmare: separation of business and personal emails. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 407–411. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Chin-Yew Lin</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Semi-supervised speech act recognition in emails and forums.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1250--1259</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1429" citStr="Jeong et al., 2009" startWordPosition="219" endWordPosition="222">tion It is common that people receive tens or hundreds of emails everyday. Reading and managing all these emails consume significant time and attention. Many efforts have been made to address the email overload problem. There are studies modeling the email importance and the recipients’ actions in order to help with the user’s interaction with emails (Dabbish and Kraut, 2006; Dabbish et al., 2005). Meanwhile, there are NLP studies on spam message filtering, email intention classification, and priority email selection to reduce the number of emails to read (Schneider, 2003; Cohen et al., 2004; Jeong et al., 2009; Dredze et al., 2009). In our project, we intend to build an email briefing system which extracts and summarizes important email information for the users. However, we believe there are critical components missing from the current research work. First, to the extent of our knowledge, there is little public email corpus with email importance labeled. Most of the prior works were either based Kui Xu Research and Technology Center Robert Bosch LLC Palo Alto, CA 94304 Kui.Xu2@us.bosch.com on surveys or private commercial data (Dabbish and Kraut, 2006; Aberdeen et al., 2010). Second, little attent</context>
</contexts>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee. 2009. Semi-supervised speech act recognition in emails and forums. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1250–1259. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Klimt</author>
<author>Yiming Yang</author>
</authors>
<title>The enron corpus: A new dataset for email classification research.</title>
<date>2004</date>
<booktitle>In Machine learning: ECML 2004,</booktitle>
<pages>217--226</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2776" citStr="Klimt and Yang, 2004" startWordPosition="445" endWordPosition="448">ior works chose the user’s action to the email (e.g. replies, opens) as the indicator of email importance. However, we argue that the user action does not necessarily indicate the importance of the email. For example, a work-related reminder email can be more important than a regular social greeting email. However, a user is more likely to reply to the later and keep the information of the former in mind. Specifically for the goal of our email briefing system, importance decided upon the user’s action is insufficient. This paper proposes to annotate email importance on the Enron email corpus (Klimt and Yang, 2004). Emails are grouped according to the recipient’s levels of hierarchy. The importance of an email is annotated not only according to the user’s action but also according to the importance of the information contained in the email. The content type of the emails are also annotated for the email importance study. Section 3 describe the annotation and analysis of the dataset. Section 4 describes our email importance prediction system trained on the annotated corpus. 2 Related work The most relevant work is the email corpus annotated by Dredze et al. (Dredze et al., 2008a; Dredze et al., 2008b). 2</context>
<context position="4360" citStr="Klimt and Yang, 2004" startWordPosition="699" endWordPosition="702">tance. While commercial products such as Gmail Priority Inbox (Aberdeen et al., 2010) has a better cover651 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 651–656, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics age of users and decides the importance of emails upon more factors1, it is unlikely to have their data accessible to public due to user privacy concerns. The Enron corpus is a public email corpus widely researched (Klimt and Yang, 2004). Lampert et al. (2010) annotated whether an email contains action request or not based on the agreed annotations of three annotators. We followed similar ideas and labeled the email importance and content type with the agreed Amazon Mechanical Turk annotations. Emails are selected from Enron employees at different levels of hierarchy and their importance are labeled according to the importance of their content. While our corpus can be less reliable without the annotations from the emails’ real recipients, it is more diverse and has better descriptions of email importance. 3 Data annotation 3.</context>
</contexts>
<marker>Klimt, Yang, 2004</marker>
<rawString>Bryan Klimt and Yiming Yang. 2004. The enron corpus: A new dataset for email classification research. In Machine learning: ECML 2004, pages 217–226. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Lampert</author>
<author>Robert Dale</author>
<author>Cecile Paris</author>
</authors>
<title>Detecting emails containing requests for action. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>984--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4383" citStr="Lampert et al. (2010)" startWordPosition="703" endWordPosition="707"> products such as Gmail Priority Inbox (Aberdeen et al., 2010) has a better cover651 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 651–656, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics age of users and decides the importance of emails upon more factors1, it is unlikely to have their data accessible to public due to user privacy concerns. The Enron corpus is a public email corpus widely researched (Klimt and Yang, 2004). Lampert et al. (2010) annotated whether an email contains action request or not based on the agreed annotations of three annotators. We followed similar ideas and labeled the email importance and content type with the agreed Amazon Mechanical Turk annotations. Emails are selected from Enron employees at different levels of hierarchy and their importance are labeled according to the importance of their content. While our corpus can be less reliable without the annotations from the emails’ real recipients, it is more diverse and has better descriptions of email importance. 3 Data annotation 3.1 Annotation scheme Ann</context>
<context position="13338" citStr="Lampert et al., 2010" startWordPosition="2209" endWordPosition="2212">xperiment 1; ∗ indicates significantly better than the Type baseline; † indicates significantly worse than the Type baseline; bold indicates better than all other methods. With only text-based features, the system achieves at least 54.67 precision in identifying unimportant emails. Groups Acc Kappa P(U) R(I) Sr. Mgrs 77.70 19.24 65.22 10.00 Mgrs 83.27 30.03 61.90 2.70 Emp 83.10 33.89 46.94 33.33 Table 4: Cross-group results of Experiment 2 4.1 Feature extraction While prior works have pointed out that the social features such as contacting frequency are related to the user’s action on emails (Lampert et al., 2010; Dredze et al., 2008a), in this paper we only focus on features that can be extracted from text. N-gram features Binary unigram features are extracted from the email subject and the email content separately. Stop words are not filtered as they might also hint the email importance. Part-of-speech tags According to our observation, the work-related emails have more content words than greeting emails. Thus, POS tag features are extracted from the email content, including the total numbers of POS tags in the text and the average numbers of tags in each sentence. 5 5The Part-of-speech (POS) tags a</context>
</contexts>
<marker>Lampert, Dale, Paris, 2010</marker>
<rawString>Andrew Lampert, Robert Dale, and Cecile Paris. 2010. Detecting emails containing requests for action. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 984–992. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nolan Lawson</author>
<author>Kevin Eustice</author>
<author>Mike Perkowitz</author>
<author>Meliha Yetisgen-Yildiz</author>
</authors>
<title>Annotating large email datasets for named entity recognition with mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>71--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6484" citStr="Lawson et al., 2010" startWordPosition="1040" endWordPosition="1043">ri et al., 2006; Goldstein et al., 2006; Dabbish et al., 2005). We choose Dabbish et al.’s definition for our work. Eight categories are included: Action Request, Info Request, Info Attachment, Status Update, Scheduling, Reminder, Social, and Other. While an email can contain more than one type of content, annotators are required to select one primary type. 1Including user actions and action time, the user actions not only include the Reply action but also includes actions such as opens, manual corrections, etc. 3.2 Annotation with AMT Amazon Mechanical Turk is widely used in data annotation (Lawson et al., 2010; Marge et al., 2010). It is typically reliable for simple tasks. Observing the fact that it takes little time for a user to decide an email’s importance, we choose AMT to do the annotations and manage to reduce the annotation noise through redundant annotation. Creamer et al. categorized the employees of the Enron dataset to 4 groups: senior managers, middle managers, traders and employees2 (Creamer et al., 2009). We hypothesized that the types of emails received by different groups were different and annotated different groups separately. Based on Creamer et al’s work, we identified 23 senio</context>
</contexts>
<marker>Lawson, Eustice, Perkowitz, Yetisgen-Yildiz, 2010</marker>
<rawString>Nolan Lawson, Kevin Eustice, Mike Perkowitz, and Meliha Yetisgen-Yildiz. 2010. Annotating large email datasets for named entity recognition with mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 71–79. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="14003" citStr="Manning et al., 2014" startWordPosition="2321" endWordPosition="2324">y focus on features that can be extracted from text. N-gram features Binary unigram features are extracted from the email subject and the email content separately. Stop words are not filtered as they might also hint the email importance. Part-of-speech tags According to our observation, the work-related emails have more content words than greeting emails. Thus, POS tag features are extracted from the email content, including the total numbers of POS tags in the text and the average numbers of tags in each sentence. 5 5The Part-of-speech (POS) tags are tagged with the Stanford CoreNLP toolkit (Manning et al., 2014; Toutanova et al., 2003), containing 36 POS tags as defined in the Penn Treebank annotation. Length features We observe that work-related emails tend to be more succinct than unimportant emails such as advertisements. Thus, length features are extracted including the length of the email subject and email content, and the average length of sentences in the email content. Content features Inspired by prior works (Lampert et al., 2010; Dredze et al., 2008a), features that provide hints of the email content are extracted, including the number of question marks, date information and capitalized wo</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Satanjeev Banerjee</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Using the amazon mechanical turk to transcribe and annotate meeting speech for extractive summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>99--107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6505" citStr="Marge et al., 2010" startWordPosition="1044" endWordPosition="1047">stein et al., 2006; Dabbish et al., 2005). We choose Dabbish et al.’s definition for our work. Eight categories are included: Action Request, Info Request, Info Attachment, Status Update, Scheduling, Reminder, Social, and Other. While an email can contain more than one type of content, annotators are required to select one primary type. 1Including user actions and action time, the user actions not only include the Reply action but also includes actions such as opens, manual corrections, etc. 3.2 Annotation with AMT Amazon Mechanical Turk is widely used in data annotation (Lawson et al., 2010; Marge et al., 2010). It is typically reliable for simple tasks. Observing the fact that it takes little time for a user to decide an email’s importance, we choose AMT to do the annotations and manage to reduce the annotation noise through redundant annotation. Creamer et al. categorized the employees of the Enron dataset to 4 groups: senior managers, middle managers, traders and employees2 (Creamer et al., 2009). We hypothesized that the types of emails received by different groups were different and annotated different groups separately. Based on Creamer et al’s work, we identified 23 senior managers with a tot</context>
</contexts>
<marker>Marge, Banerjee, Rudnicky, 2010</marker>
<rawString>Matthew Marge, Satanjeev Banerjee, and Alexander I Rudnicky. 2010. Using the amazon mechanical turk to transcribe and annotate meeting speech for extractive summarization. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 99–107. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl-Michael Schneider</author>
</authors>
<title>A comparison of event models for naive bayes anti-spam e-mail filtering.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1,</booktitle>
<pages>307--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1389" citStr="Schneider, 2003" startWordPosition="212" endWordPosition="213"> only text-based features. 1 Introduction It is common that people receive tens or hundreds of emails everyday. Reading and managing all these emails consume significant time and attention. Many efforts have been made to address the email overload problem. There are studies modeling the email importance and the recipients’ actions in order to help with the user’s interaction with emails (Dabbish and Kraut, 2006; Dabbish et al., 2005). Meanwhile, there are NLP studies on spam message filtering, email intention classification, and priority email selection to reduce the number of emails to read (Schneider, 2003; Cohen et al., 2004; Jeong et al., 2009; Dredze et al., 2009). In our project, we intend to build an email briefing system which extracts and summarizes important email information for the users. However, we believe there are critical components missing from the current research work. First, to the extent of our knowledge, there is little public email corpus with email importance labeled. Most of the prior works were either based Kui Xu Research and Technology Center Robert Bosch LLC Palo Alto, CA 94304 Kui.Xu2@us.bosch.com on surveys or private commercial data (Dabbish and Kraut, 2006; Aberd</context>
</contexts>
<marker>Schneider, 2003</marker>
<rawString>Karl-Michael Schneider. 2003. A comparison of event models for naive bayes anti-spam e-mail filtering. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1, pages 307–314. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14028" citStr="Toutanova et al., 2003" startWordPosition="2325" endWordPosition="2328">at can be extracted from text. N-gram features Binary unigram features are extracted from the email subject and the email content separately. Stop words are not filtered as they might also hint the email importance. Part-of-speech tags According to our observation, the work-related emails have more content words than greeting emails. Thus, POS tag features are extracted from the email content, including the total numbers of POS tags in the text and the average numbers of tags in each sentence. 5 5The Part-of-speech (POS) tags are tagged with the Stanford CoreNLP toolkit (Manning et al., 2014; Toutanova et al., 2003), containing 36 POS tags as defined in the Penn Treebank annotation. Length features We observe that work-related emails tend to be more succinct than unimportant emails such as advertisements. Thus, length features are extracted including the length of the email subject and email content, and the average length of sentences in the email content. Content features Inspired by prior works (Lampert et al., 2010; Dredze et al., 2008a), features that provide hints of the email content are extracted, including the number of question marks, date information and capitalized words, etc. 4.2 Experiments</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>