<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.829277">
Smart Selection
</title>
<author confidence="0.968199">
Patrick Pantel
</author>
<affiliation confidence="0.916047">
Microsoft Research
</affiliation>
<address confidence="0.9594445">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.998106">
ppantel@microsoft.com
</email>
<author confidence="0.93245">
Michael Gamon
</author>
<affiliation confidence="0.884894">
Microsoft Research
</affiliation>
<address confidence="0.9570125">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.998592">
mgamon@microsoft.com
</email>
<author confidence="0.95236">
Ariel Fuxman
</author>
<affiliation confidence="0.928721">
Microsoft Research
</affiliation>
<address confidence="0.7741205">
1065 La Avenida St.
Mountain View, CA 94043, USA
</address>
<email confidence="0.997271">
arielf@microsoft.com
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999459086956522">
Natural touch interfaces, common now in
devices such as tablets and smartphones,
make it cumbersome for users to select
text. There is a need for a new text selec-
tion paradigm that goes beyond the high
acuity selection-by-mouse that we have re-
lied on for decades. In this paper, we in-
troduce such a paradigm, called Smart Se-
lection, which aims to recover a user’s in-
tended text selection from her touch input.
We model the problem using an ensemble
learning approach, which leverages mul-
tiple linguistic analysis techniques com-
bined with information from a knowledge
base and a Web graph. We collect a dataset
of true intended user selections and simu-
lated user touches via a large-scale crowd-
sourcing task, which we release to the
academic community. We show that our
model effectively addresses the smart se-
lection task and significantly outperforms
various baselines and standalone linguistic
analysis techniques.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977630434783">
The process of using a pointing device to select
a span of text has a long history dating back to
the invention of the mouse. It serves to access
functions on text spans, such as copying/pasting,
looking up a word in a dictionary, searching the
Web, or accessing other accelerators. As con-
sumers move from traditional PCs to mobile de-
vices (e.g., tablets and smartphones), touch inter-
action is replacing the pointing devices of yore.
Although more intuitive and arguably a more natu-
ral form of interaction, touch offers much less acu-
ity (colloquially referred to as the fat finger prob-
lem). To select multi-word spans today, mobile
devices require an intricate series of gestures that
results in cumbersome user experiences1. Conse-
quently, there is an opportunity to reinvent the way
users select text in such devices.
Our task is, given a single user touch, to pre-
dict the span that the user likely intended to se-
lect. We call this task smart selection. We re-
strict our prediction task to cases where a user in-
tends to perform research on a text span (dictio-
nary/thesaurus lookup, translation, searching). We
specifically consider operations on text spans that
do not form a single unit (i.e., an entity, a concept,
a topic, etc.) to be out of scope. For example, full
sentences, paragraph and page fragments are out
of scope.
Smart selection, as far as we know, is a new re-
search problem. Yet there are many threads of re-
search in the NLP community that identify multi-
word sequences, which have coherent properties.
For example, named-entity recognizers identify
entities such as people/places/organizations, chun-
kers and parsers identify syntactic constituents
such as noun phrases, key phrase detectors or term
segmentors identify term boundaries. While each
of these techniques retrieve meaningful linguistic
units, our problem is a semantic one of recovering
a user’s intent, and as such none alone solves the
entire smart selection problem.
In this paper, we model the problem of smart
selection using an ensemble learning approach.
We leverage various linguistic techniques, such as
those discussed above, and augment them with
other sources of information from a knowledge
</bodyText>
<footnote confidence="0.791333">
1In order to select a multi-word span, a user would first
have to touch on either word, then drag the left and right
boundary handles to expand it to the adjacent words.
</footnote>
<page confidence="0.891881">
1524
</page>
<note confidence="0.8338375">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1524–1533,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998700833333333">
base and a web graph. We evaluate our meth-
ods using a novel dataset constructed for our
task. We construct our dataset of true user-
intended selections by crowdsourcing the task of
a user selecting spans of text in a researching
task. We obtain 13,681 data points. For each in-
tended selection, we construct test cases for each
individual sub-word, simulating the user select-
ing via touch. The resulting testset consists of
33,912 (simulated selection, intended selection)-
pairs, which we further stratify into head, torso,
and tail subsets. We release the full dataset and
testset to the academic community for further re-
search on this new NLP task. Finally, we empir-
ically show that our ensemble model significantly
improves upon various baseline systems.
In summary, the major contributions of our re-
search are:
</bodyText>
<listItem confidence="0.995833533333333">
• We introduce a new natural language pro-
cessing task, called smart selection, which
aims to address an important problem in text
selection for touch-enabled devices;
• We conduct a large crowd-sourced user study
to collect a dataset of intended selections and
simulated user selections, which we release
to the academic community;
• We propose a machine-learned ensemble
model for smart selection, which combines
various linguistic annotation methods with
information from a large knowledge base and
web graph;
• We empirically show that our model can ef-
fectively address the smart selection task.
</listItem>
<sectionHeader confidence="0.999256" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996215">
Related work falls into three broad categories: lin-
guistic unit detection, human computer interaction
(HCI), and intent detection.
</bodyText>
<subsectionHeader confidence="0.989831">
2.1 Linguistic Unit Detection
</subsectionHeader>
<bodyText confidence="0.999936735294118">
Smart selection is closely related to the detection
of syntactic and semantic units: user selections are
often entities, noun phrases, or concepts. A first
approach to solving smart selection is to select an
entity, noun phrase, or concept that subsumes the
user selection. However, no single approach alone
can cover the entire smart selection problem. For
example, consider an approach that uses a state-of-
the-art named-entity recognizer (NER) (Chinchor,
1998; Tjong Kim Sang and De Meulder, 2003;
Finkel et al., 2005; Ratinov and Roth, 2009). We
found in our dataset (see Section 3.2) that only
a quarter of what users intend to select consists
in fact of named entities. Although an NER ap-
proach can be very useful, it is certainly not suf-
ficient. The remainder of the data can be partially
addressed with noun phrase (NP) detectors (Ab-
ney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et
al., 1999; Kudo and Matsumoto, 2001) and lists of
items in a knowledge base (KB), but again, each is
not alone sufficient. NP detectors and KB-based
methods are further very susceptible to the gen-
eration of false positives (i.e., text contains many
nested noun phrases and knowledge base items in-
clude highly ambiguous terms).
In our work, we leverage all three techniques in
order to benefit from their complementary cover-
age of user selections. We further create a novel
unit detector, called the hyperlink intent model.
Based on the assumption that Wikipedia anchor
texts are similar in nature to what users would se-
lect in a researching task, it models the problem
of recovering Wikipedia anchor texts from partial
selections.
</bodyText>
<subsectionHeader confidence="0.999906">
2.2 Human Computer Interaction
</subsectionHeader>
<bodyText confidence="0.999996928571429">
There is a substantial amount of research in the
HCI community on how to facilitate interaction
of a user with touch and speech enabled devices.
To give but a few examples of trends in this field,
Gunawardana et al. (2010) address the fat finger
problem in the use of soft keyboards on mobile de-
vices, Kumar et al. (2012) explore a novel speech
interaction paradigm for text entry, and Sakamoto
et al. (2013) introduce a technique that combines
touch and voice input on a mobile device for im-
proved navigation of user interface elements such
as commands and controls. To the best of our
knowledge, however, the problem of smart selec-
tion as we defined it has not been addressed.
</bodyText>
<subsectionHeader confidence="0.998229">
2.3 Intent detection
</subsectionHeader>
<bodyText confidence="0.9994039">
There is a long line of research in the web lit-
erature on understanding user intent. The clos-
est to smart selection is query recommendation
(Baeza-Yates et al., 2005; Zhang and Nasraoui,
2006; Boldi et al., 2008), where the goal is to sug-
gest queries that may be related to a user’s intent.
Query recommendation techniques are based ei-
ther on clustering queries by their co-clicked URL
patterns (Baeza-Yates et al., 2005) or on leverag-
ing co-occurrences of sequential queries in web
</bodyText>
<page confidence="0.985159">
1525
</page>
<bodyText confidence="0.999559363636364">
search sessions (Zhang and Nasraoui, 2006; Boldi
et al., 2008; Sadikov et al., 2010). The key dif-
ference from smart selection is that in our task the
output is a selection that is relevant to the context
of the document where the original selection ap-
pears (e.g., by adding terms neighboring the selec-
tion). In query recommendation, however, there is
no notion of a document being read by the user
and, instead, the recommendations are based ex-
clusively on the aggregation of behavior of multi-
ple users.
</bodyText>
<sectionHeader confidence="0.618248" genericHeader="method">
3 Problem Setting and Data
</sectionHeader>
<subsectionHeader confidence="0.998921">
3.1 Smart Selection Definition
</subsectionHeader>
<bodyText confidence="0.9791335">
Let D be the set of all documents. We define a
selection to be a character (offset, length)-tuple in
a document d E D. Let S be the set of all possible
selections in D and let Sd be the set of all possible
selections in d.
We define a scored smart selection, σ, in a doc-
ument d, as a pair σ = (x, y) where x E Sd is a
selection and y E R+ is a score for the selection.
We formally define the smart selection function
φ as producing a ranked scored list of all possi-
ble selections from a document and user selection
pair 2:
</bodyText>
<equation confidence="0.9256915">
φ : D x S _* (σ1, ...,σ|Sd  ||xi E Sd, yi ? yi+1)
(1)
</equation>
<bodyText confidence="0.998014421052632">
Consider a user who selects s in a document d.
Let τ be the target selection that best captures what
the user intended to select. We define the smart
selection task as recovering τ given the pair (d, s).
Our problem then is to learn a function φ that best
recovers the target selection from any user selec-
tion.
Note that even for a human, reconstructing an
intended selection from a single word selection is
not trivial. While there are some fairly clear cut
cases such as expanding the selection “Obama”
to Barack Obama in the sentence “While in
DC, Barack Obama met with...”, there are cases
where the user intention depends on extrinsic fac-
tors such as the user’s interests. For example, in
a phrase “University of California at Santa Cruz”
with a selection “California”, some (albeit proba-
bly few) users may indeed be interested in the state
of California, others in the University
</bodyText>
<footnote confidence="0.865484">
2The output consists of a ranked list of selections instead
of a single selection to allow experiences such as proposing
an n-best list to the user.
</footnote>
<bodyText confidence="0.9995948">
of California system of universities, and
yet others specifically in the University of
California at Santa Cruz. In the next
section, we describe how we obtained a dataset of
true intended user selections.
</bodyText>
<subsectionHeader confidence="0.99843">
3.2 Data
</subsectionHeader>
<bodyText confidence="0.999795777777778">
In order to obtain a representative dataset for the
smart selection task, we focus on a real-world ap-
plication of users interacting with a touch-enabled
e-reader device. In this application, a user is read-
ing a book and chooses phrases for which she
would like to get information from resources such
as a dictionary, Wikipedia, or web search. Yet, be-
cause of the touch interface, she may only touch
on a single word.
</bodyText>
<subsubsectionHeader confidence="0.580406">
3.2.1 Crowdsourced Intended Selections
</subsubsectionHeader>
<bodyText confidence="0.999901939393939">
We obtain the intended selections through the fol-
lowing crowdsourcing exercise. We use the en-
tire collection of textbooks in English from Wik-
ibooks3, a repository of publicly available text-
books. The corpus consists of 2,696 textbooks that
span a large variety of categories such as Comput-
ing, Humanities, Science, etc. We first produce
a uniform random sample of 100 books, and then
sample one paragraph from each book. The result-
ing set of 100 paragraphs is then sent to the crowd-
sourcing system. Each paragraph is evaluated by
100 judges, using a pool of 152 judges. For each
paragraph, we request the judges to select com-
plete phrases for which they would like to “learn
more in resources such as Wikipedia, search en-
gines and dictionaries”, i.e., our true user intended
selections. As a result of this exercise, we obtain
13,681 judgments, corresponding to 4,067 unique
intended selections. The distribution of number of
unique judges who selected each unique intended
selection, in a log-log scale, is shown in Figure
1. Notice that this is a Zipfian distribution since it
follows a linear trend in the log-log scale.
Intuitively, the likelihood that a phrase is of
interest to a user correlates with the number of
judges who select that phrase. We thus use the
number of judges who selected each phrase as a
proxy for the likelihood that the phrase will be
chosen by users.
The resulting dataset consists of 4,067 (d, τ)-
pairs where d is a Wikibook document paragraph
and τ is an intended selection, along with the num-
ber of judges who selected it. We further assigned
</bodyText>
<footnote confidence="0.99662">
3Available at http://wikibooks.org.
</footnote>
<page confidence="0.988047">
1526
</page>
<figure confidence="0.997101666666667">
12
10
8
6
4
2
0
0 1 2 3 4 5 6
LOG2(Unique judges that selected the intended selection)
</figure>
<figureCaption confidence="0.998148666666667">
Figure 1: Zipfian distribution of unique intended
selections vs. the number of judges who selected
them, in log-log scale.
</figureCaption>
<bodyText confidence="0.8696765">
each pair to one of five randomly chosen folds,
which are used for cross-validation experiments.
</bodyText>
<subsectionHeader confidence="0.485141">
3.2.2 Testset Construction
</subsectionHeader>
<bodyText confidence="0.99977417948718">
We define a test case as a triple (d, s, T) where
s is a simulated user selection. For each (d, T)-
pair in our dataset we construct n correspond-
ing test cases by simulating the user selections
{(d, T, si), ... , (d, T, sn)I where si, ... , sn corre-
spond to the individual words in T. In other words,
each word in T is considered as a candidate user
selection.
We discard all target selections that only a sin-
gle judge annotated since we observed that these
mostly contained errors and noise, such as full sen-
tences or nonsensical long sentence fragments.
Our first testset, labeled TALL, is the resulting
traffic-weighted multiset. That is, each test case
(d, s, T) appears k times, where k is the number
of judges who selected T in d. TALL consists of
33,913 test cases.
We further utilize the distribution of judgments
in the creation of three other testsets. Following
the stratified sampling methodology commonly
employed in the IR community, we construct
testsets for the frequently, less frequently, and
rarely annotated intended selections, which we
call HEAD, TORSO, and TAIL, respectively. We
obtain these testsets by first sorting each unique
selection according to their frequency of occur-
rence, and then partitioning the set so that HEAD
corresponds to the elements at the top of the list
that account for 20% of the judgments; TAIL cor-
responds to the elements at the bottom also ac-
counting for 20% of the judgments; and TORSO
corresponds to the remaining elements. The re-
sulting test sets, THEAD, TTORSO, TTAIL consist of
114, 2115, and 5798 test cases, respectively4.
Test sets along with fold assignments
and annotation guidelines are avail-
able at http://research.microsoft.com/en-
us/downloads/eb42522c-068e-404c-b63f-
cf632bd27344/.
</bodyText>
<subsectionHeader confidence="0.99213">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999716">
Our focus on single word selections is motivated
by the touchscreen scenario presented in Sec-
tion 1. Although our touch simulation assumes
that each word in a target selection is equally likely
to be selected by a user, in fact we expect this dis-
tribution to be non-uniform. For example, users
may tend to select the first or last word more fre-
quently than words in the middle of the target se-
lection. Or perhaps users tend to select nouns and
verbs more frequently than function words. We
consider this out of scope for our paper, but view it
as an important avenue of future investigation. Fi-
nally, for non-touchscreen environments, such as
the desktop case, it would also be interesting to
study the problem on multi-word user selections.
To get an idea of the kind of intended selections
that comprise our dataset, we broke them down ac-
cording to whether they referred to named entities
or not. Perhaps surprisingly, the fraction of named
entities in the dataset is quite low, 24.3%5. The
rest of the intended selections mostly correspond
to concepts and topics such as embouchure forma-
tion, vocal fold relaxation, NHS voucher values,
time-domain graphs, etc.
</bodyText>
<sectionHeader confidence="0.997861" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.9999891">
As argued in Section 1, existing techniques,
such as NER taggers, chunkers, Knowledge Base
lookup, etc., are geared towards aspects of the
task (i.e., NEs, concepts, KB entries), but not the
task as a whole. We can, however, combine the
outputs of these systems with a learned “meta-
model”. The meta-model ranks the combined can-
didates according to a criterion that is derived from
data that resembles real usage of smart selection
as closely as possible. This technique is known
</bodyText>
<footnote confidence="0.917260555555556">
4We stress that TALL is a multi-set, reflecting the over-
all expected user traffic from our 100 judges per paragraph.
THEAD, TTORSO, TTAIL, in contrast, are not multi-sets since
judgment frequency is already accounted for in the stratifi-
cation process, as commonly done in the IR community.
5Becker et al. (2012) report a similar finding, showing that
only 26% of questions, which a user might ask after reading
a Wikipedia article, are focused on named entities.
LOG2(Unique intended selections)
</footnote>
<page confidence="0.965432">
1527
</page>
<bodyText confidence="0.9997847">
in the machine learning community as ensemble
learning (Dietterich, 1997).
Our ensemble approach, described in this sec-
tion, serves as our main implementation of the
smart selection function 0 of Equation 1. Each of
the ensemble members are themselves a separate
implementation of 0 and will be used as a point
of comparison in our experiments. Below, we de-
scribe the ensemble members before turning to the
ensemble learner.
</bodyText>
<subsectionHeader confidence="0.628321">
4.1 Ensemble Members
4.1.1 Hyperlink Intent Model
</subsectionHeader>
<bodyText confidence="0.999735842105263">
The Hyperlink Intent Model (HIM), which lever-
ages web graph information, is a machine-learned
system based on the intuition that anchor texts in
Wikipedia are good representations of what users
might want to learn about. We build upon the fact
that Wikipedia editors write anchor texts for enti-
ties, concepts, and things of potential interest for
follow-up to other content. HIM learns to recover
anchor texts from their single word subselections.
Specifically, HIM iteratively decides whether to
expand the current selection (initially a single
word) one word to the left or right via greedy bi-
nary decisions, until a stopping condition is met.
At each step, two binary classifiers are consulted.
The first one scores the left expansion decision
and the second one scores the right expansion de-
cision. In addition, we use the same two classi-
fiers to evaluate the expansion decision “from the
outside in”, i.e., from the word next to the current
selection (left and right, respectively) to the clos-
est word in the current selection. If the probabil-
ity for expansion of any model exceeds a prede-
fined threshold, then the most probable expansion
is chosen and we continue the iteration with the
newly expanded selection as input. The algorithm
is illustrated in Figure 2.
We automatically create our training set for HIM
by first taking a random sample of 8K Wikipedia
anchor texts. We treat each anchor text as an in-
tended selection, and each word in the anchor text
as a simulated user selection. For each word to the
left (or the right) of the user selection that is part
of the anchor text, we create a positive training ex-
ample. Similarly, for each word to the left (or the
right) that is outside of the anchor text, we create a
negative training example. We include additional
negative examples using random word selections
from Wikipedia content. For this purpose we sam-
</bodyText>
<figureCaption confidence="0.9034545">
Figure 2: Hyperlink Intent Model (HIM) decoding
flow for smart selection.
</figureCaption>
<bodyText confidence="0.9994565">
ple random words that are not part of an anchor
text. Our final data consists of 2.6M data points,
with a 1:20 ratio of positive to negative examples6.
We use logistic regression as the classification
algorithm for our binary classifiers. The fea-
tures used by each model are computed over three
strings: the current selection s (initially the single-
word simulated user selection), the candidate ex-
pansion word w, and one word over from the
right or left of s. The features fall into five fea-
ture families: (1) character-level features, includ-
ing capitalization, all-cap formatting, character
length, presence of opening/closing parentheses,
presence and position of digits and non-alphabetic
characters, and minimum and average character
uni/bi/trigram frequencies (based on frequency ta-
bles computed offline from Wikipedia article con-
tent); (2) stopword features, which indicate the
presence of a stop word (from a stop word list);
(3) tf.idf scores precomputed from Wikipedia con-
tent statistics; (4) knowledge base features, which
indicate whether a string matches an item or a sub-
string of an item in the knowledge base described
in Section 4.1.2 below; and (5) lexical features,
which capture the actual string of the current se-
lection and the candidate expansion word.
</bodyText>
<subsectionHeader confidence="0.637687">
4.1.2 Unit Spotting
</subsectionHeader>
<bodyText confidence="0.999955555555556">
Our second qualitative class of ensemble members
use notions of unit that are either based on linguis-
tic constituency or knowledge base presence. The
general process is that any unit that subsumes the
user selection is treated as a smart selection can-
didate. Scoring of candidates is by normalized
length, under the assumption that in general the
most specific (longest) unit is more likely to be the
intended selection.
</bodyText>
<footnote confidence="0.941479666666667">
6Note that this training set is generated automatically and
is, by design, of a different nature than the manually labeled
data we use to train and test the ensemble model.
</footnote>
<figure confidence="0.998260125">
Left
Context
Candidate
Left
Context2 Context1
Context4
Current selection
Selected
Word 1
Selected
Word 2
Context3
Candidate
Right
Right
Context
</figure>
<page confidence="0.981717">
1528
</page>
<bodyText confidence="0.999886904761905">
Our first unit spotter, labeled NER is geared
towards recognizing named entities. We use
a commercial and proprietary state-of-the-art
NER system, trained using the perceptron algo-
rithm (Collins, 2002) over more than a million
hand-annotated labels.
Our second approach uses purely syntactic in-
formation and treats noun phrases as units. We la-
bel this model as NP. For this purpose we parse the
sentence containing the user selection with a syn-
tactic parser following (Ratnaparkhi, 1999). We
then treat every noun phrase that subsumes the
user selection as a candidate smart selection.
Finally, our third unit spotter, labeled KB, is
based on the assumption that concepts and other
entries in a knowledge base are, by nature, things
that can be of interest to people. For our knowl-
edge base lookup, we use a proprietary graph con-
sisting of knowledge from Wikipedia, Freebase,
and paid feeds from various providers from do-
mains such as entertainment, local, and finance.
</bodyText>
<subsectionHeader confidence="0.804154">
4.1.3 Heuristics
</subsectionHeader>
<bodyText confidence="0.99988125">
Our third family of ensemble members imple-
ments simple heuristics, which tend to be high pre-
cision especially in the HEAD of our data.
The first heuristic, representing the current
touch-enabled selection paradigm seen in many of
today’s tablets and smartphones, is labeled CUR. It
simply assumes that the intended selection is al-
ways the user-selected word.
The second is a capitalization-based heuristic
(CAP), which simply expands every selected capi-
talized word selection to the longest uninterrupted
sequence of capitalized words.
</bodyText>
<subsectionHeader confidence="0.99222">
4.2 Ensemble Learning
</subsectionHeader>
<bodyText confidence="0.999785735294118">
In this section, we describe how we train our meta-
learner, labeled ENS, which takes as input the can-
didate lists produced by the ensemble members
from Section 4.1, and scores each candidate, pro-
ducing a final scored ranked list.
We use logistic regression as a classification al-
gorithm to address this task. Our 22 features in
ENS consist of three main classes: (1) features
related to the individual ensemble members; (2)
features related to the user selection; and (3) fea-
tures related to the candidate smart selection. For
(1), the features consist of whether a particular
ensemble member generated the candidate smart
selection and its score for that candidate. If the
candidate smart selection is not in the candidate
list of an ensemble member, its score is set to
zero. For both (2) and (3), features account for
length and capitalization properties of the user se-
lection and the candidate smart selection (e.g., to-
ken length, ratio of capitalized tokens, ratio of cap-
italized characters, whether or not the first and last
tokens are capitalized.)
Although training data for the HIM model was
automatically generated from Wikipedia, for ENS
we desire training data that reflects the true ex-
pected user experience. For this, we use five-
fold cross-validation over our data collection de-
scribed in Section 3.2. That is, to decode a fold
with our meta-learner, we train ENS with the other
four folds. Note that every candidate selection for
a (document, user selection)-pair, (d, s), for the
same d and s, are assigned to a single fold, hence
the training process does not see any user selection
from the test set.
</bodyText>
<sectionHeader confidence="0.997381" genericHeader="method">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.938513">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999884">
Recall our testsets TALL, THEAD, TTORSO, and TTAIL
from Section 3.2.2, where a test case is defined as
a triple (d, s, τ), and where d is a document, s is a
user selection, and τ is the intended user selection.
In this section, we describe our evaluation metric
and summarize the system configurations that we
evaluate.
</bodyText>
<sectionHeader confidence="0.450349" genericHeader="method">
5.1.1 Metric
</sectionHeader>
<bodyText confidence="0.999424111111111">
In our evaluation, we apply the smart selection
function φ(d, s) (see Eq. 1) to each test case and
measure how well it recovers τ.
Let A be the set of (d, τ)-pairs from our dataset
described in Section 3.2.1 that corresponds to a
testset T. Let T (d r) be the set of all test cases
in T with a fixed d and τ. We define the macro
precision of a smart selection function, Pφ, as fol-
lows:
</bodyText>
<equation confidence="0.998589181818182">
1 Pφ =  |A  |1: Pφ(d,τ) (2)
(d,τ)EA
Pφ(d,τ) =  |T(1  |1:Pφ(d,s,τ)
(d,s,τ)ET(d,τ)
1 1:
Pφ(d, s, τ) = |φ(d, s) |
σEφ(d,s)
� 1 if σ = (x, y) ∧ x = τ
I(σ, τ) =
0 otherwise
I(σ, τ)
</equation>
<page confidence="0.987209">
1529
</page>
<table confidence="0.999546625">
CP@1 CP@2 CP@3 CP@4 CP@5
CUR 39.3 - - - -
CAP 48.9 51.0 51.2 51.8 51.8
NER 43.5 - - - -
NP 34.1 50.2 55.5 57.1 57.6
KB 50.2 50.8 50.9 50.9 50.9
HIM 48.1 48.8 48.8 48.8 48.8
ENS 56.8† 76.0$ 82.6$ 85.2$ 86.6$
</table>
<tableCaption confidence="0.999523">
Table 1: Smart selection performance, as a func-
</tableCaption>
<bodyText confidence="0.990023666666667">
tion of CP, on TALL. ‡ and † indicate statistical
significance with p = 0.01 and 0.05, respectively.
An oracle ensemble would achieve an upper bound
CP of 87.3%.
We report cumulative macro precision at
rank (CP@k) in our experiments since our
testsets contain a single true user-intended
selection for each test case7. However,
this is an overly conservative metric since
in many cases an alternative smart selection
might equally please the user. For example,
if our testset contains a user intended selec-
tion T = The University of Southern
California, then given the simulated selec-
tion “California”, both T and University of
Southern California would most likely
equally satisfy the user intent (whereas the latter
would be considered incorrect in our evaluation).
In fact, the ideal testset would further evaluate the
distance or relevance of the smart selection to the
intended user selection. We would then find per-
haps that Southern California is a more
reasonable smart selection than of Southern
California. However, precisely defining such
a relevance function and designing the guidelines
for a user study is non-trivial and left for future
work.
</bodyText>
<subsectionHeader confidence="0.743189">
5.1.2 Systems
</subsectionHeader>
<bodyText confidence="0.9986295">
In our experiments, we evaluate the follow-
ing systems, each described in detail in Sec-
tion 4: Passthrough (CUR), Capitalization (CAP),
Named-Entity Recognizer (NER), Noun Phrase
(NP), Knowledge Base (KB), Hyperlink Intent
Model (HIM), Ensemble (ENS).
</bodyText>
<subsectionHeader confidence="0.898084">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9082055">
Table 1 reports the smart selection performance on
the full traffic weighted testset TALL, as a func-
</bodyText>
<footnote confidence="0.9919905">
7Because there is only a single true intended selection for
each test case, Recall@k = CP@k.
</footnote>
<bodyText confidence="0.999960509803922">
tion of CP@k. Our ensemble approach recovers
the true user-intended selection in 56.8% of the
cases. In its top-2 and top-3 ranked smart selec-
tions, the true user-intended selection is retrieved
76.0% and 82.6% of the time, respectively. In po-
sition 1, ENS significantly outperforms all other
systems with 95% confidence. Moreover, we no-
tice that the divergence between ENS and the other
systems greatly increases for K ≥ 2, where the
significance is now at the 99% level.
The CUR system models the selection paradigm
of today’s consumer touch-enabled devices (i.e., it
assumes that the intented selection is always the
touched word). Without changing the user inter-
face, we report a 45% improvement in predicting
what the user intended to select over this baseline.
If we changed the user interface to allow two or
three options to be displayed to the user, then we
would improve by 93% and 110%, respectively.
For CUR and NER, we report results only at
K = 1 since these systems only ever return a sin-
gle smart selection. Note also that when no named
entity is found by NER, or no noun phrase is found
by NP or no knowledge base entry is found by KB,
the corresponding systems return the original user
selection as their smart selection.
CAP does not vary much across K: when the
intended selection is a capitalized multi-word, the
longest string tends to be the intended selection.
The same holds for KB.
Whereas Table 1 reports the aggregate expected
traffic performance, we further explore the per-
formance against the stratified TEEAD, TTORSO, and
TTAIL testsets. The results are summarized in Ta-
ble 2. As outlined in Section 3.2, the HEAD se-
lections tend to be disproportionately entities and
capitalized terms when compared to the TORSO
and TAIL. Hence CAP, NER and KB perform much
better on the HEAD. In fact, on the HEAD, CAP per-
forms statistically as well as the ENS model. This
means that at position 1, for systems that need to
focus only on the HEAD, a very simple solution is
adequate. For TORSO and TAIL, however, ENS
performs better. At positions 2 and 3, across all
strata, the ENS model significantly outperforms all
other systems (with 99% confidence).
Next, we studied the relative contribution of
each ensemble member to the ENS model. Fig-
ure 3 illustrates the results of the ablation study.
The ensemble member that results in the biggest
performance drop when removed is HIM. Perhaps
</bodyText>
<page confidence="0.961046">
1530
</page>
<table confidence="0.997464925925926">
CP@1 HEAD CP@3
CP@2
CUR 48.5 - -
CAP 74.2 74.7 74.8
NER 60.6 - -
NP 52.3 64.9 69.4
KB 66.7 66.7 66.7
HIM 64.4 65.7 65.7
ENS 75.8 91.8# 96.5#
TORSO
CP@1 CP@2 CP@3
36.7 - -
43.0 45.0 45.1
39.2 - -
31.0 48.2 53.8
47.0 47.9 48.1
44.7 45.2 45.4
52.71 73.7# 81.5#
TAIL
CP@1 CP@2 CP@3
26.6 - -
26.1 27.4 28.2
26.7 - -
20.0 32.2 35.7
29.9 30.1 30.1
27.9 28.2 28.2
32.41 50.7# 58.5#
</table>
<tableCaption confidence="0.943492">
Table 2: Smart selection performance, as a function of CP, on the THEAD, TTORSO, and TTAIL testsets.
‡ and † indicate statistical significance with p = 0.01 and 0.05, respectively. An oracle ensemble would
achieve an upper bound CP of 98.5%, 86.8% and 64.8% for THEAD, TTORSO, and TTAIL, respectively.
</tableCaption>
<figureCaption confidence="0.920056333333333">
Figure 3: Ablation of ensemble model members
over TALL. Each consecutive model removes one
member specified in the series name.
</figureCaption>
<bodyText confidence="0.999741454545454">
surprisingly, a first ablation of either the CAP or
KB model, two of the better individual performing
models from Table 1, leads to an ablated-ENS per-
formance that is nearly identical to the full ENS
model. One possible reason is that both tend to
generate similar candidates (i.e., many entities in
our KB are capitalized). Although the HIM model
as a standalone system does not outperform sim-
ple linguistic unit selection models, it appears to
be the most important contributor to the overall
ensemble.
</bodyText>
<subsectionHeader confidence="0.785632">
5.3 Error Analysis: Oracle Ensemble
</subsectionHeader>
<bodyText confidence="0.999971592592593">
We begin by assessing an upper bound for our en-
semble, i.e., an oracle ensemble, by assuming that
if a correct candidate is generated by any ensem-
ble member, the oracle ensemble model places it
in first position. For TALL the oracle performance
is 87.3%. In other words, our choice of ensemble
members was able to recover a correct smart se-
lection as a candidate in 87.3% of the user study
cases. For THEAD, TTORSO, and TTAIL, the oracle
performance is 98.5%, 86.8%, and 64.8%, respec-
tively.
Although our ENS model’s CP@3 is within 2-6
points of the oracle, there is room to significantly
improve our CP@1, see Table 1 and Table 2. We
analyze this opportunity by inspecting a random
sample of 200 test cases where ENS produced an
incorrect smart selection in position 1. The break-
down of these cases is: 1 case from THEAD; 50
cases from TTORSO; 149 cases from TTAIL, i.e.,
most errors occur in the TAIL.
For 146 of these cases (73%), not a single en-
semble member produced the correct target selec-
tion r as a candidate. We analyze these cases in
detail in Section 5.4. Of the remaining cases, 25,
10, 9, 4, 4, and 2 were correct in positions 2, 3, 4,
5, 6, 7, respectively. Table 3 lists some examples.
In 18 cases (33%), the result in position 1 is
very reasonable given the context and user selec-
tion (see lines 1-4 in Table 3 for examples). Often
the target selection was also found in second po-
sition. These cases highlight the need for a more
relaxed, relevance-based user study, as pointed out
at the end of Section 5.1.1.
We attributed 7 (13%) of the cases to data prob-
lems: some cases had a punctuation as a sole char-
acter user selection, some had a mishandled es-
caped quotation character, and some had a UTF-8
encoding error.
The remaining 29 (54%) were truly model er-
rors. Some examples are shown in lines 5-8 in Ta-
ble 3. We found three categories of errors here.
First, our model has learned a strong prior on pre-
ferring the original user selection (see example
line 5). From a user experience point of view,
when the model is unsure of itself, it is in fact
better not to alter her selection. Second, we also
learned a strong capitalization prior, i.e., to trust
the CAP member (see example line 6). Finally, we
noticed that we have difficulty handling user selec-
tions consisting of a stopword (we noted determin-
ers, prepositions, and the word “and”). Adding a
few simple features to ENS based on a stopwords
list or a list of closed-class words should address
this problem.
</bodyText>
<figure confidence="0.994523266666667">
Smart Selection Cumulative Precision @ Rank (ALL)
Ensemble Member Ablation
1
0.9
0.8
0.7
0.6
0.5
Cumulative Precision ENS
-HIM
-KB
-NER
-NP
1 2 3 4 5
Rank
</figure>
<page confidence="0.944659">
1531
</page>
<table confidence="0.999548411764706">
Text Snippet User Selection
1 “The Russian conquest of the South Caucasus in the 19th century split the Caucasus
speech community across two states...”
2 “...are generally something that transportation agencies would like to mini- transportation
mize...”
3 “The vocal organ of birds, the syrinx, is located at the base of the blackbird’s vocal
trachea.”
4 “An example of this may be an idealised waveform like a square wave...” waveform
5 “Tickets may be purchased from either the ticket counter or from automatic counter
machines...”
6 “PBXT features include the following: MVCC Support: MVCC stands for MVCC
Multi-version Concurrency Control.”
7 “Centers for song production pathways include the High vocal center; ro- robust
bust nucleus of archistriatum (RA); and the tracheosyringeal part of the hy-
poglossal nucleus...”
8 “...and get an 11gR2 RAC cluster database running inside virtual ma- cluster
chines...”
</table>
<tableCaption confidence="0.9848245">
Table 3: Position 1 errors when applying ENS to our test cases. The text snippet is a substring of a
paragraph presented to our judges with the target selection (7-) indicated in bold.
</tableCaption>
<figure confidence="0.995155111111111">
ENS 1st Result
South Caucasus
transportation agencies
vocal organ
idealised waveform
counter
MVCC Support
robust nucleus
RAC cluster
</figure>
<sectionHeader confidence="0.490643" genericHeader="method">
5.4 Error Analysis: Ensemble Members
</sectionHeader>
<bodyText confidence="0.991596181818182">
Over all test cases, the distribution of cases with-
out a correct candidate generated by an ensem-
ble member in the HEAD, TORSO, TAIL is 0.3%,
34.6%, and 65.1%, respectively. We manually in-
spected a random sample of 100 such test cases.
The majority of them, 83%, were large sentence
fragments, which we consider out of scope ac-
cording to our prediction task definition outlined
in Section 1. The average token length of the tar-
get selection 7- for these was 15.3. In compari-
son, we estimate the average token length of the
task-admissable cases to be 2.7 tokens. Although
most of these long fragment selections seem to
be noise, a few cases are statements that a user
would reasonably want to know more about, such
as: (i) “Talks of a merger between the NHL and
the WHA were growing” or (ii) “NaN + NaN *
1.0i”.
In 10% of the cases, we face a punctuation-
handling issue, and in each case our ensemble was
able to generate a correct candidate when fixing
the punctuation. For example, for the book title
7- = What is life?, our ensemble found the
candidate What is life, dropping the ques-
tion mark. For 7- = Near Earth Asteroid.
our ensemble found Near Earth Asteroid,
dropping the period. Similar problems occurred
with parentheses and quotation marks.
In two cases, our ensemble members dropped
a leading “the” token, e.g., for 7- = the Hume
Highway, we found Hume Highway.
Finally, 2 cases were UTF-8 encoding mistakes,
leaving five “true error” cases.
</bodyText>
<sectionHeader confidence="0.998066" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999995423076923">
We introduced a new paradigm, smart selection,
to address the cumbersome text selection capabil-
ities of today’s touch-enabled mobile devices. We
report 45% improvement in predicting what the
user intended to select over current touch-enabled
consumer platforms, such as iOS, Android and
Windows. We release to the community a dataset
of 33,912 crowdsourced true intended user selec-
tions and corresponding simulated user touches.
There are many avenues for future work, includ-
ing understanding the distribution of user touches
on their intended selection, other interesting sce-
narios (e.g., going beyond the e-reader towards
document editors and web browsers may show dif-
ferent distributions in what users select), leverag-
ing other sources of signal such as a user’s profile,
her interests and her local session context, and ex-
ploring user interfaces that leverage n-best smart
selection prediction lists, for example by provid-
ing selection options to the user after her touch.
With the release of our 33, 912-crowdsourced
dataset and our model analyses, it is our hope that
the research community can help accelerate the
progress towards reinventing the way text selec-
tion occurs today, the initial steps for which we
have taken in this paper.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.937097">
The authors thank Aitao Chen for sharing his
NER tagger for our experiments, and Bernhard
Kohlmeier, Pradeep Chilakamarri, Ashok Chan-
dra, David Hamilton, and Bo Zhao for their guid-
ance and valuable discussions.
</bodyText>
<page confidence="0.993459">
1532
</page>
<sectionHeader confidence="0.995872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709706666667">
Steven. P. Abney. 1991. Parsing by chunks. In
Robert C. Berwick, Steven P. Abney, and Carol
Tenny, editors, Principle-Based Parsing: Computa-
tion and Psycholinguistics, pages 257–278. Kluwer,
Dordrecht.
Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo
Mendoza. 2005. Query recommendation using
query logs in search engines. In Current Trends
in Database Technology-EDBT 2004 Workshops,
pages 588–596. Springer.
Lee Becker, Sumit Basu, and Lucy Vanderwende.
2012. Mind the gap: Learning to choose gaps for
question generation. In Proceedings of NAACL HLT
’12, pages 742–751.
Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-
ora Donato, Aristides Gionis, and Sebastiano Vigna.
2008. The query-flow graph: model and applica-
tions. In Proceedings of CIKM ’08, pages 609–618.
ACM.
Nancy A. Chinchor. 1998. Named entity task defini-
tion. In Proceedings of the Seventh Message Under-
standing Conference (MUC-7), Fairfax, VA.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Thomas G. Dietterich. 1997. Machine Learning Re-
search - Four Current Directions. AI Magazine,
18:4:97–136.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In In ACL, pages 363–370.
Asela Gunawardana, Tim Paek, and Christopher Meek.
2010. Usability guided key-target resizing for soft
keyboards. In Proceedings of IUI ’10, pages 111–
118.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
’01, pages 1–8.
Anuj Kumar, Tim Paek, and Bongshin Lee. 2012.
Voice typing: A new speech interaction model for
dictation on touchscreen devices. In Proceedings of
CHI’12, pages 2277–2286.
Marcia Mu˜noz, Vasin Punyakanok, Dan Roth, and Dav
Zimak. 1999. A learning approach to shallow pars-
ing. In Proceedings of EMNLP/VLC, pages 168–
178.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the 3rd ACL Workshop on Very
Large Corpora, pages 82–94. Cambridge MA, USA.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL-2009, pages 147–155.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34(1-3):151–175, February.
Eldar Sadikov, Jayant Madhavan, Lu Wang, and Alon
Halevy. 2010. Clustering query refinements by user
intent. In Proceedings of the 19th international con-
ference on World wide web, pages 841–850. ACM.
Daisuke Sakamoto, Takanori Komatsu, and Takeo
Igarashi. 2013. Voice augmented manipulation: us-
ing paralinguistic information to manipulate mobile
devices. In Mobile HCI, pages 69–78.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003, pages 142–147. Ed-
monton, Canada.
Zhiyong Zhang and Olfa Nasraoui. 2006. Mining
search engine query logs for query recommendation.
In Proceedings of the 15th international conference
on World Wide Web, pages 1039–1040. ACM.
</reference>
<page confidence="0.977884">
1533
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.541062">
<title confidence="0.999821">Smart Selection</title>
<author confidence="0.998888">Patrick Pantel</author>
<affiliation confidence="0.999411">Microsoft Research</affiliation>
<address confidence="0.8808775">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.999609">ppantel@microsoft.com</email>
<author confidence="0.999985">Michael Gamon</author>
<affiliation confidence="0.999796">Microsoft Research</affiliation>
<address confidence="0.8797265">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.999494">mgamon@microsoft.com</email>
<author confidence="0.965195">Ariel</author>
<affiliation confidence="0.974342">Microsoft</affiliation>
<address confidence="0.9989625">1065 La Avenida Mountain View, CA 94043,</address>
<email confidence="0.999546">arielf@microsoft.com</email>
<abstract confidence="0.999774458333333">Natural touch interfaces, common now in devices such as tablets and smartphones, make it cumbersome for users to select text. There is a need for a new text selection paradigm that goes beyond the high acuity selection-by-mouse that we have relied on for decades. In this paper, we insuch a paradigm, called Sewhich aims to recover a user’s intended text selection from her touch input. We model the problem using an ensemble learning approach, which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph. We collect a dataset of true intended user selections and simulated user touches via a large-scale crowdsourcing task, which we release to the academic community. We show that our model effectively addresses the smart selection task and significantly outperforms various baselines and standalone linguistic analysis techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>Principle-Based Parsing: Computation and Psycholinguistics,</booktitle>
<pages>257--278</pages>
<editor>In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="6244" citStr="Abney, 1991" startWordPosition="998" endWordPosition="1000">t that subsumes the user selection. However, no single approach alone can cover the entire smart selection problem. For example, consider an approach that uses a state-ofthe-art named-entity recognizer (NER) (Chinchor, 1998; Tjong Kim Sang and De Meulder, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested noun phrases and knowledge base items include highly ambiguous terms). In our work, we leverage all three techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia </context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven. P. Abney. 1991. Parsing by chunks. In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors, Principle-Based Parsing: Computation and Psycholinguistics, pages 257–278. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Carlos Hurtado</author>
<author>Marcelo Mendoza</author>
</authors>
<title>Query recommendation using query logs in search engines.</title>
<date>2005</date>
<booktitle>In Current Trends in Database Technology-EDBT 2004 Workshops,</booktitle>
<pages>588--596</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7909" citStr="Baeza-Yates et al., 2005" startWordPosition="1279" endWordPosition="1282">nger problem in the use of soft keyboards on mobile devices, Kumar et al. (2012) explore a novel speech interaction paradigm for text entry, and Sakamoto et al. (2013) introduce a technique that combines touch and voice input on a mobile device for improved navigation of user interface elements such as commands and controls. To the best of our knowledge, however, the problem of smart selection as we defined it has not been addressed. 2.3 Intent detection There is a long line of research in the web literature on understanding user intent. The closest to smart selection is query recommendation (Baeza-Yates et al., 2005; Zhang and Nasraoui, 2006; Boldi et al., 2008), where the goal is to suggest queries that may be related to a user’s intent. Query recommendation techniques are based either on clustering queries by their co-clicked URL patterns (Baeza-Yates et al., 2005) or on leveraging co-occurrences of sequential queries in web 1525 search sessions (Zhang and Nasraoui, 2006; Boldi et al., 2008; Sadikov et al., 2010). The key difference from smart selection is that in our task the output is a selection that is relevant to the context of the document where the original selection appears (e.g., by adding ter</context>
</contexts>
<marker>Baeza-Yates, Hurtado, Mendoza, 2005</marker>
<rawString>Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Mendoza. 2005. Query recommendation using query logs in search engines. In Current Trends in Database Technology-EDBT 2004 Workshops, pages 588–596. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Becker</author>
<author>Sumit Basu</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Mind the gap: Learning to choose gaps for question generation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL HLT ’12,</booktitle>
<pages>742--751</pages>
<contexts>
<context position="16752" citStr="Becker et al. (2012)" startWordPosition="2801" endWordPosition="2804">ntries), but not the task as a whole. We can, however, combine the outputs of these systems with a learned “metamodel”. The meta-model ranks the combined candidates according to a criterion that is derived from data that resembles real usage of smart selection as closely as possible. This technique is known 4We stress that TALL is a multi-set, reflecting the overall expected user traffic from our 100 judges per paragraph. THEAD, TTORSO, TTAIL, in contrast, are not multi-sets since judgment frequency is already accounted for in the stratification process, as commonly done in the IR community. 5Becker et al. (2012) report a similar finding, showing that only 26% of questions, which a user might ask after reading a Wikipedia article, are focused on named entities. LOG2(Unique intended selections) 1527 in the machine learning community as ensemble learning (Dietterich, 1997). Our ensemble approach, described in this section, serves as our main implementation of the smart selection function 0 of Equation 1. Each of the ensemble members are themselves a separate implementation of 0 and will be used as a point of comparison in our experiments. Below, we describe the ensemble members before turning to the ens</context>
</contexts>
<marker>Becker, Basu, Vanderwende, 2012</marker>
<rawString>Lee Becker, Sumit Basu, and Lucy Vanderwende. 2012. Mind the gap: Learning to choose gaps for question generation. In Proceedings of NAACL HLT ’12, pages 742–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Boldi</author>
<author>Francesco Bonchi</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Sebastiano Vigna</author>
</authors>
<title>The query-flow graph: model and applications.</title>
<date>2008</date>
<booktitle>In Proceedings of CIKM ’08,</booktitle>
<pages>609--618</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7956" citStr="Boldi et al., 2008" startWordPosition="1287" endWordPosition="1290">devices, Kumar et al. (2012) explore a novel speech interaction paradigm for text entry, and Sakamoto et al. (2013) introduce a technique that combines touch and voice input on a mobile device for improved navigation of user interface elements such as commands and controls. To the best of our knowledge, however, the problem of smart selection as we defined it has not been addressed. 2.3 Intent detection There is a long line of research in the web literature on understanding user intent. The closest to smart selection is query recommendation (Baeza-Yates et al., 2005; Zhang and Nasraoui, 2006; Boldi et al., 2008), where the goal is to suggest queries that may be related to a user’s intent. Query recommendation techniques are based either on clustering queries by their co-clicked URL patterns (Baeza-Yates et al., 2005) or on leveraging co-occurrences of sequential queries in web 1525 search sessions (Zhang and Nasraoui, 2006; Boldi et al., 2008; Sadikov et al., 2010). The key difference from smart selection is that in our task the output is a selection that is relevant to the context of the document where the original selection appears (e.g., by adding terms neighboring the selection). In query recomme</context>
</contexts>
<marker>Boldi, Bonchi, Castillo, Donato, Gionis, Vigna, 2008</marker>
<rawString>Paolo Boldi, Francesco Bonchi, Carlos Castillo, Debora Donato, Aristides Gionis, and Sebastiano Vigna. 2008. The query-flow graph: model and applications. In Proceedings of CIKM ’08, pages 609–618. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy A Chinchor</author>
</authors>
<title>Named entity task definition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<location>Fairfax, VA.</location>
<contexts>
<context position="5856" citStr="Chinchor, 1998" startWordPosition="928" endWordPosition="929">work falls into three broad categories: linguistic unit detection, human computer interaction (HCI), and intent detection. 2.1 Linguistic Unit Detection Smart selection is closely related to the detection of syntactic and semantic units: user selections are often entities, noun phrases, or concepts. A first approach to solving smart selection is to select an entity, noun phrase, or concept that subsumes the user selection. However, no single approach alone can cover the entire smart selection problem. For example, consider an approach that uses a state-ofthe-art named-entity recognizer (NER) (Chinchor, 1998; Tjong Kim Sang and De Meulder, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very </context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>Nancy A. Chinchor. 1998. Named entity task definition. In Proceedings of the Seventh Message Understanding Conference (MUC-7), Fairfax, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="21604" citStr="Collins, 2002" startWordPosition="3589" endWordPosition="3590">t in general the most specific (longest) unit is more likely to be the intended selection. 6Note that this training set is generated automatically and is, by design, of a different nature than the manually labeled data we use to train and test the ensemble model. Left Context Candidate Left Context2 Context1 Context4 Current selection Selected Word 1 Selected Word 2 Context3 Candidate Right Right Context 1528 Our first unit spotter, labeled NER is geared towards recognizing named entities. We use a commercial and proprietary state-of-the-art NER system, trained using the perceptron algorithm (Collins, 2002) over more than a million hand-annotated labels. Our second approach uses purely syntactic information and treats noun phrases as units. We label this model as NP. For this purpose we parse the sentence containing the user selection with a syntactic parser following (Ratnaparkhi, 1999). We then treat every noun phrase that subsumes the user selection as a candidate smart selection. Finally, our third unit spotter, labeled KB, is based on the assumption that concepts and other entries in a knowledge base are, by nature, things that can be of interest to people. For our knowledge base lookup, we</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning Research - Four Current Directions. AI Magazine,</booktitle>
<pages>18--4</pages>
<contexts>
<context position="17015" citStr="Dietterich, 1997" startWordPosition="2842" endWordPosition="2843">osely as possible. This technique is known 4We stress that TALL is a multi-set, reflecting the overall expected user traffic from our 100 judges per paragraph. THEAD, TTORSO, TTAIL, in contrast, are not multi-sets since judgment frequency is already accounted for in the stratification process, as commonly done in the IR community. 5Becker et al. (2012) report a similar finding, showing that only 26% of questions, which a user might ask after reading a Wikipedia article, are focused on named entities. LOG2(Unique intended selections) 1527 in the machine learning community as ensemble learning (Dietterich, 1997). Our ensemble approach, described in this section, serves as our main implementation of the smart selection function 0 of Equation 1. Each of the ensemble members are themselves a separate implementation of 0 and will be used as a point of comparison in our experiments. Below, we describe the ensemble members before turning to the ensemble learner. 4.1 Ensemble Members 4.1.1 Hyperlink Intent Model The Hyperlink Intent Model (HIM), which leverages web graph information, is a machine-learned system based on the intuition that anchor texts in Wikipedia are good representations of what users migh</context>
</contexts>
<marker>Dietterich, 1997</marker>
<rawString>Thomas G. Dietterich. 1997. Machine Learning Research - Four Current Directions. AI Magazine, 18:4:97–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In In ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="5914" citStr="Finkel et al., 2005" startWordPosition="937" endWordPosition="940">it detection, human computer interaction (HCI), and intent detection. 2.1 Linguistic Unit Detection Smart selection is closely related to the detection of syntactic and semantic units: user selections are often entities, noun phrases, or concepts. A first approach to solving smart selection is to select an entity, noun phrase, or concept that subsumes the user selection. However, no single approach alone can cover the entire smart selection problem. For example, consider an approach that uses a state-ofthe-art named-entity recognizer (NER) (Chinchor, 1998; Tjong Kim Sang and De Meulder, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., te</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In In ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asela Gunawardana</author>
<author>Tim Paek</author>
<author>Christopher Meek</author>
</authors>
<title>Usability guided key-target resizing for soft keyboards.</title>
<date>2010</date>
<booktitle>In Proceedings of IUI ’10,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="7266" citStr="Gunawardana et al. (2010)" startWordPosition="1167" endWordPosition="1170">ee techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar in nature to what users would select in a researching task, it models the problem of recovering Wikipedia anchor texts from partial selections. 2.2 Human Computer Interaction There is a substantial amount of research in the HCI community on how to facilitate interaction of a user with touch and speech enabled devices. To give but a few examples of trends in this field, Gunawardana et al. (2010) address the fat finger problem in the use of soft keyboards on mobile devices, Kumar et al. (2012) explore a novel speech interaction paradigm for text entry, and Sakamoto et al. (2013) introduce a technique that combines touch and voice input on a mobile device for improved navigation of user interface elements such as commands and controls. To the best of our knowledge, however, the problem of smart selection as we defined it has not been addressed. 2.3 Intent detection There is a long line of research in the web literature on understanding user intent. The closest to smart selection is que</context>
</contexts>
<marker>Gunawardana, Paek, Meek, 2010</marker>
<rawString>Asela Gunawardana, Tim Paek, and Christopher Meek. 2010. Usability guided key-target resizing for soft keyboards. In Proceedings of IUI ’10, pages 111– 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL ’01,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6318" citStr="Kudo and Matsumoto, 2001" startWordPosition="1009" endWordPosition="1012">ach alone can cover the entire smart selection problem. For example, consider an approach that uses a state-ofthe-art named-entity recognizer (NER) (Chinchor, 1998; Tjong Kim Sang and De Meulder, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested noun phrases and knowledge base items include highly ambiguous terms). In our work, we leverage all three techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar in nature to what users would select in a researc</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL ’01, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anuj Kumar</author>
<author>Tim Paek</author>
<author>Bongshin Lee</author>
</authors>
<title>Voice typing: A new speech interaction model for dictation on touchscreen devices.</title>
<date>2012</date>
<booktitle>In Proceedings of CHI’12,</booktitle>
<pages>2277--2286</pages>
<contexts>
<context position="7365" citStr="Kumar et al. (2012)" startWordPosition="1186" endWordPosition="1189"> a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar in nature to what users would select in a researching task, it models the problem of recovering Wikipedia anchor texts from partial selections. 2.2 Human Computer Interaction There is a substantial amount of research in the HCI community on how to facilitate interaction of a user with touch and speech enabled devices. To give but a few examples of trends in this field, Gunawardana et al. (2010) address the fat finger problem in the use of soft keyboards on mobile devices, Kumar et al. (2012) explore a novel speech interaction paradigm for text entry, and Sakamoto et al. (2013) introduce a technique that combines touch and voice input on a mobile device for improved navigation of user interface elements such as commands and controls. To the best of our knowledge, however, the problem of smart selection as we defined it has not been addressed. 2.3 Intent detection There is a long line of research in the web literature on understanding user intent. The closest to smart selection is query recommendation (Baeza-Yates et al., 2005; Zhang and Nasraoui, 2006; Boldi et al., 2008), where t</context>
</contexts>
<marker>Kumar, Paek, Lee, 2012</marker>
<rawString>Anuj Kumar, Tim Paek, and Bongshin Lee. 2012. Voice typing: A new speech interaction model for dictation on touchscreen devices. In Proceedings of CHI’12, pages 2277–2286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcia Mu˜noz</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Dav Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC,</booktitle>
<pages>168--178</pages>
<marker>Mu˜noz, Punyakanok, Roth, Zimak, 1999</marker>
<rawString>Marcia Mu˜noz, Vasin Punyakanok, Dan Roth, and Dav Zimak. 1999. A learning approach to shallow parsing. In Proceedings of EMNLP/VLC, pages 168– 178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd ACL Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<location>Cambridge MA, USA.</location>
<contexts>
<context position="6270" citStr="Ramshaw and Marcus, 1995" startWordPosition="1001" endWordPosition="1004">es the user selection. However, no single approach alone can cover the entire smart selection problem. For example, consider an approach that uses a state-ofthe-art named-entity recognizer (NER) (Chinchor, 1998; Tjong Kim Sang and De Meulder, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested noun phrases and knowledge base items include highly ambiguous terms). In our work, we leverage all three techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar i</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the 3rd ACL Workshop on Very Large Corpora, pages 82–94. Cambridge MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL-2009,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="5939" citStr="Ratinov and Roth, 2009" startWordPosition="941" endWordPosition="944">omputer interaction (HCI), and intent detection. 2.1 Linguistic Unit Detection Smart selection is closely related to the detection of syntactic and semantic units: user selections are often entities, noun phrases, or concepts. A first approach to solving smart selection is to select an entity, noun phrase, or concept that subsumes the user selection. However, no single approach alone can cover the entire smart selection problem. For example, consider an approach that uses a state-ofthe-art named-entity recognizer (NER) (Chinchor, 1998; Tjong Kim Sang and De Meulder, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested n</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of CoNLL-2009, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<pages>34--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="21890" citStr="Ratnaparkhi, 1999" startWordPosition="3636" endWordPosition="3637">date Left Context2 Context1 Context4 Current selection Selected Word 1 Selected Word 2 Context3 Candidate Right Right Context 1528 Our first unit spotter, labeled NER is geared towards recognizing named entities. We use a commercial and proprietary state-of-the-art NER system, trained using the perceptron algorithm (Collins, 2002) over more than a million hand-annotated labels. Our second approach uses purely syntactic information and treats noun phrases as units. We label this model as NP. For this purpose we parse the sentence containing the user selection with a syntactic parser following (Ratnaparkhi, 1999). We then treat every noun phrase that subsumes the user selection as a candidate smart selection. Finally, our third unit spotter, labeled KB, is based on the assumption that concepts and other entries in a knowledge base are, by nature, things that can be of interest to people. For our knowledge base lookup, we use a proprietary graph consisting of knowledge from Wikipedia, Freebase, and paid feeds from various providers from domains such as entertainment, local, and finance. 4.1.3 Heuristics Our third family of ensemble members implements simple heuristics, which tend to be high precision e</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Mach. Learn., 34(1-3):151–175, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eldar Sadikov</author>
<author>Jayant Madhavan</author>
<author>Lu Wang</author>
<author>Alon Halevy</author>
</authors>
<title>Clustering query refinements by user intent.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>841--850</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8316" citStr="Sadikov et al., 2010" startWordPosition="1347" endWordPosition="1350"> it has not been addressed. 2.3 Intent detection There is a long line of research in the web literature on understanding user intent. The closest to smart selection is query recommendation (Baeza-Yates et al., 2005; Zhang and Nasraoui, 2006; Boldi et al., 2008), where the goal is to suggest queries that may be related to a user’s intent. Query recommendation techniques are based either on clustering queries by their co-clicked URL patterns (Baeza-Yates et al., 2005) or on leveraging co-occurrences of sequential queries in web 1525 search sessions (Zhang and Nasraoui, 2006; Boldi et al., 2008; Sadikov et al., 2010). The key difference from smart selection is that in our task the output is a selection that is relevant to the context of the document where the original selection appears (e.g., by adding terms neighboring the selection). In query recommendation, however, there is no notion of a document being read by the user and, instead, the recommendations are based exclusively on the aggregation of behavior of multiple users. 3 Problem Setting and Data 3.1 Smart Selection Definition Let D be the set of all documents. We define a selection to be a character (offset, length)-tuple in a document d E D. Let</context>
</contexts>
<marker>Sadikov, Madhavan, Wang, Halevy, 2010</marker>
<rawString>Eldar Sadikov, Jayant Madhavan, Lu Wang, and Alon Halevy. 2010. Clustering query refinements by user intent. In Proceedings of the 19th international conference on World wide web, pages 841–850. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Sakamoto</author>
<author>Takanori Komatsu</author>
<author>Takeo Igarashi</author>
</authors>
<title>Voice augmented manipulation: using paralinguistic information to manipulate mobile devices.</title>
<date>2013</date>
<booktitle>In Mobile HCI,</booktitle>
<pages>69--78</pages>
<contexts>
<context position="7452" citStr="Sakamoto et al. (2013)" startWordPosition="1200" endWordPosition="1203">hat Wikipedia anchor texts are similar in nature to what users would select in a researching task, it models the problem of recovering Wikipedia anchor texts from partial selections. 2.2 Human Computer Interaction There is a substantial amount of research in the HCI community on how to facilitate interaction of a user with touch and speech enabled devices. To give but a few examples of trends in this field, Gunawardana et al. (2010) address the fat finger problem in the use of soft keyboards on mobile devices, Kumar et al. (2012) explore a novel speech interaction paradigm for text entry, and Sakamoto et al. (2013) introduce a technique that combines touch and voice input on a mobile device for improved navigation of user interface elements such as commands and controls. To the best of our knowledge, however, the problem of smart selection as we defined it has not been addressed. 2.3 Intent detection There is a long line of research in the web literature on understanding user intent. The closest to smart selection is query recommendation (Baeza-Yates et al., 2005; Zhang and Nasraoui, 2006; Boldi et al., 2008), where the goal is to suggest queries that may be related to a user’s intent. Query recommendat</context>
</contexts>
<marker>Sakamoto, Komatsu, Igarashi, 2013</marker>
<rawString>Daisuke Sakamoto, Takanori Komatsu, and Takeo Igarashi. 2013. Voice augmented manipulation: using paralinguistic information to manipulate mobile devices. In Mobile HCI, pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyong Zhang</author>
<author>Olfa Nasraoui</author>
</authors>
<title>Mining search engine query logs for query recommendation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>1039--1040</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7935" citStr="Zhang and Nasraoui, 2006" startWordPosition="1283" endWordPosition="1286"> soft keyboards on mobile devices, Kumar et al. (2012) explore a novel speech interaction paradigm for text entry, and Sakamoto et al. (2013) introduce a technique that combines touch and voice input on a mobile device for improved navigation of user interface elements such as commands and controls. To the best of our knowledge, however, the problem of smart selection as we defined it has not been addressed. 2.3 Intent detection There is a long line of research in the web literature on understanding user intent. The closest to smart selection is query recommendation (Baeza-Yates et al., 2005; Zhang and Nasraoui, 2006; Boldi et al., 2008), where the goal is to suggest queries that may be related to a user’s intent. Query recommendation techniques are based either on clustering queries by their co-clicked URL patterns (Baeza-Yates et al., 2005) or on leveraging co-occurrences of sequential queries in web 1525 search sessions (Zhang and Nasraoui, 2006; Boldi et al., 2008; Sadikov et al., 2010). The key difference from smart selection is that in our task the output is a selection that is relevant to the context of the document where the original selection appears (e.g., by adding terms neighboring the selecti</context>
</contexts>
<marker>Zhang, Nasraoui, 2006</marker>
<rawString>Zhiyong Zhang and Olfa Nasraoui. 2006. Mining search engine query logs for query recommendation. In Proceedings of the 15th international conference on World Wide Web, pages 1039–1040. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>