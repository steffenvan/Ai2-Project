<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.9974785">
Combining Machine Learning and rule-based approaches in Spanish
and Japanese sentence realization
</title>
<author confidence="0.945351">
Maite Melero
</author>
<affiliation confidence="0.905373">
Microsoft Research
</affiliation>
<address confidence="0.961238">
One Microsoft Way
Redmond, WA 98008, USA
</address>
<email confidence="0.998976">
maitem@microsoft.com
</email>
<author confidence="0.852803">
Takako Aikawa
</author>
<affiliation confidence="0.850039">
Microsoft Research
</affiliation>
<address confidence="0.918528">
One Microsoft Way
Redmond, WA 98008, USA
</address>
<email confidence="0.998501">
takakoa@microsoft.com
</email>
<author confidence="0.974985">
Lee Schwartz
</author>
<affiliation confidence="0.951557">
Microsoft Research
</affiliation>
<address confidence="0.9380475">
One Microsoft Way
Redmond, WA 98008, USA
</address>
<email confidence="0.999122">
leesc@microsoft.com
</email>
<sectionHeader confidence="0.997403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999525384615385">
In this paper we describe two parallel ex-
periments on the integration of machine
learning (ML) methods into the Spanish
and Japanese rule-based sentence realiza-
tion modules developed at Microsoft Re-
search. The paper explores the use of
decision trees (DT) for the lexical selec-
tion of the copula in Spanish and the in-
sertion of a locative postposition in
Japanese. We show that it is possible to
machine-learn the contexts for these two
non-trivial linguistic phenomena with
high accuracy.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999781657894737">
The two experiments described in this paper were
carried out in the framework of the Spanish and
Japanese sentence generation modules that are part
of MSR-MT, the multilingual Machine Translation
system developed at Microsoft Research. MSR-
MT is a hybrid system that uses hand-written, rule-
based linguistic components for analysis and gen-
eration, and example-based, statistical components
for transfer (Richardson et al., 2001). The output of
the analysis, as well as the input to generation is an
annotated predicate-argument structure or logical
form (LF) (Heidorn, 2000). Transfer takes place
between source LF and target LF using an auto-
matically generated knowledge base known as
MindNet, built by aligning logical forms of bilin-
gual text. As described in (Aikawa et al., 2001),
the rule-based generation module generates the
surface string in the target language from the trans-
ferred LF.
Here we explore the integration of a machine
learning technique into two generation components
in order to deal with two different sentence realiza-
tion problems: the selection of the copula in Span-
ish and the insertion of a locative postposition in
Japanese. As shown by (Gamon et al. 2002) among
others, many linguistic operations can be viewed as
classification tasks, thus lending themselves to sta-
tistical methods such as decision tree classifiers.
Following the questions raised by (Bangalore et
al., 2001) on the impact that the type of corpus has
on the quality of the stochastic generation compo-
nents, we wanted to perform our experiments using
two very different types of texts. For this purpose
we built two different models for each experiment:
one using text coming from the Encarta encyclo-
pedia and another using text from technical and
computer manuals.
Our goals can be summarized as follows:
</bodyText>
<listItem confidence="0.993894857142857">
• To integrate a ML approach for a well-
defined linguistic operation into an otherwise to-
tally hand-coded rule-based generation module;
• To evaluate the usefulness of such an ap-
proach vs. hand-coded rules;
• To evaluate the impact of the type of the
training data on the accuracy of the model.
</listItem>
<bodyText confidence="0.996342666666667">
To build the statistical models, we used the
WinMine toolkit (Chickering et al., 1997) which
has been used to build a machine-learned genera-
tion module (Corston-Oliver et al., 2002). As train-
ing data, we used logical forms produced by
analyzing text in the languages of interest, Spanish
and Japanese, respectively. The data was automati-
cally split 70/30 for training and parameter tuning
by the WinMine toolkit, which then built different
decision trees with different degrees of granularity,
by manipulating the prior probability of tree struc-
tures to favor simpler ones. The best model was
chosen and then evaluated using a different blind
set of sentences. We also performed an evaluation
across text types.
</bodyText>
<subsectionHeader confidence="0.945627">
2 Selection of the Spanish copula
2.1 Description of the problem
</subsectionHeader>
<bodyText confidence="0.998608764705882">
Spanish has two different copulas, ser and estar,
which are both translated into English as ‘to be.’
Ser is used to express permanence, identity or in-
herent quality and estar is used for temporary con-
ditions and location. The correct generation of the
copula is a specific instance of the general problem
of lexical selection. In the context of our MT sys-
tem, this problem is generally solved by transfer,
which can make decisions that are context sensi-
tive. However, as pointed out in (Aikawa et al.,
2000), the generation component, being ultimately
responsible for the fluency and grammaticality of
the output, re-evaluates some of the decisions
made by transfer.
The main uses of ser and estar, following (Por-
roche, 1988), are summarized in Table 1, leaving
out the auxiliary uses.
</bodyText>
<table confidence="0.994355764705882">
Predicative function Attributive function Identity func- Stative
tion passive
Existential (case 1) With nouns (case 3) (case 6)
La reunión es a las 6 Juan es (un) médico Juan es el mé-
(The meeting is at 6) (Juan is a doctor) dico
La fiesta es en mi casa With adjectival phrases (Juan is the
SER (The party is at my place) (case 4) doctor)
Juan es guapo
(Juan is handsome)
El globo es de colores
(The balloon is multicolored)
Locative (case 2) With adjectival phrases (case 7)
Él está en casa. (case 5) La casa
(He is at home) María está muy guapa está cons-
ESTAR El libro está sobre la mesa. (Mary looks very pretty) truida
(The book is on the table) Mi jefe está de vacaciones (The house
(My boss is on vacation) is built)
</table>
<tableCaption confidence="0.999885">
Table 1: Uses of ser and estar
</tableCaption>
<bodyText confidence="0.999837176470588">
Each of these cases presents a different degree
of difficulty from a generation perspective. Thus,
cases 3, 6 and 7 can be easily addressed using ba-
sic morphosyntactic information: only ser can take
an NP argument, and only estar can appear as a
main verb with a past participle. The distinction
between cases 1 and 2 is more challenging and in-
volves properties of the subject as well as of the
predicate. Cases 4 and 5 are the hardest to predict
and entail aspectual interpretations sometimes dif-
ficult to deduce from context.
The use of ser in the &lt;copula+AJP&gt; construc-
tions implies that the attribute is an inherent quality
of the subject, while the use of estar implies that
the condition expressed by the attribute is acciden-
tal or circumstantial. Some attributes can be used
with both verbs, provided that the nature of these
attributes allows for the two aspectual interpreta-
tions. Thus, La nieve es fría and La nieve está fría
(both translate as ‘The snow is cold’) are both pos-
sible. Other attributes do not have this flexibility.
For example, disponible ‘available’ can only go
with estar and eterno ‘eternal’ can only go with
ser. Many of the attributes that have a strong pref-
erence for ser could also go with estar in very spe-
cific contexts, and then only if the subject is able to
experience change, as rojo ‘red’ in El semáforo
está rojo ‘The traffic light is red’.
The problem of selecting the right copula is
complex because it has to take many different
types of information into account. Nonetheless, it
can be easily mapped into a classification problem.
For these reasons, it is a good candidate for ma-
chine learning techniques.
</bodyText>
<subsectionHeader confidence="0.999793">
2.2 Experiment design and evaluation
</subsectionHeader>
<bodyText confidence="0.988150325000001">
2.2.1 Decision Tree model for selecting the
copula
We built two different DT models: Model A, using
131K sentences from the Encarta encyclopedia;
and Model B, using 55K sentences from technical
and computer manuals. All the sentences used for
the two models contained at least one instance of
ser or estar. The target feature, i.e. feature we
wanted to predict, was expressed in terms of the
copula being estar (the less frequent value) or not.
This translates into the Boolean values “no” for ser
and “yes” for estar1.
We parsed the sentences up to their logical
form and then automatically extracted 290 vari-
ables from each sentence (or clause containing ser
or estar). A variable is the combination of a posi-
tion or node in the LF structure we want the DT to
consider, and a linguistic attribute or feature that
may be present in this node. Thus, for instance:
Anim(Tsub) means “presence of the feature
Anim(ate) in the (logical) subject”; Time(Tobj)
means “presence of a Time attribute in the (logical)
object”. Most of these variables were binary, with
1 representing presence of the corresponding fea-
ture and 0 representing absence. In a few cases, we
used the actual value of the attribute: namely, syn-
tactic category of the logical object and lemma2 of
the preposition in the prepositional complement.
Although we manually selected the positions in
the LF to be considered by the DT, we did not per-
form any manual selection of features but rather let
WinMine choose the best predictors among them.
The number of predictors, or variables that are
predictive of the target feature, selected by the de-
cision tree algorithm was the same in both models:
55 out of the original 290. The set of predictive
variables in both models was very similar, and as
expected, the strongest predictors were the same in
both cases. They can be grouped in the following
way:
</bodyText>
<listItem confidence="0.995964117647059">
• semantic relation of the argument of the
copula (object, prepositional complement, locative)
1 There is some noise in the corpus, as sentences where ser or
estar are auxiliaries have not been excluded. However, the
frequency of this use is proportionally low.
2 A lemma of a word is its lexeme or citation form, e.g. the
infinitive form of a verb.
• morphological properties of the argument
(past participle)
• lexical semantic features of the subject
(animate, proper name, count/mass)
• lexical semantic features of the argument
(color, animate, count/mass, location)
• presence of a modifier (manner, time,
means)
• lemma of the preposition of the preposi-
tional complement
</listItem>
<bodyText confidence="0.711958333333333">
Most of the predictors have intuitive linguistic
relevance to the problem, but some of them were
not expected, as:
</bodyText>
<listItem confidence="0.99468325">
• presence of an intensifier, classifier or op-
erator on the argument (only in Model A)
• coordination in the main node or in an ar-
gument
</listItem>
<bodyText confidence="0.999462">
The overall accuracy of each model, as well as
the values for precision and recall, are measured on
the 30% part of the training data that is held out for
the purpose of selecting the best tree. Those values,
as well as the size of the two models, measured by
the number of their branching nodes, are summa-
rized in Tables 2 and 3.
</bodyText>
<table confidence="0.9605706">
Model A (Encarta) B (technical)
#Branching 109 117
nodes
Baseline3 82.85% 68.21%4
Accuracy 95.10% 90.36%
</table>
<tableCaption confidence="0.992847">
Table 2: Size, baseline and overall accuracy for the
two models
</tableCaption>
<footnote confidence="0.900640428571428">
3 The baseline represents the overall accuracy if the most fre-
quent value (i.e. ser) would have been selected in all cases.
4 There is a big difference in the value of the baseline (82.85%
for Encarta and 68.21% for technical manuals), indicating that
the preference of ser over estar is much more pronounced in
Encarta than in the manuals
5 F-measure is the harmonic mean of precision and recall.
</footnote>
<figure confidence="0.915209285714286">
Case Precision Recall F-measure5
(%) (%) (%)
Mo- A B A B A B
del
Ser 95.78 89.82 98.43 96.84 97.09 93.20
Es- 91.23 91.86 79.07 76.45 84.71 83.45
tar
</figure>
<tableCaption confidence="0.953872">
Table 3: Precision and recall for ser/estar
</tableCaption>
<bodyText confidence="0.99995575">
Once we built the decision trees, we wanted to
see whether their accuracy varied across different
domains. Finally, we were interested in evaluating
their performance against a hand-coded rule.
</bodyText>
<subsectionHeader confidence="0.614991">
2.2.2 Evaluation of the models
</subsectionHeader>
<bodyText confidence="0.998451603773585">
Even though our main interest is to use the result
of this experiment in an application environment
such as MT, we used Spanish texts for evaluation
purposes. It may seem that evaluating the results
using Spanish data constitutes an artificial envi-
ronment: after all, we are generating Spanish sen-
tences from structures resulting from the analysis
of the same Spanish sentences. Nonetheless, this
enables us to perform an automatic evaluation of
the results. The procedure is the following: we ana-
lyze and regenerate the Spanish sentences (with the
right copula in them) and we create a master file
with the results; we then run regression testing
against this file by removing the copula and recal-
culating it using the model. The number of changes
equals the number of regressions6 .
We used two blind testing sets of 10K sen-
tences each, one for each type of text (Encarta and
technical manuals). Since we were interested in
evaluating the usefulness of the ML approach with
respect to encoding the information in the form of
a rule, we also measured the accuracy of a not-too-
complex-but-not-too-dumb hand-coded rule that
uses some of the linguistic insights revealed by the
inspection of the models. Table 4, which gives the
number of errors in the generation of the copula
and the accuracy as a percentage, summarizes the
results of our evaluation on a blind corpus.
6 If there is more than one copula in a sentence and there is
more than one regression in this sentence, we will only be able
to count one regression. However, we consider that the evalua-
tion set is large enough to account for that noise.
From these results we observe that there is an
expected correlation between the type of text and
the type of model: Model A is the best model for
the Encarta text and Model B is the best model for
the technical text. Interestingly, the model trained
on technical data increases its accuracy when
tested on text from Encarta. This is consistent with
the fact that all three methods have better results
on text from Encarta. The reasons are not clear but
one possible explanation is that, as seen with the
values for the baseline above (82% vs. 68%), in
this type of text the copula insertion is “easier” to
predict. The hand-coded rule does a poorer job
overall. Error analysis shows that the rule is
slightly more biased towards estar than the model.
The formulation of the contextual constraints is
necessarily simpler in the rule than in the models
(which each have over a hundred branching condi-
tions). Both the models and the rule perform
poorly on &lt;copula+AJP&gt; constructions (cases 4
and 5 above) defaulting to ser most of the time.
</bodyText>
<subsectionHeader confidence="0.5932955">
2.2.3 Enriched models using the lemma of
the attribute
</subsectionHeader>
<bodyText confidence="0.99950825">
In order to provide a solution for the &lt;cop-
ula+AJP&gt; cases, we built a version of the models
that looks at the lemma of the argument. The DT is
able to cluster lemmas on a statistical basis, obviat-
ing the need to encode this sort of selectional in-
formation in the dictionary. The expected
improvement is hardly noticeable in Model A.
However, in the case of Model B, the enriched ver-
sion (Model B’) is much smaller (72 vs. 117
branching nodes) and its overall accuracy jumps to
97% (notable, especially if we consider that the
baseline for this type of text is 68%).
</bodyText>
<table confidence="0.996888625">
Model A A’ B B’
Text type En- En- techni- techni-
carta carta cal cal
Lemma no yes no yes
#Predictors 55 41 55 35
#Branching 109 101 117 72
Overall accu- 95.10 95.40 90.36 97.04
racy (%)
</table>
<tableCaption confidence="0.997746">
Table 5: Comparison of models according to text
type and use of lemma of the adjective
</tableCaption>
<table confidence="0.998303428571429">
Model A Model B Hand-
(Encarta) (Technical) coded
rule
Encarta 447/10k 753/10k 959/10k
text (95.53%) (92.47%) (90.41%)
Technical 1022/10k 966/10k 1383/10k
manuals (89.78%) (90.34%) (86.17%)
</table>
<tableCaption confidence="0.988765">
Table 4: Accuracy of the two models vs the hand-
coded rule
</tableCaption>
<bodyText confidence="0.9999255">
We wanted to evaluate how well Model B’
would do in the blind set used in our previous
evaluation. The result, shown in Table 6, with
blind data from the technical domain is predictably
good, but a more surprising result is the 95.10%
accuracy on the sentences from Encarta.
</bodyText>
<table confidence="0.988294428571429">
Model B’(Technical
trained model using
lemmas)
Encarta text (10K sen- 490 (95.10%)
tences)
Technical manuals 306 (96.94%)
(10K sentences)
</table>
<tableCaption confidence="0.999365">
Table 6: Evaluation of Model B’ on the blind set.
</tableCaption>
<sectionHeader confidence="0.4770225" genericHeader="method">
2.2.4 Integration of the DT Model in an
MT system
</sectionHeader>
<bodyText confidence="0.996586813953488">
The generation rule that predicts the lemma of the
copula calls the DT model by invoking a function
that returns a Boolean value. This function takes as
parameters the DT model, the target feature we are
trying to predict (estar in our case), and the LF
node we are considering (in our case the node of
the copula).
The Spanish generation grammar in the context
of which this experiment has been performed is
currently being used to generate the Spanish output
of an MT system that has English as input. In this
MT system, all lexical selections are, in principle,
performed by transfer. Transfer rules are automati-
cally extracted from parsed aligned corpora
(Menezes &amp; Richardson, 2001). Thus, the lemma
of the copula is also computed by transfer rules,
with a varying degree of accuracy. We wanted to
perform a second evaluation of our best DT model,
this time in an MT environment. We picked the
model that had been trained on technical text and
used information about the lemma of the adjective
(i.e. Model B’). We had two goals in mind:
- prove that a model trained on a monolin-
gual Spanish corpus could be used on
structures coming from transfer;
- compare the degree of accuracy of the
model vs the transfer component in the
task of copula selection.
We took about 9K English sentences from
computer manuals and processed them with our
English-Spanish MT system, keeping the copula
that transfer had found. We then kept these results
in a master file. We included a rule in the genera-
tion grammar that removed the lemma of the cop-
ula and recalculated it using the DT model, and
then ran regressions on the previous master file.
We obtained 154 differences. Those were the cases
for which transfer and DT predicted a different
copula. Since we were only looking at the differ-
ences we were in fact ignoring the cases where
transfer and DT were both right or both wrong. We
reviewed all the differences manually and obtained
the results shown in Table 7.
</bodyText>
<table confidence="0.976964">
#differences
DT was better 116/154 (73.00%)
Transfer was 22/154 (14.20%)
better
Neither7 16/154 (10.30%)
</table>
<tableCaption confidence="0.959622">
Table 7: Comparison of transfer vs. DT results on
the task of copula selection
</tableCaption>
<bodyText confidence="0.997749666666667">
The DT model beat the copula selection per-
formed by transfer in 116 cases, versus 22 where
transfer was right and the model was not.
</bodyText>
<sectionHeader confidence="0.9815" genericHeader="method">
3 Selection of a locative postposition in
Japanese
</sectionHeader>
<subsectionHeader confidence="0.999966">
3.1 Description of the problem
</subsectionHeader>
<bodyText confidence="0.99930025">
The Japanese experiment deals with the use of the
two postpositions for location nouns: (i) de (‘at’)
and (ii) ni (‘in’). The choice between the two de-
pends on the type of eventuality that a sentence
denotes: if the sentence denotes an event, de is
used for the location noun; if the sentence is stative,
ni is used. The following examples illustrate this
difference.
</bodyText>
<figure confidence="0.544223181818182">
(1) a.
John -wa koko-ni sunde-iru
John -Top this place-in live-ing
“John lives here.” (stative -&gt; “ni”)
7 Those were cases were the output was too ill-formed to con-
sider correctness of the copula.
(LF)
John -wa koko-de taberu.
John -Top this place-at eat
“John eats here.” (event -&gt; “de”)
(LF)
</figure>
<bodyText confidence="0.999833541666667">
The predicate in (1a), sunde-iru (‘to live’), is
stative and hence, the location noun koko (‘this
place’) is marked by ni. On the other hand, the
predicate in (1b), taberu (‘to eat’), denotes the
event of John’s eating and the location noun is
therefore marked by de. In the LFs above, the lo-
cation nouns are indicated as Locn and the postpo-
sitions are provided. Thus, in generating surface
strings using native Japanese LFs, the sentence
realization component has no problem; i.e., de/ni is
given and hence, no decision is necessary. How-
ever, when the Japanese generation module takes
as input a transferred LF (as in the MT scenario),
the correct postposition is not always provided.
For instance, the following is the transferred LF
from the English sentence, ‘John eats lunch in that
room’. Here, the transferred LF provides the
wrong postposition ni to indicate the place of
John’s action of eating lunch.
times is contingent upon other factors. For in-
stance, both (3a) and (3b) below have the same
predicate (i.e., aru ‘to exist’). However, the loca-
tion noun Tookyoo ‘Tokyo’ in (3a) is marked by de
whereas in (3b), it is marked by ni.
</bodyText>
<listItem confidence="0.733272">
(3) a.
</listItem>
<table confidence="0.433320142857143">
Tookyoo-de robotto-no tenjikai-ga aru
Tokyo-in robot-Gen exhibit-Nom exist
“There is a robot exhibit in Tokyo.”
b.
Tookyoo-ni furansu-ryooriten-ga takusan aru
Tokyo-in French restaurants many exist
“There are many French restaurants in Tokyo.”
</table>
<bodyText confidence="0.9999341875">
The contrast above can be reduced to the differ-
ence in types between the two subjects: the subject
in (3a) (i.e., robotto-no tenjikai ‘the robot exhibit’)
is an event nominal and hence, Tookyoo is marked
by de whereas the subject in (3b) (i.e., furansu
ryooriten ‘French restaurants’) is not an event
nominal and hence, Tookyoo is marked by ni.
Given the complication of the linguistic phenom-
ena involved in choosing between de and ni and
the limited amount of subcategorization informa-
tion or semantic information available for verbs
and nouns in the dictionary, it is almost impossible
for linguists to write rules to determine this choice.
We believe that this is exactly one of the situations
in which machine-learning approaches such as DT
can be utilized.
</bodyText>
<subsectionHeader confidence="0.999904">
3.2 Experiment Design and Evaluation
</subsectionHeader>
<bodyText confidence="0.992759333333333">
b.
Such mistakes are common in transferred LFs.
The Japanese generation component thus needs to
have an independent mechanism to handle this
phenomenon. However, predicting which postpo-
sition (de or ni) occurs in which contexts is a diffi-
cult task. As mentioned above, the choice between
de and ni depends on the type of eventuality that a
sentence denotes. Thus, predicting the correct
choice between these two postpositions requires
fine-grained lexical-semantic coding on all the
verbs in Japanese. Furthermore, the choice some-
</bodyText>
<subsectionHeader confidence="0.959963333333333">
3.2.1 Decision Tree model for predicting
the insertion of the locative postposi-
tion
</subsectionHeader>
<bodyText confidence="0.999917333333333">
Like the Spanish experiment discussed in Section 2,
two types of models were built for the Japanese
experiment; one model was trained on Encarta
(76.7K sentences) and the other on technical and
computer documents (27.4K sentences). The vari-
ables selected for the Japanese experiment involve:
(i) the lemma of the parent predicate of a location
noun and its linguistic features and (ii) linguistic
features associated with the location noun and the
subject of the predicate. Table 8 provides the
number of branching nodes, the overall accuracy
and the baseline for both these models.
</bodyText>
<table confidence="0.891942">
(2) (Transferred LF of ‘John eats lunch in the room.’)
Model A(Encarta) B (technical)
# of Branching nodes 878 102
Baseline 62.27% 79.40%
Accuracy 79.10% 90.44%
</table>
<tableCaption confidence="0.999332">
Table 8: Size, baseline and accuracy of the models
</tableCaption>
<bodyText confidence="0.812224">
In Model A (Encarta Model), 69 variables were
selected and 160 variables were rejected. Features
selected in Model A include the following:
</bodyText>
<listItem confidence="0.9933338">
• lemma of the parent predicate of a location
noun
• subcategorization features of the predicate
(e.g., intransitive; transitive; ditransitive;
unaccusative; etc.).
• voice information for the predicate (i.e.,
passive or not)
• presence of modifier(s) of the predicate
(e.g., time; prepositional modifier; coordi-
nation; etc.)
• presence of modifier(s) of the location
noun (e.g., possessor; prepositional modi-
fier; appositive; etc.)
• lexical semantic features of the subject of
the parent predicate.
</listItem>
<bodyText confidence="0.99831875">
In Model B (Technical Model), 30 variables
were selected and 138 variables were rejected. As
for Model A, the predictors selected in Model B
predominantly involve the lemmas of the parent
verbs of location nouns and their subcategorization
features. Precision and recall information for
Model A and Model B are provided in Table 9 be-
low.
</bodyText>
<table confidence="0.9976825">
Case Precision Recall F-measure
(%) (%) (%)
Mo- A B A B A B
del
de 75.08 77.93 66.78 74.79 70.69 76.33
ni 81.13 93.53 86.57 94.50 83.76 94.01
</table>
<tableCaption confidence="0.7670525">
Table 9: Precision and recall for de/ni in the two
models
</tableCaption>
<sectionHeader confidence="0.569921" genericHeader="evaluation">
3.2.2 Evaluation
</sectionHeader>
<bodyText confidence="0.999975">
Parallel to the Spanish experiment, we used two
types of blind test data; one from Encarta (1K sen-
tences) and the other from technical documents
(1K sentences). Using the DT model, we regener-
ated the test data and compared the regenerated
strings with the original sentences to find out how
many sentences were the same as the original sen-
tences with respect to the assignment of de/ni for
location nouns.
We did the same using the hand-coded rule.
Our hand-coded rule for the choice between de and
ni used the subcategorization features of the predi-
cates available in our Japanese dictionary. Basi-
cally, the hand-code rule assigned ni to a location
noun if the parent verb belongs to one of the fol-
lowing types of verbs: (i) directional motion verbs
(e.g., iku ‘to go’); (ii) verbs that require a locative
argument (e.g., oku ‘to put’); and (iii) existential
verbs (e.g., aru or iru ‘to be/to exist’). For other
types of verbs, the rule assigned de to a location
noun. Table 10 gives the number of errors in the
generation of the postposition de/ni in our test data
sets and the accuracy as a percentage.
</bodyText>
<table confidence="0.994944428571429">
Model A Model B Hand-
(Encarta) (Technical) coded
rule8
Encarta 291/1K 421/1 K 305/1 K
text (70.90 %) (57.90%) (69.50%)
Technical 440/1 K 215/1 K 240/1 K
manuals (56.00%) (78.50%) (76.00%)
</table>
<tableCaption confidence="0.983794">
Table 10: Comparison between the DT models and
</tableCaption>
<bodyText confidence="0.980032782608696">
the hand-coded rule on blind data
In the Japanese experiment, the two models per-
formed slightly better than the hand-coded rule
with respect to the same domain test set. However,
with respect to the different domain test set, they
performed worse than the hand-coded rule. This
means that, unlike the Spanish experiment de-
scribed in Section 2, both Japanese models are sen-
sitive to the domain of the test sets: Model A
(Encarta Model) achieves 70.90% accuracy for the
Encarta test data but its accuracy sharply drops for
the technical data (56.00%). Model B (Technical
Model) achieves 78.50% for the technical test data
but its accuracy drops sharply again for the Encarta
data (57.90%). That the two models are sensitive
to the domain of the test sets makes sense: the
types of predicates used in Encarta data are quite
different from those used in technical documents.
8 The hand-coded rule here examines the subcategorization
information of the predicate.
Thus, it is reasonable to assume that the set of dis-
tinguishing features selected by Model A may not
work for technical documents and vice versa.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999991032258065">
The results of the two experiments presented in
this paper show that it is possible to machine learn
the contexts for non-trivial linguistic phenomena
such as the selection of the copula in Spanish and
de/ni-assignment in Japanese.
Particularly, in the case of the selection of the
Spanish copula, the complexity of the task gives a
clear advantage to the statistical approach over the
hand-written rule, especially when the lemma of
the adjective is included in the model.
As for sensitivity of the models to type of text,
the results for the Spanish experiment show that
the model trained on data coming from technical
manuals performed better across different text
types than the model trained on Encarta, whereas
in the case of the Japanese experiment, the models
were highly sensitive to the type of data on which
they were trained.
Using the model for copula selection in Spanish,
we have shown how the models can be used in the
context of an application such as MT. With this
experiment, we have also demonstrated that a
model that has been trained on a monolingual
(Spanish) corpus can be used on logical form struc-
tures coming from transfer.
Whether DTs are used directly in the code, or
the information they provide is used to write a
more accurate rule or to encode information in the
dictionary, they seem to be a useful tool for ad-
dressing complex linguistic phenomena, such as
the two addressed in this paper.
</bodyText>
<sectionHeader confidence="0.997513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999984333333333">
Our special thanks go to Michael Gamon, Simon
Corston-Oliver, and Max Chickering for assistance
and advice while conducting these experiments.
We also are grateful to the members of the NLP
group at Microsoft Research for their useful com-
ments during the process of writing this paper.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898347826087">
Aikawa, T., Melero, M., Schwartz, L. and Wu, A.
(2001). Generation for Multilingual MT. In Proceed-
ings of the VIIIth MT-Summit, Santiago de Compos-
tela (Spain), September 2001.
Bangalore, S., Chen, J. and Rambow, O. (2001). Impact
of Quality and Quantity of Corpora on Stochastic
Generation . In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Process-
ing, Pittsburgh, Pennsylvania, 2001.
Chickering D. M., Heckerman D. and Meek C. (1997).
A Bayesian approach to learning Bayesian networks
with local structure. In Uncertainty in Artificial Intel-
ligence: Proceedings of the Thirteenth Conference,
D. Geiger and P. Punadlik Shenoy, ed., Morgan
Kaufman, San Francisco, California, pp. 80-89.
Chickering, D. Max. nd. WinMine Toolkit Home Page.
http://research.microsoft.com/~dmax/WinMine/Tool
doc.htm
Corston-Oliver S., Gamon M., Ringger E. and Moore B.
(2002). An overview of Amalgam: a machine-learned
generation module. To appear in Proceedings of the
Second International Natural Language Generation
Conference 2002, New York
Gamon, M., Ringger, E., Corston-Oliver, S. &amp; Moore,
R. (2002). Machine-learned contexts for linguistic
operations in German sentence realization. To appear
in Proceedings of the Association for Computational
Linguistics 2002, Pennsilvania.
Heidorn, G. E. (2000). Intelligence Writing Assistance.
In Dale R., Moisl H., and Somers H. (eds.), A Hand-
book of Natural Language Processing: Techniques
and Applications for the Processing of Language as
Text. Marcel Dekker, New York, 1998 (published in
August 2000), pages 181-207.
Menezes A. and Richardson S. (2001). A best-first
alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Pro-
ceedings of the Association for Computational Lin-
guistics 2001, Toulouse, France.
Porroche Ballesteros, M. (1988). Ser, estar y verbos de
cambio. Arco/Libros, Madrid.
Richardson, S., Dolan, W., Menezes, A. &amp; Pinkham J.
(2001). Achieving commercial-quality translation
with example-based methods. In Proceedings of the
VIIIth MT-Summit, Santiago de Compostela (Spain),
September 2001, pp. 293-298.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367374">
<title confidence="0.99071">Combining Machine Learning and rule-based approaches in and Japanese sentence realization</title>
<author confidence="0.975809">Maite Melero</author>
<affiliation confidence="0.999648">Microsoft Research</affiliation>
<address confidence="0.994342">One Microsoft Way Redmond, WA 98008, USA</address>
<email confidence="0.999737">maitem@microsoft.com</email>
<author confidence="0.752653">Takako</author>
<affiliation confidence="0.927256">Microsoft</affiliation>
<address confidence="0.994162">One Microsoft Way Redmond, WA 98008, USA</address>
<email confidence="0.999816">takakoa@microsoft.com</email>
<author confidence="0.858908">Lee</author>
<affiliation confidence="0.815482">Microsoft</affiliation>
<address confidence="0.877632">One Microsoft Redmond, WA 98008,</address>
<email confidence="0.99982">leesc@microsoft.com</email>
<abstract confidence="0.998957214285714">In this paper we describe two parallel experiments on the integration of machine learning (ML) methods into the Spanish and Japanese rule-based sentence realization modules developed at Microsoft Research. The paper explores the use of decision trees (DT) for the lexical selection of the copula in Spanish and the insertion of a locative postposition in Japanese. We show that it is possible to machine-learn the contexts for these two non-trivial linguistic phenomena with high accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Aikawa</author>
<author>M Melero</author>
<author>L Schwartz</author>
<author>A Wu</author>
</authors>
<title>Generation for Multilingual MT.</title>
<date>2001</date>
<booktitle>In Proceedings of the VIIIth MT-Summit, Santiago de Compostela</booktitle>
<contexts>
<context position="1676" citStr="Aikawa et al., 2001" startWordPosition="248" endWordPosition="251">e multilingual Machine Translation system developed at Microsoft Research. MSRMT is a hybrid system that uses hand-written, rulebased linguistic components for analysis and generation, and example-based, statistical components for transfer (Richardson et al., 2001). The output of the analysis, as well as the input to generation is an annotated predicate-argument structure or logical form (LF) (Heidorn, 2000). Transfer takes place between source LF and target LF using an automatically generated knowledge base known as MindNet, built by aligning logical forms of bilingual text. As described in (Aikawa et al., 2001), the rule-based generation module generates the surface string in the target language from the transferred LF. Here we explore the integration of a machine learning technique into two generation components in order to deal with two different sentence realization problems: the selection of the copula in Spanish and the insertion of a locative postposition in Japanese. As shown by (Gamon et al. 2002) among others, many linguistic operations can be viewed as classification tasks, thus lending themselves to statistical methods such as decision tree classifiers. Following the questions raised by (</context>
</contexts>
<marker>Aikawa, Melero, Schwartz, Wu, 2001</marker>
<rawString>Aikawa, T., Melero, M., Schwartz, L. and Wu, A. (2001). Generation for Multilingual MT. In Proceedings of the VIIIth MT-Summit, Santiago de Compostela (Spain), September 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>J Chen</author>
<author>O Rambow</author>
</authors>
<title>Impact of Quality and Quantity of Corpora on Stochastic Generation .</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Pittsburgh, Pennsylvania,</location>
<contexts>
<context position="2299" citStr="Bangalore et al., 2001" startWordPosition="346" endWordPosition="349">, the rule-based generation module generates the surface string in the target language from the transferred LF. Here we explore the integration of a machine learning technique into two generation components in order to deal with two different sentence realization problems: the selection of the copula in Spanish and the insertion of a locative postposition in Japanese. As shown by (Gamon et al. 2002) among others, many linguistic operations can be viewed as classification tasks, thus lending themselves to statistical methods such as decision tree classifiers. Following the questions raised by (Bangalore et al., 2001) on the impact that the type of corpus has on the quality of the stochastic generation components, we wanted to perform our experiments using two very different types of texts. For this purpose we built two different models for each experiment: one using text coming from the Encarta encyclopedia and another using text from technical and computer manuals. Our goals can be summarized as follows: • To integrate a ML approach for a welldefined linguistic operation into an otherwise totally hand-coded rule-based generation module; • To evaluate the usefulness of such an approach vs. hand-coded rule</context>
</contexts>
<marker>Bangalore, Chen, Rambow, 2001</marker>
<rawString>Bangalore, S., Chen, J. and Rambow, O. (2001). Impact of Quality and Quantity of Corpora on Stochastic Generation . In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, Pittsburgh, Pennsylvania, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
<author>D Heckerman</author>
<author>C Meek</author>
</authors>
<title>A Bayesian approach to learning Bayesian networks with local structure.</title>
<date>1997</date>
<booktitle>In Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference,</booktitle>
<pages>80--89</pages>
<editor>D. Geiger and P. Punadlik Shenoy, ed., Morgan Kaufman,</editor>
<location>San Francisco, California,</location>
<contexts>
<context position="3076" citStr="Chickering et al., 1997" startWordPosition="479" endWordPosition="482">fferent types of texts. For this purpose we built two different models for each experiment: one using text coming from the Encarta encyclopedia and another using text from technical and computer manuals. Our goals can be summarized as follows: • To integrate a ML approach for a welldefined linguistic operation into an otherwise totally hand-coded rule-based generation module; • To evaluate the usefulness of such an approach vs. hand-coded rules; • To evaluate the impact of the type of the training data on the accuracy of the model. To build the statistical models, we used the WinMine toolkit (Chickering et al., 1997) which has been used to build a machine-learned generation module (Corston-Oliver et al., 2002). As training data, we used logical forms produced by analyzing text in the languages of interest, Spanish and Japanese, respectively. The data was automatically split 70/30 for training and parameter tuning by the WinMine toolkit, which then built different decision trees with different degrees of granularity, by manipulating the prior probability of tree structures to favor simpler ones. The best model was chosen and then evaluated using a different blind set of sentences. We also performed an eval</context>
</contexts>
<marker>Chickering, Heckerman, Meek, 1997</marker>
<rawString>Chickering D. M., Heckerman D. and Meek C. (1997). A Bayesian approach to learning Bayesian networks with local structure. In Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference, D. Geiger and P. Punadlik Shenoy, ed., Morgan Kaufman, San Francisco, California, pp. 80-89.</rawString>
</citation>
<citation valid="false">
<authors>
<author>nd</author>
</authors>
<title>WinMine Toolkit Home Page.</title>
<note>http://research.microsoft.com/~dmax/WinMine/Tool doc.htm</note>
<marker>nd, </marker>
<rawString>Chickering, D. Max. nd. WinMine Toolkit Home Page. http://research.microsoft.com/~dmax/WinMine/Tool doc.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>M Gamon</author>
<author>E Ringger</author>
<author>B Moore</author>
</authors>
<title>An overview of Amalgam: a machine-learned generation module.</title>
<date>2002</date>
<booktitle>Proceedings of the Second International Natural Language Generation Conference</booktitle>
<location>New York</location>
<note>To appear in</note>
<contexts>
<context position="3171" citStr="Corston-Oliver et al., 2002" startWordPosition="494" endWordPosition="497"> one using text coming from the Encarta encyclopedia and another using text from technical and computer manuals. Our goals can be summarized as follows: • To integrate a ML approach for a welldefined linguistic operation into an otherwise totally hand-coded rule-based generation module; • To evaluate the usefulness of such an approach vs. hand-coded rules; • To evaluate the impact of the type of the training data on the accuracy of the model. To build the statistical models, we used the WinMine toolkit (Chickering et al., 1997) which has been used to build a machine-learned generation module (Corston-Oliver et al., 2002). As training data, we used logical forms produced by analyzing text in the languages of interest, Spanish and Japanese, respectively. The data was automatically split 70/30 for training and parameter tuning by the WinMine toolkit, which then built different decision trees with different degrees of granularity, by manipulating the prior probability of tree structures to favor simpler ones. The best model was chosen and then evaluated using a different blind set of sentences. We also performed an evaluation across text types. 2 Selection of the Spanish copula 2.1 Description of the problem Span</context>
</contexts>
<marker>Corston-Oliver, Gamon, Ringger, Moore, 2002</marker>
<rawString>Corston-Oliver S., Gamon M., Ringger E. and Moore B. (2002). An overview of Amalgam: a machine-learned generation module. To appear in Proceedings of the Second International Natural Language Generation Conference 2002, New York</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>E Ringger</author>
<author>S Corston-Oliver</author>
<author>R Moore</author>
</authors>
<title>Machine-learned contexts for linguistic operations in German sentence realization.</title>
<date>2002</date>
<booktitle>Proceedings of the Association for Computational Linguistics</booktitle>
<location>Pennsilvania.</location>
<note>To appear in</note>
<contexts>
<context position="2078" citStr="Gamon et al. 2002" startWordPosition="314" endWordPosition="317">, 2000). Transfer takes place between source LF and target LF using an automatically generated knowledge base known as MindNet, built by aligning logical forms of bilingual text. As described in (Aikawa et al., 2001), the rule-based generation module generates the surface string in the target language from the transferred LF. Here we explore the integration of a machine learning technique into two generation components in order to deal with two different sentence realization problems: the selection of the copula in Spanish and the insertion of a locative postposition in Japanese. As shown by (Gamon et al. 2002) among others, many linguistic operations can be viewed as classification tasks, thus lending themselves to statistical methods such as decision tree classifiers. Following the questions raised by (Bangalore et al., 2001) on the impact that the type of corpus has on the quality of the stochastic generation components, we wanted to perform our experiments using two very different types of texts. For this purpose we built two different models for each experiment: one using text coming from the Encarta encyclopedia and another using text from technical and computer manuals. Our goals can be summa</context>
</contexts>
<marker>Gamon, Ringger, Corston-Oliver, Moore, 2002</marker>
<rawString>Gamon, M., Ringger, E., Corston-Oliver, S. &amp; Moore, R. (2002). Machine-learned contexts for linguistic operations in German sentence realization. To appear in Proceedings of the Association for Computational Linguistics 2002, Pennsilvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Intelligence Writing Assistance.</title>
<date>2000</date>
<booktitle>A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text.</booktitle>
<pages>181--207</pages>
<editor>In Dale R., Moisl H., and Somers H. (eds.),</editor>
<publisher>Marcel Dekker,</publisher>
<location>New York,</location>
<note>published in</note>
<contexts>
<context position="1467" citStr="Heidorn, 2000" startWordPosition="215" endWordPosition="216">nomena with high accuracy. 1 Introduction The two experiments described in this paper were carried out in the framework of the Spanish and Japanese sentence generation modules that are part of MSR-MT, the multilingual Machine Translation system developed at Microsoft Research. MSRMT is a hybrid system that uses hand-written, rulebased linguistic components for analysis and generation, and example-based, statistical components for transfer (Richardson et al., 2001). The output of the analysis, as well as the input to generation is an annotated predicate-argument structure or logical form (LF) (Heidorn, 2000). Transfer takes place between source LF and target LF using an automatically generated knowledge base known as MindNet, built by aligning logical forms of bilingual text. As described in (Aikawa et al., 2001), the rule-based generation module generates the surface string in the target language from the transferred LF. Here we explore the integration of a machine learning technique into two generation components in order to deal with two different sentence realization problems: the selection of the copula in Spanish and the insertion of a locative postposition in Japanese. As shown by (Gamon e</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>Heidorn, G. E. (2000). Intelligence Writing Assistance. In Dale R., Moisl H., and Somers H. (eds.), A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text. Marcel Dekker, New York, 1998 (published in August 2000), pages 181-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Menezes</author>
<author>S Richardson</author>
</authors>
<title>A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the Association for Computational Linguistics</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="16090" citStr="Menezes &amp; Richardson, 2001" startWordPosition="2726" endWordPosition="2729">lls the DT model by invoking a function that returns a Boolean value. This function takes as parameters the DT model, the target feature we are trying to predict (estar in our case), and the LF node we are considering (in our case the node of the copula). The Spanish generation grammar in the context of which this experiment has been performed is currently being used to generate the Spanish output of an MT system that has English as input. In this MT system, all lexical selections are, in principle, performed by transfer. Transfer rules are automatically extracted from parsed aligned corpora (Menezes &amp; Richardson, 2001). Thus, the lemma of the copula is also computed by transfer rules, with a varying degree of accuracy. We wanted to perform a second evaluation of our best DT model, this time in an MT environment. We picked the model that had been trained on technical text and used information about the lemma of the adjective (i.e. Model B’). We had two goals in mind: - prove that a model trained on a monolingual Spanish corpus could be used on structures coming from transfer; - compare the degree of accuracy of the model vs the transfer component in the task of copula selection. We took about 9K English sent</context>
</contexts>
<marker>Menezes, Richardson, 2001</marker>
<rawString>Menezes A. and Richardson S. (2001). A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora. In Proceedings of the Association for Computational Linguistics 2001, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Porroche Ballesteros</author>
<author>M</author>
</authors>
<title>Ser, estar y verbos de cambio. Arco/Libros,</title>
<date>1988</date>
<location>Madrid.</location>
<marker>Ballesteros, M, 1988</marker>
<rawString>Porroche Ballesteros, M. (1988). Ser, estar y verbos de cambio. Arco/Libros, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Richardson</author>
<author>W Dolan</author>
<author>A Menezes</author>
<author>J Pinkham</author>
</authors>
<title>Achieving commercial-quality translation with example-based methods.</title>
<date>2001</date>
<booktitle>In Proceedings of the VIIIth MT-Summit, Santiago de Compostela</booktitle>
<pages>293--298</pages>
<contexts>
<context position="1321" citStr="Richardson et al., 2001" startWordPosition="190" endWordPosition="193">and the insertion of a locative postposition in Japanese. We show that it is possible to machine-learn the contexts for these two non-trivial linguistic phenomena with high accuracy. 1 Introduction The two experiments described in this paper were carried out in the framework of the Spanish and Japanese sentence generation modules that are part of MSR-MT, the multilingual Machine Translation system developed at Microsoft Research. MSRMT is a hybrid system that uses hand-written, rulebased linguistic components for analysis and generation, and example-based, statistical components for transfer (Richardson et al., 2001). The output of the analysis, as well as the input to generation is an annotated predicate-argument structure or logical form (LF) (Heidorn, 2000). Transfer takes place between source LF and target LF using an automatically generated knowledge base known as MindNet, built by aligning logical forms of bilingual text. As described in (Aikawa et al., 2001), the rule-based generation module generates the surface string in the target language from the transferred LF. Here we explore the integration of a machine learning technique into two generation components in order to deal with two different se</context>
</contexts>
<marker>Richardson, Dolan, Menezes, Pinkham, 2001</marker>
<rawString>Richardson, S., Dolan, W., Menezes, A. &amp; Pinkham J. (2001). Achieving commercial-quality translation with example-based methods. In Proceedings of the VIIIth MT-Summit, Santiago de Compostela (Spain), September 2001, pp. 293-298.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>