<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.110103">
<title confidence="0.9582725">
Improving generative statistical parsing with semi-supervised word
clustering
</title>
<author confidence="0.616945">
Marie Candito and Benoît Crabbé
</author>
<affiliation confidence="0.405342">
Université Paris 7/INRIA (Alpage), 30 rue du Château des Rentiers, 75013 Paris
</affiliation>
<sectionHeader confidence="0.96907" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983217391304">
We present a semi-supervised method to
improve statistical parsing performance.
We focus on the well-known problem of
lexical data sparseness and present exper-
iments of word clustering prior to pars-
ing. We use a combination of lexicon-
aided morphological clustering that pre-
serves tagging ambiguity, and unsuper-
vised word clustering, trained on a large
unannotated corpus. We apply these clus-
terings to the French Treebank, and we
train a parser with the PCFG-LA unlex-
icalized algorithm of (Petrov et al., 2006).
We find a gain in French parsing perfor-
mance: from a baseline of F1=86.76% to
F1=87.37% using morphological cluster-
ing, and up to F1=88.29% using further
unsupervised clustering. This is the best
known score for French probabilistic pars-
ing. These preliminary results are encour-
aging for statistically parsing morpholog-
ically rich languages, and languages with
small amount of annotated data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999762838709678">
Lexical information is known crucial in natural
language parsing. For probabilistic parsing, one
main drawback of the plain PCFG approach is to
lack sensitivity to the lexicon. The symbols acces-
sible to context-free rules are part-of-speech tags,
which encode generalizations that are too coarse
for many parsing decisions (for instance subcat-
egorization information is generally absent from
tagsets). The lexicalized models first proposed
by Collins reintroduced words at every depth of a
parse tree, insuring that attachments receive prob-
abilities that take lexical information into account.
On the other hand, (Matsuzaki et al., 2005) have
proposed probabilistic CFG learning with latent
annotation (hereafter PCFG-LA), as a way to au-
tomate symbol splitting in unlexicalized proba-
bilistic parsing (cf. adding latent annotations to
a symbol is comparable to splitting this symbol).
(Petrov et al., 2006) rendered the method usable in
practice, with a tractable technique to retain only
the beneficial splits.
We know that both lexicalized parsing algo-
rithm and PCFG-LA algorithm suffer from lex-
ical data sparseness. For lexicalized parsers,
(Gildea, 2001) shows that bilexical dependencies
parameters are almost useless in the probabilistic
scoring of parser because they are too scarce.
For PCFG-LA, we have previously studied the
lexicon impact on this so-called “unlexicalized”
algorithm, for French parsing (Crabbé and Can-
dito, 2008), (Candito et al., 2009). We have tested
a totally unlexicalized parser, trained on a treebank
where words are replaced by their POS tags. It ob-
tains a parseval F1=86.28 (note that it induces per-
fect tagging). We compared it to a parser trained
with word+tag as terminal symbols (to simulate a
perfect tagging), achieving F1=87.79. This proves
that lexical information is indeed used by the “un-
lexicalized” PCFG-LA algorithm: some lexical
information percolates through parse trees via the
latent annotations.
We have also reported a slight improvement
(F1=88.18) when word forms are clustered on a
morphological basis, into lemma+tag clusters. So
PCFG-LA uses lexical information, but it is too
sparse, hence it benefits from word clustering. Yet
the use of lemma+tag terminals supposes tagging
prior to parsing. We propose here to apply rather
a deterministic supervised morphological cluster-
ing that preserves tagging ambiguities, leaving it
to the parser to disambiguate POS tags.
We also investigate the use of unsupervised
word clustering, obtained from unannotated text.
It has been proved useful for parsing by (Koo et
al., 2008) and their work directly inspired ours.
They have shown that parsing improves when
cluster information is used as features in a discrim-
inative training method that learns dependency
parsers. We investigate in this paper the use of
such clusters in a generative approach to proba-
bilistic phrase-structure parsing, simply by replac-
ing each token by its cluster.
</bodyText>
<page confidence="0.975754">
138
</page>
<bodyText confidence="0.948225666666667">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 138–141,
Paris, October 2009. c�2009 Association for Computational Linguistics
We present in section 2 the treebank instanti-
ation we use for our experiments, the morpho-
logical clustering in section 3, and the Brown al-
gorithm for unsupervised clustering in section 4.
Section 5 presents our experiments, results and
discussion. Section 6 discusses related work. Sec-
tion 7 concludes with some ideas for future work.
</bodyText>
<sectionHeader confidence="0.927608" genericHeader="method">
2 French Treebank
</sectionHeader>
<bodyText confidence="0.999607">
For our experiments, we use the French Treebank
(hereafter FTB) (Abeillé et al., 2003), containing
12531 sentences of the newspaper Le Monde. We
started with the treebank instantiation defined in
(Crabbé and Candito, 2008), where the rich origi-
nal annotation containing morphological and func-
tional information is mapped to a plain phrase-
structure treebank with a tagset of 28 POS tags.
In the original treebank, 17% of the tokens be-
long to a compound, and compounds range from
very frozen multi word expressions like y com-
pris (literally there included, meaning including)
to syntactically regular entities like loi agraire
(land law). In most of the experiments with the
FTB, each compound is merged into a single to-
ken: (P (CL y) (A compris)) is merged as (P
y_compris). But because our experiments aim at
reducing lexical sparseness but also at augmenting
lexical coverage using an unannotated corpus, we
found it necessary to make the unannotated cor-
pus tokenisation and the FTB tokenisation consis-
tent. To set up a robust parser, we chose to avoid
recognizing compounds that exhibit syntactically
regular patterns. We create a new instance of the
treebank (hereafter FTB-UC), where syntactically
regular patterns are “undone” (Figure 1). This re-
duces the number of distinct compounds in the
whole treebank from 6125 to 3053.
</bodyText>
<figure confidence="0.739149">
NP NP
</figure>
<figureCaption confidence="0.955326">
Figure 1: A NP with a compound (left) changed
into a regular structure with simple words (right)
</figureCaption>
<sectionHeader confidence="0.963122" genericHeader="method">
3 Morphological clustering
</sectionHeader>
<bodyText confidence="0.99933831372549">
The aim of this step is to reduce lexical sparseness
caused by inflection, without hurting parsability,
and without committing ourselves as far as ambi-
guity is concerned. Hence, a morphological clus-
tering using lemmas is not possible, since lemma
assignment supposes POS disambiguation. Fur-
ther, information such as mood on verbs is nec-
essary to capture for instance that infinitive verbs
have no overt subject, that participial clauses are
sentence modifiers, etc... This is encoded in the
FTB with different projections for finite verbs
(projecting sentences) versus non finite verbs (pro-
jecting VPpart or VPinf).
We had the intuition that the other inflection
marks in French (gender and number for determin-
ers, adjectives, pronouns and nouns, tense and per-
son for verbs) are not crucial to infer the correct
phrase-structure projected by a given word1.
So to achieve morphological clustering, we de-
signed a process of desinflection, namely of re-
moving some inflection marks. It makes use of
the Lefff, a freely available rich morphological and
syntactic French lexicon (Sagot et al., 2006), con-
taining around 116000 lemmas (simple and com-
pounds) and 535000 inflected forms. The desin-
flection is as follows: for a token t to desin-
flect, if it is known in the lexicon, for all the in-
flected lexical entries le of t, try to get corre-
sponding singular entries. If for all the le, cor-
responding singular entries exist and all have the
same form, then replace t by the corresponding
singular. For instance for wt=entrées (ambigu-
ous between entrances and entered, fem, plural),
the two lexical entries are [entrées/N/fem/pluJ and
[entrées/V/fem/plu/part/pastJ2, each have a corre-
sponding singular lexical entry, with form entrée.
Then the same process applies to map feminine
forms to corresponding masculine forms. This
allows to change mangée (eaten, fem, sing) into
mangé (eaten, masc, sing). But for the form en-
trée, ambiguous between N and Vpastpart entries,
only the participle has a corresponding masculine
entry (with form entré). In that case, in order
to preserve the original ambiguity, entrée is not
replaced by entré. Finite verb forms, when un-
ambiguous with other POS, are mapped to sec-
ond person plural present indicative corresponding
forms. This choice was made in order to avoid cre-
ating ambiguity: the second person plural forms
end with a very typical -ez suffix, and the result-
ing form is very unlikely ambiguous. For the first
</bodyText>
<footnote confidence="0.9959816">
1For instance, French oral comprehension does not seem
to need plural marks very much, since a majority of French
singular forms have their corresponding plural form pro-
nounced in the same way.
2This is just an example and not the real Lefff format.
</footnote>
<figure confidence="0.992714625">
N
Union
N
A
économique
A
monétaire
N
Union
A
économique
COORD
AP
A
monétaire
D
l’
C
et
D
l’
AP
C
et
</figure>
<page confidence="0.994808">
139
</page>
<bodyText confidence="0.9996698">
token of a sentence, if unknown in the lexicon,
the algorithm tries to desinflect the low case cor-
responding form.
This desinflection reduces the number of dis-
tinct tokens in the FTB-UC from 27143 to 20268.
</bodyText>
<sectionHeader confidence="0.866887" genericHeader="method">
4 Unsupervised word clustering
</sectionHeader>
<bodyText confidence="0.999966863636363">
We chose to use the (Brown et al., 1992) hard clus-
tering algorithm, which has proven useful for var-
ious NLP tasks, such as dependency parsing (Koo
et al., 2008) or named entity recognition (Liang,
2005). The algorithm to obtain C clusters is as
follows: each of the C most frequent tokens of
the corpus is assigned its own distinct cluster. For
the (C+1)th most frequent token, create a (C+1)th
cluster. Then for each pair among the C+1 result-
ing clusters, merge the pair that minimizes the loss
in the likelihood of the corpus, according to a bi-
gram language model defined on the clusters. Re-
peat this operation for the (C+2)th most frequent
token, etc... This results in a hard clustering into
C clusters. The process can be continued to fur-
ther merge pairs of clusters among the C clusters,
ending with a unique cluster for the whole vocab-
ulary. This can be traced to obtain a binary tree
representing the merges of the C clusters. A clus-
ter can be identified by its path within this binary
tree. Hence, clusters can be used at various levels
of granularity.
</bodyText>
<sectionHeader confidence="0.992744" genericHeader="method">
5 Experiments and discussion
</sectionHeader>
<bodyText confidence="0.999901375">
For the Brown clustering algorithm, we used Percy
Liang’s code3, run on the L’Est Républicain cor-
pus, a 125 million word journalistic corpus, freely
available at CNRTL4. The corpus was tokenised5,
segmented into sentences and desinflected using
the process described in section 3. We ran the clus-
tering into 1000 clusters for the desinflected forms
appearing at least 20 times.
We tested the use of word clusters for parsing
with the Berkeley algorithm (Petrov et al., 2006).
Clustering words in this case has a double advan-
tage. First, it augments the known vocabulary,
which is made of all the forms of all the clus-
ters appearing in the treebank. Second, it reduces
sparseness for the latent annotations learning on
the lexical rules of the PCFG-LA grammar.
</bodyText>
<footnote confidence="0.99790675">
3http://www.eecs.berkeley.edu/pliang/software
4http://www.cnrtl.fr/corpus/estrepublicain
5The 200 most frequent compounds of the FTB-UC were
systematically recognized as one token.
</footnote>
<bodyText confidence="0.9998567">
We used Petrov’s code, adapted to French by
(Crabbé and Candito, 2008), for the suffixes used
to classify unknown words, and we used the same
training(80%)/dev(10%)/test(10%) partition. We
used the FTB-UC treebank to train a baseline
parser, and three other parsers by changing the ter-
minal symbols used in training data:
desinflected forms: as described in section 3
clusters + cap: each desinflected form is re-
placed by its cluster bit string. If the desinflected
form has no corresponding cluster (it did not ap-
pear 20 times in the unannotated corpus), a spe-
cial cluster UNKC is used. Further, a _C suffix is
added if the form starts with a capital.
clusters + cap + suffixes: same as before, ex-
cept that 9 additional features are used as suffixes
to the cluster: if form is all digits, ends with ant,
or r, or ez (cf. this is how end desinflected forms
of unambiguous finite verbs), ...
We give in table 1 parsing performance in terms
of labeled precision/recall/Fscore, and also the
more neutral unlabeled attachment score (UAS)6.
The desinflection process does help: benefits
from reducing data sparseness exceed the loss
of agreement markers. Yet tagging decreases a
little, and this directly impacts the dependency
score, because the dependency extraction uses
head propagation rules that are sensitive to tag-
ging. In the same way, the use of bare clusters
increases labeled recall/precision, but the tagging
accuracy decreases, and thus the UAS. This can
be due to the coarseness of the clustering method,
which sometimes groups words that have differ-
ent POS (for instance among a cluster of infinite
verbs, one may find a present participle). The
quality of the clusters is more crucial in our case
than when clusters are features, whose informativ-
ity is discriminatively learnt. This observation led
us to append a restricted set of suffixes to the clus-
ters, which gives us the best results for now.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.9999458">
We already mentioned that we were inspired by
the success of (Koo et al., 2008) in using word
clusters as features for the discriminative learning
of dependency parsers. Another approach to aug-
ment the known vocabulary for a generative prob-
</bodyText>
<footnote confidence="0.9973542">
6In all metrics punctuation tokens are ignored and all re-
sults are for sentences of less than 40 words. Note that we
used the FTB-UC treebank. There are mors tokens in sen-
tences than in the FTB with all compounds merged, and base-
line Fl scores are a little higher (86.79 versus 86.41).
</footnote>
<page confidence="0.97435">
140
</page>
<table confidence="0.9903224">
terminal symbols LP LR Fl UAS Vocab. size Tagging Acc.
inflected forms (baseline) 86.94 86.65 86.79 91.00 27143 96.90
desinflected forms 87.42 87.32 87.37 91.14 20268 96.81
clusters + cap 88.08 87.50 87.79 91.12 1201 96.37
clusters + cap + suffixes 88.43 88.14 88.29 91.68 1987 97.04
</table>
<tableCaption confidence="0.999973">
Table 1: Parsing performance when training and parsing use clustered terminal symbols
</tableCaption>
<bodyText confidence="0.999460857142857">
abilistic parser is the one pursued in (Goldberg et
al., 2009). Within a plain PCFG, the lexical proba-
bilities for words that are rare or absent in the tree-
bank are taken from an external lexical probabil-
ity distribution, estimated using a lexicon and the
Baulm-Welch training of an HMM tagger. This is
proved useful to better parse Hebrew.
</bodyText>
<sectionHeader confidence="0.969057" genericHeader="conclusions">
7 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999996541666667">
We have tested the very simple method of replac-
ing inflected forms by clusters of forms in a gener-
ative probabilistic parser. This crude technique has
surprisingly good results and offers a very cheap
and simple way to augment the vocabulary seen at
training time. It seems interesting to try the tech-
nique on other generative approaches such as lex-
icalized probabilistic parsing.
We plan to optimize the exact shape of termi-
nal symbols to use. Bare unsupervised clusters are
unsatisfactory, and we have seen that adding sim-
ple suffixes to the clusters improved performance.
Learning such suffixes is a path to explore. Also,
the hierarchical organization of the clusters could
be used, in the generative approach adopted here,
by modulating the granularity of the clusters de-
pending on their frequency in the treebank.
We also need to check to what extent the desin-
flection step helps for taking advantage of the very
local information captured by the Brown cluster-
ing.Finally, we could try using other kinds of clus-
tering, such as the approach of (Lin, 1998), which
captures similarity between syntactic dependen-
cies beared by nouns and verbs.
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996045">
The authors truly thank Percy Liang and Slav
Petrov for providing their code for respec-
tively Brown clustering and PCFG-LA. This
work was supported by the French National
Research Agency (SEQUOIA project ANR-08-
EMER-013).
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910586956521">
Anne Abeillé, Lionel Clément, and François Toussenel,
2003. Building a Treebank for French. Kluwer,
Dordrecht.
Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Compu-
tationallinguistics, 18(4):467–479.
Marie Candito, Benoit Crabbé, and Djamé Seddah.
2009. On statistical parsing of french with super-
vised and semi-supervised strategies. In EACL 2009
Workshop Grammatical inference for Computa-
tional Linguistics, Athens, Greece.
Benoit Crabbé and Marie Candito. 2008. Expériences
d’analyse syntaxique statistique du français. In
Actes de la 15ème Conférence sur le Traitement Au-
tomatique des Langues Naturelles (TALN’08), pages
45–54, Avignon, France.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc. of EMNLP’01, pages 167–202,
Pittsburgh, USA.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and
Michael Elhadad. 2009. Enhancing unlexicalized
parsing performance using a wide coverage lexicon,
fuzzy tag-set mapping, and EM-HMM-based lexical
probabilities. In Proc. of EACL-09, pages 327–335,
Athens, Greece.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proc. ofACL-08, Columbus, USA.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. In MIT Master’s thesis, Cambridge,
USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL-98, pages 768–
774, Montreal, Canada.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proc. ofACL-05, pages 75–82, Ann Arbor, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL-06, Syd-
ney, Australia.
Benoît Sagot, Lionel Clément, Éric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for french: architecture, acquisi-
tion, use. In Proc. ofLREC’06, Genova, Italy.
</reference>
<page confidence="0.998263">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.478878">
<title confidence="0.8902785">Improving generative statistical parsing with semi-supervised word clustering</title>
<author confidence="0.802088">Marie Candito</author>
<author confidence="0.802088">Benoît Crabbé</author>
<note confidence="0.655611">Université Paris 7/INRIA (Alpage), 30 rue du Château des Rentiers, 75013 Paris</note>
<abstract confidence="0.999713541666667">We present a semi-supervised method to improve statistical parsing performance. We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing. We use a combination of lexiconaided morphological clustering that preserves tagging ambiguity, and unsupervised word clustering, trained on a large unannotated corpus. We apply these clusterings to the French Treebank, and we train a parser with the PCFG-LA unlexicalized algorithm of (Petrov et al., 2006). We find a gain in French parsing perforfrom a baseline of to using morphological clusterand up to using further unsupervised clustering. This is the best known score for French probabilistic parsing. These preliminary results are encouraging for statistically parsing morphologically rich languages, and languages with small amount of annotated data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeillé</author>
<author>Lionel Clément</author>
<author>François Toussenel</author>
</authors>
<title>Building a Treebank for French.</title>
<date>2003</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="4663" citStr="Abeillé et al., 2003" startWordPosition="705" endWordPosition="708">ts cluster. 138 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 138–141, Paris, October 2009. c�2009 Association for Computational Linguistics We present in section 2 the treebank instantiation we use for our experiments, the morphological clustering in section 3, and the Brown algorithm for unsupervised clustering in section 4. Section 5 presents our experiments, results and discussion. Section 6 discusses related work. Section 7 concludes with some ideas for future work. 2 French Treebank For our experiments, we use the French Treebank (hereafter FTB) (Abeillé et al., 2003), containing 12531 sentences of the newspaper Le Monde. We started with the treebank instantiation defined in (Crabbé and Candito, 2008), where the rich original annotation containing morphological and functional information is mapped to a plain phrasestructure treebank with a tagset of 28 POS tags. In the original treebank, 17% of the tokens belong to a compound, and compounds range from very frozen multi word expressions like y compris (literally there included, meaning including) to syntactically regular entities like loi agraire (land law). In most of the experiments with the FTB, each com</context>
</contexts>
<marker>Abeillé, Clément, Toussenel, 2003</marker>
<rawString>Anne Abeillé, Lionel Clément, and François Toussenel, 2003. Building a Treebank for French. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della</author>
<author>Peter V Desouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computationallinguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="9116" citStr="Brown et al., 1992" startWordPosition="1442" endWordPosition="1445">, French oral comprehension does not seem to need plural marks very much, since a majority of French singular forms have their corresponding plural form pronounced in the same way. 2This is just an example and not the real Lefff format. N Union N A économique A monétaire N Union A économique COORD AP A monétaire D l’ C et D l’ AP C et 139 token of a sentence, if unknown in the lexicon, the algorithm tries to desinflect the low case corresponding form. This desinflection reduces the number of distinct tokens in the FTB-UC from 27143 to 20268. 4 Unsupervised word clustering We chose to use the (Brown et al., 1992) hard clustering algorithm, which has proven useful for various NLP tasks, such as dependency parsing (Koo et al., 2008) or named entity recognition (Liang, 2005). The algorithm to obtain C clusters is as follows: each of the C most frequent tokens of the corpus is assigned its own distinct cluster. For the (C+1)th most frequent token, create a (C+1)th cluster. Then for each pair among the C+1 resulting clusters, merge the pair that minimizes the loss in the likelihood of the corpus, according to a bigram language model defined on the clusters. Repeat this operation for the (C+2)th most freque</context>
</contexts>
<marker>Brown, Della, Desouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computationallinguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabbé</author>
<author>Djamé Seddah</author>
</authors>
<title>On statistical parsing of french with supervised and semi-supervised strategies.</title>
<date>2009</date>
<booktitle>In EACL 2009 Workshop Grammatical inference for Computational Linguistics,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2581" citStr="Candito et al., 2009" startWordPosition="382" endWordPosition="385"> is comparable to splitting this symbol). (Petrov et al., 2006) rendered the method usable in practice, with a tractable technique to retain only the beneficial splits. We know that both lexicalized parsing algorithm and PCFG-LA algorithm suffer from lexical data sparseness. For lexicalized parsers, (Gildea, 2001) shows that bilexical dependencies parameters are almost useless in the probabilistic scoring of parser because they are too scarce. For PCFG-LA, we have previously studied the lexicon impact on this so-called “unlexicalized” algorithm, for French parsing (Crabbé and Candito, 2008), (Candito et al., 2009). We have tested a totally unlexicalized parser, trained on a treebank where words are replaced by their POS tags. It obtains a parseval F1=86.28 (note that it induces perfect tagging). We compared it to a parser trained with word+tag as terminal symbols (to simulate a perfect tagging), achieving F1=87.79. This proves that lexical information is indeed used by the “unlexicalized” PCFG-LA algorithm: some lexical information percolates through parse trees via the latent annotations. We have also reported a slight improvement (F1=88.18) when word forms are clustered on a morphological basis, into</context>
</contexts>
<marker>Candito, Crabbé, Seddah, 2009</marker>
<rawString>Marie Candito, Benoit Crabbé, and Djamé Seddah. 2009. On statistical parsing of french with supervised and semi-supervised strategies. In EACL 2009 Workshop Grammatical inference for Computational Linguistics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Crabbé</author>
<author>Marie Candito</author>
</authors>
<title>Expériences d’analyse syntaxique statistique du français.</title>
<date>2008</date>
<booktitle>In Actes de la 15ème Conférence sur le Traitement Automatique des Langues Naturelles (TALN’08),</booktitle>
<pages>45--54</pages>
<location>Avignon, France.</location>
<contexts>
<context position="2557" citStr="Crabbé and Candito, 2008" startWordPosition="377" endWordPosition="381">tent annotations to a symbol is comparable to splitting this symbol). (Petrov et al., 2006) rendered the method usable in practice, with a tractable technique to retain only the beneficial splits. We know that both lexicalized parsing algorithm and PCFG-LA algorithm suffer from lexical data sparseness. For lexicalized parsers, (Gildea, 2001) shows that bilexical dependencies parameters are almost useless in the probabilistic scoring of parser because they are too scarce. For PCFG-LA, we have previously studied the lexicon impact on this so-called “unlexicalized” algorithm, for French parsing (Crabbé and Candito, 2008), (Candito et al., 2009). We have tested a totally unlexicalized parser, trained on a treebank where words are replaced by their POS tags. It obtains a parseval F1=86.28 (note that it induces perfect tagging). We compared it to a parser trained with word+tag as terminal symbols (to simulate a perfect tagging), achieving F1=87.79. This proves that lexical information is indeed used by the “unlexicalized” PCFG-LA algorithm: some lexical information percolates through parse trees via the latent annotations. We have also reported a slight improvement (F1=88.18) when word forms are clustered on a m</context>
<context position="4799" citStr="Crabbé and Candito, 2008" startWordPosition="725" endWordPosition="728"> c�2009 Association for Computational Linguistics We present in section 2 the treebank instantiation we use for our experiments, the morphological clustering in section 3, and the Brown algorithm for unsupervised clustering in section 4. Section 5 presents our experiments, results and discussion. Section 6 discusses related work. Section 7 concludes with some ideas for future work. 2 French Treebank For our experiments, we use the French Treebank (hereafter FTB) (Abeillé et al., 2003), containing 12531 sentences of the newspaper Le Monde. We started with the treebank instantiation defined in (Crabbé and Candito, 2008), where the rich original annotation containing morphological and functional information is mapped to a plain phrasestructure treebank with a tagset of 28 POS tags. In the original treebank, 17% of the tokens belong to a compound, and compounds range from very frozen multi word expressions like y compris (literally there included, meaning including) to syntactically regular entities like loi agraire (land law). In most of the experiments with the FTB, each compound is merged into a single token: (P (CL y) (A compris)) is merged as (P y_compris). But because our experiments aim at reducing lexi</context>
<context position="11178" citStr="Crabbé and Candito, 2008" startWordPosition="1781" endWordPosition="1784">d the use of word clusters for parsing with the Berkeley algorithm (Petrov et al., 2006). Clustering words in this case has a double advantage. First, it augments the known vocabulary, which is made of all the forms of all the clusters appearing in the treebank. Second, it reduces sparseness for the latent annotations learning on the lexical rules of the PCFG-LA grammar. 3http://www.eecs.berkeley.edu/pliang/software 4http://www.cnrtl.fr/corpus/estrepublicain 5The 200 most frequent compounds of the FTB-UC were systematically recognized as one token. We used Petrov’s code, adapted to French by (Crabbé and Candito, 2008), for the suffixes used to classify unknown words, and we used the same training(80%)/dev(10%)/test(10%) partition. We used the FTB-UC treebank to train a baseline parser, and three other parsers by changing the terminal symbols used in training data: desinflected forms: as described in section 3 clusters + cap: each desinflected form is replaced by its cluster bit string. If the desinflected form has no corresponding cluster (it did not appear 20 times in the unannotated corpus), a special cluster UNKC is used. Further, a _C suffix is added if the form starts with a capital. clusters + cap + </context>
</contexts>
<marker>Crabbé, Candito, 2008</marker>
<rawString>Benoit Crabbé and Marie Candito. 2008. Expériences d’analyse syntaxique statistique du français. In Actes de la 15ème Conférence sur le Traitement Automatique des Langues Naturelles (TALN’08), pages 45–54, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP’01,</booktitle>
<pages>167--202</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="2275" citStr="Gildea, 2001" startWordPosition="339" endWordPosition="340">ities that take lexical information into account. On the other hand, (Matsuzaki et al., 2005) have proposed probabilistic CFG learning with latent annotation (hereafter PCFG-LA), as a way to automate symbol splitting in unlexicalized probabilistic parsing (cf. adding latent annotations to a symbol is comparable to splitting this symbol). (Petrov et al., 2006) rendered the method usable in practice, with a tractable technique to retain only the beneficial splits. We know that both lexicalized parsing algorithm and PCFG-LA algorithm suffer from lexical data sparseness. For lexicalized parsers, (Gildea, 2001) shows that bilexical dependencies parameters are almost useless in the probabilistic scoring of parser because they are too scarce. For PCFG-LA, we have previously studied the lexicon impact on this so-called “unlexicalized” algorithm, for French parsing (Crabbé and Candito, 2008), (Candito et al., 2009). We have tested a totally unlexicalized parser, trained on a treebank where words are replaced by their POS tags. It obtains a parseval F1=86.28 (note that it induces perfect tagging). We compared it to a parser trained with word+tag as terminal symbols (to simulate a perfect tagging), achiev</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proc. of EMNLP’01, pages 167–202, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities.</title>
<date>2009</date>
<booktitle>In Proc. of EACL-09,</booktitle>
<pages>327--335</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="13996" citStr="Goldberg et al., 2009" startWordPosition="2255" endWordPosition="2258">ds. Note that we used the FTB-UC treebank. There are mors tokens in sentences than in the FTB with all compounds merged, and baseline Fl scores are a little higher (86.79 versus 86.41). 140 terminal symbols LP LR Fl UAS Vocab. size Tagging Acc. inflected forms (baseline) 86.94 86.65 86.79 91.00 27143 96.90 desinflected forms 87.42 87.32 87.37 91.14 20268 96.81 clusters + cap 88.08 87.50 87.79 91.12 1201 96.37 clusters + cap + suffixes 88.43 88.14 88.29 91.68 1987 97.04 Table 1: Parsing performance when training and parsing use clustered terminal symbols abilistic parser is the one pursued in (Goldberg et al., 2009). Within a plain PCFG, the lexical probabilities for words that are rare or absent in the treebank are taken from an external lexical probability distribution, estimated using a lexicon and the Baulm-Welch training of an HMM tagger. This is proved useful to better parse Hebrew. 7 Conclusion and future work We have tested the very simple method of replacing inflected forms by clusters of forms in a generative probabilistic parser. This crude technique has surprisingly good results and offers a very cheap and simple way to augment the vocabulary seen at training time. It seems interesting to try</context>
</contexts>
<marker>Goldberg, Tsarfaty, Adler, Elhadad, 2009</marker>
<rawString>Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael Elhadad. 2009. Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities. In Proc. of EACL-09, pages 327–335, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. ofACL-08,</booktitle>
<location>Columbus, USA.</location>
<contexts>
<context position="3694" citStr="Koo et al., 2008" startWordPosition="555" endWordPosition="558">so reported a slight improvement (F1=88.18) when word forms are clustered on a morphological basis, into lemma+tag clusters. So PCFG-LA uses lexical information, but it is too sparse, hence it benefits from word clustering. Yet the use of lemma+tag terminals supposes tagging prior to parsing. We propose here to apply rather a deterministic supervised morphological clustering that preserves tagging ambiguities, leaving it to the parser to disambiguate POS tags. We also investigate the use of unsupervised word clustering, obtained from unannotated text. It has been proved useful for parsing by (Koo et al., 2008) and their work directly inspired ours. They have shown that parsing improves when cluster information is used as features in a discriminative training method that learns dependency parsers. We investigate in this paper the use of such clusters in a generative approach to probabilistic phrase-structure parsing, simply by replacing each token by its cluster. 138 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 138–141, Paris, October 2009. c�2009 Association for Computational Linguistics We present in section 2 the treebank instantiation we use for our expe</context>
<context position="9236" citStr="Koo et al., 2008" startWordPosition="1463" endWordPosition="1466">heir corresponding plural form pronounced in the same way. 2This is just an example and not the real Lefff format. N Union N A économique A monétaire N Union A économique COORD AP A monétaire D l’ C et D l’ AP C et 139 token of a sentence, if unknown in the lexicon, the algorithm tries to desinflect the low case corresponding form. This desinflection reduces the number of distinct tokens in the FTB-UC from 27143 to 20268. 4 Unsupervised word clustering We chose to use the (Brown et al., 1992) hard clustering algorithm, which has proven useful for various NLP tasks, such as dependency parsing (Koo et al., 2008) or named entity recognition (Liang, 2005). The algorithm to obtain C clusters is as follows: each of the C most frequent tokens of the corpus is assigned its own distinct cluster. For the (C+1)th most frequent token, create a (C+1)th cluster. Then for each pair among the C+1 resulting clusters, merge the pair that minimizes the loss in the likelihood of the corpus, according to a bigram language model defined on the clusters. Repeat this operation for the (C+2)th most frequent token, etc... This results in a hard clustering into C clusters. The process can be continued to further merge pairs </context>
<context position="13113" citStr="Koo et al., 2008" startWordPosition="2106" endWordPosition="2109">l/precision, but the tagging accuracy decreases, and thus the UAS. This can be due to the coarseness of the clustering method, which sometimes groups words that have different POS (for instance among a cluster of infinite verbs, one may find a present participle). The quality of the clusters is more crucial in our case than when clusters are features, whose informativity is discriminatively learnt. This observation led us to append a restricted set of suffixes to the clusters, which gives us the best results for now. 6 Related work We already mentioned that we were inspired by the success of (Koo et al., 2008) in using word clusters as features for the discriminative learning of dependency parsers. Another approach to augment the known vocabulary for a generative prob6In all metrics punctuation tokens are ignored and all results are for sentences of less than 40 words. Note that we used the FTB-UC treebank. There are mors tokens in sentences than in the FTB with all compounds merged, and baseline Fl scores are a little higher (86.79 versus 86.41). 140 terminal symbols LP LR Fl UAS Vocab. size Tagging Acc. inflected forms (baseline) 86.94 86.65 86.79 91.00 27143 96.90 desinflected forms 87.42 87.32 </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. ofACL-08, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<booktitle>In MIT Master’s thesis,</booktitle>
<location>Cambridge, USA.</location>
<contexts>
<context position="9278" citStr="Liang, 2005" startWordPosition="1471" endWordPosition="1472">e same way. 2This is just an example and not the real Lefff format. N Union N A économique A monétaire N Union A économique COORD AP A monétaire D l’ C et D l’ AP C et 139 token of a sentence, if unknown in the lexicon, the algorithm tries to desinflect the low case corresponding form. This desinflection reduces the number of distinct tokens in the FTB-UC from 27143 to 20268. 4 Unsupervised word clustering We chose to use the (Brown et al., 1992) hard clustering algorithm, which has proven useful for various NLP tasks, such as dependency parsing (Koo et al., 2008) or named entity recognition (Liang, 2005). The algorithm to obtain C clusters is as follows: each of the C most frequent tokens of the corpus is assigned its own distinct cluster. For the (C+1)th most frequent token, create a (C+1)th cluster. Then for each pair among the C+1 resulting clusters, merge the pair that minimizes the loss in the likelihood of the corpus, according to a bigram language model defined on the clusters. Repeat this operation for the (C+2)th most frequent token, etc... This results in a hard clustering into C clusters. The process can be continued to further merge pairs of clusters among the C clusters, ending w</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. In MIT Master’s thesis, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of ACL-98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of ACL-98, pages 768– 774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic cfg with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. ofACL-05,</booktitle>
<pages>75--82</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1755" citStr="Matsuzaki et al., 2005" startWordPosition="259" endWordPosition="262">ation is known crucial in natural language parsing. For probabilistic parsing, one main drawback of the plain PCFG approach is to lack sensitivity to the lexicon. The symbols accessible to context-free rules are part-of-speech tags, which encode generalizations that are too coarse for many parsing decisions (for instance subcategorization information is generally absent from tagsets). The lexicalized models first proposed by Collins reintroduced words at every depth of a parse tree, insuring that attachments receive probabilities that take lexical information into account. On the other hand, (Matsuzaki et al., 2005) have proposed probabilistic CFG learning with latent annotation (hereafter PCFG-LA), as a way to automate symbol splitting in unlexicalized probabilistic parsing (cf. adding latent annotations to a symbol is comparable to splitting this symbol). (Petrov et al., 2006) rendered the method usable in practice, with a tractable technique to retain only the beneficial splits. We know that both lexicalized parsing algorithm and PCFG-LA algorithm suffer from lexical data sparseness. For lexicalized parsers, (Gildea, 2001) shows that bilexical dependencies parameters are almost useless in the probabil</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Proc. ofACL-05, pages 75–82, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="708" citStr="Petrov et al., 2006" startWordPosition="103" endWordPosition="106">ito and Benoît Crabbé Université Paris 7/INRIA (Alpage), 30 rue du Château des Rentiers, 75013 Paris Abstract We present a semi-supervised method to improve statistical parsing performance. We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing. We use a combination of lexiconaided morphological clustering that preserves tagging ambiguity, and unsupervised word clustering, trained on a large unannotated corpus. We apply these clusterings to the French Treebank, and we train a parser with the PCFG-LA unlexicalized algorithm of (Petrov et al., 2006). We find a gain in French parsing performance: from a baseline of F1=86.76% to F1=87.37% using morphological clustering, and up to F1=88.29% using further unsupervised clustering. This is the best known score for French probabilistic parsing. These preliminary results are encouraging for statistically parsing morphologically rich languages, and languages with small amount of annotated data. 1 Introduction Lexical information is known crucial in natural language parsing. For probabilistic parsing, one main drawback of the plain PCFG approach is to lack sensitivity to the lexicon. The symbols a</context>
<context position="2023" citStr="Petrov et al., 2006" startWordPosition="299" endWordPosition="302">too coarse for many parsing decisions (for instance subcategorization information is generally absent from tagsets). The lexicalized models first proposed by Collins reintroduced words at every depth of a parse tree, insuring that attachments receive probabilities that take lexical information into account. On the other hand, (Matsuzaki et al., 2005) have proposed probabilistic CFG learning with latent annotation (hereafter PCFG-LA), as a way to automate symbol splitting in unlexicalized probabilistic parsing (cf. adding latent annotations to a symbol is comparable to splitting this symbol). (Petrov et al., 2006) rendered the method usable in practice, with a tractable technique to retain only the beneficial splits. We know that both lexicalized parsing algorithm and PCFG-LA algorithm suffer from lexical data sparseness. For lexicalized parsers, (Gildea, 2001) shows that bilexical dependencies parameters are almost useless in the probabilistic scoring of parser because they are too scarce. For PCFG-LA, we have previously studied the lexicon impact on this so-called “unlexicalized” algorithm, for French parsing (Crabbé and Candito, 2008), (Candito et al., 2009). We have tested a totally unlexicalized p</context>
<context position="10641" citStr="Petrov et al., 2006" startWordPosition="1704" endWordPosition="1707">can be identified by its path within this binary tree. Hence, clusters can be used at various levels of granularity. 5 Experiments and discussion For the Brown clustering algorithm, we used Percy Liang’s code3, run on the L’Est Républicain corpus, a 125 million word journalistic corpus, freely available at CNRTL4. The corpus was tokenised5, segmented into sentences and desinflected using the process described in section 3. We ran the clustering into 1000 clusters for the desinflected forms appearing at least 20 times. We tested the use of word clusters for parsing with the Berkeley algorithm (Petrov et al., 2006). Clustering words in this case has a double advantage. First, it augments the known vocabulary, which is made of all the forms of all the clusters appearing in the treebank. Second, it reduces sparseness for the latent annotations learning on the lexical rules of the PCFG-LA grammar. 3http://www.eecs.berkeley.edu/pliang/software 4http://www.cnrtl.fr/corpus/estrepublicain 5The 200 most frequent compounds of the FTB-UC were systematically recognized as one token. We used Petrov’s code, adapted to French by (Crabbé and Candito, 2008), for the suffixes used to classify unknown words, and we used </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of ACL-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoît Sagot</author>
</authors>
<title>Lionel Clément, Éric Villemonte de La Clergerie, and Pierre Boullier.</title>
<date>2006</date>
<journal>The Lefff</journal>
<booktitle>In Proc. ofLREC’06,</booktitle>
<volume>2</volume>
<location>Genova, Italy.</location>
<marker>Sagot, 2006</marker>
<rawString>Benoît Sagot, Lionel Clément, Éric Villemonte de La Clergerie, and Pierre Boullier. 2006. The Lefff 2 syntactic lexicon for french: architecture, acquisition, use. In Proc. ofLREC’06, Genova, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>