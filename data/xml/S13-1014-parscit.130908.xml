<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013370">
<title confidence="0.9870775">
DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level
Semantic Similarity Estimation
</title>
<author confidence="0.786675">
Nikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,
Shrikanth Narayanan1
</author>
<affiliation confidence="0.9688485">
1Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2Department of ECE, Technical University of Crete, 73100 Chania, Greece
</affiliation>
<email confidence="0.96912">
malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,
shri@sipi.usc.edu
</email>
<sectionHeader confidence="0.998569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999700083333333">
This paper describes our submission for the
*SEM shared task of Semantic Textual Sim-
ilarity. We estimate the semantic similarity
between two sentences using regression mod-
els with features: 1) n-gram hit rates (lexical
matches) between sentences, 2) lexical seman-
tic similarity between non-matching words, 3)
string similarity metrics, 4) affective content
similarity and 5) sentence length. Domain
adaptation is applied in the form of indepen-
dent models and a model selection strategy
achieving a mean correlation of 0.47.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952319148936">
Text semantic similarity estimation has been an ac-
tive research area, thanks to a variety of potential ap-
plications and the wide availability of data afforded
by the world wide web. Semantic textual similar-
ity (STS) estimates can be used for information ex-
traction (Szpektor and Dagan, 2008), question an-
swering (Harabagiu and Hickl, 2006) and machine
translation (Mirkin et al., 2009). Term-level simi-
larity has been successfully applied to problems like
grammar induction (Meng and Siu, 2002) and affec-
tive text categorization (Malandrakis et al., 2011). In
this work, we built on previous research and our sub-
mission to SemEval’2012 (Malandrakis et al., 2012)
to create a sentence-level STS model for the shared
task of *SEM 2013 (Agirre et al., 2013).
Semantic similarity between words has been
well researched, with a variety of knowledge-based
(Miller, 1990; Budanitsky and Hirst, 2006) and
corpus-based (Baroni and Lenci, 2010; Iosif and
Potamianos, 2010) metrics proposed. Moving to
sentences increases the complexity exponentially
and as a result has led to measurements of simi-
larity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al., 2009), and semantic (Rinaldi
et al., 2003; Bos and Markert, 2005). Machine trans-
lation evaluation metrics can be used to estimate lex-
ical level similarity (Finch et al., 2005; Perez and
Alfonseca, 2005), including BLEU (Papineni et al.,
2002), a metric using word n-gram hit rates. The pi-
lot task of sentence STS in SemEval 2012 (Agirre et
al., 2012) showed a similar trend towards multi-level
similarity, with the top performing systems utilizing
large amounts of partial similarity metrics and do-
main adaptation (the use of separate models for each
input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012).
Our approach is originally motivated by BLEU
and primarily utilizes “hard” and “soft” n-gram hit
rates to estimate similarity. Compared to last year,
we utilize different alignment strategies (to decide
which n-grams should be compared with which).
We also include string similarities (at the token and
character level) and similarity of affective content,
expressed through the difference in sentence arousal
and valence ratings. Finally we added domain adap-
tation: the creation of separate models per domain
and a strategy to select the most appropriate model.
</bodyText>
<sectionHeader confidence="0.988476" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.99865325">
Our model is based upon that submitted for the same
task in 2012 (Malandrakis et al., 2012). To esti-
mate semantic similarity metrics we use a super-
vised model with features extracted using corpus-
</bodyText>
<page confidence="0.987041">
103
</page>
<subsubsectionHeader confidence="0.25271">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.919541285714286">
and the Shared Task, pages 103–108, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
based word-level similarity metrics. To combine
these metrics into a sentence-level similarity score
we use a modification of BLEU (Papineni et al.,
2002) that utilizes word-level semantic similarities,
string level comparisons and comparisons of affec-
tive content, detailed below.
</bodyText>
<subsectionHeader confidence="0.995916">
2.1 Word level semantic similarity
</subsectionHeader>
<bodyText confidence="0.988611823529412">
Co-occurrence-based. The semantic similarity be-
tween two words, wi and wj, is estimated as their
pointwise mutual information (Church and Hanks,
p(i,j), where ˆp(i) and ˆp(j) are
ˆp(i)ˆp(j )
the occurrence probabilities of wi and wj, respec-
tively, while the probability of their co-occurrence
is denoted by ˆp(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al., 2012)
we employed a modification of the pointwise mutual
information based on the maximum sense similar-
ity assumption (Resnik, 1995) and the minimization
of the respective error in similarity estimation. In
particular, exponential weights α were introduced in
order to reduce the overestimation of denominator
probabilities. The modified metric Ia(i, j), is de-
fined as:
</bodyText>
<equation confidence="0.988206">
Ia(i,j)=2 Llogˆpα(i)ˆp(j) + logp(i)ˆpα(j)I (1)
</equation>
<bodyText confidence="0.984718290322581">
The weight α was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of α = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets
(Rubenstein and Goodenough, 1965; Miller and
Charles, 1998; Finkelstein et al., 2002).
Context-based: The fundamental assumption
behind context-based metrics is that similarity
of context implies similarity of meaning (Harris,
1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj, is computed as the cosine
of their feature vectors: QH(i, j) = vi.v;
||v�  |v;||.
The elements of feature vectors can be weighted
according various schemes [(Iosif and Potamianos,
2010)], while, here we use a binary scheme.
Network-based: The aforementioned similarity
metrics were used for the definition of a semantic
network (Iosif and Potamianos, 2013; Iosif et al.,
2013). A number of similarity metrics were pro-
posed under either the attributional similarity (Tur-
ney, 2006) or the maximum sense similarity (Resnik,
1995) assumptions of lexical semantics1.
</bodyText>
<subsectionHeader confidence="0.999241">
2.2 Sentence level similarities
</subsectionHeader>
<bodyText confidence="0.99999816">
To utilize word-level semantic similarities in the
sentence-level task we use a modified version of
BLEU (Papineni et al., 2002). The model works in
two passes: the first pass identifies exact matches
(similar to baseline BLEU), the second pass com-
pares non-matched terms using semantic similarity.
Non-matched terms from the hypothesis sentence
are compared with all terms of the reference sen-
tence (regardless of whether they were matched dur-
ing the first pass). In the case of bigram and higher
order terms, the process is applied recursively: the
bigrams are decomposed into two words and the
similarity between them is estimated by applying the
same method to the words. All word similarity met-
rics used are peak-to-peak normalized in the [0,1]
range, so they serve as a “degree-of-match”. The se-
mantic similarity scores from term pairs are summed
(just like n-gram hits) to obtain a BLEU-like hit-rate.
Alignment is performed via maximum similarity:
we iterate on the hypothesis n-grams, left-to-right,
and compare each with the most similar n-gram in
the reference. The features produced by this process
are “soft” hit-rates (for 1-, 2-, 3-, 4-grams)2. We also
use the “hard” hit rates produced by baseline BLEU
as features of the final model.
</bodyText>
<subsectionHeader confidence="0.999857">
2.3 String similarities
</subsectionHeader>
<bodyText confidence="0.9039584">
We use the following string-based similarity fea-
tures: 1) Longest Common Subsequence Similarity
(LCSS) (Lin and Och, 2004) based on the Longest
Common Subsequence (LCS) character-based dy-
1The network-based metrics were applied only during the
training phase of the shared task, due to time limitations. They
exhibited almost identical performance as the metric defined by
(1), which was used in the test runs.
2Note that the features are computed twice on each sentence
pair and then averaged.
</bodyText>
<equation confidence="0.84774">
1990): I(i, j) = log
</equation>
<page confidence="0.985758">
104
</page>
<bodyText confidence="0.9988845">
namic programming algorithm. LCSS represents the
length of the longest string (or strings) that is a sub-
string (or are substrings) of two or more strings. 2)
Skip bigram co-occurrence measures the overlap of
skip-bigrams between two sentences or phrases. A
skip-bigram is defined as any pair of words in the
sentence order, allowing for arbitrary gaps between
words (Lin and Och, 2004). 3) Containment is de-
fined as the percentage of a sentence that is con-
tained in another sentence. It is a number between
0 and 1, where 1 means the hypothesis sentence is
fully contained in the reference sentence (Broder,
1997). We express containment as the amount of n-
grams of a sentence contained in another. The con-
tainment metric is not symmetric and is calculated
as: c(X,Y ) = JS(X) n S(Y )J/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.
</bodyText>
<subsectionHeader confidence="0.99722">
2.4 Affective similarity
</subsectionHeader>
<bodyText confidence="0.999994166666667">
We used the method proposed in (Malandrakis et al.,
2011) to estimate affective features. Continuous (va-
lence and arousal) ratings in [−1, 1] of any term are
represented as a linear combination of a function of
its semantic similarities to a set of seed words and
the affective ratings of these words, as follows:
</bodyText>
<equation confidence="0.985574">
N
v(wj) = a0 + ai v(wi) dij, (2)
i=1
</equation>
<bodyText confidence="0.9998723125">
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
and wj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English
Words (ANEW) (Bradley and Lang, 1999) corpus.
Using this model we generate affective ratings for
every content word (noun, verb, adjective or adverb)
of every sentence. We assume that these can ad-
equately describe the affective content of the sen-
tences. To create an “affective similarity metric” we
use the difference of means of the word affective rat-
ings between two sentences.
</bodyText>
<equation confidence="0.989077">
daffect = 2 − Jµ(�v(s1)) − µ(�v(s2))J (3)
</equation>
<bodyText confidence="0.9997045">
where µ(v(si)) the mean of content word ratings in-
cluded in sentence i.
</bodyText>
<subsectionHeader confidence="0.886804">
2.5 Fusion
</subsectionHeader>
<bodyText confidence="0.995665333333333">
The aforementioned features are combined using
one of two possible models. The first model is a
Multiple Linear Regression (MLR) model
</bodyText>
<equation confidence="0.988774666666667">
k
DL = a0 + an fk, (4)
n=1
</equation>
<bodyText confidence="0.995274652173913">
where DL is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.
The second model is motivated by an assumption
of cognitive scaling of similarity scores: we expect
that the perception of hit rates is non-linearly af-
fected by the length of the sentences. We call this the
hierarchical fusion scheme. It is a combination of
(overlapping) MLR models, each matching a range
of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l &lt; l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l E [1, l1] are decoded
with DL1, sentences with length l E (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,
we use four models with l1 = 10, l2 = 20, l3 = 30,
l4 = 00.
Domain adaptation is employed, by creating sep-
arate models per domain (training data source). Be-
yond that, we also create a unified model, trained
on all data to be used as a fallback if an appropriate
model can not be decided upon during evaluation.
</bodyText>
<sectionHeader confidence="0.994895" genericHeader="method">
3 Experimental Procedure and Results
</sectionHeader>
<bodyText confidence="0.999850466666667">
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al., 2005; Toutanova et al.,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. We evaluated
multiple types of preprocessing per unsupervised
metric and chose different ones depending on the
metric. Word-level semantic similarities, used for
soft comparisons and affective feature extraction,
were computed over a corpus of 116 million web
snippets collected by posing one query for every
word in the Aspell spellchecker (asp, ) vocabulary to
the Yahoo! search engine. Word-level emotional rat-
ings in continuous valence and arousal scales were
produced by a model trained on the ANEW dataset
</bodyText>
<page confidence="0.998498">
105
</page>
<bodyText confidence="0.999908888888889">
and using contextual similarities. Finally, string sim-
ilarities were calculated over the original unmodified
sentences.
Next, results are reported in terms of correla-
tion between the generated scores and the ground
truth, for each corpus in the shared task, as well as
their weighted mean. Feature selection is applied
to the large candidate feature set using a wrapper-
based backward selection approach on the train-
ing data.The final feature set contains 15 features:
soft hit rates calculated over content word 1- to 4-
grams (4 features), soft hit rates calculated over un-
igrams per part-of-speech, for adjectives, nouns, ad-
verbs, verbs (4 features), BLEU unigram hit rates
for all words and content words (2 features), skip
and containment similarities, containment normal-
ized by sum of sentence lengths or product of sen-
tence lengths (3 features) and affective similarities
for arousal and valence (2 features).
Domain adaptation methods are the only dif-
ference between the three submitted runs. For all
three runs we train one linear model per training set
and a fallback model. For the first run, dubbed lin-
ear, the fallback model is linear and model selection
during evaluation is performed by file name, there-
fore results for the OnWN set are produced by a
model trained with OnWN data, while the rest are
produced by the fallback model. The second run,
dubbed length, uses a hierarchical fallback model
and model selection is performed by file name. The
third run, dubbed adapt, uses the same models as
the first run and each test set is assigned to a model
(i.e., the fallback model is never used). The test set -
model (training) mapping for this run is: OnWN -*
OnWN, headlines -* SMTnews, SMT -* Europarl
and FNWN -* OnWN.
</bodyText>
<tableCaption confidence="0.993097">
Table 1: Correlation performance for the linear model us-
ing lexical (L), string (S) and affect (A) features
</tableCaption>
<table confidence="0.843707">
Feature headl. OnWN FNWN SMT mean
L 0.68 0.51 0.23 0.25 0.46
L+S 0.69 0.49 0.23 0.26 0.46
L+S+A 0.69 0.51 0.27 0.28 0.47
</table>
<bodyText confidence="0.948875">
Results are shown in Tables 1 and 2. Results for
the linear run using subsets of the final feature set
are shown in Table 1. Lexical features (hit rates) are
obviously the most valuable features. String similar-
ities provided us with an improvement in the train-
</bodyText>
<tableCaption confidence="0.999006">
Table 2: Correlation performance on the evaluation set.
</tableCaption>
<table confidence="0.95556725">
Run headl. OnWN FNWN SMT mean
linear 0.69 0.51 0.27 0.28 0.47
length 0.65 0.51 0.25 0.28 0.46
adapt 0.62 0.51 0.33 0.30 0.46
</table>
<bodyText confidence="0.999867714285714">
ing set which is not reflected in the test set. Af-
fect proved valuable, particularly in the most diffi-
cult sets of FNWN and SMT.
Results for the three submission runs are shown
in Table 2. Our best run was the simplest one, using
a purely linear model and effectively no adaptation.
Adding a more aggressive adaptation strategy im-
proved results in the FNWN and SMT sets, so there
is definitely some potential, however the improve-
ment observed is nowhere near that observed in the
training data or the same task of SemEval 2012. We
have to question whether this improvement is an ar-
tifact of the rating distributions of these two sets
(SMT contains virtually only high ratings, FNWN
contains virtually only low ratings): such wild mis-
matches in priors among training and test sets can
be mitigated using more elaborate machine learning
algorithms (rather than employing better semantic
similarity features or algorithms). Overall the sys-
tem performs well in the two sets containing large
similarity rating ranges.
</bodyText>
<sectionHeader confidence="0.998171" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999705">
We have improved over our previous model of sen-
tence semantic similarity. The inclusion of string-
based similarities and more so of affective content
measures proved significant, but domain adaptation
provided mixed results. While expanding the model
to include more layers of similarity estimates is
clearly a step in the right direction, further work is
required to include even more layers. Using syntac-
tic information and more levels of abstraction (e.g.
concepts) are obvious next steps.
</bodyText>
<sectionHeader confidence="0.995939" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997592">
The first four authors have been partially funded
by the PortDial project (Language Resources for
Portable Multilingual Spoken Dialog Systems) sup-
ported by the EU Seventh Framework Programme
(FP7), grant number 296170.
</bodyText>
<page confidence="0.998231">
106
</page>
<sectionHeader confidence="0.985664" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999443371428571">
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. SemEval, pages 385–393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In Proc. *SEM.
Gnu aspell. http://www.aspell.net.
D. B¨ar, C. Biemann, I. Gurevych, and T. Zesch. 2012.
Ukp: Computing semantic textual similarity by com-
bining multiple content similarity measures. In Proc.
SemEval, pages 435–440.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
M. Bradley and P. Lang. 1999. Affective norms for En-
glish words (ANEW): Stimuli, instruction manual and
affective ratings. Technical report C-1. The Center for
Research in Psychophysiology, University of Florida.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In In Compression and Com-
plexity of Sequences (SEQUENCES97, pages 21–29.
IEEE Computer Society.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13–47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22–29.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363–370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116–131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905–912.
Z. Harris. 1954. Distributional structure. Word,
10(23):146–162.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637–1647.
E. Iosif and A. Potamianos. 2012. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. In Proc. Eighth International Con-
ference on Language Resources and Evaluation, pages
3499–3504.
Elias Iosif and Alexandros Potamianos. 2013. Similarity
Computation Using Semantic Networks Created From
Web-Harvested Data. Natural Language Engineering,
(submitted).
E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-
vanou. 2013. Semantic similarity computation for ab-
stract and concrete nouns using network-based distri-
butional semantic models. In 10th International Con-
ference on Computational Semantics (IWCS), pages
328–334.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42–47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42–47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977–2980.
N. Malandrakis, E. Iosif, and A. Potamianos. 2012.
DeepPurple: Estimating sentence semantic similarity
using n-gram regression models and web snippets. In
Proc. Sixth International Workshop on Semantic Eval-
uation (SemEval) – The First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
565–570.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
</reference>
<page confidence="0.983748">
107
</page>
<reference confidence="0.998800452830189">
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172–181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1–28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235–312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th Annual Meeting ofACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791–799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311–318.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448–453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25–32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627–633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849–856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173–180.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379–416.
F. &amp;quot;Sari´c, G. Glava&amp;quot;s, M. Karan, J. &amp;quot;Snajder, and B. Dal-
belo Ba&amp;quot;si´c. 2012. Takelab: Systems for measuring
semantic text similarity. In Proc. SemEval, pages 441–
448.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
</reference>
<page confidence="0.998364">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684226">
<title confidence="0.999347">DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level Semantic Similarity Estimation</title>
<author confidence="0.951439">Elias Vassiliki Alexandros</author>
<address confidence="0.872935">Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, of ECE, Technical University of Crete, 73100 Chania, Greece</address>
<email confidence="0.980669">malandra@usc.edu,iosife@telecom.tuc.gr,vprokopi@isc.tuc.gr,potam@telecom.tuc.gr,shri@sipi.usc.edu</email>
<abstract confidence="0.997179615384615">This paper describes our submission for the *SEM shared task of Semantic Textual Similarity. We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Cer</author>
<author>M Diab</author>
<author>A Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proc. SemEval,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="2579" citStr="Agirre et al., 2012" startWordPosition="372" endWordPosition="375">and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012). Our approach is originally motivated by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize different alignment strategies (to decide which n-grams should be compared with which). We also include string similarities (at the token and character level) and similarity of affe</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proc. SemEval, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In Proc. *SEM.</booktitle>
<contexts>
<context position="1772" citStr="Agirre et al., 2013" startWordPosition="249" endWordPosition="252">ability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In Proc. *SEM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gnu aspell</author>
</authors>
<note>http://www.aspell.net.</note>
<marker>aspell, </marker>
<rawString>Gnu aspell. http://www.aspell.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B¨ar</author>
<author>C Biemann</author>
<author>I Gurevych</author>
<author>T Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proc. SemEval,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>D. B¨ar, C. Biemann, I. Gurevych, and T. Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proc. SemEval, pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="1951" citStr="Baroni and Lenci, 2010" startWordPosition="274" endWordPosition="277"> (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEva</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>K Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>628635</pages>
<contexts>
<context position="2294" citStr="Bos and Markert, 2005" startWordPosition="323" endWordPosition="326">., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012). Our approach is originally motivated by BLEU and primaril</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>J. Bos and K. Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, page 628635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bradley</author>
<author>P Lang</author>
</authors>
<title>Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings. Technical report C-1. The Center for Research in Psychophysiology,</title>
<date>1999</date>
<institution>University of Florida.</institution>
<contexts>
<context position="9934" citStr="Bradley and Lang, 1999" startWordPosition="1552" endWordPosition="1555">ar combination of a function of its semantic similarities to a set of seed words and the affective ratings of these words, as follows: N v(wj) = a0 + ai v(wi) dij, (2) i=1 where wj is the term we mean to characterize, w1...wN are the seed words, v(wi) is the valence rating for seed word wi, ai is the weight corresponding to seed word wi (that is estimated as described next), dij is a measure of semantic similarity between wi and wj (for the purposes of this work, cosine similarity between context vectors is used). The weights ai are estimated over the Affective norms for English Words (ANEW) (Bradley and Lang, 1999) corpus. Using this model we generate affective ratings for every content word (noun, verb, adjective or adverb) of every sentence. We assume that these can adequately describe the affective content of the sentences. To create an “affective similarity metric” we use the difference of means of the word affective ratings between two sentences. daffect = 2 − Jµ(�v(s1)) − µ(�v(s2))J (3) where µ(v(si)) the mean of content word ratings included in sentence i. 2.5 Fusion The aforementioned features are combined using one of two possible models. The first model is a Multiple Linear Regression (MLR) mo</context>
</contexts>
<marker>Bradley, Lang, 1999</marker>
<rawString>M. Bradley and P. Lang. 1999. Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings. Technical report C-1. The Center for Research in Psychophysiology, University of Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Z Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1997</date>
<booktitle>In In Compression and Complexity of Sequences (SEQUENCES97,</booktitle>
<pages>21--29</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="8853" citStr="Broder, 1997" startWordPosition="1358" endWordPosition="1359">namic programming algorithm. LCSS represents the length of the longest string (or strings) that is a substring (or are substrings) of two or more strings. 2) Skip bigram co-occurrence measures the overlap of skip-bigrams between two sentences or phrases. A skip-bigram is defined as any pair of words in the sentence order, allowing for arbitrary gaps between words (Lin and Och, 2004). 3) Containment is defined as the percentage of a sentence that is contained in another sentence. It is a number between 0 and 1, where 1 means the hypothesis sentence is fully contained in the reference sentence (Broder, 1997). We express containment as the amount of ngrams of a sentence contained in another. The containment metric is not symmetric and is calculated as: c(X,Y ) = JS(X) n S(Y )J/S(X), where S(X) and S(Y ) are all the n-grams of sentences X and Y respectively. 2.4 Affective similarity We used the method proposed in (Malandrakis et al., 2011) to estimate affective features. Continuous (valence and arousal) ratings in [−1, 1] of any term are represented as a linear combination of a function of its semantic similarities to a set of seed words and the affective ratings of these words, as follows: N v(wj)</context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>Andrei Z. Broder. 1997. On the resemblance and containment of documents. In In Compression and Complexity of Sequences (SEQUENCES97, pages 21–29. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of semantic distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>32--13</pages>
<contexts>
<context position="1910" citStr="Budanitsky and Hirst, 2006" startWordPosition="268" endWordPosition="271">(Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates.</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased measures of semantic distance. Computational Linguistics, 32:13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Church, Hanks, 1990</marker>
<rawString>K. W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finch</author>
<author>S Y Hwang</author>
<author>E Sumita</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Workshop on Paraphrasing,</booktitle>
<pages>1724</pages>
<contexts>
<context position="2403" citStr="Finch et al., 2005" startWordPosition="341" endWordPosition="344">milarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012). Our approach is originally motivated by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize diffe</context>
</contexts>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proceedings of the 3rd International Workshop on Paraphrasing, page 1724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="11793" citStr="Finkel et al., 2005" startWordPosition="1884" endWordPosition="1887">ntences with length l E [1, l1] are decoded with DL1, sentences with length l E (l1, l2] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (4). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = 00. Domain adaptation is employed, by creating separate models per domain (training data source). Beyond that, we also create a unified model, trained on all data to be used as a fallback if an appropriate model can not be decided upon during evaluation. 3 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. We evaluated multiple types of preprocessing per unsupervised metric and chose different ones depending on the metric. Word-level semantic similarities, used for soft comparisons and affective feature extraction, were computed over a corpus of 116 million web snippets collected by posing one query for every word in the Aspell spellchecker (asp, ) vocabulary to the Yahoo! search engine. Word-level emotional ratings in continuous valence a</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. D. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="5394" citStr="Finkelstein et al., 2002" startWordPosition="797" endWordPosition="800"> exponential weights α were introduced in order to reduce the overestimation of denominator probabilities. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 Llogˆpα(i)ˆp(j) + logp(i)ˆpα(j)I (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). Context-based: The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954). A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors can be weighted according va</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for Using Textual Entailment in Open-Domain Question Answering.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="1357" citStr="Harabagiu and Hickl, 2006" startWordPosition="182" endWordPosition="185">xical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosi</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. 2006. Methods for Using Textual Entailment in Open-Domain Question Answering. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 905–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="5541" citStr="Harris, 1954" startWordPosition="817" endWordPosition="818">=2 Llogˆpα(i)ˆp(j) + logp(i)ˆpα(j)I (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). Context-based: The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954). A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used fo</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Unsupervised semantic similarity computation between terms using web documents.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>11</issue>
<contexts>
<context position="1980" citStr="Iosif and Potamianos, 2010" startWordPosition="278" endWordPosition="281">006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) </context>
<context position="6037" citStr="Iosif and Potamianos, 2010" startWordPosition="904" endWordPosition="907"> The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954). A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1. 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exac</context>
</contexts>
<marker>Iosif, Potamianos, 2010</marker>
<rawString>E. Iosif and A. Potamianos. 2010. Unsupervised semantic similarity computation between terms using web documents. IEEE Transactions on Knowledge and Data Engineering, 22(11):1637–1647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Semsim: Resources for normalized semantic similarity computation using lexical networks.</title>
<date>2012</date>
<booktitle>In Proc. Eighth International Conference on Language Resources and Evaluation,</booktitle>
<pages>3499--3504</pages>
<contexts>
<context position="5040" citStr="Iosif and Potamianos, 2012" startWordPosition="742" endWordPosition="745">e the probability of their co-occurrence is denoted by ˆp(i, j). In our previous participation in SemEval12-STS task (Malandrakis et al., 2012) we employed a modification of the pointwise mutual information based on the maximum sense similarity assumption (Resnik, 1995) and the minimization of the respective error in similarity estimation. In particular, exponential weights α were introduced in order to reduce the overestimation of denominator probabilities. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 Llogˆpα(i)ˆp(j) + logp(i)ˆpα(j)I (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). Context-based: The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954). A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical featu</context>
</contexts>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012. Semsim: Resources for normalized semantic similarity computation using lexical networks. In Proc. Eighth International Conference on Language Resources and Evaluation, pages 3499–3504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Iosif</author>
<author>Alexandros Potamianos</author>
</authors>
<title>Similarity Computation Using Semantic Networks Created From Web-Harvested Data. Natural Language Engineering,</title>
<date>2013</date>
<location>(submitted).</location>
<contexts>
<context position="6207" citStr="Iosif and Potamianos, 2013" startWordPosition="929" endWordPosition="932">ds is centered on the word of interest wi and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1. 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity. Non-matched terms from the hypothesis sentence are compared wi</context>
</contexts>
<marker>Iosif, Potamianos, 2013</marker>
<rawString>Elias Iosif and Alexandros Potamianos. 2013. Similarity Computation Using Semantic Networks Created From Web-Harvested Data. Natural Language Engineering, (submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
<author>M Giannoudaki</author>
<author>K Zervanou</author>
</authors>
<title>Semantic similarity computation for abstract and concrete nouns using network-based distributional semantic models.</title>
<date>2013</date>
<booktitle>In 10th International Conference on Computational Semantics (IWCS),</booktitle>
<pages>328--334</pages>
<contexts>
<context position="6228" citStr="Iosif et al., 2013" startWordPosition="933" endWordPosition="936">f interest wi and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1. 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity. Non-matched terms from the hypothesis sentence are compared with all terms of the r</context>
</contexts>
<marker>Iosif, Potamianos, Giannoudaki, Zervanou, 2013</marker>
<rawString>E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zervanou. 2013. Semantic similarity computation for abstract and concrete nouns using network-based distributional semantic models. In 10th International Conference on Computational Semantics (IWCS), pages 328–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7843" citStr="Lin and Och, 2004" startWordPosition="1187" endWordPosition="1190">gree-of-match”. The semantic similarity scores from term pairs are summed (just like n-gram hits) to obtain a BLEU-like hit-rate. Alignment is performed via maximum similarity: we iterate on the hypothesis n-grams, left-to-right, and compare each with the most similar n-gram in the reference. The features produced by this process are “soft” hit-rates (for 1-, 2-, 3-, 4-grams)2. We also use the “hard” hit rates produced by baseline BLEU as features of the final model. 2.3 String similarities We use the following string-based similarity features: 1) Longest Common Subsequence Similarity (LCSS) (Lin and Och, 2004) based on the Longest Common Subsequence (LCS) character-based dy1The network-based metrics were applied only during the training phase of the shared task, due to time limitations. They exhibited almost identical performance as the metric defined by (1), which was used in the test runs. 2Note that the features are computed twice on each sentence pair and then averaged. 1990): I(i, j) = log 104 namic programming algorithm. LCSS represents the length of the longest string (or strings) that is a substring (or are substrings) of two or more strings. 2) Skip bigram co-occurrence measures the overla</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="2179" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="306" endWordPosition="309">zation (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each</context>
</contexts>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>P. Malakasiotis and I. Androutsopoulos. 2007. Learning textual entailment using svms and string similarity measures. In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
</authors>
<title>Paraphrase recognition using machine learning to combine similarity measures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="2210" citStr="Malakasiotis, 2009" startWordPosition="311" endWordPosition="312">built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 20</context>
</contexts>
<marker>Malakasiotis, 2009</marker>
<rawString>P. Malakasiotis. 2009. Paraphrase recognition using machine learning to combine similarity measures. In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Malandrakis</author>
<author>A Potamianos</author>
<author>E Iosif</author>
<author>S Narayanan</author>
</authors>
<title>Kernel models for affective lexicon creation.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2977--2980</pages>
<contexts>
<context position="1573" citStr="Malandrakis et al., 2011" startWordPosition="214" endWordPosition="217">lection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos,</context>
<context position="9189" citStr="Malandrakis et al., 2011" startWordPosition="1417" endWordPosition="1420">allowing for arbitrary gaps between words (Lin and Och, 2004). 3) Containment is defined as the percentage of a sentence that is contained in another sentence. It is a number between 0 and 1, where 1 means the hypothesis sentence is fully contained in the reference sentence (Broder, 1997). We express containment as the amount of ngrams of a sentence contained in another. The containment metric is not symmetric and is calculated as: c(X,Y ) = JS(X) n S(Y )J/S(X), where S(X) and S(Y ) are all the n-grams of sentences X and Y respectively. 2.4 Affective similarity We used the method proposed in (Malandrakis et al., 2011) to estimate affective features. Continuous (valence and arousal) ratings in [−1, 1] of any term are represented as a linear combination of a function of its semantic similarities to a set of seed words and the affective ratings of these words, as follows: N v(wj) = a0 + ai v(wi) dij, (2) i=1 where wj is the term we mean to characterize, w1...wN are the seed words, v(wi) is the valence rating for seed word wi, ai is the weight corresponding to seed word wi (that is estimated as described next), dij is a measure of semantic similarity between wi and wj (for the purposes of this work, cosine sim</context>
</contexts>
<marker>Malandrakis, Potamianos, Iosif, Narayanan, 2011</marker>
<rawString>N. Malandrakis, A. Potamianos, E. Iosif, and S. Narayanan. 2011. Kernel models for affective lexicon creation. In Proc. Interspeech, pages 2977–2980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Malandrakis</author>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>DeepPurple: Estimating sentence semantic similarity using n-gram regression models and web snippets.</title>
<date>2012</date>
<booktitle>In Proc. Sixth International Workshop on Semantic Evaluation (SemEval) – The First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>565--570</pages>
<contexts>
<context position="1680" citStr="Malandrakis et al., 2012" startWordPosition="232" endWordPosition="235">as been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and </context>
<context position="3499" citStr="Malandrakis et al., 2012" startWordPosition="516" endWordPosition="519">by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize different alignment strategies (to decide which n-grams should be compared with which). We also include string similarities (at the token and character level) and similarity of affective content, expressed through the difference in sentence arousal and valence ratings. Finally we added domain adaptation: the creation of separate models per domain and a strategy to select the most appropriate model. 2 Model Our model is based upon that submitted for the same task in 2012 (Malandrakis et al., 2012). To estimate semantic similarity metrics we use a supervised model with features extracted using corpus103 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 103–108, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics based word-level similarity metrics. To combine these metrics into a sentence-level similarity score we use a modification of BLEU (Papineni et al., 2002) that utilizes word-level semantic similarities, string level comparisons and comparisons of affectiv</context>
</contexts>
<marker>Malandrakis, Iosif, Potamianos, 2012</marker>
<rawString>N. Malandrakis, E. Iosif, and A. Potamianos. 2012. DeepPurple: Estimating sentence semantic similarity using n-gram regression models and web snippets. In Proc. Sixth International Workshop on Semantic Evaluation (SemEval) – The First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Meng</author>
<author>K-C Siu</author>
</authors>
<title>Semi-automatic acquisition of semantic structures for understanding domainspecific natural language queries.</title>
<date>2002</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="1512" citStr="Meng and Siu, 2002" startWordPosition="205" endWordPosition="208">pplied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity </context>
</contexts>
<marker>Meng, Siu, 2002</marker>
<rawString>H. Meng and K.-C. Siu. 2002. Semi-automatic acquisition of semantic structures for understanding domainspecific natural language queries. IEEE Transactions on Knowledge and Data Engineering, 14(1):172–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="5367" citStr="Miller and Charles, 1998" startWordPosition="793" endWordPosition="796">estimation. In particular, exponential weights α were introduced in order to reduce the overestimation of denominator probabilities. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 Llogˆpα(i)ˆp(j) + logp(i)ˆpα(j)I (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). Context-based: The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954). A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors c</context>
</contexts>
<marker>Miller, Charles, 1998</marker>
<rawString>G. Miller and W. Charles. 1998. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1881" citStr="Miller, 1990" startWordPosition="266" endWordPosition="267">on extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mirkin</author>
<author>L Specia</author>
<author>N Cancedda</author>
<author>I Dagan</author>
<author>M Dymetman</author>
<author>S Idan</author>
</authors>
<title>Source-language entailment modeling for translating unknown terms.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting ofACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>791--799</pages>
<contexts>
<context position="1403" citStr="Mirkin et al., 2009" startWordPosition="189" endWordPosition="192">, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Movi</context>
</contexts>
<marker>Mirkin, Specia, Cancedda, Dagan, Dymetman, Idan, 2009</marker>
<rawString>S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymetman, and S. Idan. 2009. Source-language entailment modeling for translating unknown terms. In Proceedings of the 47th Annual Meeting ofACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 791–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="2471" citStr="Papineni et al., 2002" startWordPosition="351" endWordPosition="354">of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012). Our approach is originally motivated by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize different alignment strategies (to decide which n-grams should be compare</context>
<context position="3998" citStr="Papineni et al., 2002" startWordPosition="589" endWordPosition="592">most appropriate model. 2 Model Our model is based upon that submitted for the same task in 2012 (Malandrakis et al., 2012). To estimate semantic similarity metrics we use a supervised model with features extracted using corpus103 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 103–108, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics based word-level similarity metrics. To combine these metrics into a sentence-level similarity score we use a modification of BLEU (Papineni et al., 2002) that utilizes word-level semantic similarities, string level comparisons and comparisons of affective content, detailed below. 2.1 Word level semantic similarity Co-occurrence-based. The semantic similarity between two words, wi and wj, is estimated as their pointwise mutual information (Church and Hanks, p(i,j), where ˆp(i) and ˆp(j) are ˆp(i)ˆp(j ) the occurrence probabilities of wi and wj, respectively, while the probability of their co-occurrence is denoted by ˆp(i, j). In our previous participation in SemEval12-STS task (Malandrakis et al., 2012) we employed a modification of the pointwi</context>
<context position="6574" citStr="Papineni et al., 2002" startWordPosition="984" endWordPosition="987">ature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1. 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity. Non-matched terms from the hypothesis sentence are compared with all terms of the reference sentence (regardless of whether they were matched during the first pass). In the case of bigram and higher order terms, the process is applied recursively: the bigrams are decomposed into two words and the similarity between them is estimated by applying the same method to the words. All word similarity metrics used are peak-to-peak no</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perez</author>
<author>E Alfonseca</author>
</authors>
<title>Application of the bleu algorithm for recognizing textual entailments.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="2431" citStr="Perez and Alfonseca, 2005" startWordPosition="345" endWordPosition="348">ds has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012). Our approach is originally motivated by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize different alignment strategies (t</context>
</contexts>
<marker>Perez, Alfonseca, 2005</marker>
<rawString>D. Perez and E. Alfonseca. 2005. Application of the bleu algorithm for recognizing textual entailments. In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxanomy.</title>
<date>1995</date>
<booktitle>In Proc. of International Joint Conference for Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="4683" citStr="Resnik, 1995" startWordPosition="691" endWordPosition="692">s and comparisons of affective content, detailed below. 2.1 Word level semantic similarity Co-occurrence-based. The semantic similarity between two words, wi and wj, is estimated as their pointwise mutual information (Church and Hanks, p(i,j), where ˆp(i) and ˆp(j) are ˆp(i)ˆp(j ) the occurrence probabilities of wi and wj, respectively, while the probability of their co-occurrence is denoted by ˆp(i, j). In our previous participation in SemEval12-STS task (Malandrakis et al., 2012) we employed a modification of the pointwise mutual information based on the maximum sense similarity assumption (Resnik, 1995) and the minimization of the respective error in similarity estimation. In particular, exponential weights α were introduced in order to reduce the overestimation of denominator probabilities. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 Llogˆpα(i)ˆp(j) + logp(i)ˆpα(j)I (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard sem</context>
<context position="6378" citStr="Resnik, 1995" startWordPosition="958" endWordPosition="959"> a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1. 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity. Non-matched terms from the hypothesis sentence are compared with all terms of the reference sentence (regardless of whether they were matched during the first pass). In the case of bigram and higher order terms, the process is applie</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxanomy. In Proc. of International Joint Conference for Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rinaldi</author>
<author>J Dowdall</author>
<author>K Kaljurand</author>
<author>M Hess</author>
<author>D Molla</author>
</authors>
<title>Exploiting paraphrases in a question answering system.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Workshop on Paraphrasing,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2270" citStr="Rinaldi et al., 2003" startWordPosition="319" endWordPosition="322">012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012). Our approach is originally motiva</context>
</contexts>
<marker>Rinaldi, Dowdall, Kaljurand, Hess, Molla, 2003</marker>
<rawString>F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and D. Molla. 2003. Exploiting paraphrases in a question answering system. In Proceedings of the 2nd International Workshop on Paraphrasing, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="5341" citStr="Rubenstein and Goodenough, 1965" startWordPosition="789" endWordPosition="792">e respective error in similarity estimation. In particular, exponential weights α were introduced in order to reduce the overestimation of denominator probabilities. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 Llogˆpα(i)ˆp(j) + logp(i)ˆpα(j)I (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). Context-based: The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954). A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The ele</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="1309" citStr="Szpektor and Dagan, 2008" startWordPosition="175" endWordPosition="178">ates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>I. Szpektor and I. Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="11818" citStr="Toutanova et al., 2003" startWordPosition="1888" endWordPosition="1891"> E [1, l1] are decoded with DL1, sentences with length l E (l1, l2] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (4). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = 00. Domain adaptation is employed, by creating separate models per domain (training data source). Beyond that, we also create a unified model, trained on all data to be used as a fallback if an appropriate model can not be decided upon during evaluation. 3 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. We evaluated multiple types of preprocessing per unsupervised metric and chose different ones depending on the metric. Word-level semantic similarities, used for soft comparisons and affective feature extraction, were computed over a corpus of 116 million web snippets collected by posing one query for every word in the Aspell spellchecker (asp, ) vocabulary to the Yahoo! search engine. Word-level emotional ratings in continuous valence and arousal scales were pr</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="6331" citStr="Turney, 2006" startWordPosition="950" endWordPosition="952"> right of wi formulate a feature vector vi. For a given value of H the context-based semantic similarity between two words, wi and wj, is computed as the cosine of their feature vectors: QH(i, j) = vi.v; ||v� |v;||. The elements of feature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1. 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity. Non-matched terms from the hypothesis sentence are compared with all terms of the reference sentence (regardless of whether they were matched during the first pass). In the case of bigra</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>P. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sari´c</author>
<author>G Glavas</author>
<author>M Karan</author>
<author>J Snajder</author>
<author>B Dalbelo Basi´c</author>
</authors>
<title>Takelab: Systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proc. SemEval,</booktitle>
<pages>441--448</pages>
<marker>Sari´c, Glavas, Karan, Snajder, Basi´c, 2012</marker>
<rawString>F. &amp;quot;Sari´c, G. Glava&amp;quot;s, M. Karan, J. &amp;quot;Snajder, and B. Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proc. SemEval, pages 441– 448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Zanzotto</author>
<author>M Pennacchiotti</author>
<author>A Moschitti</author>
</authors>
<title>A machine-learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="2234" citStr="Zanzotto et al., 2009" startWordPosition="313" endWordPosition="316">search and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (B¨ar et al., 2012; ˇSari´c et al., 2012</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>F. Zanzotto, M. Pennacchiotti, and A. Moschitti. 2009. A machine-learning approach to textual entailment recognition. Natural Language Engineering, 15(4):551582.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>