<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000246">
<title confidence="0.9874485">
Semeval-2007 Task 02:
Evaluating Word Sense Induction and Discrimination Systems
</title>
<author confidence="0.714339">
Eneko Agirre
</author>
<affiliation confidence="0.670286333333333">
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
</affiliation>
<email confidence="0.992197">
e.agirre@ehu.es
</email>
<author confidence="0.523652">
Aitor Soroa
</author>
<affiliation confidence="0.506769666666667">
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
</affiliation>
<email confidence="0.992681">
a.soroa@ehu.es
</email>
<sectionHeader confidence="0.993716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933928571428">
The goal of this task is to allow for com-
parison across sense-induction and discrim-
ination systems, and also to compare these
systems to other supervised and knowledge-
based systems. In total there were 6 partic-
ipating systems. We reused the SemEval-
2007 English lexical sample subtask of task
17, and set up both clustering-style unsuper-
vised evaluation (using OntoNotes senses as
gold-standard) and a supervised evaluation
(using the part of the dataset for mapping).
We provide a comparison to the results of
the systems participating in the lexical sam-
ple subtask of task 17.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998386333333333">
Word Sense Disambiguation (WSD) is a key
enabling-technology. Supervised WSD techniques
are the best performing in public evaluations, but
need large amounts of hand-tagging data. Exist-
ing hand-annotated corpora like SemCor (Miller
et al., 1993), which is annotated with WordNet
senses (Fellbaum, 1998) allow for a small improve-
ment over the simple most frequent sense heuristic,
as attested in the all-words track of the last Sense-
val competition (Snyder and Palmer, 2004). In the-
ory, larger amounts of training data (SemCor has
approx. 500M words) would improve the perfor-
mance of supervised WSD, but no current project
exists to provide such an expensive resource. An-
other problem of the supervised approach is that the
inventory and distribution of senses changes dra-
matically from one domain to the other, requiring
additional hand-tagging of corpora (Mart´ınez and
Agirre, 2000; Koeling et al., 2005).
Supervised WSD is based on the “fixed-list of
senses” paradigm, where the senses for a target word
are a closed list coming from a dictionary or lex-
icon. Lexicographers and semanticists have long
warned about the problems of such an approach,
where senses are listed separately as discrete enti-
ties, and have argued in favor of more complex rep-
resentations, where, for instance, senses are dense
regions in a continuum (Cruse, 2000).
Unsupervised Word Sense Induction and Dis-
crimination (WSID, also known as corpus-based un-
supervised systems) has followed this line of think-
ing, and tries to induce word senses directly from
the corpus. Typical WSID systems involve cluster-
ing techniques, which group together similar exam-
ples. Given a set of induced clusters (which repre-
sent word uses or senses1), each new occurrence of
the target word will be compared to the clusters and
the most similar cluster will be selected as its sense.
One of the problems of unsupervised systems is
that of managing to do a fair evaluation. Most of cur-
rent unsupervised systems are evaluated in-house,
with a brief comparison to a re-implementation of a
former system, leading to a proliferation of unsuper-
vised systems with little ground to compare among
them. The goal of this task is to allow for compar-
ison across sense-induction and discrimination sys-
tems, and also to compare these systems to other su-
pervised and knowledge-based systems.
The paper is organized as follows. Section 2
presents the evaluation framework used in this task.
Section 3 presents the systems that participated in
</bodyText>
<footnote confidence="0.9993795">
1WSID approaches prefer the term ’word uses’ to ’word
senses’. In this paper we use them interchangeably to refer to
both the induced clusters, and to the word senses from some
reference lexicon.
</footnote>
<page confidence="0.994754">
7
</page>
<bodyText confidence="0.91963875">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 7–12,
Prague, June 2007. c�2007 Association for Computational Linguistics
the task, and the official results. Finally, Section 5
draws the conclusions.
</bodyText>
<sectionHeader confidence="0.957847" genericHeader="method">
2 Evaluating WSID systems
</sectionHeader>
<bodyText confidence="0.998729555555556">
All WSID algorithms need some addition in order
to be evaluated. One alternative is to manually de-
cide the correctness of the clusters assigned to each
occurrence of the words. This approach has two
main disadvantages. First, it is expensive to man-
ually verify each occurrence of the word, and dif-
ferent runs of the algorithm need to be evaluated
in turn. Second, it is not an easy task to manu-
ally decide if an occurrence of a word effectively
corresponds with the use of the word the assigned
cluster refers to, especially considering that the per-
son is given a short list of words linked to the clus-
ter. We also think that instead of judging whether
the cluster returned by the algorithm is correct, the
person should have independently tagged the occur-
rence with his own senses, which should have been
then compared to the cluster returned by the system.
This is paramount to compare a corpus which has
been hand-tagged with some reference senses (also
known as the gold-standard) with the clustering re-
sult. The gold standard tags are taken to be the def-
inition of the classes, and standard measures from
the clustering literature can be used to evaluate the
clusters against the classes.
A second alternative would be to devise a method
to map the clusters returned by the systems to the
senses in a lexicon. Pantel and Lin (2002) automat-
ically map the senses to WordNet, and then mea-
sure the quality of the mapping. More recently, the
mapping has been used to test the system on pub-
licly available benchmarks (Purandare and Peder-
sen, 2004; Niu et al., 2005).
A third alternative is to evaluate the systems ac-
cording to some performance in an application, e.g.
information retrieval (Sch¨utze, 1998). This is a very
attractive idea, but requires expensive system devel-
opment and it is sometimes difficult to separate the
reasons for the good (or bad) performance.
In this task we decided to adopt the first two alter-
natives, since they allow for comparison over pub-
licly available systems of any kind. With this goal on
mind we gave all the participants an unlabeled cor-
pus, and asked them to induce the senses and create
a clustering solution on it. We evaluate the results
according to the following types of evaluation:
</bodyText>
<listItem confidence="0.705804">
1. Evaluate the induced senses as clusters of ex-
</listItem>
<bodyText confidence="0.996491">
amples. The induced clusters are compared to
the sets of examples tagged with the given gold
standard word senses (classes), and evaluated
using the FScore measure for clusters. We will
call this evaluation unsupervised.
</bodyText>
<sectionHeader confidence="0.415873" genericHeader="method">
2. Map the induced senses to gold standard
</sectionHeader>
<bodyText confidence="0.999877125">
senses, and use the mapping to tag the test cor-
pus with gold standard tags. The mapping is
automatically produced by the organizers, and
the resulting results evaluated according to the
usual precision and recall measures for super-
vised word sense disambiguation systems. We
call this evaluation supervised.
We will see each of them in turn.
</bodyText>
<subsectionHeader confidence="0.989116">
2.1 Unsupervised evaluation
</subsectionHeader>
<bodyText confidence="0.999923954545455">
In this setting the results of the systems are treated
as clusters of examples and gold standard senses are
classes. In order to compare the clusters with the
classes, hand annotated corpora is needed. The test
set is first tagged with the induced senses. A per-
fect clustering solution will be the one where each
cluster has exactly the same examples as one of the
classes, and vice versa.
Following standard cluster evaluation prac-
tice (Zhao and Karypis, 2005), we consider the FS-
core measure for measuring the performance of the
systems. The FScore is used in a similar fashion
to Information Retrieval exercises, with precision
and recall defined as the percentage of correctly “re-
trieved” examples for a cluster (divided by total clus-
ter size), and recall as the percentage of correctly
“retrieved” examples for a cluster (divided by total
class size).
Given a particular class sr of size nr and a cluster
hi of size ni, suppose nir examples in the class sr
belong to hi. The F value of this class and cluster is
defined to be:
</bodyText>
<equation confidence="0.6871005">
2P (sr, hi)R(sr, hi)
f(sr, hi) = P(sr, hi) + R(sr, hi)
</equation>
<bodyText confidence="0.9959826">
where P(sr, hi) = n��
n� is the precision value and
R(sr, hi) = n��n� is the recall value defined for class
sr and cluster hi. The FScore of class sr is the max-
imum F value attained at any cluster, that is,
</bodyText>
<page confidence="0.786624">
8
</page>
<equation confidence="0.914979">
F(sr) = max f(sr, hi)
hi
</equation>
<bodyText confidence="0.999337529411765">
and the FScore of the entire clustering solution is:
where q is the number of classes and n is the size
of the clustering solution. If the clustering is the
identical to the original classes in the datasets, FS-
core will be equal to one which means that the higher
the FScore, the better the clustering is.
For the sake of completeness we also include the
standard entropy and purity measures in the unsu-
pervised evaluation. The entropy measure consid-
ers how the various classes of objects are distributed
within each cluster. In general, the smaller the en-
tropy value, the better the clustering algorithm per-
forms. The purity measure considers the extent to
which each cluster contained objects from primarily
one class. The larger the values of purity, the bet-
ter the clustering algorithm performs. For a formal
definition refer to (Zhao and Karypis, 2005).
</bodyText>
<subsectionHeader confidence="0.99923">
2.2 Supervised evaluation
</subsectionHeader>
<bodyText confidence="0.999913318181818">
We have followed the supervised evaluation frame-
work for evaluating WSID systems as described in
(Agirre et al., 2006). First, we split the corpus into
a train/test part. Using the hand-annotated sense in-
formation in the train part, we compute a mapping
matrix M that relates clusters and senses in the fol-
lowing way. Suppose there are m clusters and n
senses for the target word. Then, M = {mij} 1 &lt;
i &lt; m,1 &lt; j &lt; n, and each mij = P(sj|hi), that
is, mij is the probability of a word having sense j
given that it has been assigned cluster i. This proba-
bility can be computed counting the times an occur-
rence with sense sj has been assigned cluster hi in
the train corpus.
The mapping matrix is used to transform any
cluster score vector h = (h1,... , hm) returned by
the WSID algorithm into a sense score vector s� =
(s1, ... , sn). It suffices to multiply the score vector
by M, i.e., s� = hM.
We use the M mapping matrix in order to convert
the cluster score vector of each test corpus instance
into a sense score vector, and assign the sense with
</bodyText>
<table confidence="0.99924525">
All Nouns Verbs
train 22281 14746 9773
test 4851 2903 2427
all 27132 17649 12200
</table>
<tableCaption confidence="0.9495155">
Table 1: Number of occurrences for the 100 target words in
the corpus following the train/test split.
</tableCaption>
<bodyText confidence="0.9983325">
maximum score to that instance. Finally, the result-
ing test corpus is evaluated according to the usual
precision and recall measures for supervised word
sense disambiguation systems.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.99993125">
In this section we will introduce the gold standard
and corpus used, the description of the systems and
the results obtained. Finally we provide some mate-
rial for discussion.
</bodyText>
<subsectionHeader confidence="0.741367">
Gold Standard
</subsectionHeader>
<bodyText confidence="0.999927411764706">
The data used for the actual evaluation was bor-
rowed from the SemEval-2007 “English lexical
sample subtask” of task 17. The texts come from the
Wall Street Journal corpus, and were hand-annotated
with OntoNotes senses (Hovy et al., 2006). Note
that OntoNotes senses are coarser than WordNet
senses, and thus the number of senses to be induced
is smaller in this case.
Participants were provided with information
about 100 target words (65 verbs and 35 nouns),
each target word having a set of contexts where the
word appears. After removing the sense tags from
the train corpus, the train and test parts were joined
into the official corpus and given to the participants.
Participants had to tag with the induced senses all
the examples in this corpus. Table 1 summarizes the
size of the corpus.
</bodyText>
<subsectionHeader confidence="0.889972">
Participant systems
</subsectionHeader>
<bodyText confidence="0.999166545454545">
In total there were 6 participant systems. One of
them (UoFL) was not a sense induction system, but
rather a knowledge-based WSD system. We include
their data in the results section below for coherence
with the official results submitted to participants, but
we will not mention it here.
I2R: This team used a cluster validation method
to estimate the number of senses of a target word in
untagged data, and then grouped the instances of this
target word into the estimated number of clusters us-
ing the sequential Information Bottleneck algorithm.
</bodyText>
<equation confidence="0.910426333333333">
nr
FScore =
F(sr)
n
�c
r=1
</equation>
<page confidence="0.915244">
9
</page>
<bodyText confidence="0.994384611111111">
UBC-AS: A two stage graph-based clustering
where a co-occurrence graph is used to compute
similarities against contexts. The context similarity
matrix is pruned and the resulting associated graph
is clustered by means of a random-walk type al-
gorithm. The parameters of the system are tuned
against the Senseval-3 lexical sample dataset, and
some manual tuning is performed in order to reduce
the overall number of induced senses. Note that this
system was submitted by the organizers. The orga-
nizers took great care in order to participate under
the same conditions as the rest of participants.
UMND2: A system which clusters the second or-
der co-occurrence vectors associated with each word
in a context. Clustering is done using k-means and
the number of clusters was automatically discovered
using the Adapted Gap Statistic. No parameter tun-
ing is performed.
upv si: A self-term expansion method based on
co-ocurrence, where the terms of the corpus are ex-
panded by its best co-ocurrence terms in the same
corpus. The clustering is done using one implemen-
tation of the KStar method where the stop criterion
has been modified. The trial data was used for de-
termining the corpus structure. No further tuning is
performed.
UOY: A graph based system which creates a co-
occurrence hypergraph model. The hypergraph is
filtered and weighted according to some associa-
tion rules. The clustering is performed by selecting
the nodes of higher degree until a stop criterion is
reached. WSD is performed by assigning to each in-
duced cluster a score equal to the sum of weights of
hyperedges found in the local context of the target
word. The system was tested and tuned on 10 nouns
of Senseval-3 lexical-sample.
</bodyText>
<sectionHeader confidence="0.666209" genericHeader="method">
Official Results
</sectionHeader>
<bodyText confidence="0.9996756">
Participants were required to induce the senses of
the target words and cluster all target word contexts
accordingly2. Table 2 summarizes the average num-
ber of induced senses as well as the real senses in
the gold standard.
</bodyText>
<footnote confidence="0.770943">
2They were allowed to label each context with a weighted
score vector, assigning a weight to each induced sense. In the
unsupervised evaluation only the sense with maximum weight
was considered, but for the supervised one the whole score vec-
tor was used. However, none of the participating systems la-
beled any instance with more than one sense.
</footnote>
<table confidence="0.9998765">
system All nouns verbs
I2R 3.08 3.11 3.06
UBC-AS* 1.32 1.63 1.15
UMND2 1.36 1.71 1.17
upv si 5.57 7.2 4.69
UOY 9.28 11.28 8.2
Gold standard
test 2.87 2.86 2.86
train 3.6 3.91 3.43
all 3.68 3.94 3.54
</table>
<tableCaption confidence="0.990887">
Table 2: Average number of clusters as returned by the par-
ticipants, and number of classes in the gold standard. Note that
UBC-AS* is the system submitted by the organizers of the task.
</tableCaption>
<table confidence="0.999503636363636">
System R. FSc. All Entr. Nouns Verbs
Pur. FSc. FSc.
1c1word 1 78.9 79.8 45.4 80.7 76.8
UBC-AS* 2 78.7 80.5 43.8 80.8 76.3
upv si 3 66.3 83.8 33.2 69.9 62.2
UMND2 4 66.1 81.7 40.5 67.1 65.0
I2R 5 63.9 84.0 32.8 68.0 59.3
UoJL** 6 61.5 82.2 37.8 62.3 60.5
UOY 7 56.1 86.1 27.1 65.8 45.1
Random 8 37.9 86.1 27.7 38.1 37.7
1c1inst 9 9.5 100 0 6.6 12.7
</table>
<tableCaption confidence="0.979296">
Table 3: Unsupervised evaluation on the test corpus (FScore),
including 3 baselines. Purity and entropy are also provided.
UBC-AS* was submitted by the organizers. UoJL** is not a
sense induction system.
</tableCaption>
<table confidence="0.999956666666666">
System Rank Supervised evaluation
All Nouns Verbs
I2R 1 81.6 86.8 75.7
UMND2 2 80.6 84.5 76.2
upv si 3 79.1 82.5 75.3
MFS 4 78.7 80.9 76.2
UBC-AS* 5 78.5 80.7 76.0
UOY 6 77.7 81.6 73.3
UoJL** 7 77.1 80.5 73.3
</table>
<tableCaption confidence="0.985556">
Table 4: Supervised evaluation as recall. UBC-AS* was sub-
mitted by the organizers. UoJL** is not a sense induction sys-
tem.
</tableCaption>
<bodyText confidence="0.9979512">
Table 3 shows the unsupervised evaluation of
the systems on the test corpus. We also include
three baselines: the “one cluster per word” baseline
(1c1word), which groups all instances of a word into
a single cluster, the “one cluster per instance” base-
line (1c1inst), where each instance is a distinct clus-
ter, and a random baseline, where the induced word
senses and their associated weights have been ran-
domly produced. The random baseline figures in this
paper are averages over 10 runs.
</bodyText>
<footnote confidence="0.628821">
As shown in Table 3, no system outperforms the
1c1word baseline, which indicates that this baseline
</footnote>
<page confidence="0.998255">
10
</page>
<bodyText confidence="0.999952107142857">
is quite strong, perhaps due the relatively small num-
ber of classes in the gold standard. However, all
systems outperform by far the random and 1c1inst
baselines, meaning that the systems are able to in-
duce correct senses. Note that the purity and entropy
measures are not very indicative in this setting. For
completeness, we also computed the FScore using
the complete corpus (both train and test). The re-
sults are similar and the ranking is the same. We
omit them for brevity.
The results of the supervised evaluation can be
seen in Table 4. The evaluation is also performed
over the test corpus. Apart from participants, we
also show the most frequent sense (MFS), which
tags every test instance with the sense that occurred
most often in the training part. Note that the su-
pervised evaluation combines the information in the
clustering solution implicitly with the MFS infor-
mation via the mapping in the training part. Pre-
vious Senseval evaluation exercises have shown that
the MFS baseline is very hard to beat by unsuper-
vised systems. In fact, only three of the participant
systems are above the MFS baseline, which shows
that the clustering information carries over the map-
ping successfully for these systems. Note that the
1c1word baseline is equivalent to MFS in this set-
ting. We will review the random baseline in the dis-
cussion section below.
</bodyText>
<subsectionHeader confidence="0.543044">
Further Results
</subsectionHeader>
<bodyText confidence="0.999722444444445">
Table 5 shows the results of the best systems from
the lexical sample subtask of task 17. The best sense
induction system is only 6.9 percentage points below
the best supervised, and 3.5 percentage points be-
low the best (and only) semi-supervised system. If
the sense induction system had participated, it would
be deemed as semi-supervised, as it uses, albeit in a
shallow way, the training data for mapping the clus-
ters into senses. In this sense, our supervised evalu-
ation does not seek to optimize the available training
data.
After the official evaluation, we realized that con-
trary to previous lexical sample evaluation exercises
task 17 organizers did not follow a random train/test
split. We decided to produce a random train/test
split following the same 82/18 proportion as the of-
ficial split, and re-evaluated the systems. The results
are presented in Table 6, where we can see that all
</bodyText>
<table confidence="0.998479333333333">
System Supervised evaluation
best supervised 88.7
best semi-supervised 85.1
best induction (semi-sup.) 81.6
MFS 78.7
best unsupervised 53.8
</table>
<tableCaption confidence="0.91135">
Table 5: Comparing the best induction system in this task with
those of task 17.
</tableCaption>
<table confidence="0.999552428571429">
System Supervised evaluation
I2R 82.2
UOY 81.3
UMND2 80.1
upv si 79.9
UBC-AS 79.0
MFS 78.4
</table>
<tableCaption confidence="0.99105">
Table 6: Supervised evaluation as recall using a random
train/test split.
</tableCaption>
<bodyText confidence="0.9992124375">
participants are above the MFS baseline, showing
that all of them learned useful clustering informa-
tion. Note that UOY was specially affected by the
original split. The distribution of senses in this split
did not vary (cf. Table 2).
Finally, we also studied the supervised evalua-
tion of several random clustering algorithms, which
can attain performances close to MFS, thanks to the
mapping information. This is due to the fact that the
random clusters would be mapped to the most fre-
quent senses. Table 7 shows the results of random
solutions using varying numbers of clusters (e.g.
random2 is a random choice between two clusters).
Random2 is only 0.1 below MFS, but as the number
of clusters increases some clusters don’t get mapped,
and the recall of the random baselines decrease.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999947769230769">
The evaluation of clustering solutions is not straight-
forward. All measures have some bias towards cer-
tain clustering strategy, and this is one of the reasons
of adding the supervised evaluation as a complemen-
tary information to the more standard unsupervised
evaluation.
In our case, we noticed that the FScore penal-
ized the systems with a high number of clusters,
and favored those that induce less senses. Given
the fact that FScore tries to balance precision (higher
for large numbers of clusters) and recall (higher for
small numbers of clusters), this was not expected.
We were also surprised to see that no system could
</bodyText>
<page confidence="0.995977">
11
</page>
<table confidence="0.9684846">
System Supervised evaluation
random2 78.6
random10 77.6
ramdom100 64.2
random1000 31.8
</table>
<tableCaption confidence="0.999677">
Table 7: Supervised evaluation of several random baselines.
</tableCaption>
<bodyText confidence="0.999938555555556">
beat the “one cluster one word” baseline. An expla-
nation might lay in that the gold-standard was based
on the coarse-grained OntoNotes senses. We also
noticed that some words had hundreds of instances
and only a single sense. We suspect that the partic-
ipating systems would have beaten all baselines if a
fine-grained sense inventory like WordNet had been
used, as was customary in previous WSD evaluation
exercises.
Supervised evaluation seems to be more neutral
regarding the number of clusters, as the ranking of
systems according to this measure include diverse
cluster averages. Each of the induced clusters is
mapped into a weighted vector of senses, and thus
inducing a number of clusters similar to the number
of senses is not a requirement for good results. With
this measure some of the systems3 are able to beat
all baselines.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999013">
We have presented the design and results of the
SemEval-2007 task 02 on evaluating word sense in-
duction and discrimination systems. 6 systems par-
ticipated, but one of them was not a sense induc-
tion system. We reused the data from the SemEval-
2007 English lexical sample subtask of task 17, and
set up both clustering-style unsupervised evaluation
(using OntoNotes senses as gold-standard) and a su-
pervised evaluation (using the training part of the
dataset for mapping). We also provide a compari-
son to the results of the systems participating in the
lexical sample subtask of task 17.
Evaluating clustering solutions is not straightfor-
ward. The unsupervised evaluation seems to be
sensitive to the number of senses in the gold stan-
dard, and the coarse grained sense inventory used
in the gold standard had a great impact in the re-
sults. The supervised evaluation introduces a map-
ping step which interacts with the clustering solu-
tion. In fact, the ranking of the participating systems
</bodyText>
<footnote confidence="0.543288">
3All systems in the case of a random train/test split
</footnote>
<bodyText confidence="0.999972">
varies according to the evaluation method used. We
think the two evaluation results should be taken to be
complementary regarding the information learned
by the clustering systems, and that the evaluation
of word sense induction and discrimination systems
needs further developments, perhaps linked to a cer-
tain application or purpose.
</bodyText>
<sectionHeader confidence="0.998279" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998481166666667">
We want too thank the organizers of SemEval-2007 task 17 for
kindly letting us use their corpus. We are also grateful to Ted
Pedersen for his comments on the evaluation results. This work
has been partially funded by the Spanish education ministry
(project KNOW) and by the regional government of Gipuzkoa
(project DAHAD).
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935888888889">
E. Agirre, D. Martfnez, O. L´opez de Lacalle, and
A. Soroa. 2006. Evaluating and optimizing the param-
eters of an unsupervised graph-based wsd algorithm.
In Proceedings of the NAACL TextGraphs workshop,
pages 89–96, New York City, June.
D. A. Cruse, 2000. Polysemy: Theoretical and Com-
putational Approaches, chapter Aspects of the Micro-
structure of Word Meanings, pages 31–51. OUP.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of HLT/NAACL.
R. Koeling, D. McCarthy, and J.D. Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition.
D. Martfnez and E. Agirre. 2000. One sense per colloca-
tion and genre/topic variations.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A semantic concordance. In Proc. of the ARPA HLT
workshop.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word
independent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL-2004, pages 41–48.
H. Sch¨utze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97–123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
Y Zhao and G Karypis. 2005. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141–168.
</reference>
<page confidence="0.998461">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336522">
<title confidence="0.9464085">Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems</title>
<author confidence="0.93683">Eneko Agirre</author>
<affiliation confidence="0.9994125">IXA NLP Group Univ. of the Basque Country</affiliation>
<address confidence="0.809758">Donostia, Basque Country</address>
<email confidence="0.913913">e.agirre@ehu.es</email>
<author confidence="0.993579">Aitor Soroa</author>
<affiliation confidence="0.999624">IXA NLP Group Univ. of the Basque Country</affiliation>
<address confidence="0.809414">Donostia, Basque Country</address>
<email confidence="0.920538">a.soroa@ehu.es</email>
<abstract confidence="0.979328733333333">The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledgebased systems. In total there were 6 participating systems. We reused the SemEval- 2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martfnez</author>
<author>O L´opez de Lacalle</author>
<author>A Soroa</author>
</authors>
<title>Evaluating and optimizing the parameters of an unsupervised graph-based wsd algorithm.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL TextGraphs workshop,</booktitle>
<pages>89--96</pages>
<location>New York City,</location>
<marker>Agirre, Martfnez, de Lacalle, Soroa, 2006</marker>
<rawString>E. Agirre, D. Martfnez, O. L´opez de Lacalle, and A. Soroa. 2006. Evaluating and optimizing the parameters of an unsupervised graph-based wsd algorithm. In Proceedings of the NAACL TextGraphs workshop, pages 89–96, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Polysemy: Theoretical and Computational Approaches, chapter Aspects of the Microstructure of Word Meanings,</title>
<date>2000</date>
<journal>OUP. C. Fellbaum.</journal>
<pages>31--51</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2213" citStr="Cruse, 2000" startWordPosition="347" endWordPosition="348">tory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart´ınez and Agirre, 2000; Koeling et al., 2005). Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000). Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce word senses directly from the corpus. Typical WSID systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which represent word uses or senses1), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. One of the problems of unsupervised systems is that of managing to do a fair evaluation. Most of cur</context>
</contexts>
<marker>Cruse, 2000</marker>
<rawString>D. A. Cruse, 2000. Polysemy: Theoretical and Computational Approaches, chapter Aspects of the Microstructure of Word Meanings, pages 31–51. OUP. C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="10794" citStr="Hovy et al., 2006" startWordPosition="1824" endWordPosition="1827">aximum score to that instance. Finally, the resulting test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems. 3 Results In this section we will introduce the gold standard and corpus used, the description of the systems and the results obtained. Finally we provide some material for discussion. Gold Standard The data used for the actual evaluation was borrowed from the SemEval-2007 “English lexical sample subtask” of task 17. The texts come from the Wall Street Journal corpus, and were hand-annotated with OntoNotes senses (Hovy et al., 2006). Note that OntoNotes senses are coarser than WordNet senses, and thus the number of senses to be induced is smaller in this case. Participants were provided with information about 100 target words (65 verbs and 35 nouns), each target word having a set of contexts where the word appears. After removing the sense tags from the train corpus, the train and test parts were joined into the official corpus and given to the participants. Participants had to tag with the induced senses all the examples in this corpus. Table 1 summarizes the size of the corpus. Participant systems In total there were 6</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Koeling</author>
<author>D McCarthy</author>
<author>J D Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<contexts>
<context position="1779" citStr="Koeling et al., 2005" startWordPosition="273" endWordPosition="276"> (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart´ınez and Agirre, 2000; Koeling et al., 2005). Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000). Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>R. Koeling, D. McCarthy, and J.D. Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martfnez</author>
<author>E Agirre</author>
</authors>
<title>One sense per collocation and genre/topic variations.</title>
<date>2000</date>
<marker>Martfnez, Agirre, 2000</marker>
<rawString>D. Martfnez and E. Agirre. 2000. One sense per collocation and genre/topic variations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>R Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proc. of the ARPA HLT workshop.</booktitle>
<contexts>
<context position="1118" citStr="Miller et al., 1993" startWordPosition="167" endWordPosition="170">systems. We reused the SemEval2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging </context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993. A semantic concordance. In Proc. of the ARPA HLT workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Niu</author>
<author>W Li</author>
<author>R K Srihari</author>
<author>H Li</author>
</authors>
<title>Word independent context pair classification model for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL-2005.</booktitle>
<contexts>
<context position="5388" citStr="Niu et al., 2005" startWordPosition="878" endWordPosition="881">ses (also known as the gold-standard) with the clustering result. The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Sch¨utze, 1998). This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance. In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind. With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solution on it. We eval</context>
</contexts>
<marker>Niu, Li, Srihari, Li, 2005</marker>
<rawString>C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word independent context pair classification model for word sense disambiguation. In Proc. of CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proc. of KDD02.</booktitle>
<contexts>
<context position="5160" citStr="Pantel and Lin (2002)" startWordPosition="837" endWordPosition="840">hould have independently tagged the occurrence with his own senses, which should have been then compared to the cluster returned by the system. This is paramount to compare a corpus which has been hand-tagged with some reference senses (also known as the gold-standard) with the clustering result. The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Sch¨utze, 1998). This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance. In this task we decided to adopt the first two alternatives, since</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In Proc. of KDD02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Purandare</author>
<author>T Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proc. of CoNLL-2004,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="5369" citStr="Purandare and Pedersen, 2004" startWordPosition="873" endWordPosition="877">tagged with some reference senses (also known as the gold-standard) with the clustering result. The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Sch¨utze, 1998). This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance. In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind. With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solu</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>A. Purandare and T. Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proc. of CoNLL-2004, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>M Palmer</author>
</authors>
<title>The english all-words task.</title>
<date>2004</date>
<booktitle>In Proc. of SENSEVAL.</booktitle>
<contexts>
<context position="1346" citStr="Snyder and Palmer, 2004" startWordPosition="204" endWordPosition="207">the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart´ınez and Agirre, 2000; Koeling et al., 2005). Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicograph</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>B. Snyder and M. Palmer. 2004. The english all-words task. In Proc. of SENSEVAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Hierarchical clustering algorithms for document datasets.</title>
<date>2005</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="7196" citStr="Zhao and Karypis, 2005" startWordPosition="1179" endWordPosition="1182">call measures for supervised word sense disambiguation systems. We call this evaluation supervised. We will see each of them in turn. 2.1 Unsupervised evaluation In this setting the results of the systems are treated as clusters of examples and gold standard senses are classes. In order to compare the clusters with the classes, hand annotated corpora is needed. The test set is first tagged with the induced senses. A perfect clustering solution will be the one where each cluster has exactly the same examples as one of the classes, and vice versa. Following standard cluster evaluation practice (Zhao and Karypis, 2005), we consider the FScore measure for measuring the performance of the systems. The FScore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly “retrieved” examples for a cluster (divided by total cluster size), and recall as the percentage of correctly “retrieved” examples for a cluster (divided by total class size). Given a particular class sr of size nr and a cluster hi of size ni, suppose nir examples in the class sr belong to hi. The F value of this class and cluster is defined to be: 2P (sr, hi)R(sr, hi) f(sr, hi</context>
<context position="8913" citStr="Zhao and Karypis, 2005" startWordPosition="1488" endWordPosition="1491">e which means that the higher the FScore, the better the clustering is. For the sake of completeness we also include the standard entropy and purity measures in the unsupervised evaluation. The entropy measure considers how the various classes of objects are distributed within each cluster. In general, the smaller the entropy value, the better the clustering algorithm performs. The purity measure considers the extent to which each cluster contained objects from primarily one class. The larger the values of purity, the better the clustering algorithm performs. For a formal definition refer to (Zhao and Karypis, 2005). 2.2 Supervised evaluation We have followed the supervised evaluation framework for evaluating WSID systems as described in (Agirre et al., 2006). First, we split the corpus into a train/test part. Using the hand-annotated sense information in the train part, we compute a mapping matrix M that relates clusters and senses in the following way. Suppose there are m clusters and n senses for the target word. Then, M = {mij} 1 &lt; i &lt; m,1 &lt; j &lt; n, and each mij = P(sj|hi), that is, mij is the probability of a word having sense j given that it has been assigned cluster i. This probability can be compu</context>
</contexts>
<marker>Zhao, Karypis, 2005</marker>
<rawString>Y Zhao and G Karypis. 2005. Hierarchical clustering algorithms for document datasets. Data Mining and Knowledge Discovery, 10(2):141–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>