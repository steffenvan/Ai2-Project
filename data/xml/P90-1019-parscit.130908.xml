<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<sectionHeader confidence="0.91037725" genericHeader="method">
ACQUIRING CORE MEANINGS OF WORDS, REPRESENTED AS
JACKENDOFF-STYLE CONCEPTUAL STRUCTURES, FROM
CORRELATED STREAMS OF LINGUISTIC AND NON-LINGUISTIC
INPUT
</sectionHeader>
<address confidence="0.6711208">
Jeffrey Mark Siskincl*
M. I. T. Artificial Intelligence Laboratory
545 Technology Square, Room NE43-800b
Cambridge MA 02139
617/253-5659
</address>
<email confidence="0.487556">
internet: QobinALMIT.EDU
</email>
<sectionHeader confidence="0.974587" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.999978761904762">
This paper describes an operational system which
can acquire the core meanings of words without any
prior knowledge of either the category or meaning
of any words it encounters. The system is given
as input, a description of sequences of scenes along
with sentences which describe the [EVENTS] taking
place as those scenes unfold, and produces as out-
put, a lexicon consisting of the category and mean-
ing of each word in the input, that allows the sen-
tences to describe the [EVENTS]. It is argued, that
each of the three main components of the system, the
parser, the linker and the inference component, make
only linguistically and cognitively plausible assump-
tions about the innate knowledge needed to support
tractable learning. The paper discusses the theory
underlying the system, the representations and al-
gorithms used in the implementation, the semantic
constraints which support the heuristics necessary
to achieve tractable learning, the limitations of the
current theory and the implications of this work for
language acquisition research.
</bodyText>
<sectionHeader confidence="0.99941" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992764">
Several natural language systems have been reported
which learn the meanings of new words[5, 7, 1, 16,
17, 13, 14]. Many of these systems (in particular
[5, 7, 1]) learn the new meanings based upon expec-
tations arising from the morphological, syntactic, se-
</bodyText>
<note confidence="0.618972333333333">
&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholar-
ship. Part of this research was performed while the author was
visiting Xerox PARC as a research intern and as a consultant.
</note>
<bodyText confidence="0.999945911764706">
mantic and pragmatic context of the unknown word
in the text being processed. For example, if such a
system encounters the sentence &amp;quot;I woke up yesterday,
turned off my alarm clock, took a shower, and cooked
myself two grimps for breakfast[51&amp;quot; it might conclude
that grimps is a noun which represents a type of
food. Such systems succeed in learning new words
only when the context offers sufficient constraint to
narrow down the possible meanings to make the ac-
quisition unambiguous. Accordingly, such a theory
accounts only for the type of learning which arises
when an adult encounters an unknown word while
reading a text comprised mostly of known words. It
can not explain the kind of learning which a young
child performs during the early stages of language
acquisition when it starts out knowing the meanings
of few if any words.
In this paper, I present a new theory which can
account for the language learning which a child ex-
hibits. In this theory, the learner is presented with
a training session consisting of a sequence of sce-
narios. Each scenario contains both linguistic and
non-linguistic (i.e. visual) information. The non-
linguistic information for each scenario consists of
a time-ordered sequence of scenes, each depicted via
a conjunction of true and negated atomic formulas
describing that scene. Likewise, the linguistic infor-
mation for each scenario consists of a time-ordered
sequence of sentences. Initially, the learner knows
nothing about the words comprising the sentences in
the training session, neither their lexical category nor
their meaning. From the two correlated sources of in-
put, the linguistic and the non-linguistic, the learner
can infer the set of possible lexicons (i.e. the possible
</bodyText>
<page confidence="0.998424">
143
</page>
<bodyText confidence="0.999908875">
categories and meanings of the words in the linguistic
input) which allow the linguistic input to describe or
account for the non-linguistic input. This inference
is accomplished by applying a compositional seman-
tics linking rule in reverse and then performing some
constraint satisfaction.
This theory has been implemented in a working
computer program. The program succeeds and is
tractable because of a small number of judicious se-
mantic constraints and a small number of heuristics
which order and eliminate much of the search. This
paper explains the general theory as well as the im-
plementation details which make it work. In ad-
dition, it discusses some limitations in the current
theory, among which is one which prevents it from
converging on a single definition of some words.
</bodyText>
<sectionHeader confidence="0.986057" genericHeader="method">
2 Background
</sectionHeader>
<bodyText confidence="0.988308833333333">
In (15], Rayner et. al. describe a system which
can determine the lexical category of each word
in a corpus of sentences. They observe that
while in the original formulation, a definite clause
grammar[12] normally defines a two-argument pred-
icate parser(Sent ence ,Tree) with the lexicon rep-
resented directly in the clauses of the grammar, an
alternative formulation would allow the lexicon to be
represented explicitly as an additional argument to
the parser relation, yielding a three argument predi-
cate par sex(Sent enc Tree , Lex icon) . This three
argument relation can be used to learn lexical cate-
gory information by a technique summarized in Fig-
ure I. Here, a query is formed containing a conjunc-
tion of calls to the parser, one for each sentence in
the corpus. All of the calls share a common Lexicon,
while in each call, the Tree is left unbound. The
Lexicon is initialized with an entry for each word
appearing in the corpus where the lexical category
of each such initial entry is left unbound. The pur-
pose of this initial lexicon is to enforce the constraint
that each word in the corpus be assigned a unique
lexical category. This restriction, the monoserray con-
straint, will play an important role in the work we
describe later. The result of issuing the query in the
above example is a lexicon, with instantiated lexical
categories for each lexical entry, such that with that
lexicon, all of the words in the corpus can be parsed.
Note that there could be several such lexicons, each
produced by backtracking.
In this paper we extend the results of Rayner et.
at. to the learning of representations of word mean-
ings in addition to lexical category information. Our
theory is implemented in an operational computer
program called MA IMRA .1 Unlike Rayner et. al.&apos;s
system, which is given only a corpus of sentences as
input, MAIMRA is given two correlated streams of
input, one linguistic and one non-linguistic, the later
modeling the visual context in which the former were
uttered. This is intended to more closely model the
kind of learning exhibited by a child with no prior
lexical knowledge. The task faced by MAIMRA is il-
lustrated in Figure 2.
MAIMRA does not attempt to solve the perception
problem; both the linguistic and non-linguistic input
are presented in symbolic form to MAIMRA. Thus,
the session given in Figure 2 would be presented to
MAIMRA as the following two input pairs:
(BE(cup, ATP olm))A
-&apos;BE(cup, AT(Mary)));
(BE(cup, AT(Mary)) A
-&apos;BE(cup, AT(John)))
The cup slid from John to Mary.
(BE(cup, AT(Mary))A
-BE(cup, AT( Bill))) ;
(BE(cup, AT(Bill))A
-&apos;BE(cup, AT(Mary)))
The cup slid from Mary to Bill.
MAIMRA attempts to infer both category and mean-
ing information from input such as this.
</bodyText>
<sectionHeader confidence="0.990499" genericHeader="method">
3 Architecture
</sectionHeader>
<bodyText confidence="0.997671454545455">
MAIMRA operates as a collection of modules which
mutually constrain various mental representations.
The organization of these modules is illustrated in
Figure 3. Conceptually, each of the modules is non-
directional; each module simply constrains the val-
ues which may appear concurrently on each of its
inputs. Thus the parser enforces a relation between
a time-ordered sequence of sentences and a corre-
sponding time-ordered sequence of syntactic struc-
tures or parse trees which are licensed by the lexi-
cal category information from a lexicon. The linker
imposes compositional semantics on the parse trees
produced by the parser, relating the meanings of in-
dividual words found in the lexicon, to the meanings
of entire utterances, through the mediation of the
syntactic structures consistent with the parser. Fi-
nally, the inference component relates a time-ordered
sequence of observations from the non-linguistic in-
put, to a time-ordered sequence of semantic struc-
tures which in some sense explain the non-linguistic
input. The non-directional collection of modules can
&apos;MAIMRA, or Ninnn, is the Aramaic word for word.
</bodyText>
<page confidence="0.997566">
144
</page>
<table confidence="0.986243636363637">
?- Lexicon = Eentry(the,_), Lexicon = (entry(the,det),
entry (cap,_), entry(cup,n),
entry (slid,_), entry(slid,v),
entry (from,_), entry(from,p),
entry(john,J, entry(john,n),
entry(to,_), entry(to,p),
entry(mary,_), entry(mary,n),
entry(bill,_)], entry(bill,n)].
pareer(Cthe,cup,slid,from,john,to,mary),_,Lexicon),
pareer(tthe,cup,slid,from,mary,to,bill],_,Lexicon),
parser( [the,cup,elid,from,bill,to,john],_,Lexicon).
</table>
<figureCaption confidence="0.998532">
Figure 1: The technique used by Rayner et. al. in [15] to acquire lexical category information from a corpus
of sentences.
</figureCaption>
<figure confidence="0.9950295">
Input:
Output:
The: DET
clip: N [Thing cup]
slid: V [Event GO(x, [path y, z])]
from: P [Path FROMaphiee AT(x)])]
to : P [Path TOUPiace AT(x)1)]
John: N [Thing John]
Mary: N [Thing Mary]
Bill: N [Thing Bill]
</figure>
<figureCaption confidence="0.998622">
Figure 2: A sample learning session with MAIMRA. MAIMRA is given the two scenarios as input. Each sce-
</figureCaption>
<bodyText confidence="0.99062325">
nario comprises linguistic information, in the form of a sequence of sentences, and non-linguistic information.
The non-linguistic information is a sequence of conceptual structure [STATE] descriptions which describe a
sequence of visual scenes. MAIMRA produces as output, a lexicon which allows the linguistic input to explain
the non-linguistic input.
</bodyText>
<page confidence="0.996398">
145
</page>
<figure confidence="0.623687">
minierces
</figure>
<figureCaption confidence="0.99944">
Figure 3: The cognitive architecture used by
</figureCaption>
<subsectionHeader confidence="0.521009">
MAIMRA.
</subsectionHeader>
<bodyText confidence="0.999933117021277">
be used in three ways. Given a lexicon and a se-
quence of sentences as input, the architecture could
produce as output, a sequence of observations which
are predicted by the sentences. This corresponds to
language understanding. Likewise, given a lexicon
and a sequence of observations as input, the archi-
tecture could produce as output, a sequence of sen-
tences which explain the observations. This corre-
sponds to language generation. Finally, given a se-
quence of observations and a sequence of sentences
as input, the architecture could produce as output,
a lexicon which allows the sentences to explain the
observations. This last alternative, corresponding to
language acquisition, is what interests us here.
Of the five mental representations used by
MAIMRA, only three are externally visible, namely
the linguistic input, the non-linguistic input and the
lexicon. Syntactic and semantic structures exist only
internal to MAIMRA and are not externally visible.
When using the cognitive architecture from Figure 3
for learning, the values of two of the mental rep-
resentations, namely the sentences and the observa-
tions, are deterministic, since they are fixed as input.
The remaining three representations may be nonde-
terministic; there may be multiple lexicons, syntac-
tic structure sequences and semantic structure se-
quences which are consistent with the fixed input.
In general, each of the three modules alone provides
only limited constraint on the possible values for each
of the mental representations. Thus taken alone, sig-
nificant nondeterminism is introduced by each mod-
ule in isolation. Taken together however, the mod-
ules offer much greater constraint on the mutually
consistent values for the mental representations, thus
reducing the amount of nondeterminism. Much of
the success of MAIMRA hinges on efficient ways of
representing this nondeterminism.
Conceptually, MAIMRA could have been imple-
mented using techniques similar to Rayner et. al.&apos;s
system. Such a naive implementation would directly
reflect the architecture given in Figure 3 and is il-
lustrated in Figure 4. The predicate maimra would
represent the conjunction of constraints introduced
by the parser, linker and inference modules, ul-
timately constraining the mutually consistent val-
ues for sentence and observation sequences and the
lexicon. Learning a lexicon would be accomplished
by forming a conjunction of queries to maimra,
one for each scenario, where a single Lexicon is
shared among the conjoined queries. This lexi-
con is a list of lexical entries, each of the form
entry(Word,Category,Meaning). The monosemy
constraint is enforced by initializing the Lexicon to
contain a single entry for each word, each entry hav-
ing unbound Category and Meaning slots. The re-
sult of processing such a query would be bindings for
those Category and Meaning slots which allow the
Sentences to explain the Observations.
The naive implementation is too inefficient to be
practical. This inefficiency results from two sources:
inefficient representation of nondeterministic values
and non-directional computation. Nondeterministic
mental representations are expressed in the naive im-
plementation via backtracking. Expressing nonde-
terminism this way requires that substructure shared
across different alternatives for a mental representa,
tion be multiplied out. For example, if MAIMRA is
given as input, a sequence of two sentences Si; .92,
where the first sentence has n parses and the sec-
ond m parses, then there would be m x n distinct
values for the parse tree sequence produced by the
parser for this sentence sequence. Each such parse
tree sequence would be represented as a distinct
backtrack possibility by the naive implementation.
The actual implementation instead represents this
nondeterminism explicitly as AND/OR trees and ad-
ditionally factors out much of the shared common
substructure to reduce the size of the mental rep-
resentations and the time needed to process them.
As noted previously, the individual modules them-
selves offer little constraint on the mental represen-
tations. A given sentence sequence corresponds to
many parse tree sequences which in turn corresponds
to an even greater number of semantic structure se-
quences. Most of these are filtered out, only at the
end by the inference component, because they do
not correspond to the non-linguistic input. Rather
then have these modules operate as non-directed sets
of constraints, direction-specific algorithms are used
which are tailored to producing the factored mental
representations in an efficient order. First, the in-
ference component is called to produce all semantic
structure sequences which correspond to the observa-
tion sequence. Then, the parser is called to produce
</bodyText>
<figure confidence="0.802647666666667">
terfogrAi
affe= •Migt#81.402r
SPLCIUM AtarOZWIlla
</figure>
<page confidence="0.808329">
1.46
</page>
<bodyText confidence="0.9272375">
Radars (Sentences , Lexicon , Observat ions ) : - Lexicon (entry (the ,det ,noSemantics) ,
parser (Sentences ,SyntacticStructures , Lexicon) , entry (cup ,n , cup) ,
linker(Trees,ConceptnalStructures,Lexicon), entry(slid,v,go(x, Efroa(y) ,to (z)] ) ,
inference(ConceptualStructures,Observations). entry (from,p , at (x) )
?- Lexicon = [entry(the._,_), entry( john ,n , j ohn)
entry(cup._,J, entry(to,p,at(x)),
entry(slid,_,_), entry(mary,n,mary),
entry(from,_,_), entry(bill,n,bill)].
entry (john,_,_)
entry(to,_,_),
entry(mary,_,_),
entry(bill,_,_)].
maimra([tthe,cup,slid,from,john,toomary]],
Lexicon,
be(cup.at(john))Vbe(cup(at(mary)));
be(cup.at(mary))Vbe(cup(at(john)))),
maimra(Uthe,cup,slid,from,mary,to,bill]],
Lexicon,
be(cnp,at(mary))k-be(cup(at(bill)));
be (cup , at (bill ) )k-be(cup (at (nary))) .
</bodyText>
<figureCaption confidence="0.683322">
Figure 4: A naive implementation of the cognitive architecture from Figure 3 using techniques similar to
those used by Rayner et. al. in [15].
</figureCaption>
<bodyText confidence="0.999824708333334">
all syntactic structure sequences which correspond
to the sentence sequence. Finally, the linking com-
ponent is run in reverse to produce meanings of lex-
ical items by correlating the syntactic and semantic
structure sequences previously produced. The de-
tails of the factored representation, and the algo-
rithms used to create it, will be discussed in Sec-
tion 5.
Several of the mental representations used by
MAIMRA require a method for representing semantic
information. We have chosen Jackendoff&apos;s theory of
conceptual structure, presented in [6], as our model
for semantic representation. It should be stressed
that although we represent conceptual structure via
a decomposition into primitives much in the same
way as does Schank[18], unlike both Schank and
Jackendoff, we do not claim that any particular such
decompositional theory is adequate as a basis for ex-
pressing the entire range of human thought and the
meanings of even most words in the lexicon. Clearly,
much of human experience is well beyond formaliza-
tion within the current state of the art in knowledge
representation. We are only concerned with repre-
senting and learning the meanings of words describ-
ing simple spatial movements of objects within the
visual field of the learner. For this limited task, a
primitive decompositionsl theory such as Jackend-
off&apos;s seems adequate.
Conceptual structures appear within three of the
mental representations used by MAD4sA. First, the
semantic structures produced by the linker, as mean-
ings of entire utterances, are represented as either
conceptual structure [STATE] or [EVENT] descrip-
tions. Second, the observation sequence comprising
the non-linguistic input is represented as a conjunc-
tion of true and negated [STATE] descriptions. Only
[STATE] descriptions appear in the observation se-
quence. It is the function of the inference component
to infer the possible [EVENT] descriptions which
account for the observed [STATE] sequences. Fi-
nally, meaning components of lexical entries are rep-
resented as fragments of conceptual structure which
contain variables. The conceptual structure frag-
ments are combined by the linker, filling in the vari-
ables with other fragments, to produce the variable
free conceptual structures representing the meanings
of whole utterances from the meanings of their con-
stituent words.
</bodyText>
<sectionHeader confidence="0.964156" genericHeader="method">
4 Learning Constraints
</sectionHeader>
<bodyText confidence="0.999976">
Each of the three modules implements some linguis-
tic or cognitive theory, and accordingly, makes some
assumptions about what knowledge is innate and
what can be learned. Additionally, each module cur-
rently implements only a simple theory and thus has
limitations on the linguistic and cognitive phenom-
ena that it can account for. This section discusses
the innateness assumptions and limitations of each
</bodyText>
<page confidence="0.966615">
147
</page>
<figure confidence="0.988343142857143">
NP VP
{COMP)
{DET} 111 {g1NPIVPIPP}*
—0 {AUX} jj {31NPIVPIPP}4
P igiNrIVPIPPr
—4. {DOIBERMODALITOI
{{MODALITO}) HAVE) {BE))
</figure>
<figureCaption confidence="0.996816">
Figure 5: The context free grammar used by
</figureCaption>
<bodyText confidence="0.923658">
MAIMRA. This grammar is motivated by X-theory.
The head of each rule is enclosed in a box. This head
information is used by the linker.
module in greater detail.
</bodyText>
<subsectionHeader confidence="0.997888">
4.1 The Parser
</subsectionHeader>
<bodyText confidence="0.997527172413793">
While MAIMRA can learn lexical category informa-
tion required by the parser, the parser is given a fixed
context-free grammar which is assumed to be innate.
This fixed grammar used by MAIMRA is shown in
Figure 5. At first glance it might seem unreasonable
to assume that the grammar given in Figure 5 is
innate. A closer look however, reveals that the par-
ticular context-free grammar we use is not entirely
arbitrary; it is motivated by X-theory[2, 3} which
many linguists take to be innate, Our grammar can
be derived from X-theory as follows. We start with a
version of X-theory which allows non-binary branch-
ing nodes and where maximal projections carry bar-
level one (i.e. XP is X). First, fix the parameters
HEAD-first and SPEC-first to yield the prototype
rule:
XP {XspEc} X complement*.
Second, instantiate this rule for each of the lexi-
cal categories N, V and P viewing NSPEC as DET,
VsPEC as AUX and making PSPEc degenerate.
Third, add the rules for S and 3 stipulating that
3 is a maximal projection.2 Fourth, declare all max-
imal projections to be valid complements. Finally,
add in the derivation for the English auxiliary sys-
tem. Thus, our particular context-free grammar is
little more than instantiating X-theory with the En-
glish lexical categories N, V and P, the English pa-
rameters HEAD-first and SPEC-first and the English
auxiliary system.
</bodyText>
<footnote confidence="0.98306">
2A more principled way of deriving the riles for S and g.
from YC.-theory is given in 141
</footnote>
<bodyText confidence="0.999987620689655">
We make no claim that the syntactic theory im-
plemented by MAIMRA is complete. Many linguistic
phenomena remain unaccounted for in our grammar,
among them agreement, tense, aspect, adjectives, ad-
verbs, negation, coordination, quantifiers, toh-words,
pronouns, reference and demonstratives. While the
grammar is motivated by GB theory, the only com-
ponents of GB theory which have been implemented
are X-theory and 0-theory. (0-theory is enforced via
the linking rule discussed in the next subsection.)
Although future work may increase the scope and
accuracy of the syntactic theory incorporated into
MAIMRA, even the current limited grammar offers
a sufficiently rich framework for investigating lan-
guage acquisition. It&apos;s most severe limitation is a
lack of subcategorization; the grammar allows nouns,
verbs and prepositions to take any number of com-
plements of any kind. This causes the grammar to
severely overgenerate and results in a high degree of
non-determinism in the representation of syntactic
structure. It is interesting that despite the use of a
highly ambiguous grammar, the combination of the
parser with the linker and inference component, to-
gether with the non-linguistic context, provide suffi-
cient constraint for the system to learn words quickly
with few training scenarios. This gives evidence that
many of the constraints normally assumed to be im-
posed by syntax, actually result from the interplay
of multiple modules in a broad cognitive system.
</bodyText>
<subsectionHeader confidence="0.996918">
4.2 The Linker
</subsectionHeader>
<bodyText confidence="0.999935181818182">
The linking component of MAIMRA implements a
single linking rule which is assumed to be innate.
This rule is best illustrated by way of the exam-
ple given in Figure 6. Linking proceeds in a bottom
up fashion from the leaves of the parse tree towards
its root. Each node in the parse tree is annotated
with a fragment of conceptual structure. The anno-
tation of leaf nodes comes from the meaning entry for
that word in the lexicon. Every non-leaf node has a
distinguished daughter called the head. Knowledge
of which daughter node is the head for any given
phrasal category is assumed to be innate. For the
grammar used by MAIMRA, this information is indi-
cated in Figure 5 by the categories enclosed in boxes.
The annotation of a non-leaf node is formed by copy-
ing the annotation of its head daughter node, which
may contain variables, and filling some of its variable
slots with the annotation of the remaining non-head
daughters. Note that this is a nondeterministic pro-
cess; there is no stipulation of which variables get
linked to which complements. Because of this non-
determinism, there can be many linkings associated
</bodyText>
<figure confidence="0.7649585">
NP
VP
PP
AUX
</figure>
<page confidence="0.978173">
148
</page>
<bodyText confidence="0.999873852941176">
with any given lexicon and parse tree. In addition
to this linking ambiguity, existence of multiple lexi-
cal entries with different meanings for the same word
can cause meaning ambiguity.
A given variable may appear multiple times in a
fragment of conceptual structure. The linking rule
stipulates that when a variable is linked to an argu-
ment, all instances of the same variable get linked to
that argument as well. Additionally, the linking rule
maintains the constraint that the annotation of the
root node, as well as any node which is a sister to a
head, must be variable free. Linkings which violate
this constraint are discarded. There must be at least
as many distinct variables in the conceptual struc-
ture annotating the head as there are sisters of the
head. Again, if there are insufficient variables in the
head the partial linking is discarded. There may be
more, however, which means that the annotation of
the parent will contain variables. This is acceptable
if the parent is not itself a sister to a head.
MAIMRA imposes two additional constraints on
the linking process. First, meanings of lexical items
must have some semantic content; they can not be
simply a variable. Second, the functor of a con-
ceptual structure fragment can not be a variable.
In other words, it is not possible to have a frag-
ment FROM(x(John)) which would link with AT
to produce FROM(AT(JoLua)). These constraints
help reduce the space of possible lexicons and sup-
port search pruning heuristics which make learning
faster.
In summary, the linking component makes use of
six pieces of knowledge which are assumed to be in-
nate.
</bodyText>
<listItem confidence="0.98235975">
1. The linking rule.
2. The head category associated with each phrasal
category.
3. The requirement that the root semantic struc-
ture be variable free.
4. The requirement that conceptual structure frag-
ments associated with sisters of heads be vari-
able free.
5. The requirement that no lexical item have
empty semantics.
6. The requirement that no conceptual structure
fragment contain variable functors.
</listItem>
<bodyText confidence="0.999915571428572">
There are at least two limitations in the theory of
linking discussed above. First, there is no attempt to
give an adequate semantics for the categories DET,
AUX and COMP. Currently, the linker assumes that
nodes labeled with these categories have no concep-
tual structure annotation. Furthermore, DET, AUX
and COMP nodes which are sisters to a head are not
linked to any variable in the conceptual structure an-
notating the head. Second, while the above linking
rule can account for predication, it cannot account
for the semantics of adjuncts. This shortcoming re-
sults not just from limitations in the linking rule but
also from the fact that Jackendoff&apos;s conceptual struc-
ture is unable to represent adjunct information.
</bodyText>
<subsectionHeader confidence="0.995491">
4.3 The Inference Component
</subsectionHeader>
<bodyText confidence="0.999949585365854">
The inference component imposes the constraint that
the linguistic input must &amp;quot;explain&amp;quot; the non-linguistic
input. This notion of explanation is assumed to be
innate and comprises four principles. First, each
sentence must describe some subsequence of scenes.
Everything the teacher says must be true in the
current non-linguistic context of the learner. The
teacher cannot say something which is either false
or unrelated to the visual field of the learner. Sec-
ond, while the teacher is constrained to making
only true statements about the visual field of the
learner, the teacher is not required to state every-
thing which is true; some non-linguistic data may go
undescribed. Third, the order of the linguistic de-
scription must match the order of occurrence of the
non-linguistic [EVENTS]. This is necessary because
the language fragment handled by MAIMRA does not
support tense and aspect. It also adds substantial
constraint to the learning process. Finally, sentences
must describe non-overlapping scene sequences. Of
these principles, the first two seem very reasonable.
The third is in accordance with the evidence that
children acquire tense and aspect later in the lan-
guage learning process. Only the fourth principle is
questionable. The motivation for the fourth principle
is that it enables the use of the inference algorithm
discussed in Section 5. More recent work, beyond the
scope of this paper, suggests using a different infer-
ence algorithm which does not require this principle.
The above four learning principles make use of
the notion of a sentence &amp;quot;describing&amp;quot; a sequence of
scenes. The notion of description is expressed via the
set of inference rules given in Figure 7. Each rule
enables the inference of the [EVENT] or [STATE]
description on its right hand side from a sequence
of [STATE] descriptions which match the pattern on
its left hand side. For example, Rule 1 states that
if there is a sequence of scenes which can be divided
into two concatenated subsequences of scenes, such
that each subsequence contains at least one scene,
and in every scene in that first subsequence, x is at
</bodyText>
<page confidence="0.993187">
149
</page>
<figure confidence="0.99934419047619">
GO(cup,[FROM(AT(John)),T0(AT(Mary))])
NP
cup
DET cup
The cup
VP
GO(x, [FROM(AT(John)),TO(AT(Mary))])
PP PP
FROM(AT(John)) TO(AT(Mary))
NP
FROM(AT(z)) John TO(AT(z))
I I
from to
John
John
slid
17\
NP
Mary
Mary
Mary
</figure>
<figureCaption confidence="0.864847">
Figure 6: An example of the linking rule used by MAIMRA showing the derivation of conceptual structure
for the sentence The cup slid from John to Mary from the conceptual structure meanings of the individual
</figureCaption>
<bodyText confidence="0.995509043478261">
words, along with a syntactic structure for the sentence.
y and not at z, while in every scene in the second
subsequence, x is at z but not at y, then we can de-
scribe that entire sequence of scenes by saying that x
went on a path from y to z. This rule does not stip-
ulate that other things can&apos;t be true in those scenes
embodying an [EVENT] of type GO, just that at
a minimum, the conditions on the right hand side
must hold over that scene sequence. In general, any
given observation may entail multiple descriptions,
each describing some subsequence of scenes which
may overlap with other descriptions.
MAIMRA currently assumes that these inference
rules are innate. This seems tenable as these rules are
very low level and are probably implemented by the
vision system. Nonetheless, current work is focus-
ing on removing the innateness requirement of these
rules from the inference component.
One severe limitation of the current set of inference
rules is the lack of rules for describing the causality
incorporated in the CAUSE and LET primitive con-
ceptual functions. One method we have considered
is to use rules like:
</bodyText>
<figureCaption confidence="0.508528666666667">
CAUSE(w, G0(x, [FROM(y), TO(z)]))
(BE(w, y) A BE(x, y) A —.13E(x, z))+;
(BE(x, z) A —,13E(x y))+
</figureCaption>
<bodyText confidence="0.999903666666667">
This states that w caused x to move from y to z if
w was at the same location y, as x was, at the start
of the motion. This is clearly unsatisfactory. One
would like to incorporate a more accurate notion of
causality such as that discussed in [91. Unfortunately,
it seems that Jackendoff&apos;s conceptual structures are
not expressive enough to support the more complex
notions of causality. This is another area for future
work.
</bodyText>
<sectionHeader confidence="0.997392" genericHeader="method">
5 Implementation
</sectionHeader>
<bodyText confidence="0.999849636363637">
As mentioned previously, MAIMRA uses directed al-
gorithms, rather than non-directed constraint pro-
cessing, to produce a Lexicon. When processing a
scenario, MAIMRA first applies the inference compo-
nent to the non-linguistic input to produce semantic
structures. Then, it applies the parser to the linguis-
tic input to produce syntactic structures. Finally,
it applies the linking component in reverse, to both
the syntactic structures and semantic structures, to
produce a lexicon as output. This process is best
illustrated by way of an example.
</bodyText>
<page confidence="0.996653">
150
</page>
<figureCaption confidence="0.992761857142857">
GO(x, [FROM(y), TOW]) 4- (BE(x, y) A -.13E(r, z))+ ; (BE(x, z) A -&apos;BE(x, y))+ ( 1 )
GO(x, FROM(y)) 4- (BE(x, y) A -d3E(z, z))+ ; (BE(x, z) A -&apos;BE(x, y))+
GO(x, TO(z)) 4- (13E(x, y) A ,BE(x, z))4 ; (BE(x, z) A -,I3E(x, y))+
GO(x, [1) 4- (BE(x, y) A -.BE(x, z))4 ; (BE(x,z) A -d3E(x, y))+
STAY(z, y) 4- BE(x, 11); (BE(x, y))+
STAY(x, [ ]) 4- BE(x, y); (BE(r, y))4
GOE„t(x, [FROM(y), TO(z)]) 4- (BE(x, y) A BE(r, z) A y z)+
GOE,t(x, FROM(y)) 4- (BE(z, y) A BE(x, z) A y z)+
GOExt(z, TO(z)) 4- (BE(x, 0 A BE(x, z) A y z)
BE(z, y) 4- BE(x, y)+
ORIENT(z, [FROM(y),T0(z)]) 4- ORIENT(z, [FROM(y), TO(z)J)+
ORIENT(z, FROM(y)) (ORIENT(x, [FROM(y), TO(z)]) V ORIENT(x, FROM(,)))+
ORIENT(z, TO(y)) (ORIENT(z, [FROM(y), TO(z)]) V ORIENT(x, TO(y)))+
Figure 7: The inference rules used by the inference component of MAIMRA to infer [EVENTS] from [STATES].
</figureCaption>
<bodyText confidence="0.886354866666667">
Consider the following input scenario.
(BE(cup, ATP ohn)));
(BE(cup, AT(Mary))A
-&apos;BE(cup, AT(John)));
(BE(cup, AT(Mary)));
(BE(cup, AT(3i11))A
---d3E(cup, AT(Mary)));
The cup slid from John to Mary.;
The cup slid from Mary to Bill.
This scenario contains four scenes and two sentences.
First, frame axioms are applied to the scene se-
quence, yielding a sequence of scene descriptions con-
taining all of the true [STATE] descriptions pertain-
ing to those scenes, and only those true [STATE]
descriptions.
</bodyText>
<construct confidence="0.559398">
BE(cup, AT(John));
BE(cup, AT(Mary));
BE(cup, AT(Mary));
BE(cup, AT(Bill))
</construct>
<bodyText confidence="0.999836111111111">
Given a scenario with n sentences and m scenes,
find all possible ways of partitioning the m scenes
into sequences of n partitions, where the partitions
each contain a contiguous subsequence of scenes, but
where the partitions themselves do not overlap and
need not be contiguous. if we abbreviate the above
sequence of four scenes as a; b; e; d, then partitioning
for a scenario containing two sentences produces the
following disjunction:
</bodyText>
<construct confidence="0.520501333333333">
t[cal; ([b] V [c] V [d] V [b; c) V [c; d] V [b; c; cI])IV
{([b] V [a ; b]); ([c] V [d] V [c; cI])}V
{([e] V [b; c] V [a; b; c]);[d]}
</construct>
<bodyText confidence="0.999369848484848">
Next, apply the inference rules from Figure 7 to each
partition in the resulting disjunctive formula, replac-
ing each partition with a disjunction of all [EVENTS]
and [STATES] which can describe that partition. For
our example, this results in the replacements given
in Figure 8.
The disjunction that remains after these replace-
ments describes all possible sequences comprised of
two [EVENTS] or [STATES] that can explain the
input scene sequence. Notice how non-determinism
is managed with a factored representation produced
directly by the algorithm.
After the inference component produces the se-
mantic structure sequences corresponding to the
non-linguistic input, the parser produces the syntac-
tic structure sequences corresponding to the linguis-
tic input. A variant of the CKY algorithm[8, 19] is
used to produce factored parse trees. Finally, the
linker is applied in reverse to each corresponding
parse-tree/semantic-structure pair.
This inverse linking process is termed fracturing.
Fracturing is a recursive process applied to a parse
tree fragment and a conceptual structure fragment.
At each step, the conceptual structure fragment is as-
signed to the root node of the parse tree fragment. If
the root node of the parse tree has n non-head daugh-
ters, then compute all possible ways of extracting
n variable-free subexpressions from the conceptual
structure fragment and assigning them to the non-
head daughters, leaving distinct variables behind as
place holders. The residue after subexpression ex-
traction is assigned to the head daughter. Fractur-
ing is applied recursively to the conceptual structures
</bodyText>
<page confidence="0.936293">
151
</page>
<equation confidence="0.970660076923077">
[a] BE(cup, AT(John))
[b] [c] BE(cup, AT(Mary))
[d] BE(cup, AT (Bill))
[a; b], [a; b; (GO(cup, [FROM(AT(John)), TO(AT(Mary))1)
GO(cup, FROM(AT(John))) V
GO(cup, TO(AT(Mary))) v
GO(cup, [1))
[b;cJ (BE(cup, AT(Mary)) v
STAY(cup, AT(Mary)))
[c; cl] [b; C; (GO(cup, [FROM(AT(Mary)), TO(AT(Bill))]) V
GO(cup, FROM (AT(Mary))) V
GO(cup, TO(AT(Bill))) v
GO(cup, [ ])).
</equation>
<figureCaption confidence="0.966415">
Figure 8: The replacements resulting from the application of the inference rules from Figure 7 to the example
given in the text.
</figureCaption>
<bodyText confidence="0.997961482142857">
assigned to daughters of the root node of the parse
tree fragment, along with their annotations. The
results of these recursive calls are then conjoined to-
gether. Finally, a disjunction is formed over each
possible way of performing the subexpression extrac-
tion. This process is illustrated by the following ex-
ample. Consider fracturing the conceptual structure
fragment
GO(x, [FROM(AT(John)), TO(AT(Mary))])
along with a VP node with a head daughter labeled
V and two sister daughters labeled PP. This produces
the set of possible extractions shown in Figure 9. The
fracturing recursion terminates when a lexical item
is fractured. This returns a lexical entry triple com-
prising the word, its category and a representation
of its meaning. The end result of the fracturing pro-
cess is a monotonic Boolean formula over definition
triples which concisely represents the set of all pos-
sible lexicons which allow the linguistic input from a
scenario to explain the non-linguistic input. Such a
factored lexicon (arising when processing a scenario
similar to the second scenario of the training session
given in Figure 2) is illustrated in Figure 10.
The disjunctive lexicon produced by the fractur-
ing process may contain lexicons which assign more
than one meaning to a given word. We incorporate a
monosemy constraint to rule out such lexicons. Con-
ceptually, this is done by converting the factored dis-
junctive lexicon to disjunctive normal form and re-
moving lexicons which contain more than one lex-
ical entry for the same word. Computationally, a
more efficient way of accomplishing the same task is
to view the factored disjunctive lexicon as a mono-
tonic Boolean formula cD whose propositions are lex-
ical entries. We conjoin cD with all conjunctions of
the form r.-N c&amp;quot;ri where the /xi and cxj are both dis-
tinct lexical entries for the same word that appear
(I. The resulting formula is no longer monotonic.
Satisfying assignments for this formula correspond
to conjunctive lexicons which meet the monosemy
constraint. The satisfying assignments can be found
using well known constraint satisfaction techniques
such as truth maintenance systems[10, 11). While
the problem of finding satisfying assignments for a
Boolean formula (i.e. SAT) is NP-complete, our ex-
perience is that in practice, the SAT problems gen-
erated by MAIMRA are easy to solve and that the
fracturing process of generating the SAT problems
takes far more time than actually solving them.
The monosemy constraint may seem a bit restric-
tive. It can be relaxed somewhat by allowing up
to n alternate meanings for a word by conjoining in
conjunctions of the form
where each of the ail are distinct lexical entries for
the same word that appear in 1, instead of the pair-
wise conjunctions used previously.
</bodyText>
<page confidence="0.993096">
152
</page>
<table confidence="0.9853063">
GO(x,fy, z)) F RO M (AT(J ohn) ) TO(AT(Mary))
GO(r, (y, x)) TO FROM(AT(John))
GO(x, [FROM(,), .4) AT(John) TO(AT(Mary))
60(x, [FROM(y),14) TO(AT(Mary)) AT(John)
60(x, [FROM(AT(y)), z}) John TO(AT(Mary))
GO(x [FROM(AT(y)), z]) TO(AT(Mary)) John
TO(2)1) FROM(AT(John)) AT(Mary)
GO(x, It&apos;, TOW]) AT(Mary) FROM(AT(John))
GO(x , [FROM(y), TO(z))) AT(John) AT(Mary) disjunction.
GO(x, [FROM(y), TO(z)]) AT(Mary) Mary )
AT(John)
GO(x, [FROM(AT(y)), TO(x)]) John AT
60(x, [FROM(AT(y)), TO(z)]) AT(Mary) John
60(x, [y, TO(AT(z))]) FROM(AT(John)) Mary
GO(x, ry, TO(AT(z))j) Mary FROM(AT(John))
60(x, [FROM(y), TO(AT(z))1) AT(John) Mary
GO(w, [FROM(y), TO(AT(z))]) Mary AT(John)
GO(x, [FROM(AT(y)), TO(AT(x)))) John Mary
GO(x, [FROM(AT(y)), TO(AT(x))]) Mary John
conjunction
</table>
<figureCaption confidence="0.4952876">
Figure ch A recursive step of the fracturing process illustrating all possible sub expression extractions from
the conceptual structure fragment given in the text, and their assignments to non-head daughters. The
center column contains fragments annotating the first PP while the rightmost column contains fragments
annotating the second PP. The leftmost column shows the residue which annotates the head. Each row is
one distinct possible extraction.
</figureCaption>
<table confidence="0.989840434782609">
(AND (DEFINITION CUP N CUP)
(OR (AND (OR (AND (DEFINITION MARY N (AT MARY))
(DEFINITION TO P (TO ?O)))
(AND (DEFINITION MARY N MARY)
(DEFINITION TO P (TO (AT ?O)))))
(OR (AND (OR (AND (DEFINITION JOHN N (AT JOHN))
(DEFINITION FROM P (FROM ?O)))
(AND (DEFINITION JOHN N JOHN)
(DEFINITION FROM P (FROM (AT TO)))))
(DEFINITION SLID V (GO TO (PATH Ti ?2))))
(AND (DEFINITION JOHN M JOHN)
(DEFINITION FROM P (AT 10))
(DEFINITION SLID V (GO TO (PATH Ti (FROM 12)))))))
(AND (DEFINITION MARY N MARY)
(DEFINITION TO P (AT ?0))
(OR (AND (OR (AND (DEFINITION JOHN N (AT JOHN))
(DEFINITION FROM P (FROM ?0)))
(AND (DEFINITION JOHN N JOHN)
(DEFINITION FROM P (FROM (AT 10)))))
(DEFINITION SLID V (GO TO (PATH ?1 (TO 12)))))
(AND (DEFINITION JOHN N JOHN)
(DEFINITION FROM P (AT 70))
(DEFINITION SLID V (GO ?0 (PATH (FROM Ti) (TO ?2)))))))))
</table>
<figureCaption confidence="0.881755">
Figure 10: A portion of the disjunctive lexicon which results from processing a scenario similar to the second
scenario of the training session given in Figure 2.
</figureCaption>
<page confidence="0.998838">
153
</page>
<sectionHeader confidence="0.998266" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999682166666667">
When presented with a training session3 much like
that given in Figure 2, MAIMRA converges to a
unique lexicon within six scenarios and several min-
utes of CPU time. It is not however, able to converge
to a unique meaning for the word enier when given
scenarios of the form:
</bodyText>
<table confidence="0.39986">
1 (DE(John, AT(outside))A
</table>
<tableCaption confidence="0.4849965">
-,13E(John, IN(roona)));
(BE(John, IN(rooni))A .
—d3E(John, AT(outside)))
John entered the room.
</tableCaption>
<bodyText confidence="0.999223014084507">
It turns out that there is no way to force MAIMRA
to realize that the sentence describes the entire sce-
nario and not just the first or last scene alone. Thus
MAIMRA does not rule out the possibility that en-
ter might mean &amp;quot;to be somewhere.&amp;quot; The reason
MAIMRA is successful with the session from Figure 2
is that the empty semantics constraint rules out asso-
ciating the sentences with just the first or last scene
because the semantic structures representing those
scene subsequences have too little semantic material
to distribute among the words of the sentence. One
way around this problem would be for MAIMRA to
attempt to choose the lexicon which maximizes the
amount of non-linguistic data which is accounted for.
Future work will investigate this issue further.
We make three claims as a result of this work.
First, this work demonstrates that the combina-
tion of syntactic, semantic and pragmatic modules,
each incorporating cognitively plausible innateness
assumptions, offers sufficient constraint for !earning
word meanings with no prior lexical knowledge in
the context of non-linguistic input. This offers a
general framework for explaining meaning acquisi-
tion. Second, appropriate choices of representation
and algorithms allow efficient implementation within
the general framework. While no claim is being made
that children employ the mechanisms described here,
they nonetheless can be used to construct useful en-
gineered systems which learn language. The third
3Although not strictly required by either the theory or
the implementation, we currently incorporate into the train-
ing session given to MA1MRA , an initial lexicon telling it that
&apos;John,&apos; Mary&apos; and &apos;Bill&apos; are nouns, &apos;from&apos; and &apos;to&apos; are preposi-
tions and &apos;the&apos; is a determiner. This is to reduce the combina-
torics of generating ambiguous parses. Category information
is not given for any other words, nor is meaning information
given for any words occurring in the training session. In the-
ory it would be possible to efficiently bootstrap the categories
for these words as well, via a longer training session containing
a few shorter sentences to constrain the possible categories for
these words. We have not done so yet, however.
claim is more bold. Most language acquisition re-
search operates under a tacit assumption that chil-
dren acquire individual pieces of knowledge about
language by experiencing single short stimuli in iso-
lation. This is often extended to an assumption that
knowledge of language is acquired by discovering dis-
tinct cues in the input, each cue elucidating one pa-
rameter setting in a parameterized linguistic theory.
We will call this assumption the local learning hy-
pothesis. This is in contrast to our approach where
knowledge of language is acquired by finding data
consistent across longer correlated sessions. Our ap-
proach requires the learner to do some puzzle solving
or constraint satisfaction.4 It is normally believed
that the latter approach is not cognitively plausi-
ble. The evidence for this is that children seem to
have short &amp;quot;input buffers.&amp;quot; The limited size of the
input buffers is taken to imply that only short iso-
lated stimuli can take part in inferring each new lan-
guage fact. MAIMRA demonstrates that despite a
short input buffer with the ability of retaining only
one scenario at a time, it is nonetheless possible to
produce a disjunctive representation which supports
constraint solving across multiple scenarios. We be-
lieve that without cross scenario constraint solving, it
is impossible to account for meaning acquisition and
thus the local learning hypothesis is wrong. Our ap-
proach offers a viable alternative to the local learning
hypothesis consistent with the observed short input
buffer effect.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999933941176471">
While most prior computational work on meaning ac-
quisition focuses on contextual learning by scanning
texts, some notable work has pursued a path simi-
lar to that described here attempting to learn from
correlated linguistic and non-linguistic input. In
[16, 17], Salveter describes a system called MORAN.
The non-linguistic component of each scenario pre-
sented to MORAN consists of a sequence of exactly
two scenes, where each scene is described by a con-
junction of atomic formula. The linguistic compo-
nent of each scenario is a preparsed case frame anal-
ysis of a single sentence describing the state change
occurring between those two scenes. From each sce-
nario in isolation, MORAN infers what Salveter calls
a Conceptual Meaning Structure (CMS) which at-
tempts to capture the essence of the meaning of the
verb in the sentence. This CMS is a subset of the
</bodyText>
<footnote confidence="0.96479575">
4 We are not claiming that such puzzle solving is conscious.
It is likely that constraint satisfaction, if done by children or
adults, is a very low level subconscious cognitive function not
subject to introspective observation.
</footnote>
<page confidence="0.999504">
154
</page>
<bodyText confidence="0.999723846153847">
two scenes identifying the portion of the scenes re-
ferred to by the sentence, with the arguments of the
atomic formula linked to noun phrases replaced by
variables labeled with the syntactic positions those
noun phrases fill in the sentence. The process of
inferring CMSs involves two processes reminiscent
of tasks performed by MAIMRA, namely the fig-
ure/ground distinction whereby the inference com-
ponent suggests possible subsets of the non-linguistic
input as being referred to by the linguistic input (as
distinct from the part which is not referred to) and
the fracturing process whereby verb meanings are
constructed by extracting out arguments from whole
sentence meanings. MoRAN&apos;s variants of these tasks
are much simpler than the analogous tasks performed
by MAIMRA. First, the figure/ground distinction is
easier since each scenario presented to MORAN con-
tains but a single sentence and a pair of scenes.
MORAN need not figure out which subsequence of
scenes corresponds to each sentence. Second, the
linguistic input comes to MORAN preparsed which
relies on preexisting knowledge of the lexical cate-
gories of the words in the sentence. MORAN does not
acquire category information, and furthermore does
not deal with any ambiguity that might arise from
the parsing process or the figure/ground distinction.
Finally, the training session presented to MORAN re-
lies on a subtle implicit link between the objects in
the world and linguistic tokens used to refer to them.
Part of the difficulty faced by MAIMRA is discerning
that the linguistic token John refers to the concep-
tual structure fragment John. MORAN is given that
information a priori by lacking a formal distinction
between the notion of a linguistic token and concep-
tual structure. Given this information, the fractur-
ing process becomes trivial. MORAN therefore, does
not exhibit the cross-scenario correlational behavior
attributed to MAIMRA and in fact learns every verb
meaning with just a single training scenario. This
seems very implausible as a model of child language
acquisition. In contrast to MAIMRA, MORAN is able
to learn polysemous senses for verbs; one for each sce-
nario provided for a given verb. MORAN focuses on
extracting out the common substructure for polyse-
mous meanings attempting to maximize commonal-
ity between different word senses and build a catalog
of higher level conceptual building blocks, a task not
attempted by MAIMRA.
In [13, 14], Pustejovsky describes a system called
TULLY, which also operates in a fashion similar to
MAIMRA and MORAN, learning word meanings from
pairs of linguistic and non-linguistic input. Like
MORAN, the linguistic input given to TULLY for
each scenario is a single parsed sentence. The non-
linguistic input given along with that parsed sentence
is a predicate calculus description of three parts of
a single event, its beginning, middle and end. From
this input, TULLY derives a Thematic Mapping In-
dex, a data structure representing the 0-roles borne
by each of the arguments to the main predicate. Like
MORAN, the task faced by TULLY is much simpler
than that faced by MAIMRA, since TULLY is pre-
sented with unambiguous parsed input, is given the
correspondence between nouns and their referents
and is given the correspondence between a single sen-
tence and the semantic representation of the event
described by that sentence. TULLY does not learn
lexical categories, does not have to determine fig-
ure/ground partitioning of non-linguistic input and
implausibly learns verb meanings from single scenar-
ios without any cross-scenario correlation. Multiple
scenarios for the same verb cause TULLY to gener-
alize to the feast common generalization of the in-
dividual instances. TULLY however, goes beyond
MAIMRA in trying to account for the acquisition of
a variety of markedness features for 0-roles includ-
ing [±motion], f±abstraci],[±direct], [Ipariiiive] and
[±animaie]
</bodyText>
<sectionHeader confidence="0.997587" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999963692307692">
The MAIMRA system successfully learns word mean-
ings with no prior lexical knowledge of any words.
It works by applying syntactic, semantic and prag-
matic constraints to correlated linguistic and non-
linguistic input. In doing so, it more accurately re-
flects the type of learning performed by children,
in contrast to previous lexical acquisition systems
which focus on learning unknown words encountered
while reading texts. Although, each module imple-
ments a weak theory, and in isolation offers only lim-
ited constraint on possible mental representations,
the collective constraint provided by the combina-
tion of modules is sufficient to reduce the nondeter-
minism to a manageable level. It demonstrates that
with a reasonable set of assumptions about innate
knowledge, combined with appropriate representa-
tions and algorithms, tractable learning is possible
with short training sessions and limited processing.
Though there may be disagreement as to the lin-
guistic and cognitive plausibility of some of the in-
nateness assumptions, and while the particular syn-
tactic, semantic and pragmatic theories currently in-
corporated into MAIMRA may be only approxima-
tions to reality, nonetheless, the general framework
shows promise of explaining how children acquire
word meanings. In particular, it offers a viable al-
</bodyText>
<page confidence="0.996611">
155
</page>
<bodyText confidence="0.999967142857143">
ternative to the local learning hypothesis which can
explain how children acquire meanings that require
correlation of experience across many input scenar-
ios, with only limited size input buffers. Future work
will attempt to address these potential shortcomings
and will focus on supporting more robust acquisition
of a broader class of word meanings.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999501">
I would like to thank Peter Szolovitz, Patrick Win-
ston and Victor Zue for giving me the freedom to
embark on this project and encouraging me to elab-
orate on it; AT&amp;T Bell Laboratories for supporting
this work through a Ph.D. scholarship; Johan deK-
leer, Kris Halvorsen and everybody at Xerox PARC
for listening to half-baked versions of this work prior
to completion; Bob Berwick, Barbara Grosz, David
McAllester and George Lakoff for many interesting
discussions; and Ron Rivest for pushing me to com-
plete this paper.
</bodyText>
<sectionHeader confidence="0.999403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999973774647887">
[1] Robert C. Berwick. Learning word meanings
from examples. In Proceedings of the Eighth In-
ternational Joint Conference on Artificial Intel-
ligence, pages 459-461, 1983.
[2] Noain Chomsky. Lectures on Government and
Binding, volume 9 of Studies in Generative
Grammar. Foris Publications, 1981.
[3] Noam Chomsky. Some Concepts and Conse-
quences of the Theory of Government and Bind-
ing, volume 6 of Linguistic Inquiry Monographs.
The M. I. T. Press, Cambridge, Massachusetts
and London, England, 1982.
[4] Noam Chomsky. Barriers, volume 13 of Lin-
guistic Inquiry Monographs. The M. I. T. Press,
Cambridge, Massachusetts and London, Eng-
land, 1986.
[5] Richard H. Granger, Jr. FOUL-UP a program
that figures out meanings of words from context.
In Proceedings of the Fifth International Joint
Conference on Artificial Intelligence, pages 172-
178, 1977.
[6] Ray Jackendoff. Semantics and Cognition. The
M. I. T. Press, Cambridge, Massachusetts and
London, England, 1983.
[7] Paul Jacobs and Uri Zernik. Acquiring lexical
knowledge from text: A case study. In Proceed-
ings of the Seventh National Conference on Ar-
tifical Intelligence, pages 739-744, August 1988.
[8] T. Kasami. An efficient recognition and syn-
tax algorithm for context-free languages. Sci-
entific Report AFCRL-65-758, Air Force Cam-
bridge Research Laboratory, Bedford MA, 1965.
[9] George Lakoff and Mark Johnson. Metaphors
We Live By. The University of Chicago Press,
1980.
[10] David Allen McAllester. Solving SAT problems
via dependency directed backtracking. Unpub-
lished manuscript received directly from author.
[11] David Allen McAllester. An outlook on truth
maintenance. A. I. Memo 551, M. I. T. Artificial
Intelligence Laboratory, August 1980.
[12] Fernando C. N. Pereira and David H. D. War-
ren. Definite clause grammars for language
analysis—a survey of the formalism and a com-
parison with augmented transition networks.
Artificial Intelligence, 13(3):231-278, 1980.
[13] James Pustejovsky. On the acquisition of lexi-
cal entries: The perceptual origin of thematic
relations. In Proceedings of the 25th Annual
Meeting of the Association for Computational
Linguistics, pages 172-178, July 1987.
[14] James Pustejovsky. Constraints on the acquisi-
tion of semantic knowledge. International Jour-
nal of Intelligent Systems, 3(3):247-268, 1988.
[15] Manny Rayner, Asa Hugossort, and Goran
Hagert. Using a logic grammar to learn a lex-
icon. Technical Report R88001, Swedish Insti-
tute of Computer Science, 1988.
[16] Sharon C. Salveter. Inferring conceptual graphs.
Cognitive Science, 3(4141-166, 1979.
[17] Sharon C. Salveter. Inferring building blocks for
knowledge representation. In Wendy G. Lehn-
ert and Martin H. Ringle, editors, Strategies for
Natural Language Processing, chapter 12, pages
327-344. Lawrence Erlbaum Associates, 1982.
[18] Roger C. Schank. The fourteen primitive actions
and their inferences. Memo AIM-183, Stanford
Artificial Intelligence Laboratory, March 1973.
[19] D. H. Younger. Recognition and parsing of
context-free languages in time 0(n3). Informa-
tion and Control, 10(4189-208, 1967.
</reference>
<page confidence="0.998791">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.483449">
<title confidence="0.707883">ACQUIRING CORE MEANINGS OF WORDS, REPRESENTED AS JACKENDOFF-STYLE CONCEPTUAL STRUCTURES, FROM CORRELATED STREAMS OF LINGUISTIC AND NON-LINGUISTIC INPUT</title>
<author confidence="0.997076">Jeffrey Mark Siskincl</author>
<affiliation confidence="0.997259">M. I. T. Artificial Intelligence Laboratory</affiliation>
<address confidence="0.999326">545 Technology Square, Room NE43-800b Cambridge MA 02139</address>
<phone confidence="0.943566">617/253-5659</phone>
<email confidence="0.988975">internet:QobinALMIT.EDU</email>
<abstract confidence="0.999275681818182">This paper describes an operational system which can acquire the core meanings of words without any prior knowledge of either the category or meaning of any words it encounters. The system is given as input, a description of sequences of scenes along with sentences which describe the [EVENTS] taking place as those scenes unfold, and produces as output, a lexicon consisting of the category and meaning of each word in the input, that allows the sentences to describe the [EVENTS]. It is argued, that each of the three main components of the system, the parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
</authors>
<title>Learning word meanings from examples.</title>
<date>1983</date>
<booktitle>In Proceedings of the Eighth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>459--461</pages>
<contexts>
<context position="1502" citStr="[5, 7, 1, 16, 17, 13, 14]" startWordPosition="220" endWordPosition="226"> parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research. 1 Introduction Several natural language systems have been reported which learn the meanings of new words[5, 7, 1, 16, 17, 13, 14]. Many of these systems (in particular [5, 7, 1]) learn the new meanings based upon expectations arising from the morphological, syntactic, se&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholarship. Part of this research was performed while the author was visiting Xerox PARC as a research intern and as a consultant. mantic and pragmatic context of the unknown word in the text being processed. For example, if such a system encounters the sentence &amp;quot;I woke up yesterday, turned off my alarm clock, took a shower, and cooked myself two grimps for breakfast[51&amp;quot; it might conclude that grimps is a no</context>
</contexts>
<marker>[1]</marker>
<rawString>Robert C. Berwick. Learning word meanings from examples. In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, pages 459-461, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noain Chomsky</author>
</authors>
<title>Lectures on Government and Binding,</title>
<date>1981</date>
<booktitle>of Studies in Generative Grammar.</booktitle>
<volume>9</volume>
<publisher>Foris Publications,</publisher>
<marker>[2]</marker>
<rawString>Noain Chomsky. Lectures on Government and Binding, volume 9 of Studies in Generative Grammar. Foris Publications, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Some Concepts and Consequences of the Theory of Government and Binding,</title>
<date>1982</date>
<journal>of Linguistic Inquiry Monographs. The M. I. T.</journal>
<volume>6</volume>
<publisher>Press,</publisher>
<location>Cambridge, Massachusetts and London, England,</location>
<marker>[3]</marker>
<rawString>Noam Chomsky. Some Concepts and Consequences of the Theory of Government and Binding, volume 6 of Linguistic Inquiry Monographs. The M. I. T. Press, Cambridge, Massachusetts and London, England, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Barriers, volume 13 of Linguistic Inquiry Monographs.</title>
<date>1986</date>
<journal>The M. I. T.</journal>
<publisher>Press,</publisher>
<location>Cambridge, Massachusetts and London, England,</location>
<marker>[4]</marker>
<rawString>Noam Chomsky. Barriers, volume 13 of Linguistic Inquiry Monographs. The M. I. T. Press, Cambridge, Massachusetts and London, England, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard H Granger</author>
</authors>
<title>FOUL-UP a program that figures out meanings of words from context.</title>
<date>1977</date>
<booktitle>In Proceedings of the Fifth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>172--178</pages>
<contexts>
<context position="1502" citStr="[5, 7, 1, 16, 17, 13, 14]" startWordPosition="220" endWordPosition="226"> parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research. 1 Introduction Several natural language systems have been reported which learn the meanings of new words[5, 7, 1, 16, 17, 13, 14]. Many of these systems (in particular [5, 7, 1]) learn the new meanings based upon expectations arising from the morphological, syntactic, se&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholarship. Part of this research was performed while the author was visiting Xerox PARC as a research intern and as a consultant. mantic and pragmatic context of the unknown word in the text being processed. For example, if such a system encounters the sentence &amp;quot;I woke up yesterday, turned off my alarm clock, took a shower, and cooked myself two grimps for breakfast[51&amp;quot; it might conclude that grimps is a no</context>
</contexts>
<marker>[5]</marker>
<rawString>Richard H. Granger, Jr. FOUL-UP a program that figures out meanings of words from context. In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, pages 172-178, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Semantics</author>
<author>Cognition The M I T</author>
</authors>
<date>1983</date>
<publisher>Press,</publisher>
<location>Cambridge, Massachusetts and London, England,</location>
<contexts>
<context position="15775" citStr="[6]" startWordPosition="2409" endWordPosition="2409">chniques similar to those used by Rayner et. al. in [15]. all syntactic structure sequences which correspond to the sentence sequence. Finally, the linking component is run in reverse to produce meanings of lexical items by correlating the syntactic and semantic structure sequences previously produced. The details of the factored representation, and the algorithms used to create it, will be discussed in Section 5. Several of the mental representations used by MAIMRA require a method for representing semantic information. We have chosen Jackendoff&apos;s theory of conceptual structure, presented in [6], as our model for semantic representation. It should be stressed that although we represent conceptual structure via a decomposition into primitives much in the same way as does Schank[18], unlike both Schank and Jackendoff, we do not claim that any particular such decompositional theory is adequate as a basis for expressing the entire range of human thought and the meanings of even most words in the lexicon. Clearly, much of human experience is well beyond formalization within the current state of the art in knowledge representation. We are only concerned with representing and learning the m</context>
</contexts>
<marker>[6]</marker>
<rawString>Ray Jackendoff. Semantics and Cognition. The M. I. T. Press, Cambridge, Massachusetts and London, England, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jacobs</author>
<author>Uri Zernik</author>
</authors>
<title>Acquiring lexical knowledge from text: A case study.</title>
<date>1988</date>
<booktitle>In Proceedings of the Seventh National Conference on Artifical Intelligence,</booktitle>
<pages>739--744</pages>
<contexts>
<context position="1502" citStr="[5, 7, 1, 16, 17, 13, 14]" startWordPosition="220" endWordPosition="226"> parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research. 1 Introduction Several natural language systems have been reported which learn the meanings of new words[5, 7, 1, 16, 17, 13, 14]. Many of these systems (in particular [5, 7, 1]) learn the new meanings based upon expectations arising from the morphological, syntactic, se&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholarship. Part of this research was performed while the author was visiting Xerox PARC as a research intern and as a consultant. mantic and pragmatic context of the unknown word in the text being processed. For example, if such a system encounters the sentence &amp;quot;I woke up yesterday, turned off my alarm clock, took a shower, and cooked myself two grimps for breakfast[51&amp;quot; it might conclude that grimps is a no</context>
</contexts>
<marker>[7]</marker>
<rawString>Paul Jacobs and Uri Zernik. Acquiring lexical knowledge from text: A case study. In Proceedings of the Seventh National Conference on Artifical Intelligence, pages 739-744, August 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages.</title>
<date>1965</date>
<tech>Scientific Report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Laboratory,</institution>
<location>Bedford MA,</location>
<contexts>
<context position="32709" citStr="[8, 19]" startWordPosition="5186" endWordPosition="5187">ibe that partition. For our example, this results in the replacements given in Figure 8. The disjunction that remains after these replacements describes all possible sequences comprised of two [EVENTS] or [STATES] that can explain the input scene sequence. Notice how non-determinism is managed with a factored representation produced directly by the algorithm. After the inference component produces the semantic structure sequences corresponding to the non-linguistic input, the parser produces the syntactic structure sequences corresponding to the linguistic input. A variant of the CKY algorithm[8, 19] is used to produce factored parse trees. Finally, the linker is applied in reverse to each corresponding parse-tree/semantic-structure pair. This inverse linking process is termed fracturing. Fracturing is a recursive process applied to a parse tree fragment and a conceptual structure fragment. At each step, the conceptual structure fragment is assigned to the root node of the parse tree fragment. If the root node of the parse tree has n non-head daughters, then compute all possible ways of extracting n variable-free subexpressions from the conceptual structure fragment and assigning them to </context>
</contexts>
<marker>[8]</marker>
<rawString>T. Kasami. An efficient recognition and syntax algorithm for context-free languages. Scientific Report AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford MA, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>The University of Chicago Press,</publisher>
<marker>[9]</marker>
<rawString>George Lakoff and Mark Johnson. Metaphors We Live By. The University of Chicago Press, 1980.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Allen McAllester</author>
</authors>
<title>Solving SAT problems via dependency directed backtracking.</title>
<note>Unpublished manuscript received directly from author.</note>
<marker>[10]</marker>
<rawString>David Allen McAllester. Solving SAT problems via dependency directed backtracking. Unpublished manuscript received directly from author.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Allen McAllester</author>
</authors>
<title>An outlook on truth maintenance.</title>
<date>1980</date>
<journal>A. I. Memo 551, M. I. T. Artificial Intelligence Laboratory,</journal>
<marker>[11]</marker>
<rawString>David Allen McAllester. An outlook on truth maintenance. A. I. Memo 551, M. I. T. Artificial Intelligence Laboratory, August 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite clause grammars for language analysis—a survey of the formalism and a comparison with augmented transition networks.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>13--3</pages>
<contexts>
<context position="4555" citStr="[12]" startWordPosition="723" endWordPosition="723">e of a small number of judicious semantic constraints and a small number of heuristics which order and eliminate much of the search. This paper explains the general theory as well as the implementation details which make it work. In addition, it discusses some limitations in the current theory, among which is one which prevents it from converging on a single definition of some words. 2 Background In (15], Rayner et. al. describe a system which can determine the lexical category of each word in a corpus of sentences. They observe that while in the original formulation, a definite clause grammar[12] normally defines a two-argument predicate parser(Sent ence ,Tree) with the lexicon represented directly in the clauses of the grammar, an alternative formulation would allow the lexicon to be represented explicitly as an additional argument to the parser relation, yielding a three argument predicate par sex(Sent enc Tree , Lex icon) . This three argument relation can be used to learn lexical category information by a technique summarized in Figure I. Here, a query is formed containing a conjunction of calls to the parser, one for each sentence in the corpus. All of the calls share a common Le</context>
</contexts>
<marker>[12]</marker>
<rawString>Fernando C. N. Pereira and David H. D. Warren. Definite clause grammars for language analysis—a survey of the formalism and a comparison with augmented transition networks. Artificial Intelligence, 13(3):231-278, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>On the acquisition of lexical entries: The perceptual origin of thematic relations.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>172--178</pages>
<contexts>
<context position="1502" citStr="[5, 7, 1, 16, 17, 13, 14]" startWordPosition="220" endWordPosition="226"> parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research. 1 Introduction Several natural language systems have been reported which learn the meanings of new words[5, 7, 1, 16, 17, 13, 14]. Many of these systems (in particular [5, 7, 1]) learn the new meanings based upon expectations arising from the morphological, syntactic, se&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholarship. Part of this research was performed while the author was visiting Xerox PARC as a research intern and as a consultant. mantic and pragmatic context of the unknown word in the text being processed. For example, if such a system encounters the sentence &amp;quot;I woke up yesterday, turned off my alarm clock, took a shower, and cooked myself two grimps for breakfast[51&amp;quot; it might conclude that grimps is a no</context>
<context position="46678" citStr="[13, 14]" startWordPosition="7403" endWordPosition="7404">fore, does not exhibit the cross-scenario correlational behavior attributed to MAIMRA and in fact learns every verb meaning with just a single training scenario. This seems very implausible as a model of child language acquisition. In contrast to MAIMRA, MORAN is able to learn polysemous senses for verbs; one for each scenario provided for a given verb. MORAN focuses on extracting out the common substructure for polysemous meanings attempting to maximize commonality between different word senses and build a catalog of higher level conceptual building blocks, a task not attempted by MAIMRA. In [13, 14], Pustejovsky describes a system called TULLY, which also operates in a fashion similar to MAIMRA and MORAN, learning word meanings from pairs of linguistic and non-linguistic input. Like MORAN, the linguistic input given to TULLY for each scenario is a single parsed sentence. The nonlinguistic input given along with that parsed sentence is a predicate calculus description of three parts of a single event, its beginning, middle and end. From this input, TULLY derives a Thematic Mapping Index, a data structure representing the 0-roles borne by each of the arguments to the main predicate. Like M</context>
</contexts>
<marker>[13]</marker>
<rawString>James Pustejovsky. On the acquisition of lexical entries: The perceptual origin of thematic relations. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 172-178, July 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>Constraints on the acquisition of semantic knowledge.</title>
<date>1988</date>
<journal>International Journal of Intelligent Systems,</journal>
<pages>3--3</pages>
<contexts>
<context position="1502" citStr="[5, 7, 1, 16, 17, 13, 14]" startWordPosition="220" endWordPosition="226"> parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research. 1 Introduction Several natural language systems have been reported which learn the meanings of new words[5, 7, 1, 16, 17, 13, 14]. Many of these systems (in particular [5, 7, 1]) learn the new meanings based upon expectations arising from the morphological, syntactic, se&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholarship. Part of this research was performed while the author was visiting Xerox PARC as a research intern and as a consultant. mantic and pragmatic context of the unknown word in the text being processed. For example, if such a system encounters the sentence &amp;quot;I woke up yesterday, turned off my alarm clock, took a shower, and cooked myself two grimps for breakfast[51&amp;quot; it might conclude that grimps is a no</context>
<context position="46678" citStr="[13, 14]" startWordPosition="7403" endWordPosition="7404">fore, does not exhibit the cross-scenario correlational behavior attributed to MAIMRA and in fact learns every verb meaning with just a single training scenario. This seems very implausible as a model of child language acquisition. In contrast to MAIMRA, MORAN is able to learn polysemous senses for verbs; one for each scenario provided for a given verb. MORAN focuses on extracting out the common substructure for polysemous meanings attempting to maximize commonality between different word senses and build a catalog of higher level conceptual building blocks, a task not attempted by MAIMRA. In [13, 14], Pustejovsky describes a system called TULLY, which also operates in a fashion similar to MAIMRA and MORAN, learning word meanings from pairs of linguistic and non-linguistic input. Like MORAN, the linguistic input given to TULLY for each scenario is a single parsed sentence. The nonlinguistic input given along with that parsed sentence is a predicate calculus description of three parts of a single event, its beginning, middle and end. From this input, TULLY derives a Thematic Mapping Index, a data structure representing the 0-roles borne by each of the arguments to the main predicate. Like M</context>
</contexts>
<marker>[14]</marker>
<rawString>James Pustejovsky. Constraints on the acquisition of semantic knowledge. International Journal of Intelligent Systems, 3(3):247-268, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>Asa Hugossort</author>
<author>Goran Hagert</author>
</authors>
<title>Using a logic grammar to learn a lexicon.</title>
<date>1988</date>
<tech>Technical Report R88001,</tech>
<institution>Swedish Institute of Computer Science,</institution>
<contexts>
<context position="8693" citStr="[15]" startWordPosition="1364" endWordPosition="1364">lain the non-linguistic input. The non-directional collection of modules can &apos;MAIMRA, or Ninnn, is the Aramaic word for word. 144 ?- Lexicon = Eentry(the,_), Lexicon = (entry(the,det), entry (cap,_), entry(cup,n), entry (slid,_), entry(slid,v), entry (from,_), entry(from,p), entry(john,J, entry(john,n), entry(to,_), entry(to,p), entry(mary,_), entry(mary,n), entry(bill,_)], entry(bill,n)]. pareer(Cthe,cup,slid,from,john,to,mary),_,Lexicon), pareer(tthe,cup,slid,from,mary,to,bill],_,Lexicon), parser( [the,cup,elid,from,bill,to,john],_,Lexicon). Figure 1: The technique used by Rayner et. al. in [15] to acquire lexical category information from a corpus of sentences. Input: Output: The: DET clip: N [Thing cup] slid: V [Event GO(x, [path y, z])] from: P [Path FROMaphiee AT(x)])] to : P [Path TOUPiace AT(x)1)] John: N [Thing John] Mary: N [Thing Mary] Bill: N [Thing Bill] Figure 2: A sample learning session with MAIMRA. MAIMRA is given the two scenarios as input. Each scenario comprises linguistic information, in the form of a sequence of sentences, and non-linguistic information. The non-linguistic information is a sequence of conceptual structure [STATE] descriptions which describe a sequ</context>
<context position="15228" citStr="[15]" startWordPosition="2325" endWordPosition="2325">_), entry( john ,n , j ohn) entry(cup._,J, entry(to,p,at(x)), entry(slid,_,_), entry(mary,n,mary), entry(from,_,_), entry(bill,n,bill)]. entry (john,_,_) entry(to,_,_), entry(mary,_,_), entry(bill,_,_)]. maimra([tthe,cup,slid,from,john,toomary]], Lexicon, be(cup.at(john))Vbe(cup(at(mary))); be(cup.at(mary))Vbe(cup(at(john)))), maimra(Uthe,cup,slid,from,mary,to,bill]], Lexicon, be(cnp,at(mary))k-be(cup(at(bill))); be (cup , at (bill ) )k-be(cup (at (nary))) . Figure 4: A naive implementation of the cognitive architecture from Figure 3 using techniques similar to those used by Rayner et. al. in [15]. all syntactic structure sequences which correspond to the sentence sequence. Finally, the linking component is run in reverse to produce meanings of lexical items by correlating the syntactic and semantic structure sequences previously produced. The details of the factored representation, and the algorithms used to create it, will be discussed in Section 5. Several of the mental representations used by MAIMRA require a method for representing semantic information. We have chosen Jackendoff&apos;s theory of conceptual structure, presented in [6], as our model for semantic representation. It should</context>
</contexts>
<marker>[15]</marker>
<rawString>Manny Rayner, Asa Hugossort, and Goran Hagert. Using a logic grammar to learn a lexicon. Technical Report R88001, Swedish Institute of Computer Science, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon C Salveter</author>
</authors>
<title>Inferring conceptual graphs.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--4141</pages>
<contexts>
<context position="1502" citStr="[5, 7, 1, 16, 17, 13, 14]" startWordPosition="220" endWordPosition="226"> parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research. 1 Introduction Several natural language systems have been reported which learn the meanings of new words[5, 7, 1, 16, 17, 13, 14]. Many of these systems (in particular [5, 7, 1]) learn the new meanings based upon expectations arising from the morphological, syntactic, se&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholarship. Part of this research was performed while the author was visiting Xerox PARC as a research intern and as a consultant. mantic and pragmatic context of the unknown word in the text being processed. For example, if such a system encounters the sentence &amp;quot;I woke up yesterday, turned off my alarm clock, took a shower, and cooked myself two grimps for breakfast[51&amp;quot; it might conclude that grimps is a no</context>
<context position="43415" citStr="[16, 17]" startWordPosition="6877" endWordPosition="6878">traint solving across multiple scenarios. We believe that without cross scenario constraint solving, it is impossible to account for meaning acquisition and thus the local learning hypothesis is wrong. Our approach offers a viable alternative to the local learning hypothesis consistent with the observed short input buffer effect. 7 Related Work While most prior computational work on meaning acquisition focuses on contextual learning by scanning texts, some notable work has pursued a path similar to that described here attempting to learn from correlated linguistic and non-linguistic input. In [16, 17], Salveter describes a system called MORAN. The non-linguistic component of each scenario presented to MORAN consists of a sequence of exactly two scenes, where each scene is described by a conjunction of atomic formula. The linguistic component of each scenario is a preparsed case frame analysis of a single sentence describing the state change occurring between those two scenes. From each scenario in isolation, MORAN infers what Salveter calls a Conceptual Meaning Structure (CMS) which attempts to capture the essence of the meaning of the verb in the sentence. This CMS is a subset of the 4 We</context>
</contexts>
<marker>[16]</marker>
<rawString>Sharon C. Salveter. Inferring conceptual graphs. Cognitive Science, 3(4141-166, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon C Salveter</author>
</authors>
<title>Inferring building blocks for knowledge representation.</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing, chapter 12,</booktitle>
<pages>327--344</pages>
<editor>In Wendy G. Lehnert and Martin H. Ringle, editors,</editor>
<contexts>
<context position="1502" citStr="[5, 7, 1, 16, 17, 13, 14]" startWordPosition="220" endWordPosition="226"> parser, the linker and the inference component, make only linguistically and cognitively plausible assumptions about the innate knowledge needed to support tractable learning. The paper discusses the theory underlying the system, the representations and algorithms used in the implementation, the semantic constraints which support the heuristics necessary to achieve tractable learning, the limitations of the current theory and the implications of this work for language acquisition research. 1 Introduction Several natural language systems have been reported which learn the meanings of new words[5, 7, 1, 16, 17, 13, 14]. Many of these systems (in particular [5, 7, 1]) learn the new meanings based upon expectations arising from the morphological, syntactic, se&apos;Supported by an AT&amp;T Bell Laboratories Ph.D. scholarship. Part of this research was performed while the author was visiting Xerox PARC as a research intern and as a consultant. mantic and pragmatic context of the unknown word in the text being processed. For example, if such a system encounters the sentence &amp;quot;I woke up yesterday, turned off my alarm clock, took a shower, and cooked myself two grimps for breakfast[51&amp;quot; it might conclude that grimps is a no</context>
<context position="43415" citStr="[16, 17]" startWordPosition="6877" endWordPosition="6878">traint solving across multiple scenarios. We believe that without cross scenario constraint solving, it is impossible to account for meaning acquisition and thus the local learning hypothesis is wrong. Our approach offers a viable alternative to the local learning hypothesis consistent with the observed short input buffer effect. 7 Related Work While most prior computational work on meaning acquisition focuses on contextual learning by scanning texts, some notable work has pursued a path similar to that described here attempting to learn from correlated linguistic and non-linguistic input. In [16, 17], Salveter describes a system called MORAN. The non-linguistic component of each scenario presented to MORAN consists of a sequence of exactly two scenes, where each scene is described by a conjunction of atomic formula. The linguistic component of each scenario is a preparsed case frame analysis of a single sentence describing the state change occurring between those two scenes. From each scenario in isolation, MORAN infers what Salveter calls a Conceptual Meaning Structure (CMS) which attempts to capture the essence of the meaning of the verb in the sentence. This CMS is a subset of the 4 We</context>
</contexts>
<marker>[17]</marker>
<rawString>Sharon C. Salveter. Inferring building blocks for knowledge representation. In Wendy G. Lehnert and Martin H. Ringle, editors, Strategies for Natural Language Processing, chapter 12, pages 327-344. Lawrence Erlbaum Associates, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
</authors>
<title>The fourteen primitive actions and their inferences.</title>
<date>1973</date>
<booktitle>Memo AIM-183, Stanford Artificial Intelligence Laboratory,</booktitle>
<contexts>
<context position="15964" citStr="[18]" startWordPosition="2438" endWordPosition="2438">roduce meanings of lexical items by correlating the syntactic and semantic structure sequences previously produced. The details of the factored representation, and the algorithms used to create it, will be discussed in Section 5. Several of the mental representations used by MAIMRA require a method for representing semantic information. We have chosen Jackendoff&apos;s theory of conceptual structure, presented in [6], as our model for semantic representation. It should be stressed that although we represent conceptual structure via a decomposition into primitives much in the same way as does Schank[18], unlike both Schank and Jackendoff, we do not claim that any particular such decompositional theory is adequate as a basis for expressing the entire range of human thought and the meanings of even most words in the lexicon. Clearly, much of human experience is well beyond formalization within the current state of the art in knowledge representation. We are only concerned with representing and learning the meanings of words describing simple spatial movements of objects within the visual field of the learner. For this limited task, a primitive decompositionsl theory such as Jackendoff&apos;s seems </context>
</contexts>
<marker>[18]</marker>
<rawString>Roger C. Schank. The fourteen primitive actions and their inferences. Memo AIM-183, Stanford Artificial Intelligence Laboratory, March 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time 0(n3). Information and Control,</booktitle>
<pages>10--4189</pages>
<contexts>
<context position="32709" citStr="[8, 19]" startWordPosition="5186" endWordPosition="5187">ibe that partition. For our example, this results in the replacements given in Figure 8. The disjunction that remains after these replacements describes all possible sequences comprised of two [EVENTS] or [STATES] that can explain the input scene sequence. Notice how non-determinism is managed with a factored representation produced directly by the algorithm. After the inference component produces the semantic structure sequences corresponding to the non-linguistic input, the parser produces the syntactic structure sequences corresponding to the linguistic input. A variant of the CKY algorithm[8, 19] is used to produce factored parse trees. Finally, the linker is applied in reverse to each corresponding parse-tree/semantic-structure pair. This inverse linking process is termed fracturing. Fracturing is a recursive process applied to a parse tree fragment and a conceptual structure fragment. At each step, the conceptual structure fragment is assigned to the root node of the parse tree fragment. If the root node of the parse tree has n non-head daughters, then compute all possible ways of extracting n variable-free subexpressions from the conceptual structure fragment and assigning them to </context>
</contexts>
<marker>[19]</marker>
<rawString>D. H. Younger. Recognition and parsing of context-free languages in time 0(n3). Information and Control, 10(4189-208, 1967.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>