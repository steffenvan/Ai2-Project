<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.9985305">
Japanese Zero Pronoun Resolution based on
Ranking Rules and Machine Learning
</title>
<author confidence="0.738718">
Hideki Isozaki and Tsutomu Hirao
</author>
<affiliation confidence="0.5590845">
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
</affiliation>
<address confidence="0.933803">
2-4 Hikaridai, Seika-cho, Souraku-gun, Kyoto, Japan, 619-0237
</address>
<email confidence="0.999189">
(isozaki,hirao)@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991888">
Anaphora resolution is one of the most
important research topics in Natural Lan-
guage Processing. In English, overt pro-
nouns such as she and definite noun
phrases such as the company are anaphors
that refer to preceding entities (an-
tecedents). In Japanese, anaphors are of-
ten omitted, and these omissions are called
zero pronouns. There are two major ap-
proaches to zero pronoun resolution: the
heuristic approach and the machine learn-
ing approach. Since we have to take var-
ious factors into consideration, it is diffi-
cult to find a good combination of heuris-
tic rules. Therefore, the machine learn-
ing approach is attractive, but it requires
a large amount of training data. In this
paper, we propose a method that com-
bines ranking rules and machine learning.
The ranking rules are simple and effective,
while machine learning can take more fac-
tors into account. From the results of our
experiments, this combination gives better
performance than either of the two previ-
ous approaches.
</bodyText>
<sectionHeader confidence="0.998872" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999801297297297">
Anaphora resolution is an important research topic
in Natural Language Processing. For instance,
machine translation systems should identify an-
tecedents of anaphors (such as he or she) in the
source language to achieve better translation quality
in the target language.
We are now studying open-domain question an-
swering systems1, and we expect QA systems to
benefit from anaphora resolution. Typical QA sys-
tems try to answer a user’s question by finding rel-
evant phrases from large corpora. When a correct
answer phrase is far from the keywords given in
the question, the systems will not succeed in find-
ing the answer. If the system can correctly resolve
anaphors, it will find keywords or answers repre-
sented by anaphors, and the chances of finding the
answer will increase. From this motivation, we are
developing our system toward the ability to resolve
anaphors in full-text newspaper articles.
In Japanese, anaphors are often omitted and these
omissions are called zero pronouns. Since they do
not give any hints (e.g., number or gender) about an-
tecedents, automatic zero pronoun resolution is dif-
ficult. In this paper, we focus on resolving the zero
pronoun, which is shortened for simplicity to ‘zero.’
Most studies on Japanese zero pronoun resolution
have not tried to resolve zeros in full-text newspa-
per articles. They have discussed simple sentenses
(Kameyama, 1986; Walker et al., 1994; Yamura-
Takei et al., 2002), dialogues (Yamamoto et al.,
1997), stereotypical lead sentences of newspaper ar-
ticles (Nakaiwa and Ikehara, 1993), intrasentential
resolution (Nakaiwa and Ikehara, 1996; Ehara and
Kim, 1996) or organization names in newspaper ar-
ticles (Aone and Bennett, 1995).
There are two approaches to the problem: the
heuristic approach and the machine learning ap-
</bodyText>
<footnote confidence="0.879779">
1http://trec.nist.gov/data/qa.html
</footnote>
<bodyText confidence="0.987917972972973">
proach. The Centering Theory (Grosz et al., 1995)
is important in the heuristic approach. Walker
et al. (1994) proposed forward center ranking for
Japanese. Kameyama (1986) emphasized the im-
portance of a property-sharing constraint. Okumura
and Tamura (1996) experimented on the roles of
conjunctive postpositions in complex sentences.
However, these improvements are not sufficient
for resolving zeros accurately. Murata and Na-
gao (1997) proposed complicated heuristic rules that
take various features of antecedents and anaphors
into account. We have to take even more factors into
account, but it is difficult to maintain such heuris-
tic rules. Therefore, recent studies employ machine
learning approaches. However, it is also difficult to
prepare a sufficient number of annotated corpora.
In this paper, we propose a method that com-
bines these two approaches. Heuristic ranking rules
give a general preference, while a machine learn-
ing method excludes inappropriate antecedent can-
didates. From the results of our experiments, the
proposed method shows better performance than ei-
ther of the two approaches alone.
Before giving a description of our methodology,
we briefly introduce the grammar of the Japanese
language here. A Japanese sentence is a sequence
of bunsetsus: . A bunsetsu is a se-
quence of content words (e.g., nouns, adjectives,
and verbs) followed by zero or more functional
words (e.g., particles and auxiliary verbs):
. A bunsetsu modifies one of
the following bunsetsus. A particle (joshi) marks the
grammatical case of the noun phrase immediately
before it. For example, ga is nominative (subject),
wo is accusative (object), ni is dative (object2), and
wa marks a topic.
Tomu ga / Bobu ni / hon wo / okutta.
</bodyText>
<subsectionHeader confidence="0.622669">
Tom=subj Bob=object2 book=object sent
</subsectionHeader>
<bodyText confidence="0.991831138888889">
(Tom sent a book to Bob.)
Bunsetsu dependency is represented by a list of
bunsetsu pairs (modifier, modified). For instance,
indicates that there
are four bunsetsus in this sentence and that the first
bunsetsu modifies the fourth bunsetsu and so on.
The last bunsetsu modifies no bunsetsu, which is in-
dicated by .
It takes a long time to construct high-quality an-
notated data, and we want to compare our results
with conventional methods. Therefore, we obtained
Seki’s data (Seki et al., 2002a; Seki et al., 2002b),
which are based on the Kyoto University Corpus 2
2.0. These data are divided into two groups: gen-
eral and editorial. General contains 30 general news
articles, and editorial contains 30 editorial articles.
According to his experiments, editorial is harder
than general. Perhaps this is caused by the differ-
ence in rhetorical styles and the lengths of articles.
The average number of sentences in an editorial ar-
ticle is 28.7, while that in a general article is 13.9.
However, we found problems in his data. For
instance, the data contained ambiguous antecedents
like dou-shi (the same person) or dou-sha (the same
company) as correct antecedents. We replaced these
‘correct answers’ with their explicit names. We also
removed zeros in quoted sentences because they are
quite different from other sentences.
In addition, we decided to use the output of
ChaSen 2.2.93 and CaboCha 0.344 instead of the
morphological information and the dependency in-
formation provided by the Kyoto Corpus since clas-
sification of the joshi (particles) in the Corpus was
not satisfactory for our purpose. Since CaboCha
was trained by Kyoto Corpus 3.0, CaboCha’s depen-
dency output is very similar to that of the Corpus.
</bodyText>
<sectionHeader confidence="0.994204" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.9990705">
In this paper, we combine heuristic ranking rules and
machine learning. First, we describe how we ex-
tract possible antecedents (candidates). Second, we
describe the rule-based ranking system and the ma-
chine learning system. Finally, we describe how to
combine these two methods.
We consider only anaphors for noun phrases fol-
lowing Seki and other studies. We assume that zeros
are already detected. We also assume zeros are lo-
cated at the starting point of a bunsetsu that contains
a yougen (a verb, an adjective, or an auxiliary verb).
From now on, we use ‘verb’ instead of ‘yougen’ for
readability. A zero’s bunsetsu is a bunsetsu that con-
tains the zero. We further assume that each zero’s
grammatical case is already determined by a zero
detector and represented by corresponding particles.
</bodyText>
<footnote confidence="0.999421333333333">
2http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html
3http://chasen.aist-nara.ac.jp/
4http://cl.aist-nara.ac.jp/˜taku-ku/software/cabocha/
</footnote>
<bodyText confidence="0.999427043478261">
If a zero is the subject of a verb, its case is repre-
sented by the particle ga. If it is an object, it is rep-
resented by wo. If it is an object2, it is represented
by ni. We consider only these three cases. A zero’s
particle means such a particle.
Since complex sentences are hard to analyze, each
sentence is automatically split at conjunctive post-
positions (setsuzoku joshi) (Okumura and Tamura,
1996; Ehara and Kim, 1996). In order to distinguish
the original complex sentence and the simpler sen-
tences after the split, we call the former just a ‘sen-
tence’ and the latter ‘post-split sentences’. When a
conjunctive postposition appears in a relative clause,
we do not split the sentence at that position. In the
examples below, we split the first sentence at ‘and’
but do not split the second sentence at ‘and’.
She bought the book and sold it to him.
She bought the book that he wrote and sold.
A zero’s sentence is the (original) sentence that
contains the zero. From now on, stands for a zero
and stands for a candidate of ’s antecedent. ’s
particle is denoted ZP, and CP stands for ’s next
word that is ’s particle or a punctuation symbol.
</bodyText>
<subsectionHeader confidence="0.997759">
2.1 Enumeration of possible antecedents
</subsectionHeader>
<bodyText confidence="0.999926766666667">
Candidates (possible antecedents) are enumerated
on the fly by using the following method.
For this task, we use a named entity recognizer
(Isozaki and Kazawa, 2002).
The first step extracts a content word sequence
from a bunsetsu. The second step excludes verb
phrases, adjective phrases, and clauses. As a re-
sult, we obtain only noun phrases. The third step ex-
cludes adverbial expressions like kotoshi (this year).
The forth step resolves anaphors like definite noun
phrases in English. We should also resolve pro-
nouns, but we did not because useful pronouns are
rare in newspaper articles.
In addition, we register a resolved zero as a new
candidate. If ’s antecedent is determined to be ,
a new candidate is created for future zeros. is
a copy of except that ’s particle is ZP and ’s
location is ’s location. In the training phase of the
machine learning approach, we consider a correct
answer as . Then, we can remove far candidates
from the list.
In this way, our zero resolver creates a ‘general
purpose’ candidate list. However, some of the can-
didates are inappropriate for certain zeros. A verb
usually does not have the same entity in two or more
cases (Murata and Nagao, 1997). Therefore, our
resolver excludes candidates that are filled in other
cases of the verb. When a verb has two or more
zeros, we resolve ga first, and its best candidate is
excluded from the candidates of wo or ni.
</bodyText>
<listItem confidence="0.798381333333333">
1. We extract a content word sequence
as a candidate if it is fol-
lowed by a case marker (kaku-joshi, e.g., ga,
wo), a topic marker (wa or mo), or a period.
2. If ’s is a verb, an adjective, an auxi-
lary verb, an adverb, or a relative pronoun
(ChaSen’s meishi-hijiritsu, e.g., koto (what he
did) and toki (when she married)), is ex-
cluded. (If is a closing quotation mark,
is checked instead.)
3. If ’s is a pronoun or an adverbial noun (a
noun that can also be used as an adverb, i.e.,
ChaSen’s meishi-fukushi-kanou), is excluded.
4. If is dou-shi (the person), it is replaced by
the latest person name. If is dou-sha (the
company), it is replaced by the latest organi-
zation name. If is dou+suffix, it is replaced
by the latest candidate that has the same suffix.
</listItem>
<subsectionHeader confidence="0.999746">
2.2 Ranking rules
</subsectionHeader>
<bodyText confidence="0.999622">
Various heuristics have been reported in past litera-
ture. Here, we use the following heuristics.
</bodyText>
<listItem confidence="0.953156461538461">
1. Forward center ranking (Walker et al., 1994):
(topic empathy subject object2 object
others).
2. Property-sharing (Kameyama, 1986): If a zero
is the subject of a verb, its antecedent is perhaps
a subject in the antecedent’s sentence. If a zero
is an object, its antecedent is perhaps an object.
3. Semantic constraints (Yamura-Takei et al.,
2002; Yoshino, 2001): If a zero is the sub-
ject of ‘eat,’ its antecedent is probably a per-
son or an animal, and so on. We use Nihongo
Goi Taikei (Ikehara et al., 1997), which has
14,730 English-to-Japanese translation patterns
</listItem>
<bodyText confidence="0.989646433333333">
for 6,103 verbs, to check the acceptability of a
candidate. Goi Taikei also has 300,000 words
in about 3,000 semantic categories. (See Ap-
pendix A for details.)
4. Demotion of candidates in a relative clause
(rentai shuushoku setsu): Usually, Japanese ze-
ros do not refer to noun phrases in relative
clauses (Ehara and Kim, 1996). (See Appendix
B for details.)
Since sentences in newspaper articles are often
complex and relative clauses are sometimes nested,
we refine this rule in the following way.
A candidate’s relative clause is the inmost rel-
ative clause that contains the candidate.
A relative clause finishes at the noun modified
by the clause.
If appears before the finishing noun of ’s rel-
ative clause, the clause is still unfinished at .
Otherwise, the clause is already finished.
A quoted clause (with or without quotation
marks “ ”) indicated by a quotation marker ‘to’
(‘that’ in ‘He said that she is ...’) is also re-
garded as a relative clause.
We demote after ’s relative clause finishes.
It is not clear how to combine the above heuris-
tics consistently. Here, we sort the candidates in
a lexicographical order based on the above fea-
tures of candidates. For instance, we can use
a lexicographically increasing order defined by
Vi Re Ag Di Sa, where
Vi (for violation) is 1 if the candidate violates
the semantic constraint. Otherwise, Vi is 0.
Re (for relative) is 1 if the candidate is in a rel-
ative clause that has already finished before .
Otherwise, Re is 0.
Ag (for agreement) is 0 if CP=ZP holds. (Since
most of wa and mo are subjects, they are re-
garded as ga here.) Otherwise, Ag is 1.
Di (for distance) is a non-negative integer that
represents the number of post-split sentences
between and . If a candidate’s Di is larger
than maxDi, it is removed from the candidate
list.
Sa (for salience) is 0 if CP is wa. Sa is 1 if CP
is ga. Sa is 2 if CP is ni. Sa is 3 if CP is wo.
Otherwise, Sa is 4. We did not implement em-
pathy because it makes the program more com-
plex, and empathy verbs are rare in newspaper
articles.
For instance, holds.
The first ranked (lexicographically smallest) candi-
date is regarded as the best candidate. We employ
lexicographical ordering because it seems the sim-
plest way to rank candidates. We put Vi in the
first place because Vi was often regarded as a con-
straint in the past literature. We put Ag before
Sa because Kameyama’s method was better than
Walker’s in Okumura and Tamura (1996). There-
fore, Vi Ag Sa is expected to be a good
ordering. The above ordering is an instance of this.
</bodyText>
<subsectionHeader confidence="0.994397">
2.3 Machine Learning
</subsectionHeader>
<bodyText confidence="0.970291303030303">
Although we can consider various other features
for zero pronoun resolution, it is difficult to com-
bine these features consistently. Therefore, we
use machine learning. Support Vector Machines
(SVMs) have shown good performance in various
tasks in Natural Language Processing (Kudo and
Matsumoto, 2001; Isozaki and Kazawa, 2002; Hi-
rao et al., 2002).
Yoshino (2001) and Iida et al.(2003b) also applied
SVM to Japanese zero pronoun resolution, but the
usefulness of each feature was not clear. Here, we
add features for complex sentences and analyze use-
ful features by examining the weights of features.
We use the following features of as well as CP.
CSem ’s semantic categories. (See Appendix A.)
CPPOS CP’s part-of-speech (POS) tags (rough
and detailed).
CPOS The POS tags of the last word of .
Siblings When CP is wa or mo, it is not clear
whether is a subject. However, a verb rarely has
the same entity in two or more cases. Therefore, if
modifies a verb that has a subject, is not a subject.
In the next example, hon is an object of katta.
Ano / hon wa / Tomu ga / katta.
that book=topic Tom=subj bought
(As for that book, Tom bought it.)
In order to learn such things, we use sibling case-
markers that modify the same verb as ’s features.
We also use the following features of as well as
ZP.
Conjunct The latest conjunctive postposition in
the sentence and its classification (Okumura and
Tamura, 1996; Yoshimoto, 1986).
ZSem Semantic categories of the verb that mod-
ifies. We use them only when the verb is sahen
meishi + ‘suru.’ Sahen meishi is a kind of noun that
can be an object of the verb ‘suru’ (do) (e.g., ‘shop-
ping’ in ‘do the shopping’).
We also use the following relations between and
as well as Ag, Vi, and Di.
Relative Whether is in a relative clause.
Unfinished Whether the relative clause is unfin-
ished at .
Intra (for intrasentential coreference) Whether
explicitly appears in ’s sentence.
Sometimes it is difficult to distinguish cataphora
from anaphora. Even if an antecedent appears in a
preceding sentence, it is sometimes easier to find a
candidate after , as illustrated by the case of ‘his’
in the next English example.
Bob and John separately drove to Charlie’s
house.... Since his car broke down, John made a
phone call.
Even if Di holds, Intra does not necessarily
hold because we introduce resolved zeros as new
candidaites.
Parallel Whether appears in a clause parallel to
a clause in which a zero appears. This will be useful
for the resolution of a zero as with ‘it’ in the next
English sentence.
He turned on the TV set and she turned it off.
Immediate Whether ’s bunsetsu appears imme-
diately before ’s. In the following sentence, a can-
didate ryoushin is located immediately before the
zero.
Kare no / ryoushin wa /
</bodyText>
<equation confidence="0.757394">
he+’s parents=topic
( ga) ikiteiru to / shinjiteiru.
( =subj) alive+that believe
</equation>
<bodyText confidence="0.987522">
(His parents believe that ( ) is still alive.)
Here, we represent all of the above features by a
boolean value: 0 or 1. Semantic categories can be
represented by a 0/1 vector whose -th component
corresponds to the -th semantic category. Similarly,
POS tags can be represented by a 0/1 vector whose
-th component corresponds to the -th POS tag. On
the other hand, Di has a non-negative integer value.
We also encode the distance by a 0/1 vector whose
-th component corresponds to the fact that the dis-
tance is . The distance has an upper bound maxDi.
In this way, we can represent a candidate by a
boolean feature vector. A candidate ’s feature vec-
tor is denoted . If a boolean feature appears only
once in the given data, we remove the feature from
the feature vectors.
The training data comprise the set of pairs
, where is if is a correct antecedent
of a zero. Otherwise, is . By using the train-
ing data, SVM finds a decision function
, where is the feature vector
of a candidate and s are support vectors selected
from the training data. is a constant. is
called a kernel function. If holds, is
classified as a correct antecedent.
</bodyText>
<subsectionHeader confidence="0.980968">
2.4 Combinations
</subsectionHeader>
<bodyText confidence="0.996016">
Here, we use the following method to combine the
ordering and SVM.
</bodyText>
<listItem confidence="0.9578875">
1. Sort candidates by using the lexicographical or-
der.
2. Classify each candidate by using SVM in this
order.
3. If is positive, stop there and sort the eval-
uated candidates by in decreasing order.
4. If no candidate satisfies , return the
best candidate in terms of
</listItem>
<sectionHeader confidence="0.999846" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.956146666666667">
We conducted leave-one(-article)-out experiments.
For each article, 29 other articles were used for
training. Table 1 compares the scores of the above
methods. ‘First’ picks up the first candidate given
by a given lexicographical ordering. The acronym
‘vrads’ stands for the lexicographical ordering of
Vi Re Ag Di Sa. ‘Best’ picks up the best can-
didate in terms of without checking whether it
.
</bodyText>
<tableCaption confidence="0.984196">
Table 1: Percentage of correctly resolved zeros
</tableCaption>
<bodyText confidence="0.7144385">
= The combination is worse than ‘first’ or ‘best.’
= (Seki et al., 2002a), = (Seki et al., 2002b)
</bodyText>
<table confidence="0.998311636363636">
general editorial
first mem svm1 svm2 first mem svm1 svm2
best 51.0 56.8 55.9 43.4 45.1 45.1
vrads 64.3 53.0 58.5 66.3 45.3 44.0 45.9 47.3
vards 64.0 53.0 58.5 66.0 45.9 44.2 45.9 46.9
rvads 63.4 51.0 58.5 66.3 44.4 43.4 46.1 47.5
avrds 62.8 53.0 58.5 66.0 44.2 44.0 45.9 46.9
vrdsa 55.9 53.0 58.5 65.7 43.4 44.0 45.9 48.6
adsvr 53.0 51.0 57.9 62.8 43.8 43.4 46.3 48.6
davrs 39.5 53.0 57.6 62.5 34.6 44.2 46.1 50.2
Seki 54.0 50.7 39.8
</table>
<bodyText confidence="0.99926356">
is positive. Consequently, it is independent of the
ordering (unless two or more candidates have the
best value). ‘Svm1’ uses the ordinary SVM (Vap-
nik, 1995) while ‘svm2’ uses a modified SVM for
unbalanced data (Morik et al., 1999), which gives
a large penalty to misclassification of a minority (=
positive) example.5 In general, svm2 accepts more
cadidates than svm1. According to this table, svm1
is too severe to exclude only bad candidates. We
also tried the maximum entropy model 6 (mem) and
C4.5, but they were also too severe.
When we use SVM, we have to choose a good
kernel for better performance. Here, we used the
linear kernel ( ) for SVM because it
was best according to our preliminary experiments.
We set maxDi at 3 because it gave the best results.
The table also shows Seki’s scores for reference,
but it is not fair to compare our scores with Seki’s
scores directly because our data is slightly different
from Seki’s. The number of zeros in general in our
data is 347, while Seki resolved 355 detected ze-
ros in (Seki et al., 2002a) and 404 in (Seki et al.,
2002b). The number of zeros in our editorial is
514, while (Seki et al., 2002a) resolved 498 detected
zeros. In order to overcome the data sparseness,
</bodyText>
<footnote confidence="0.8333625">
5An ordinary SVM minimizes while
the modified SVM minimizes
where = number of negative exam-
ples/number of positive examples.
6http://www2.crl.go.jp/jt/a132/members/mutiyama/software.
html
</footnote>
<bodyText confidence="0.998676838709677">
Seki used unannotated articles to get co-occurrence
statistics. Without the data, their scores degraded
about 5 points. We have not conducted experiments
that use unannotated corpora; this task is our future
work.
As we expected, instances ofVi Ag Sa
show good performance. Without SVMs, ‘vrads’
is the best for general in the table. It is interest-
ing that such a simple ordering gives better perfor-
mance than SVMs. However, the combination of
‘vrads’ and ‘svm2’ (= vrads+svm2) gives even bet-
ter results. In general, ‘ +svm2’ is better than ‘first’
and ‘ +svm1.’ With SVM, ‘davrs+svm2’ gave the
best result for editorial. Editorial articles some-
times use anthropomorphism (e.g., The report says
... ) that violates semantic constraints. Therefore,
‘vrads’ does not work well for such cases.
Table 2 shows the weights of the above features
determined by svm2 for a fold of the leave-one-
out experiment of ‘vrads+svm2.’ The weights can
be given by rewriting as
. This table shows that Kameyama’s
property-sharing (Ag), semantic violation (Vi), can-
didate’s particle (CP), and distance (Di) are very
important features. Our new features Parallel, Un-
finished, and Intra also obtained relatively large
weights. Semantic categories ‘suggestions’ and ‘re-
port’ reflect the fact that some articles use anthro-
pomorphism. These weights will be useful to de-
sign better heuristic rules. The fact that Unfinished’s
weight almost cancels Relative’s weight justifies the
</bodyText>
<tableCaption confidence="0.980021">
Table 2: Weights of features
</tableCaption>
<table confidence="0.99238815">
general editorial
Ag=0 Ag=0
ZP=ni Parallel
concrete CSem Di=0
CP=ga Intra
Intra CP=ga
agents CSem suggestion CSem
CP=wa report CSem
Di=0 agents CSem
Parallel concrete CSem
Unfinished
CP=wa
Unfinished
Relative
CP=mo CPPOS=‘case marker’
CP=no Relative
ZP=wo CP=no
Di=3 Di=3
Vi=1 Vi=1
definition of Re.
</table>
<sectionHeader confidence="0.999116" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999971965517241">
Yoshino (2001) used an ordinary SVM with
. He tried to find use-
ful features by feature elimination. Since features
are not completely independent, removing a heav-
ily weighted feature does not necessarily degrade the
system’s performance. Hence, feature elimination is
more reliable for reducing the number of features.
However, feature elimination takes a long time. On
the other hand, feature weights can give rough guid-
ance. According to the table, our new features (Par-
allel, Unfinished, and Intra) obtained relatively large
weights. This implies their importance. When we
eliminated these three features, vrads+svm2’s score
for editorial dropped by 4 points. Therefore, combi-
nations of these three features are useful.
Recently, Iida et al. (2003a) proposed an SVM-
based tournament model that compares two candi-
dates and selects the better one. We would like to
compare or combine their method with our method.
For further improvement, we have to make the mor-
phological analyzer and the dependency analyzer
more reliable because they make many mistakes
when they process complex sentences.
SVM has often been criticized as being too slow.
However, the above data were small enough for the
state-of-the-art SVM programs. The number of ex-
amples in each set of training data was about 5,000–
6,100, and each training phase took only 5–18 sec-
onds on a 2.4-GHz Pentium 4 machine.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999993636363636">
In order to make Japanese zero pronoun resolu-
tion more reliable, we have to maintain complicated
heuristic rules or prepare a large amount of training
data. In order to alleviate this problem, we com-
bined simple lexicographical orderings and SVMs.
It turned out that a simple lexicographical ordering
performed better than SVM, but their combination
gave even better performance. By examining feature
weights, we found that features for complex sen-
tences are important in zero pronoun resolution. We
confirmed this by feature elimination.
</bodyText>
<sectionHeader confidence="0.997825" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998230953488372">
Chinatsu Aone and Scott William Bennett. 1995. Evalu-
ating automated and manual acquisition of anaphora
resolution strategies. In Proc. of ACL-1995, pages
122–129.
Terumasa Ehara and Yeun-Bae Kim. 1996. Zero-subject
resolution by probabilistic model (in Japanese). Jour-
nal ofNatural Language Processing, 3(4):67–86.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–226.
Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji
Matsumoto. 2002. Extracting important sentences
with support vector machines. In Proc. of COLING-
2002, pages 342–348.
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Mat-
sumoto. 2003a. Incorporating contextual cues in
trainable models for coreference resolution (to ap-
pear). In Proc. of EACL Workshop on the Computa-
tional Treatment ofAnaphora.
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003b. One method for resolving
Japanese zero pronouns with machine learning model
(in Japanese). In IPSJ SIG-NL 154.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei —
A Japanese Lexicon (in Japanese). Iwanami Shoten.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proc. of COLING-2002, pages 390–396.
Megumi Kameyama. 1986. A property-sharing con-
straint in centering. In Proc. ofACL-1986, pages 200–
206.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proc. of NAACL-2001,
pages 192–199.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning with
a knowledge-based approach — a case study in inten-
sive care monitoring. In Proc. of ICML-1999, pages
268–277.
Masaki Murata and Makoto Nagao. 1997. An estimate
of referents of pronouns in Japanese sentences using
examples and surface expressions (in Japanese). Jour-
nal ofNatural Language Processing, 4(1):41–56.
Hiromi Nakaiwa and Satoru Ikehara. 1993. Zero pro-
noun resolution in a Japanese to English machine
translation system using verbal semantic attributes (in
Japanese). Transaction of the Information Processing
Society ofJapan, 34(8):1705–1715.
Hiromi Nakaiwa and Satoru Ikehara. 1996. Intrasenten-
tial resolution of Japanese zero pronouns using prag-
matic and semantic constraints (in Japanese). Journal
ofNatural Language Processing, 3(4):49–65.
Manabu Okumura and Kouji Tamura. 1996. Zero pro-
noun resolution based on centering theory. In Proc. of
COLING-1996, pages 871–876.
Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002a. Japanese zero pronoun resolution using a prob-
abilistic model (in Japanese). Journal ofNatural Lan-
guage Processing, 9(3):63–85.
Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002b. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and reso-
lution. In Proc. of COLING-2002, pages 911–917.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
Marilyn Walker, Masayo Iida, and Sharon Cote. 1994.
Japanese discourse and the process of centering. Com-
putational Linguistics, 20(2):193–233.
Kazuhide Yamamoto, Eiichiro Sumita, Osamu Furuse,
and Hitoshi Iida. 1997. Ellipsis resolution in dia-
logues via decision-tree learning. In Proc. of NLPRS-
1997, pages 423–428.
Mitsuko Yamura-Takei, Miho Fujiwara, Makoto Yoshie,
and Teruaki Aizawa. 2002. Automatic linguistic anal-
ysis for language teachers: The case of zeros. In Proc.
of COLING-2002, pages 1114–1120.
Kei Yoshimoto. 1986. Study of Japanese zero pronouns
in discourse processing (in Japanese). In IPSJ SIG
notes, NL-56-4, pages 1–8.
Keiichi Yoshino. 2001. Anaphora resolution of Japanese
zero pronouns using machine learning (in Japanese).
Master’s thesis, Nara Institute of Science and Technol-
ogy.
</reference>
<sectionHeader confidence="0.757066" genericHeader="method">
Appendix A: Semantic constraint check
</sectionHeader>
<bodyText confidence="0.999959857142857">
One word may belong to two or more semantic cate-
gories, and each semantic category has superclasses
(e.g., ‘father’ has the superclass ‘parent’). There-
fore, we keep all of these categories and their su-
perclasses in a category list for the candidate. If the
candidate is not registered in Goi Taikei and can be
decomposed into shorter words, we use the seman-
tic categories of the last candidate word because the
last word is usually the head word.
Furthermore, we use named entity recognition.
When the candidate contains a person name, an or-
ganization name, or a location name, a correspond-
ing semantic category is added to the list.
A verb may have two or more translation patterns.
Here, we use disjunction of the constraints. For in-
stance, the verb ‘yomu’ (to read) has three transla-
tion patterns. The first and second patterns’ subjects
are restricted to AGENT, and the third pattern’s sub-
ject is restricted to PEOPLE. Therefore, the subject
of yomu is accepted if and only if it is AGENT or
PEOPLE.
</bodyText>
<sectionHeader confidence="0.909514" genericHeader="method">
Appendix B: Relative clause analysis
</sectionHeader>
<bodyText confidence="0.97251425">
We have to be careful about parallel structures for
this analysis. According to CaboCha, Kare ga in the
next example modifies a verb katte, which modifies
another verb karita. However, katte is contained in
a clause that modifies the noun hon.
Kare ga / katte / kanojo ga / karita /
he=subj bought she=subj borrowed
hon wa / omoshiroi .
</bodyText>
<subsectionHeader confidence="0.465059">
book=topic interesting
</subsectionHeader>
<bodyText confidence="0.994713285714286">
(The book that he bought and she borrowed is
interesting.)
The particle no (= “’s” in English) directly modi-
fies a noun. For instance, Taro in Taro no hon (Taro’s
book) is a book that Taro wrote or a book that Taro
has. From this point of view, we also mark A in A no
B (A’s B) as a candidate in a relative clause.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.645707">
<title confidence="0.9963865">Japanese Zero Pronoun Resolution based Ranking Rules and Machine Learning</title>
<author confidence="0.741465">Isozaki</author>
<affiliation confidence="0.8382585">NTT Communication Science Nippon Telegraph and Telephone</affiliation>
<address confidence="0.991096">2-4 Hikaridai, Seika-cho, Souraku-gun, Kyoto, Japan,</address>
<email confidence="0.994963">(isozaki,hirao)@cslab.kecl.ntt.co.jp</email>
<abstract confidence="0.998738615384616">Anaphora resolution is one of the most important research topics in Natural Language Processing. In English, overt prosuch as definite noun such as company anaphors refer to preceding entities In Japanese, anaphors are often omitted, and these omissions are called There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach. Since we have to take various factors into consideration, it is difficult to find a good combination of heuristic rules. Therefore, the machine learning approach is attractive, but it requires a large amount of training data. In this paper, we propose a method that combines ranking rules and machine learning. The ranking rules are simple and effective, while machine learning can take more factors into account. From the results of our experiments, this combination gives better performance than either of the two previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Scott William Bennett</author>
</authors>
<title>Evaluating automated and manual acquisition of anaphora resolution strategies.</title>
<date>1995</date>
<booktitle>In Proc. of ACL-1995,</booktitle>
<pages>122--129</pages>
<contexts>
<context position="2986" citStr="Aone and Bennett, 1995" startWordPosition="463" endWordPosition="466">automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic ru</context>
</contexts>
<marker>Aone, Bennett, 1995</marker>
<rawString>Chinatsu Aone and Scott William Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. In Proc. of ACL-1995, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terumasa Ehara</author>
<author>Yeun-Bae Kim</author>
</authors>
<title>Zero-subject resolution by probabilistic model (in Japanese).</title>
<date>1996</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="2917" citStr="Ehara and Kim, 1996" startWordPosition="452" endWordPosition="455">do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros </context>
<context position="7972" citStr="Ehara and Kim, 1996" startWordPosition="1249" endWordPosition="1252">tector and represented by corresponding particles. 2http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html 3http://chasen.aist-nara.ac.jp/ 4http://cl.aist-nara.ac.jp/˜taku-ku/software/cabocha/ If a zero is the subject of a verb, its case is represented by the particle ga. If it is an object, it is represented by wo. If it is an object2, it is represented by ni. We consider only these three cases. A zero’s particle means such a particle. Since complex sentences are hard to analyze, each sentence is automatically split at conjunctive postpositions (setsuzoku joshi) (Okumura and Tamura, 1996; Ehara and Kim, 1996). In order to distinguish the original complex sentence and the simpler sentences after the split, we call the former just a ‘sentence’ and the latter ‘post-split sentences’. When a conjunctive postposition appears in a relative clause, we do not split the sentence at that position. In the examples below, we split the first sentence at ‘and’ but do not split the second sentence at ‘and’. She bought the book and sold it to him. She bought the book that he wrote and sold. A zero’s sentence is the (original) sentence that contains the zero. From now on, stands for a zero and stands for a candidat</context>
<context position="11909" citStr="Ehara and Kim, 1996" startWordPosition="1939" endWordPosition="1942">s perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Appendix A for details.) 4. Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese zeros do not refer to noun phrases in relative clauses (Ehara and Kim, 1996). (See Appendix B for details.) Since sentences in newspaper articles are often complex and relative clauses are sometimes nested, we refine this rule in the following way. A candidate’s relative clause is the inmost relative clause that contains the candidate. A relative clause finishes at the noun modified by the clause. If appears before the finishing noun of ’s relative clause, the clause is still unfinished at . Otherwise, the clause is already finished. A quoted clause (with or without quotation marks “ ”) indicated by a quotation marker ‘to’ (‘that’ in ‘He said that she is ...’) is also</context>
</contexts>
<marker>Ehara, Kim, 1996</marker>
<rawString>Terumasa Ehara and Yeun-Bae Kim. 1996. Zero-subject resolution by probabilistic model (in Japanese). Journal ofNatural Language Processing, 3(4):67–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="3163" citStr="Grosz et al., 1995" startWordPosition="487" endWordPosition="490">noun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account. We have to take even more factors into account, but it is difficult to maintain such heuristic rules. Th</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Hideki Isozaki</author>
<author>Eisaku Maeda</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting important sentences with support vector machines.</title>
<date>2002</date>
<booktitle>In Proc. of COLING2002,</booktitle>
<pages>342--348</pages>
<contexts>
<context position="14489" citStr="Hirao et al., 2002" startWordPosition="2397" endWordPosition="2401">n regarded as a constraint in the past literature. We put Ag before Sa because Kameyama’s method was better than Walker’s in Okumura and Tamura (1996). Therefore, Vi Ag Sa is expected to be a good ordering. The above ordering is an instance of this. 2.3 Machine Learning Although we can consider various other features for zero pronoun resolution, it is difficult to combine these features consistently. Therefore, we use machine learning. Support Vector Machines (SVMs) have shown good performance in various tasks in Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Hirao et al., 2002). Yoshino (2001) and Iida et al.(2003b) also applied SVM to Japanese zero pronoun resolution, but the usefulness of each feature was not clear. Here, we add features for complex sentences and analyze useful features by examining the weights of features. We use the following features of as well as CP. CSem ’s semantic categories. (See Appendix A.) CPPOS CP’s part-of-speech (POS) tags (rough and detailed). CPOS The POS tags of the last word of . Siblings When CP is wa or mo, it is not clear whether is a subject. However, a verb rarely has the same entity in two or more cases. Therefore, if modif</context>
</contexts>
<marker>Hirao, Isozaki, Maeda, Matsumoto, 2002</marker>
<rawString>Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Matsumoto. 2002. Extracting important sentences with support vector machines. In Proc. of COLING2002, pages 342–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Hiroya Takamura</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Incorporating contextual cues in trainable models for coreference resolution (to appear).</title>
<date>2003</date>
<booktitle>In Proc. of EACL Workshop on the Computational Treatment ofAnaphora.</booktitle>
<contexts>
<context position="23406" citStr="Iida et al. (2003" startWordPosition="3920" endWordPosition="3923"> independent, removing a heavily weighted feature does not necessarily degrade the system’s performance. Hence, feature elimination is more reliable for reducing the number of features. However, feature elimination takes a long time. On the other hand, feature weights can give rough guidance. According to the table, our new features (Parallel, Unfinished, and Intra) obtained relatively large weights. This implies their importance. When we eliminated these three features, vrads+svm2’s score for editorial dropped by 4 points. Therefore, combinations of these three features are useful. Recently, Iida et al. (2003a) proposed an SVMbased tournament model that compares two candidates and selects the better one. We would like to compare or combine their method with our method. For further improvement, we have to make the morphological analyzer and the dependency analyzer more reliable because they make many mistakes when they process complex sentences. SVM has often been criticized as being too slow. However, the above data were small enough for the state-of-the-art SVM programs. The number of examples in each set of training data was about 5,000– 6,100, and each training phase took only 5–18 seconds on a</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Matsumoto. 2003a. Incorporating contextual cues in trainable models for coreference resolution (to appear). In Proc. of EACL Workshop on the Computational Treatment ofAnaphora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Hiroya Takamura</author>
<author>Yuji Matsumoto</author>
</authors>
<title>One method for resolving Japanese zero pronouns with machine learning model (in Japanese).</title>
<date>2003</date>
<booktitle>In IPSJ SIG-NL 154.</booktitle>
<contexts>
<context position="23406" citStr="Iida et al. (2003" startWordPosition="3920" endWordPosition="3923"> independent, removing a heavily weighted feature does not necessarily degrade the system’s performance. Hence, feature elimination is more reliable for reducing the number of features. However, feature elimination takes a long time. On the other hand, feature weights can give rough guidance. According to the table, our new features (Parallel, Unfinished, and Intra) obtained relatively large weights. This implies their importance. When we eliminated these three features, vrads+svm2’s score for editorial dropped by 4 points. Therefore, combinations of these three features are useful. Recently, Iida et al. (2003a) proposed an SVMbased tournament model that compares two candidates and selects the better one. We would like to compare or combine their method with our method. For further improvement, we have to make the morphological analyzer and the dependency analyzer more reliable because they make many mistakes when they process complex sentences. SVM has often been criticized as being too slow. However, the above data were small enough for the state-of-the-art SVM programs. The number of examples in each set of training data was about 5,000– 6,100, and each training phase took only 5–18 seconds on a</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Matsumoto. 2003b. One method for resolving Japanese zero pronouns with machine learning model (in Japanese). In IPSJ SIG-NL 154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei — A Japanese Lexicon (in Japanese). Iwanami Shoten.</booktitle>
<contexts>
<context position="11522" citStr="Ikehara et al., 1997" startWordPosition="1878" endWordPosition="1881"> Ranking rules Various heuristics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic empathy subject object2 object others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Appendix A for details.) 4. Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese zeros do not refer to noun phrases in relative clauses (Ehara and Kim, 1996). (See Appendix B for details.) Since sentences in newspaper articles are often complex and relative clauses are sometimes nested, we refine this rule in the following way. A candidate’s relative clause is the inm</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei — A Japanese Lexicon (in Japanese). Iwanami Shoten.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-2002,</booktitle>
<pages>390--396</pages>
<contexts>
<context position="8900" citStr="Isozaki and Kazawa, 2002" startWordPosition="1411" endWordPosition="1414">ow, we split the first sentence at ‘and’ but do not split the second sentence at ‘and’. She bought the book and sold it to him. She bought the book that he wrote and sold. A zero’s sentence is the (original) sentence that contains the zero. From now on, stands for a zero and stands for a candidate of ’s antecedent. ’s particle is denoted ZP, and CP stands for ’s next word that is ’s particle or a punctuation symbol. 2.1 Enumeration of possible antecedents Candidates (possible antecedents) are enumerated on the fly by using the following method. For this task, we use a named entity recognizer (Isozaki and Kazawa, 2002). The first step extracts a content word sequence from a bunsetsu. The second step excludes verb phrases, adjective phrases, and clauses. As a result, we obtain only noun phrases. The third step excludes adverbial expressions like kotoshi (this year). The forth step resolves anaphors like definite noun phrases in English. We should also resolve pronouns, but we did not because useful pronouns are rare in newspaper articles. In addition, we register a resolved zero as a new candidate. If ’s antecedent is determined to be , a new candidate is created for future zeros. is a copy of except that ’s</context>
<context position="14468" citStr="Isozaki and Kazawa, 2002" startWordPosition="2393" endWordPosition="2396"> place because Vi was often regarded as a constraint in the past literature. We put Ag before Sa because Kameyama’s method was better than Walker’s in Okumura and Tamura (1996). Therefore, Vi Ag Sa is expected to be a good ordering. The above ordering is an instance of this. 2.3 Machine Learning Although we can consider various other features for zero pronoun resolution, it is difficult to combine these features consistently. Therefore, we use machine learning. Support Vector Machines (SVMs) have shown good performance in various tasks in Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Hirao et al., 2002). Yoshino (2001) and Iida et al.(2003b) also applied SVM to Japanese zero pronoun resolution, but the usefulness of each feature was not clear. Here, we add features for complex sentences and analyze useful features by examining the weights of features. We use the following features of as well as CP. CSem ’s semantic categories. (See Appendix A.) CPPOS CP’s part-of-speech (POS) tags (rough and detailed). CPOS The POS tags of the last word of . Siblings When CP is wa or mo, it is not clear whether is a subject. However, a verb rarely has the same entity in two or more cases</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In Proc. of COLING-2002, pages 390–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megumi Kameyama</author>
</authors>
<title>A property-sharing constraint in centering.</title>
<date>1986</date>
<booktitle>In Proc. ofACL-1986,</booktitle>
<pages>200--206</pages>
<contexts>
<context position="2677" citStr="Kameyama, 1986" startWordPosition="419" endWordPosition="420">ill increase. From this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles. In Japanese, anaphors are often omitted and these omissions are called zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameya</context>
<context position="11145" citStr="Kameyama, 1986" startWordPosition="1809" endWordPosition="1810">If ’s is a pronoun or an adverbial noun (a noun that can also be used as an adverb, i.e., ChaSen’s meishi-fukushi-kanou), is excluded. 4. If is dou-shi (the person), it is replaced by the latest person name. If is dou-sha (the company), it is replaced by the latest organization name. If is dou+suffix, it is replaced by the latest candidate that has the same suffix. 2.2 Ranking rules Various heuristics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic empathy subject object2 object others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Appendix A for details.) 4. </context>
</contexts>
<marker>Kameyama, 1986</marker>
<rawString>Megumi Kameyama. 1986. A property-sharing constraint in centering. In Proc. ofACL-1986, pages 200– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL-2001,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="14442" citStr="Kudo and Matsumoto, 2001" startWordPosition="2389" endWordPosition="2392">es. We put Vi in the first place because Vi was often regarded as a constraint in the past literature. We put Ag before Sa because Kameyama’s method was better than Walker’s in Okumura and Tamura (1996). Therefore, Vi Ag Sa is expected to be a good ordering. The above ordering is an instance of this. 2.3 Machine Learning Although we can consider various other features for zero pronoun resolution, it is difficult to combine these features consistently. Therefore, we use machine learning. Support Vector Machines (SVMs) have shown good performance in various tasks in Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Hirao et al., 2002). Yoshino (2001) and Iida et al.(2003b) also applied SVM to Japanese zero pronoun resolution, but the usefulness of each feature was not clear. Here, we add features for complex sentences and analyze useful features by examining the weights of features. We use the following features of as well as CP. CSem ’s semantic categories. (See Appendix A.) CPPOS CP’s part-of-speech (POS) tags (rough and detailed). CPOS The POS tags of the last word of . Siblings When CP is wa or mo, it is not clear whether is a subject. However, a verb rarely has the same e</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proc. of NAACL-2001, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Morik</author>
<author>Peter Brockhausen</author>
<author>Thorsten Joachims</author>
</authors>
<title>Combining statistical learning with a knowledge-based approach — a case study in intensive care monitoring.</title>
<date>1999</date>
<booktitle>In Proc. of ICML-1999,</booktitle>
<pages>268--277</pages>
<contexts>
<context position="19677" citStr="Morik et al., 1999" startWordPosition="3325" endWordPosition="3328"> svm1 svm2 best 51.0 56.8 55.9 43.4 45.1 45.1 vrads 64.3 53.0 58.5 66.3 45.3 44.0 45.9 47.3 vards 64.0 53.0 58.5 66.0 45.9 44.2 45.9 46.9 rvads 63.4 51.0 58.5 66.3 44.4 43.4 46.1 47.5 avrds 62.8 53.0 58.5 66.0 44.2 44.0 45.9 46.9 vrdsa 55.9 53.0 58.5 65.7 43.4 44.0 45.9 48.6 adsvr 53.0 51.0 57.9 62.8 43.8 43.4 46.3 48.6 davrs 39.5 53.0 57.6 62.5 34.6 44.2 46.1 50.2 Seki 54.0 50.7 39.8 is positive. Consequently, it is independent of the ordering (unless two or more candidates have the best value). ‘Svm1’ uses the ordinary SVM (Vapnik, 1995) while ‘svm2’ uses a modified SVM for unbalanced data (Morik et al., 1999), which gives a large penalty to misclassification of a minority (= positive) example.5 In general, svm2 accepts more cadidates than svm1. According to this table, svm1 is too severe to exclude only bad candidates. We also tried the maximum entropy model 6 (mem) and C4.5, but they were also too severe. When we use SVM, we have to choose a good kernel for better performance. Here, we used the linear kernel ( ) for SVM because it was best according to our preliminary experiments. We set maxDi at 3 because it gave the best results. The table also shows Seki’s scores for reference, but it is not f</context>
</contexts>
<marker>Morik, Brockhausen, Joachims, 1999</marker>
<rawString>Katharina Morik, Peter Brockhausen, and Thorsten Joachims. 1999. Combining statistical learning with a knowledge-based approach — a case study in intensive care monitoring. In Proc. of ICML-1999, pages 268–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Makoto Nagao</author>
</authors>
<title>An estimate of referents of pronouns in Japanese sentences using examples and surface expressions (in Japanese).</title>
<date>1997</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="3552" citStr="Murata and Nagao (1997)" startWordPosition="541" endWordPosition="545">tion names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account. We have to take even more factors into account, but it is difficult to maintain such heuristic rules. Therefore, recent studies employ machine learning approaches. However, it is also difficult to prepare a sufficient number of annotated corpora. In this paper, we propose a method that combines these two approaches. Heuristic ranking rules give a general preference, while a machine learning method excludes inappropriate antecedent candidates. From the results of our experiments, the propo</context>
<context position="9922" citStr="Murata and Nagao, 1997" startWordPosition="1590" endWordPosition="1593"> rare in newspaper articles. In addition, we register a resolved zero as a new candidate. If ’s antecedent is determined to be , a new candidate is created for future zeros. is a copy of except that ’s particle is ZP and ’s location is ’s location. In the training phase of the machine learning approach, we consider a correct answer as . Then, we can remove far candidates from the list. In this way, our zero resolver creates a ‘general purpose’ candidate list. However, some of the candidates are inappropriate for certain zeros. A verb usually does not have the same entity in two or more cases (Murata and Nagao, 1997). Therefore, our resolver excludes candidates that are filled in other cases of the verb. When a verb has two or more zeros, we resolve ga first, and its best candidate is excluded from the candidates of wo or ni. 1. We extract a content word sequence as a candidate if it is followed by a case marker (kaku-joshi, e.g., ga, wo), a topic marker (wa or mo), or a period. 2. If ’s is a verb, an adjective, an auxilary verb, an adverb, or a relative pronoun (ChaSen’s meishi-hijiritsu, e.g., koto (what he did) and toki (when she married)), is excluded. (If is a closing quotation mark, is checked inste</context>
</contexts>
<marker>Murata, Nagao, 1997</marker>
<rawString>Masaki Murata and Makoto Nagao. 1997. An estimate of referents of pronouns in Japanese sentences using examples and surface expressions (in Japanese). Journal ofNatural Language Processing, 4(1):41–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiromi Nakaiwa</author>
<author>Satoru Ikehara</author>
</authors>
<title>Zero pronoun resolution in a Japanese to English machine translation system using verbal semantic attributes (in</title>
<date>1993</date>
<booktitle>Japanese). Transaction of the Information Processing Society ofJapan,</booktitle>
<pages>34--8</pages>
<contexts>
<context position="2840" citStr="Nakaiwa and Ikehara, 1993" startWordPosition="442" endWordPosition="445">naphors are often omitted and these omissions are called zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex s</context>
</contexts>
<marker>Nakaiwa, Ikehara, 1993</marker>
<rawString>Hiromi Nakaiwa and Satoru Ikehara. 1993. Zero pronoun resolution in a Japanese to English machine translation system using verbal semantic attributes (in Japanese). Transaction of the Information Processing Society ofJapan, 34(8):1705–1715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiromi Nakaiwa</author>
<author>Satoru Ikehara</author>
</authors>
<title>Intrasentential resolution of Japanese zero pronouns using pragmatic and semantic constraints (in Japanese).</title>
<date>1996</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="2895" citStr="Nakaiwa and Ikehara, 1996" startWordPosition="448" endWordPosition="451"> zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficien</context>
</contexts>
<marker>Nakaiwa, Ikehara, 1996</marker>
<rawString>Hiromi Nakaiwa and Satoru Ikehara. 1996. Intrasentential resolution of Japanese zero pronouns using pragmatic and semantic constraints (in Japanese). Journal ofNatural Language Processing, 3(4):49–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Kouji Tamura</author>
</authors>
<title>Zero pronoun resolution based on centering theory.</title>
<date>1996</date>
<booktitle>In Proc. of COLING-1996,</booktitle>
<pages>871--876</pages>
<contexts>
<context position="3372" citStr="Okumura and Tamura (1996)" startWordPosition="517" endWordPosition="520">et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account. We have to take even more factors into account, but it is difficult to maintain such heuristic rules. Therefore, recent studies employ machine learning approaches. However, it is also difficult to prepare a sufficient number of annotated corpora. In this paper, we propose a method that combines these two approac</context>
<context position="7950" citStr="Okumura and Tamura, 1996" startWordPosition="1245" endWordPosition="1248">dy determined by a zero detector and represented by corresponding particles. 2http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html 3http://chasen.aist-nara.ac.jp/ 4http://cl.aist-nara.ac.jp/˜taku-ku/software/cabocha/ If a zero is the subject of a verb, its case is represented by the particle ga. If it is an object, it is represented by wo. If it is an object2, it is represented by ni. We consider only these three cases. A zero’s particle means such a particle. Since complex sentences are hard to analyze, each sentence is automatically split at conjunctive postpositions (setsuzoku joshi) (Okumura and Tamura, 1996; Ehara and Kim, 1996). In order to distinguish the original complex sentence and the simpler sentences after the split, we call the former just a ‘sentence’ and the latter ‘post-split sentences’. When a conjunctive postposition appears in a relative clause, we do not split the sentence at that position. In the examples below, we split the first sentence at ‘and’ but do not split the second sentence at ‘and’. She bought the book and sold it to him. She bought the book that he wrote and sold. A zero’s sentence is the (original) sentence that contains the zero. From now on, stands for a zero and</context>
<context position="14020" citStr="Okumura and Tamura (1996)" startWordPosition="2322" endWordPosition="2325">e) is 0 if CP is wa. Sa is 1 if CP is ga. Sa is 2 if CP is ni. Sa is 3 if CP is wo. Otherwise, Sa is 4. We did not implement empathy because it makes the program more complex, and empathy verbs are rare in newspaper articles. For instance, holds. The first ranked (lexicographically smallest) candidate is regarded as the best candidate. We employ lexicographical ordering because it seems the simplest way to rank candidates. We put Vi in the first place because Vi was often regarded as a constraint in the past literature. We put Ag before Sa because Kameyama’s method was better than Walker’s in Okumura and Tamura (1996). Therefore, Vi Ag Sa is expected to be a good ordering. The above ordering is an instance of this. 2.3 Machine Learning Although we can consider various other features for zero pronoun resolution, it is difficult to combine these features consistently. Therefore, we use machine learning. Support Vector Machines (SVMs) have shown good performance in various tasks in Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Hirao et al., 2002). Yoshino (2001) and Iida et al.(2003b) also applied SVM to Japanese zero pronoun resolution, but the usefulness of each feature wa</context>
<context position="15547" citStr="Okumura and Tamura, 1996" startWordPosition="2590" endWordPosition="2593">ast word of . Siblings When CP is wa or mo, it is not clear whether is a subject. However, a verb rarely has the same entity in two or more cases. Therefore, if modifies a verb that has a subject, is not a subject. In the next example, hon is an object of katta. Ano / hon wa / Tomu ga / katta. that book=topic Tom=subj bought (As for that book, Tom bought it.) In order to learn such things, we use sibling casemarkers that modify the same verb as ’s features. We also use the following features of as well as ZP. Conjunct The latest conjunctive postposition in the sentence and its classification (Okumura and Tamura, 1996; Yoshimoto, 1986). ZSem Semantic categories of the verb that modifies. We use them only when the verb is sahen meishi + ‘suru.’ Sahen meishi is a kind of noun that can be an object of the verb ‘suru’ (do) (e.g., ‘shopping’ in ‘do the shopping’). We also use the following relations between and as well as Ag, Vi, and Di. Relative Whether is in a relative clause. Unfinished Whether the relative clause is unfinished at . Intra (for intrasentential coreference) Whether explicitly appears in ’s sentence. Sometimes it is difficult to distinguish cataphora from anaphora. Even if an antecedent appears</context>
</contexts>
<marker>Okumura, Tamura, 1996</marker>
<rawString>Manabu Okumura and Kouji Tamura. 1996. Zero pronoun resolution based on centering theory. In Proc. of COLING-1996, pages 871–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Seki</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Japanese zero pronoun resolution using a probabilistic model (in Japanese).</title>
<date>2002</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="5374" citStr="Seki et al., 2002" startWordPosition="837" endWordPosition="840">t), ni is dative (object2), and wa marks a topic. Tomu ga / Bobu ni / hon wo / okutta. Tom=subj Bob=object2 book=object sent (Tom sent a book to Bob.) Bunsetsu dependency is represented by a list of bunsetsu pairs (modifier, modified). For instance, indicates that there are four bunsetsus in this sentence and that the first bunsetsu modifies the fourth bunsetsu and so on. The last bunsetsu modifies no bunsetsu, which is indicated by . It takes a long time to construct high-quality annotated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki’s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus 2 2.0. These data are divided into two groups: general and editorial. General contains 30 general news articles, and editorial contains 30 editorial articles. According to his experiments, editorial is harder than general. Perhaps this is caused by the difference in rhetorical styles and the lengths of articles. The average number of sentences in an editorial article is 28.7, while that in a general article is 13.9. However, we found problems in his data. For instance, the data contained ambiguous antecedents like dou-shi (</context>
<context position="18984" citStr="Seki et al., 2002" startWordPosition="3197" endWordPosition="3200">y in decreasing order. 4. If no candidate satisfies , return the best candidate in terms of 3 Results We conducted leave-one(-article)-out experiments. For each article, 29 other articles were used for training. Table 1 compares the scores of the above methods. ‘First’ picks up the first candidate given by a given lexicographical ordering. The acronym ‘vrads’ stands for the lexicographical ordering of Vi Re Ag Di Sa. ‘Best’ picks up the best candidate in terms of without checking whether it . Table 1: Percentage of correctly resolved zeros = The combination is worse than ‘first’ or ‘best.’ = (Seki et al., 2002a), = (Seki et al., 2002b) general editorial first mem svm1 svm2 first mem svm1 svm2 best 51.0 56.8 55.9 43.4 45.1 45.1 vrads 64.3 53.0 58.5 66.3 45.3 44.0 45.9 47.3 vards 64.0 53.0 58.5 66.0 45.9 44.2 45.9 46.9 rvads 63.4 51.0 58.5 66.3 44.4 43.4 46.1 47.5 avrds 62.8 53.0 58.5 66.0 44.2 44.0 45.9 46.9 vrdsa 55.9 53.0 58.5 65.7 43.4 44.0 45.9 48.6 adsvr 53.0 51.0 57.9 62.8 43.8 43.4 46.3 48.6 davrs 39.5 53.0 57.6 62.5 34.6 44.2 46.1 50.2 Seki 54.0 50.7 39.8 is positive. Consequently, it is independent of the ordering (unless two or more candidates have the best value). ‘Svm1’ uses the ordinary</context>
<context position="20494" citStr="Seki et al., 2002" startWordPosition="3472" endWordPosition="3475">ad candidates. We also tried the maximum entropy model 6 (mem) and C4.5, but they were also too severe. When we use SVM, we have to choose a good kernel for better performance. Here, we used the linear kernel ( ) for SVM because it was best according to our preliminary experiments. We set maxDi at 3 because it gave the best results. The table also shows Seki’s scores for reference, but it is not fair to compare our scores with Seki’s scores directly because our data is slightly different from Seki’s. The number of zeros in general in our data is 347, while Seki resolved 355 detected zeros in (Seki et al., 2002a) and 404 in (Seki et al., 2002b). The number of zeros in our editorial is 514, while (Seki et al., 2002a) resolved 498 detected zeros. In order to overcome the data sparseness, 5An ordinary SVM minimizes while the modified SVM minimizes where = number of negative examples/number of positive examples. 6http://www2.crl.go.jp/jt/a132/members/mutiyama/software. html Seki used unannotated articles to get co-occurrence statistics. Without the data, their scores degraded about 5 points. We have not conducted experiments that use unannotated corpora; this task is our future work. As we expected, ins</context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa. 2002a. Japanese zero pronoun resolution using a probabilistic model (in Japanese). Journal ofNatural Language Processing, 9(3):63–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Seki</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-2002,</booktitle>
<pages>911--917</pages>
<contexts>
<context position="5374" citStr="Seki et al., 2002" startWordPosition="837" endWordPosition="840">t), ni is dative (object2), and wa marks a topic. Tomu ga / Bobu ni / hon wo / okutta. Tom=subj Bob=object2 book=object sent (Tom sent a book to Bob.) Bunsetsu dependency is represented by a list of bunsetsu pairs (modifier, modified). For instance, indicates that there are four bunsetsus in this sentence and that the first bunsetsu modifies the fourth bunsetsu and so on. The last bunsetsu modifies no bunsetsu, which is indicated by . It takes a long time to construct high-quality annotated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki’s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus 2 2.0. These data are divided into two groups: general and editorial. General contains 30 general news articles, and editorial contains 30 editorial articles. According to his experiments, editorial is harder than general. Perhaps this is caused by the difference in rhetorical styles and the lengths of articles. The average number of sentences in an editorial article is 28.7, while that in a general article is 13.9. However, we found problems in his data. For instance, the data contained ambiguous antecedents like dou-shi (</context>
<context position="18984" citStr="Seki et al., 2002" startWordPosition="3197" endWordPosition="3200">y in decreasing order. 4. If no candidate satisfies , return the best candidate in terms of 3 Results We conducted leave-one(-article)-out experiments. For each article, 29 other articles were used for training. Table 1 compares the scores of the above methods. ‘First’ picks up the first candidate given by a given lexicographical ordering. The acronym ‘vrads’ stands for the lexicographical ordering of Vi Re Ag Di Sa. ‘Best’ picks up the best candidate in terms of without checking whether it . Table 1: Percentage of correctly resolved zeros = The combination is worse than ‘first’ or ‘best.’ = (Seki et al., 2002a), = (Seki et al., 2002b) general editorial first mem svm1 svm2 first mem svm1 svm2 best 51.0 56.8 55.9 43.4 45.1 45.1 vrads 64.3 53.0 58.5 66.3 45.3 44.0 45.9 47.3 vards 64.0 53.0 58.5 66.0 45.9 44.2 45.9 46.9 rvads 63.4 51.0 58.5 66.3 44.4 43.4 46.1 47.5 avrds 62.8 53.0 58.5 66.0 44.2 44.0 45.9 46.9 vrdsa 55.9 53.0 58.5 65.7 43.4 44.0 45.9 48.6 adsvr 53.0 51.0 57.9 62.8 43.8 43.4 46.3 48.6 davrs 39.5 53.0 57.6 62.5 34.6 44.2 46.1 50.2 Seki 54.0 50.7 39.8 is positive. Consequently, it is independent of the ordering (unless two or more candidates have the best value). ‘Svm1’ uses the ordinary</context>
<context position="20494" citStr="Seki et al., 2002" startWordPosition="3472" endWordPosition="3475">ad candidates. We also tried the maximum entropy model 6 (mem) and C4.5, but they were also too severe. When we use SVM, we have to choose a good kernel for better performance. Here, we used the linear kernel ( ) for SVM because it was best according to our preliminary experiments. We set maxDi at 3 because it gave the best results. The table also shows Seki’s scores for reference, but it is not fair to compare our scores with Seki’s scores directly because our data is slightly different from Seki’s. The number of zeros in general in our data is 347, while Seki resolved 355 detected zeros in (Seki et al., 2002a) and 404 in (Seki et al., 2002b). The number of zeros in our editorial is 514, while (Seki et al., 2002a) resolved 498 detected zeros. In order to overcome the data sparseness, 5An ordinary SVM minimizes while the modified SVM minimizes where = number of negative examples/number of positive examples. 6http://www2.crl.go.jp/jt/a132/members/mutiyama/software. html Seki used unannotated articles to get co-occurrence statistics. Without the data, their scores degraded about 5 points. We have not conducted experiments that use unannotated corpora; this task is our future work. As we expected, ins</context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa. 2002b. A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution. In Proc. of COLING-2002, pages 911–917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="19603" citStr="Vapnik, 1995" startWordPosition="3313" endWordPosition="3315">(Seki et al., 2002b) general editorial first mem svm1 svm2 first mem svm1 svm2 best 51.0 56.8 55.9 43.4 45.1 45.1 vrads 64.3 53.0 58.5 66.3 45.3 44.0 45.9 47.3 vards 64.0 53.0 58.5 66.0 45.9 44.2 45.9 46.9 rvads 63.4 51.0 58.5 66.3 44.4 43.4 46.1 47.5 avrds 62.8 53.0 58.5 66.0 44.2 44.0 45.9 46.9 vrdsa 55.9 53.0 58.5 65.7 43.4 44.0 45.9 48.6 adsvr 53.0 51.0 57.9 62.8 43.8 43.4 46.3 48.6 davrs 39.5 53.0 57.6 62.5 34.6 44.2 46.1 50.2 Seki 54.0 50.7 39.8 is positive. Consequently, it is independent of the ordering (unless two or more candidates have the best value). ‘Svm1’ uses the ordinary SVM (Vapnik, 1995) while ‘svm2’ uses a modified SVM for unbalanced data (Morik et al., 1999), which gives a large penalty to misclassification of a minority (= positive) example.5 In general, svm2 accepts more cadidates than svm1. According to this table, svm1 is too severe to exclude only bad candidates. We also tried the maximum entropy model 6 (mem) and C4.5, but they were also too severe. When we use SVM, we have to choose a good kernel for better performance. Here, we used the linear kernel ( ) for SVM because it was best according to our preliminary experiments. We set maxDi at 3 because it gave the best </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Masayo Iida</author>
<author>Sharon Cote</author>
</authors>
<title>Japanese discourse and the process of centering.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2698" citStr="Walker et al., 1994" startWordPosition="421" endWordPosition="424">om this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles. In Japanese, anaphors are often omitted and these omissions are called zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized </context>
<context position="11060" citStr="Walker et al., 1994" startWordPosition="1797" endWordPosition="1800">when she married)), is excluded. (If is a closing quotation mark, is checked instead.) 3. If ’s is a pronoun or an adverbial noun (a noun that can also be used as an adverb, i.e., ChaSen’s meishi-fukushi-kanou), is excluded. 4. If is dou-shi (the person), it is replaced by the latest person name. If is dou-sha (the company), it is replaced by the latest organization name. If is dou+suffix, it is replaced by the latest candidate that has the same suffix. 2.2 Ranking rules Various heuristics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic empathy subject object2 object others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also ha</context>
</contexts>
<marker>Walker, Iida, Cote, 1994</marker>
<rawString>Marilyn Walker, Masayo Iida, and Sharon Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhide Yamamoto</author>
</authors>
<title>Eiichiro Sumita, Osamu Furuse, and Hitoshi Iida.</title>
<date>1997</date>
<booktitle>In Proc. of NLPRS1997,</booktitle>
<pages>423--428</pages>
<marker>Yamamoto, 1997</marker>
<rawString>Kazuhide Yamamoto, Eiichiro Sumita, Osamu Furuse, and Hitoshi Iida. 1997. Ellipsis resolution in dialogues via decision-tree learning. In Proc. of NLPRS1997, pages 423–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitsuko Yamura-Takei</author>
<author>Miho Fujiwara</author>
<author>Makoto Yoshie</author>
<author>Teruaki Aizawa</author>
</authors>
<title>Automatic linguistic analysis for language teachers: The case of zeros.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-2002,</booktitle>
<pages>1114--1120</pages>
<contexts>
<context position="11360" citStr="Yamura-Takei et al., 2002" startWordPosition="1845" endWordPosition="1848">e. If is dou-sha (the company), it is replaced by the latest organization name. If is dou+suffix, it is replaced by the latest candidate that has the same suffix. 2.2 Ranking rules Various heuristics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic empathy subject object2 object others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Appendix A for details.) 4. Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese zeros do not refer to noun phrases in relative clauses (Ehara and Kim, 1996). (See Appendix B for details.) Since sentences in </context>
</contexts>
<marker>Yamura-Takei, Fujiwara, Yoshie, Aizawa, 2002</marker>
<rawString>Mitsuko Yamura-Takei, Miho Fujiwara, Makoto Yoshie, and Teruaki Aizawa. 2002. Automatic linguistic analysis for language teachers: The case of zeros. In Proc. of COLING-2002, pages 1114–1120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kei Yoshimoto</author>
</authors>
<title>Study of Japanese zero pronouns in discourse processing (in Japanese).</title>
<date>1986</date>
<booktitle>In IPSJ SIG notes,</booktitle>
<pages>56--4</pages>
<contexts>
<context position="15565" citStr="Yoshimoto, 1986" startWordPosition="2594" endWordPosition="2595">n CP is wa or mo, it is not clear whether is a subject. However, a verb rarely has the same entity in two or more cases. Therefore, if modifies a verb that has a subject, is not a subject. In the next example, hon is an object of katta. Ano / hon wa / Tomu ga / katta. that book=topic Tom=subj bought (As for that book, Tom bought it.) In order to learn such things, we use sibling casemarkers that modify the same verb as ’s features. We also use the following features of as well as ZP. Conjunct The latest conjunctive postposition in the sentence and its classification (Okumura and Tamura, 1996; Yoshimoto, 1986). ZSem Semantic categories of the verb that modifies. We use them only when the verb is sahen meishi + ‘suru.’ Sahen meishi is a kind of noun that can be an object of the verb ‘suru’ (do) (e.g., ‘shopping’ in ‘do the shopping’). We also use the following relations between and as well as Ag, Vi, and Di. Relative Whether is in a relative clause. Unfinished Whether the relative clause is unfinished at . Intra (for intrasentential coreference) Whether explicitly appears in ’s sentence. Sometimes it is difficult to distinguish cataphora from anaphora. Even if an antecedent appears in a preceding se</context>
</contexts>
<marker>Yoshimoto, 1986</marker>
<rawString>Kei Yoshimoto. 1986. Study of Japanese zero pronouns in discourse processing (in Japanese). In IPSJ SIG notes, NL-56-4, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiichi Yoshino</author>
</authors>
<title>Anaphora resolution of Japanese zero pronouns using machine learning (in Japanese). Master’s thesis,</title>
<date>2001</date>
<institution>Nara Institute of Science and Technology.</institution>
<contexts>
<context position="11376" citStr="Yoshino, 2001" startWordPosition="1849" endWordPosition="1850">ny), it is replaced by the latest organization name. If is dou+suffix, it is replaced by the latest candidate that has the same suffix. 2.2 Ranking rules Various heuristics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic empathy subject object2 object others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Appendix A for details.) 4. Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese zeros do not refer to noun phrases in relative clauses (Ehara and Kim, 1996). (See Appendix B for details.) Since sentences in newspaper articl</context>
<context position="14505" citStr="Yoshino (2001)" startWordPosition="2402" endWordPosition="2403">raint in the past literature. We put Ag before Sa because Kameyama’s method was better than Walker’s in Okumura and Tamura (1996). Therefore, Vi Ag Sa is expected to be a good ordering. The above ordering is an instance of this. 2.3 Machine Learning Although we can consider various other features for zero pronoun resolution, it is difficult to combine these features consistently. Therefore, we use machine learning. Support Vector Machines (SVMs) have shown good performance in various tasks in Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Hirao et al., 2002). Yoshino (2001) and Iida et al.(2003b) also applied SVM to Japanese zero pronoun resolution, but the usefulness of each feature was not clear. Here, we add features for complex sentences and analyze useful features by examining the weights of features. We use the following features of as well as CP. CSem ’s semantic categories. (See Appendix A.) CPPOS CP’s part-of-speech (POS) tags (rough and detailed). CPOS The POS tags of the last word of . Siblings When CP is wa or mo, it is not clear whether is a subject. However, a verb rarely has the same entity in two or more cases. Therefore, if modifies a verb that </context>
<context position="22670" citStr="Yoshino (2001)" startWordPosition="3809" endWordPosition="3810">weights. Semantic categories ‘suggestions’ and ‘report’ reflect the fact that some articles use anthropomorphism. These weights will be useful to design better heuristic rules. The fact that Unfinished’s weight almost cancels Relative’s weight justifies the Table 2: Weights of features general editorial Ag=0 Ag=0 ZP=ni Parallel concrete CSem Di=0 CP=ga Intra Intra CP=ga agents CSem suggestion CSem CP=wa report CSem Di=0 agents CSem Parallel concrete CSem Unfinished CP=wa Unfinished Relative CP=mo CPPOS=‘case marker’ CP=no Relative ZP=wo CP=no Di=3 Di=3 Vi=1 Vi=1 definition of Re. 4 Discussion Yoshino (2001) used an ordinary SVM with . He tried to find useful features by feature elimination. Since features are not completely independent, removing a heavily weighted feature does not necessarily degrade the system’s performance. Hence, feature elimination is more reliable for reducing the number of features. However, feature elimination takes a long time. On the other hand, feature weights can give rough guidance. According to the table, our new features (Parallel, Unfinished, and Intra) obtained relatively large weights. This implies their importance. When we eliminated these three features, vrads</context>
</contexts>
<marker>Yoshino, 2001</marker>
<rawString>Keiichi Yoshino. 2001. Anaphora resolution of Japanese zero pronouns using machine learning (in Japanese). Master’s thesis, Nara Institute of Science and Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>