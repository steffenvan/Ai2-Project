<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.989078">
Context-aware Entity Morph Decoding
</title>
<author confidence="0.9953295">
Boliang Zhang1, Hongzhao Huang1, Xiaoman Pan1, Sujian Li2, Chin-Yew Lin3
Heng Ji1, Kevin Knight4, Zhen Wen5, Yizhou Sun6, Jiawei Han7, Bulent Yener1
</author>
<affiliation confidence="0.90241">
1Rensselaer Polytechnic Institute, 2Peking University, 3Microsoft Research Asia, 4University of Southern California
</affiliation>
<address confidence="0.712403">
5IBM T. J. Watson Research Center, 6Northeastern University, 7Univerisity of Illinois at Urbana-Champaign
</address>
<email confidence="0.813305">
1{zhangb8,huangh9,panx2,jih,yener}@rpi.edu, 2lisujian@pku.edu.cn, 3cyl@microsoft.com
4hanj@illinois.edu, 5zhenwen@us.ibm.com, 6yzsun@ccs.neu.edu, 7hanj@illinois.edu
</email>
<sectionHeader confidence="0.995068" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999445">
People create morphs, a special type of
fake alternative names, to achieve certain
communication goals such as expressing
strong sentiment or evading censors. For
example, “Black Mamba”, the name for a
highly venomous snake, is a morph that
Kobe Bryant created for himself due to his
agility and aggressiveness in playing bas-
ketball games. This paper presents the first
end-to-end context-aware entity morph de-
coding system that can automatically iden-
tify, disambiguate, verify morph mentions
based on specific contexts, and resolve
them to target entities. Our approach is
based on an absolute “cold-start” - it does
not require any candidate morph or tar-
get entity lists as input, nor any manually
constructed morph-target pairs for train-
ing. We design a semi-supervised collec-
tive inference framework for morph men-
tion extraction, and compare various deep
learning based approaches for morph res-
olution. Our approach achieved signifi-
cant improvement over the state-of-the-art
method (Huang et al., 2013), which used a
large amount of training data. 1
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99880275">
Morphs (Huang et al., 2013; Zhang et al., 2014)
refer to the fake alternative names created by so-
cial media users to entertain readers or evade cen-
sors. For example, during the World Cup in 2014,
</bodyText>
<footnote confidence="0.991055333333333">
1The data set and programs are publicly avail-
able at: http://nlp.cs.rpi.edu/data/morphdecoding.zip and
http://nlp.cs.rpi.edu/software/morphdecoding.tar.gz
</footnote>
<bodyText confidence="0.999809966666667">
a morph “Su-tooth” was created to refer to the
Uruguay striker “Luis Suarez” for his habit of bit-
ing other players. Automatically decoding human-
generated morphs in text is critical for downstream
deep language understanding tasks such as entity
linking and event argument extraction.
However, even for human, it is difficult to de-
code many morphs without certain historical, cul-
tural, or political background knowledge (Zhang
et al., 2014). For example, “The Hutt” can be used
to refer to a fictional alien entity in the Star Wars
universe (“The Hutt stayed and established himself
as ruler of Nam Chorios”), or the governor of New
Jersey, Chris Christie (“The Hutt announced a bid
for a seat in the New Jersey General Assembly”).
Huang et al. (2013) did a pioneering pilot study on
morph resolution, but their approach assumed the
entity morphs were already extracted and used a
large amount of labeled data. In fact, they resolved
morphs on corpus-level instead of mention-level
and thus their approach was context-independent.
A practical morph decoder, as depicted in Fig-
ure 1, consists of two problems: (1) Morph Ex-
traction: given a corpus, extract morph mentions;
and (2). Morph Resolution: For each morph men-
tion, figure out the entity that it refers to.
In this paper, we aim to solve the fundamental
research problem of end-to-end morph decoding
and propose a series of novel solutions to tackle
the following challenges.
</bodyText>
<subsectionHeader confidence="0.788455">
Challenge 1: Large-scope candidates
</subsectionHeader>
<bodyText confidence="0.9998142">
Only a very small percentage of terms can be used
as morphs, which should be interesting and fun.
As we annotate a sample of 4,668 Chinese weibo
tweets, only 450 out of 19, 704 unique terms are
morphs. To extract morph mentions, we propose a
</bodyText>
<page confidence="0.97542">
586
</page>
<note confidence="0.987433666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 586–595,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.857738714285714">
Tweets Target Entities
6�72�&lt;��, �����%�4 !yWK
~
, ����~��!. (Wang Lijun)
d4 (Wu Sangui met Wei Xiaobao, and led the
army of Qing dynasty into China, and then
became Conquer West King.)
</figure>
<figureCaption confidence="0.999866">
Figure 1: An Illustration of Morph Decoding Task.
</figureCaption>
<bodyText confidence="0.999871">
two-step approach to first identify individual men-
tion candidates to narrow down the search scope,
and then verify whether they refer to morphed en-
tities instead of their original meanings.
</bodyText>
<sectionHeader confidence="0.837011" genericHeader="introduction">
Challenge 2: Ambiguity, Implicitness,
Informality
</sectionHeader>
<bodyText confidence="0.99574208">
Compared to regular entities, many morphs con-
tain informal terms with hidden information. For
example, “不厚 (not thick)” is used to refer to
“4熙来 (Bo Xilai)” whose last name “4 (Bo)”
means “thin”. Therefore we attempt to model
the rich contexts with careful considerations for
morph characteristics both globally (e.g., language
models learned from a large amount of data) and
locally (e.g. phonetic anomaly analysis) to extract
morph mentions.
For morph resolution, the main challenge lies
in that the surface forms of morphs usually ap-
pear quite different from their target entity names.
Based on the distributional hypothesis (Harris,
1954) which states that words that often occur in
similar contexts tend to have similar meanings, we
propose to use deep learning techniques to capture
and compare the deep semantic representations of
a morph and its candidate target entities based on
their contextual clues. For example, the morph
“平西王(Conquer West King)” and its target entity
“4熙来 (Bo Xilai)” share similar implicit contex-
tual representations such as “重庆(Chongqing)”
(Bo was the governor of Chongqing) and “倒台
(fall from power)”.
</bodyText>
<note confidence="0.407291">
Challenge 3: Lack of labeled data
</note>
<bodyText confidence="0.999895842105263">
To the best of our knowledge, no sufficient
mention-level morph annotations exist for training
an end-to-end decoder. Manual morph annotations
require native speakers who have certain cultural
background (Zhang et al., 2014). In this paper
we focus on exploring novel approaches to save
annotation cost in each step. For morph extrac-
tion, based on the observation that morphs tend to
share similar characteristics and appear together,
we propose a semi-supervised collective inference
approach to extract morph mentions from multiple
tweets simultaneously. Deep learning techniques
have been successfully used to model word rep-
resentation in an unsupervised fashion. For morph
resolution, we make use of a large amount of unla-
beled data to learn the semantic representations of
morphs and target entities based on the unsuper-
vised continuous bag-of-words method (Mikolov
et al., 2013b).
</bodyText>
<sectionHeader confidence="0.955127" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999758344827586">
Following the recent work on morphs (Huang
et al., 2013; Zhang et al., 2014), we use Chi-
nese Weibo tweets for experiments. Our goal
is to develop an end-to-end system that auto-
matically extract morph mentions and resolve
them to their target entities. Given a corpus
of tweets D = {d1, d2, ..., d|D|}, we define a
candidate morph mi as a unique term tj in T,
where T = {t1, t2, ..., t|T|} is the set of unique
terms in D. To extract T, we first apply sev-
eral well-developed Natural Language Process-
ing tools, including Stanford Chinese word seg-
menter (Chang et al., 2008), Stanford part-of-
speech tagger (Toutanova et al., 2003) and Chinese
lexical analyzer ICTCLAS (Zhang et al., 2003),
to process the tweets and identify noun phrases.
Then we define a morph mention mpi of mi as the
p-th occurrence of mi in a specific document dj.
Note that a mention with the same surface form as
mi but referring to its original entity is not consid-
ered as a morph mention. For instance, the “平西
王 (Conquer West King)” in d1 and d3 in Figure 1
are morph mentions since they refer to the modern
politician “4熙来 (Bo Xilai)”, while the one in d4
is not a morph mention since it refers to the origi-
nal entity, who was king “吴三桂 (Wu Sangui)”.
For each morph mention, we discover a list of
target candidates E = {e1, e2, ..., e|E|} from Chi-
nese web data for morph mention resolution. We
</bodyText>
<figure confidence="0.987983666666667">
d3
41#
(Ma Ying-jeou)
� ! �-��!! � ! WW
(Attention! Chongqing Conquer West King!
Attention! Brother Jun!)
*A!U~)ft~, iJf~~#!V~?
(Conquer West King from Chongqing fell
from power, do we still need to sing red songs?)
TI0~41.4f.
(Buhou and Little Brother Ma.)
d1
d2
):*
(Bo Xilai)
</figure>
<page confidence="0.994319">
587
</page>
<bodyText confidence="0.9974395">
design an end-to-end morph decoder which con-
sists of the following procedure:
</bodyText>
<listItem confidence="0.996798">
• Morph Mention Extraction
</listItem>
<bodyText confidence="0.731962333333333">
– Potential Morph Discovery: This first step
aims to obtain a set of potential entity-level
morphs M = {m1, m2, ...}(M ⊆ T). Then,
we only verify and resolve the mentions of
these potential morphs, instead of all the
terms in T in a large corpus.
</bodyText>
<listItem confidence="0.833494444444444">
– Morph Mention Verification: In this step, we
aim to verify whether each mention mpi of the
potential morph mi(mi E M) from a specific
context dj is a morph mention or not.
• Morph Mention Resolution: The final step is
to resolve each morph mention mpi to its target
entity (e.g., “4熙A,- (Bo Xilai)” for the morph
mention “平西王 (Conquer West King)” in d1
in Figure 1).
</listItem>
<sectionHeader confidence="0.994927" genericHeader="method">
3 Morph Mention Extraction
</sectionHeader>
<subsectionHeader confidence="0.9890415">
3.1 Why Traditional Entity Mention
Extraction doesn’t Work
</subsectionHeader>
<bodyText confidence="0.998844875">
In order to automatically extract morph mentions
from any given documents, our first reflection is
to formulate the task as a sequence labeling prob-
lem, just like labeling regular entity mentions. We
adopted the commonly used conditional random
fields (CRFs) (Lafferty et al., 2001) and got only
6% F-score. Many morphs are not presented as
regular entity mentions. For example, the morph
“天 线 (Antenna)” refers to “温 家 宝 (Wen Ji-
abao)” because it shares one character “宝 (baby)”
with the famous children’s television series “天
线宝宝 (Teletubbies)”. Even when they are pre-
sented as regular entity mentions, they must refer
to new target entities which are different from the
regular ones. So we propose the following novel
two-step solution.
</bodyText>
<subsectionHeader confidence="0.998215">
3.2 Potential Morph Discovery
</subsectionHeader>
<bodyText confidence="0.982822472727273">
We first introduce the first step of our approach
– potential morph discovery, which aims to nar-
row down the scope of morph candidates with-
out losing recall. This step takes advantage of
the common characteristics shared among morphs
and identifies the potential morphs using a super-
vised method, since it is relatively easy to collect
a certain number of corpus-level morphs as train-
ing data compared to labeling morph mentions.
Through formulating this task as a binary classifi-
cation problem, we adopt the Support Vector Ma-
chines (SVMs) (Cortes and Vapnik, 1995) as the
learning model. We propose the following four
categories of features.
Basic: (i) character unigram, bigram, trigram,
and surface form; (ii) part-of-speech tags; (iii) the
number of characters; (iv) whether some charac-
ters are identical. These basic features will help
identify several common characteristics of morph
candidates (e.g., they are very likely to be nouns,
and very unlikely to contain single characters).
Dictionary: Many morphs are non-regular
names derived from proper names while retain-
ing some characteristics. For example, the morphs
“4 督 (Governor Bo)” and “吃 省 (Gourmand
Province)” are derived from their target entity
names “4熙A,- (Bo Xilai)” and “广东省 (Guan-
dong Province)”, respectively. Therefore, we
adopt a dictionary of proper names (Li et al., 2012)
and propose the following features: (i) Whether
a term occurs in the dictionary. (ii) Whether a
term starts with a commonly used last name, and
includes uncommonly used characters as its first
name. (iii) Whether a term ends with a geo-
political entity or organization suffix word, but it’s
not in the dictionary.
Phonetic: Many morphs are created based on
phonetic (Chinese pinyin in our case) modifica-
tions. For instance, the morph “饭 * * (Rice
Cake)” has the same phonetic transcription as
its target entity name “范** (Fan Bingbing)”.
To extract phonetic-based features, we compile
a dictionary composed of (phonetic transcription,
term) pairs from the Chinese Gigaword corpus 2.
Then for each term, we check whether it has the
same phonetic transcription as any entry in the dic-
tionary but they include different characters.
Language Modeling: Many morphs rarely ap-
pear in a general news corpus (e.g., “六 步 郎
(Six Step Man)” refers to the NBA baseketball
player “勒布朗·詹姆斯 (Lebron James)”.). There-
fore, we use the character-based language models
trained from Gigaword to calculate the occurrence
probabilities of each term, and use n-gram proba-
bilities (n E [1 : 5]) as features.
</bodyText>
<subsectionHeader confidence="0.997776">
3.3 Morph Mention Verification
</subsectionHeader>
<bodyText confidence="0.999619333333333">
The second step is to verify whether a mention of
the discovered potential morphs is indeed used as
a morph in a specific context. Based on the ob-
</bodyText>
<footnote confidence="0.834378">
2https://catalog.ldc.upenn.edu/LDC2011T07
</footnote>
<page confidence="0.994814">
588
</page>
<bodyText confidence="0.992442714285714">
servation that closely related morph mentions of-
ten occur together, we propose a semi-supervised
graph-based method to leverage a small set of la-
beled seeds, coreference and correlation relations,
and a large amount of unlabeled data to perform
collective inference and thus save annotation cost.
According to our observation of morph mentions,
we propose the following two hypotheses:
Hypothesis 1: If two mentions are coreferen-
tial, then they both should either be morph men-
tions or non-morph mentions. For instance, the
morph mentions “ 西_. (Conquer West King)”
in d1 and d3 in Figure 1 are coreferential, they both
refer to the modern politician “薄熙来 (Bo Xilai)”.
Hypothesis 2: Those highly correlated men-
tions tend to either be morph mentions or non-
morph mentions. From our annotated dataset, 49%
morph mentions co-occur on tweet level. For ex-
ample, “ 西 _.(Conquer West King)” and “军
哥(Brother Jun)” are used together in d3 in Fig-
ure 1.
Based on these hypotheses, we aim to design
an effective approach to compensate for the lim-
ited annotated data. Graph-based semi-supervised
learning approaches (Zhu et al., 2003; Smola and
Kondor, 2003; Zhou et al., 2004) have been suc-
cessfully applied many NLP tasks (Niu et al.,
2005; Chen et al., 2006; Huang et al., 2014).
Therefore we build a mention graph to capture
the semantic relatedness (weighted arcs) between
potential morph mentions (nodes) and propose a
semi-supervised graph-based algorithm to collec-
tively verify a set of relevant mentions using a
small amount of labeled data. We now describe
the detailed algorithm as follows.
</bodyText>
<subsectionHeader confidence="0.760219">
Mention Graph Construction
</subsectionHeader>
<bodyText confidence="0.999506833333333">
First, we construct a mention graph that can reflect
the association between all the mentions of poten-
tial morphs. According to the above two hypothe-
ses, mention coreference and correlation relations
are the basis to build our mention graph, which is
represented by a matrix.
In Chinese Weibo, their exist rich and clean
social relations including authorship, replying,
retweeting, or user mentioning relations. We make
use of these social relations to judge the possibility
of two mentions of the same potential morph be-
ing coreferential. If there exists one social relation
between two mentions mpi and mqi of the morph
mi, they are usually coreferential and assigned an
association score 1. We also detect coreferential
relations by performing content similarity analy-
sis. The cosine similarity is adopted with the tf-idf
representation for the contexts of two mentions.
Then we get a coreference matrix W1:
where mpi and mqi are two mentions from the
same potential morph mi, and kNN means that
each mention is connected to its k nearest neigh-
boring mentions.
Users tend to use morph mentions together to
achieve their communication goals. To incorpo-
rate such evidence, we measure the correlation be-
tween two mentions mpi and mqj of two different
potential morphs mi and mj as corr(mpi , mqj) =
1.0 if there exists a certain social relation between
them. Otherwise, corr(mpi ,mqj) = 0. Then
</bodyText>
<equation confidence="0.546987333333333">
we can obtain the correlation matrix: W2 =
mp i ,m9
corr(mpi ,mqj).
</equation>
<bodyText confidence="0.999976142857143">
To tune the balance of coreferential relation and
correlation relation during learning, we first get
two matrices Wˆ 1 and Wˆ2 by row-normalizing W1
and W2, respectively. Then we obtain the final
mention matrix W with a linear combination of
Wˆ1 and Wˆ2: W = α Wˆ1 + (1 − α) Wˆ2, where α
is the coefficient between 0 and 1 3.
</bodyText>
<subsectionHeader confidence="0.847586">
Graph-based Semi-supervised Learning
</subsectionHeader>
<bodyText confidence="0.99720425">
Intuitively, if two mentions are strongly con-
nected, they tend to hold the same label. The
label of 1 indicates a mention is a morph men-
tion, and 0 means a non-morph mention. We use
Y = [Yl Yu]T to denote the label vector of all
mentions, where the first l nodes are verified men-
tions labeled as 1 or 0, and the remaining u nodes
need to be verified and initialized with the label
0.5. Our final goal is to obtain the final label vec-
tor Yu by incorporating evidence from initial la-
bels and the mention graph.
Following the graph-based semi-supervised
learning algorithm (Zhu et al., 2003), the mention
verification problem is formulated to optimize the
objective function 2(Y) = µ Eli=1(yi − y0i )2 +
2 Ei,j Wij(yi − yj)2 where y0i denotes the initial
</bodyText>
<footnote confidence="0.9235645">
3α is set to 0.8 in this paper, optimized from the develop-
ment set.
</footnote>
<table confidence="0.95581875">
Wi;,Mp ,,Mqi= I 1.0 if mP and m9 are linked
with certain social relation
cos(m0i , m9) else if q E kNN(p)
0 Otherwise
</table>
<page confidence="0.995086">
589
</page>
<bodyText confidence="0.9998926">
label, and p is a regularization parameter that con-
trols the trade-off between initial labels and the
consistency of labels on the mention graph. Zhu
et al. (2003) has proven that this formula has both
closed-form and iterative solutions.
</bodyText>
<sectionHeader confidence="0.994965" genericHeader="method">
4 Morph Mention Resolution
</sectionHeader>
<bodyText confidence="0.9927885">
The final step is to resolve the extracted morph
mentions to their target entities.
</bodyText>
<subsectionHeader confidence="0.994435">
4.1 Candidate Target Identification
</subsectionHeader>
<bodyText confidence="0.9999875625">
We start from identifying a list of target candidates
for each morph mention from the comparable cor-
pora including Sina Weibo, Chinese News and
English Twitter. After preprocessing the corpora
using word segmentation, noun phrase chunking
and name tagging, the name entity list is still too
large and too noisy for candidate ranking. To
clean the name entity list, we adopt the tempo-
ral Distribution Assumption proposed in our re-
cent work (Huang et al., 2013). It assumes that
a morph m and its real target a should have sim-
ilar temporal distributions in terms of their occur-
rences. Following the same heuristic we assume
that an entity is a valid candidate for a morph if
and only if the candidate appears fewer than seven
days after the morph’s appearance.
</bodyText>
<subsectionHeader confidence="0.9944645">
4.2 Candidate Target Ranking
Motivations of Using Deep Learning
</subsectionHeader>
<bodyText confidence="0.999984259259259">
Compared to regular entity linking tasks (Ji et al.,
2010; Ji et al., 2011; Ji et al., 2014), the major
challenge of ranking a morph’s candidate target
entities lies in that the surface features such as the
orthographic similarity between morph and target
candidates have been proven inadequate (Huang
et al., 2013). Therefore, it is crucial to capture
the semantics of both mentions and target candi-
dates. For instance, in order to correctly resolve
“ 西-E (Conquer West King)” from d1 and d3
in Figure 1 to the modern politician “4熙来(Bo
Xilai)” instead of the ancient king “吴三桂 (Wu
Sangui)”, it is important to model the surround-
ing contextual information effectively to capture
important information (e.g., “重庆 (Chongqing)”,
“倒台 (fall from power)”, and “唱红歌 (sing red
songs)”) to represent the mentions and target en-
tity candidates. Inspired by the recent success
achieved by deep learning based techniques on
learning semantic representations for various NLP
tasks (e.g., (Bengio et al., 2003; Collobert et al.,
2011; Mikolov et al., 2013b; He et al., 2013)), we
design and compare the following two approaches
to employ hierarchical architectures with multiple
hidden layers to extract useful features and map
morphs and target entities into a latent semantic
space.
</bodyText>
<subsectionHeader confidence="0.93245">
Pairwise Cross-genre Supervised Learning
</subsectionHeader>
<bodyText confidence="0.943791777777778">
Ideally, we hope to obtain a large amount of coref-
erential entity mention pairs for training. A nat-
ural knowledge resource is Wikipedia which in-
cludes anchor links. We compose an anchor’s sur-
face string and the title of the entity it’s linked to as
a positive training pair. Then we randomly sample
negative training instances from those pairs that
don’t share any links.
Our approach consists of the following steps:
(1) generating high quality embedding for each
training instance; (2) pre-training with the stacked
denoising auto-encoder (Bengio et al., 2003) for
feature dimension reduction; and (3) supervised
fine-tuning to optimize the neural networks to-
wards a similarity measure (e.g., dot product).
Figure 2 depicts the overall architecture of this ap-
proach.
mention candidate target
</bodyText>
<figureCaption confidence="0.991461">
Figure 2: Overall Architecture of Pairwise Cross-
genre Supervised Learning
</figureCaption>
<bodyText confidence="0.9999892">
However, morph resolution is significantly dif-
ferent from the traditional entity linking task since
the latter mainly focuses on formal and explicit
entities (e.g., “4熙来 (Bo Xilai)”) which tend
to have stable referents in Wikipedia. In con-
trast, morphs tend to be informal, implicit and
have newly emergent meanings which evolve over
time. In fact, these morph mentions rarely appear
in Wikipedia. For example, almost all “ 西-E
(Conquer West King)” mentions in Wikipedia re-
fer to the ancient king instead of the modern politi-
cian “4熙来 (Bo Xilai)”. In addition, the contex-
tual words in Wikipedia used to describe entities
are quite different from those in social media. For
example, to describe a death event, Wikipedia usu-
</bodyText>
<figure confidence="0.987098571428571">
sim(m,c) = Dot(f (m), f (c))
pair-wise supervised
fine-tuning layer
n layers stacked
auto-encoders
f f
É. É.
</figure>
<page confidence="0.978397">
590
</page>
<bodyText confidence="0.9947142">
ally uses a formal expression “去世 (pass away)”
while an informal expression “挂了 (hang up)” is
used more often in tweets. Therefore this approach
suffers from the knowledge discrepancy between
these two genres.
</bodyText>
<figure confidence="0.753341">
Within-genre Unsupervised Learning
contezt(Etk[already])
</figure>
<figureCaption confidence="0.999741">
Figure 3: Continuous Bag-of-Words Architecture
</figureCaption>
<bodyText confidence="0.995982555555555">
To address the above challenge, we propose
the second approach to learn semantic embed-
dings of both morph mentions and entities di-
rectly from tweets. Also we prefer unsuper-
vised learning methods due to the lack of train-
ing data. Following (Mikolov et al., 2013a),
we develop a continuous bag-of-words (CBOW)
model that can effectively model the surround-
ing contextual information. CBOW is discrimina-
tively trained by maximizing the conditional prob-
ability of a term wi given its contexts c(wi) =
{wi−n, ..., wi−1, wi+1,..., wi+n}, where n is the
contextual window size, and wi is a term obtained
using the preprocessing step introduced in Sec-
tion 2 4. The architecture of CBOW is depicted in
Figure 3. We obtain a vector Xwi through the pro-
jection layer by summing up the embedding vec-
tors of all terms in c(wi), and then use the sigmoid
activation function to obtain the final embedding
of wi in c(wi) in the output layer.
Formally, the objective function of
CBOW can be formulated as L(B) =
EwiEW Ew jEW log p(wj|c(wi)), where W
is the set of unique terms obtained from the whole
training corpus. p(wj|c(wi)) is the conditional
likelihood of wj given the context c(wi) and it is
formulated as follows:
</bodyText>
<equation confidence="0.98293">
p(wj|c(wi)) = [U(XTw Bwj)]Lwi(wj) ×
`[1 − σ(XTwiBwj)]1−Lwi(wj),
</equation>
<footnote confidence="0.992694">
4Each wi is not limited to noun phrases we consider as
candidate morphs.
</footnote>
<table confidence="0.9998958">
Data Training Development Testing
# Tweets 1,500 500 2,688
# Unique Terms 10,098 4, 848 15,108
# Morphs 250 110 341
# Morph Mentions 1,342 487 2,469
</table>
<tableCaption confidence="0.995617">
Table 1: Data Statistics
</tableCaption>
<equation confidence="0.559037">
� 1, wi = wj
</equation>
<bodyText confidence="0.978239">
where Lwi(wj) = , σ is the
</bodyText>
<sectionHeader confidence="0.666068" genericHeader="method">
0, Otherwise
</sectionHeader>
<bodyText confidence="0.998134">
sigmoid activation function, and Bwi is the embed-
dings of wi to be learned with back-propagation
during training.
</bodyText>
<sectionHeader confidence="0.999211" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.950337">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999984863636364">
We retrieved 1,553,347 tweets from Chinese Sina
Weibo from May 1 to June 30, 2013 and 66,
559 web documents from the embedded URLs
in tweets for experiments. We then randomly
sampled 4,688 non-redundant tweets and asked
two Chinese native speakers to manually anno-
tate morph mentions in these tweets. The anno-
tated dataset is randomly split into training, de-
velopment, and testing sets, with detailed statistics
shown in Table 1 5. We used 225 positive instances
and 225 negative instances to train the model in the
first step of potential morph discovery.
We collected a Chinese Wikipedia dump of Oc-
tober 9th, 2014, which contains 2,539,355 pages.
We pulled out person, organization and geo-
political pages based on entity type matching with
DBpedia 6. We also filter out the pages with fewer
than 300 words. For training the model, we use
60,000 mention-target pairs along with one neg-
ative sample randomly generated for each pair,
among which, 20% pairs are reserved for parame-
ter tuning.
</bodyText>
<subsectionHeader confidence="0.99862">
5.2 Overall: End-to-End Decoding
</subsectionHeader>
<bodyText confidence="0.9996638">
In this subsection, we first study the end-to-end
decoding performance of our best system, and
compare it with the state-of-the-art supervised
learning-to-rank approach proposed by (Huang et
al., 2013) based on information networks con-
struction and traverse with meta-paths. We use
the 225 extracted morphs as input to feed (Huang
et al., 2013) system. The experiment setting, im-
plementation and evaluation process are similar
to (Huang et al., 2013).
</bodyText>
<footnote confidence="0.999684">
5We will make all of these annotations and other resources
available for research purposes if this paper gets accepted.
6http://dbpedia.org
</footnote>
<figure confidence="0.986413714285714">
contezt(�a[fell from power])
summation
contezt(09[sing])
σ(XwTθ)
Xw
contezt(_* [red song])
Input Layer Projection Layer Output Layer
</figure>
<page confidence="0.99458">
591
</page>
<bodyText confidence="0.999879625">
The overall performance of our approach us-
ing within-genre learning for resolution is shown
in Table 2. We can see that our system
achieves significantly better performance (95.0%
confidence level by the Wilcoxon Matched-Pairs
Signed-Ranks Test) than the approach proposed
by (Huang et al., 2013). We found that (Huang
et al., 2013) failed to resolve many unpopular
morphs (e.g., “小马 (Little Ma)” is a morph re-
ferring to Ma Yingjiu, and it only appeared once
in the data), because it heavily relies on aggre-
gating contextual and temporal information from
multiple instances of each morph. In contrast, our
unsupervised resolution approach only leverages
the pre-trained word embeddings to capture the se-
mantics of morph mentions and entities.
</bodyText>
<table confidence="0.999102666666667">
Model Precision Recall Fl
Huang et al., 2013 40.2 33.3 36.4
Our Approach 41.1 35.9 38.3
</table>
<tableCaption confidence="0.997791">
Table 2: End-to-End Morph Decoding (%)
</tableCaption>
<subsectionHeader confidence="0.970159">
5.3 Diagnosis: Morph Mention Extraction
</subsectionHeader>
<bodyText confidence="0.99882">
The first step discovered 888 potential morphs
(80.1% of all morphs, 5.9% of all terms), which
indicates that this step successfully narrowed
down the scope of candidate morphs.
</bodyText>
<table confidence="0.9989035">
Method Precision Recall Fl
Naive 58.0 83.1 68.3
SVMs 61.3 80.7 69.7
Our Approach 88.2 77.2 82.3
</table>
<tableCaption confidence="0.99949">
Table 3: Morph Mention Verification (%)
</tableCaption>
<bodyText confidence="0.999987928571429">
Now we evaluate the performance of morph
mention verification. We compare our approach
with two baseline methods: (i) Naive, which con-
siders all mentions as morph mentions; (ii) SVMs,
a fully supervised model using Support Vector
Machines (Cortes and Vapnik, 1995) based on un-
igrams and bigrams features. Table 3 shows the
results. We can see that our approach achieves sig-
nificantly better performance than the baseline ap-
proaches. In particular it can verify the mentions
of newly emergent morphs. For instance, “棒棒
棒 (Good Good Good)” is mistakenly identified by
the first step as a potential morph, but the second
step correctly filters it out.
</bodyText>
<subsectionHeader confidence="0.949227">
5.4 Diagnosis: Morph Mention Resolution
</subsectionHeader>
<bodyText confidence="0.9999682">
The target candidate identification step success-
fully filters 86% irrelevant entities with high preci-
sion (98.5% of morphs retain their target entitis).
For candidate ranking, we compare with several
baseline approaches as follows:
</bodyText>
<listItem confidence="0.9997118">
• BOW: We compute cosine similarity over bag-
of-words vectors with tf-idf values to measure
the context similarity between a mention and its
candidates.
• Pair-wise Cross-genre Supervised Learning:
</listItem>
<bodyText confidence="0.933007692307693">
We first construct a vocabulary by choosing the
top 100,000 frequent terms. Then we randomly
sample 48,000 instances for training and 12,000
instances for development. At the pre-training
step, we set the number of hidden layers as 3,
the size of each hidden layer as 1000, the mask-
ing noise probability for the first layer as 0.7,
and a Gaussian noise with standard deviation of
0.1 for higher layers. The learning rate is set to
be 0.01. At the fine-tuning stage, we add a 200
units layer on top of auto-encoders and optimize
the neural network models based on the training
data.
</bodyText>
<listItem confidence="0.78218">
• Within-genre Unsupervised Learning: We di-
rectly train morph mention and entity embed-
</listItem>
<bodyText confidence="0.973686263157895">
dings from the large-scale tweets and web doc-
uments that we collect. We set the window size
as 10 and the vector dimension as 800 based on
the development set.
The overall performance of various resolu-
tion approaches using perfect morph mentions is
shown in Figure 4. We can clearly see that our
second within-genre learning approach achieves
the best performance. Figure 5 demonstrates the
differences between our two deep learning based
methods. When learning semantic embeddings di-
rectly from Wikipedia, we can see that the top 10
closest entities of the mention “平西王(Conquer
West King)” are all related to the ancient king “吴
三桂(Wu Sangui)”. Therefore this method is only
able to capture the original meanings of morphs.
In contrast, when we learn embeddings directly
from tweets, most of the closest entities are rel-
evant to its target entity “薄熙来 (Bo Xilai)”.
</bodyText>
<sectionHeader confidence="0.999919" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.998165">
The first morph decoding work (Huang et al.,
2013) assumed morph mentions are already dis-
covered and didn’t take contexts into account. To
the best of our knowledge, this is the first work on
context-aware end-to-end morph decoding.
Morph decoding is related to several traditional
</bodyText>
<page confidence="0.990758">
592
</page>
<figure confidence="0.995395301587302">
“-*�_(Conquer West King)” “-*�_(Conquer West King)” “ 0*(Bo Xilai)”
in Wikipedia in tweets in tweets/web docs
Ii
(Fall of Qin Dynasty)
X)t
(Break the Defense)
rANN
(Chen Yuanyuan)
AM
(Eight Beauties)
xWK
(Army of Qing)
m
(Surrender to
Qing Dynasty)
R=4
(Wu Sangui)
1644*
(Year 1644)
NIJ
(Entitled as)
��
(Qinhuai)
Ii
(Fall of Qin Dynasty)
4XV9
(Zhang Dejiang)
N_
(King of Han)
A=4
(Wu Sangui)
BXL
(Bo Xilai)
_AWK
(Wang Lijun)
m
(Manchuria)
-ta
(Wen Qiang)
—11
(Bo Yibo)
�
(Bo)
&apos;S15f
(Introduce Investment)
ffm
(Suppress Gangster)
Aid
(Violation of Rules)
X03F
(Be Distinguished)
MIi*
(Murdering Case)
19:F·*V
(Neil Heywood)
WK
(Huang Qifan)
4XV9
(Zhang Dejiang)
_AWK
(Wang Lijun)
BXL
(Bo Xilai)
</figure>
<figureCaption confidence="0.985305333333333">
Figure 5: Top 10 closest entities to morph and target in different genres
Figure 4: Resolution Acc@K for Perfect Morph
Mentions
</figureCaption>
<bodyText confidence="0.99925012244898">
NLP tasks: entity mention extraction (e.g., (Zi-
touni and Florian, 2008; Ohta et al., 2012; Li and
Ji, 2014)), metaphor detection (e.g., (Wang et al.,
2006; Tsvetkov, 2013; Heintz et al., 2013)), word
sense disambiguation (WSD) (e.g., (Yarowsky,
1995; Mihalcea, 2007; Navigli, 2009)), and entity
linking (EL) (e.g., (Mihalcea and Csomai, 2007;
Ji et al., 2010; Ji et al., 2011; Ji et al., 2014).
However, none of these previous techniques can
be applied directly to tackle this problem. As
mentioned in section 3.1, entity morphs are fun-
damentally different from regular entity mentions.
Our task is also different from metaphor detec-
tion because morphs cover a much wider range
of semantic categories and can include either ab-
stractive or concrete information. Some common
features for detecting metaphors (e.g. (Tsvetkov,
2013)) are not effective for morph extraction: (1).
Semantic categories. Metaphors usually fall into
certain semantic categories such as noun.animal
and noun.cognition. (2). Degree of abstractness.
If the subject or an object of a concrete verb is
abstract then the verb is likely to be a metaphor.
In contrast, morphs can be very abstract (e.g., “函
数 (Function)” refers to “杨 幂 (Yang Mi)” be-
cause her first name “幂 (Mi)” means the Power
Function) or very concrete (e.g., “4督 (Governor
Bo)” refers to “4熙来 (Bo Xilai)”). In contrast
to traditional WSD where the senses of a word are
usually quite stable, the “sense” (target entity) of
a morph may be newly emergent or evolve over
time rapidly. The same morph can also have mul-
tiple senses. The EL task focuses more on explicit
and formal entities (e.g., named entities), while
morphs tend to be informal and convey implicit
information.
Morph mention detection is also related to mal-
ware detection (e.g., (Firdausi et al., 2010; Chan-
dola et al., 2009; Firdausi et al., 2010; Christodor-
escu and Jha, 2003)) which discovers abnormal
behavior in code and malicious software. In con-
trast our task tackles anomaly texts in semantic
context.
Deep learning-based approaches have been
demonstrated to be effective in disambiguation re-
lated tasks such as WSD (Bordes et al., 2012), en-
tity linking (He et al., 2013) and question link-
ing (Yih et al., 2014; Bordes et al., 2014; Yang
et al., 2014). In this paper we proved that it’s cru-
</bodyText>
<page confidence="0.996229">
593
</page>
<bodyText confidence="0.950947">
cial to keep the genres consistent between learning
embeddings and applying embeddings.
P. Chang, M. Galley, and D. Manning. 2008. Optimiz-
ing chinese word segmentation for machine transla-
tion performance. In Proc. of the Third Workshop on
Statistical Machine Translation (StatMT 2008).
</bodyText>
<sectionHeader confidence="0.790579" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977571428571">
This paper describes the first work of context-
aware end-to-end morph decoding. By conduct-
ing deep analysis to identity the common charac-
teristics of morphs and the unique challenges of
this task, we leverage a large amount of unlabeled
data and the coreferential and correlation relations
to perform collective inference to extract morph
mentions. Then we explore deep learning-based
techniques to capture the semantics of morph men-
tions and entities and resolve morph mentions on
the fly. Our future work includes exploiting the
profiles of target entities as feedback to refine the
results of morph mention extraction. We will also
extend the framework for event morph decoding.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991">
This work was supported by the US ARL
NS-CTA No. W911NF-09-2-0053, DARPA
DEFT No. FA8750-13-2-0041, NSF Awards
IIS-1523198, IIS-1017362, IIS-1320617, IIS-
1354329 and HDTRA1-10-1-0120, gift awards
from IBM, Google, Disney and Bosch. The views
and conclusions contained in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
</bodyText>
<sectionHeader confidence="0.998552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999461119402986">
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137–1155,
March.
A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012.
Joint learning of words and meaning representations
for open-text semantic parsing. In Proc. of the 15th
International Conference on Artificial Intelligence
and Statistics (AISTATS2012).
A. Bordes, S. Chopra, and J. Weston. 2014. Question
answering with subgraph embeddings. In Proc. of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP2014).
V. Chandola, A. Banerjee, and V. Kumar. 2009.
Anomaly detection: A survey. ACM Computing
Surveys (CSUR), 41(3):15.
J. Chen, D. Ji, C Tan, and Z. Niu. 2006. Rela-
tion extraction using label propagation based semi-
supervised learning. In Proc. of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics (ACL2006).
M. Christodorescu and S. Jha. 2003. Static analysis
of executables to detect malicious patterns. In Proc.
of the 12th Conference on USENIX Security Sympo-
sium (SSYM2003).
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural
language processing (almost) from scratch. Jour-
nal of Machine Learning Research, 12:2493–2537,
November.
C. Cortes and V. Vapnik. 1995. Support-vector net-
works. Machine Learning, 20:273–297, September.
I. Firdausi, C. Lim, A. Erwin, and A. Nugroho. 2010.
Analysis of machine learning techniques used in
behavior-based malware detection. In Proc. of
the 2010 Second International Conference on Ad-
vances in Computing, Control, and Telecommunica-
tion Technologies (ACT2010).
Z. Harris. 1954. Distributional structure. Word,
10:146–162.
Z. He, S. Liu, M. Li, M. Zhou, L. Zhang, and H. Wang.
2013. Learning entity representation for entity dis-
ambiguation. In Proc. of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL2013).
I. Heintz, R. Gabbard, M. Srivastava, D. Barner,
D. Black, M. Friedman, and R. Weischedel. 2013.
Automatic extraction of linguistic metaphors with
lda topic modeling. In Proc. of the ACl2013 Work-
shop on Metaphor in NLP.
H. Huang, Z. Wen, D. Yu, H. Ji, Y. Sun, J. Han, and
H. Li. 2013. Resolving entity morphs in censored
data. In Proc. of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (ACL2013).
H. Huang, Y. Cao, X. Huang, H. Ji, and C. Lin.
2014. Collective tweet wikification based on semi-
supervised graph regularization. In Proc. of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL2014).
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. El-
lis. 2010. Overview of the tac 2010 knowledge base
population track. In Proc. of the Text Analysis Con-
ference (TAC2010).
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview
of the tac 2011 knowledge base population track. In
Proc. of the Text Analysis Conference (TAC2011).
</reference>
<page confidence="0.98533">
594
</page>
<reference confidence="0.9997091">
H. Ji, J. Nothman, and H. Ben. 2014. Overview of tac-
kbp2014 entity discovery and linking tasks. In Proc.
of the TextAnalysis Conference (TAC2014).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the
Eighteenth International Conference on Machine
Learning (ICML2001).
Q. Li and H. Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proc. of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL2014).
Q. Li, H. Li, H. Ji, W. Wang, J. Zheng, and F. Huang.
2012. Joint bilingual name tagging for parallel cor-
pora. In Proc. of the 21st ACM International Con-
ference on Information and Knowledge Manage-
ment (CIKM2012).
R. Mihalcea and A. Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In
Proc. of the sixteenth ACM conference on Confer-
ence on information and knowledge management
(CIKM2007).
R. Mihalcea. 2007. Using wikipedia for auto-
matic word sense disambiguation. In Proc. of the
Conference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL2007).
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient estimation of word representations in vec-
tor space. CoRR, abs/1301.3781.
T. Mikolov, I. Sutskever, K. Chen, S.G. Corrado, and
J. Dean. 2013b. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing Systems
26.
R. Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:10:1–10:69,
February.
Z. Niu, D. Ji, and C. Tan. 2005. Word sense dis-
ambiguation using label propagation based semi-
supervised learning. In Proc. of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL2005).
T. Ohta, S. Pyysalo, J. Tsujii, and S. Ananiadou. 2012.
Open-domain anatomical entity mention detection.
In Proc. of the ACL2012 Workshop on Detecting
Structure in Scholarly Discourse.
A. Smola and R. Kondor. 2003. Kernels and regu-
larization on graphs. In Proc. of the Annual Confer-
ence on Computational Learning Theory and Kernel
Workshop (COLT2003).
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology (NAACL2003).
Y. Tsvetkov. 2013. Cross-lingual metaphor detection
using common semantic features. In Proc. of the
ACL2013 Workshop on Metaphor in NLP.
Z. Wang, H. Wang, H. Duan, S. Han, and S. Yu.
2006. Chinese noun phrase metaphor recogni-
tion with maximum entropy approach. In Proc. of
the Seventh International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing2006).
M. Yang, N. Duan, M. Zhou, and H. Rim. 2014. Joint
relational embeddings for knowledge-based ques-
tion answering. In Proc. of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP2014).
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
the 33rd Annual Meeting on Association for Com-
putational Linguistics (ACL1995).
W. Yih, X. He, and C. Meek. 2014. Semantic pars-
ing for single-relation question answering. In Proc.
of the 52nd Annual Meeting of the Association for
Computational Linguistics (ACL2014).
H. Zhang, H. Yu, D. Xiong, and Q. Liu. 2003. Hhmm-
based chinese lexical analyzer ictclas. In Proc. of
the second SIGHAN workshop on Chinese language
processing (SIGHAN2003).
B. Zhang, H. Huang, X. Pan, H. Ji, K. Knight, Z. Wen,
Y. Sun, J. Han, and B. Yener. 2014. Be appropriate
and funny: Automatic entity morph encoding. In
Proc. of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL2014).
D. Zhou, O. Bousquet, T. Lal, J. Weston, and
B. Sch¨olkopf. 2004. Learning with local and global
consistency. In Advances in Neural Information
Processing Systems 16, pages 321–328.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML2003).
I. Zitouni and R. Florian. 2008. Mention detection
crossing the language barrier. In Proc. of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2008).
</reference>
<page confidence="0.998659">
595
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510961">
<title confidence="0.9772075">Context-aware Entity Morph Decoding Hongzhao Xiaoman Sujian Chin-Yew</title>
<author confidence="0.954196">Kevin Zhen Yizhou Jiawei Bulent</author>
<affiliation confidence="0.890755">Polytechnic Institute, University, Research Asia, of Southern</affiliation>
<author confidence="0.641174">T J Watson Research Center</author>
<author confidence="0.641174">of Illinois at University</author>
<abstract confidence="0.999857296296296">People create morphs, a special type of fake alternative names, to achieve certain communication goals such as expressing strong sentiment or evading censors. For the name for a highly venomous snake, is a morph that Bryant for himself due to his agility and aggressiveness in playing basketball games. This paper presents the first end-to-end context-aware entity morph decoding system that can automatically identify, disambiguate, verify morph mentions based on specific contexts, and resolve them to target entities. Our approach is on an absolute it does not require any candidate morph or target entity lists as input, nor any manually constructed morph-target pairs for training. We design a semi-supervised collective inference framework for morph mention extraction, and compare various deep learning based approaches for morph resolution. Our approach achieved significant improvement over the state-of-the-art method (Huang et al., 2013), which used a amount of training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="19098" citStr="Bengio et al., 2003" startWordPosition="3099" endWordPosition="3102">entions and target candidates. For instance, in order to correctly resolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information effectively to capture important information (e.g., “重庆 (Chongqing)”, “倒台 (fall from power)”, and “唱红歌 (sing red songs)”) to represent the mentions and target entity candidates. Inspired by the recent success achieved by deep learning based techniques on learning semantic representations for various NLP tasks (e.g., (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013b; He et al., 2013)), we design and compare the following two approaches to employ hierarchical architectures with multiple hidden layers to extract useful features and map morphs and target entities into a latent semantic space. Pairwise Cross-genre Supervised Learning Ideally, we hope to obtain a large amount of coreferential entity mention pairs for training. A natural knowledge resource is Wikipedia which includes anchor links. We compose an anchor’s surface string and the title of the entity it’s linked to as a positive training pair. Then we </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bordes</author>
<author>X Glorot</author>
<author>J Weston</author>
<author>Y Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In Proc. of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS2012).</booktitle>
<contexts>
<context position="32034" citStr="Bordes et al., 2012" startWordPosition="5190" endWordPosition="5193">e morph can also have multiple senses. The EL task focuses more on explicit and formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M. Galley, and D. Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proc. of the Third Workshop on Statistical Machine Translation (StatMT 2008). 7 Conclusions and Future Work This paper describes the first work of contextaware end-to-end morph decoding. By conducting deep analysis to identity the comm</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In Proc. of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bordes</author>
<author>S Chopra</author>
<author>J Weston</author>
</authors>
<title>Question answering with subgraph embeddings.</title>
<date>2014</date>
<booktitle>In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP2014).</booktitle>
<contexts>
<context position="32128" citStr="Bordes et al., 2014" startWordPosition="5209" endWordPosition="5212"> (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M. Galley, and D. Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proc. of the Third Workshop on Statistical Machine Translation (StatMT 2008). 7 Conclusions and Future Work This paper describes the first work of contextaware end-to-end morph decoding. By conducting deep analysis to identity the common characteristics of morphs and the unique challenges of this task, we leverage a large amoun</context>
</contexts>
<marker>Bordes, Chopra, Weston, 2014</marker>
<rawString>A. Bordes, S. Chopra, and J. Weston. 2014. Question answering with subgraph embeddings. In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Chandola</author>
<author>A Banerjee</author>
<author>V Kumar</author>
</authors>
<title>Anomaly detection: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>3</issue>
<contexts>
<context position="31713" citStr="Chandola et al., 2009" startWordPosition="5139" endWordPosition="5143">(Yang Mi)” because her first name “幂 (Mi)” means the Power Function) or very concrete (e.g., “4督 (Governor Bo)” refers to “4熙来 (Bo Xilai)”). In contrast to traditional WSD where the senses of a word are usually quite stable, the “sense” (target entity) of a morph may be newly emergent or evolve over time rapidly. The same morph can also have multiple senses. The EL task focuses more on explicit and formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M. Galley, and D. Mannin</context>
</contexts>
<marker>Chandola, Banerjee, Kumar, 2009</marker>
<rawString>V. Chandola, A. Banerjee, and V. Kumar. 2009. Anomaly detection: A survey. ACM Computing Surveys (CSUR), 41(3):15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>D Ji</author>
<author>C Tan</author>
<author>Z Niu</author>
</authors>
<title>Relation extraction using label propagation based semisupervised learning.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL2006).</booktitle>
<contexts>
<context position="13757" citStr="Chen et al., 2006" startWordPosition="2200" endWordPosition="2203">he modern politician “薄熙来 (Bo Xilai)”. Hypothesis 2: Those highly correlated mentions tend to either be morph mentions or nonmorph mentions. From our annotated dataset, 49% morph mentions co-occur on tweet level. For example, “ 西 _.(Conquer West King)” and “军 哥(Brother Jun)” are used together in d3 in Figure 1. Based on these hypotheses, we aim to design an effective approach to compensate for the limited annotated data. Graph-based semi-supervised learning approaches (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004) have been successfully applied many NLP tasks (Niu et al., 2005; Chen et al., 2006; Huang et al., 2014). Therefore we build a mention graph to capture the semantic relatedness (weighted arcs) between potential morph mentions (nodes) and propose a semi-supervised graph-based algorithm to collectively verify a set of relevant mentions using a small amount of labeled data. We now describe the detailed algorithm as follows. Mention Graph Construction First, we construct a mention graph that can reflect the association between all the mentions of potential morphs. According to the above two hypotheses, mention coreference and correlation relations are the basis to build our ment</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2006</marker>
<rawString>J. Chen, D. Ji, C Tan, and Z. Niu. 2006. Relation extraction using label propagation based semisupervised learning. In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Christodorescu</author>
<author>S Jha</author>
</authors>
<title>Static analysis of executables to detect malicious patterns.</title>
<date>2003</date>
<booktitle>In Proc. of the 12th Conference on USENIX Security Symposium (SSYM2003).</booktitle>
<contexts>
<context position="31767" citStr="Christodorescu and Jha, 2003" startWordPosition="5148" endWordPosition="5152">ns the Power Function) or very concrete (e.g., “4督 (Governor Bo)” refers to “4熙来 (Bo Xilai)”). In contrast to traditional WSD where the senses of a word are usually quite stable, the “sense” (target entity) of a morph may be newly emergent or evolve over time rapidly. The same morph can also have multiple senses. The EL task focuses more on explicit and formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M. Galley, and D. Manning. 2008. Optimizing chinese word segmentation for mach</context>
</contexts>
<marker>Christodorescu, Jha, 2003</marker>
<rawString>M. Christodorescu and S. Jha. 2003. Static analysis of executables to detect malicious patterns. In Proc. of the 12th Conference on USENIX Security Symposium (SSYM2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="19122" citStr="Collobert et al., 2011" startWordPosition="3103" endWordPosition="3106">ndidates. For instance, in order to correctly resolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information effectively to capture important information (e.g., “重庆 (Chongqing)”, “倒台 (fall from power)”, and “唱红歌 (sing red songs)”) to represent the mentions and target entity candidates. Inspired by the recent success achieved by deep learning based techniques on learning semantic representations for various NLP tasks (e.g., (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013b; He et al., 2013)), we design and compare the following two approaches to employ hierarchical architectures with multiple hidden layers to extract useful features and map morphs and target entities into a latent semantic space. Pairwise Cross-genre Supervised Learning Ideally, we hope to obtain a large amount of coreferential entity mention pairs for training. A natural knowledge resource is Wikipedia which includes anchor links. We compose an anchor’s surface string and the title of the entity it’s linked to as a positive training pair. Then we randomly sample negative</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="10323" citStr="Cortes and Vapnik, 1995" startWordPosition="1646" endWordPosition="1649">ing novel two-step solution. 3.2 Potential Morph Discovery We first introduce the first step of our approach – potential morph discovery, which aims to narrow down the scope of morph candidates without losing recall. This step takes advantage of the common characteristics shared among morphs and identifies the potential morphs using a supervised method, since it is relatively easy to collect a certain number of corpus-level morphs as training data compared to labeling morph mentions. Through formulating this task as a binary classification problem, we adopt the Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) as the learning model. We propose the following four categories of features. Basic: (i) character unigram, bigram, trigram, and surface form; (ii) part-of-speech tags; (iii) the number of characters; (iv) whether some characters are identical. These basic features will help identify several common characteristics of morph candidates (e.g., they are very likely to be nouns, and very unlikely to contain single characters). Dictionary: Many morphs are non-regular names derived from proper names while retaining some characteristics. For example, the morphs “4 督 (Governor Bo)” and “吃 省 (Gourmand P</context>
<context position="26378" citStr="Cortes and Vapnik, 1995" startWordPosition="4268" endWordPosition="4271">g (%) 5.3 Diagnosis: Morph Mention Extraction The first step discovered 888 potential morphs (80.1% of all morphs, 5.9% of all terms), which indicates that this step successfully narrowed down the scope of candidate morphs. Method Precision Recall Fl Naive 58.0 83.1 68.3 SVMs 61.3 80.7 69.7 Our Approach 88.2 77.2 82.3 Table 3: Morph Mention Verification (%) Now we evaluate the performance of morph mention verification. We compare our approach with two baseline methods: (i) Naive, which considers all mentions as morph mentions; (ii) SVMs, a fully supervised model using Support Vector Machines (Cortes and Vapnik, 1995) based on unigrams and bigrams features. Table 3 shows the results. We can see that our approach achieves significantly better performance than the baseline approaches. In particular it can verify the mentions of newly emergent morphs. For instance, “棒棒 棒 (Good Good Good)” is mistakenly identified by the first step as a potential morph, but the second step correctly filters it out. 5.4 Diagnosis: Morph Mention Resolution The target candidate identification step successfully filters 86% irrelevant entities with high precision (98.5% of morphs retain their target entitis). For candidate ranking,</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine Learning, 20:273–297, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Firdausi</author>
<author>C Lim</author>
<author>A Erwin</author>
<author>A Nugroho</author>
</authors>
<title>Analysis of machine learning techniques used in behavior-based malware detection.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Second International Conference on Advances in Computing, Control, and Telecommunication Technologies (ACT2010).</booktitle>
<contexts>
<context position="31690" citStr="Firdausi et al., 2010" startWordPosition="5135" endWordPosition="5138">ction)” refers to “杨 幂 (Yang Mi)” because her first name “幂 (Mi)” means the Power Function) or very concrete (e.g., “4督 (Governor Bo)” refers to “4熙来 (Bo Xilai)”). In contrast to traditional WSD where the senses of a word are usually quite stable, the “sense” (target entity) of a morph may be newly emergent or evolve over time rapidly. The same morph can also have multiple senses. The EL task focuses more on explicit and formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M</context>
</contexts>
<marker>Firdausi, Lim, Erwin, Nugroho, 2010</marker>
<rawString>I. Firdausi, C. Lim, A. Erwin, and A. Nugroho. 2010. Analysis of machine learning techniques used in behavior-based malware detection. In Proc. of the 2010 Second International Conference on Advances in Computing, Control, and Telecommunication Technologies (ACT2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--146</pages>
<contexts>
<context position="5093" citStr="Harris, 1954" startWordPosition="766" endWordPosition="767"> many morphs contain informal terms with hidden information. For example, “不厚 (not thick)” is used to refer to “4熙来 (Bo Xilai)” whose last name “4 (Bo)” means “thin”. Therefore we attempt to model the rich contexts with careful considerations for morph characteristics both globally (e.g., language models learned from a large amount of data) and locally (e.g. phonetic anomaly analysis) to extract morph mentions. For morph resolution, the main challenge lies in that the surface forms of morphs usually appear quite different from their target entity names. Based on the distributional hypothesis (Harris, 1954) which states that words that often occur in similar contexts tend to have similar meanings, we propose to use deep learning techniques to capture and compare the deep semantic representations of a morph and its candidate target entities based on their contextual clues. For example, the morph “平西王(Conquer West King)” and its target entity “4熙来 (Bo Xilai)” share similar implicit contextual representations such as “重庆(Chongqing)” (Bo was the governor of Chongqing) and “倒台 (fall from power)”. Challenge 3: Lack of labeled data To the best of our knowledge, no sufficient mention-level morph annotat</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word, 10:146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z He</author>
<author>S Liu</author>
<author>M Li</author>
<author>M Zhou</author>
<author>L Zhang</author>
<author>H Wang</author>
</authors>
<title>Learning entity representation for entity disambiguation.</title>
<date>2013</date>
<booktitle>In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013).</booktitle>
<contexts>
<context position="19163" citStr="He et al., 2013" startWordPosition="3111" endWordPosition="3114">esolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information effectively to capture important information (e.g., “重庆 (Chongqing)”, “倒台 (fall from power)”, and “唱红歌 (sing red songs)”) to represent the mentions and target entity candidates. Inspired by the recent success achieved by deep learning based techniques on learning semantic representations for various NLP tasks (e.g., (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013b; He et al., 2013)), we design and compare the following two approaches to employ hierarchical architectures with multiple hidden layers to extract useful features and map morphs and target entities into a latent semantic space. Pairwise Cross-genre Supervised Learning Ideally, we hope to obtain a large amount of coreferential entity mention pairs for training. A natural knowledge resource is Wikipedia which includes anchor links. We compose an anchor’s surface string and the title of the entity it’s linked to as a positive training pair. Then we randomly sample negative training instances from those pairs that</context>
<context position="32068" citStr="He et al., 2013" startWordPosition="5197" endWordPosition="5200"> The EL task focuses more on explicit and formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M. Galley, and D. Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proc. of the Third Workshop on Statistical Machine Translation (StatMT 2008). 7 Conclusions and Future Work This paper describes the first work of contextaware end-to-end morph decoding. By conducting deep analysis to identity the common characteristics of morphs and t</context>
</contexts>
<marker>He, Liu, Li, Zhou, Zhang, Wang, 2013</marker>
<rawString>Z. He, S. Liu, M. Li, M. Zhou, L. Zhang, and H. Wang. 2013. Learning entity representation for entity disambiguation. In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Heintz</author>
<author>R Gabbard</author>
<author>M Srivastava</author>
<author>D Barner</author>
<author>D Black</author>
<author>M Friedman</author>
<author>R Weischedel</author>
</authors>
<title>Automatic extraction of linguistic metaphors with lda topic modeling.</title>
<date>2013</date>
<booktitle>In Proc. of the ACl2013 Workshop on Metaphor in NLP.</booktitle>
<contexts>
<context position="30082" citStr="Heintz et al., 2013" startWordPosition="4873" endWordPosition="4876">L (Bo Xilai) _AWK (Wang Lijun) m (Manchuria) -ta (Wen Qiang) —11 (Bo Yibo) � (Bo) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detect</context>
</contexts>
<marker>Heintz, Gabbard, Srivastava, Barner, Black, Friedman, Weischedel, 2013</marker>
<rawString>I. Heintz, R. Gabbard, M. Srivastava, D. Barner, D. Black, M. Friedman, and R. Weischedel. 2013. Automatic extraction of linguistic metaphors with lda topic modeling. In Proc. of the ACl2013 Workshop on Metaphor in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Huang</author>
<author>Z Wen</author>
<author>D Yu</author>
<author>H Ji</author>
<author>Y Sun</author>
<author>J Han</author>
<author>H Li</author>
</authors>
<title>Resolving entity morphs in censored data.</title>
<date>2013</date>
<booktitle>In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013).</booktitle>
<contexts>
<context position="1587" citStr="Huang et al., 2013" startWordPosition="210" endWordPosition="213">t-aware entity morph decoding system that can automatically identify, disambiguate, verify morph mentions based on specific contexts, and resolve them to target entities. Our approach is based on an absolute “cold-start” - it does not require any candidate morph or target entity lists as input, nor any manually constructed morph-target pairs for training. We design a semi-supervised collective inference framework for morph mention extraction, and compare various deep learning based approaches for morph resolution. Our approach achieved significant improvement over the state-of-the-art method (Huang et al., 2013), which used a large amount of training data. 1 1 Introduction Morphs (Huang et al., 2013; Zhang et al., 2014) refer to the fake alternative names created by social media users to entertain readers or evade censors. For example, during the World Cup in 2014, 1The data set and programs are publicly available at: http://nlp.cs.rpi.edu/data/morphdecoding.zip and http://nlp.cs.rpi.edu/software/morphdecoding.tar.gz a morph “Su-tooth” was created to refer to the Uruguay striker “Luis Suarez” for his habit of biting other players. Automatically decoding humangenerated morphs in text is critical for d</context>
<context position="6586" citStr="Huang et al., 2013" startWordPosition="993" endWordPosition="996">n the observation that morphs tend to share similar characteristics and appear together, we propose a semi-supervised collective inference approach to extract morph mentions from multiple tweets simultaneously. Deep learning techniques have been successfully used to model word representation in an unsupervised fashion. For morph resolution, we make use of a large amount of unlabeled data to learn the semantic representations of morphs and target entities based on the unsupervised continuous bag-of-words method (Mikolov et al., 2013b). 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1, d2, ..., d|D|}, we define a candidate morph mi as a unique term tj in T, where T = {t1, t2, ..., t|T|} is the set of unique terms in D. To extract T, we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyze</context>
<context position="17739" citStr="Huang et al., 2013" startWordPosition="2876" endWordPosition="2879">e solutions. 4 Morph Mention Resolution The final step is to resolve the extracted morph mentions to their target entities. 4.1 Candidate Target Identification We start from identifying a list of target candidates for each morph mention from the comparable corpora including Sina Weibo, Chinese News and English Twitter. After preprocessing the corpora using word segmentation, noun phrase chunking and name tagging, the name entity list is still too large and too noisy for candidate ranking. To clean the name entity list, we adopt the temporal Distribution Assumption proposed in our recent work (Huang et al., 2013). It assumes that a morph m and its real target a should have similar temporal distributions in terms of their occurrences. Following the same heuristic we assume that an entity is a valid candidate for a morph if and only if the candidate appears fewer than seven days after the morph’s appearance. 4.2 Candidate Target Ranking Motivations of Using Deep Learning Compared to regular entity linking tasks (Ji et al., 2010; Ji et al., 2011; Ji et al., 2014), the major challenge of ranking a morph’s candidate target entities lies in that the surface features such as the orthographic similarity betwe</context>
<context position="24362" citStr="Huang et al., 2013" startWordPosition="3955" endWordPosition="3958">9th, 2014, which contains 2,539,355 pages. We pulled out person, organization and geopolitical pages based on entity type matching with DBpedia 6. We also filter out the pages with fewer than 300 words. For training the model, we use 60,000 mention-target pairs along with one negative sample randomly generated for each pair, among which, 20% pairs are reserved for parameter tuning. 5.2 Overall: End-to-End Decoding In this subsection, we first study the end-to-end decoding performance of our best system, and compare it with the state-of-the-art supervised learning-to-rank approach proposed by (Huang et al., 2013) based on information networks construction and traverse with meta-paths. We use the 225 extracted morphs as input to feed (Huang et al., 2013) system. The experiment setting, implementation and evaluation process are similar to (Huang et al., 2013). 5We will make all of these annotations and other resources available for research purposes if this paper gets accepted. 6http://dbpedia.org contezt(�a[fell from power]) summation contezt(09[sing]) σ(XwTθ) Xw contezt(_* [red song]) Input Layer Projection Layer Output Layer 591 The overall performance of our approach using within-genre learning for </context>
<context position="25677" citStr="Huang et al., 2013" startWordPosition="4157" endWordPosition="4160">nce (95.0% confidence level by the Wilcoxon Matched-Pairs Signed-Ranks Test) than the approach proposed by (Huang et al., 2013). We found that (Huang et al., 2013) failed to resolve many unpopular morphs (e.g., “小马 (Little Ma)” is a morph referring to Ma Yingjiu, and it only appeared once in the data), because it heavily relies on aggregating contextual and temporal information from multiple instances of each morph. In contrast, our unsupervised resolution approach only leverages the pre-trained word embeddings to capture the semantics of morph mentions and entities. Model Precision Recall Fl Huang et al., 2013 40.2 33.3 36.4 Our Approach 41.1 35.9 38.3 Table 2: End-to-End Morph Decoding (%) 5.3 Diagnosis: Morph Mention Extraction The first step discovered 888 potential morphs (80.1% of all morphs, 5.9% of all terms), which indicates that this step successfully narrowed down the scope of candidate morphs. Method Precision Recall Fl Naive 58.0 83.1 68.3 SVMs 61.3 80.7 69.7 Our Approach 88.2 77.2 82.3 Table 3: Morph Mention Verification (%) Now we evaluate the performance of morph mention verification. We compare our approach with two baseline methods: (i) Naive, which considers all mentions as morph </context>
<context position="28831" citStr="Huang et al., 2013" startWordPosition="4670" endWordPosition="4673">nre learning approach achieves the best performance. Figure 5 demonstrates the differences between our two deep learning based methods. When learning semantic embeddings directly from Wikipedia, we can see that the top 10 closest entities of the mention “平西王(Conquer West King)” are all related to the ancient king “吴 三桂(Wu Sangui)”. Therefore this method is only able to capture the original meanings of morphs. In contrast, when we learn embeddings directly from tweets, most of the closest entities are relevant to its target entity “薄熙来 (Bo Xilai)”. 6 Related Work The first morph decoding work (Huang et al., 2013) assumed morph mentions are already discovered and didn’t take contexts into account. To the best of our knowledge, this is the first work on context-aware end-to-end morph decoding. Morph decoding is related to several traditional 592 “-*�_(Conquer West King)” “-*�_(Conquer West King)” “ 0*(Bo Xilai)” in Wikipedia in tweets in tweets/web docs Ii (Fall of Qin Dynasty) X)t (Break the Defense) rANN (Chen Yuanyuan) AM (Eight Beauties) xWK (Army of Qing) m (Surrender to Qing Dynasty) R=4 (Wu Sangui) 1644* (Year 1644) NIJ (Entitled as) �� (Qinhuai) Ii (Fall of Qin Dynasty) 4XV9 (Zhang Dejiang) N_ (</context>
</contexts>
<marker>Huang, Wen, Yu, Ji, Sun, Han, Li, 2013</marker>
<rawString>H. Huang, Z. Wen, D. Yu, H. Ji, Y. Sun, J. Han, and H. Li. 2013. Resolving entity morphs in censored data. In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Huang</author>
<author>Y Cao</author>
<author>X Huang</author>
<author>H Ji</author>
<author>C Lin</author>
</authors>
<title>Collective tweet wikification based on semisupervised graph regularization.</title>
<date>2014</date>
<booktitle>In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</booktitle>
<contexts>
<context position="13778" citStr="Huang et al., 2014" startWordPosition="2204" endWordPosition="2207">n “薄熙来 (Bo Xilai)”. Hypothesis 2: Those highly correlated mentions tend to either be morph mentions or nonmorph mentions. From our annotated dataset, 49% morph mentions co-occur on tweet level. For example, “ 西 _.(Conquer West King)” and “军 哥(Brother Jun)” are used together in d3 in Figure 1. Based on these hypotheses, we aim to design an effective approach to compensate for the limited annotated data. Graph-based semi-supervised learning approaches (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004) have been successfully applied many NLP tasks (Niu et al., 2005; Chen et al., 2006; Huang et al., 2014). Therefore we build a mention graph to capture the semantic relatedness (weighted arcs) between potential morph mentions (nodes) and propose a semi-supervised graph-based algorithm to collectively verify a set of relevant mentions using a small amount of labeled data. We now describe the detailed algorithm as follows. Mention Graph Construction First, we construct a mention graph that can reflect the association between all the mentions of potential morphs. According to the above two hypotheses, mention coreference and correlation relations are the basis to build our mention graph, which is r</context>
</contexts>
<marker>Huang, Cao, Huang, Ji, Lin, 2014</marker>
<rawString>H. Huang, Y. Cao, X. Huang, H. Ji, and C. Lin. 2014. Collective tweet wikification based on semisupervised graph regularization. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
<author>H T Dang</author>
<author>K Griffitt</author>
<author>J Ellis</author>
</authors>
<title>Overview of the tac 2010 knowledge base population track.</title>
<date>2010</date>
<booktitle>In Proc. of the Text Analysis Conference (TAC2010).</booktitle>
<contexts>
<context position="18160" citStr="Ji et al., 2010" startWordPosition="2949" endWordPosition="2952"> entity list is still too large and too noisy for candidate ranking. To clean the name entity list, we adopt the temporal Distribution Assumption proposed in our recent work (Huang et al., 2013). It assumes that a morph m and its real target a should have similar temporal distributions in terms of their occurrences. Following the same heuristic we assume that an entity is a valid candidate for a morph if and only if the candidate appears fewer than seven days after the morph’s appearance. 4.2 Candidate Target Ranking Motivations of Using Deep Learning Compared to regular entity linking tasks (Ji et al., 2010; Ji et al., 2011; Ji et al., 2014), the major challenge of ranking a morph’s candidate target entities lies in that the surface features such as the orthographic similarity between morph and target candidates have been proven inadequate (Huang et al., 2013). Therefore, it is crucial to capture the semantics of both mentions and target candidates. For instance, in order to correctly resolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information </context>
<context position="30248" citStr="Ji et al., 2010" startWordPosition="4897" endWordPosition="4900">stinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 2013)) are not effective for morph extraction: (1). Semantic categories. Metaphors usually fall into certain semantic categories such a</context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2010</marker>
<rawString>H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis. 2010. Overview of the tac 2010 knowledge base population track. In Proc. of the Text Analysis Conference (TAC2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
<author>H T Dang</author>
</authors>
<title>Overview of the tac 2011 knowledge base population track.</title>
<date>2011</date>
<booktitle>In Proc. of the Text Analysis Conference (TAC2011).</booktitle>
<contexts>
<context position="18177" citStr="Ji et al., 2011" startWordPosition="2953" endWordPosition="2956">till too large and too noisy for candidate ranking. To clean the name entity list, we adopt the temporal Distribution Assumption proposed in our recent work (Huang et al., 2013). It assumes that a morph m and its real target a should have similar temporal distributions in terms of their occurrences. Following the same heuristic we assume that an entity is a valid candidate for a morph if and only if the candidate appears fewer than seven days after the morph’s appearance. 4.2 Candidate Target Ranking Motivations of Using Deep Learning Compared to regular entity linking tasks (Ji et al., 2010; Ji et al., 2011; Ji et al., 2014), the major challenge of ranking a morph’s candidate target entities lies in that the surface features such as the orthographic similarity between morph and target candidates have been proven inadequate (Huang et al., 2013). Therefore, it is crucial to capture the semantics of both mentions and target candidates. For instance, in order to correctly resolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information effectively to ca</context>
<context position="30265" citStr="Ji et al., 2011" startWordPosition="4901" endWordPosition="4904"> (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 2013)) are not effective for morph extraction: (1). Semantic categories. Metaphors usually fall into certain semantic categories such as noun.animal and</context>
</contexts>
<marker>Ji, Grishman, Dang, 2011</marker>
<rawString>H. Ji, R. Grishman, and H.T. Dang. 2011. Overview of the tac 2011 knowledge base population track. In Proc. of the Text Analysis Conference (TAC2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>J Nothman</author>
<author>H Ben</author>
</authors>
<title>Overview of tackbp2014 entity discovery and linking tasks.</title>
<date>2014</date>
<booktitle>In Proc. of the TextAnalysis Conference (TAC2014).</booktitle>
<contexts>
<context position="18195" citStr="Ji et al., 2014" startWordPosition="2957" endWordPosition="2960">d too noisy for candidate ranking. To clean the name entity list, we adopt the temporal Distribution Assumption proposed in our recent work (Huang et al., 2013). It assumes that a morph m and its real target a should have similar temporal distributions in terms of their occurrences. Following the same heuristic we assume that an entity is a valid candidate for a morph if and only if the candidate appears fewer than seven days after the morph’s appearance. 4.2 Candidate Target Ranking Motivations of Using Deep Learning Compared to regular entity linking tasks (Ji et al., 2010; Ji et al., 2011; Ji et al., 2014), the major challenge of ranking a morph’s candidate target entities lies in that the surface features such as the orthographic similarity between morph and target candidates have been proven inadequate (Huang et al., 2013). Therefore, it is crucial to capture the semantics of both mentions and target candidates. For instance, in order to correctly resolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information effectively to capture important in</context>
<context position="30283" citStr="Ji et al., 2014" startWordPosition="4905" endWordPosition="4908"> 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 2013)) are not effective for morph extraction: (1). Semantic categories. Metaphors usually fall into certain semantic categories such as noun.animal and noun.cognition. (</context>
</contexts>
<marker>Ji, Nothman, Ben, 2014</marker>
<rawString>H. Ji, J. Nothman, and H. Ben. 2014. Overview of tackbp2014 entity discovery and linking tasks. In Proc. of the TextAnalysis Conference (TAC2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of the Eighteenth International Conference on Machine Learning (ICML2001).</booktitle>
<contexts>
<context position="9271" citStr="Lafferty et al., 2001" startWordPosition="1472" endWordPosition="1475">E M) from a specific context dj is a morph mention or not. • Morph Mention Resolution: The final step is to resolve each morph mention mpi to its target entity (e.g., “4熙A,- (Bo Xilai)” for the morph mention “平西王 (Conquer West King)” in d1 in Figure 1). 3 Morph Mention Extraction 3.1 Why Traditional Entity Mention Extraction doesn’t Work In order to automatically extract morph mentions from any given documents, our first reflection is to formulate the task as a sequence labeling problem, just like labeling regular entity mentions. We adopted the commonly used conditional random fields (CRFs) (Lafferty et al., 2001) and got only 6% F-score. Many morphs are not presented as regular entity mentions. For example, the morph “天 线 (Antenna)” refers to “温 家 宝 (Wen Jiabao)” because it shares one character “宝 (baby)” with the famous children’s television series “天 线宝宝 (Teletubbies)”. Even when they are presented as regular entity mentions, they must refer to new target entities which are different from the regular ones. So we propose the following novel two-step solution. 3.2 Potential Morph Discovery We first introduce the first step of our approach – potential morph discovery, which aims to narrow down the scop</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of the Eighteenth International Conference on Machine Learning (ICML2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Li</author>
<author>H Ji</author>
</authors>
<title>Incremental joint extraction of entity mentions and relations.</title>
<date>2014</date>
<booktitle>In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</booktitle>
<contexts>
<context position="29997" citStr="Li and Ji, 2014" startWordPosition="4860" endWordPosition="4863">Ii (Fall of Qin Dynasty) 4XV9 (Zhang Dejiang) N_ (King of Han) A=4 (Wu Sangui) BXL (Bo Xilai) _AWK (Wang Lijun) m (Manchuria) -ta (Wen Qiang) —11 (Bo Yibo) � (Bo) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and ca</context>
</contexts>
<marker>Li, Ji, 2014</marker>
<rawString>Q. Li and H. Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Li</author>
<author>H Li</author>
<author>H Ji</author>
<author>W Wang</author>
<author>J Zheng</author>
<author>F Huang</author>
</authors>
<title>Joint bilingual name tagging for parallel corpora.</title>
<date>2012</date>
<booktitle>In Proc. of the 21st ACM International Conference on Information and Knowledge Management (CIKM2012).</booktitle>
<contexts>
<context position="11106" citStr="Li et al., 2012" startWordPosition="1765" endWordPosition="1768">; (iii) the number of characters; (iv) whether some characters are identical. These basic features will help identify several common characteristics of morph candidates (e.g., they are very likely to be nouns, and very unlikely to contain single characters). Dictionary: Many morphs are non-regular names derived from proper names while retaining some characteristics. For example, the morphs “4 督 (Governor Bo)” and “吃 省 (Gourmand Province)” are derived from their target entity names “4熙A,- (Bo Xilai)” and “广东省 (Guandong Province)”, respectively. Therefore, we adopt a dictionary of proper names (Li et al., 2012) and propose the following features: (i) Whether a term occurs in the dictionary. (ii) Whether a term starts with a commonly used last name, and includes uncommonly used characters as its first name. (iii) Whether a term ends with a geopolitical entity or organization suffix word, but it’s not in the dictionary. Phonetic: Many morphs are created based on phonetic (Chinese pinyin in our case) modifications. For instance, the morph “饭 * * (Rice Cake)” has the same phonetic transcription as its target entity name “范** (Fan Bingbing)”. To extract phonetic-based features, we compile a dictionary co</context>
</contexts>
<marker>Li, Li, Ji, Wang, Zheng, Huang, 2012</marker>
<rawString>Q. Li, H. Li, H. Ji, W. Wang, J. Zheng, and F. Huang. 2012. Joint bilingual name tagging for parallel corpora. In Proc. of the 21st ACM International Conference on Information and Knowledge Management (CIKM2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proc. of the sixteenth ACM conference on Conference on information and knowledge management (CIKM2007).</booktitle>
<contexts>
<context position="30231" citStr="Mihalcea and Csomai, 2007" startWordPosition="4893" endWordPosition="4896">ation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 2013)) are not effective for morph extraction: (1). Semantic categories. Metaphors usually fall into certain semantic </context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>R. Mihalcea and A. Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In Proc. of the sixteenth ACM conference on Conference on information and knowledge management (CIKM2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Using wikipedia for automatic word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL2007).</booktitle>
<contexts>
<context position="30155" citStr="Mihalcea, 2007" startWordPosition="4884" endWordPosition="4885">o) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 2013)) are not effective for morph extract</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>R. Mihalcea. 2007. Using wikipedia for automatic word sense disambiguation. In Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="6505" citStr="Mikolov et al., 2013" startWordPosition="980" endWordPosition="983">ovel approaches to save annotation cost in each step. For morph extraction, based on the observation that morphs tend to share similar characteristics and appear together, we propose a semi-supervised collective inference approach to extract morph mentions from multiple tweets simultaneously. Deep learning techniques have been successfully used to model word representation in an unsupervised fashion. For morph resolution, we make use of a large amount of unlabeled data to learn the semantic representations of morphs and target entities based on the unsupervised continuous bag-of-words method (Mikolov et al., 2013b). 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1, d2, ..., d|D|}, we define a candidate morph mi as a unique term tj in T, where T = {t1, t2, ..., t|T|} is the set of unique terms in D. To extract T, we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), S</context>
<context position="19144" citStr="Mikolov et al., 2013" startWordPosition="3107" endWordPosition="3110">in order to correctly resolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information effectively to capture important information (e.g., “重庆 (Chongqing)”, “倒台 (fall from power)”, and “唱红歌 (sing red songs)”) to represent the mentions and target entity candidates. Inspired by the recent success achieved by deep learning based techniques on learning semantic representations for various NLP tasks (e.g., (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013b; He et al., 2013)), we design and compare the following two approaches to employ hierarchical architectures with multiple hidden layers to extract useful features and map morphs and target entities into a latent semantic space. Pairwise Cross-genre Supervised Learning Ideally, we hope to obtain a large amount of coreferential entity mention pairs for training. A natural knowledge resource is Wikipedia which includes anchor links. We compose an anchor’s surface string and the title of the entity it’s linked to as a positive training pair. Then we randomly sample negative training instances fr</context>
<context position="21692" citStr="Mikolov et al., 2013" startWordPosition="3505" endWordPosition="3508"> n layers stacked auto-encoders f f É. É. 590 ally uses a formal expression “去世 (pass away)” while an informal expression “挂了 (hang up)” is used more often in tweets. Therefore this approach suffers from the knowledge discrepancy between these two genres. Within-genre Unsupervised Learning contezt(Etk[already]) Figure 3: Continuous Bag-of-Words Architecture To address the above challenge, we propose the second approach to learn semantic embeddings of both morph mentions and entities directly from tweets. Also we prefer unsupervised learning methods due to the lack of training data. Following (Mikolov et al., 2013a), we develop a continuous bag-of-words (CBOW) model that can effectively model the surrounding contextual information. CBOW is discriminatively trained by maximizing the conditional probability of a term wi given its contexts c(wi) = {wi−n, ..., wi−1, wi+1,..., wi+n}, where n is the contextual window size, and wi is a term obtained using the preprocessing step introduced in Section 2 4. The architecture of CBOW is depicted in Figure 3. We obtain a vector Xwi through the projection layer by summing up the embedding vectors of all terms in c(wi), and then use the sigmoid activation function to</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>I Sutskever</author>
<author>K Chen</author>
<author>S G Corrado</author>
<author>J Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26.</booktitle>
<contexts>
<context position="6505" citStr="Mikolov et al., 2013" startWordPosition="980" endWordPosition="983">ovel approaches to save annotation cost in each step. For morph extraction, based on the observation that morphs tend to share similar characteristics and appear together, we propose a semi-supervised collective inference approach to extract morph mentions from multiple tweets simultaneously. Deep learning techniques have been successfully used to model word representation in an unsupervised fashion. For morph resolution, we make use of a large amount of unlabeled data to learn the semantic representations of morphs and target entities based on the unsupervised continuous bag-of-words method (Mikolov et al., 2013b). 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1, d2, ..., d|D|}, we define a candidate morph mi as a unique term tj in T, where T = {t1, t2, ..., t|T|} is the set of unique terms in D. To extract T, we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), S</context>
<context position="19144" citStr="Mikolov et al., 2013" startWordPosition="3107" endWordPosition="3110">in order to correctly resolve “ 西-E (Conquer West King)” from d1 and d3 in Figure 1 to the modern politician “4熙来(Bo Xilai)” instead of the ancient king “吴三桂 (Wu Sangui)”, it is important to model the surrounding contextual information effectively to capture important information (e.g., “重庆 (Chongqing)”, “倒台 (fall from power)”, and “唱红歌 (sing red songs)”) to represent the mentions and target entity candidates. Inspired by the recent success achieved by deep learning based techniques on learning semantic representations for various NLP tasks (e.g., (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013b; He et al., 2013)), we design and compare the following two approaches to employ hierarchical architectures with multiple hidden layers to extract useful features and map morphs and target entities into a latent semantic space. Pairwise Cross-genre Supervised Learning Ideally, we hope to obtain a large amount of coreferential entity mention pairs for training. A natural knowledge resource is Wikipedia which includes anchor links. We compose an anchor’s surface string and the title of the entity it’s linked to as a positive training pair. Then we randomly sample negative training instances fr</context>
<context position="21692" citStr="Mikolov et al., 2013" startWordPosition="3505" endWordPosition="3508"> n layers stacked auto-encoders f f É. É. 590 ally uses a formal expression “去世 (pass away)” while an informal expression “挂了 (hang up)” is used more often in tweets. Therefore this approach suffers from the knowledge discrepancy between these two genres. Within-genre Unsupervised Learning contezt(Etk[already]) Figure 3: Continuous Bag-of-Words Architecture To address the above challenge, we propose the second approach to learn semantic embeddings of both morph mentions and entities directly from tweets. Also we prefer unsupervised learning methods due to the lack of training data. Following (Mikolov et al., 2013a), we develop a continuous bag-of-words (CBOW) model that can effectively model the surrounding contextual information. CBOW is discriminatively trained by maximizing the conditional probability of a term wi given its contexts c(wi) = {wi−n, ..., wi−1, wi+1,..., wi+n}, where n is the contextual window size, and wi is a term obtained using the preprocessing step introduced in Section 2 4. The architecture of CBOW is depicted in Figure 3. We obtain a vector Xwi through the projection layer by summing up the embedding vectors of all terms in c(wi), and then use the sigmoid activation function to</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, I. Sutskever, K. Chen, S.G. Corrado, and J. Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<pages>41--10</pages>
<contexts>
<context position="30171" citStr="Navigli, 2009" startWordPosition="4886" endWordPosition="4887">uce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 2013)) are not effective for morph extraction: (1). Semant</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>R. Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41:10:1–10:69, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Niu</author>
<author>D Ji</author>
<author>C Tan</author>
</authors>
<title>Word sense disambiguation using label propagation based semisupervised learning.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL2005).</booktitle>
<contexts>
<context position="13738" citStr="Niu et al., 2005" startWordPosition="2196" endWordPosition="2199">ey both refer to the modern politician “薄熙来 (Bo Xilai)”. Hypothesis 2: Those highly correlated mentions tend to either be morph mentions or nonmorph mentions. From our annotated dataset, 49% morph mentions co-occur on tweet level. For example, “ 西 _.(Conquer West King)” and “军 哥(Brother Jun)” are used together in d3 in Figure 1. Based on these hypotheses, we aim to design an effective approach to compensate for the limited annotated data. Graph-based semi-supervised learning approaches (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004) have been successfully applied many NLP tasks (Niu et al., 2005; Chen et al., 2006; Huang et al., 2014). Therefore we build a mention graph to capture the semantic relatedness (weighted arcs) between potential morph mentions (nodes) and propose a semi-supervised graph-based algorithm to collectively verify a set of relevant mentions using a small amount of labeled data. We now describe the detailed algorithm as follows. Mention Graph Construction First, we construct a mention graph that can reflect the association between all the mentions of potential morphs. According to the above two hypotheses, mention coreference and correlation relations are the basi</context>
</contexts>
<marker>Niu, Ji, Tan, 2005</marker>
<rawString>Z. Niu, D. Ji, and C. Tan. 2005. Word sense disambiguation using label propagation based semisupervised learning. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ohta</author>
<author>S Pyysalo</author>
<author>J Tsujii</author>
<author>S Ananiadou</author>
</authors>
<title>Open-domain anatomical entity mention detection.</title>
<date>2012</date>
<booktitle>In Proc. of the ACL2012 Workshop on Detecting Structure in Scholarly Discourse.</booktitle>
<contexts>
<context position="29979" citStr="Ohta et al., 2012" startWordPosition="4856" endWordPosition="4859">d as) �� (Qinhuai) Ii (Fall of Qin Dynasty) 4XV9 (Zhang Dejiang) N_ (King of Han) A=4 (Wu Sangui) BXL (Bo Xilai) _AWK (Wang Lijun) m (Manchuria) -ta (Wen Qiang) —11 (Bo Yibo) � (Bo) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic</context>
</contexts>
<marker>Ohta, Pyysalo, Tsujii, Ananiadou, 2012</marker>
<rawString>T. Ohta, S. Pyysalo, J. Tsujii, and S. Ananiadou. 2012. Open-domain anatomical entity mention detection. In Proc. of the ACL2012 Workshop on Detecting Structure in Scholarly Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smola</author>
<author>R Kondor</author>
</authors>
<title>Kernels and regularization on graphs.</title>
<date>2003</date>
<booktitle>In Proc. of the Annual Conference on Computational Learning Theory and Kernel Workshop (COLT2003).</booktitle>
<contexts>
<context position="13654" citStr="Smola and Kondor, 2003" startWordPosition="2180" endWordPosition="2183">e morph mentions “ 西_. (Conquer West King)” in d1 and d3 in Figure 1 are coreferential, they both refer to the modern politician “薄熙来 (Bo Xilai)”. Hypothesis 2: Those highly correlated mentions tend to either be morph mentions or nonmorph mentions. From our annotated dataset, 49% morph mentions co-occur on tweet level. For example, “ 西 _.(Conquer West King)” and “军 哥(Brother Jun)” are used together in d3 in Figure 1. Based on these hypotheses, we aim to design an effective approach to compensate for the limited annotated data. Graph-based semi-supervised learning approaches (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004) have been successfully applied many NLP tasks (Niu et al., 2005; Chen et al., 2006; Huang et al., 2014). Therefore we build a mention graph to capture the semantic relatedness (weighted arcs) between potential morph mentions (nodes) and propose a semi-supervised graph-based algorithm to collectively verify a set of relevant mentions using a small amount of labeled data. We now describe the detailed algorithm as follows. Mention Graph Construction First, we construct a mention graph that can reflect the association between all the mentions of potential morphs. According to </context>
</contexts>
<marker>Smola, Kondor, 2003</marker>
<rawString>A. Smola and R. Kondor. 2003. Kernels and regularization on graphs. In Proc. of the Annual Conference on Computational Learning Theory and Kernel Workshop (COLT2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL2003).</booktitle>
<contexts>
<context position="7158" citStr="Toutanova et al., 2003" startWordPosition="1097" endWordPosition="1100">owing the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1, d2, ..., d|D|}, we define a candidate morph mi as a unique term tj in T, where T = {t1, t2, ..., t|T|} is the set of unique terms in D. To extract T, we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj. Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “4熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. Fo</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsvetkov</author>
</authors>
<title>Cross-lingual metaphor detection using common semantic features.</title>
<date>2013</date>
<booktitle>In Proc. of the ACL2013 Workshop on Metaphor in NLP.</booktitle>
<contexts>
<context position="30060" citStr="Tsvetkov, 2013" startWordPosition="4871" endWordPosition="4872">4 (Wu Sangui) BXL (Bo Xilai) _AWK (Wang Lijun) m (Manchuria) -ta (Wen Qiang) —11 (Bo Yibo) � (Bo) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some comm</context>
</contexts>
<marker>Tsvetkov, 2013</marker>
<rawString>Y. Tsvetkov. 2013. Cross-lingual metaphor detection using common semantic features. In Proc. of the ACL2013 Workshop on Metaphor in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wang</author>
<author>H Wang</author>
<author>H Duan</author>
<author>S Han</author>
<author>S Yu</author>
</authors>
<title>Chinese noun phrase metaphor recognition with maximum entropy approach.</title>
<date>2006</date>
<booktitle>In Proc. of the Seventh International Conference on Intelligent Text Processing and Computational Linguistics (CICLing2006).</booktitle>
<contexts>
<context position="30044" citStr="Wang et al., 2006" startWordPosition="4867" endWordPosition="4870">N_ (King of Han) A=4 (Wu Sangui) BXL (Bo Xilai) _AWK (Wang Lijun) m (Manchuria) -ta (Wen Qiang) —11 (Bo Yibo) � (Bo) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete inform</context>
</contexts>
<marker>Wang, Wang, Duan, Han, Yu, 2006</marker>
<rawString>Z. Wang, H. Wang, H. Duan, S. Han, and S. Yu. 2006. Chinese noun phrase metaphor recognition with maximum entropy approach. In Proc. of the Seventh International Conference on Intelligent Text Processing and Computational Linguistics (CICLing2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yang</author>
<author>N Duan</author>
<author>M Zhou</author>
<author>H Rim</author>
</authors>
<title>Joint relational embeddings for knowledge-based question answering.</title>
<date>2014</date>
<booktitle>In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP2014).</booktitle>
<contexts>
<context position="32148" citStr="Yang et al., 2014" startWordPosition="5213" endWordPosition="5216">s), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M. Galley, and D. Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proc. of the Third Workshop on Statistical Machine Translation (StatMT 2008). 7 Conclusions and Future Work This paper describes the first work of contextaware end-to-end morph decoding. By conducting deep analysis to identity the common characteristics of morphs and the unique challenges of this task, we leverage a large amount of unlabeled data </context>
</contexts>
<marker>Yang, Duan, Zhou, Rim, 2014</marker>
<rawString>M. Yang, N. Duan, M. Zhou, and H. Rim. 2014. Joint relational embeddings for knowledge-based question answering. In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. of the 33rd Annual Meeting on Association for Computational Linguistics (ACL1995).</booktitle>
<contexts>
<context position="30139" citStr="Yarowsky, 1995" startWordPosition="4882" endWordPosition="4883">1 (Bo Yibo) � (Bo) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 2013)) are not effective f</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of the 33rd Annual Meeting on Association for Computational Linguistics (ACL1995).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yih</author>
<author>X He</author>
<author>C Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</booktitle>
<contexts>
<context position="32107" citStr="Yih et al., 2014" startWordPosition="5205" endWordPosition="5208">nd formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cru593 cial to keep the genres consistent between learning embeddings and applying embeddings. P. Chang, M. Galley, and D. Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proc. of the Third Workshop on Statistical Machine Translation (StatMT 2008). 7 Conclusions and Future Work This paper describes the first work of contextaware end-to-end morph decoding. By conducting deep analysis to identity the common characteristics of morphs and the unique challenges of this task, we l</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>W. Yih, X. He, and C. Meek. 2014. Semantic parsing for single-relation question answering. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>H Yu</author>
<author>D Xiong</author>
<author>Q Liu</author>
</authors>
<title>Hhmmbased chinese lexical analyzer ictclas.</title>
<date>2003</date>
<booktitle>In Proc. of the second SIGHAN workshop on Chinese language processing (SIGHAN2003).</booktitle>
<contexts>
<context position="7216" citStr="Zhang et al., 2003" startWordPosition="1106" endWordPosition="1109">al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1, d2, ..., d|D|}, we define a candidate morph mi as a unique term tj in T, where T = {t1, t2, ..., t|T|} is the set of unique terms in D. To extract T, we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj. Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “4熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. For each morph mention, we discover a list of target candida</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>H. Zhang, H. Yu, D. Xiong, and Q. Liu. 2003. Hhmmbased chinese lexical analyzer ictclas. In Proc. of the second SIGHAN workshop on Chinese language processing (SIGHAN2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhang</author>
<author>H Huang</author>
<author>X Pan</author>
<author>H Ji</author>
<author>K Knight</author>
<author>Z Wen</author>
<author>Y Sun</author>
<author>J Han</author>
<author>B Yener</author>
</authors>
<title>Be appropriate and funny: Automatic entity morph encoding.</title>
<date>2014</date>
<booktitle>In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</booktitle>
<contexts>
<context position="1697" citStr="Zhang et al., 2014" startWordPosition="230" endWordPosition="233">d on specific contexts, and resolve them to target entities. Our approach is based on an absolute “cold-start” - it does not require any candidate morph or target entity lists as input, nor any manually constructed morph-target pairs for training. We design a semi-supervised collective inference framework for morph mention extraction, and compare various deep learning based approaches for morph resolution. Our approach achieved significant improvement over the state-of-the-art method (Huang et al., 2013), which used a large amount of training data. 1 1 Introduction Morphs (Huang et al., 2013; Zhang et al., 2014) refer to the fake alternative names created by social media users to entertain readers or evade censors. For example, during the World Cup in 2014, 1The data set and programs are publicly available at: http://nlp.cs.rpi.edu/data/morphdecoding.zip and http://nlp.cs.rpi.edu/software/morphdecoding.tar.gz a morph “Su-tooth” was created to refer to the Uruguay striker “Luis Suarez” for his habit of biting other players. Automatically decoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, eve</context>
<context position="5846" citStr="Zhang et al., 2014" startWordPosition="879" endWordPosition="882">s to capture and compare the deep semantic representations of a morph and its candidate target entities based on their contextual clues. For example, the morph “平西王(Conquer West King)” and its target entity “4熙来 (Bo Xilai)” share similar implicit contextual representations such as “重庆(Chongqing)” (Bo was the governor of Chongqing) and “倒台 (fall from power)”. Challenge 3: Lack of labeled data To the best of our knowledge, no sufficient mention-level morph annotations exist for training an end-to-end decoder. Manual morph annotations require native speakers who have certain cultural background (Zhang et al., 2014). In this paper we focus on exploring novel approaches to save annotation cost in each step. For morph extraction, based on the observation that morphs tend to share similar characteristics and appear together, we propose a semi-supervised collective inference approach to extract morph mentions from multiple tweets simultaneously. Deep learning techniques have been successfully used to model word representation in an unsupervised fashion. For morph resolution, we make use of a large amount of unlabeled data to learn the semantic representations of morphs and target entities based on the unsupe</context>
</contexts>
<marker>Zhang, Huang, Pan, Ji, Knight, Wen, Sun, Han, Yener, 2014</marker>
<rawString>B. Zhang, H. Huang, X. Pan, H. Ji, K. Knight, Z. Wen, Y. Sun, J. Han, and B. Yener. 2014. Be appropriate and funny: Automatic entity morph encoding. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhou</author>
<author>O Bousquet</author>
<author>T Lal</author>
<author>J Weston</author>
<author>B Sch¨olkopf</author>
</authors>
<title>Learning with local and global consistency.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 16,</booktitle>
<pages>321--328</pages>
<marker>Zhou, Bousquet, Lal, Weston, Sch¨olkopf, 2004</marker>
<rawString>D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch¨olkopf. 2004. Learning with local and global consistency. In Advances in Neural Information Processing Systems 16, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML2003).</booktitle>
<contexts>
<context position="13630" citStr="Zhu et al., 2003" startWordPosition="2176" endWordPosition="2179">. For instance, the morph mentions “ 西_. (Conquer West King)” in d1 and d3 in Figure 1 are coreferential, they both refer to the modern politician “薄熙来 (Bo Xilai)”. Hypothesis 2: Those highly correlated mentions tend to either be morph mentions or nonmorph mentions. From our annotated dataset, 49% morph mentions co-occur on tweet level. For example, “ 西 _.(Conquer West King)” and “军 哥(Brother Jun)” are used together in d3 in Figure 1. Based on these hypotheses, we aim to design an effective approach to compensate for the limited annotated data. Graph-based semi-supervised learning approaches (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004) have been successfully applied many NLP tasks (Niu et al., 2005; Chen et al., 2006; Huang et al., 2014). Therefore we build a mention graph to capture the semantic relatedness (weighted arcs) between potential morph mentions (nodes) and propose a semi-supervised graph-based algorithm to collectively verify a set of relevant mentions using a small amount of labeled data. We now describe the detailed algorithm as follows. Mention Graph Construction First, we construct a mention graph that can reflect the association between all the mentions of potenti</context>
<context position="16540" citStr="Zhu et al., 2003" startWordPosition="2671" endWordPosition="2674">sed Semi-supervised Learning Intuitively, if two mentions are strongly connected, they tend to hold the same label. The label of 1 indicates a mention is a morph mention, and 0 means a non-morph mention. We use Y = [Yl Yu]T to denote the label vector of all mentions, where the first l nodes are verified mentions labeled as 1 or 0, and the remaining u nodes need to be verified and initialized with the label 0.5. Our final goal is to obtain the final label vector Yu by incorporating evidence from initial labels and the mention graph. Following the graph-based semi-supervised learning algorithm (Zhu et al., 2003), the mention verification problem is formulated to optimize the objective function 2(Y) = µ Eli=1(yi − y0i )2 + 2 Ei,j Wij(yi − yj)2 where y0i denotes the initial 3α is set to 0.8 in this paper, optimized from the development set. Wi;,Mp ,,Mqi= I 1.0 if mP and m9 are linked with certain social relation cos(m0i , m9) else if q E kNN(p) 0 Otherwise 589 label, and p is a regularization parameter that controls the trade-off between initial labels and the consistency of labels on the mention graph. Zhu et al. (2003) has proven that this formula has both closed-form and iterative solutions. 4 Morph</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semisupervised learning using gaussian fields and harmonic functions. In Proc. of the International Conference on Machine Learning (ICML2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>R Florian</author>
</authors>
<title>Mention detection crossing the language barrier.</title>
<date>2008</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP2008).</booktitle>
<contexts>
<context position="29960" citStr="Zitouni and Florian, 2008" startWordPosition="4851" endWordPosition="4855">4* (Year 1644) NIJ (Entitled as) �� (Qinhuai) Ii (Fall of Qin Dynasty) 4XV9 (Zhang Dejiang) N_ (King of Han) A=4 (Wu Sangui) BXL (Bo Xilai) _AWK (Wang Lijun) m (Manchuria) -ta (Wen Qiang) —11 (Bo Yibo) � (Bo) &apos;S15f (Introduce Investment) ffm (Suppress Gangster) Aid (Violation of Rules) X03F (Be Distinguished) MIi* (Murdering Case) 19:F·*V (Neil Heywood) WK (Huang Qifan) 4XV9 (Zhang Dejiang) _AWK (Wang Lijun) BXL (Bo Xilai) Figure 5: Top 10 closest entities to morph and target in different genres Figure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wide</context>
</contexts>
<marker>Zitouni, Florian, 2008</marker>
<rawString>I. Zitouni and R. Florian. 2008. Mention detection crossing the language barrier. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP2008).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>