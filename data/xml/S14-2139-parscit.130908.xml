<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013981">
<title confidence="0.9975805">
UoW: NLP Techniques Developed at the University of Wolverhampton for
Semantic Similarity and Textual Entailment
</title>
<author confidence="0.994417">
Rohit Gupta, Hanna B´echara, Ismail El Maarouf and Constantin Or˘asan
</author>
<affiliation confidence="0.987123666666667">
Research Group in Computational Linguistics,
Research Institute of Information and Language Processing,
University of Wolverhampton, UK
</affiliation>
<email confidence="0.991836">
{R.Gupta, Hanna.Bechara, I.El-Maarouf, C.Orasan}@wlv.ac.uk
</email>
<sectionHeader confidence="0.993672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975416666667">
This paper presents the system submit-
ted by University of Wolverhampton for
SemEval-2014 task 1. We proposed a ma-
chine learning approach which is based
on features extracted using Typed Depen-
dencies, Paraphrasing, Machine Transla-
tion evaluation metrics, Quality Estima-
tion metrics and Corpus Pattern Analysis.
Our system performed satisfactorily and
obtained 0.711 Pearson correlation for the
semantic relatedness task and 78.52% ac-
curacy for the textual entailment task.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.954742306122449">
The SemEval task 1 (Marelli et al., 2014a) in-
volves two subtasks: predicting the degree of re-
latedness between two sentences and detecting the
entailment relation holding between them. The
task uses SICK dataset (Marelli et al., 2014b),
consisting of 10000 pairs, each annotated with re-
latedness in meaning and entailment relationship
holding between them. Similarity measures be-
tween sentences are required in a wide variety of
NLP applications. In applications like Informa-
tion Retrieval (IR), measuring similarity is a vi-
tal step in order to determine the best result for
a related query. Other applications such as Para-
phrasing and Translation Memory (TM) rely on
similarity measures to weight results. However,
computing semantic similarity between sentences
is a complex and difficult task, due to the fact that
the same meaning can be expressed in a variety of
ways. For this reason it is necessary to have more
than a surface-form comparison.
We present a method based on machine learning
which exploits available NLP technology. Our ap-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
proach relies on features inspired by deep seman-
tics (such as parsing and paraphrasing), machine
translation quality estimation, machine translation
evaluation and Corpus Pattern Analysis (CPA1).
We use the same features to measure both se-
mantic relatedness and textual entailment. Our hy-
pothesis is that each feature covers a particular as-
pect of implicit similarity and entailment informa-
tion contained within the pair of sentences. Train-
ing is performed in a regression framework for se-
mantic relatedness and in a classification frame-
work for textual entailment.
The remainder of the paper is structured as fol-
lows. In Section 2, we review the work related
to our study and the existing NLP technologies
used to measure sentence similarity. In Sections 3
and 4, we describe our approach and the similarity
measures we used. In Section 5, we present the re-
sults and an analysis of our runs based on the test
and training data provided by the SemEval-2014
task. Finally, our work is summed up in Section 6
with perspectives for future work we would like to
explore.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998866642857143">
The areas of semantic relatedness and entailment
have received extensive interest from the research
community in the last decade. Earlier work in
relatedness (Baner ee and Pedersen, 2003; Li et
al., 2006) exploited WordNet in various ways to
extract the semantic relatedness. Baner ee and
Pedersen (2003) presented a measure using ex-
tended gloss overlap. This measure takes two
WordNet synsets as input and uses the overlap
of their WordNet glosses to compute their degree
of semantic relatedness. Li et al. (2006) pre-
sented a semantic similarity metric based on the
semantic similarity of words in a sentence. Re-
cently, Wang and Cer (2012) presented an ap-
</bodyText>
<footnote confidence="0.992479">
1http://pdev.org.uk
</footnote>
<page confidence="0.92735">
785
</page>
<note confidence="0.7359195">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 785–789,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999830391304348">
proach that uses probabilistic edit-distance to mea-
sure semantic similarity. The approach uses prob-
abilistic finite state and pushdown automata to
model weighted edit-distance where state transi-
tions correspond to edit-operations. In some as-
pects, our work is similar to B¨ar et al. (2012),
who presented an approach which combines var-
ious text similarity measures using a log-linear re-
gression model.
Entailment has been modelled using various ap-
proaches. The main approaches are based on
logic inferencing (Moldovan et al., 2003), ma-
chine learning (Hickl et al., 2006; Castillo, 2010)
and tree edit-distance (Kouylekov and Magnini,
2005). Most of the recent approaches employ var-
ious syntactic or tree edit models (Heilman and
Smith, 2010; Mai et al., 2011; Rios and Gelbukh,
2012; Alabbas and Ramsay, 2013). Recently, Al-
abbas and Ramsay (2013) presented a modified
tree edit distance approach, which extends tree
edit distance to the level of subtrees. The ap-
proach extends Zhang-Shasha’s algorithm (Zhang
and Shasha, 1989).
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.99987175">
Our system uses the same 31 features for both sub-
tasks. This section explains them and the code
which implements most of them can be found on
GitHub2.
</bodyText>
<subsectionHeader confidence="0.999335">
3.1 Language Technology Features
</subsectionHeader>
<bodyText confidence="0.9994654">
We used existing language processing tools to ex-
tract features. Stanford CoreNLP3 toolkit provides
lemma, parts of speech (POS), named entities, de-
pendencies relations of words in each sentence.
We calculated Jaccard similarity on surface
form, lemma, dependencies relations, POS and
named entities to get the feature values. The Jac-
card similarity computes sentence similarity by di-
viding the overlap of words on the total number of
words of both sentences.
</bodyText>
<equation confidence="0.9950505">
5im(s1, s2) = |s1 n s2 |(1)
|s1 U s2|
</equation>
<bodyText confidence="0.999177">
where in equation (1), 5im(s1, s2) is the Jaccard
similarity between sets of words s1 and s2.
We used the same toolkit to identify corefer-
ence relations and determine clusters of corefer-
ential entities. The coreference feature value was
</bodyText>
<footnote confidence="0.998023">
2https://github.com/rohitguptacs/wlvsimilarity
3http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<bodyText confidence="0.999922142857143">
calculated using clusters of coreferential entities.
The intuition is that sentences containing corefer-
ential entities should have some semantic related-
ness. In order to extract clusters of coreferential
entities, the pair of sentences was treated as a doc-
ument. The coreference feature value using these
clusters was calculated as follows:
</bodyText>
<equation confidence="0.974331">
CC
V aluecoref = (2)
TC
</equation>
<bodyText confidence="0.999990380952381">
where CC is the number of clusters formed by the
participation of entities (at least one entity from
each sentence of the pair) in both sentences and
TC is the total number of clusters.
We calculated two separate feature values for
dependency relations: the first feature concate-
nated the words involved in a dependency relation
and the second used grammatical relation tags. For
example, for the sentence pair “the kids are play-
ing outdoors” and “the students are playing out-
doors” the Jaccard similarity is calculated based
on concatenated words “kids::the, playing::kids,
playing::are, ROOT::playing, playing::outdoors”
and “students::the, playing::students, playing::are,
ROOT::playing, playing::outdoors” to get the
value for the first feature and “det, nsubj, aux, root,
dobj” and “det, nsubj, aux, root, dobj” to get the
value for the second feature.
These language technology features try to cap-
ture the token based similarity and grammatical
similarity between a pair of sentences.
</bodyText>
<subsectionHeader confidence="0.999877">
3.2 Paraphrasing Features
</subsectionHeader>
<bodyText confidence="0.999995636363636">
We used the PPDB paraphrase database (Ganitke-
vitch et al., 2013) to get the paraphrases. We used
lexical and phrasal paraphrases of “L” size. For
each sentence of the pair, we created two sets of
bags of n-grams (1 &lt; n &lt; length of the sentence).
We extended each set with paraphrases for each n-
gram available from paraphrase database. We then
calculated the Jaccard similarity (see Section 3.1)
between these extended bag of n-grams to get the
feature value. This feature capture the cases where
one sentence is a paraphrase of the other.
</bodyText>
<subsectionHeader confidence="0.998377">
3.3 Negation Feature
</subsectionHeader>
<bodyText confidence="0.999895166666667">
Our system does not attempt to model similar-
ity with negation, but since negation is an impor-
tant feature for contradiction in textual entailment,
we designed a non-similarity feature. The system
checks for the presence of a negation word such as
‘no’, ‘never’ and ‘not’ in the pair of sentences and
</bodyText>
<page confidence="0.991905">
786
</page>
<bodyText confidence="0.8392725">
returns “1” (“0” otherwise) if both or none of the
sentences contain any of these words.
</bodyText>
<subsectionHeader confidence="0.586112">
3.4 Machine Translation Quality Estimation
Features
</subsectionHeader>
<bodyText confidence="0.999741739130435">
Seventeen of the features consist of Machine
Translation Quality Estimation (QE) features,
based on the work of (Specia et al., 2009) and used
as a baseline in recent QE tasks (such as (Callison-
Burch et al., 2012)). We extracted these features
by treating the first set of sentences as the Machine
Translation (MT) “source”, and the second set of
sentences as the MT “target”. In Machine Trans-
lation, these features are used to access the quality
of MT “target”. The QE features include shallow
surface features such as the number of punctua-
tion marks, the average length of words, the num-
ber of words. Furthermore, these features include
n-gram frequencies and language model probabil-
ities. A full list of the QE features is provided in
the documentation of the QE system4 (Specia et
al., 2009).
QE features relate to well-formedness and syn-
tax, and are not usually used to compute seman-
tic relatedness between sentences. We have used
them in the hope that the surface features at least
will show us some structural similarity between
sentences.
</bodyText>
<subsectionHeader confidence="0.721966">
3.5 Machine Translation Evaluation Features
</subsectionHeader>
<bodyText confidence="0.999984636363636">
Additionally, we used BLEU (Papineni et al.,
2002), a very popular machine translation evalu-
ation metric, as a feature. BLEU is based on n-
gram counts. It is meant to capture the similarity
between translated text and references for machine
translation evaluation. The BLEU score over sur-
face, lemma and POS was calculated to get three
feature values. In a pair of sentences, one side was
treated as a translation and another as a reference.
We applied it at the sentence level to capture the
similarity between two sentences.
</bodyText>
<subsectionHeader confidence="0.991259">
3.6 Corpus Pattern Analysis Features
</subsectionHeader>
<bodyText confidence="0.997322">
Corpus Pattern Analysis (CPA) (Hanks, 2013) is
a procedure in corpus linguistics that associates
word meaning with word use by means of seman-
tic patterns. CPA is a new technique for map-
ping meaning onto words in text. It is currently
being used to build a “Pattern Dictionary of En-
glish Verbs”(PDEV5). It is based on the Theory of
</bodyText>
<footnote confidence="0.999707">
4https://github.com/lspecia/quest
5http://pdev.org.uk
</footnote>
<bodyText confidence="0.994949304347826">
Norms and Exploitations (Hanks, 2013).
There are two features extracted from PDEV.
They both make use of a derived resource called
the CPA network (Bradbury and El Maarouf,
2013). The CPA network links verbs according
to similar semantic patterns (e.g. both ‘pour’ and
‘trickle’ share an intransitive use where the subject
is “liquid”).
The first feature value compares the main verbs
in both sentences. When both verbs share a pat-
tern, the system returns a value of “1” (otherwise
“0”). The second feature extends the CPA network
to compute the probability of a PDEV pattern,
given a word. This probability is computed over
the portion of the British National Corpus which is
manually tagged with PDEV patterns. The prob-
ability of a pattern given each word of a sentence
of the dataset is obtained by the product of those
probabilities. The feature value is the (normalised)
number of common patterns from the three most
probable patterns in each sentence. These features
try to capture similarity based on semantic pat-
terns.
</bodyText>
<sectionHeader confidence="0.871848" genericHeader="method">
4 Predicting Through Machine Learning
</sectionHeader>
<subsectionHeader confidence="0.992124">
4.1 Model Description
</subsectionHeader>
<bodyText confidence="0.999941888888889">
We used a support vector machine in order to build
a regression model to predict semantic relatedness
and a classification model to predict textual entail-
ment. For the actual implementation we used Lib-
SVM6 (Chang and Lin, 2011).
We used a regression model for the related-
ness task that estimates a continuous score be-
tween 1 and 5 for each sentence. For the entail-
ment task, we trained a classification model which
assigns one of three different labels (ENTAIL-
MENT, CONTRADICTION, NEUTRAL) to each
sentence pair. We trained both systems on the
4500 sentence training set, augmented with the
500 sentence trial data. The values of C and -y
have been optimised through a grid-search which
uses a 5-fold cross-validation method.
The RBF kernel proved to be the best for both
tasks.
</bodyText>
<sectionHeader confidence="0.997261" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<footnote confidence="0.7340135">
We submitted 4 runs of our system (Run-1 to Run-
4). Run-1 was submitted as primary run. Run-2,
Run-3 and Run-4 systems were identical except
6http://www.csie.ntu.edu.tw/ cjlin/libsvm/
</footnote>
<page confidence="0.987501">
787
</page>
<table confidence="0.99959125">
Run-1 Run-2 Run-3 Run-4
C 8 8 2 2
ry 0.0441 0.0441 0.125 0.125
Pearson 0.7111 0.7166 0.6968 0.6975
</table>
<tableCaption confidence="0.999922">
Table 1: Results: Relatedness.
</tableCaption>
<bodyText confidence="0.999953820512821">
for some parameter differences for SVM train-
ing and the replacement of the values which were
outside the boundaries (1-5). If relatedness val-
ues predicted by the system were less than 1 or
greater than 5, these values were replaced by 1
and 5 respectively for Run-1, Run-2 and Run-4
and 1.5 and 4.5 respectively for Run-3. Our pri-
mary run also used one extra feature for related-
ness, which was obtained by considering entail-
ment judgement as a feature. Our hypothesis was
that entailment judgement may help in measur-
ing relatedness. In the actual test this feature was
not helpful and we obtained Pearson correlation of
0.711 for the primary run, compared to 0.716 for
Run-2. The details of runs are given in Table 1 and
2.
After training both models, we ran a feature
selection algorithm to determine which features
yielded the highest accuracy, and therefore had the
highest impact on our system. Perhaps unsurpris-
ingly, the QE features were not very useful in pre-
dicting semantic similarity or entailment. How-
ever, despite their focus on fluency rather than se-
mantic correctness, the QE features still managed
to contribute to some improvements in the textual
entailment task (increasing accuracy by 1%), and
the semantic relatedness task (0.027 increase in
Pearson correlation).
In the entailment (classification) task, the
strongest feature proved to be the negation fea-
ture with 70% accuracy (on the training set) when
training on this feature only. This suggests that
some measure of negation is crucial in determin-
ing whether a sentence contradicts or entails an-
other sentence. Other strong features were lemma,
paraphrasing and dependencies.
In the relatedness (regression) task, the lemma,
surface, paraphrasing, dependencies, PDEV fea-
tures were the strongest contributors to accuracy.
</bodyText>
<table confidence="0.9992305">
Run-1 Run-2 Run-3 Run-4
C 16 16 8 8
ry 0.0625 0.0625 0.5 0.5
Accuracy 78.526 78.526 78.343 78.343
</table>
<tableCaption confidence="0.997779">
Table 2: Results: Entailment.
</tableCaption>
<sectionHeader confidence="0.930363" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99992535">
We have presented an efficient approach to calcu-
late semantic relatedness and textual entailment.
One noticeable point of our approach is that we
have used the same features for both tasks and
our system performed well in each of these tasks.
Therefore, our system captures reasonably good
models to compute semantic relatedness and tex-
tual entailment.
In the future we would like to explore more fea-
tures and particularly those based on tree edit dis-
tance, WordNet and PDEV. Our intuition suggests
that tree edit distance seems to be more helpful for
entailment, whereas WordNet and PDEV seem to
be more helpful for similarity measurement. Ad-
ditionally, we would like to combine our tech-
niques for measuring relatedness and entailment
with MT evaluation techniques. We would fur-
ther like to apply these techniques cross-lingually,
moving into other areas like machine translation
evaluation and quality estimation.
</bodyText>
<sectionHeader confidence="0.965663" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999859428571429">
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement no. 317471 and partly supported
by an AHRC grant “Disambiguating Verbs by Col-
location project, AH/J005940/1, 2012-2015”.
</bodyText>
<sectionHeader confidence="0.992722" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.76604">
Maytham Alabbas and Allan Ramsay. 2013. Natural
language inference for Arabic using extended tree
edit distance with subtrees. Journal of Artificial In-
telligence Research, 48:1–22.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In IJCAI, volume 3, pages 805–810.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In First Joint Conference on
</reference>
<page confidence="0.991153">
788
</page>
<reference confidence="0.995342730337079">
Lexical and Computational Semantics, Association
for Computational Linguistics, pages 435–440.
Jane Bradbury and Isma¨ıl El Maarouf. 2013. An
empirical classification of verbs based on Semantic
Types: the case of the ’poison’ verbs. In Proceed-
ings of the Joint Symposium on Semantic Process-
ing. Textual Inference and Structures in Corpora,
pages 70–74.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia, editors.
2012. Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation. Association for Com-
putational Linguistics, Montr´eal, Canada, June.
Julio J. Castillo. 2010. Recognizing textual en-
tailment: experiments with machine learning al-
gorithms and RTE corpora. Special issue: Natu-
ral Language Processings and its Applications, Re-
search in Computing Science, 46:155–164.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
Juri Ganitkevitch, Van Durme Benjamin, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758–764, Atlanta, Georgia.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In The 2010 An-
nual Conference of the North American Chapter of
the ACL, number June, pages 1011–1019.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Rec-
ognizing textual entailment with LCC’s GROUND-
HOG system. In Proceedings of the Second PAS-
CAL Challenges Workshop.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In Proceedings of the First Challenge
Workshop Recognising Textual Entailment, pages
17–20.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O’shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138–1150.
Zhewei Mai, Y Zhang, and Donghong Ji. 2011. Rec-
ognizing text entailment via syntactic tree match-
ing. In Proceedings of NTCIR-9 Workshop Meeting,
pages 361–364, Tokyo, Japan.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014).
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. COGEX : A Logic Prover
for Question Answering. In Proceedings of HLT-
NAACL, number June, pages 87–93.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311–318.
Miguel Rios and Alexander Gelbukh. 2012. Recog-
nizing Textual Entailment with a Semantic Edit Dis-
tance Metric. In 11th Mexican International Confer-
ence on Artificial Intelligence, pages 15–20. IEEE.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In 13th Conference of the European Asso-
ciation for Machine Translation, pages 28–37.
Mengqiu Wang and Daniel Cer. 2012. Stanford: prob-
abilistic edit distance metrics for STS. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 648–654.
Kaizhong Zhang and Dennis Shasha. 1989. Simple
Fast Algorithms for the Editing Distance between
Trees and Related Problems. SIAM Journal on Com-
puting, 18(6):1245–1262.
</reference>
<page confidence="0.998611">
789
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551567">
<title confidence="0.979603">UoW: NLP Techniques Developed at the University of Wolverhampton for Semantic Similarity and Textual Entailment</title>
<author confidence="0.988969">Hanna B´echara Gupta</author>
<author confidence="0.988969">Ismail El_Maarouf</author>
<affiliation confidence="0.987907666666667">Research Group in Computational Research Institute of Information and Language University of Wolverhampton,</affiliation>
<address confidence="0.633332">Hanna.Bechara, I.El-Maarouf,</address>
<abstract confidence="0.995160769230769">This paper presents the system submitted by University of Wolverhampton for SemEval-2014 task 1. We proposed a machine learning approach which is based on features extracted using Typed Dependencies, Paraphrasing, Machine Translation evaluation metrics, Quality Estimation metrics and Corpus Pattern Analysis. Our system performed satisfactorily and obtained 0.711 Pearson correlation for the semantic relatedness task and 78.52% accuracy for the textual entailment task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Maytham Alabbas</author>
<author>Allan Ramsay</author>
</authors>
<title>Natural language inference for Arabic using extended tree edit distance with subtrees.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>48--1</pages>
<contexts>
<context position="4837" citStr="Alabbas and Ramsay, 2013" startWordPosition="740" endWordPosition="743">ate transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can be found on GitHub2. 3.1 Language Technology Features We used existing language processing tools to extract features. Stanford CoreNLP3 toolkit provides lemma, parts of speech (POS), named entities, dependencies relations of words in each s</context>
</contexts>
<marker>Alabbas, Ramsay, 2013</marker>
<rawString>Maytham Alabbas and Allan Ramsay. 2013. Natural language inference for Arabic using extended tree edit distance with subtrees. Journal of Artificial Intelligence Research, 48:1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In IJCAI,</booktitle>
<volume>3</volume>
<pages>805--810</pages>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In IJCAI, volume 3, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics, Association for Computational Linguistics,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In First Joint Conference on Lexical and Computational Semantics, Association for Computational Linguistics, pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Bradbury</author>
<author>Isma¨ıl El Maarouf</author>
</authors>
<title>An empirical classification of verbs based on Semantic Types: the case of the ’poison’ verbs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,</booktitle>
<pages>70--74</pages>
<marker>Bradbury, El Maarouf, 2013</marker>
<rawString>Jane Bradbury and Isma¨ıl El Maarouf. 2013. An empirical classification of verbs based on Semantic Types: the case of the ’poison’ verbs. In Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora, pages 70–74.</rawString>
</citation>
<citation valid="true">
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation. Association for Computational Linguistics,</booktitle>
<editor>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia, editors.</editor>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="3851" citStr="(2012)" startWordPosition="594" endWordPosition="594">and entailment have received extensive interest from the research community in the last decade. Earlier work in relatedness (Baner ee and Pedersen, 2003; Li et al., 2006) exploited WordNet in various ways to extract the semantic relatedness. Baner ee and Pedersen (2003) presented a measure using extended gloss overlap. This measure takes two WordNet synsets as input and uses the overlap of their WordNet glosses to compute their degree of semantic relatedness. Li et al. (2006) presented a semantic similarity metric based on the semantic similarity of words in a sentence. Recently, Wang and Cer (2012) presented an ap1http://pdev.org.uk 785 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 785–789, Dublin, Ireland, August 23-24, 2014. proach that uses probabilistic edit-distance to measure semantic similarity. The approach uses probabilistic finite state and pushdown automata to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been m</context>
</contexts>
<marker>2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia, editors. 2012. Proceedings of the Seventh Workshop on Statistical Machine Translation. Association for Computational Linguistics, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio J Castillo</author>
</authors>
<title>Recognizing textual entailment: experiments with machine learning algorithms and RTE corpora.</title>
<date>2010</date>
<booktitle>Special issue: Natural Language Processings and its Applications, Research in Computing Science,</booktitle>
<pages>46--155</pages>
<contexts>
<context position="4614" citStr="Castillo, 2010" startWordPosition="706" endWordPosition="707">Ireland, August 23-24, 2014. proach that uses probabilistic edit-distance to measure semantic similarity. The approach uses probabilistic finite state and pushdown automata to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can be found on GitHu</context>
</contexts>
<marker>Castillo, 2010</marker>
<rawString>Julio J. Castillo. 2010. Recognizing textual entailment: experiments with machine learning algorithms and RTE corpora. Special issue: Natural Language Processings and its Applications, Research in Computing Science, 46:155–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="11820" citStr="Chang and Lin, 2011" startWordPosition="1860" endWordPosition="1863">ith PDEV patterns. The probability of a pattern given each word of a sentence of the dataset is obtained by the product of those probabilities. The feature value is the (normalised) number of common patterns from the three most probable patterns in each sentence. These features try to capture similarity based on semantic patterns. 4 Predicting Through Machine Learning 4.1 Model Description We used a support vector machine in order to build a regression model to predict semantic relatedness and a classification model to predict textual entailment. For the actual implementation we used LibSVM6 (Chang and Lin, 2011). We used a regression model for the relatedness task that estimates a continuous score between 1 and 5 for each sentence. For the entailment task, we trained a classification model which assigns one of three different labels (ENTAILMENT, CONTRADICTION, NEUTRAL) to each sentence pair. We trained both systems on the 4500 sentence training set, augmented with the 500 sentence trial data. The values of C and -y have been optimised through a grid-search which uses a 5-fold cross-validation method. The RBF kernel proved to be the best for both tasks. 5 Results and Analysis We submitted 4 runs of ou</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Van Durme Benjamin</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Ppdb: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>758--764</pages>
<location>Atlanta,</location>
<contexts>
<context position="7526" citStr="Ganitkevitch et al., 2013" startWordPosition="1147" endWordPosition="1151">dents are playing outdoors” the Jaccard similarity is calculated based on concatenated words “kids::the, playing::kids, playing::are, ROOT::playing, playing::outdoors” and “students::the, playing::students, playing::are, ROOT::playing, playing::outdoors” to get the value for the first feature and “det, nsubj, aux, root, dobj” and “det, nsubj, aux, root, dobj” to get the value for the second feature. These language technology features try to capture the token based similarity and grammatical similarity between a pair of sentences. 3.2 Paraphrasing Features We used the PPDB paraphrase database (Ganitkevitch et al., 2013) to get the paraphrases. We used lexical and phrasal paraphrases of “L” size. For each sentence of the pair, we created two sets of bags of n-grams (1 &lt; n &lt; length of the sentence). We extended each set with paraphrases for each ngram available from paraphrase database. We then calculated the Jaccard similarity (see Section 3.1) between these extended bag of n-grams to get the feature value. This feature capture the cases where one sentence is a paraphrase of the other. 3.3 Negation Feature Our system does not attempt to model similarity with negation, but since negation is an important featur</context>
</contexts>
<marker>Ganitkevitch, Benjamin, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Van Durme Benjamin, and Chris Callison-Burch. 2013. Ppdb: The paraphrase database. In Proceedings of NAACL-HLT, pages 758–764, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Lexical Analysis: Norms and Exploitations.</title>
<date>2013</date>
<publisher>Mit Press.</publisher>
<contexts>
<context position="10164" citStr="Hanks, 2013" startWordPosition="1591" endWordPosition="1592">tures Additionally, we used BLEU (Papineni et al., 2002), a very popular machine translation evaluation metric, as a feature. BLEU is based on ngram counts. It is meant to capture the similarity between translated text and references for machine translation evaluation. The BLEU score over surface, lemma and POS was calculated to get three feature values. In a pair of sentences, one side was treated as a translation and another as a reference. We applied it at the sentence level to capture the similarity between two sentences. 3.6 Corpus Pattern Analysis Features Corpus Pattern Analysis (CPA) (Hanks, 2013) is a procedure in corpus linguistics that associates word meaning with word use by means of semantic patterns. CPA is a new technique for mapping meaning onto words in text. It is currently being used to build a “Pattern Dictionary of English Verbs”(PDEV5). It is based on the Theory of 4https://github.com/lspecia/quest 5http://pdev.org.uk Norms and Exploitations (Hanks, 2013). There are two features extracted from PDEV. They both make use of a derived resource called the CPA network (Bradbury and El Maarouf, 2013). The CPA network links verbs according to similar semantic patterns (e.g. both </context>
</contexts>
<marker>Hanks, 2013</marker>
<rawString>Patrick Hanks. 2013. Lexical Analysis: Norms and Exploitations. Mit Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In The 2010 Annual Conference of the North American Chapter of the ACL, number June,</booktitle>
<pages>1011--1019</pages>
<contexts>
<context position="4768" citStr="Heilman and Smith, 2010" startWordPosition="728" endWordPosition="731">tate and pushdown automata to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can be found on GitHub2. 3.1 Language Technology Features We used existing language processing tools to extract features. Stanford CoreNLP3 toolkit provides lemma, parts of sp</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In The 2010 Annual Conference of the North American Chapter of the ACL, number June, pages 1011–1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Jeremy Bensley</author>
<author>John Williams</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCC’s GROUNDHOG system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="4597" citStr="Hickl et al., 2006" startWordPosition="702" endWordPosition="705">es 785–789, Dublin, Ireland, August 23-24, 2014. proach that uses probabilistic edit-distance to measure semantic similarity. The approach uses probabilistic finite state and pushdown automata to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can </context>
</contexts>
<marker>Hickl, Bensley, Williams, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, Jeremy Bensley, John Williams, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with LCC’s GROUNDHOG system. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
</authors>
<title>Recognizing textual entailment with tree edit distance algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the First Challenge Workshop Recognising Textual Entailment,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="4667" citStr="Kouylekov and Magnini, 2005" startWordPosition="711" endWordPosition="714"> uses probabilistic edit-distance to measure semantic similarity. The approach uses probabilistic finite state and pushdown automata to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can be found on GitHub2. 3.1 Language Technology Features We used existing</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>Milen Kouylekov and Bernardo Magnini. 2005. Recognizing textual entailment with tree edit distance algorithms. In Proceedings of the First Challenge Workshop Recognising Textual Entailment, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics. Knowledge and Data Engineering,</title>
<date>2006</date>
<journal>IEEE Transactions on,</journal>
<volume>18</volume>
<issue>8</issue>
<marker>Li, McLean, Bandar, O’shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A Bandar, James D O’shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. Knowledge and Data Engineering, IEEE Transactions on, 18(8):1138–1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhewei Mai</author>
<author>Y Zhang</author>
<author>Donghong Ji</author>
</authors>
<title>Recognizing text entailment via syntactic tree matching.</title>
<date>2011</date>
<booktitle>In Proceedings of NTCIR-9 Workshop Meeting,</booktitle>
<pages>361--364</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="4786" citStr="Mai et al., 2011" startWordPosition="732" endWordPosition="735">a to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can be found on GitHub2. 3.1 Language Technology Features We used existing language processing tools to extract features. Stanford CoreNLP3 toolkit provides lemma, parts of speech (POS), named </context>
</contexts>
<marker>Mai, Zhang, Ji, 2011</marker>
<rawString>Zhewei Mai, Y Zhang, and Donghong Ji. 2011. Recognizing text entailment via syntactic tree matching. In Proceedings of NTCIR-9 Workshop Meeting, pages 361–364, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<contexts>
<context position="913" citStr="Marelli et al., 2014" startWordPosition="121" endWordPosition="124">ity of Wolverhampton, UK {R.Gupta, Hanna.Bechara, I.El-Maarouf, C.Orasan}@wlv.ac.uk Abstract This paper presents the system submitted by University of Wolverhampton for SemEval-2014 task 1. We proposed a machine learning approach which is based on features extracted using Typed Dependencies, Paraphrasing, Machine Translation evaluation metrics, Quality Estimation metrics and Corpus Pattern Analysis. Our system performed satisfactorily and obtained 0.711 Pearson correlation for the semantic relatedness task and 78.52% accuracy for the textual entailment task. 1 Introduction The SemEval task 1 (Marelli et al., 2014a) involves two subtasks: predicting the degree of relatedness between two sentences and detecting the entailment relation holding between them. The task uses SICK dataset (Marelli et al., 2014b), consisting of 10000 pairs, each annotated with relatedness in meaning and entailment relationship holding between them. Similarity measures between sentences are required in a wide variety of NLP applications. In applications like Information Retrieval (IR), measuring similarity is a vital step in order to determine the best result for a related query. Other applications such as Paraphrasing and Tran</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014a. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A sick cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="913" citStr="Marelli et al., 2014" startWordPosition="121" endWordPosition="124">ity of Wolverhampton, UK {R.Gupta, Hanna.Bechara, I.El-Maarouf, C.Orasan}@wlv.ac.uk Abstract This paper presents the system submitted by University of Wolverhampton for SemEval-2014 task 1. We proposed a machine learning approach which is based on features extracted using Typed Dependencies, Paraphrasing, Machine Translation evaluation metrics, Quality Estimation metrics and Corpus Pattern Analysis. Our system performed satisfactorily and obtained 0.711 Pearson correlation for the semantic relatedness task and 78.52% accuracy for the textual entailment task. 1 Introduction The SemEval task 1 (Marelli et al., 2014a) involves two subtasks: predicting the degree of relatedness between two sentences and detecting the entailment relation holding between them. The task uses SICK dataset (Marelli et al., 2014b), consisting of 10000 pairs, each annotated with relatedness in meaning and entailment relationship holding between them. Similarity measures between sentences are required in a wide variety of NLP applications. In applications like Information Retrieval (IR), measuring similarity is a vital step in order to determine the best result for a related query. Other applications such as Paraphrasing and Tran</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014b. A sick cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Christine Clark</author>
<author>Sanda Harabagiu</author>
<author>Steve Maiorano</author>
</authors>
<title>COGEX : A Logic Prover for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL, number June,</booktitle>
<pages>87--93</pages>
<contexts>
<context position="4559" citStr="Moldovan et al., 2003" startWordPosition="695" endWordPosition="698">on Semantic Evaluation (SemEval 2014), pages 785–789, Dublin, Ireland, August 23-24, 2014. proach that uses probabilistic edit-distance to measure semantic similarity. The approach uses probabilistic finite state and pushdown automata to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the c</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>Dan Moldovan, Christine Clark, Sanda Harabagiu, and Steve Maiorano. 2003. COGEX : A Logic Prover for Question Answering. In Proceedings of HLTNAACL, number June, pages 87–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="9608" citStr="Papineni et al., 2002" startWordPosition="1497" endWordPosition="1500"> the number of punctuation marks, the average length of words, the number of words. Furthermore, these features include n-gram frequencies and language model probabilities. A full list of the QE features is provided in the documentation of the QE system4 (Specia et al., 2009). QE features relate to well-formedness and syntax, and are not usually used to compute semantic relatedness between sentences. We have used them in the hope that the surface features at least will show us some structural similarity between sentences. 3.5 Machine Translation Evaluation Features Additionally, we used BLEU (Papineni et al., 2002), a very popular machine translation evaluation metric, as a feature. BLEU is based on ngram counts. It is meant to capture the similarity between translated text and references for machine translation evaluation. The BLEU score over surface, lemma and POS was calculated to get three feature values. In a pair of sentences, one side was treated as a translation and another as a reference. We applied it at the sentence level to capture the similarity between two sentences. 3.6 Corpus Pattern Analysis Features Corpus Pattern Analysis (CPA) (Hanks, 2013) is a procedure in corpus linguistics that a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Recognizing Textual Entailment with a Semantic Edit Distance Metric.</title>
<date>2012</date>
<booktitle>In 11th Mexican International Conference on Artificial Intelligence,</booktitle>
<pages>15--20</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4810" citStr="Rios and Gelbukh, 2012" startWordPosition="736" endWordPosition="739">d edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can be found on GitHub2. 3.1 Language Technology Features We used existing language processing tools to extract features. Stanford CoreNLP3 toolkit provides lemma, parts of speech (POS), named entities, dependencies r</context>
</contexts>
<marker>Rios, Gelbukh, 2012</marker>
<rawString>Miguel Rios and Alexander Gelbukh. 2012. Recognizing Textual Entailment with a Semantic Edit Distance Metric. In 11th Mexican International Conference on Artificial Intelligence, pages 15–20. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Marco Turchi</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In 13th Conference of the European Association for Machine Translation,</booktitle>
<pages>28--37</pages>
<contexts>
<context position="8600" citStr="Specia et al., 2009" startWordPosition="1329" endWordPosition="1332">phrase of the other. 3.3 Negation Feature Our system does not attempt to model similarity with negation, but since negation is an important feature for contradiction in textual entailment, we designed a non-similarity feature. The system checks for the presence of a negation word such as ‘no’, ‘never’ and ‘not’ in the pair of sentences and 786 returns “1” (“0” otherwise) if both or none of the sentences contain any of these words. 3.4 Machine Translation Quality Estimation Features Seventeen of the features consist of Machine Translation Quality Estimation (QE) features, based on the work of (Specia et al., 2009) and used as a baseline in recent QE tasks (such as (CallisonBurch et al., 2012)). We extracted these features by treating the first set of sentences as the Machine Translation (MT) “source”, and the second set of sentences as the MT “target”. In Machine Translation, these features are used to access the quality of MT “target”. The QE features include shallow surface features such as the number of punctuation marks, the average length of words, the number of words. Furthermore, these features include n-gram frequencies and language model probabilities. A full list of the QE features is provide</context>
</contexts>
<marker>Specia, Turchi, Cancedda, Dymetman, Cristianini, 2009</marker>
<rawString>Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In 13th Conference of the European Association for Machine Translation, pages 28–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Daniel Cer</author>
</authors>
<title>Stanford: probabilistic edit distance metrics for STS.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>648--654</pages>
<contexts>
<context position="3851" citStr="Wang and Cer (2012)" startWordPosition="591" endWordPosition="594"> relatedness and entailment have received extensive interest from the research community in the last decade. Earlier work in relatedness (Baner ee and Pedersen, 2003; Li et al., 2006) exploited WordNet in various ways to extract the semantic relatedness. Baner ee and Pedersen (2003) presented a measure using extended gloss overlap. This measure takes two WordNet synsets as input and uses the overlap of their WordNet glosses to compute their degree of semantic relatedness. Li et al. (2006) presented a semantic similarity metric based on the semantic similarity of words in a sentence. Recently, Wang and Cer (2012) presented an ap1http://pdev.org.uk 785 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 785–789, Dublin, Ireland, August 23-24, 2014. proach that uses probabilistic edit-distance to measure semantic similarity. The approach uses probabilistic finite state and pushdown automata to model weighted edit-distance where state transitions correspond to edit-operations. In some aspects, our work is similar to B¨ar et al. (2012), who presented an approach which combines various text similarity measures using a log-linear regression model. Entailment has been m</context>
</contexts>
<marker>Wang, Cer, 2012</marker>
<rawString>Mengqiu Wang and Daniel Cer. 2012. Stanford: probabilistic edit distance metrics for STS. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 648–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaizhong Zhang</author>
<author>Dennis Shasha</author>
</authors>
<title>Simple Fast Algorithms for the Editing Distance between Trees and Related Problems.</title>
<date>1989</date>
<journal>SIAM Journal on Computing,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="5054" citStr="Zhang and Shasha, 1989" startWordPosition="773" endWordPosition="776">ntailment has been modelled using various approaches. The main approaches are based on logic inferencing (Moldovan et al., 2003), machine learning (Hickl et al., 2006; Castillo, 2010) and tree edit-distance (Kouylekov and Magnini, 2005). Most of the recent approaches employ various syntactic or tree edit models (Heilman and Smith, 2010; Mai et al., 2011; Rios and Gelbukh, 2012; Alabbas and Ramsay, 2013). Recently, Alabbas and Ramsay (2013) presented a modified tree edit distance approach, which extends tree edit distance to the level of subtrees. The approach extends Zhang-Shasha’s algorithm (Zhang and Shasha, 1989). 3 Features Our system uses the same 31 features for both subtasks. This section explains them and the code which implements most of them can be found on GitHub2. 3.1 Language Technology Features We used existing language processing tools to extract features. Stanford CoreNLP3 toolkit provides lemma, parts of speech (POS), named entities, dependencies relations of words in each sentence. We calculated Jaccard similarity on surface form, lemma, dependencies relations, POS and named entities to get the feature values. The Jaccard similarity computes sentence similarity by dividing the overlap o</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>Kaizhong Zhang and Dennis Shasha. 1989. Simple Fast Algorithms for the Editing Distance between Trees and Related Problems. SIAM Journal on Computing, 18(6):1245–1262.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>