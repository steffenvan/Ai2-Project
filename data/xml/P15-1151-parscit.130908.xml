<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998934">
A Convolutional Architecture for Word Sequence Prediction
</title>
<author confidence="0.996257">
Mingxuan Wang1 Zhengdong Lu2 Hang Li2 Wenbin Jiang1 Qun Liu3,1
</author>
<affiliation confidence="0.9964785">
1Key Laboratory of Intelligent Information Processing,
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<email confidence="0.909008">
{wangmingxuan,jiangwenbin,liuqun}@ict.ac.cn
</email>
<address confidence="0.47511">
2Noah’s Ark Lab, Huawei Technologies
</address>
<email confidence="0.901162">
{Lu.Zhengdong,HangLi.HL}@huawei.com
</email>
<affiliation confidence="0.662635">
3ADAPT Centre, School of Computing, Dublin City University
</affiliation>
<sectionHeader confidence="0.986791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988535714286">
We propose a convolutional neural net-
work, named genCNN, for word se-
quence prediction. Different from
previous work on neural network-
based language modeling and genera-
tion (e.g., RNN or LSTM), we choose
not to greedily summarize the history
of words as a fixed length vector. In-
stead, we use a convolutional neural
network to predict the next word with
the history of words of variable length.
Also different from the existing feed-
forward networks for language mod-
eling, our model can effectively fuse
the local correlation and global cor-
relation in the word sequence, with
a convolution-gating strategy specifi-
cally designed for the task. We argue
that our model can give adequate rep-
resentation of the history, and there-
fore can naturally exploit both the short
and long range dependencies. Our
model is fast, easy to train, and read-
ily parallelized. Our extensive exper-
iments on text generation and n-best
re-ranking in machine translation show
that genCNN outperforms the state-of-
the-arts with big margins.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995605195652174">
Both language modeling (Wu and Khudanpur,
2003; Mikolov et al., 2010; Bengio et al.,
2003) and text generation (Axelrod et al., 2011)
boil down to modeling the conditional proba-
bility of a word given the proceeding words.
Previously, it is mostly done through purely
memory-based approaches, such as n-grams,
which cannot deal with long sequences and has
to use some heuristics (called smoothing) for
rare ones. Another family of methods are based
on distributed representations of words, which
is usually tied with a neural-network (NN) ar-
chitecture for estimating the conditional prob-
abilities of words.
Two categories of neural networks have been
used for language modeling: 1) recurrent neu-
ral networks (RNN), and 2) feedfoward net-
work (FFN):
• The RNN-based models, including its
variants like LSTM, enjoy more popu-
larity, mainly due to their flexible struc-
tures for processing word sequences of ar-
bitrary lengths, and their recent empiri-
cal success(Sutskever et al., 2014; Graves,
2013). We however argue that RNNs,
with their power built on the recursive use
of a relatively simple computation units,
are forced to make greedy summarization
of the history and consequently not effi-
cient on modeling word sequences, which
clearly have a bottom-up structures.
• The FFN-based models, on the other
hand, avoid this difficulty by feeding di-
rectly on the history. However, the FFNs
are built on fully-connected networks,
rendering them inefficient on capturing
local structures of languages. Moreover
their “rigid” architectures make it futile to
handle the great variety of patterns in long
range correlations of words.
We propose a novel convolutional architec-
ture, named genCNN, as a model that can ef-
ficiently combine local and long range struc-
tures of language for the purpose of modeling
conditional probabilities. genCNN can be di-
rectly used in generating a word sequence (i.e.,
</bodyText>
<page confidence="0.94571">
1567
</page>
<note confidence="0.984636333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1567–1576,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.869238333333333">
prediction: “sandwich”?
... βCNN βCNN αCNN
history: / / I was starving after this long meeting, so I rushed to wal-mart to buy a
</figure>
<figureCaption confidence="0.9573925">
Figure 1: The overall diagram of a genCNN. Here “/” stands for a zero padding. In this example,
each CNN component covers 6 words, while in practice the coverage is 30-40 words.
</figureCaption>
<bodyText confidence="0.929222545454546">
text generation) or evaluating the likelihood of
word sequences (i.e., language modeling). We
also show the empirical superiority of genCNN
on both tasks over traditional n-grams and its
RNN or FFN counterparts.
Notations: We will use V to denote the vo-
cabulary, et (E 11, • • • , |V|}) to denote the tth
def
word in a sequence e1:t = [e1, , et], and
et if the sequence is further indexed by n.
(n)
</bodyText>
<sectionHeader confidence="0.996577" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.999607666666667">
As shown in Figure 1, genCNN is overall re-
cursive, consisting of CNN-based processing
units of two types:
</bodyText>
<listItem confidence="0.9371625">
• αCNN as the “front-end”, dealing with
the history that is closest to the prediction;
• QCNNs (which can repeat), in charge of
more “ancient” history.
</listItem>
<bodyText confidence="0.979363666666667">
Together, genCNN takes history e1:t of arbi-
trary length to predict the next word et+1 with
probability
</bodyText>
<equation confidence="0.995675">
p(et+1 |e1:t; O), (1)
</equation>
<bodyText confidence="0.9968455">
based on a representation O(e1:t; O) produced
by the CNN, and a |V|-class soft-max:
</bodyText>
<equation confidence="0.996985">
p(et+1|e1:t; ¯O) OC eµ�et+1φ(e1:t)+bet+1 .(2)
</equation>
<bodyText confidence="0.957927928571428">
genCNN is devised (tailored) fully for mod-
eling the sequential structure in natural lan-
guage, notably different from conventional
CNN (Lawrence et al., 1997; Hu et al., 2014)
in 1) its specifically designed weights-sharing
strategy (in αCNN), 2) its gating design, and
3) certainly its recursive architectures. Also
distinct from RNN, genCNN gains most of
its processing power from the heavy-duty pro-
cessing units (i.e.,αCNN and QCNNs), which
follow a bottom-up information flow and yet
can adequately capture the temporal structure
in word sequence with its convolutional-gating
architecture.
</bodyText>
<sectionHeader confidence="0.986053" genericHeader="method">
3 genCNN: Architecture
</sectionHeader>
<bodyText confidence="0.999936125">
We start with discussing the convolutional ar-
chitecture of αCNN as a stand-alone sentence
model, and then proceed to the recursive struc-
ture. After that we give a comparative analysis
on the mechanism of genCNN.
αCNN, just like a normal CNN, has fixed
architecture with predefined maximum words
(denoted as Lα). History shorter than Lα will
filled with zero paddings, and history longer
than that will be folded to feed to QCNN after
it, as will be elaborated in Section 3.3. Similar
to most other CNNs, αCNN alternates between
convolution layers and pooling layers, and fi-
nally a fully connected layer to reach the repre-
sentation before soft-max, as illustrated by Fig-
ure 2. Unlike the toyish example in Figure 2, in
practice we use a larger and deeper αCNN with
Lα = 30 or 40, and two or three convolution
layers (see Section 4.1). Different from con-
ventional CNN, genCNN has 1) weight shar-
ing strategy for convolution, and 2)“external”
gating networks to replace the normal pooling
mechanism, both of which are specifically de-
signed for word sequence prediction.
</bodyText>
<subsectionHeader confidence="0.98642">
3.1 αCNN: Convolution
</subsectionHeader>
<bodyText confidence="0.9997586">
Different from conventional CNN, the weights
of convolution units in αCNN is only partially
shared. More specifically, in the convolution
units there are two types feature-maps: TIME-
FLOW and the TIME-ARROW, illustrated re-
</bodyText>
<page confidence="0.984622">
1568
</page>
<listItem confidence="0.9251772">
probability of next word
• σ(&apos;) is the activation function, e.g., Sig-
moid or Relu (Dahl et al., 2013)
• wTF denotes the location-independent
(`,f)
parameters for f ETIME-FLOW on Layer-
`, while w(`,f,i)
TA stands for that for f E
TIME-ARROW and location i on Layer-`;
• ˆz(`−1)
</listItem>
<bodyText confidence="0.962698">
i denotes the segment of Layer-`−1
for the convolution at location i , while
</bodyText>
<figure confidence="0.941937333333333">
“dinner”
“breakfast”
“us”
“the”
...
ˆz(0) def = &gt; T T
[xi , xT
&apos; &apos; &apos; , xT
what did you have for
A 3-layer αCNN
1&gt;
Time-Flow Time-Arrow Gating
</figure>
<figureCaption confidence="0.999793">
Figure 2: Illustration of a 3-layer αCNN.
</figureCaption>
<bodyText confidence="0.9525234">
Here the shadowed nodes stand for the TIME-
ARROW feature-maps and the unfilled nodes
for the TIME-FLOW.
spectively with the unfilled nodes and filled
nodes in Figure 2. The parameters for TIME-
FLOW are shared among different convolution
units, while for TIME-ARROW the parame-
ters are location-dependent. Intuitively, TIME-
FLOW acts more like a conventional CNN (e.g.,
that in (Hu et al., 2014)), aiming to understand
the overall temporal structure in the word se-
quences; TIME-ARROW, on the other hand,
works more like a traditional NN-based lan-
guage model (Vaswani et al., 2013; Bengio et
al., 2003): with its location-dependent param-
eters, it focuses on capturing the direction of
time and prediction task.
For sentence input x = {x1, &apos; &apos; &apos; , xT}, the
feature-map of type-f on Layer-` is
if f E TIME-FLOW:
</bodyText>
<equation confidence="0.983731333333333">
zi (x) = σ(w(`,f)
(`,f) TF ˆz(`−1)
i + b(`,f)
TF ), (3)
if f E TIME-ARROW:
zi (x) = σ(w(`,f,i) i + b(`,f,i)
</equation>
<bodyText confidence="0.811968">
(`,f) TA ˆz(`−1) TA ), (4)
where
</bodyText>
<listItem confidence="0.978783">
• zi (x) gives the output of feature-map
(`,f)
of type-f for location i in Layer-`;
</listItem>
<bodyText confidence="0.9750805">
concatenates the vectors for k1 words
from sentence input x.
</bodyText>
<subsectionHeader confidence="0.998517">
3.2 Gating Network
</subsectionHeader>
<bodyText confidence="0.999964375">
Previous CNNs, including those for NLP
tasks (Hu et al., 2014; Kalchbrenner et al.,
2014), take a straightforward convolution-
pooling strategy, in which the “fusion” deci-
sions (e.g., selecting the largest one in max-
pooling) are based on the values of feature-
maps. This is essentially a soft template match-
ing, which works for tasks like classification,
but undesired for maintaining the composition
functionality of convolution. In this paper, we
propose to use separate gating networks to re-
lease the scoring duty from the convolution,
and let it focus on composition. Similar idea
has been proposed by (Socher et al., 2011) for
recursive neural networks on parsing, but never
been combined with a convolutional structure.
</bodyText>
<figureCaption confidence="0.991892">
Figure 3: Illustration for gating network.
</figureCaption>
<bodyText confidence="0.825052777777778">
Suppose we have convolution feature-maps
on Layer-` and gating (with window size =
2) on Layer-`+1. For the jth gating win-
dow (2j−1, 2j), we merge ˆz(`−1)
2j−1 and ˆz(`−1)
2j as
the input (denoted as ¯z(`)
j ) for gating network,
as illustrated in Figure 3. We use a separate
</bodyText>
<figure confidence="0.9710258">
...
gating
Layer-
Layer-
Layer-
</figure>
<page confidence="0.984103">
1569
</page>
<bodyText confidence="0.982553125">
gate for each feature-map, but follow a differ-
ent parametrization strategy for TIME-FLOW
and TIME-ARROW. With window size = 2, the
gating is binary, we use a logistic regressor to
determine the weights of two candidates. For
f ∈ TIME-ARROW, with location-dependent
w(`,f,j), the normalized weight for le side is
gate g
</bodyText>
<equation confidence="0.9896475">
gj,f)= 1/(1 + e−wgate,7(`)
j ),
</equation>
<bodyText confidence="0.995796666666667">
while for For f ∈TIME-FLOW, the parameters
for the corresponding gating network, denoted
as wgate , are shared. The gated feature map is
</bodyText>
<equation confidence="0.732209625">
(`,f)
then a weighted sum to feature-maps from the
two windows:
z(`+1,f) z(`,f)
= g(`+1,f) 2j−1 + (1 − g(`+1,f)
j )z(`,f)
2j . (5)
j j
Figure 4: genCNN with recursive structure.
prediction for e10
αCNN
e5 e6 e7 e8 e7 e8 e9
“/ ”
βCNN
/ / / e1 e2 e3 e4
...
</equation>
<bodyText confidence="0.99995325">
We find that this gating strategy works signifi-
cantly better than pooling directly over feature-
maps, and slightly better than a hard gate ver-
sion of Equation 5
</bodyText>
<subsectionHeader confidence="0.999682">
3.3 Recursive Architecture
</subsectionHeader>
<bodyText confidence="0.999988290322581">
As suggested early on in Section 2 and Fig-
ure 1, we use extra CNNs with conventional
weight-sharing, named QCNN, to summarize
the history out of scope of αCNN. More specif-
ically, the output of QCNN (with the same di-
mension of word-embedding) is put before the
first word as the input to the αCNN, as il-
lustrated in Figure 4. Different from αCNN,
QCNN is designed just to summarize the his-
tory, with weight shared across its convolution
units. In a sense, QCNN has only TIME-FLOW
feature-maps. All QCNN are identical and re-
cursively aligned, enabling genCNN to handle
sentences with arbitrary length. We put a spe-
cial switch after each QCNN to turn it off (re-
placing a pading vector shown as “/” in Fig-
ure 4 ) when there is no history assigned to it.
As the result, when the history is shorter than
Lα, the recursive structure reduces to αCNN.
In practice, 90+% sentences can be mod-
eled by αCNN with Lα = 40 and 99+% sen-
tences can be contained with one extra QCNN.
Our experiment shows that this recursive strat-
egy yields better estimate of conditional den-
sity than neglecting the out-of-scope history
(Section 6.1.2). In practice, we found that a
larger (greater Lα) and deeper αCNN works
better than small αCNN and more recursion,
which is consistent with our intuition that the
convolutional architecture is better suited for
modeling the sequence.
</bodyText>
<subsectionHeader confidence="0.8837005">
3.4 Analysis
3.4.1 TIME-FLOW vs. TIME-ARROW
</subsectionHeader>
<bodyText confidence="0.999255782608696">
Both conceptually and systemically, genCNN
gives two interweaved treatments of word his-
tory. With the globally-shared parameters in
the convolution units, TIME-FLOW summa-
rizes what has been said. The hierarchi-
cal convolution+gating architecture in TIME-
FLOW enables it to model the composition in
language, yielding representation of segments
at different intermediate layers. TIME-FLOW
is aware of the sequential direction, inherited
from the space-awareness of CNN, but it is not
sensitive enough about the prediction task, due
to the uniform weights in the convolution.
On the other hand, TIME-ARROW, living
in location-dependent parameters of convolu-
tion units, acts like an arrow pin-pointing the
prediction task. TIME-ARROW has predictive
power all by itself, but it concentrates on cap-
turing the direction of time and consequently
short on modelling the long-range dependency.
TIME-FLOW and TIME-ARROW have to
work together for optimal performance in pre-
dicting what is going to be said. This intuition
</bodyText>
<page confidence="0.954924">
1570
</page>
<bodyText confidence="0.999968625">
has been empirically verified, as our experi-
ments have demonstrated that TIME-FLOW or
TIME-ARROW alone perform inferiorly. One
can imagine, through the layer-by-layer convo-
lution and gating, the TIME-ARROW gradually
picks the most relevant part from the represen-
tation of TIME-FLOW for the prediction task,
even if that part is long distance ahead.
</bodyText>
<subsectionHeader confidence="0.758154">
3.4.2 genCNN vs. RNN-LM
</subsectionHeader>
<bodyText confidence="0.870974857142857">
Different from RNNs, which recursively ap-
plies a relatively simple processing units,
genCNN gains its ability on sequence mod-
eling mostly from its flexible and power-
ful bottom-up and convolution architecture.
genCNN takes the “uncompressed” history,
therefore avoids
</bodyText>
<listItem confidence="0.87626775">
• the difficulty in finding the representation
for history, e.g., those end in the middle of
a chunk (e.g.,“the cat sat on the”),
• the damping effort in RNN when the
</listItem>
<bodyText confidence="0.983865375">
history-summarizing hidden state is up-
dated at each time stamp, which renders
the long-range memory rather difficult,
both of which can only be partially ameliorated
with complicated design of gates (Hochreiter
and Schmidhuber, 1997) and or more heavy
processing units (essentially a fully connected
DNN) (Sutskever et al., 2014).
</bodyText>
<sectionHeader confidence="0.984241" genericHeader="method">
4 genCNN: Training
</sectionHeader>
<bodyText confidence="0.999683857142857">
The parameters of a genCNN Θ¯ consists of
the parameters for CNN Θnn, word-embedding
Θembed, and the parameters for soft-max
Θsoftmax. All the parameters are jointly
learned by maximizing the likelihood of ob-
served sentences. Formally the log-likelihood
of sentence Sn ( def= [e(n)
</bodyText>
<equation confidence="0.984866833333333">
1 , e(n)
2 , · · · , e(Tn
n)]) is
log p(e(n)
t |e(n)
1:t−1; ¯Θ),
</equation>
<bodyText confidence="0.9849556">
which can be trivially split into Tn training in-
stances during the optimization, in contrast to
the training of RNN that requires unfolding
through time due to the temporal-dependency
of the hidden states.
</bodyText>
<subsectionHeader confidence="0.996126">
4.1 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999970866666666">
Architectures: In all of our experiments
(Section 5 and 6) we set the maximum words
for αCNN to be 30 and that for QCNN to be 20.
αCNN have two convolution layers (both con-
taining TIME-FLOW and TIME-ARROW con-
volution) and two gating layers, followed by
a fully connected layer (400 dimension) and
then a soft-max layer. The numbers of feature-
maps for TIME-FLOW are respectively 150
(1st convolution layer) and 100 (2nd convolu-
tion layer), while TIME-ARROW has the same
feature-maps. QCNN is relatively simple, with
two convolution layer containing only TIME-
FLOW with 150 feature-maps, two gating lay-
ers and a fully connected layer. We use ReLU
as the activation function for convolution lay-
ers and switch to Sigmoid for fully connected
layers. We use word embedding with dimen-
sion 100.
Soft-max: Calculating a full soft-max is ex-
pensive since it has to enumerate all the words
in vocabulary (in our case 40K words) in the
denominator. Here we take a simple hierarchi-
cal approximation of it, following (Bahdanau
et al., 2014). Basically we group the words
into 200 clusters (indexed by cm), and factor-
ize (in an approximate sense) the conditional
probability of a word p(et|e1:t−1; Θ) into the
probability of its cluster and the probability of
et given its cluster
</bodyText>
<equation confidence="0.942299">
p(cm|e1:t−1; Θ) p(et|cm; Θsoftmax).
</equation>
<bodyText confidence="0.999956866666667">
We found that this simple heuristic can speed-
up the optimization by 5 times with only slight
loss of accuracy.
Optimization: We use stochastic gradient
descent with mini-batch (size 500) for opti-
mization, aided further by AdaGrad (Duchi
et al., 2011). For initialization, we use
Word2Vec (Mikolov et al., 2013) for the start-
ing state of the word-embeddings (trained on
the same dataset as the main task), and set
all the other parameters by randomly sampling
from uniform distribution in [−0.1, 0.1]. The
optimization is done mainly on a Tesla K40
GPU, which takes about 2 days for the train-
ing on a dataset containing 1M sentences.
</bodyText>
<equation confidence="0.941543">
Tn
log p(Sn; Θ) =
t=1
</equation>
<page confidence="0.989068">
1571
</page>
<sectionHeader confidence="0.992463" genericHeader="method">
5 Experiments: Sentence Generation
</sectionHeader>
<bodyText confidence="0.9951435">
In this experiment, we randomly generate sen-
tences by recurrently sampling
</bodyText>
<equation confidence="0.852146">
et+1 ∼ p(et+1je1:t; O),
</equation>
<bodyText confidence="0.999935833333333">
and put the newly generated word into history,
until EOS (end-of-sentence) is generated. We
consider generating two types of sentences: 1)
the plain sentences, and 2) sentences with de-
pendency parsing, which will be covered re-
spectively in Section 5.1 and 5.2.
</bodyText>
<subsectionHeader confidence="0.962323">
5.1 Natural Sentences
</subsectionHeader>
<bodyText confidence="0.999978647058824">
We train genCNN on Wiki data with 112M
words for one week, with some representative
examples randomly generated given in Table 1
(upper and middle blocks). We try two settings,
by letting genCNN generate a sentence 1)from
the very beginning (middle block), or 2) start-
ing with a few words given by human (upper
block). It is fairly clear that most of the time
genCNN can generate sentences that are syn-
tactically grammatical and semantically mean-
ingful. More specifically, most of the sentences
can be aligned to a parse tree with reasonable
structure. It is also worth noting that quotation
marks (‘‘ and ’’) are always generated in pairs
and in the correct order, even across a relatively
long distance, as exemplified by the first gener-
ated sentence in the upper block.
</bodyText>
<subsectionHeader confidence="0.999396">
5.2 Sentences with Dependency Tags
</subsectionHeader>
<bodyText confidence="0.999796666666667">
For training, we first parse(Klein and Man-
ning, 2002) the English sentences and feed se-
quences with dependency tags as follows
</bodyText>
<equation confidence="0.516359">
( I * like ( red * apple ) )
</equation>
<bodyText confidence="0.9948439">
to genCNN in training, where 1) each paired
parentheses contain a subtree, and 2) the sym-
bol “*” indicates that the word next to it is
the dependency head in the corresponding sub-
tree. Some representative examples gener-
ated by genCNN are given in Table 1 (bottom
block). As it suggests, genCNN is fairly ac-
curate on respecting the rules of parentheses,
and probably more remarkably, it can get the
dependency tree head right most of the time.
</bodyText>
<sectionHeader confidence="0.993226" genericHeader="method">
6 Experiments: Language Modeling
</sectionHeader>
<bodyText confidence="0.998892375">
We evaluate our model as a language model in
terms of both perplexity (Brown et al., 1992)
and its efficacy in re-ranking the n-best can-
didates from state-of-the-art models in statisti-
cal machine translation, with comparison to the
following competitor language models.
Competitor Models we compare genCNN
to the following competitor models
</bodyText>
<listItem confidence="0.998477789473684">
• 5-gram: We use SRI Language Modeling
Toolkit (Stolcke and others, 2002) to train
a 5-gram language model with modified
Kneser-Ney smoothing;
• FFN-LM: The neural language model
based on feedfoward network (Vaswani et
al., 2013). We vary the input window-size
from 5 to 20, while the performance stops
increasing after window size 20;
• RNN: we use the implementation1 of
RNN-based language model with hidden
size 600;
• LSTM: we adopt the code in Ground-
hog2, but vary the hyper-parameters,
including the depth and word-embedding
dimension, for best performance.
LSTM (Hochreiter and Schmidhuber,
1997) is widely considered to be the
state-of-the-art for sequence modeling.
</listItem>
<subsectionHeader confidence="0.982558">
6.1 Perplexity
</subsectionHeader>
<bodyText confidence="0.999929333333333">
We test the performance of genCNN on PENN
TREEBANK and FBIS, two public datasets
with different sizes.
</bodyText>
<subsectionHeader confidence="0.4251">
6.1.1 On PENN TREEBANK
</subsectionHeader>
<bodyText confidence="0.999951888888889">
Although a relatively small dataset 3, PENN
TREEBANK is widely used as a language mod-
elling benchmark (Graves, 2013; Mikolov et
al., 2010). It has 930, 000 words in train-
ing set, 74, 000 words in validation set, and
82, 000 words in test set. We use exactly the
same settings as in (Mikolov et al., 2010),
with a 10, 000-words vocabulary (all out-of-
vocabulary words are replaced with unknown)
</bodyText>
<footnote confidence="0.970935">
1http://rnnlm.org/
2https://github.com/lisa-groundhog/GroundHog
3http://www.fit.vutbr.cz/∼imikolov/rnnlm/simple-
examples.tgz
</footnote>
<page confidence="0.981483">
1572
</page>
<bodyText confidence="0.900007928571429">
“we are in the building of china ’s social development and the businessmen
audience , ”he said .
clinton was born in DDDD , and was educated at the university of edinburgh.
bush ’s first album , “the man ”, was released on DD november DDDD .
it is one of the first section of the act in which one is covered in real
place that recorded in norway .
this objective is brought to us the welfare of our country
russian president putin delivered a speech to the sponsored by the 15th asia
pacific economic cooperation ( apec ) meeting in an historical arena on oct .
light and snow came in kuwait and became operational , but was rarely
placed in houston .
johnson became a drama company in the DDDDs , a television broadcasting
company owned by the broadcasting program .
( ( the two * sides ) * should ( * assume ( a strong * target ) ) ) . )
</bodyText>
<table confidence="0.9763325">
( it * is time ( * in ( every * country ) * signed ( the * speech ) ) . )
( ( initial * investigations ) * showed ( * that ( spot * could ( * be (
further * improved significantly ) ) . )
( ( a * book ( to * northern ( the 21 st * century ) ) ) . )
</table>
<tableCaption confidence="0.998016">
Table 1: Examples of sentences generated by genCNN. In the upper block (row 1-4) the underline
</tableCaption>
<bodyText confidence="0.9909323125">
words are given by the human; In the middle block (row 5-8), all the sentences are generated
without any hint. The bottom block (row 9-12) shows the sentences with dependency tag generated
by genCNN trained with parsed examples.
and end-of-sentence token (EOS) at the end of
each sentence. In addition to the conventional
testing strategy where the models are kept un-
changed during testing, Mikolov et al. (2010)
proposes to also update the parameters in an
online fashion when seeing test sentences. This
new way of testing, named “dynamic evalua-
tion”, is also adopted by Graves (2013).
From Table 2 genCNN manages to give per-
plexity superior in both metrics, with about 25
point reduction over the widely used 5-gram,
and over 10 point reduction from LSTM, the
state-of-the-art and the second-best performer.
</bodyText>
<subsectionHeader confidence="0.896135">
6.1.2 On FBIS
</subsectionHeader>
<bodyText confidence="0.995541727272727">
The FBIS corpus (LDC2003E14) is relatively
large, with 22.5K sentences and 8.6M English
words. The validation set is NIST MT06 and
test set is NIST MT08. For training the neural
network, we limit the vocabulary to the most
frequent 40,000 words, covering ∼ 99.4% of
the corpus. Similar to the first experiment,
all out-of-vocabulary words are replaced with
unknown and the EOS token is counted in the
sequence loss.
From Table 3 (upper block), genCNN
</bodyText>
<table confidence="0.999154333333333">
Model Perplexity Dynamic
5-gram, KN5 141.2 –
FFNN-LM 140.2 –
RNN 124.7 123.2
LSTM 126 117
genCNN 116.4 106.3
</table>
<tableCaption confidence="0.988273">
Table 2: PENN TREEBANK results, where the
</tableCaption>
<bodyText confidence="0.9859934375">
3rd column are the perplexity in dynamic eval-
uation, while the numbers for RNN and LSTM
are taken as reported in the paper cited above.
The numbers in boldface indicate that the re-
sult is significantly better than all competitors
in the same setting.
clearly wins again in the comparison to com-
petitors, with over 25 point margin over LSTM
(in its optimal setting), the second best per-
former. Interestingly genCNN outperforms its
variants also quite significantly (bottom block):
1) with only TIME-ARROW (same number
of feature-maps), the performance deteriorates
considerably for losing the ability of capturing
long range correlation reliably; 2) with only
TIME-TIME the performance gets even worse,
</bodyText>
<page confidence="0.930897">
1573
</page>
<table confidence="0.9970197">
Model Perplexity
5-gram, KN5 278.6
FFN-LM(5-gram) 248.3
FFN-LM(20-gram) 228.2
RNN 223.4
LSTM 206.9
genCNN 181.2
TIME-ARROW only 192
TIME-FLOW only 203
αCNN only 184.4
</table>
<tableCaption confidence="0.99976">
Table 3: FBIS results. The upper block
</tableCaption>
<bodyText confidence="0.997775454545454">
(row 1-6) compares genCNN and the competi-
tor models, and the bottom block (row 7-9)
compares different variants of genCNN.
for partially losing the sensitivity to the predic-
tion task. It is quite remarkable that, although
αCNN (with Lα = 30) can achieve good re-
sults, the recursive structure in full genCNN
can further decrease the perplexity by over
3 points, indicating that genCNN can benefit
from modeling the dependency over range as
long as 30 words.
</bodyText>
<subsectionHeader confidence="0.999364">
6.2 Re-ranking for Machine Translation
</subsectionHeader>
<bodyText confidence="0.999907409090909">
In this experiment, we re-rank the 1000-best
English translation candidates for Chinese sen-
tences generated by statistical machine transla-
tion (SMT) system, and compare it with other
language models in the same setting.
SMT setup The baseline hierarchical phrase-
based SMT system ( Chines→ English) was
built using Moses, a widely accepted state-
of-the-art, with default settings. The bilin-
gual training data is from NIST MT2012 con-
strained track, with reduced size of 1.1M sen-
tence pairs using selection strategy in (Axel-
rod et al., 2011). The baseline use conven-
tional 5-gram language model (LM), estimated
with modified Kneser-Ney smoothing (Chen
and Goodman, 1996) on the English side of the
329M-word Xinhua portion of English Giga-
word(LDC2011T07). We also try FFN-LM, as
a much stronger language model in decoding.
The weights of all the features are tuned via
MERT (Och and Ney, 2002) on NIST MT05,
and tested on NIST MT06 and MT08. Case-
</bodyText>
<table confidence="0.99945725">
Models MT06 MT08 Ave.
Baseline 38.63 31.11 34.87
RNN rerank 39.03 31.50 35.26
LSTM rerank 39.20 31.90 35.55
FFN-LM rerank 38.93 31.41 35.14
genCNN rerank 39.90 32.50 36.20
Base+FFN-LM 39.08 31.60 35.34
genCNN rerank 40.4 32.85 36.63
</table>
<tableCaption confidence="0.753753333333333">
Table 4: The results for re-ranking the 1000-
best of Moses. Note that the two bottom rows
are on a baseline with enhanced LM.
</tableCaption>
<bodyText confidence="0.998555071428571">
insensitive NIST BLEU4 is used in evaluation.
Re-ranking with genCNN significantly im-
proves the quality of the final translation. In-
deed, it can increase the BLEU score by over
1.33 point over Moses baseline on average.
This boosting force barely slacks up on trans-
lation with a enhanced language model in de-
coding: genCNN re-ranker still achieves 1.29
point improvement on top of Moses with FFN-
LM, which is 1.76 point over the Moses (de-
fault setting). To see the significance of this
improvement, the state-of-the-art Neural Net-
work Joint Model (Devlin et al., 2014) usually
brings less than one point increase on this task.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999898222222222">
In addition to the long thread of work on neu-
ral network based language model (Auli et al.,
2013; Mikolov et al., 2010; Graves, 2013; Ben-
gio et al., 2003; Vaswani et al., 2013), our work
is also related to the effort on modeling long
range dependency in word sequence predic-
tion(Wu and Khudanpur, 2003). Different from
those work on hand-crafting features for incor-
porating long range dependency, our model can
elegantly assimilate relevant information in an
unified way, in both long and short range, with
the bottom-up information flow and convolu-
tional architecture.
CNN has been widely used in computer
vision and speech (Lawrence et al., 1997;
Krizhevsky et al., 2012; LeCun and Bengio,
1995; Abdel-Hamid et al., 2012), and lately
in sentence representation(Kalchbrenner and
</bodyText>
<footnote confidence="0.6375445">
4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-
v11b.pl
</footnote>
<page confidence="0.992101">
1574
</page>
<bodyText confidence="0.998692846153846">
Blunsom, 2013), matching(Hu et al., 2014) and
classification(Kalchbrenner et al., 2014). To
our best knowledge, it is the first time this is
used in word sequence prediction. Model-wise
the previous work that is closest to genCNN is
the convolution model for predicting moves in
the Go game (Maddison et al., 2014), which,
when applied recurrently, essentially gener-
ates a sequence. Different from the conven-
tional CNN taken in (Maddison et al., 2014),
genCNN has architectures designed for mod-
eling the composition in natural language and
the temporal structure of word sequence.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999190833333333">
We propose a convolutional architecture for
natural language generation and modeling. Our
extensive experiments on sentence generation,
perplexity, and n-best re-ranking for machine
translation show that our model can signifi-
cantly improve upon state-of-the-arts.
</bodyText>
<sectionHeader confidence="0.993677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998060870370371">
[Abdel-Hamid et al.2012] Ossama Abdel-Hamid,
Abdel-rahman Mohamed, Hui Jiang, and Gerald
Penn. 2012. Applying convolutional neural
networks concepts to hybrid nn-hmm model
for speech recognition. In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE
International Conference on, pages 4277–4280.
IEEE.
[Auli et al.2013] Michael Auli, Michel Galley,
Chris Quirk, and Geoffrey Zweig. 2013. Joint
language and translation modeling with recur-
rent neural networks. In Proceedings of the
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1044–1054,
Seattle, Washington, USA, October.
[Axelrod et al.2011] Amittai Axelrod, Xiaodong
He, and Jianfeng Gao. 2011. Domain adapta-
tion via pseudo in-domain data selection. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 355–
362. Association for Computational Linguistics.
[Bahdanau et al.2014] Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio. 2014.
Neural machine translation by jointly learn-
ing to align and translate. arXiv preprint
arXiv:1409.0473.
[Bengio et al.2003] Yoshua Bengio, Rjean
Ducharme, Pascal Vincent, and Christian
Jauvin. 2003. A neural probabilistic lan-
guage model. Journal OF Machine Learning
Research, 3:1137–1155.
[Brown et al.1992] Peter F. Brown, Vincent J. Della
Pietra, Robert L. Mercer, Stephen A. Della
Pietra, and Jennifer C. Lai. 1992. An estimate of
an upper bound for the entropy of english. Com-
put. Linguist., 18(1):31–40, March.
[Chen and Goodman1996] Stanley F Chen and
Joshua Goodman. 1996. An empirical study of
smoothing techniques for language modeling.
In Proceedings of the 34th annual meeting
on Association for Computational Linguistics,
pages 310–318. Association for Computational
Linguistics.
[Dahl et al.2013] George E Dahl, Tara N Sainath,
and Geoffrey E. Hinton. 2013. Improving deep
neural networks for lvcsr using rectified linear
units and dropout. In Proceedings of ICASSP.
[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics, pages 1370–1380.
</reference>
<page confidence="0.78444">
1575
</page>
<reference confidence="0.999822604651163">
[Duchi et al.2011] John Duchi, Elad Hazan, and
Yoram Singer. 2011. Adaptive subgradient
methods for online learning and stochastic opti-
mization. The Journal of Machine Learning Re-
search, 12:2121–2159.
[Graves2013] Alex Graves. 2013. Generating se-
quences with recurrent neural networks. CoRR,
abs/1308.0850.
[Hochreiter and Schmidhuber1997] Sepp Hochre-
iter and J¨urgen Schmidhuber. 1997. Long short-
term memory. Neural Comput., 9(8):1735–
1780, November.
[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang
Li, and Qingcai Chen. 2014. Convolutional
neural network architectures for matching natu-
ral language sentences. In NIPS.
[Kalchbrenner and Blunsom2013] Nal Kalchbren-
ner and Phil Blunsom. 2013. Recurrent contin-
uous translation models. In Proceedings of the
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709,
Seattle, Washington, USA, October.
[Kalchbrenner et al.2014] Nal Kalchbrenner, Ed-
ward Grefenstette, and Phil Blunsom. 2014. A
convolutional neural network for modelling sen-
tences. ACL.
[Klein and Manning2002] Dan Klein and Christo-
pher D Manning. 2002. Fast exact inference
with a factored model for natural language pars-
ing. In Advances in neural information process-
ing systems, volume 15, pages 3–10.
[Krizhevsky et al.2012] Alex Krizhevsky, Ilya
Sutskever, and Geoffrey E Hinton. 2012.
Imagenet classification with deep convolutional
neural networks. In Advances in neural infor-
mation processing systems, pages 1097–1105.
[Lawrence et al.1997] Steve Lawrence, C Lee Giles,
Ah Chung Tsoi, and Andrew D Back. 1997.
Face recognition: A convolutional neural-
network approach. Neural Networks, IEEE
Transactions on, 8(1):98–113.
[LeCun and Bengio1995] Yann LeCun and Yoshua
Bengio. 1995. Convolutional networks for im-
ages, speech, and time series. The handbook of
brain theory and neural networks, 3361:310.
[Maddison et al.2014] Chris J. Maddison, Aja
Huang, Ilya Sutskever, and David Silver. 2014.
Move evaluation in go using deep convolutional
neural networks. CoRR, abs/1412.6564.
[Mikolov et al.2010] Tomas Mikolov, Martin
Karafit, Lukas Burget, Jan Cernocky, and
Sanjeev Khudanpur. 2010. In INTERSPEECH,
pages 1045–1048.
[Mikolov et al.2013] Tomas Mikolov, Kai Chen,
Greg Corrado, and Jeffrey Dean. 2013. Effi-
cient estimation of word representations in vec-
tor space. CoRR, abs/1301.3781.
[Och and Ney2002] Franz Josef Och and Hermann
Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine
translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Lin-
guistics, pages 295–302.
[Socher et al.2011] Richard Socher, Cliff C. Lin,
Andrew Y. Ng, and Christopher D. Manning.
2011. Parsing Natural Scenes and Natural Lan-
guage with Recursive Neural Networks. In Pro-
ceedings of the 26th International Conference on
Machine Learning (ICML).
[Stolcke and others2002] Andreas Stolcke et al.
2002. Srilm-an extensible language modeling
toolkit. In Proceedings of the international
conference on spoken language processing, vol-
ume 2, pages 901–904.
[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc V Le. 2014. Sequence to sequence
learning with neural networks. In NIPS.
[Vaswani et al.2013] Ashish Vaswani, Yinggong
Zhao, Victoria Fossum, and David Chiang.
2013. Decoding with large-scale neural lan-
guage models improves translation. In EMNLP,
pages 1387–1392. Citeseer.
[Wu and Khudanpur2003] Jun Wu and Sanjeev
Khudanpur. 2003. Maximum entropy language
modeling with non-local dependencies. Ph.D.
thesis, Johns Hopkins University.
</reference>
<page confidence="0.992546">
1576
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.443398">
<title confidence="0.999601">A Convolutional Architecture for Word Sequence Prediction</title>
<author confidence="0.989894">Zhengdong Hang Wenbin Qun</author>
<affiliation confidence="0.96114">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of</affiliation>
<author confidence="0.528184">Ark Lab</author>
<author confidence="0.528184">Huawei</author>
<affiliation confidence="0.990754">Centre, School of Computing, Dublin City University</affiliation>
<abstract confidence="0.997855689655172">We propose a convolutional neural network, named genCNN, for word sequence prediction. Different from previous work on neural networkbased language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-ofthe-arts with big margins.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ossama Abdel-Hamid</author>
<author>Abdel-rahman Mohamed</author>
<author>Hui Jiang</author>
<author>Gerald Penn</author>
</authors>
<title>Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition.</title>
<date>2012</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on,</booktitle>
<pages>4277--4280</pages>
<publisher>IEEE.</publisher>
<marker>[Abdel-Hamid et al.2012]</marker>
<rawString>Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, and Gerald Penn. 2012. Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 4277–4280. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<location>Seattle, Washington, USA,</location>
<marker>[Auli et al.2013]</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044–1054, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Axelrod et al.2011]</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 355– 362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</title>
<date>2014</date>
<marker>[Bahdanau et al.2014]</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rjean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal OF Machine Learning Research,</journal>
<pages>3--1137</pages>
<marker>[Bengio et al.2003]</marker>
<rawString>Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal OF Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
<author>Stephen A Della Pietra</author>
<author>Jennifer C Lai</author>
</authors>
<title>An estimate of an upper bound for the entropy of english.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<volume>18</volume>
<issue>1</issue>
<marker>[Brown et al.1992]</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer, Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An estimate of an upper bound for the entropy of english. Comput. Linguist., 18(1):31–40, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Chen and Goodman1996]</marker>
<rawString>Stanley F Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 310–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Tara N Sainath</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Improving deep neural networks for lvcsr using rectified linear units and dropout.</title>
<date>2013</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<marker>[Dahl et al.2013]</marker>
<rawString>George E Dahl, Tara N Sainath, and Geoffrey E. Hinton. 2013. Improving deep neural networks for lvcsr using rectified linear units and dropout. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1370--1380</pages>
<marker>[Devlin et al.2014]</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<marker>[Duchi et al.2011]</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks.</title>
<date>2013</date>
<tech>CoRR, abs/1308.0850.</tech>
<marker>[Graves2013]</marker>
<rawString>Alex Graves. 2013. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long shortterm memory.</title>
<date>1997</date>
<journal>Neural Comput.,</journal>
<volume>9</volume>
<issue>8</issue>
<pages>1780</pages>
<marker>[Hochreiter and Schmidhuber1997]</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long shortterm memory. Neural Comput., 9(8):1735– 1780, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In NIPS.</booktitle>
<marker>[Hu et al.2014]</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<location>Seattle, Washington, USA,</location>
<marker>[Kalchbrenner and Blunsom2013]</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<publisher>ACL.</publisher>
<marker>[Kalchbrenner et al.2014]</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<volume>15</volume>
<pages>3--10</pages>
<marker>[Klein and Manning2002]</marker>
<rawString>Dan Klein and Christopher D Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Advances in neural information processing systems, volume 15, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1097--1105</pages>
<marker>[Krizhevsky et al.2012]</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Lawrence</author>
<author>C Lee Giles</author>
<author>Ah Chung Tsoi</author>
<author>Andrew D Back</author>
</authors>
<title>Face recognition: A convolutional neuralnetwork approach. Neural Networks,</title>
<date>1997</date>
<journal>IEEE Transactions on,</journal>
<volume>8</volume>
<issue>1</issue>
<marker>[Lawrence et al.1997]</marker>
<rawString>Steve Lawrence, C Lee Giles, Ah Chung Tsoi, and Andrew D Back. 1997. Face recognition: A convolutional neuralnetwork approach. Neural Networks, IEEE Transactions on, 8(1):98–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>Yoshua Bengio</author>
</authors>
<title>Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks,</title>
<date>1995</date>
<pages>3361--310</pages>
<marker>[LeCun and Bengio1995]</marker>
<rawString>Yann LeCun and Yoshua Bengio. 1995. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361:310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris J Maddison</author>
<author>Aja Huang</author>
<author>Ilya Sutskever</author>
<author>David Silver</author>
</authors>
<title>Move evaluation in go using deep convolutional neural networks.</title>
<date>2014</date>
<tech>CoRR, abs/1412.6564.</tech>
<marker>[Maddison et al.2014]</marker>
<rawString>Chris J. Maddison, Aja Huang, Ilya Sutskever, and David Silver. 2014. Move evaluation in go using deep convolutional neural networks. CoRR, abs/1412.6564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafit</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>[Mikolov et al.2010]</marker>
<rawString>Tomas Mikolov, Martin Karafit, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<marker>[Mikolov et al.2013]</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<marker>[Och and Ney2002]</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML).</booktitle>
<marker>[Socher et al.2011]</marker>
<rawString>Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In Proceedings of the 26th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on spoken language processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<marker>[Stolcke and others2002]</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the international conference on spoken language processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In NIPS.</booktitle>
<marker>[Sutskever et al.2014]</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1387--1392</pages>
<publisher>Citeseer.</publisher>
<marker>[Vaswani et al.2013]</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In EMNLP, pages 1387–1392. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wu</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Maximum entropy language modeling with non-local dependencies.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<marker>[Wu and Khudanpur2003]</marker>
<rawString>Jun Wu and Sanjeev Khudanpur. 2003. Maximum entropy language modeling with non-local dependencies. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>