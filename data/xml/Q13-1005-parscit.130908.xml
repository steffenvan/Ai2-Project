<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9995455">
Weakly Supervised Learning of Semantic Parsers
for Mapping Instructions to Actions
</title>
<author confidence="0.983744">
Yoav Artzi and Luke Zettlemoyer
</author>
<affiliation confidence="0.989845">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.91998">
Seattle, WA 98195
</address>
<email confidence="0.999439">
{yoav,lsz}@cs.washington.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881111111111">
The context in which language is used pro-
vides a strong signal for learning to recover
its meaning. In this paper, we show it can be
used within a grounded CCG semantic parsing
approach that learns a joint model of mean-
ing and context for interpreting and executing
natural language instructions, using various
types of weak supervision. The joint nature
provides crucial benefits by allowing situated
cues, such as the set of visible objects, to di-
rectly influence learning. It also enables algo-
rithms that learn while executing instructions,
for example by trying to replicate human ac-
tions. Experiments on a benchmark naviga-
tional dataset demonstrate strong performance
under differing forms of supervision, includ-
ing correctly executing 60% more instruction
sets relative to the previous state of the art.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989961933333333">
The context in which natural language is used pro-
vides a strong signal to reason about its meaning.
However, using such a signal to automatically learn
to understand unrestricted natural language remains
a challenging, unsolved problem.
For example, consider the instructions in Figure 1.
Correct interpretation requires us to solve many sub-
problems, such as resolving all referring expres-
sions to specific objects in the environment (includ-
ing, “the corner” or “the third intersection”), disam-
biguating word sense based on context (e.g., “the
chair” could refer to a chair or sofa), and finding
executable action sequences that satisfy stated con-
straints (such as “twice” or “to face the blue hall”).
move forward twice to the chair
</bodyText>
<equation confidence="0.641028666666667">
Aa.move(a) n dir(a, forward) n len(a, 2) n
to(a,ιx.chair(x))
at the corner turn left to face the blue hall
</equation>
<construct confidence="0.3166428">
Aa.pre(a, ιx.corner(x)) n turn(a) n dir(a, left) n
post(a,front(you,ιx.blue(x) n hall(x)))
move to the chair in the third intersection
Aa.move(a) n to(a, ιx.sofa(x)) n
intersect(order(Ay.junction(y), frontdist, 3), x)
</construct>
<figureCaption confidence="0.995868">
Figure 1: A sample navigation instruction set, paired
with lambda-calculus meaning representations.
</figureCaption>
<bodyText confidence="0.999932083333333">
We must also understand implicit requests, for ex-
ample from the phrase “at the corner,” that describe
goals to be achieved without specifying the specific
steps. Finally, to do all of this robustly without pro-
hibitive engineering effort, we need grounded learn-
ing approaches that jointly reason about meaning
and context to learn directly from their interplay,
with as little human intervention as possible.
Although many of these challenges have been
studied separately, as we will review in Section 3,
this paper represents, to the best of our knowledge,
the first attempt at a comprehensive model that ad-
dresses them all. Our approach induces a weighted
Combinatory Categorial Grammar (CCG), includ-
ing both the parameters of the linear model and a
CCG lexicon. To model complex instructional lan-
guage, we introduce a new semantic modeling ap-
proach that can represent a number of key linguistic
constructs that are common in spatial and instruc-
tional language. To learn from indirect supervision,
we define the notion of a validation function, for
example that tests the state of the agent after in-
terpreting an instruction. We then show how this
function can be used to drive online learning. For
</bodyText>
<page confidence="0.997601">
49
</page>
<bodyText confidence="0.952733777777778">
Transactions of the Association for Computational Linguistics, 1 (2013) 49–62. Action Editor: Jason Eisner.
Submitted 11/2012; Published 3/2013. c�2013 Association for Computational Linguistics.
that purpose, we adapt the loss-sensitive Perceptron
algorithm (Singh-Miller &amp; Collins, 2007; Artzi &amp;
Zettlemoyer, 2011) to use a validation function and
coarse-to-fine inference for lexical induction.
The joint nature of this approach provides crucial
benefits in that it allows situated cues, such as the
set of visible objects, to directly influence parsing
and learning. It also enables the model to be learned
while executing instructions, for example by trying
to replicate actions taken by humans. In particular,
we show that, given only a small seed lexicon and
a task-specific executor, we can induce high quality
models for interpreting complex instructions.
We evaluate the method on a benchmark naviga-
tional instructions dataset (MacMahon et al., 2006;
Chen &amp; Mooney, 2011). Our joint approach suc-
cessfully completes 60% more instruction sets rel-
ative to the previous state of the art. We also re-
port experiments that vary supervision type, finding
that observing the final position of an instruction ex-
ecution is nearly as informative as observing the en-
tire path. Finally, we present improved results on a
new version of the MacMahon et al. (2006) corpus,
which we filtered to include only executable instruc-
tions paired with correct traces.
</bodyText>
<sectionHeader confidence="0.989838" genericHeader="introduction">
2 Technical Overview
</sectionHeader>
<bodyText confidence="0.999871410714286">
Task Let S be the set of possible environment
states and A be the set of possible actions. Given
a start state s E S and a natural language instruc-
tion x, we aim to generate a sequence of actions
a� = (a1,... , ani, with each ai E A, that performs
the steps described in x.
For example, in the navigation domain (MacMa-
hon et al., 2006), S is a set of positions on a map.
Each state s = (x, y, o) is a triple, where x and y are
integer grid coordinates and o E {0, 90,180, 270} is
an orientation. Figure 2 shows an example map with
36 states; the ones we use in our experiments con-
tain an average of 141. The space of possible actions
A is {LEFT, RIGHT, MOVE, NULL}. Actions change
the state of the world according to a transition func-
tion T : A x S → S. In our navigation example,
moving forward can change the x or y coordinates
while turning changes the orientation o.
Model To map instructions to actions, we jointly
reason about linguistic meaning and action execu-
tion. We use a weighted CCG grammar to rank pos-
sible meanings z for each instruction x. Section 6
defines how to design such grammars for instruc-
tional language. Each logical form z is mapped to a
sequence of actions a� with a deterministic executor,
as described in Section 7. The final grounded CCG
model, detailed in Section 6.3, jointly constructs and
scores z and d, allowing for robust situated reason-
ing during semantic interpretation.
Learning We assume access to a training set con-
taining n examples {(xi, si, Vi) : i = 1... n}, each
containing a natural language sentence xi, a start
state si, and a validation function Vi. The validation
function Vi : A → {0, 1} maps an action sequence
a� E A to 1 if it’s correct according to available su-
pervision, or 0 otherwise. This training data contains
no direct evidence about the logical form zi for each
xi, or the grounded CCG analysis used to construct
zi. We model all these choices as latent variables.
We experiment with two validation functions. The
first, VD(d), has access to an observable demonstra-
tion of the execution di, a given a� is valid iff a� = di.
The second, VSi (d), only encodes the final state si
of the execution of x, therefore a� is valid iff its final
state is si. Since numerous logical forms often ex-
ecute identically, both functions provide highly am-
biguous supervision.
Evaluation We evaluate task completion for sin-
gle instructions on a test set {(xi, si, si) : i =
1... n}, where si is the final state of an oracle agent
following the execution of xi starting at state si. We
will also report accuracies for correctly interpreting
instruction sequences x, where a single error can
cause the entire sequence to fail. Finally, we report
accuracy on recovering correct logical forms zi on a
manually annotated subset of the test set.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999710333333333">
Our learning is inspired by the reinforcement learn-
ing (RL) approach of Branavan et al. (2009), and
related methods (Vogel &amp; Jurafsky, 2010), but uses
latent variable model updates within a semantic
parser. Branavan et al. (2010) extended their RL ap-
proach to model high-level instructions, which cor-
respond to implicit actions in our domain. Wei et al.
(2009) and Kollar et al. (2010) used shallow linguis-
tic representations for instructions. Recently, Tellex
</bodyText>
<page confidence="0.992399">
50
</page>
<bodyText confidence="0.999902767441861">
et al. (2011) used a graphical model semantics rep-
resentation to learn from instructions paired with
demonstrations. In contrast, we model significantly
more complex linguistic phenomena than these ap-
proaches, as required for the navigation domain.
Other research has adopted expressive meaning
representations, with differing learning approaches.
Matuszek et al. (2010, 2012) describe supervised al-
gorithms that learn semantic parsers for navigation
instructions. Chen and Mooney (2011), Chen (2012)
and Kim and Mooney (2012) present state-of-the-
art algorithms for the navigation task, by training a
supervised semantic parser from automatically in-
duced labels. Our work differs in the use of joint
learning and inference approaches.
Supervised approaches for learning semantic
parsers have received significant attention, e.g. Kate
and Mooney (2006), Wong and Mooney (2007),
Muresan (2011) and Kwiatkowski et al. (2010,
2012). The algorithms we develop in this pa-
per combine ideas from previous supervised CCG
learning work (Zettlemoyer &amp; Collins, 2005, 2007;
Kwiatkowski et al., 2011), as we describe in Sec-
tion 4. Recently, various alternative forms of su-
pervision were introduced. Clarke et al. (2010),
Goldwasser and Roth (2011) and Liang et al. (2011)
describe approaches for learning semantic parsers
from sentences paired with responses, Krishna-
murthy and Mitchell (2012) describe using distant
supervision, Artzi and Zettlemoyer (2011) use weak
supervision from conversational logs and Gold-
wasser et al. (2011) present work on unsupervised
learning. We discuss various forms of supervision
that complement these approaches. There has also
been work on learning for semantic analysis tasks
from grounded data, including event streams (Liang
et al., 2009; Chen et al., 2010) and language paired
with visual perception (Matuszek et al., 2012).
Finally, the topic of executing instructions in
non-learning settings has received significant atten-
tion (e.g., Winograd (1972), Di Eugenio and White
(1992), Webber et al. (1995), Bugmann et al. (2004),
MacMahon et al. (2006) and Dzifcak et al. (2009)).
</bodyText>
<sectionHeader confidence="0.996808" genericHeader="method">
4 Background
</sectionHeader>
<bodyText confidence="0.993506595238095">
We use a weighted linear CCG grammar for seman-
tic parsing, as briefly reviewed in this section.
Combinatory Categorial Grammars (CCGs)
CCGs are a linguistically-motivated formalism for
modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by a
lexicon and a set of combinators. The lexicon con-
tains entries that pair words or phrases with cate-
gories. For example, the lexical entry chair �- N :
Ax.chair(x) for the word “chair” in the parse in Fig-
ure 4 pairs it with a category that has syntactic type
N and meaning Ax.chair(x). Figure 4 shows how a
CCG parse builds a logical form for a complete sen-
tence in our example navigation domain. Starting
from lexical entries, each intermediate parse node,
including syntax and semantics, is constructed with
one of a small set of CCG combinators (Steedman,
1996, 2000). We use the application, composition
and coordination combinators, and three others de-
scribed in Section 6.3.
Factored CCG Lexicons Recently, Kwiatkowski
et al. (2011) introduced a factored CCG lexicon
representation. Each lexical item is composed of
a lexeme and a template. For example, the entry
chair �- N : Ax.chair(x) would be constructed by
combining the lexeme chair �- [chair], which con-
tains a word paired with logical constants, with the
template Av.[N : Ax.v(x)], that defines the rest of
the category by abstracting over logical constants.
This approach allows the reuse of common syntactic
structures through a small set of templates. Section 8
describes how we learn such lexical entries.
Weighted Linear CCGs A weighted linear
CCG (Clark &amp; Curran, 2007) ranks the space of
possible parses under the grammar, and is closely
related to several other approaches (Lafferty et al.,
2001; Collins, 2004; Taskar et al., 2004). Let x be a
sentence, y be a CCG parse, and GEN(x; A) be the
set of all possible CCG parses for x given the lexi-
con A. Define O(x, y) E Rd to be a d-dimensional
feature–vector representation and 0 E Rd to be a pa-
rameter vector. The optimal parse for sentence x is
</bodyText>
<equation confidence="0.995418">
y*(x) = arg max
yEGEN(x;A)
</equation>
<bodyText confidence="0.9996152">
and the final output logical form z is the A-calculus
expression at the root of y*(x). Section 7.2 de-
scribes how we efficiently compute an approxima-
tion to y*(x) within the joint interpretation and exe-
cution model.
</bodyText>
<equation confidence="0.935225">
0 · O(x, y)
</equation>
<page confidence="0.992751">
51
</page>
<bodyText confidence="0.986480052631579">
Supervised learning with GENLEX Previous
work (Zettlemoyer &amp; Collins, 2005) introduced a
function GENLEX(x, z) to map a sentence x and its
meaning z to a large set of potential lexical entries.
These entries are generated by rules that consider the
logical form z and guess potential CCG categories.
For example, the rule p -+ N : λx.p(x) introduces
categories commonly used to model certain types of
nouns. This rule would, for example, introduce the
category N : λx.chair(x) for any logical form z
that contains the constant chair. GENLEX uses a
small set of such rules to generate categories that
are paired with all possible substrings in x, to create
a large set of lexical entries. The complete learning
algorithm then simultaneously selects a small sub-
set of these entries and estimates parameter values
θ. In Section 8, we will introduce a new way of
using GENLEX to learn from different signals that,
crucially, do not require a labeled logical form z.
</bodyText>
<sectionHeader confidence="0.995536" genericHeader="method">
5 Spatial Environment Modeling
</sectionHeader>
<bodyText confidence="0.997621272727273">
We will execute instructions in an environment, see
Section 2, which has a set of positions. A position
is a triple (x, y, o), where x and y are horizontal and
vertical coordinates, and o E O = {0, 90,180, 270}
is an orientation. A position also includes properties
indicating the object it contains, its floor pattern and
its wallpaper. For example, the square at (4,3) in
Figure 2 has four positions, one per orientation.
Because instructional language refers to objects
and other structures in an environment, we introduce
the notion of a position set. For example, in Figure 2,
the position set D = {(5, 3, o) : o E O} represents
a chair, while B = {(x, 3, o) : o E O, x E [0 ... 5]}
represents the blue floor. Both sets contain all ori-
entations for each (x, y) pair, thereby representing
properties of regions. Position sets can have many
properties. For example, E, in addition to being a
chair, is also an intersection because it overlaps with
the neighboring halls A and B. The set of possi-
ble entities includes all position sets and a few addi-
tional entries. For example, set C = {(4, 3, 90)} in
Figure 2 represents the agent’s position.
</bodyText>
<sectionHeader confidence="0.94139" genericHeader="method">
6 Modeling Instructional Language
</sectionHeader>
<bodyText confidence="0.9988275">
We aim to design a semantic representation that is
learnable, models grounded phenomena such as spa-
</bodyText>
<figure confidence="0.9556446">
J D E l (a) chair
l f Ax.chair(x)
(b) hall
Ax.hall(x)
E (c) the chair
ιx.chair(x)
C (d) you
you
(e) blue hall
Ax.hall(x) ∧ blue(x)
(f) chair in the intersection
Ax.chair(x) ∧
intersect(ιy. junction(y), x)
J A B E I (g) in front of you
l Ax.in front of(you, x)
</figure>
<figureCaption confidence="0.9922725">
Figure 2: Schematic diagram of a map environment
and example of semantics of spatial phrases.
</figureCaption>
<bodyText confidence="0.989667545454546">
tial relations and object reference, and is executable.
Our semantic representation combines ideas from
Carpenter (1997) and Neo-Davidsonian event se-
mantics (Parsons, 1990) in a simply typed λ-
calculus. There are four basic types: (1) entities e
that are objects in the world, (2) events ev that spec-
ify actions in the world, (3) truth values t, and (4)
meta-entities m, such as numbers or directions. We
also allow functional types, which are defined by in-
put and output types. For example, (e, t) is the type
of function from entities to truth values.
</bodyText>
<subsectionHeader confidence="0.997069">
6.1 Spatial Language Modeling
</subsectionHeader>
<bodyText confidence="0.999768666666667">
Nouns and Noun Phrases Noun phrases are
paired with e-type constants that name specific en-
tities and nouns are mapped to (e, t)-type expres-
sions that define a property. For example, the noun
“chair” (Figure 2a) is paired with the expression
λx.chair(x), which defines the set of objects for
</bodyText>
<figure confidence="0.999560142857143">
1 2 3 4 5
0
90 270
180
E D
C
A
B
X
Y
1
2
3
4
5
I �
A B
I �
B
� �
E
</figure>
<page confidence="0.989391">
52
</page>
<bodyText confidence="0.998595095890411">
which the constant chair returns true. The deno-
tation of this expression is the set {D, E} in Fig-
ure 2 and the denotation of λx.hall(x) (Figure 2b)
is {A, B}. Also, the noun phrase “you” (Figure 2d),
which names the agent, is represented by the con-
stant you with denotation C, the agent’s position.
Determiners Noun phrases can also be formed by
combining nouns with determiners that pick out spe-
cific objects in the world. We consider both definite
reference, which names contextually unique objects,
and indefinites, which are less constrained.
The definite article is paired with a logical expres-
sion ι of type ((e, t), e),1 which will name a sin-
gle object in the world. For example, the phrase
“the chair” in Figure 2c will be represented by
ιx.chair(x) which will denote the appropriate chair.
However, computing this denotation is challenging
when there is perceptual ambiguity, for positions
where multiple chairs are visible. We adopt a sim-
ple heuristic approach that ranks referents based on
a combination of their distance from the agent and
whether they are in front of it. For our example,
from position C our agent would pick the chair E
in front of it as the denotation. The approach dif-
fers from previous, non-grounded models that fail to
name objects when faced with such ambiguity (e.g.,
Carpenter (1997), Heim and Kratzer (1998)).
To model the meaning of indefinite articles, we
depart from the Frege-Montague tradition of us-
ing existential quantifiers (Lewis, 1970; Montague,
1973; Barwise &amp; Cooper, 1981), and instead in-
troduce a new quantifier A that, like ι, has type
((e, t), e). For example, the phrase “a chair” would
be paired with Ax.chair(x) which denotes an arbi-
trary entry from the set of chairs in the world. Com-
puting the denotation for such expressions in a world
will require picking a specific object, without fur-
ther restrictions. This approach is closely related to
Steedman’s generalized Skolem terms (2011).2
Meta Entities We use m-typed terms to represent
non-physical entities, such as numbers (1, 2, etc.)
and directions (left, right, etc.) whose denotations
1Although quantifiers are logical constants with type
((e, t), e) or ((e, t), t), we use a notation similar to that used
for first-order logic. For example, the notation ιx.f(x) repre-
sents the logical expression ι(Ax.f(x))
2Steedman (2011) uses generalized Skolem terms as a tool
for resolving anaphoric pronouns, which we do not model.
are fixed. The ability to refer to directions allows
us to manipulate position sets. For example, the
phrase “your left” is mapped to the logical expres-
sion orient(you, left), which denotes the position
set containing the position to the left of the agent.
Prepositions and Adjectives Noun phrases with
modifiers, such as adjectives and prepositional
phrases are (e, t)-type expressions that implement
set intersection with logical conjunctions. For ex-
ample in Figure 2, the phrase “blue hall” is paired
with λx.hall(x) ∧blue(x) with denotation {B} and
the phrase “chair in the intersection” is paired with
λx.chair(x) ∧ intersect(ιy.junction(y),x) with
denotation {E}. Intuitively, the adjective “blue”
introduces the constant blue and “in the” adds a
intersect. We will describe the full details of how
these expressions are constructed in Section 6.3.
Spatial Relations The semantic representational-
lows more complex reasoning over position sets and
the relations between them. For example, the bi-
nary relation in front of (Figure 2g) tests if the
first argument is in front of the second from the point
of view of the agent. Additional relations are used
to model set intersection, relative direction, relative
distance, and relative position by distance.
</bodyText>
<subsectionHeader confidence="0.999738">
6.2 Modeling Instructions
</subsectionHeader>
<bodyText confidence="0.9318961">
To model actions in the world, we adopt Neo-
Davidsonian event semantics (Davidson, 1967; Par-
sons, 1990), which treats events as ev-type primitive
objects. Such an approach allows for a compact lex-
icon where adverbial modifiers introduce predicates,
which are linked by a shared event argument.
Instructional language is characterized by heavy
usage of imperatives, which we model as func-
tions from events to truth values.3 For example, an
imperative such as “move” would have the mean-
ing λa.move(a), which defines a set of events that
match the specified constraints. Here, this set would
include all events that involve moving actions.
The denotation of ev-type terms is a sequence
of n instances of the same action. In this way, an
event defines a function ev : s → s&apos;, where s is
the start state and s&apos; the end state. For example, the
3Imperatives are (ev, t)-type, much like (e, t)-type wh-
interrogatives. Both define sets, the former includes actions to
execute, the later defines answers to a question.
</bodyText>
<page confidence="0.995216">
53
</page>
<bodyText confidence="0.913487352941177">
denotation of Aa.move(a) is the set of move action
sequences {(MOVE1, ... , MOVEn) : n ≥ 1}. Al-
though performing actions often require performing
additional ones (e.g., the agent might have to turn
before being able to move), we treat such actions as
implicit (Section 7.1), and don’t model them explic-
itly within the logical form.
Predicates such as move (seen above) and
turn are introduced by verbs. Events can also
be modified by adverbials, which are intersective,
much like prepositional phrases. For example in the
imperative, logical form (LF) pair:
Imp.: move from the sofa to the chair
LF: Aa.move(a) n to(a, bx.chair(x)) n
from(a, by.sofa(y))
Each adverbial phrase provides a constraint, and
changing their order will not change the LF.
</bodyText>
<subsectionHeader confidence="0.997998">
6.3 Parsing Instructional Language with CCG
</subsectionHeader>
<bodyText confidence="0.998780571428572">
To compose logical expressions from sentences we
use CCG, as described in Section 4. Figures 3 and 4
present a sample of lexical entries and how they are
combined, as we will describe in this section. The
basic syntactic categories are N (noun), NP (noun
phrase), S (sentence), PP (prepositional phrase),
AP (adverbial phrase), ADJ (adjective) and C (a
special category for coordinators).
Type Raising To compactly model syntactic vari-
ations, we follow Carpenter (1997), who argues for
polymorphic typing. We include the more simple, or
lower type, entry in the lexicon and introduce type-
raising rules to reconstruct the other when necessary
at parse time. We use four rules:
</bodyText>
<construct confidence="0.972715">
PP : g N\N : Af.Ax.f(x) n g(x)
ADJ : g N/N : Af.Ax.f(x) n g(x)
AP : g S\S : Af.Aa.f(a) n g(a)
AP : g S/S : Af.Aa.f(a) n g(a)
</construct>
<bodyText confidence="0.994756166666667">
where the first three are for prepositional, adjectival
and adverbial modifications, and the fourth models
the fact that adverbials are often topicalized.4 Fig-
ures 3 and 4 show parses that use type-raising rules.
Indefinites As discussed in Section 6.1, we use
a new syntactic analysis for indefinites, follow-
</bodyText>
<footnote confidence="0.38714025">
4Using type-raising rules can be particularly useful when
learning from sparse data. For example, it will no longer be
necessary to learn three lexical entries for each adverbial phrase
(with syntax AP, S\S, and S/S).
</footnote>
<table confidence="0.5067275">
chair in the corner
N PP/NP NP/N N
</table>
<equation confidence="0.984387222222222">
ax.chair(x) ax.ay.intersect(x, y) af.Ax.f(x) ax.corner(x)
NP
ιx.corner(x)
PP
ay.intersect(ιx.corner(x), y)
N\N
af.ay.f(y) h intersect(ιx.chair(x), y)
N
ay.chair(y) h intersect(ιx.chair(x), y)
</equation>
<figureCaption confidence="0.998166">
Figure 3: A CCG parse with a prepositional phrase.
</figureCaption>
<bodyText confidence="0.8722765">
ing Steedman (2011). Previous approaches would
build parses such as
</bodyText>
<equation confidence="0.925501">
with a lamp
PP\(PP/NP)
ag.ay.∃x.g(x, y) h lamp(x)
PP
ay.∃x.intersect(x, y) h lamp(x)
</equation>
<bodyText confidence="0.996546571428572">
where “a” has the relatively complex syntactic cate-
gory PP\(PP/NP)/N and where similar entries
would be needed to quantify over different types
of verbs (e.g., S\(S/NP)/N) and adverbials (e.g.,
AP\(AP/NP)/N). Instead, we include a single
lexical entry a �- NP/N : Af.Ax.f(x) which can
be used to construct the correct meaning in all cases.
</bodyText>
<sectionHeader confidence="0.882354" genericHeader="method">
7 Joint Parsing and Execution
</sectionHeader>
<bodyText confidence="0.9999904">
Our inference includes an execution component and
a parser. The parser maps sentences to logical forms,
and incorporates the grounded execution model. We
first discuss how to execute logical forms, and then
describe the joint model for execution and parsing.
</bodyText>
<subsectionHeader confidence="0.993124">
7.1 Executing Logical Expressions
</subsectionHeader>
<bodyText confidence="0.999625333333333">
Dynamic Models In spatial environments, such as
the ones in our task, the agent’s ability to observe the
world depends on its current state. Taking this aspect
of spatial environments into account is challenging,
but crucial for correct evaluation.
To represent the agent’s point of view, for each
state s E S, as defined in Section 2, let Ms be the
state-dependent logical model. A model M consists
of a domain DM,T of objects for each type T and
an interpretation function ZM,T : OT DM,T,
where OT is the set of T-type constants. ZM,T
maps logical symbols to T-type objects, for exam-
ple, it will map you to the agent’s position. We have
domains for position sets, actions and so on. Fi-
nally, let VT be the set of variables of type T, and
</bodyText>
<table confidence="0.656345875">
PP/NP PP\(PP/NP)/N N
ax.ay.intersect(x, y) af.ag.ay.∃x.g(x, y) h f(x) ax.lamp(x)
&gt;
54
facing the lamp go until you reach a chair
AP/NP NP/N N S AP/S NP S\NP/NP NP/N N
ax.aa.pre(a, af.ιx.f(x) ax.lamp(x) aa.move(a) as.aa.post(a, s) you ax.ay.intersect(x, y) af.Ax.f(x) ax.chair(x)
front(you, x))
</table>
<figure confidence="0.44815">
&gt; &gt;
NP NP
ιx.lamp(x) Ax.chair(x)
&gt; &gt;
AP S\NP
aa.pre(a, front(you, ιx.lamp(x))) ay.intersect(Ax.chair(x), y)
S/S S
af.aa.f(a) h pre(a, front(you, ιx.lamp(x))) intersect(Ax.chair(x), you)
AP
aa.post(a, intersect(Ax.chair(x), you))
S\S
af.aa.f(a) h post(a, intersect(Ax.chair(x), you))
S
aa.move(a) h post(a, intersect(Ax.chair(x), you))
S
aa.move(a) h post(a, intersect(Ax.chair(x), you)) h pre(a, front(you, ιx.lamp(x)))
</figure>
<figureCaption confidence="0.99065">
Figure 4: A CCG parse showing adverbial phrases and topicalization.
</figureCaption>
<figure confidence="0.8403196">
&lt;
&gt;
&lt;
&gt;
AT : VT Us∈S DMs,T be the assignment func-
</figure>
<bodyText confidence="0.998059209302326">
tion, which maps variables to domain objects.
For each model Ms the domain DMs,ev is a set
of action sequences {(a1, ...,an) : n &gt; 11. Each a�
defines a sequences of states si, as defined in Sec-
tion 6.2, and associated models Msi. The key chal-
lenge for execution is that modifiers of the event will
need to be evaluated under different models from
this sequence. For example, consider the sentence
in Figure 4. To correctly execute, the pre literal, in-
troduced by the “facing” phrase, it must be evaluated
in the model Ms0 for the initial state s0. Similarly,
the literal including post requires the final model
Msn+1. Such state dependent predicates, including
pre and post, are called stateful. The list of stateful
predicates is pre-defined and includes event modi-
fiers, as well the ι quantifier, which is evaluated un-
der Ms0, since definite determiners are assumed to
name objects visible from the start position. In gen-
eral, a logical expression is traversed depth first and
the model is updated every time a stateful predicate
is reached. For example, the two e-type you con-
stants in Figure 4 will be evaluated under different
models: the one within the pre literal under Ms0,
and the one inside the post literal under Msn+1.
Evaluation Given a logical expression l, we can
compute the interpretation ZMs0,T(l) by recursively
mapping each subexpression to an entry on the ap-
propriate model M.
To reflect the changing state of the agent during
evaluation, we define the function update(d, pred).
Given an action sequence a� and a stateful predi-
cate pred, update returns a model Ms, where s
is the state under which the literal containing pred
should be interpreted, either the initial state or one
visited while executing d. For example, given the
predicate post and the action sequence (a1,... , an),
update((a1, ... , an), post) = Msn+1, where sn+1
the state of the agent following action an. By con-
vention, we place the event variable as the first argu-
ment in literals that include one.
Given a T-type logical expression l and a start-
ing state s0, we compute its interpretation ZMs0,T (l)
recursively, following these three base cases:
</bodyText>
<listItem confidence="0.854748611111111">
• If l is a λ operator of type (T1, T2) binding vari-
able v and body b, ZMs,T(l) is a set of pairs
from DT1 x DT2, where DT1, DT2 E Ms. For
each object o E DT1, we create a pair (o, i)
where i is the interpretation ZMs,T2(b) com-
puted under a variable assignment function ex-
tended to map AT2(v) = o.
• If l is a literal c(c1, ... , cn) with n argu-
ments where c has type P and each ci has
type Pi, ZMs,T(l) is computed by first in-
terpreting the predicate c to the function
f = ZMs,T(c). In most cases, ZMs,T(l) =
f(ZMs,P1(c1), ... , ZMs,Pn(cn)). However, if c
is a stateful predicate, such as pre or post, we
instead first retrieve the appropriate new model
Ms, = update(ZMs P1(c1), c), where c1 is the
event argument and ZMs,P1(c1) is its interpre-
tation. Then, the final results
</listItem>
<equation confidence="0.8076365">
\\is ZMs,T(l) =
f(ZMs ,P1(c1),...,ZMsl,Pn(cn))
</equation>
<listItem confidence="0.937217">
• If l is a T-type constant or variable, ZMs,T(l).
</listItem>
<bodyText confidence="0.999968">
The worst case complexity of the process is ex-
ponential in the number of bound variables. Al-
though in practice we observed tractable evaluation
in the majority of development cases we considered,
a more comprehensive and tractable evaluation pro-
cedure is an issue that we leave for future work.
</bodyText>
<page confidence="0.996913">
55
</page>
<bodyText confidence="0.999714740740741">
Implicit Actions Instructional language rarely
specifies every action required for execution, see
MacMahon (2007) for a detailed discussion in the
maps domain. For example, the sentence in Fig-
ure 4 can be said even if the agent is not facing a
blue hallway, with the clear implicit request that it
should turn to face such a hallway before moving.
To allow our agent to perform implicit actions, we
extend the domain of ev-type variables by allowing
the agent to prefix up to kI action sequences before
each explicit event. For example, in the agent’s po-
sition in Figure 2 (set C), the set of possible events
includes (MOVEI, MOVEI, RIGHTI, MOVE), which
contains two implicit sequences (marked by I).
Resolving Action Ambiguity Logical forms of-
ten fail to determine a unique action sequences,
due to instruction ambiguity. For example, con-
sider the instruction “go forward” and the agent state
as specified in Figure 2 (set C). The instruction,
which maps to λa.move(a) ∧ forward(a), evalu-
ates to the set containing (MOVE), (MOVE, MOVE)
and (MOVE, MOVE, MOVE), as well as five other se-
quences that have implicit prefixes followed by ex-
plicit MOVE actions. To resolve such ambiguity, we
prefer shorter actions without implicit actions. In
the example above, we will select (MOVE), which
includes a single action and no implicit actions.
</bodyText>
<subsectionHeader confidence="0.993122">
7.2 Joint Inference
</subsectionHeader>
<bodyText confidence="0.999978142857143">
We incorporate the execution procedure described
above with a linear weighted CCG parser, as de-
scribed in Section 4, to create a joint model of pars-
ing and execution. Specifically, we execute logi-
cal forms in the current state and observe the result
of their execution. For example, the word “chair”
can be used to refer to different types of objects, in-
cluding chairs, sofas, and barstools, in the maps do-
mains. Our CCG grammar would include a lexical
item for each meaning, but execution might fail de-
pending on the presence of objects in the world, in-
fluencing the final parse output. Similarly, allowing
implicit actions provides robustness when resolv-
ing these and other ambiguities. For example, an
instruction with the precondition phrase “from the
chair” might require additional actions to reach the
position with the named object.
To allow such joint reasoning we define an ex-
ecution e to include a parse tree ey and trace e
and define our feature function to be Φ(xi, si, e),
where xi is an instruction and si is the start state.
This approach allows joint dependencies: the state
of the world influences how the agent interprets
words, phrases and even complete sentences, while
language understanding determines actions.
Finally, to execute sequences of instructions, we
execute each starting from the end state of the previ-
ous one, using a beam of size ks.
</bodyText>
<sectionHeader confidence="0.981749" genericHeader="method">
8 Learning
</sectionHeader>
<bodyText confidence="0.936226028571428">
Figure 5 presents the complete learning algorithm.
Our approach is online, considering each example in
turn and performing two steps: expanding the lex-
icon and updating parameters. The algorithm as-
sumes access to a training set {(xi, si, Vi) : i =
1... n}, where each example includes an instruction
xi, starting state si and a validation function Vi, as
defined in Section 2. In addition the algorithm takes
a seed lexicon Ao. The output is a joint model, that
includes a lexicon A and parameters 0.
Coarse Lexical Generation To generate po-
tential lexical entries we use the function
GENLEX(x, s, V; A, 0), where x is an in-
struction, s is a state and V is a validation function.
A is the current lexicon and 0 is a parameter vector.
In GENLEX we use coarse logical constants,
as described below, to efficiently prune the set of
potential lexical entries. This set is then pruned
further using more precise inference in Step 1.
To compute GENLEX, we initially generate a
large set of lexical entries and then prune most of
them. The full set is generated by taking the cross
product of a set of templates, computed by factor-
ing out all templates in the seed lexicon Ao, and all
logical constants. For example, if Ao has a lexical
item with the category AP/NP : λx.λa.to(a, x) we
would create entries w �- AP/NP : λx.λa.p(a, x)
for every phrase w in x and all constants p with the
same type as to.5
In our development work, this approach often
generated nearly 100k entries per sentence. To ease
5Generalizing previous work (Kwiatkowski et al., 2011), we
allow templates that abstract subsets of the constants in a lex-
ical item. For example, the seed entry facing �- AP/NP :
λx.λa.pre(a, front(you, x)) would create 7 templates.
</bodyText>
<figure confidence="0.9336865">
a
,
</figure>
<page confidence="0.984538">
56
</page>
<bodyText confidence="0.974180625">
Inputs: Training set {(xi, si, Vi) : i = 1 ... n} where xi is a
sentence, si is a state and Vi is a validation function, as de-
scribed in Section 2. Initial lexicon A0. Number of iterations
T. Margin γ. Beam size k for lexicon generation.
Definitions: Let an execution e include a parse tree ey and
a trace e6. GEN(x, s; A) is the set of all possible execu-
tions for the instruction x and state s, given the lexicon A.
LEX(y) is the set of lexical entries used in the parse tree y.
Let 4)i(e) be shorthand for the feature function 4)(xi, si, e)
defined in Section 7.2. Define Δi(e, e0) = |4)i(e)−4)i(e0)|1.
GENLEX(x, s, V; λ, B) takes as input an instruction x,
state s, validation function V, lexicon λ and model param-
eters B, and returns a set of lexical entries, as defined in Sec-
tion 8. Finally, for a set of executions E let MAXVi(E; B)
be {e|∀e0 ∈ E, hB, 4)i(e0)i ≤ hB, 4)i(e)i ∧ Vi(e6) = 1}, the
set of highest scoring valid executions.
</bodyText>
<figure confidence="0.971994285714286">
Algorithm:
Initialize B using A0 , A ← A0
Fort= 1 ... T, i = 1 ... n :
Step 1: (Lexical generation)
a. Set λG ← GENLEX(xi, si, Vi; A, B), λ ← A ∪ λG
b. Let E be the k highest scoring executions from
GEN(xi, si; λ) which use at most one entry from λG
c. Select lexical entries from the highest scoring valid
parses: λi ← Ue∈MAXVi(E;θ) LEX(ey)
d. Update lexicon: A ← A ∪ λi
Step 2: (Update parameters)
a. Set Gi ← MAXVi(GEN(xi, si; A); B)
and Bi ← {e|e ∈ GEN(xi, si; A) ∧ Vi(e6) =6 1}
b. Construct sets of margin violating good and bad parses:
Ri ← {g|g ∈ Gi ∧
∃b ∈ Bi s.t. hB, 4)i(g) − 4)i(b)i &lt; γΔi(g, b)}
Ei ← {b|b ∈ Bi ∧
∃g ∈ Gi s.t. hB, 4)i(g) − 4)i(b)i &lt; γΔi(g, b)}
c. Apply the additive update:
B ← B + |R  |L�r∈Ri 4)i(r) Ei |Ee∈Ei 4)i(e)
Output: Parameters B and lexicon A
</figure>
<figureCaption confidence="0.999977">
Figure 5: The learning algorithm.
</figureCaption>
<bodyText confidence="0.998468583333333">
the cost of parsing at this scale, we developed a
coarse-to-fine two-pass parsing approach that lim-
its the number of new entries considered. The algo-
rithm first parses with coarse lexical entries that ab-
stract the identities of the logical constants in their
logical forms, thereby greatly reducing the search
space. It then uses the highest scoring coarse parses
to constrain the lexical entries for a final, fine parse.
Formally, we construct the coarse lexicon aa by
replacing all constants of the same type with a single
newly created, temporary constant. We then parse to
create a set of trees A, such that each y E A
</bodyText>
<listItem confidence="0.844380428571428">
1. is a parse for sentence x, given the world state
s with the combined lexicon A U aa,
2. scored higher than ey by at least a margin of
SL, where ey is the tree of e, the highest scoring
execution of x, at position s under the current
model, s.t. Xe�a) = 1,
3. contains at most one entry from aa.
</listItem>
<bodyText confidence="0.99281958974359">
Finally, from each entry l E 1l1l E aa ∧ l E
y ∧ y E Al, we create multiple lexical entries by
replacing all temporary constants with all possible
appropriately typed constants from the original set.
GENLEX returns all these lexical entries, which
will be used to form our final fine-level analysis.
Step 1: Lexical Induction To expand our model’s
lexicon, we use GENLEX to generate candidate
lexical entries and then further refine this set by pars-
ing with the current model. Step 1(a) in Figure 5
uses GENLEX to create a temporary set of po-
tential lexical entries aG. Steps (b-d) select a small
subset of these lexical entries to add to the current
lexicon A: we find the k-best executions under the
model, which use at most one entry from aG, find
the entries used in the best valid executions and add
them to the current lexicon.
Step 2: Parameter Update We use a variant of
a loss-driven perceptron (Singh-Miller &amp; Collins,
2007; Artzi &amp; Zettlemoyer, 2011) for parameter up-
dates. However, instead of taking advantage of a loss
function we use a validation signal. In step (a) we
collect the highest scoring valid parses and all in-
valid parses. Then, in step (b) we construct the set
Ri of valid analyses and Ei of invalid ones, such
that their model scores are not separated by a mar-
gin S scaled by the number of wrong features (Taskar
et al., 2003). Finally, step (f) applies the update.
Discussion The algorithm uses the validation sig-
nal to drive both lexical induction and parameter
updates. Unlike previous work (Zettlemoyer &amp;
Collins, 2005, 2007; Artzi &amp; Zettlemoyer, 2011),
we have no access to a set of logical constants,
either through the the labeled logical form or the
weak supervision signal, to guide the GENLEX
procedure. Therefore, to avoid over-generating lex-
ical entries, thereby making parsing and learning
intractable, we leverage typing for coarse parsing
to prune the generated set. By allowing a single
</bodyText>
<page confidence="0.995564">
57
</page>
<table confidence="0.99965125">
Oracle SAIL
# of instruction sequences 501 706
# of instruction sequences 431
with implicit actions
Total # of sentences 2679 3233
Avg. sentences per sequence 5.35 4.61
Avg. tokens per sentence 7.5 7.94
Vocabulary size 373 522
</table>
<tableCaption confidence="0.999889">
Table 1: Corpora statistics (lower-cased data).
</tableCaption>
<bodyText confidence="0.9999055">
new entry per parse, we create a conservative, cas-
cading effect, whereas a lexical entry that is intro-
duced opens the way for many other sentence to be
parsed and introduce new lexical entries. Further-
more, grounded features improve parse selection,
thereby generating higher quality lexical entries.
</bodyText>
<sectionHeader confidence="0.987641" genericHeader="method">
9 Experimental Setup
</sectionHeader>
<bodyText confidence="0.982472096774193">
Data For evaluation, we use the navigation task
from MacMahon et al. (2006), which includes three
environments and the SAIL corpus of instructions
and follower traces. Chen and Mooney (2011) seg-
mented the data, aligned traces to instructions, and
merged traces created by different subjects. The
corpus includes raw sentences, without any form of
linguistic annotation. The original collection pro-
cess (MacMahon et al., 2006) created many unin-
terpretable instructions and incorrect traces. To fo-
cus on the learning and interpretation tasks, we also
created a new dataset that includes only accurate in-
structions labeled with a single, correct execution
trace. From this oracle corpus, we randomly sam-
pled 164 instruction sequences (816 sentences) for
evaluation, leaving 337 (1863 sentences) for train-
ing. This simple effort will allow us to measure the
effects of noise on the learning approach and pro-
vides a resource for building more accurate algo-
rithms. Table 1 compares the two sets.
Features and Parser Following Zettlemoyer and
Collins (2005), we use a CKY parser with a beam
of k. To boost recall, we adopt a two-pass strategy,
which allows for word skipping if the initial parse
fails. We use features that indicate usage of lexical
entries, templates, lexemes and type-raising rules, as
described in Section 6.3, and repetitions in logical
coordinations. Finally, during joint parsing, we con-
sider only parses executable at si as complete.
Seed Lexicon To construct our seed lexicon we la-
beled 12 instruction sequences with 141 lexical en-
</bodyText>
<table confidence="0.999395888888889">
Single Sentence Sequence
Final state validation
Complete system 81.98 (2.33) 59.32 (6.66)
No implicit actions 77.7 (3.7) 38.46 (1.12)
No joint execution 73.27 (3.98) 31.51 (6.66)
Trace validation
Complete system 82.74 (2.53) 58.95 (6.88)
No implicit actions 77.64 (3.46) 38.34 (6.23)
No joint execution 72.85 (4.73) 30.89 (6.08)
</table>
<tableCaption confidence="0.7678965">
Table 2: Cross-validation development accuracy and
standard deviation on the oracle corpus.
</tableCaption>
<bodyText confidence="0.999706666666667">
tries. The sequences were randomly selected from
the training set, so as to include two sequences for
each participant in the original experiment. Fig-
ures 3 and 4 include a sample of our seed lexicon.
Initialization and Parameters We set the weight
of each template indicator feature to the number of
times it is used in the seed lexicon and each repeti-
tion feature to -10. Learning parameters were tuned
using cross-validation on the training set: the mar-
gin S is set to 1, the GENLEX margin SL is set to
2, we use 6 iterations (8 for experiments on SAIL)
and take the 250 top parses during lexical genera-
tion (step 1, Figure 5). For parameter update (step
2, Figure 5) we use a parser with a beam of 100.
GENLEX generates lexical entries for token se-
quences up to length 4. ks, the instruction sequence
execution beam, is set to 10. Finally, kI is set to
2, allowing up to two implicit action sequences per
explicit one.
Evaluation Metrics To evaluate single instruc-
tions x, we compare the agent’s end state to a labeled
state s&apos;, as described in Section 2. We use a similar
method to evaluate the execution of instruction se-
quences x, but disregard the orientation, since end
goals in MacMahon et al. (2006) are defined with-
out orientation. When evaluating logical forms we
measure exact match accuracy.
</bodyText>
<sectionHeader confidence="0.999869" genericHeader="evaluation">
10 Results
</sectionHeader>
<bodyText confidence="0.999963285714286">
We repeated each experiment five times, shuffling
the training set between runs. For the development
cross-validation runs, we also shuffled the folds. As
our learning approach is online, this allows us to ac-
count for performance variations arising from train-
ing set ordering. We report mean accuracy and stan-
dard deviation across all runs (and all folds).
</bodyText>
<page confidence="0.995807">
58
</page>
<table confidence="0.999150857142857">
Single Sentence Sequence
Chen and Mooney (2011) 54.4 16.18
Chen (2012) 57.28 19.18
+ additional data 57.62 20.64
Kim and Mooney (2012) 57.22 20.17
Trace validation 65.28 (5.09) 31.93 (3.26)
Final state validation 64.25 (5.12) 30.9 (2.16)
</table>
<tableCaption confidence="0.826911333333333">
Table 3: Cross-validation accuracy and standard de-
viation for the SAIL corpus.
Table 2 shows accuracy for 5-fold cross-
</tableCaption>
<bodyText confidence="0.998750918918919">
validation on the oracle training data. We first varied
the validation signal by providing the complete ac-
tion sequence or the final state only, as described in
Section 2. Although the final state signal is weaker,
the results are similar. The relatively large difference
between single sentence and sequence performance
is due to (1) cascading errors in the more difficult
task of sequential execution, and (2) corpus repe-
titions, where simple sentences are common (e.g.,
“turn left”). Next, we disabled the system’s ability
to introduce implicit actions, which was especially
harmful to the full sequence performance. Finally,
ablating the joint execution decreases performance,
showing the benefit of the joint model.
Table 3 lists cross validation results on the SAIL
corpus. To compare to previous work (Chen &amp;
Mooney, 2011), we report cross-validation results
over the three maps. The approach was able to cor-
rectly execute 60% more sequences then the previ-
ous state of the art (Kim &amp; Mooney, 2012). We
also outperform the results of Chen (2012), which
used 30% more training data.6 Using the weaker
validation signal creates a marginal decrease in per-
formance. However, we still outperform all previ-
ous work, despite using weaker supervision. Inter-
estingly, these increases were achieved with a rel-
atively simple executor, while previous work used
MARCO (MacMahon et al., 2006), which supports
sophisticated recovery strategies.
Finally, we evaluate our approach on the held out
test set for the oracle corpus (Table 4). In contrast
to experiments on the Chen and Mooney (2011) cor-
pus, we use a held out set for evaluation. Due to this
discrepancy, all development was done on the train-
ing set only. The increase in accuracy over learning
with the original corpus demonstrates the significant
impact of noise on our performance. In addition to
</bodyText>
<footnote confidence="0.839519">
6This additional training data isn’t publicly available.
</footnote>
<table confidence="0.999670333333333">
Validation Single Sentence Sequence LF
Final state 77.6 (1.14) 54.63 (3.5) 44 (6.12)
Trace 78.63 (0.84) 58.05 (3.12) 51.05 (1.14)
</table>
<tableCaption confidence="0.926395">
Table 4: Oracle corpus test accuracy and standard
deviation results.
</tableCaption>
<bodyText confidence="0.9993915">
execution results, we also report exact match logi-
cal form (LF) accuracy results. For this purpose, we
annotated 18 instruction sequences (105 sentences)
with logical forms. The gap between execution and
LF accuracy can be attributed to the complexity of
the linguistic representation and redundancy in in-
structions. These results provide a new baseline for
studying learning from cleaner supervision.
</bodyText>
<sectionHeader confidence="0.997052" genericHeader="discussions">
11 Discussion
</sectionHeader>
<bodyText confidence="0.999985090909091">
We showed how to do grounded learning of a CCG
semantic parser that includes a joint model of mean-
ing and context for executing natural language in-
structions. The joint nature allows situated cues
to directly influence parsing and also enables algo-
rithms that learn while executing instructions.
This style of algorithm, especially when using the
weaker end state validation, is closely related to re-
inforcement learning approaches (Branavan et al.,
2009, 2010). However, we differ on optimization
and objective function, where we aim for minimal
loss. We expect many RL techniques to be useful
to scale to more complex environments, including
sampling actions and using an exploration strategy.
We also designed a semantic representation to
closely match the linguistic structure of instructional
language, combining ideas from many semantic
theories, including, for example, Neo-Davidsonian
events (Parsons, 1990). This approach allowed us to
learn a compact and executable grammar that gen-
eralized well. We expect, in future work, that such
modeling can be reused for more general language.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99995075">
The research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), and the NSF (IIS-1115966).
The authors thank Tom Kwiatkowski, Nicholas
FitzGerald and Alan Ritter for helpful discussions,
David Chen for providing the evaluation corpus, and
the anonymous reviewers for helpful comments.
</bodyText>
<page confidence="0.998514">
59
</page>
<sectionHeader confidence="0.990205" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999699463917526">
Artzi, Y., &amp; Zettlemoyer, L. (2011). Bootstrapping Se-
mantic Parsers from Conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Barwise, J., &amp; Cooper, R. (1981). Generalized Quanti-
fiers and Natural Language. Linguistics and Phi-
losophy, 4(2), 159–219.
Branavan, S., Chen, H., Zettlemoyer, L., &amp; Barzilay, R.
(2009). Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint
Conference of the Association for Computational
Linguistics and the International Joint Conference
on Natural Language Processing.
Branavan, S., Zettlemoyer, L., &amp; Barzilay, R. (2010).
Reading between the lines: learning to map high-
level instructions to commands. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics.
Bugmann, G., Klein, E., Lauria, S., &amp; Kyriacou, T.
(2004). Corpus-based robotics: A route instruc-
tion example. In Proceedings of Intelligent Au-
tonomous Systems.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Chen, D. L. (2012). Fast Online Lexicon Learning for
Grounded Language Acquisition. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics.
Chen, D., Kim, J., &amp; Mooney, R. (2010). Training a mul-
tilingual sportscaster: using perceptual context to
learn language. Journal of Artificial Intelligence
Research, 37(1), 397–436.
Chen, D., &amp; Mooney, R. (2011). Learning to Interpret
Natural Language Navigation Instructions from
Observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S., &amp; Curran, J. (2007). Wide-coverage efficient
statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493–552.
Clarke, J., Goldwasser, D., Chang, M., &amp; Roth, D.
(2010). Driving Semantic Parsing from the
World’s Response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Collins, M. (2004). Parameter estimation for statis-
tical parsing models: Theory and practice of
distribution-free methods. In New Developments
in Parsing Technology.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, 105–148.
Di Eugenio, B., &amp; White, M. (1992). On the Interpre-
tation of Natural Language Instructions. In Pro-
ceedings of the Conference of the Association of
Computational Linguistics.
Dzifcak, J., Scheutz, M., Baral, C., &amp; Schermerhorn, P.
(2009). What to Do and How to Do It: Trans-
lating Natural Language Directives Into Temporal
and Dynamic Logic Representation for Goal Man-
agement and Action Execution. In Proceedings
of the IEEE International Conference on Robotics
and Automation.
Goldwasser, D., Reichart, R., Clarke, J., &amp; Roth, D.
(2011). Confidence Driven Unsupervised Seman-
tic Parsing. In Proceedings of the Association of
Computational Linguistics.
Goldwasser, D., &amp; Roth, D. (2011). Learning from Nat-
ural Instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
Heim, I., &amp; Kratzer, A. (1998). Semantics in Generative
Grammar. Blackwell Oxford.
Kate, R., &amp; Mooney, R. (2006). Using String-Kernels for
Learning Semantic Parsers. In Proceedings of the
Conference of the Association for Computational
Linguistics.
Kim, J., &amp; Mooney, R. J. (2012). Unsupervised PCFG
Induction for Grounded Language Learning with
Highly Ambiguous Supervision. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing.
Kollar, T., Tellex, S., Roy, D., &amp; Roy, N. (2010). Toward
Understanding Natural Language Directions. In
Proceedings of the ACM/IEEE International Con-
ference on Human-Robot Interaction.
Krishnamurthy, J., &amp; Mitchell, T. (2012). Weakly Super-
vised Training of Semantic Parsers. In Proceed-
ings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning.
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., &amp;
Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter of
the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &amp;
Steedman, M. (2010). Inducing probabilistic CCG
grammars from logical form with higher-order
</reference>
<page confidence="0.96686">
60
</page>
<reference confidence="0.999908447916667">
unification. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &amp;
Steedman, M. (2011). Lexical Generalization in
CCG Grammar Induction for Semantic Parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Con-
ditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Pro-
ceedings of the International Conference on Ma-
chine Learning.
Lewis, D. (1970). General Semantics. Synthese, 22(1),
18–67.
Liang, P., Jordan, M., &amp; Klein, D. (2009). Learning se-
mantic correspondences with less supervision. In
Proceedings of the Joint Conference of the Asso-
ciation for Computational Linguistics the Interna-
tional Joint Conference on Natural Language Pro-
cessing.
Liang, P., Jordan, M., &amp; Klein, D. (2011). Learning
Dependency-Based Compositional Semantics. In
Proceedings of the Conference of the Association
for Computational Linguistics.
MacMahon, M. (2007). Following Natural Language
Route Instructions. Ph.D. thesis, University of
Texas at Austin.
MacMahon, M., Stankiewics, B., &amp; Kuipers, B. (2006).
Walk the Talk: Connecting Language, Knowl-
edge, Action in Route Instructions. In Proceed-
ings of the National Conference on Artificial Intel-
ligence.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo, L., &amp;
Fox, D. (2012). A Joint Model of Language and
Perception for Grounded Attribute Learning. Pro-
ceedings of the International Conference on Ma-
chine Learning.
Matuszek, C., Fox, D., &amp; Koscher, K. (2010). Follow-
ing directions using statistical machine translation.
In Proceedings of the international conference on
Human-robot interaction.
Matuszek, C., Herbst, E., Zettlemoyer, L. S., &amp; Fox, D.
(2012). Learning to Parse Natural Language Com-
mands to a Robot Control System. In Proceedings
of the International Symposium on Experimental
Robotics.
Montague, R. (1973). The Proper Treatment of Quantifi-
cation in Ordinary English. Approaches to natural
language, 49, 221–242.
Muresan, S. (2011). Learning for Deep Language Under-
standing. In Proceedings of the International Joint
Conference on Artificial Intelligence.
Parsons, T. (1990). Events in the Semantics of English.
The MIT Press.
Singh-Miller, N., &amp; Collins, M. (2007). Trigger-based
language modeling using a loss-sensitive percep-
tron algorithm. In IEEE International Conference
on Acoustics, Speech and Signal Processing.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Steedman, M. (2011). Taking Scope. The MIT Press.
Taskar, B., Guestrin, C., &amp; Koller, D. (2003). Max-
Margin Markov Networks. In Proceedings of
the Conference on Neural Information Processing
Systems.
Taskar, B., Klein, D., Collins, M., Koller, D., &amp; Manning,
C. (2004). Max-Margin Parsing. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee,
A., Teller, S., &amp; Roy, N. (2011). Understanding
Natural Language Commands for Robotic Naviga-
tion and Mobile Manipulation. In Proceedings of
the National Conference on Artificial Intelligence.
Vogel, A., &amp; Jurafsky, D. (2010). Learning to follow nav-
igational directions. In Proceedings of the Con-
ference of the Association for Computational Lin-
guistics.
Webber, B., Badler, N., Di Eugenio, B., Geib, C., Lev-
ison, L., &amp; Moore, M. (1995). Instructions, In-
tentions and Expectations. Artificial Intelligence,
73(1), 253–269.
Wei, Y., Brunskill, E., Kollar, T., &amp; Roy, N. (2009).
Where To Go: Interpreting Natural Directions
Using Global Inference. In Proceedings of the
IEEE International Conference on Robotics and
Automation.
Winograd, T. (1972). Understanding Natural Language.
Cognitive Psychology, 3(1), 1–191.
Wong, Y., &amp; Mooney, R. (2007). Learning Synchronous
Grammars for Semantic Parsing with Lambda Cal-
culus. In Proceedings of the Conference of the As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.983409">
61
</page>
<reference confidence="0.996417363636364">
Zettlemoyer, L., &amp; Collins, M. (2005). Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Ar-
tificial Intelligence.
Zettlemoyer, L., &amp; Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical
form. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing.
</reference>
<page confidence="0.999167">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916423">
<title confidence="0.9975085">Weakly Supervised Learning of Semantic for Mapping Instructions to Actions</title>
<author confidence="0.961007">Artzi</author>
<affiliation confidence="0.9997875">Computer Science &amp; University of</affiliation>
<address confidence="0.996339">Seattle, WA</address>
<abstract confidence="0.997659368421053">The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Bootstrapping Semantic Parsers from Conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9597" citStr="Artzi and Zettlemoyer (2011)" startWordPosition="1551" endWordPosition="1554">ntion, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al.</context>
<context position="3729" citStr="Artzi &amp; Zettlemoyer, 2011" startWordPosition="566" endWordPosition="569">stic constructs that are common in spatial and instructional language. To learn from indirect supervision, we define the notion of a validation function, for example that tests the state of the agent after interpreting an instruction. We then show how this function can be used to drive online learning. For 49 Transactions of the Association for Computational Linguistics, 1 (2013) 49–62. Action Editor: Jason Eisner. Submitted 11/2012; Published 3/2013. c�2013 Association for Computational Linguistics. that purpose, we adapt the loss-sensitive Perceptron algorithm (Singh-Miller &amp; Collins, 2007; Artzi &amp; Zettlemoyer, 2011) to use a validation function and coarse-to-fine inference for lexical induction. The joint nature of this approach provides crucial benefits in that it allows situated cues, such as the set of visible objects, to directly influence parsing and learning. It also enables the model to be learned while executing instructions, for example by trying to replicate actions taken by humans. In particular, we show that, given only a small seed lexicon and a task-specific executor, we can induce high quality models for interpreting complex instructions. We evaluate the method on a benchmark navigational </context>
<context position="37089" citStr="Artzi &amp; Zettlemoyer, 2011" startWordPosition="6284" endWordPosition="6287"> our model’s lexicon, we use GENLEX to generate candidate lexical entries and then further refine this set by parsing with the current model. Step 1(a) in Figure 5 uses GENLEX to create a temporary set of potential lexical entries aG. Steps (b-d) select a small subset of these lexical entries to add to the current lexicon A: we find the k-best executions under the model, which use at most one entry from aG, find the entries used in the best valid executions and add them to the current lexicon. Step 2: Parameter Update We use a variant of a loss-driven perceptron (Singh-Miller &amp; Collins, 2007; Artzi &amp; Zettlemoyer, 2011) for parameter updates. However, instead of taking advantage of a loss function we use a validation signal. In step (a) we collect the highest scoring valid parses and all invalid parses. Then, in step (b) we construct the set Ri of valid analyses and Ei of invalid ones, such that their model scores are not separated by a margin S scaled by the number of wrong features (Taskar et al., 2003). Finally, step (f) applies the update. Discussion The algorithm uses the validation signal to drive both lexical induction and parameter updates. Unlike previous work (Zettlemoyer &amp; Collins, 2005, 2007; Art</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Artzi, Y., &amp; Zettlemoyer, L. (2011). Bootstrapping Semantic Parsers from Conversations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>R Cooper</author>
</authors>
<title>Generalized Quantifiers and Natural Language.</title>
<date>1981</date>
<journal>Linguistics and Philosophy,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>159--219</pages>
<contexts>
<context position="17668" citStr="Barwise &amp; Cooper, 1981" startWordPosition="2948" endWordPosition="2951">ere multiple chairs are visible. We adopt a simple heuristic approach that ranks referents based on a combination of their distance from the agent and whether they are in front of it. For our example, from position C our agent would pick the chair E in front of it as the denotation. The approach differs from previous, non-grounded models that fail to name objects when faced with such ambiguity (e.g., Carpenter (1997), Heim and Kratzer (1998)). To model the meaning of indefinite articles, we depart from the Frege-Montague tradition of using existential quantifiers (Lewis, 1970; Montague, 1973; Barwise &amp; Cooper, 1981), and instead introduce a new quantifier A that, like ι, has type ((e, t), e). For example, the phrase “a chair” would be paired with Ax.chair(x) which denotes an arbitrary entry from the set of chairs in the world. Computing the denotation for such expressions in a world will require picking a specific object, without further restrictions. This approach is closely related to Steedman’s generalized Skolem terms (2011).2 Meta Entities We use m-typed terms to represent non-physical entities, such as numbers (1, 2, etc.) and directions (left, right, etc.) whose denotations 1Although quantifiers a</context>
</contexts>
<marker>Barwise, Cooper, 1981</marker>
<rawString>Barwise, J., &amp; Cooper, R. (1981). Generalized Quantifiers and Natural Language. Linguistics and Philosophy, 4(2), 159–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>H Chen</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="7782" citStr="Branavan et al. (2009)" startWordPosition="1283" endWordPosition="1286">h functions provide highly ambiguous supervision. Evaluation We evaluate task completion for single instructions on a test set {(xi, si, si) : i = 1... n}, where si is the final state of an oracle agent following the execution of xi starting at state si. We will also report accuracies for correctly interpreting instruction sequences x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel &amp; Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the </context>
<context position="45640" citStr="Branavan et al., 2009" startWordPosition="7666" endWordPosition="7669">ty of the linguistic representation and redundancy in instructions. These results provide a new baseline for studying learning from cleaner supervision. 11 Discussion We showed how to do grounded learning of a CCG semantic parser that includes a joint model of meaning and context for executing natural language instructions. The joint nature allows situated cues to directly influence parsing and also enables algorithms that learn while executing instructions. This style of algorithm, especially when using the weaker end state validation, is closely related to reinforcement learning approaches (Branavan et al., 2009, 2010). However, we differ on optimization and objective function, where we aim for minimal loss. We expect many RL techniques to be useful to scale to more complex environments, including sampling actions and using an exploration strategy. We also designed a semantic representation to closely match the linguistic structure of instructional language, combining ideas from many semantic theories, including, for example, Neo-Davidsonian events (Parsons, 1990). This approach allowed us to learn a compact and executable grammar that generalized well. We expect, in future work, that such modeling c</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>Branavan, S., Chen, H., Zettlemoyer, L., &amp; Barzilay, R. (2009). Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reading between the lines: learning to map highlevel instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7917" citStr="Branavan et al. (2010)" startWordPosition="1304" endWordPosition="1307">, si) : i = 1... n}, where si is the final state of an oracle agent following the execution of xi starting at state si. We will also report accuracies for correctly interpreting instruction sequences x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel &amp; Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>Branavan, S., Zettlemoyer, L., &amp; Barzilay, R. (2010). Reading between the lines: learning to map highlevel instructions to commands. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bugmann</author>
<author>E Klein</author>
<author>S Lauria</author>
<author>T Kyriacou</author>
</authors>
<title>Corpus-based robotics: A route instruction example.</title>
<date>2004</date>
<booktitle>In Proceedings of Intelligent Autonomous Systems.</booktitle>
<contexts>
<context position="10204" citStr="Bugmann et al. (2004)" startWordPosition="1643" endWordPosition="1646">lemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with categories. For example, the lexical entry chair �- N : Ax.chair(x) for the word “chair” in the parse in Figure 4 pairs it with a category that has syntactic type N</context>
</contexts>
<marker>Bugmann, Klein, Lauria, Kyriacou, 2004</marker>
<rawString>Bugmann, G., Klein, E., Lauria, S., &amp; Kyriacou, T. (2004). Corpus-based robotics: A route instruction example. In Proceedings of Intelligent Autonomous Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>Type-Logical Semantics.</title>
<date>1997</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="15318" citStr="Carpenter (1997)" startWordPosition="2532" endWordPosition="2533">osition. 6 Modeling Instructional Language We aim to design a semantic representation that is learnable, models grounded phenomena such as spaJ D E l (a) chair l f Ax.chair(x) (b) hall Ax.hall(x) E (c) the chair ιx.chair(x) C (d) you you (e) blue hall Ax.hall(x) ∧ blue(x) (f) chair in the intersection Ax.chair(x) ∧ intersect(ιy. junction(y), x) J A B E I (g) in front of you l Ax.in front of(you, x) Figure 2: Schematic diagram of a map environment and example of semantics of spatial phrases. tial relations and object reference, and is executable. Our semantic representation combines ideas from Carpenter (1997) and Neo-Davidsonian event semantics (Parsons, 1990) in a simply typed λ- calculus. There are four basic types: (1) entities e that are objects in the world, (2) events ev that specify actions in the world, (3) truth values t, and (4) meta-entities m, such as numbers or directions. We also allow functional types, which are defined by input and output types. For example, (e, t) is the type of function from entities to truth values. 6.1 Spatial Language Modeling Nouns and Noun Phrases Noun phrases are paired with e-type constants that name specific entities and nouns are mapped to (e, t)-type ex</context>
<context position="17465" citStr="Carpenter (1997)" startWordPosition="2920" endWordPosition="2921">air” in Figure 2c will be represented by ιx.chair(x) which will denote the appropriate chair. However, computing this denotation is challenging when there is perceptual ambiguity, for positions where multiple chairs are visible. We adopt a simple heuristic approach that ranks referents based on a combination of their distance from the agent and whether they are in front of it. For our example, from position C our agent would pick the chair E in front of it as the denotation. The approach differs from previous, non-grounded models that fail to name objects when faced with such ambiguity (e.g., Carpenter (1997), Heim and Kratzer (1998)). To model the meaning of indefinite articles, we depart from the Frege-Montague tradition of using existential quantifiers (Lewis, 1970; Montague, 1973; Barwise &amp; Cooper, 1981), and instead introduce a new quantifier A that, like ι, has type ((e, t), e). For example, the phrase “a chair” would be paired with Ax.chair(x) which denotes an arbitrary entry from the set of chairs in the world. Computing the denotation for such expressions in a world will require picking a specific object, without further restrictions. This approach is closely related to Steedman’s general</context>
<context position="22139" citStr="Carpenter (1997)" startWordPosition="3670" endWordPosition="3671">sofa(y)) Each adverbial phrase provides a constraint, and changing their order will not change the LF. 6.3 Parsing Instructional Language with CCG To compose logical expressions from sentences we use CCG, as described in Section 4. Figures 3 and 4 present a sample of lexical entries and how they are combined, as we will describe in this section. The basic syntactic categories are N (noun), NP (noun phrase), S (sentence), PP (prepositional phrase), AP (adverbial phrase), ADJ (adjective) and C (a special category for coordinators). Type Raising To compactly model syntactic variations, we follow Carpenter (1997), who argues for polymorphic typing. We include the more simple, or lower type, entry in the lexicon and introduce typeraising rules to reconstruct the other when necessary at parse time. We use four rules: PP : g N\N : Af.Ax.f(x) n g(x) ADJ : g N/N : Af.Ax.f(x) n g(x) AP : g S\S : Af.Aa.f(a) n g(a) AP : g S/S : Af.Aa.f(a) n g(a) where the first three are for prepositional, adjectival and adverbial modifications, and the fourth models the fact that adverbials are often topicalized.4 Figures 3 and 4 show parses that use type-raising rules. Indefinites As discussed in Section 6.1, we use a new s</context>
</contexts>
<marker>Carpenter, 1997</marker>
<rawString>Carpenter, B. (1997). Type-Logical Semantics. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
</authors>
<title>Fast Online Lexicon Learning for Grounded Language Acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8652" citStr="Chen (2012)" startWordPosition="1412" endWordPosition="1413"> (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we descr</context>
<context position="42377" citStr="Chen (2012)" startWordPosition="7155" endWordPosition="7156"> but disregard the orientation, since end goals in MacMahon et al. (2006) are defined without orientation. When evaluating logical forms we measure exact match accuracy. 10 Results We repeated each experiment five times, shuffling the training set between runs. For the development cross-validation runs, we also shuffled the folds. As our learning approach is online, this allows us to account for performance variations arising from training set ordering. We report mean accuracy and standard deviation across all runs (and all folds). 58 Single Sentence Sequence Chen and Mooney (2011) 54.4 16.18 Chen (2012) 57.28 19.18 + additional data 57.62 20.64 Kim and Mooney (2012) 57.22 20.17 Trace validation 65.28 (5.09) 31.93 (3.26) Final state validation 64.25 (5.12) 30.9 (2.16) Table 3: Cross-validation accuracy and standard deviation for the SAIL corpus. Table 2 shows accuracy for 5-fold crossvalidation on the oracle training data. We first varied the validation signal by providing the complete action sequence or the final state only, as described in Section 2. Although the final state signal is weaker, the results are similar. The relatively large difference between single sentence and sequence perfo</context>
<context position="43714" citStr="Chen (2012)" startWordPosition="7368" endWordPosition="7369">mple sentences are common (e.g., “turn left”). Next, we disabled the system’s ability to introduce implicit actions, which was especially harmful to the full sequence performance. Finally, ablating the joint execution decreases performance, showing the benefit of the joint model. Table 3 lists cross validation results on the SAIL corpus. To compare to previous work (Chen &amp; Mooney, 2011), we report cross-validation results over the three maps. The approach was able to correctly execute 60% more sequences then the previous state of the art (Kim &amp; Mooney, 2012). We also outperform the results of Chen (2012), which used 30% more training data.6 Using the weaker validation signal creates a marginal decrease in performance. However, we still outperform all previous work, despite using weaker supervision. Interestingly, these increases were achieved with a relatively simple executor, while previous work used MARCO (MacMahon et al., 2006), which supports sophisticated recovery strategies. Finally, we evaluate our approach on the held out test set for the oracle corpus (Table 4). In contrast to experiments on the Chen and Mooney (2011) corpus, we use a held out set for evaluation. Due to this discrepa</context>
</contexts>
<marker>Chen, 2012</marker>
<rawString>Chen, D. L. (2012). Fast Online Lexicon Learning for Grounded Language Acquisition. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>J Kim</author>
<author>R Mooney</author>
</authors>
<title>Training a multilingual sportscaster: using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>397--436</pages>
<contexts>
<context position="9934" citStr="Chen et al., 2010" startWordPosition="1603" endWordPosition="1606">sion were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is de</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>Chen, D., Kim, J., &amp; Mooney, R. (2010). Training a multilingual sportscaster: using perceptual context to learn language. Journal of Artificial Intelligence Research, 37(1), 397–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>R Mooney</author>
</authors>
<title>Learning to Interpret Natural Language Navigation Instructions from Observations.</title>
<date>2011</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8639" citStr="Chen and Mooney (2011)" startWordPosition="1408" endWordPosition="1411">n our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011)</context>
<context position="38849" citStr="Chen and Mooney (2011)" startWordPosition="6572" endWordPosition="6575">.61 Avg. tokens per sentence 7.5 7.94 Vocabulary size 373 522 Table 1: Corpora statistics (lower-cased data). new entry per parse, we create a conservative, cascading effect, whereas a lexical entry that is introduced opens the way for many other sentence to be parsed and introduce new lexical entries. Furthermore, grounded features improve parse selection, thereby generating higher quality lexical entries. 9 Experimental Setup Data For evaluation, we use the navigation task from MacMahon et al. (2006), which includes three environments and the SAIL corpus of instructions and follower traces. Chen and Mooney (2011) segmented the data, aligned traces to instructions, and merged traces created by different subjects. The corpus includes raw sentences, without any form of linguistic annotation. The original collection process (MacMahon et al., 2006) created many uninterpretable instructions and incorrect traces. To focus on the learning and interpretation tasks, we also created a new dataset that includes only accurate instructions labeled with a single, correct execution trace. From this oracle corpus, we randomly sampled 164 instruction sequences (816 sentences) for evaluation, leaving 337 (1863 sentences</context>
<context position="42354" citStr="Chen and Mooney (2011)" startWordPosition="7149" endWordPosition="7152">cution of instruction sequences x, but disregard the orientation, since end goals in MacMahon et al. (2006) are defined without orientation. When evaluating logical forms we measure exact match accuracy. 10 Results We repeated each experiment five times, shuffling the training set between runs. For the development cross-validation runs, we also shuffled the folds. As our learning approach is online, this allows us to account for performance variations arising from training set ordering. We report mean accuracy and standard deviation across all runs (and all folds). 58 Single Sentence Sequence Chen and Mooney (2011) 54.4 16.18 Chen (2012) 57.28 19.18 + additional data 57.62 20.64 Kim and Mooney (2012) 57.22 20.17 Trace validation 65.28 (5.09) 31.93 (3.26) Final state validation 64.25 (5.12) 30.9 (2.16) Table 3: Cross-validation accuracy and standard deviation for the SAIL corpus. Table 2 shows accuracy for 5-fold crossvalidation on the oracle training data. We first varied the validation signal by providing the complete action sequence or the final state only, as described in Section 2. Although the final state signal is weaker, the results are similar. The relatively large difference between single sent</context>
<context position="44247" citStr="Chen and Mooney (2011)" startWordPosition="7449" endWordPosition="7452">ious state of the art (Kim &amp; Mooney, 2012). We also outperform the results of Chen (2012), which used 30% more training data.6 Using the weaker validation signal creates a marginal decrease in performance. However, we still outperform all previous work, despite using weaker supervision. Interestingly, these increases were achieved with a relatively simple executor, while previous work used MARCO (MacMahon et al., 2006), which supports sophisticated recovery strategies. Finally, we evaluate our approach on the held out test set for the oracle corpus (Table 4). In contrast to experiments on the Chen and Mooney (2011) corpus, we use a held out set for evaluation. Due to this discrepancy, all development was done on the training set only. The increase in accuracy over learning with the original corpus demonstrates the significant impact of noise on our performance. In addition to 6This additional training data isn’t publicly available. Validation Single Sentence Sequence LF Final state 77.6 (1.14) 54.63 (3.5) 44 (6.12) Trace 78.63 (0.84) 58.05 (3.12) 51.05 (1.14) Table 4: Oracle corpus test accuracy and standard deviation results. execution results, we also report exact match logical form (LF) accuracy resu</context>
<context position="4394" citStr="Chen &amp; Mooney, 2011" startWordPosition="670" endWordPosition="673">ine inference for lexical induction. The joint nature of this approach provides crucial benefits in that it allows situated cues, such as the set of visible objects, to directly influence parsing and learning. It also enables the model to be learned while executing instructions, for example by trying to replicate actions taken by humans. In particular, we show that, given only a small seed lexicon and a task-specific executor, we can induce high quality models for interpreting complex instructions. We evaluate the method on a benchmark navigational instructions dataset (MacMahon et al., 2006; Chen &amp; Mooney, 2011). Our joint approach successfully completes 60% more instruction sets relative to the previous state of the art. We also report experiments that vary supervision type, finding that observing the final position of an instruction execution is nearly as informative as observing the entire path. Finally, we present improved results on a new version of the MacMahon et al. (2006) corpus, which we filtered to include only executable instructions paired with correct traces. 2 Technical Overview Task Let S be the set of possible environment states and A be the set of possible actions. Given a start sta</context>
<context position="43492" citStr="Chen &amp; Mooney, 2011" startWordPosition="7327" endWordPosition="7330">aker, the results are similar. The relatively large difference between single sentence and sequence performance is due to (1) cascading errors in the more difficult task of sequential execution, and (2) corpus repetitions, where simple sentences are common (e.g., “turn left”). Next, we disabled the system’s ability to introduce implicit actions, which was especially harmful to the full sequence performance. Finally, ablating the joint execution decreases performance, showing the benefit of the joint model. Table 3 lists cross validation results on the SAIL corpus. To compare to previous work (Chen &amp; Mooney, 2011), we report cross-validation results over the three maps. The approach was able to correctly execute 60% more sequences then the previous state of the art (Kim &amp; Mooney, 2012). We also outperform the results of Chen (2012), which used 30% more training data.6 Using the weaker validation signal creates a marginal decrease in performance. However, we still outperform all previous work, despite using weaker supervision. Interestingly, these increases were achieved with a relatively simple executor, while previous work used MARCO (MacMahon et al., 2006), which supports sophisticated recovery strat</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>Chen, D., &amp; Mooney, R. (2011). Learning to Interpret Natural Language Navigation Instructions from Observations. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>493--552</pages>
<contexts>
<context position="11884" citStr="Clark &amp; Curran, 2007" startWordPosition="1920" endWordPosition="1923">owski et al. (2011) introduced a factored CCG lexicon representation. Each lexical item is composed of a lexeme and a template. For example, the entry chair �- N : Ax.chair(x) would be constructed by combining the lexeme chair �- [chair], which contains a word paired with logical constants, with the template Av.[N : Ax.v(x)], that defines the rest of the category by abstracting over logical constants. This approach allows the reuse of common syntactic structures through a small set of templates. Section 8 describes how we learn such lexical entries. Weighted Linear CCGs A weighted linear CCG (Clark &amp; Curran, 2007) ranks the space of possible parses under the grammar, and is closely related to several other approaches (Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the A-calculus expression at the root of y*(x). Section 7.2 describes how we efficiently compute an appr</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Clark, S., &amp; Curran, J. (2007). Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4), 493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving Semantic Parsing from the World’s Response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="9358" citStr="Clarke et al. (2010)" startWordPosition="1518" endWordPosition="1521"> by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired wit</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>Clarke, J., Goldwasser, D., Chang, M., &amp; Roth, D. (2010). Driving Semantic Parsing from the World’s Response. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2004</date>
<booktitle>In New Developments in Parsing Technology.</booktitle>
<contexts>
<context position="12027" citStr="Collins, 2004" startWordPosition="1945" endWordPosition="1946">chair �- N : Ax.chair(x) would be constructed by combining the lexeme chair �- [chair], which contains a word paired with logical constants, with the template Av.[N : Ax.v(x)], that defines the rest of the category by abstracting over logical constants. This approach allows the reuse of common syntactic structures through a small set of templates. Section 8 describes how we learn such lexical entries. Weighted Linear CCGs A weighted linear CCG (Clark &amp; Curran, 2007) ranks the space of possible parses under the grammar, and is closely related to several other approaches (Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the A-calculus expression at the root of y*(x). Section 7.2 describes how we efficiently compute an approximation to y*(x) within the joint interpretation and execution model. 0 · O(x, y) 51 Supervised learning with GENLEX Previous work (Zettlemoy</context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Collins, M. (2004). Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In New Developments in Parsing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidson</author>
</authors>
<title>The logical form of action sentences.</title>
<date>1967</date>
<booktitle>Essays on actions and events,</booktitle>
<pages>105--148</pages>
<contexts>
<context position="19954" citStr="Davidson, 1967" startWordPosition="3310" endWordPosition="3311">sect. We will describe the full details of how these expressions are constructed in Section 6.3. Spatial Relations The semantic representationallows more complex reasoning over position sets and the relations between them. For example, the binary relation in front of (Figure 2g) tests if the first argument is in front of the second from the point of view of the agent. Additional relations are used to model set intersection, relative direction, relative distance, and relative position by distance. 6.2 Modeling Instructions To model actions in the world, we adopt NeoDavidsonian event semantics (Davidson, 1967; Parsons, 1990), which treats events as ev-type primitive objects. Such an approach allows for a compact lexicon where adverbial modifiers introduce predicates, which are linked by a shared event argument. Instructional language is characterized by heavy usage of imperatives, which we model as functions from events to truth values.3 For example, an imperative such as “move” would have the meaning λa.move(a), which defines a set of events that match the specified constraints. Here, this set would include all events that involve moving actions. The denotation of ev-type terms is a sequence of n</context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Davidson, D. (1967). The logical form of action sentences. Essays on actions and events, 105–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
<author>M White</author>
</authors>
<title>On the Interpretation of Natural Language Instructions.</title>
<date>1992</date>
<booktitle>In Proceedings of the Conference of the Association of Computational Linguistics.</booktitle>
<marker>Di Eugenio, White, 1992</marker>
<rawString>Di Eugenio, B., &amp; White, M. (1992). On the Interpretation of Natural Language Instructions. In Proceedings of the Conference of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dzifcak</author>
<author>M Scheutz</author>
<author>C Baral</author>
<author>P Schermerhorn</author>
</authors>
<title>What to Do and How to Do It: Translating Natural Language Directives Into Temporal and Dynamic Logic Representation for Goal Management and Action Execution.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE International Conference on Robotics and Automation.</booktitle>
<contexts>
<context position="10254" citStr="Dzifcak et al. (2009)" startWordPosition="1652" endWordPosition="1655">ional logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with categories. For example, the lexical entry chair �- N : Ax.chair(x) for the word “chair” in the parse in Figure 4 pairs it with a category that has syntactic type N and meaning Ax.chair(x). Figure 4 shows how a CCG</context>
</contexts>
<marker>Dzifcak, Scheutz, Baral, Schermerhorn, 2009</marker>
<rawString>Dzifcak, J., Scheutz, M., Baral, C., &amp; Schermerhorn, P. (2009). What to Do and How to Do It: Translating Natural Language Directives Into Temporal and Dynamic Logic Representation for Goal Management and Action Execution. In Proceedings of the IEEE International Conference on Robotics and Automation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>R Reichart</author>
<author>J Clarke</author>
<author>D Roth</author>
</authors>
<title>Confidence Driven Unsupervised Semantic Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="9672" citStr="Goldwasser et al. (2011)" startWordPosition="1562" endWordPosition="1566">Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Goldwasser, D., Reichart, R., Clarke, J., &amp; Roth, D. (2011). Confidence Driven Unsupervised Semantic Parsing. In Proceedings of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Learning from Natural Instructions.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="9386" citStr="Goldwasser and Roth (2011)" startWordPosition="1522" endWordPosition="1525">sed semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matusze</context>
</contexts>
<marker>Goldwasser, Roth, 2011</marker>
<rawString>Goldwasser, D., &amp; Roth, D. (2011). Learning from Natural Instructions. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Heim</author>
<author>A Kratzer</author>
</authors>
<title>Semantics in Generative Grammar.</title>
<date>1998</date>
<publisher>Blackwell</publisher>
<location>Oxford.</location>
<contexts>
<context position="17490" citStr="Heim and Kratzer (1998)" startWordPosition="2922" endWordPosition="2925">will be represented by ιx.chair(x) which will denote the appropriate chair. However, computing this denotation is challenging when there is perceptual ambiguity, for positions where multiple chairs are visible. We adopt a simple heuristic approach that ranks referents based on a combination of their distance from the agent and whether they are in front of it. For our example, from position C our agent would pick the chair E in front of it as the denotation. The approach differs from previous, non-grounded models that fail to name objects when faced with such ambiguity (e.g., Carpenter (1997), Heim and Kratzer (1998)). To model the meaning of indefinite articles, we depart from the Frege-Montague tradition of using existential quantifiers (Lewis, 1970; Montague, 1973; Barwise &amp; Cooper, 1981), and instead introduce a new quantifier A that, like ι, has type ((e, t), e). For example, the phrase “a chair” would be paired with Ax.chair(x) which denotes an arbitrary entry from the set of chairs in the world. Computing the denotation for such expressions in a world will require picking a specific object, without further restrictions. This approach is closely related to Steedman’s generalized Skolem terms (2011).</context>
</contexts>
<marker>Heim, Kratzer, 1998</marker>
<rawString>Heim, I., &amp; Kratzer, A. (1998). Semantics in Generative Grammar. Blackwell Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kate</author>
<author>R Mooney</author>
</authors>
<title>Using String-Kernels for Learning Semantic Parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9003" citStr="Kate and Mooney (2006)" startWordPosition="1461" endWordPosition="1464"> navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use w</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Kate, R., &amp; Mooney, R. (2006). Using String-Kernels for Learning Semantic Parsers. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>R J Mooney</author>
</authors>
<title>Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8678" citStr="Kim and Mooney (2012)" startWordPosition="1415" endWordPosition="1418">ar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently</context>
<context position="42441" citStr="Kim and Mooney (2012)" startWordPosition="7164" endWordPosition="7167">Mahon et al. (2006) are defined without orientation. When evaluating logical forms we measure exact match accuracy. 10 Results We repeated each experiment five times, shuffling the training set between runs. For the development cross-validation runs, we also shuffled the folds. As our learning approach is online, this allows us to account for performance variations arising from training set ordering. We report mean accuracy and standard deviation across all runs (and all folds). 58 Single Sentence Sequence Chen and Mooney (2011) 54.4 16.18 Chen (2012) 57.28 19.18 + additional data 57.62 20.64 Kim and Mooney (2012) 57.22 20.17 Trace validation 65.28 (5.09) 31.93 (3.26) Final state validation 64.25 (5.12) 30.9 (2.16) Table 3: Cross-validation accuracy and standard deviation for the SAIL corpus. Table 2 shows accuracy for 5-fold crossvalidation on the oracle training data. We first varied the validation signal by providing the complete action sequence or the final state only, as described in Section 2. Although the final state signal is weaker, the results are similar. The relatively large difference between single sentence and sequence performance is due to (1) cascading errors in the more difficult task</context>
<context position="43667" citStr="Kim &amp; Mooney, 2012" startWordPosition="7358" endWordPosition="7361">uential execution, and (2) corpus repetitions, where simple sentences are common (e.g., “turn left”). Next, we disabled the system’s ability to introduce implicit actions, which was especially harmful to the full sequence performance. Finally, ablating the joint execution decreases performance, showing the benefit of the joint model. Table 3 lists cross validation results on the SAIL corpus. To compare to previous work (Chen &amp; Mooney, 2011), we report cross-validation results over the three maps. The approach was able to correctly execute 60% more sequences then the previous state of the art (Kim &amp; Mooney, 2012). We also outperform the results of Chen (2012), which used 30% more training data.6 Using the weaker validation signal creates a marginal decrease in performance. However, we still outperform all previous work, despite using weaker supervision. Interestingly, these increases were achieved with a relatively simple executor, while previous work used MARCO (MacMahon et al., 2006), which supports sophisticated recovery strategies. Finally, we evaluate our approach on the held out test set for the oracle corpus (Table 4). In contrast to experiments on the Chen and Mooney (2011) corpus, we use a he</context>
</contexts>
<marker>Kim, Mooney, 2012</marker>
<rawString>Kim, J., &amp; Mooney, R. J. (2012). Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kollar</author>
<author>S Tellex</author>
<author>D Roy</author>
<author>N Roy</author>
</authors>
<title>Toward Understanding Natural Language Directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction.</booktitle>
<contexts>
<context position="8073" citStr="Kollar et al. (2010)" startWordPosition="1331" endWordPosition="1334">rectly interpreting instruction sequences x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel &amp; Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (</context>
</contexts>
<marker>Kollar, Tellex, Roy, Roy, 2010</marker>
<rawString>Kollar, T., Tellex, S., Roy, D., &amp; Roy, N. (2010). Toward Understanding Natural Language Directions. In Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krishnamurthy</author>
<author>T Mitchell</author>
</authors>
<title>Weakly Supervised Training of Semantic Parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="9532" citStr="Krishnamurthy and Mitchell (2012)" startWordPosition="1542" endWordPosition="1546">pproaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), </context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Krishnamurthy, J., &amp; Mitchell, T. (2012). Weakly Supervised Training of Semantic Parsers. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>S Goldwater</author>
<author>L Zettlemoyer</author>
<author>M Steedman</author>
</authors>
<title>A probabilistic model of syntactic and semantic acquisition from childdirected utterances and their meanings.</title>
<date>2012</date>
<booktitle>Proceedings of the Conference of the European Chapter of the Association of Computational Linguistics.</booktitle>
<marker>Kwiatkowski, Goldwater, Zettlemoyer, Steedman, 2012</marker>
<rawString>Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., &amp; Steedman, M. (2012). A probabilistic model of syntactic and semantic acquisition from childdirected utterances and their meanings. Proceedings of the Conference of the European Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9072" citStr="Kwiatkowski et al. (2010" startWordPosition="1472" endWordPosition="1475">epresentations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011)</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &amp; Steedman, M. (2010). Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Lexical Generalization in CCG Grammar Induction for Semantic Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9239" citStr="Kwiatkowski et al., 2011" startWordPosition="1498" endWordPosition="1501">s. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic ana</context>
<context position="11282" citStr="Kwiatkowski et al. (2011)" startWordPosition="1822" endWordPosition="1825">xample, the lexical entry chair �- N : Ax.chair(x) for the word “chair” in the parse in Figure 4 pairs it with a category that has syntactic type N and meaning Ax.chair(x). Figure 4 shows how a CCG parse builds a logical form for a complete sentence in our example navigation domain. Starting from lexical entries, each intermediate parse node, including syntax and semantics, is constructed with one of a small set of CCG combinators (Steedman, 1996, 2000). We use the application, composition and coordination combinators, and three others described in Section 6.3. Factored CCG Lexicons Recently, Kwiatkowski et al. (2011) introduced a factored CCG lexicon representation. Each lexical item is composed of a lexeme and a template. For example, the entry chair �- N : Ax.chair(x) would be constructed by combining the lexeme chair �- [chair], which contains a word paired with logical constants, with the template Av.[N : Ax.v(x)], that defines the rest of the category by abstracting over logical constants. This approach allows the reuse of common syntactic structures through a small set of templates. Section 8 describes how we learn such lexical entries. Weighted Linear CCGs A weighted linear CCG (Clark &amp; Curran, 200</context>
<context position="33267" citStr="Kwiatkowski et al., 2011" startWordPosition="5554" endWordPosition="5557"> 1. To compute GENLEX, we initially generate a large set of lexical entries and then prune most of them. The full set is generated by taking the cross product of a set of templates, computed by factoring out all templates in the seed lexicon Ao, and all logical constants. For example, if Ao has a lexical item with the category AP/NP : λx.λa.to(a, x) we would create entries w �- AP/NP : λx.λa.p(a, x) for every phrase w in x and all constants p with the same type as to.5 In our development work, this approach often generated nearly 100k entries per sentence. To ease 5Generalizing previous work (Kwiatkowski et al., 2011), we allow templates that abstract subsets of the constants in a lexical item. For example, the seed entry facing �- AP/NP : λx.λa.pre(a, front(you, x)) would create 7 templates. a , 56 Inputs: Training set {(xi, si, Vi) : i = 1 ... n} where xi is a sentence, si is a state and Vi is a validation function, as described in Section 2. Initial lexicon A0. Number of iterations T. Margin γ. Beam size k for lexicon generation. Definitions: Let an execution e include a parse tree ey and a trace e6. GEN(x, s; A) is the set of all possible executions for the instruction x and state s, given the lexicon </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &amp; Steedman, M. (2011). Lexical Generalization in CCG Grammar Induction for Semantic Parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="12012" citStr="Lafferty et al., 2001" startWordPosition="1941" endWordPosition="1944">For example, the entry chair �- N : Ax.chair(x) would be constructed by combining the lexeme chair �- [chair], which contains a word paired with logical constants, with the template Av.[N : Ax.v(x)], that defines the rest of the category by abstracting over logical constants. This approach allows the reuse of common syntactic structures through a small set of templates. Section 8 describes how we learn such lexical entries. Weighted Linear CCGs A weighted linear CCG (Clark &amp; Curran, 2007) ranks the space of possible parses under the grammar, and is closely related to several other approaches (Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the A-calculus expression at the root of y*(x). Section 7.2 describes how we efficiently compute an approximation to y*(x) within the joint interpretation and execution model. 0 · O(x, y) 51 Supervised learning with GENLEX Previous </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>General Semantics.</title>
<date>1970</date>
<journal>Synthese,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>18--67</pages>
<contexts>
<context position="17627" citStr="Lewis, 1970" startWordPosition="2944" endWordPosition="2945">l ambiguity, for positions where multiple chairs are visible. We adopt a simple heuristic approach that ranks referents based on a combination of their distance from the agent and whether they are in front of it. For our example, from position C our agent would pick the chair E in front of it as the denotation. The approach differs from previous, non-grounded models that fail to name objects when faced with such ambiguity (e.g., Carpenter (1997), Heim and Kratzer (1998)). To model the meaning of indefinite articles, we depart from the Frege-Montague tradition of using existential quantifiers (Lewis, 1970; Montague, 1973; Barwise &amp; Cooper, 1981), and instead introduce a new quantifier A that, like ι, has type ((e, t), e). For example, the phrase “a chair” would be paired with Ax.chair(x) which denotes an arbitrary entry from the set of chairs in the world. Computing the denotation for such expressions in a world will require picking a specific object, without further restrictions. This approach is closely related to Steedman’s generalized Skolem terms (2011).2 Meta Entities We use m-typed terms to represent non-physical entities, such as numbers (1, 2, etc.) and directions (left, right, etc.) </context>
</contexts>
<marker>Lewis, 1970</marker>
<rawString>Lewis, D. (1970). General Semantics. Synthese, 22(1), 18–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="9914" citStr="Liang et al., 2009" startWordPosition="1599" endWordPosition="1602">ive forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Liang, P., Jordan, M., &amp; Klein, D. (2009). Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the Association for Computational Linguistics the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning Dependency-Based Compositional Semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9410" citStr="Liang et al. (2011)" startWordPosition="1527" endWordPosition="1530">tically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Liang, P., Jordan, M., &amp; Klein, D. (2011). Learning Dependency-Based Compositional Semantics. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M MacMahon</author>
</authors>
<title>Following Natural Language Route Instructions.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Texas at Austin.</institution>
<contexts>
<context position="29093" citStr="MacMahon (2007)" startWordPosition="4836" endWordPosition="4837"> P1(c1), c), where c1 is the event argument and ZMs,P1(c1) is its interpretation. Then, the final results \\is ZMs,T(l) = f(ZMs ,P1(c1),...,ZMsl,Pn(cn)) • If l is a T-type constant or variable, ZMs,T(l). The worst case complexity of the process is exponential in the number of bound variables. Although in practice we observed tractable evaluation in the majority of development cases we considered, a more comprehensive and tractable evaluation procedure is an issue that we leave for future work. 55 Implicit Actions Instructional language rarely specifies every action required for execution, see MacMahon (2007) for a detailed discussion in the maps domain. For example, the sentence in Figure 4 can be said even if the agent is not facing a blue hallway, with the clear implicit request that it should turn to face such a hallway before moving. To allow our agent to perform implicit actions, we extend the domain of ev-type variables by allowing the agent to prefix up to kI action sequences before each explicit event. For example, in the agent’s position in Figure 2 (set C), the set of possible events includes (MOVEI, MOVEI, RIGHTI, MOVE), which contains two implicit sequences (marked by I). Resolving Ac</context>
</contexts>
<marker>MacMahon, 2007</marker>
<rawString>MacMahon, M. (2007). Following Natural Language Route Instructions. Ph.D. thesis, University of Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M MacMahon</author>
<author>B Stankiewics</author>
<author>B Kuipers</author>
</authors>
<title>Walk the Talk: Connecting Language, Knowledge, Action in Route Instructions.</title>
<date>2006</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="4372" citStr="MacMahon et al., 2006" startWordPosition="666" endWordPosition="669">unction and coarse-to-fine inference for lexical induction. The joint nature of this approach provides crucial benefits in that it allows situated cues, such as the set of visible objects, to directly influence parsing and learning. It also enables the model to be learned while executing instructions, for example by trying to replicate actions taken by humans. In particular, we show that, given only a small seed lexicon and a task-specific executor, we can induce high quality models for interpreting complex instructions. We evaluate the method on a benchmark navigational instructions dataset (MacMahon et al., 2006; Chen &amp; Mooney, 2011). Our joint approach successfully completes 60% more instruction sets relative to the previous state of the art. We also report experiments that vary supervision type, finding that observing the final position of an instruction execution is nearly as informative as observing the entire path. Finally, we present improved results on a new version of the MacMahon et al. (2006) corpus, which we filtered to include only executable instructions paired with correct traces. 2 Technical Overview Task Let S be the set of possible environment states and A be the set of possible acti</context>
<context position="10228" citStr="MacMahon et al. (2006)" startWordPosition="1647" endWordPosition="1650"> supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with categories. For example, the lexical entry chair �- N : Ax.chair(x) for the word “chair” in the parse in Figure 4 pairs it with a category that has syntactic type N and meaning Ax.chair(x)</context>
<context position="38734" citStr="MacMahon et al. (2006)" startWordPosition="6555" endWordPosition="6558">f instruction sequences 431 with implicit actions Total # of sentences 2679 3233 Avg. sentences per sequence 5.35 4.61 Avg. tokens per sentence 7.5 7.94 Vocabulary size 373 522 Table 1: Corpora statistics (lower-cased data). new entry per parse, we create a conservative, cascading effect, whereas a lexical entry that is introduced opens the way for many other sentence to be parsed and introduce new lexical entries. Furthermore, grounded features improve parse selection, thereby generating higher quality lexical entries. 9 Experimental Setup Data For evaluation, we use the navigation task from MacMahon et al. (2006), which includes three environments and the SAIL corpus of instructions and follower traces. Chen and Mooney (2011) segmented the data, aligned traces to instructions, and merged traces created by different subjects. The corpus includes raw sentences, without any form of linguistic annotation. The original collection process (MacMahon et al., 2006) created many uninterpretable instructions and incorrect traces. To focus on the learning and interpretation tasks, we also created a new dataset that includes only accurate instructions labeled with a single, correct execution trace. From this oracl</context>
<context position="41839" citStr="MacMahon et al. (2006)" startWordPosition="7068" endWordPosition="7071">ing lexical generation (step 1, Figure 5). For parameter update (step 2, Figure 5) we use a parser with a beam of 100. GENLEX generates lexical entries for token sequences up to length 4. ks, the instruction sequence execution beam, is set to 10. Finally, kI is set to 2, allowing up to two implicit action sequences per explicit one. Evaluation Metrics To evaluate single instructions x, we compare the agent’s end state to a labeled state s&apos;, as described in Section 2. We use a similar method to evaluate the execution of instruction sequences x, but disregard the orientation, since end goals in MacMahon et al. (2006) are defined without orientation. When evaluating logical forms we measure exact match accuracy. 10 Results We repeated each experiment five times, shuffling the training set between runs. For the development cross-validation runs, we also shuffled the folds. As our learning approach is online, this allows us to account for performance variations arising from training set ordering. We report mean accuracy and standard deviation across all runs (and all folds). 58 Single Sentence Sequence Chen and Mooney (2011) 54.4 16.18 Chen (2012) 57.28 19.18 + additional data 57.62 20.64 Kim and Mooney (201</context>
<context position="44047" citStr="MacMahon et al., 2006" startWordPosition="7417" endWordPosition="7420"> the SAIL corpus. To compare to previous work (Chen &amp; Mooney, 2011), we report cross-validation results over the three maps. The approach was able to correctly execute 60% more sequences then the previous state of the art (Kim &amp; Mooney, 2012). We also outperform the results of Chen (2012), which used 30% more training data.6 Using the weaker validation signal creates a marginal decrease in performance. However, we still outperform all previous work, despite using weaker supervision. Interestingly, these increases were achieved with a relatively simple executor, while previous work used MARCO (MacMahon et al., 2006), which supports sophisticated recovery strategies. Finally, we evaluate our approach on the held out test set for the oracle corpus (Table 4). In contrast to experiments on the Chen and Mooney (2011) corpus, we use a held out set for evaluation. Due to this discrepancy, all development was done on the training set only. The increase in accuracy over learning with the original corpus demonstrates the significant impact of noise on our performance. In addition to 6This additional training data isn’t publicly available. Validation Single Sentence Sequence LF Final state 77.6 (1.14) 54.63 (3.5) 4</context>
</contexts>
<marker>MacMahon, Stankiewics, Kuipers, 2006</marker>
<rawString>MacMahon, M., Stankiewics, B., &amp; Kuipers, B. (2006). Walk the Talk: Connecting Language, Knowledge, Action in Route Instructions. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>N FitzGerald</author>
<author>L Zettlemoyer</author>
<author>L Bo</author>
<author>D Fox</author>
</authors>
<title>A Joint Model of Language and Perception for Grounded Attribute Learning.</title>
<date>2012</date>
<booktitle>Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="10001" citStr="Matuszek et al., 2012" startWordPosition="1613" endWordPosition="1616"> (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains e</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo, L., &amp; Fox, D. (2012). A Joint Model of Language and Perception for Grounded Attribute Learning. Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>D Fox</author>
<author>K Koscher</author>
</authors>
<title>Following directions using statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the international conference on Human-robot interaction.</booktitle>
<contexts>
<context position="8521" citStr="Matuszek et al. (2010" startWordPosition="1392" endWordPosition="1395">van et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper </context>
</contexts>
<marker>Matuszek, Fox, Koscher, 2010</marker>
<rawString>Matuszek, C., Fox, D., &amp; Koscher, K. (2010). Following directions using statistical machine translation. In Proceedings of the international conference on Human-robot interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>E Herbst</author>
<author>L S Zettlemoyer</author>
<author>D Fox</author>
</authors>
<title>Learning to Parse Natural Language Commands to a Robot Control System.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Symposium on Experimental Robotics.</booktitle>
<contexts>
<context position="10001" citStr="Matuszek et al., 2012" startWordPosition="1613" endWordPosition="1616"> (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains e</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2012</marker>
<rawString>Matuszek, C., Herbst, E., Zettlemoyer, L. S., &amp; Fox, D. (2012). Learning to Parse Natural Language Commands to a Robot Control System. In Proceedings of the International Symposium on Experimental Robotics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>The Proper Treatment of Quantification in Ordinary English. Approaches to natural language,</title>
<date>1973</date>
<volume>49</volume>
<pages>221--242</pages>
<contexts>
<context position="17643" citStr="Montague, 1973" startWordPosition="2946" endWordPosition="2947">for positions where multiple chairs are visible. We adopt a simple heuristic approach that ranks referents based on a combination of their distance from the agent and whether they are in front of it. For our example, from position C our agent would pick the chair E in front of it as the denotation. The approach differs from previous, non-grounded models that fail to name objects when faced with such ambiguity (e.g., Carpenter (1997), Heim and Kratzer (1998)). To model the meaning of indefinite articles, we depart from the Frege-Montague tradition of using existential quantifiers (Lewis, 1970; Montague, 1973; Barwise &amp; Cooper, 1981), and instead introduce a new quantifier A that, like ι, has type ((e, t), e). For example, the phrase “a chair” would be paired with Ax.chair(x) which denotes an arbitrary entry from the set of chairs in the world. Computing the denotation for such expressions in a world will require picking a specific object, without further restrictions. This approach is closely related to Steedman’s generalized Skolem terms (2011).2 Meta Entities We use m-typed terms to represent non-physical entities, such as numbers (1, 2, etc.) and directions (left, right, etc.) whose denotation</context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Montague, R. (1973). The Proper Treatment of Quantification in Ordinary English. Approaches to natural language, 49, 221–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muresan</author>
</authors>
<title>Learning for Deep Language Understanding.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="9043" citStr="Muresan (2011)" startWordPosition="1469" endWordPosition="1470">xpressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs</context>
</contexts>
<marker>Muresan, 2011</marker>
<rawString>Muresan, S. (2011). Learning for Deep Language Understanding. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Parsons</author>
</authors>
<title>Events in the Semantics of English.</title>
<date>1990</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="15370" citStr="Parsons, 1990" startWordPosition="2539" endWordPosition="2540">esign a semantic representation that is learnable, models grounded phenomena such as spaJ D E l (a) chair l f Ax.chair(x) (b) hall Ax.hall(x) E (c) the chair ιx.chair(x) C (d) you you (e) blue hall Ax.hall(x) ∧ blue(x) (f) chair in the intersection Ax.chair(x) ∧ intersect(ιy. junction(y), x) J A B E I (g) in front of you l Ax.in front of(you, x) Figure 2: Schematic diagram of a map environment and example of semantics of spatial phrases. tial relations and object reference, and is executable. Our semantic representation combines ideas from Carpenter (1997) and Neo-Davidsonian event semantics (Parsons, 1990) in a simply typed λ- calculus. There are four basic types: (1) entities e that are objects in the world, (2) events ev that specify actions in the world, (3) truth values t, and (4) meta-entities m, such as numbers or directions. We also allow functional types, which are defined by input and output types. For example, (e, t) is the type of function from entities to truth values. 6.1 Spatial Language Modeling Nouns and Noun Phrases Noun phrases are paired with e-type constants that name specific entities and nouns are mapped to (e, t)-type expressions that define a property. For example, the n</context>
<context position="19970" citStr="Parsons, 1990" startWordPosition="3312" endWordPosition="3314">scribe the full details of how these expressions are constructed in Section 6.3. Spatial Relations The semantic representationallows more complex reasoning over position sets and the relations between them. For example, the binary relation in front of (Figure 2g) tests if the first argument is in front of the second from the point of view of the agent. Additional relations are used to model set intersection, relative direction, relative distance, and relative position by distance. 6.2 Modeling Instructions To model actions in the world, we adopt NeoDavidsonian event semantics (Davidson, 1967; Parsons, 1990), which treats events as ev-type primitive objects. Such an approach allows for a compact lexicon where adverbial modifiers introduce predicates, which are linked by a shared event argument. Instructional language is characterized by heavy usage of imperatives, which we model as functions from events to truth values.3 For example, an imperative such as “move” would have the meaning λa.move(a), which defines a set of events that match the specified constraints. Here, this set would include all events that involve moving actions. The denotation of ev-type terms is a sequence of n instances of th</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>Parsons, T. (1990). Events in the Semantics of English. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Singh-Miller</author>
<author>M Collins</author>
</authors>
<title>Trigger-based language modeling using a loss-sensitive perceptron algorithm.</title>
<date>2007</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="3701" citStr="Singh-Miller &amp; Collins, 2007" startWordPosition="562" endWordPosition="565">present a number of key linguistic constructs that are common in spatial and instructional language. To learn from indirect supervision, we define the notion of a validation function, for example that tests the state of the agent after interpreting an instruction. We then show how this function can be used to drive online learning. For 49 Transactions of the Association for Computational Linguistics, 1 (2013) 49–62. Action Editor: Jason Eisner. Submitted 11/2012; Published 3/2013. c�2013 Association for Computational Linguistics. that purpose, we adapt the loss-sensitive Perceptron algorithm (Singh-Miller &amp; Collins, 2007; Artzi &amp; Zettlemoyer, 2011) to use a validation function and coarse-to-fine inference for lexical induction. The joint nature of this approach provides crucial benefits in that it allows situated cues, such as the set of visible objects, to directly influence parsing and learning. It also enables the model to be learned while executing instructions, for example by trying to replicate actions taken by humans. In particular, we show that, given only a small seed lexicon and a task-specific executor, we can induce high quality models for interpreting complex instructions. We evaluate the method </context>
<context position="37061" citStr="Singh-Miller &amp; Collins, 2007" startWordPosition="6280" endWordPosition="6283">1: Lexical Induction To expand our model’s lexicon, we use GENLEX to generate candidate lexical entries and then further refine this set by parsing with the current model. Step 1(a) in Figure 5 uses GENLEX to create a temporary set of potential lexical entries aG. Steps (b-d) select a small subset of these lexical entries to add to the current lexicon A: we find the k-best executions under the model, which use at most one entry from aG, find the entries used in the best valid executions and add them to the current lexicon. Step 2: Parameter Update We use a variant of a loss-driven perceptron (Singh-Miller &amp; Collins, 2007; Artzi &amp; Zettlemoyer, 2011) for parameter updates. However, instead of taking advantage of a loss function we use a validation signal. In step (a) we collect the highest scoring valid parses and all invalid parses. Then, in step (b) we construct the set Ri of valid analyses and Ei of invalid ones, such that their model scores are not separated by a margin S scaled by the number of wrong features (Taskar et al., 2003). Finally, step (f) applies the update. Discussion The algorithm uses the validation signal to drive both lexical induction and parameter updates. Unlike previous work (Zettlemoye</context>
</contexts>
<marker>Singh-Miller, Collins, 2007</marker>
<rawString>Singh-Miller, N., &amp; Collins, M. (2007). Trigger-based language modeling using a loss-sensitive perceptron algorithm. In IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="10514" citStr="Steedman, 1996" startWordPosition="1693" endWordPosition="1694">g et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with categories. For example, the lexical entry chair �- N : Ax.chair(x) for the word “chair” in the parse in Figure 4 pairs it with a category that has syntactic type N and meaning Ax.chair(x). Figure 4 shows how a CCG parse builds a logical form for a complete sentence in our example navigation domain. Starting from lexical entries, each intermediate parse node, including syntax and semantics, is constructed with one of a small set of CCG combinators (Steedman, 1996, 2000)</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Steedman, M. (1996). Surface Structure and Interpretation. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<marker>Steedman, 2000</marker>
<rawString>Steedman, M. (2000). The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Taking Scope.</title>
<date>2011</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="18484" citStr="Steedman (2011)" startWordPosition="3083" endWordPosition="3084"> in the world. Computing the denotation for such expressions in a world will require picking a specific object, without further restrictions. This approach is closely related to Steedman’s generalized Skolem terms (2011).2 Meta Entities We use m-typed terms to represent non-physical entities, such as numbers (1, 2, etc.) and directions (left, right, etc.) whose denotations 1Although quantifiers are logical constants with type ((e, t), e) or ((e, t), t), we use a notation similar to that used for first-order logic. For example, the notation ιx.f(x) represents the logical expression ι(Ax.f(x)) 2Steedman (2011) uses generalized Skolem terms as a tool for resolving anaphoric pronouns, which we do not model. are fixed. The ability to refer to directions allows us to manipulate position sets. For example, the phrase “your left” is mapped to the logical expression orient(you, left), which denotes the position set containing the position to the left of the agent. Prepositions and Adjectives Noun phrases with modifiers, such as adjectives and prepositional phrases are (e, t)-type expressions that implement set intersection with logical conjunctions. For example in Figure 2, the phrase “blue hall” is paire</context>
<context position="23295" citStr="Steedman (2011)" startWordPosition="3860" endWordPosition="3861">es. Indefinites As discussed in Section 6.1, we use a new syntactic analysis for indefinites, follow4Using type-raising rules can be particularly useful when learning from sparse data. For example, it will no longer be necessary to learn three lexical entries for each adverbial phrase (with syntax AP, S\S, and S/S). chair in the corner N PP/NP NP/N N ax.chair(x) ax.ay.intersect(x, y) af.Ax.f(x) ax.corner(x) NP ιx.corner(x) PP ay.intersect(ιx.corner(x), y) N\N af.ay.f(y) h intersect(ιx.chair(x), y) N ay.chair(y) h intersect(ιx.chair(x), y) Figure 3: A CCG parse with a prepositional phrase. ing Steedman (2011). Previous approaches would build parses such as with a lamp PP\(PP/NP) ag.ay.∃x.g(x, y) h lamp(x) PP ay.∃x.intersect(x, y) h lamp(x) where “a” has the relatively complex syntactic category PP\(PP/NP)/N and where similar entries would be needed to quantify over different types of verbs (e.g., S\(S/NP)/N) and adverbials (e.g., AP\(AP/NP)/N). Instead, we include a single lexical entry a �- NP/N : Af.Ax.f(x) which can be used to construct the correct meaning in all cases. 7 Joint Parsing and Execution Our inference includes an execution component and a parser. The parser maps sentences to logical</context>
</contexts>
<marker>Steedman, 2011</marker>
<rawString>Steedman, M. (2011). Taking Scope. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>MaxMargin Markov Networks.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="37482" citStr="Taskar et al., 2003" startWordPosition="6358" endWordPosition="6361">ry from aG, find the entries used in the best valid executions and add them to the current lexicon. Step 2: Parameter Update We use a variant of a loss-driven perceptron (Singh-Miller &amp; Collins, 2007; Artzi &amp; Zettlemoyer, 2011) for parameter updates. However, instead of taking advantage of a loss function we use a validation signal. In step (a) we collect the highest scoring valid parses and all invalid parses. Then, in step (b) we construct the set Ri of valid analyses and Ei of invalid ones, such that their model scores are not separated by a margin S scaled by the number of wrong features (Taskar et al., 2003). Finally, step (f) applies the update. Discussion The algorithm uses the validation signal to drive both lexical induction and parameter updates. Unlike previous work (Zettlemoyer &amp; Collins, 2005, 2007; Artzi &amp; Zettlemoyer, 2011), we have no access to a set of logical constants, either through the the labeled logical form or the weak supervision signal, to guide the GENLEX procedure. Therefore, to avoid over-generating lexical entries, thereby making parsing and learning intractable, we leverage typing for coarse parsing to prune the generated set. By allowing a single 57 Oracle SAIL # of ins</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Taskar, B., Guestrin, C., &amp; Koller, D. (2003). MaxMargin Markov Networks. In Proceedings of the Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-Margin Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="12049" citStr="Taskar et al., 2004" startWordPosition="1947" endWordPosition="1950">.chair(x) would be constructed by combining the lexeme chair �- [chair], which contains a word paired with logical constants, with the template Av.[N : Ax.v(x)], that defines the rest of the category by abstracting over logical constants. This approach allows the reuse of common syntactic structures through a small set of templates. Section 8 describes how we learn such lexical entries. Weighted Linear CCGs A weighted linear CCG (Clark &amp; Curran, 2007) ranks the space of possible parses under the grammar, and is closely related to several other approaches (Lafferty et al., 2001; Collins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the A-calculus expression at the root of y*(x). Section 7.2 describes how we efficiently compute an approximation to y*(x) within the joint interpretation and execution model. 0 · O(x, y) 51 Supervised learning with GENLEX Previous work (Zettlemoyer &amp; Collins, 2005) in</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Taskar, B., Klein, D., Collins, M., Koller, D., &amp; Manning, C. (2004). Max-Margin Parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M Walter</author>
<author>A Banerjee</author>
<author>S Teller</author>
<author>N Roy</author>
</authors>
<title>Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation.</title>
<date>2011</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., &amp; Roy, N. (2011). Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vogel</author>
<author>D Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7828" citStr="Vogel &amp; Jurafsky, 2010" startWordPosition="1290" endWordPosition="1293">ion. Evaluation We evaluate task completion for single instructions on a test set {(xi, si, si) : i = 1... n}, where si is the final state of an oracle agent following the execution of xi starting at state si. We will also report accuracies for correctly interpreting instruction sequences x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel &amp; Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted </context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Vogel, A., &amp; Jurafsky, D. (2010). Learning to follow navigational directions. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
<author>N Badler</author>
<author>B Di Eugenio</author>
<author>C Geib</author>
<author>L Levison</author>
<author>M Moore</author>
</authors>
<title>Instructions, Intentions and Expectations.</title>
<date>1995</date>
<journal>Artificial Intelligence,</journal>
<volume>73</volume>
<issue>1</issue>
<pages>253--269</pages>
<marker>Webber, Badler, Di Eugenio, Geib, Levison, Moore, 1995</marker>
<rawString>Webber, B., Badler, N., Di Eugenio, B., Geib, C., Levison, L., &amp; Moore, M. (1995). Instructions, Intentions and Expectations. Artificial Intelligence, 73(1), 253–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wei</author>
<author>E Brunskill</author>
<author>T Kollar</author>
<author>N Roy</author>
</authors>
<title>Where To Go: Interpreting Natural Directions Using Global Inference.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE International Conference on Robotics and Automation.</booktitle>
<contexts>
<context position="8048" citStr="Wei et al. (2009)" startWordPosition="1326" endWordPosition="1329">ort accuracies for correctly interpreting instruction sequences x, where a single error can cause the entire sequence to fail. Finally, we report accuracy on recovering correct logical forms zi on a manually annotated subset of the test set. 3 Related Work Our learning is inspired by the reinforcement learning (RL) approach of Branavan et al. (2009), and related methods (Vogel &amp; Jurafsky, 2010), but uses latent variable model updates within a semantic parser. Branavan et al. (2010) extended their RL approach to model high-level instructions, which correspond to implicit actions in our domain. Wei et al. (2009) and Kollar et al. (2010) used shallow linguistic representations for instructions. Recently, Tellex 50 et al. (2011) used a graphical model semantics representation to learn from instructions paired with demonstrations. In contrast, we model significantly more complex linguistic phenomena than these approaches, as required for the navigation domain. Other research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2</context>
</contexts>
<marker>Wei, Brunskill, Kollar, Roy, 2009</marker>
<rawString>Wei, Y., Brunskill, E., Kollar, T., &amp; Roy, N. (2009). Where To Go: Interpreting Natural Directions Using Global Inference. In Proceedings of the IEEE International Conference on Robotics and Automation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>1--191</pages>
<contexts>
<context position="10130" citStr="Winograd (1972)" startWordPosition="1632" endWordPosition="1633">d Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been work on learning for semantic analysis tasks from grounded data, including event streams (Liang et al., 2009; Chen et al., 2010) and language paired with visual perception (Matuszek et al., 2012). Finally, the topic of executing instructions in non-learning settings has received significant attention (e.g., Winograd (1972), Di Eugenio and White (1992), Webber et al. (1995), Bugmann et al. (2004), MacMahon et al. (2006) and Dzifcak et al. (2009)). 4 Background We use a weighted linear CCG grammar for semantic parsing, as briefly reviewed in this section. Combinatory Categorial Grammars (CCGs) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with categories. For example, the lexical entry chair �- N : Ax.chair(x) for the word “chair” i</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T. (1972). Understanding Natural Language. Cognitive Psychology, 3(1), 1–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9027" citStr="Wong and Mooney (2007)" startWordPosition="1465" endWordPosition="1468">r research has adopted expressive meaning representations, with differing learning approaches. Matuszek et al. (2010, 2012) describe supervised algorithms that learn semantic parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from con</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Y., &amp; Mooney, R. (2007). Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="39709" citStr="Zettlemoyer and Collins (2005)" startWordPosition="6706" endWordPosition="6709">2006) created many uninterpretable instructions and incorrect traces. To focus on the learning and interpretation tasks, we also created a new dataset that includes only accurate instructions labeled with a single, correct execution trace. From this oracle corpus, we randomly sampled 164 instruction sequences (816 sentences) for evaluation, leaving 337 (1863 sentences) for training. This simple effort will allow us to measure the effects of noise on the learning approach and provides a resource for building more accurate algorithms. Table 1 compares the two sets. Features and Parser Following Zettlemoyer and Collins (2005), we use a CKY parser with a beam of k. To boost recall, we adopt a two-pass strategy, which allows for word skipping if the initial parse fails. We use features that indicate usage of lexical entries, templates, lexemes and type-raising rules, as described in Section 6.3, and repetitions in logical coordinations. Finally, during joint parsing, we consider only parses executable at si as complete. Seed Lexicon To construct our seed lexicon we labeled 12 instruction sequences with 141 lexical enSingle Sentence Sequence Final state validation Complete system 81.98 (2.33) 59.32 (6.66) No implicit</context>
<context position="9206" citStr="Zettlemoyer &amp; Collins, 2005" startWordPosition="1493" endWordPosition="1496"> parsers for navigation instructions. Chen and Mooney (2011), Chen (2012) and Kim and Mooney (2012) present state-of-theart algorithms for the navigation task, by training a supervised semantic parser from automatically induced labels. Our work differs in the use of joint learning and inference approaches. Supervised approaches for learning semantic parsers have received significant attention, e.g. Kate and Mooney (2006), Wong and Mooney (2007), Muresan (2011) and Kwiatkowski et al. (2010, 2012). The algorithms we develop in this paper combine ideas from previous supervised CCG learning work (Zettlemoyer &amp; Collins, 2005, 2007; Kwiatkowski et al., 2011), as we describe in Section 4. Recently, various alternative forms of supervision were introduced. Clarke et al. (2010), Goldwasser and Roth (2011) and Liang et al. (2011) describe approaches for learning semantic parsers from sentences paired with responses, Krishnamurthy and Mitchell (2012) describe using distant supervision, Artzi and Zettlemoyer (2011) use weak supervision from conversational logs and Goldwasser et al. (2011) present work on unsupervised learning. We discuss various forms of supervision that complement these approaches. There has also been </context>
<context position="12646" citStr="Zettlemoyer &amp; Collins, 2005" startWordPosition="2060" endWordPosition="2063">ins, 2004; Taskar et al., 2004). Let x be a sentence, y be a CCG parse, and GEN(x; A) be the set of all possible CCG parses for x given the lexicon A. Define O(x, y) E Rd to be a d-dimensional feature–vector representation and 0 E Rd to be a parameter vector. The optimal parse for sentence x is y*(x) = arg max yEGEN(x;A) and the final output logical form z is the A-calculus expression at the root of y*(x). Section 7.2 describes how we efficiently compute an approximation to y*(x) within the joint interpretation and execution model. 0 · O(x, y) 51 Supervised learning with GENLEX Previous work (Zettlemoyer &amp; Collins, 2005) introduced a function GENLEX(x, z) to map a sentence x and its meaning z to a large set of potential lexical entries. These entries are generated by rules that consider the logical form z and guess potential CCG categories. For example, the rule p -+ N : λx.p(x) introduces categories commonly used to model certain types of nouns. This rule would, for example, introduce the category N : λx.chair(x) for any logical form z that contains the constant chair. GENLEX uses a small set of such rules to generate categories that are paired with all possible substrings in x, to create a large set of lexi</context>
<context position="37678" citStr="Zettlemoyer &amp; Collins, 2005" startWordPosition="6387" endWordPosition="6390">lins, 2007; Artzi &amp; Zettlemoyer, 2011) for parameter updates. However, instead of taking advantage of a loss function we use a validation signal. In step (a) we collect the highest scoring valid parses and all invalid parses. Then, in step (b) we construct the set Ri of valid analyses and Ei of invalid ones, such that their model scores are not separated by a margin S scaled by the number of wrong features (Taskar et al., 2003). Finally, step (f) applies the update. Discussion The algorithm uses the validation signal to drive both lexical induction and parameter updates. Unlike previous work (Zettlemoyer &amp; Collins, 2005, 2007; Artzi &amp; Zettlemoyer, 2011), we have no access to a set of logical constants, either through the the labeled logical form or the weak supervision signal, to guide the GENLEX procedure. Therefore, to avoid over-generating lexical entries, thereby making parsing and learning intractable, we leverage typing for coarse parsing to prune the generated set. By allowing a single 57 Oracle SAIL # of instruction sequences 501 706 # of instruction sequences 431 with implicit actions Total # of sentences 2679 3233 Avg. sentences per sequence 5.35 4.61 Avg. tokens per sentence 7.5 7.94 Vocabulary si</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Zettlemoyer, L., &amp; Collins, M. (2005). Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, L., &amp; Collins, M. (2007). Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>