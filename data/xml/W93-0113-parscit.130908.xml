<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002102">
<title confidence="0.998212">
Evaluation Techniques for Automatic Semantic
Extraction: Comparing Syntactic and Window Based
Approaches
</title>
<author confidence="0.995266">
Gregory Grefenstette
</author>
<affiliation confidence="0.865510333333333">
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
</affiliation>
<email confidence="0.988837">
grefen@cs.pittedu
</email>
<sectionHeader confidence="0.959135" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924416666667">
As large on-line corpora become more prevalent, a number of attempts have been
made to automatically extract thesaurus-like relations directly from text using knowl-
edge poor methods. In the absence of any specific application, comparing the results
of these attempts is difficult. Here we propose an evaluation method using gold stan-
dards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction
techniques. Using this evaluation method, we compare two semantic extraction tech-
niques which produce similar word lists, one using syntactic context of words , and
the other using windows of heuristically tagged words. The two techniques are very
similar except that in one case selective natural language processing, a partial syn-
tactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce
significantly better results against the gold standards for the most characteristic
words in the corpus, while windows produce better results for rare words.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995880952381">
As more text becomes available electronically, it is tempting to imagine the development
of automatic filters able to screen these tremendous flows of text extracting useful bits of
information. In order to properly filter, it is useful to know when two words are similar
in a corpus. Knowing this would alleviate part of the term variability problem of natural
language discussed in Furnas et, al. (1987) . Individuals will choose a variety of words
to name the same object or operation, with little overlap between people&apos;s choices. This
variability in naming was cited as the principal reason for large numbers of missed citations
in a large-scale evaluation of an information retrieval system [Blair and Maron, 1985]. A
proper filter must be able to access information in the text using any word of a set of
similar words. A number of knowledge-rich [Jacobs and Rau, 1990, Calzolari and Bindi,
1990, Mauldin, 1991] and knowledge-poor [Brown et at., 1992, Hindle, 1990, Ruge, 1991,
Grefenstette, 1992] methods have been proposed for recognizing when words are similar.
The knowledge-rich approaches require either a conceptual dependency representation, or
semantic tagging of the words, while the knowledge-poor approaches require no previously
encoded semantic information, and depend on frequency of co-occurrence of word contexts
to determine similarity. Evaluations of results produced by the above systems are often
been limited to visual verification by a human subject or left to the human reader.
In this paper, we propose gold standard evaluation techniques, allowing us to ob-
jectively evaluate and to compare two knowledge-poor approaches for extracting word
similarity relations from large text corpora. In order to evaluate the relations extracted,
we measure the overlap of the results of each technique against existing hand-created
</bodyText>
<page confidence="0.998425">
143
</page>
<bodyText confidence="0.999947230769231">
repositories of semantic information such as thesauri and dictionaries. We describe below
how such resources can be used as evaluation tools, and apply them to two knowledge-poor
approaches.
One of the tested semantic extraction approaches uses selective natural language pro-
cessing, in this case the lexical-syntactic relations that can be extracted for each word in
a corpus by robust parsers [Hindle, 1983, Grefenstette, 1993]. The other approach uses a
variation on a classic windowing technique around each word such as was used in [Phillips,
1985]. Both techniques are applied to the same 4 megabyte corpus. We evaluate the re-
sults of both techniques using our gold standard evaluations over thesauri and dictionaries
and compare the results obtained by the syntactic based method to those obtained by the
windowing method. The syntax-based method provides a better overlap with the manu-
ally defined thesaurus classes for the 600 most frequently appearing words in the corpus,
while for rare words the windowing method performs slightly better for rare words.
</bodyText>
<sectionHeader confidence="0.99503" genericHeader="method">
2 Gold Standards Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99476">
2.1 Thesauri
</subsectionHeader>
<bodyText confidence="0.992651666666666">
Roget&apos;s Thesaurus is readily available via anonymous ftpl. In it are collected more than
30,000 unique words arranged in a shallow hierarchy under 1000 topic numbers such as
Existence (Topic Number 1), Inexistence (2), Substantiality (3), Unsubstantiality (4),
..., Rite (998), Canonicals (999), and Temple (1000). Although this is far from the
total number of semantic axes of which one could think, it does provide a wide swath of
commonly accepted associations of English language words. We would expect that any
system claiming to extract semantics from text should find some of the relations contained
in this resource.
By transforming the online source of such a thesaurus, we use it as a gold standard by
which to measure the results of different similarity extraction techniques. This measure-
ment is done by checking whether the &apos;similar words&apos; discovered by each technique are
placed under the same heading in this thesaurus.
In order to create this evaluation tool, we extracted a list consisting of all single-word
entries from our thesauri with their topic number or numbers. A portion of the extracted
Roget list in Figure 1 shows that abatement appears under two topics: Nonincrease (36)
and Discount (813). Abbe and abbess both belong under the same topic heading 996
(Clergy). The extracted Roget&apos;s list has 60,071 words (an average of 60 words for each
of the 1000 topics). Of these 32,000 are unique (an average of two occurrence for each
word). If we assume for simplicity that each word appears under exactly 2 of the 1000
topics, and that the words are uniformly distributed, the chance that two words w1 and
w2 occur under the same topic is
</bodyText>
<equation confidence="0.993806">
PRoget = 2 * (2/1000),
</equation>
<bodyText confidence="0.9940394">
since wi is under 2 topic headings and since the chance that w2 is under any specific topic
heading is 2/1000. The probability of finding two randomly chosen words together under
the same heading, then, is 0.4%.
Our measurement of a similarity extraction technique using this gold standard is per-
formed as follows.
</bodyText>
<footnote confidence="0.9858375">
1For example, in March 1993 it was available via anonymous ftp at the Internet site world.std.com in
the directory /obi/obi2/Gutenbergietext91, as well at over 30 other sites.
</footnote>
<page confidence="0.994273">
144
</page>
<table confidence="0.988260266666667">
Roget&apos;s Macquarie
entry Topic entry subheading
Abatement 36 disesteem 036406
abatement 813 disesteem 063701
Abatis 717 diseur 022701
abatj our 260 disfavour 003901
abattis 717 disfavour 056601
abattoir 361 disfavour 063701
abba 166 disfeature 018212
abbacy 995 disfeaturement 018201
abbatial 995 disfigure 006804
abbatical 995 disfigure 018212
abbatis 717 disfigure 020103
abbe 996 disfigured 006803
abbess 996 disfigured 020102
</table>
<figureCaption confidence="0.995935">
Figure 1: Samples from One Word Entries in Both Thesauri
</figureCaption>
<bodyText confidence="0.99996925">
Given a corpus, use the similarity extraction method to derive similarity judge-
ments between the words appearing in the corpus. For each word, take the
word appearing as most similar. Examine the human compiled thesaurus to
see if that pair of words appears under the same topic number. If it does,
count this as a hit.
This procedure was followed on the 4 megabyte corpus described below to test two seman-
tic extraction techniques, one using syntactically derived contexts to judge similarity and
one using window-based contexts. The results of these evaluations are also given below.
</bodyText>
<subsectionHeader confidence="0.992363">
2.2 Dictionary
</subsectionHeader>
<bodyText confidence="0.999789363636364">
We also use an online dictionary as a gold standard following a slightly different procedure.
Many researchers have drawn on online dictionaries in attempts to do semantic discovery
[Sparck Jones, 1986, Vossen et cd., 1989, Wilks et al., 1989], whereas we use it here only
as a tool for evaluating extraction techniques from unstructured text. We have an online
version of Webster&apos;s 7th available, and we use it in evaluating discovered similarity pairs.
This evaluation is based on the assumption that similar words will share some overlap in
their dictionary definitions. In order to determine overlap, each the entire literal definition
is broken into a list of individual words. This list of tokens contains all the words in the
dictionary entry, including dictionary-related markings and abbreviations. In order to
clean this list of non-information-bearing words, we automatically removed any word or
token
</bodyText>
<listItem confidence="0.99480575">
1. of fewer than 4 characters,
2. among the most common 50 words of 4 or more letters in the Brown corpus,
3. among the most common 50 words of 4 or more letters appearing in the definitions
of Webster&apos;s 7th,
</listItem>
<page confidence="0.996397">
145
</page>
<bodyText confidence="0.966474">
ad-min-is-tra-tion n. I. the act or process of administering 2. performance of executive
duties :: c&lt;MANAGEMENT&gt; 3. the execution of public affairs as distinguished from
policy making 4. a) a body of persons who administer b) i&lt;cap&gt; :: a group constituting
the political executive in a presidential government c) a governmental agency or board 5.
the term of office of an administrative officer. or body.
administer, administering, administrative, affairs, agency, board,
constituting, distinguished, duties, execution, executive, government,
governmental, making, management, office, officer, performance,
persons, policy, political, presidential, public, term
</bodyText>
<figureCaption confidence="0.9599825">
Figure 2: Webster definition of &amp;quot;administration,&amp;quot; and resulting definition list after filtering
through stoplist.
</figureCaption>
<listItem confidence="0.953902">
4. listed as a preposition, quantifier, or determiner in our lexicon,
5. of 4 or more letters from a common information retrieval stoplist,
6. among the dictionary-related set: slang, attrib, kind, word, brit, 71eSS, lion, meld.
</listItem>
<bodyText confidence="0.999937411764706">
These conditions generated a list of 434 stopwords of 4 or more characters which are
retracted from any dictionary definition, The remaining words are sorted into a list. For
example, the list produced for the definition of the word administration is given in Figure 2.
For simplicity no morphological analysis or any other modifications were performed on
the tokens in these lists.
To compare two words using these lists, the intersection of each word&apos;s filtered defi-
nition list is performed. For example, the intersection between the lists derived from the
dictionary entries of diamond and ruby is (precious, stone); between right and freedom it
is (acting, condition, political, power, privilege, right). In order to use these dictionary-
derived lists as an evaluation tool, we perform the following experiment on a corpus.
Given a corpus, take the similarity pairs derived by the semantic
extraction technique in order of decreasing frequency of the first
term. Perform the intersection of their respective two dictionary
definitions as described above. If this intersection contains
two or more elements, count this as a hit.
This evaluation method was also performed on the results of both semantic extraction
techniques applied to the corpus described in the next section.
</bodyText>
<sectionHeader confidence="0.993273" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.9688945">
The corpus used for the evaluating the two techniques was extracted from Grolier &apos;s En-
cyclopedia for other experiments in semantic extraction. In order to generate a relatively
coherent corpus, the corpus was created by extracting only those those sentences which
contained the word Harvard or one of the thirty hyponyms found under the word insti-
tution in WordNet2 [Miller et al., 1990], viz, institution, establishment, charity, religion,
..., settlement. This produced a corpus of 3.9 megabytes of text.
2 WordNet was not used itself as a gold standard since its hierarchy is very deep and its inherent notion
of semantic classes is not as clearly defined as in Roget.
</bodyText>
<page confidence="0.997862">
146
</page>
<sectionHeader confidence="0.989287" genericHeader="method">
4 Semantic Extraction Techniques
</sectionHeader>
<bodyText confidence="0.999978657894737">
We will use these gold standard evaluation techniques to compare two techniques for
extracting similarity lists from raw text.
The first technique [Grefenstette, 1992] extracts the syntactic context of each word
throughout the corpus. The corpus is divided into lexical units via a regular grammar,
each lexical unit is assigned a list of context-free syntactic categories, and a normalized
form. Then a time linear stochastic grammar similar to the one described in [de Marcken,
1990] selects a most probable category for each word. A syntactic analyzer described in
[Grefenstette, 1993] chunks nouns and verb phrases and create relations within chunks
and between chunks. A noun&apos;s context becomes all the other adjectives, nouns, and verbs
that enter into syntactic relations with it.
As a second technique, more similar to classical knowledge-poor techniques [Phillips,
1985] for judging word similarity, we do not perform syntactic disambiguation and analysis,
but simply consider some window of words around a given word as forming the context
of that word. We suppose that we have a lexicon, which we do, that gives all the possible
parts of speech for a word. Each word in the corpus is looked up in this lexicon as in
the first technique, in order to normalize the word and know its possible parts of speech
[Evans et al., 1991]. A noun&apos;s context will be all the words that can be nouns, adjectives,
or verbs within a certain window around the noun. The window that was used was all
nouns, adjectives, or verbs on either side of the noun within ten and within the same
sentence.
In both cases we will compare nouns to each other, using their contexts. In the first
case, the disambiguator determines whether a given ambiguous word is a noun or not. In
the second case, we will simply decide that if a word can be at once a noun or verb, or a
noun or adjective, that it is a noun. This distinction between the two techniques of using
a cursory syntactic analysis or not allows us to evaluate what is gained by the addition of
this processing step.
Figure 3 below shows the types of contexts extracted by the selective syntactic tech-
nique and by the windowing technique for a sentence from the corpus.
Once context is extracted for each noun, the contexts are compared for similarity
using a weighted Jaccard measure [Grefenstette, 1993]. In order to reduce run time for
the similarity comparison, only those nouns appearing more than 10 times in the corpus
were retained. 2661 unique nouns appear 10 times or more. For the windowing technique
33,283 unique attributes with which to judge the words are extracted. The similarity
judging run takes 4 full days on a DEC 5000, compared to 3 and 1/2 hours for the
similarity calculation using data from the syntactic technique, due to greatly increased
number of attributes for each word. For each noun, we retain the noun rated as most
similar by the Jaccard similarity measure. Figure 4 shows some examples of words found
most similar by both techniques.
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99993225">
The first table, in Figure 5, compares the hits produced by the two techniques over Roget&apos;s
and over another online thesaurus, Macquarie &apos;s, that we had available in the Laboratory
for Computational Linguistics at Carnegie Mellon University. This table compares the re-
sults obtained from the windowing technique described in preceding paragraphs to those
</bodyText>
<page confidence="0.991783">
147
</page>
<bodyText confidence="0.925394">
With the arrival of Europeans in 1788 , many Aboriginal societies
, caught within the coils of expanding white settlement , were
gradually destroyed .
</bodyText>
<figure confidence="0.4132208">
Contexts of nouns extracted after syntactic analysis
arrival european society aboriginal society destroy-DOBJ
society catch-SUBJ coil catch-IOBJ settlement white
settlement expand-DOBJ
Some contexts extracted with 10 full-word window
</figure>
<tableCaption confidence="0.932374363636363">
arrival aboriginal arrival society arrival catch
arrival coil arrival expand arrival white
arrival settlement
european aboriginal
european coil
european settlement
society european
society coil
society settlement
arrival destroy
european society
european expand
european destroy
society aboriginal
society expand
society destroy
european arrival
european catch
european white
society arrival
society catch
society white
</tableCaption>
<figureCaption confidence="0.996728185185186">
Figure 3: Comparison of Extracted Contexts using Syntactic and Non-Syntactic Tech-
niques
Corpus word Technique used
Syntax Window
formation creation system
work school religious
foundation institution system
government constitution state
education training public
religious religion century
university institution institution
group institution member
establishment creation government
power authority government
creation establishment state
state law government
program institution education
law constitution public
year century government
center development city
art architecture science
form group life
century year religious
member group group
part center government
Figure 4: Sample of words found to be most similar, by the syntactic based technique,
and by the window technique, to some frequently occurring words in the corpus
</figureCaption>
<page confidence="0.962378">
148
</page>
<table confidence="0.999804473684211">
resUlts over corpus using Window vs Syntactic Contexts
ROGET MACQUARIE WEBSTER
RANK WINDOW SYNTAX WINDOW SYNTAX WINDOW SYNTAX
1-20 25% 50% 15% 40% 55% 50%
21-40 10% 30% 20% 45% 40% 60%
41-60 25% 30% 30% 35% 55% 70%
61-80 15% 30% 20% 30% 45% 65%
81-100 15% 40% 15% 35% 35% 55%
101-200 14% 31% 19% 34% 34% 55%
201-300 21% 29% 20% 30% 29% 34%
301-400 13% 17% 12% 18% 25% 29%
401-500 15% 16% 12% 13% 24% 26%
501-600 13% 11% 10% 15% 19% 16%
601-700 8% 11% 11% 14% 20% 14%
701-800 11% 9% 9% 9% 17% 17%
801-900 17% 6% 13% 7% 25% 12%
901-1000 8% 10% 9% 9% 29% 12%
1001-2000 10.2% 4.9% 11.8% 5.3% 19.2% 6.9%
2001-3000 7.9% 2.4% 7.9% 2.1% 15.2% 5.2%
</table>
<figureCaption confidence="0.8350265">
Figure 5: Windowing vs Syntactic Percentage of Hits for words from most frequent to
least
</figureCaption>
<figure confidence="0.9881686">
ROGET hits
Percentage of hits 1
0 ii
fAm
1-20 21-40 41-60 61-80 81-100 100 200 300 400 500 600 700 800 900 1000 &gt;2000
</figure>
<figureCaption confidence="0.994347333333333">
Figure 6: Comparison of hit percentage in Roget&apos;s using simple 10-word windowing tech-
nique(clear) vs syntactic technique(black). The y-axis gives the percentage of hits for each
group of frequency-ranked terms.
</figureCaption>
<page confidence="0.938679">
149
</page>
<figure confidence="0.984586">
MACQUARIE hits
dd
1-20 2 -40 41-60 61-80 81-100 100 200 300 400 500 600 700 800 900 1000 &gt;2000
</figure>
<figureCaption confidence="0.997697">
Figure 7: Comparison of hits in Macquarte&apos;s using simple 10-word windowing tech-
nique(clear) vs syntactic technique(black). The y-axis gives the percentage of hits for
each group of frequency-ranked terms.
</figureCaption>
<figure confidence="0.993829">
WEBSTER hits
8
ii
1-20 21-40 41-60 61-80 81-100 100 200 300 400 500 600 700 800 900 1000 &gt;2000
</figure>
<figureCaption confidence="0.999246">
Figure 8: Comparison of hit percentage in Webster&apos;s using simple 10-word windowing
</figureCaption>
<bodyText confidence="0.7709385">
technique (hashed bars) vs syntactic technique (solid bars). The y-axis gives the percent-
age of hits for each group of frequency-ranked terms.
</bodyText>
<page confidence="0.989009">
150
</page>
<table confidence="0.993123916666667">
Roget SYNTACTIC Macquarie SYNTACTIC
First 600 HITS MISS First 600 HITS MISS
WINDOW WINDOW
HITS 48 60 HITS 42 54
MISS 91 401 MISS 103 401
= 6.4 x2 = 15.3
p &lt; .025 p &lt; .005
Roget SYNTACTIC Macquarie SYNTACTIC
Last 600 HITS MISS Last 600 HITS MISS
WINDOW WINDOW
HITS 2 28 HITS 4 40
MISS 14 556 MISS 14 542
</table>
<figure confidence="0.658653">
x2 = 4.6 x2 = 12.5
p &lt; .05 p &lt; .0005
</figure>
<figureCaption confidence="0.986604">
Figure 9: x2 results comparing Syntactic and windowing hits in man-made thesauri
</figureCaption>
<bodyText confidence="0.983572545454546">
obtained from the syntactic technique, retaining only words for which similarity judge-
ments were made by both techniques.
It can be seen in Figure 5 that simple technique of moving a window over a large
corpus, counting co-occurrences of words, and eliminating empty words, provides a good
hit ratio for frequently appearing words, since about 1 out of 5 of the 100 most frequent
words are found similar to words appearing in the same heading in a hand-built thesaurus.
It can also be seen that the performance of the partial syntactic analysis based tech-
nique is better for the 600 most frequently appearing nouns, which may be considered as
the characteristic vocabulary of the corpus. The difference in performance between the
two techniques is statistically significant (p i 0.05). The results of a x2 test are given in
Figure 9. Figures 6 and 7 show the same results as histograms. In these histograms it
becomes more evident that the window co-occurrence techniques give more hits for less
frequently occurring words, after the 600th most frequent word. One reason for this can
be seen by examining the 900th most frequent word, employment. Since the windowing
technique extracts up to 20 non-stopwords from either side, there are still 537 context
words attached to this word, while the syntactically-based technique, which examines
finer-grained contexts, only provides 32 attributes.
Figure 8 shows the results of applying the less focused dictionary gold standard exper-
iment to the similarities obtained from the corpus by each technique. For this experiment,
both techniques provide about the same overlap for frequent words, and the same signifi-
cantly stronger showing for the rare words for the windowing technique.
1 5 1
</bodyText>
<sectionHeader confidence="0.99705" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998590181818182">
In this paper we presented a general method for comparing the results of two similarity
extraction techniques via gold standards. This method can he used when no application-
specific evaluation technique exists and provides a relative measurenictit of techniques
against human-generated standard semantic resources. We showed how these gold stan-
dards could be processed to produce a tool for measuring overlap between their contents
and the results of a semantic extraction method. We applied these gold standard evalu-
ations to two different semantic extraction techniques passed over the same 4 megabyte
corpus. The syntactic-based technique produced greater overlap with the gold standards
derived from thesauri for the characteristic vocabulary of the corpus, while the window-
based technique provided relatively better results for rare words.
This dichotomous result suggests that no one statistical technique is adapted to all
ranges of frequencies of words from a corpus. Everyday experience suggests that frequently
occurring events can be more finely analyzed than rarer ones. In the domain of corpus
linguistics, the same reasoning can be applied. For frequent words, finer grained context
such as that provided by even rough syntactic analysis, is rich enough to judge similarity.
For less frequent words, reaping more though less exact information such as that given
by windows of N words provides more information about each word. For rare words, the
context may have to be extended beyond a window, to the paragraph, or section, or entire
document level, as Crouch (1990) did for rarely appearing words.
Acknowledgements. This research was performed under the auspices of the Labora-
tory for Computational Linguistics (Carnegie Mellon University) directed by Professor David A.
Evans.
</bodyText>
<sectionHeader confidence="0.998695" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995548625">
[Blair and Maron, 1985] D.C. Blair and M.E. Maron. An evaluation of retrieval effective-
ness. Communications of the ACM, 28:289-299,1985.
[Brown et al., 1992] Peter F. Brown, Vincent J. Della Pietra, Petere V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467-479,1992.
[Calzolari and Bindi, 1990] Nicoletta Calzolari and Remo Bindi. Acquisition of lexical
information from a large textual italian corpus. In Proceedings of the Thirteenth Inter-
national Conference on Computational Linguistics, Helsinki, 1990.
[Crouch, 1990] C. J. Crouch. An approach to the automatic construction of global the-
sauri. Information Processing and Management, 26(5):629-640,1990.
[de Marcken, 19901 Carl G. de Marcken. Parsing the LOB corpus. In 28th Annual Meeting
of the Association for Computational Linguistics, pages 243-251, Pittsburgh, PA, June
6-9 1990. ACL.
[Evans et at., 1991] David A. Evans, Steve K. Handerson, Robert G. Lefferts, and Ira A.
Monarch. A summary of the CLARIT project. Technical Report CMU-LCL-91-2,
Laboratory for Computational Linguistics, Carnegie-Mellon University, November 1991.
</reference>
<page confidence="0.98031">
152
</page>
<reference confidence="0.953577540540541">
[Furnas et al., 1987] George W. Furnas, Tomas K. Landauer, L.M. Gomez, and Susan T.
Dumais. The vocabulary problem in human-system communication. Communications
of the ACM, 30(11):964-971, November 1987.
[Grefenstette, 1992] G. Grefenstette. Sextant: Exploring unexplored contexts for semantic
extraction from syntactic analysis. In 30th Annual Meeting of the Association for
Computational Linguistics, Newark, Delaware, 28 June — 2 July 1992. ACL&apos;92.
[Grefenstette, 1993] Gregory Grefenstette. Extracting semantics from raw text, imple-
mentation details. Heuristics: the Journal of Knowledge Engineering, 1993. To Appear
in the Special Issue on Knowledge Extraction from Text, Available as TR CS92-05,
from the University of Pittsburgh, CS Dept.
[Hindle, 1983] Donald Hindle. User manual for Fidditch. Technical Report 7590-142,
Naval Research Laboratory, 1983.
[Hindle, 1990] D. Hindle. Noun classification from predicate-argument structures. In Pro-
ceedings of the 38th Annual Meeting of the Association for Computational Linguistics,
pages 268-275, Pittsburgh, 1990. ACL.
[Jacobs and Rau, 1990] Paul Jacobs and Lisa Rau. SCISOR: Extracting information from
on-line news. Communications of the ACM, 33(11):88-97,1990.
[Mauldin, 1991] M. L. Mauldin. Conceptual Information Retrieval: A case study in adap-
tive parsing. Kluwer, Norwell, MA, 1991.
[Miller et al., 1990] George A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J.
Miller. Introduction to WordNet: An on-line lexical database. Journal of Lexicography,
3(4):235-244,1990.
[Phillips, 1985] Martin Phillips. Aspects of Text Structure: An investigation of the lexical
organization of text. Elsevier, Amsterdam, 1985.
[Ruge, 19911 Gerda Ruge. Experiments on linguistically based term associations. In
RIA0&apos;91, pages 528-545, Barcelona, April 2-5 1991. CID, Paris.
[Sparck Jones, 1986] Karen Sparck Jones. Synonymy and Semantic Classification. Ed-
inburgh University Press, Edinburgh, 1986. PhD thesis delivered by University of
Cambridge in 1964.
[Vossen et al., 1989] P. Vossen, W. Meijs, and M. den Broeder. Meaning and structure
in dictionary definitions. In Bran Boguraev and Ted Briscoe, editors, Computational
Lexicography for Natural Language Processing, pages 171-190. Longman Group UK
Limited, London, 1989.
[Wilks et al., 1989] Yorick Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and B. Slator.
A tractable machine dictionary as a resource for computational semantics. In Bran
Boguraev and Ted Briscoe, editors, Computational Lexicography for Natural Language
Processing, pages 193-228. Longman Group UK Limited, London, 1989.
</reference>
<page confidence="0.999166">
153
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833080">
<title confidence="0.998196">Evaluation Techniques for Automatic Extraction: Comparing Syntactic and Window Approaches</title>
<author confidence="0.999615">Gregory Grefenstette</author>
<affiliation confidence="0.999931">Department of Computer University of</affiliation>
<address confidence="0.864607">Pittsburgh, PA</address>
<email confidence="0.999079">grefen@cs.pittedu</email>
<abstract confidence="0.997629769230769">As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristic words in the corpus, while windows produce better results for rare words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>D C Blair</author>
<author>M E Maron</author>
</authors>
<title>An evaluation of retrieval effectiveness.</title>
<journal>Communications of the ACM,</journal>
<pages>28--289</pages>
<marker>[Blair and Maron, 1985]</marker>
<rawString>D.C. Blair and M.E. Maron. An evaluation of retrieval effectiveness. Communications of the ACM, 28:289-299,1985.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Petere V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<marker>[Brown et al., 1992]</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Petere V. deSouza, Jenifer C. Lai, and Robert L. Mercer. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479,1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicoletta Calzolari</author>
<author>Remo Bindi</author>
</authors>
<title>Acquisition of lexical information from a large textual italian corpus.</title>
<date>1990</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Computational Linguistics,</booktitle>
<location>Helsinki,</location>
<marker>[Calzolari and Bindi, 1990]</marker>
<rawString>Nicoletta Calzolari and Remo Bindi. Acquisition of lexical information from a large textual italian corpus. In Proceedings of the Thirteenth International Conference on Computational Linguistics, Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Crouch</author>
</authors>
<title>An approach to the automatic construction of global thesauri.</title>
<date>1990</date>
<booktitle>Information Processing and Management, 26(5):629-640,1990. [de Marcken, 19901 Carl G. de Marcken. Parsing the LOB corpus. In 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>243--251</pages>
<publisher>ACL.</publisher>
<location>Pittsburgh, PA,</location>
<marker>[Crouch, 1990]</marker>
<rawString>C. J. Crouch. An approach to the automatic construction of global thesauri. Information Processing and Management, 26(5):629-640,1990. [de Marcken, 19901 Carl G. de Marcken. Parsing the LOB corpus. In 28th Annual Meeting of the Association for Computational Linguistics, pages 243-251, Pittsburgh, PA, June 6-9 1990. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Evans</author>
<author>Steve K Handerson</author>
<author>Robert G Lefferts</author>
<author>Ira A Monarch</author>
</authors>
<title>A summary of the CLARIT project.</title>
<date>1991</date>
<tech>Technical Report CMU-LCL-91-2,</tech>
<institution>Laboratory for Computational Linguistics, Carnegie-Mellon University,</institution>
<marker>[Evans et at., 1991]</marker>
<rawString>David A. Evans, Steve K. Handerson, Robert G. Lefferts, and Ira A. Monarch. A summary of the CLARIT project. Technical Report CMU-LCL-91-2, Laboratory for Computational Linguistics, Carnegie-Mellon University, November 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George W Furnas</author>
<author>Tomas K Landauer</author>
<author>L M Gomez</author>
<author>Susan T Dumais</author>
</authors>
<title>The vocabulary problem in human-system communication.</title>
<date>1987</date>
<journal>Communications of the ACM,</journal>
<pages>30--11</pages>
<marker>[Furnas et al., 1987]</marker>
<rawString>George W. Furnas, Tomas K. Landauer, L.M. Gomez, and Susan T. Dumais. The vocabulary problem in human-system communication. Communications of the ACM, 30(11):964-971, November 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Sextant: Exploring unexplored contexts for semantic extraction from syntactic analysis.</title>
<date></date>
<journal></journal>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>92</pages>
<location>Newark, Delaware,</location>
<marker>[Grefenstette, 1992]</marker>
<rawString>G. Grefenstette. Sextant: Exploring unexplored contexts for semantic extraction from syntactic analysis. In 30th Annual Meeting of the Association for Computational Linguistics, Newark, Delaware, 28 June — 2 July 1992. ACL&apos;92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Extracting semantics from raw text, implementation details. Heuristics:</title>
<date>1993</date>
<journal>the Journal of Knowledge Engineering,</journal>
<institution>University of Pittsburgh, CS Dept.</institution>
<note>To Appear in the Special Issue on Knowledge Extraction from Text, Available as TR CS92-05, from the</note>
<marker>[Grefenstette, 1993]</marker>
<rawString>Gregory Grefenstette. Extracting semantics from raw text, implementation details. Heuristics: the Journal of Knowledge Engineering, 1993. To Appear in the Special Issue on Knowledge Extraction from Text, Available as TR CS92-05, from the University of Pittsburgh, CS Dept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>User manual for Fidditch.</title>
<date>1983</date>
<tech>Technical Report 7590-142,</tech>
<institution>Naval Research Laboratory,</institution>
<marker>[Hindle, 1983]</marker>
<rawString>Donald Hindle. User manual for Fidditch. Technical Report 7590-142, Naval Research Laboratory, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<publisher>ACL.</publisher>
<location>Pittsburgh,</location>
<marker>[Hindle, 1990]</marker>
<rawString>D. Hindle. Noun classification from predicate-argument structures. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 268-275, Pittsburgh, 1990. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Paul Jacobs</author>
<author>Lisa Rau</author>
</authors>
<title>SCISOR: Extracting information from on-line news.</title>
<journal>Communications of the ACM,</journal>
<pages>33--11</pages>
<marker>[Jacobs and Rau, 1990]</marker>
<rawString>Paul Jacobs and Lisa Rau. SCISOR: Extracting information from on-line news. Communications of the ACM, 33(11):88-97,1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Mauldin</author>
</authors>
<title>Conceptual Information Retrieval: A case study in adaptive parsing.</title>
<date>1991</date>
<publisher>Kluwer,</publisher>
<location>Norwell, MA,</location>
<marker>[Mauldin, 1991]</marker>
<rawString>M. L. Mauldin. Conceptual Information Retrieval: A case study in adaptive parsing. Kluwer, Norwell, MA, 1991.</rawString>
</citation>
<citation valid="false">
<authors>
<author>George A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<journal>Journal of Lexicography,</journal>
<pages>3--4</pages>
<marker>[Miller et al., 1990]</marker>
<rawString>George A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. Introduction to WordNet: An on-line lexical database. Journal of Lexicography, 3(4):235-244,1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Phillips</author>
</authors>
<title>Aspects of Text Structure: An investigation of the lexical organization of text.</title>
<date>1985</date>
<booktitle>In RIA0&apos;91,</booktitle>
<pages>528--545</pages>
<publisher>Elsevier,</publisher>
<location>Amsterdam,</location>
<marker>[Phillips, 1985]</marker>
<rawString>Martin Phillips. Aspects of Text Structure: An investigation of the lexical organization of text. Elsevier, Amsterdam, 1985. [Ruge, 19911 Gerda Ruge. Experiments on linguistically based term associations. In RIA0&apos;91, pages 528-545, Barcelona, April 2-5 1991. CID, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>Synonymy and Semantic Classification.</title>
<date>1986</date>
<publisher>Edinburgh University Press,</publisher>
<location>Edinburgh,</location>
<marker>[Sparck Jones, 1986]</marker>
<rawString>Karen Sparck Jones. Synonymy and Semantic Classification. Edinburgh University Press, Edinburgh, 1986. PhD thesis delivered by University of Cambridge in 1964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>W Meijs</author>
<author>M den Broeder</author>
</authors>
<title>Meaning and structure in dictionary definitions.</title>
<date>1989</date>
<booktitle>In Bran Boguraev and Ted Briscoe, editors, Computational Lexicography for Natural Language Processing,</booktitle>
<pages>171--190</pages>
<publisher>Longman Group UK Limited,</publisher>
<location>London,</location>
<marker>[Vossen et al., 1989]</marker>
<rawString>P. Vossen, W. Meijs, and M. den Broeder. Meaning and structure in dictionary definitions. In Bran Boguraev and Ted Briscoe, editors, Computational Lexicography for Natural Language Processing, pages 171-190. Longman Group UK Limited, London, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>D Fass</author>
<author>C Guo</author>
<author>J McDonald</author>
<author>T Plate</author>
<author>B Slator</author>
</authors>
<title>A tractable machine dictionary as a resource for computational semantics.</title>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing,</booktitle>
<pages>193--228</pages>
<editor>In Bran Boguraev and Ted Briscoe, editors,</editor>
<publisher>Longman Group UK Limited,</publisher>
<location>London,</location>
<marker>[Wilks et al., 1989]</marker>
<rawString>Yorick Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and B. Slator. A tractable machine dictionary as a resource for computational semantics. In Bran Boguraev and Ted Briscoe, editors, Computational Lexicography for Natural Language Processing, pages 193-228. Longman Group UK Limited, London, 1989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>