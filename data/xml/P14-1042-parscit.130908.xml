<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000850">
<title confidence="0.9991265">
Grammatical Relations in Chinese:
GB-Ground Extraction and Data-Driven Parsing
</title>
<author confidence="0.997662">
Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding, Xiaojun Wan∗
</author>
<affiliation confidence="0.993608">
Institute of Computer Science and Technology, Peking University
</affiliation>
<note confidence="0.339021">
The MOE Key Laboratory of Computational Linguistics, Peking University
</note>
<email confidence="0.984379">
{ws,duyantao,kouxin,wanxiaojun}@pku.edu.cn,dsy100@gmail.com
</email>
<sectionHeader confidence="0.99675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.955949">
This paper is concerned with building linguistic re-
sources and statistical parsers for deep grammatical
relation (GR) analysis of Chinese texts. A set of
linguistic rules is defined to explore implicit phrase
structural information and thus build high-quality
GR annotations that are represented as general di-
rected dependency graphs. The reliability of this
linguistically-motivated GR extraction procedure is
highlighted by manual evaluation. Based on the
converted corpus, we study transition-based, data-
driven models for GR parsing. We present a novel
transition system which suits GR graphs better than
existing systems. The key idea is to introduce a new
type of transition that reorders top k elements in the
memory module. Evaluation gauges how successful
GR parsing for Chinese can be by applying data-
driven models.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993742868852459">
Grammatical relations (GRs) represent functional
relationships between language units in a sen-
tence. They are exemplified in traditional gram-
mars by the notions of subject, direct/indirect
object, etc. GRs have assumed an important
role in linguistic theorizing, within a variety of
approaches ranging from generative grammar to
functional theories. For example, several com-
putational grammar formalisms, such as Lexi-
cal Function Grammar (LFG; Bresnan and Ka-
plan, 1982; Dalrymple, 2001) and Head-driven
Phrase Structure Grammar (HPSG; Pollard and
Sag, 1994) encode grammatical functions directly.
In particular, GRs can be viewed as the depen-
dency backbone of an LFG analysis that provide
general linguistic insights, and have great potential
advantages for NLP applications, (Kaplan et al.,
2004; Briscoe and Carroll, 2006; Clark and Cur-
ran, 2007a; Miyao et al., 2007).
∗Email correspondence.
In this paper, we address the question of an-
alyzing Chinese sentences with deep GRs. To
acquire high-quality GR corpus, we propose a
linguistically-motivated algorithm to translate a
Government and Binding (GB; Chomsky, 1981;
Carnie, 2007) grounded phrase structure treebank,
i.e. Chinese Treebank (CTB; Xue et al., 2005)
to a deep dependency bank where GRs are ex-
plicitly represented. Different from popular shal-
low dependency parsing that focus on tree-shaped
structures, our GR annotations are represented as
general directed graphs that express not only lo-
cal but also various long-distance dependencies,
such as coordinations, control/raising construc-
tions, topicalization, relative clauses and many
other complicated linguistic phenomena that goes
beyond shallow syntax (see Fig. 1 for example.).
Manual evaluation highlights the reliability of our
linguistically-motivated GR extraction algorithm:
The overall dependency-based precision and recall
are 99.17 and 98.87. The automatically-converted
corpus would be of use for a wide variety of NLP
tasks.
Recent years have seen the introduction of a
number of treebank-guided statistical parsers ca-
pable of generating considerably accurate parses
for Chinese. With the high-quality GR resource
at hand, we study data-driven GR parsing. Previ-
ous work on dependency parsing mainly focused
on structures that can be represented in terms of
directed trees. We notice two exceptions. Sagae
and Tsujii (2008) and Titov et al. (2009) individ-
ually studied two transition systems that can gen-
erate more general graphs rather than trees. In-
spired by their work, we study transition-based
models for building deep dependency structures.
The existence of a large number of crossing arcs in
GR graphs makes left-to-right, incremental graph
spanning computationally hard. Applied to our
data, the two existing systems cover only 51.0%
and 76.5% GR graphs respectively. To better suit
</bodyText>
<page confidence="0.986677">
446
</page>
<note confidence="0.853416">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 446–456,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.984210588235294">
obj
subj
comp temp
subj
root root
temp
prt
prt
obj
comp
nmod
obj
subj*ldd
relative
nmod
a-11 r, iET* WAi 5�� T OR fN� �R:K n Mplit �Z4
Pudong recently issue practice involve economic field regulatory document
</figure>
<figureCaption confidence="0.997596">
Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field.
</figureCaption>
<bodyText confidence="0.999641088235294">
The symbol “*ldd” indicates long-distance dependencies; “subj*ldd” between the word “‰Ê/involve”
and the word “‡ö/documents” represents a long-range subject-predicate relation. The arguments and
adjuncts of the coordinated verbs, namely “�TIK/issue” and “Vi/practice,” are separately yet distribu-
tively linked the two heads.
our problem, we extend Titov et al.’s work and
study what we call K-permutation transition sys-
tem. The key idea is to introduce a new type of
transition that reorders top k (2 ≤ k ≤ K) el-
ements in the memory module of a stack-based
transition system. With the increase of K, the ex-
pressiveness of the corresponding system strictly
increases. We propose an oracle deriving method
which is guaranteed to find a sound transition se-
quence if one exits. Moreover, we introduce an
effective approximation of that oracle, which de-
creases decoding ambiguity but practically covers
almost exactly the same graphs for our data.
Based on the stronger transition system, we
build a GR parser with a discriminative model for
disambiguation and a beam decoder for inference.
We conduct experiments on CTB 6.0 to profile this
parser. With the increase of the K, the parser is
able to utilize more GR graphs for training and
the numeric performance is improved. Evaluation
gauges how successful GR parsing for Chinese
can be by applying data-driven models. Detailed
analysis reveal some important factors that may
possibly boost the performance. To our knowl-
edge, this work provides the first result of exten-
sive experiments of parsing Chinese with GRs.
We release our GR processing kit and gold-
standard annotations for research purposes. These
resources can be downloaded at http://www.
icst.pku.edu.cn/lcwm/omg.
</bodyText>
<sectionHeader confidence="0.971713" genericHeader="method">
2 GB-grounded GR Extraction
</sectionHeader>
<bodyText confidence="0.999776277777778">
In this section, we discuss the construction of the
GR annotations. Basically, the annotations are au-
tomatically converted from a GB-grounded phrase-
structure treebank, namely CTB. Conceptually,
this conversion is similar to the conversions from
CTB structures to representations in deep gram-
mar formalisms (Tse and Curran, 2010; Yu et al.,
2010; Guo et al., 2007; Xia, 2001). However, our
work is grounded in GB, which is the linguistic ba-
sis of the construction of CTB. We argue that this
theoretical choice makes the conversion process
more compatible with the original annotations and
therefore more accurate. We use directed graphs to
explicitly encode bi-lexical dependencies involved
in coordination, raising/control constructions, ex-
traction, topicalization, and many other compli-
cated phenomena. Fig. 1 shows an example of
such a GR graph and its original CTB annotation.
</bodyText>
<subsectionHeader confidence="0.99731">
2.1 Linguistic Basis
</subsectionHeader>
<bodyText confidence="0.999962388888889">
GRs are encoded in different ways in different lan-
guages. In some languages, e.g. Turkish, gram-
matical function is encoded by means of morpho-
logical marking, while in highly configurational
languages, e.g. Chinese, the grammatical function
of a phrase is heavily determined by its constituent
structure position. Dominant Chomskyan theo-
ries, including GB, have defined GRs as configu-
rations at phrase structures. Following this princi-
ple, CTB groups words into constituents through
the use of a limited set of fundamental grammat-
ical functions. Transformational grammar utilizes
empty categories (ECs) to represent long-distance
dependencies. In CTB, traces are provided by
relating displaced linguistic material to where it
should be interpreted semantically. By exploiting
configurational information, traces and functional
tag annotations, GR information can be hopefully
</bodyText>
<page confidence="0.994275">
447
</page>
<figure confidence="0.998971076923077">
IP
VP
↓=↑
VP
NP
↓=↑
↓=(↑ OBJ)
VP{Mi,�n}
VP{Mi,�n}
NP
CP{n} NP
{jZ#}
DEC
</figure>
<page confidence="0.998996">
449
</page>
<bodyText confidence="0.999832">
graphs than syntactic dependency trees. In the
training data (defined in Section 4.1), there are
558132 arcs and 86534 crossing pairs, About half
of the sentences have crossing arcs (10930 out of
22277). The wide existence of crossing arcs poses
an essential challenge for GR parsing, namely, to
find methods for handling crossing arcs without a
significant loss in accuracy and efficiency.
</bodyText>
<sectionHeader confidence="0.986876" genericHeader="method">
3 Transition-based GR Parsing
</sectionHeader>
<bodyText confidence="0.9996263">
The availability of large-scale treebanks has con-
tributed to the blossoming of statistical approaches
to build accurate shallow constituency and depen-
dency parsers. With high-quality GR resources at
hand, it is possible to study statistical approaches
to automatically parse GR graphs. In this section,
we investigate the feasibility of applying a data-
driven, grammar-free approach to build GRs di-
rectly. In particular, transition-based dependency
parsing method is studied.
</bodyText>
<subsectionHeader confidence="0.999829">
3.1 Data-Driven Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999903047619048">
Data-driven, grammar-free dependency parsing
has received an increasing amount of attention in
the past decade. Such approaches, e.g. transition-
based (Yamada and Matsumoto, 2003; Nivre,
2008) and graph-based (McDonald, 2006; Tor-
res Martins et al., 2009) models have attracted
the most attention of dependency parsing in re-
cent years. Transition-based parsers utilize tran-
sition systems to derive dependency trees together
with treebank-induced statistical models for pre-
dicting transitions. This approach was pioneered
by (Yamada and Matsumoto, 2003) and (Nivre
et al., 2004). Most research concentrated on sur-
face syntactic structures, and the majority of ex-
isting approaches are limited to producing only
trees. We notice two exceptions. Sagae and Tsu-
jii (2008) and Titov et al. (2009) individually in-
troduced two transition systems that can generate
specific graphs rather than trees. Inspired by their
work, we study transition-based approach to build
GR graphs.
</bodyText>
<subsectionHeader confidence="0.999834">
3.2 Transition Systems
</subsectionHeader>
<bodyText confidence="0.921559666666667">
Following (Nivre, 2008), we define a transition
system for dependency parsing as a quadruple S =
(C, T, cs, Ct), where
</bodyText>
<listItem confidence="0.824484666666667">
1. C is a set of configurations, each of which
contains a buffer β of (remaining) words and
a set A of dependency arcs,
</listItem>
<equation confidence="0.926386666666667">
Transitions
SHIFT (σ,j|β,A) ⇒ (σ|j,β,A)
LEFT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(j,l,i)})
RIGHT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(i,l,j)})
POP (σ|i, β, A) ⇒ (σ, β, A)
ROTATEk (σ|ik |... |i2|i1,β,A) ⇒ (σ|i1|ik |... |i2,β,A)
</equation>
<tableCaption confidence="0.764959">
Table 2: K-permutation System.
</tableCaption>
<listItem confidence="0.995738166666667">
2. T is a set of transitions, each of which is a
(partial) function t : C 7→ C,
3. cs is an initialization function, mapping a
sentence x to a configuration, with β =
[1, . . . ,n],
4. Ct ⊆ C is a set of terminal configurations.
</listItem>
<bodyText confidence="0.869927153846154">
Given a sentence x = w1, ... , wn and a graph
G = (V, A) on it, if there is a sequence of tran-
sitions t1, ... , tm and a sequence of configura-
tions c0, ... , cm such that c0 = cs(x), ti(ci−1) =
ci(i = 1, ... , m), cm ∈ Ct, and Acm = A, we say
the sequence of transitions is an oracle sequence.
And we define ¯Aci = A − Aci for the arcs to be
built in ci. In a typical transition-based parsing
process, the input words are put into a queue and
partially built structures are organized by a stack.
A set of SHIFT/REDUCE actions are performed se-
quentially to consume words from the queue and
update the partial parsing results.
</bodyText>
<subsectionHeader confidence="0.998515">
3.3 Online Reordering
</subsectionHeader>
<bodyText confidence="0.999979352941177">
Among existing systems, Sagae and Tsujii’s is de-
signed for projective graphs (denoted by G1 in
Definition 1), and Titov et al.’s handles only a
specific subset of non-projective graphs as well
as projective graphs (G2). Applied to our data,
only 51.0% and 76.5% of the extracted graphs are
parsable with their systems. Obviously, it is nec-
essary to investigate new transition systems for the
parsing task in our study. To deal with crossing
arcs, Titov et al. (2009) and Nivre (2009) designed
a SWAP transition that switches the position of the
two topmost nodes on the stack. Inspired by their
work, we extend this approach to parse more gen-
eral graphs. The basic idea is to provide our new
system with an ability to reorder more nodes dur-
ing decoding in an online fashion, which we refer
to as online reordering.
</bodyText>
<subsectionHeader confidence="0.985229">
3.4 K-Permutation System
</subsectionHeader>
<bodyText confidence="0.989403">
We define a K-permutation transition system
SK = (C, T, cs, Ct), where a configuration c =
</bodyText>
<page confidence="0.98366">
450
</page>
<bodyText confidence="0.996738">
(Q, β, A) E C contains a stack Q of nodes be-
sides β and A. We set the initial configuration
for a sentence x = w1, ... , wn to be cs(x) =
([], [1, ... , n],11), and take Ct to be the set of all
configurations of the form ct = (Q, [], A) (for any
arc set A). The set of transitions T contains five
types of actions, as shown in Tab. 2:
</bodyText>
<listItem confidence="0.98426">
1. SHIFT removes the front element from β and
pushes it onto Q.
2. LEFT-ARCl/RIGHT-ARCl updates a configu-
ration by adding (i, l, j)/(j,l, i) to A where i
is the top of Q, and j is the front of β.
3. POP deletes the top element of Q.
4. ROTATEk updates a configuration with stack
Q|ik |... |i2|i1 by rotating the top k nodes
in stack left by one index, obtaining
Q|i1|ik |... |i2, with constraint 2 G k G K.
</listItem>
<bodyText confidence="0.975570807692308">
We refer to this system as K-permutation because
by rotating the top k (2 G k G K) nodes in the
stack, we can obtain all the permutations of the
top K nodes. Note that S2 is identical to Titov
et al.’s; S∞ is complete with respect to the class of
all directed graphs without self-loop, since we can
arbitrarily permute the nodes in the stack. The K-
permutation system exhibits a nice property: The
sets of corresponding graphs are strictly mono-
tonic with respect to the C operation.
Definition 1. If a graph G can be parsed with tran-
sition system SK, we say G is a K-perm graph.
We use !gK to denote the set of all k-perm graphs.
Specially, !g0 = 0, !g1 is the set of all projective
graphs, and !g∞ = U∞k=0 !gk.
Theorem 1. !gi C !gi+1, bi &gt; 0.
Proof. It is obvious that !gi C !gi+1 and !g0 C !g1.
Fig. 4 gives an example which is in !gi+1 but not
in !gi for all i &gt; 0, indicating !gi =� !gi+1.
Theorem 2. !g∞ is the set of all graphs without
self-loop.
Proof. It follows immediately from the fact that
G E !g|V |,bG = (V, E).
The transition systems introduced in (Sagae and
Tsujii, 2008) and (Titov et al., 2009) can be viewed
as S11 and S2.
</bodyText>
<footnote confidence="0.963891333333333">
1Though Sagae and Tsujii (2008) introduced additional
constraints to exclude cyclic path, the fundamental transition
mechanism of their system is the same to S1.
</footnote>
<figureCaption confidence="0.997387">
Figure 4: A graph which is in !gi+1, but not in !gi.
</figureCaption>
<subsectionHeader confidence="0.647764">
3.5 Normal Form Oracle
</subsectionHeader>
<bodyText confidence="0.999826862068966">
The K-permutation transition system may allow
multiple oracle transition sequences on one graph,
but trying to sum all the possible oracles is usu-
ally computational expensive. Here we give a con-
struction procedure which is guaranteed to find an
oracle sequence if one exits. We refer it as normal
form oracle (NFO).
Let L(j) be the ordered list of nodes connected
to j in ¯Aci_1 for j E Qci_1, and let LK(Qci_1) =
[L(j1), ... , L(jmax{l,K})]. If Qci_1 is empty, then
we set ti to SHIFT; if there is no arc linked to
j1 in ¯Aci_1, then we set ti to POP; if there exits
a E ¯Aci_1 linking j1 and b, then we set ti to LEFT-
ARC or RIGHT-ARC correspondingly. When there
are only SHIFT and ROTATE left, we first apply
a sequence of ROTATE’s to make LK(Q) com-
plete ordered by lexicographical order, then apply
a SHIFT. Let ci = ti(ci−1), we continue to com-
pute ti+1, until βci is empty.
Theorem 3. If a graph is parsable with the transi-
tion system SK then the construction procedure is
guaranteed to find an oracle transition sequence.
Proof. During the construction, all the arcs are
built by LEFT-ARC or RIGHT-ARC, which links
the top of the stack and the front of the buffer.
Therefore, we prefer L(Q) to be as orderly as pos-
sible, to make the words to be linked sooner on the
top of the stack. the construction procedure above
does best within the power of the system SK.
</bodyText>
<subsectionHeader confidence="0.995658">
3.6 An Approximation for NFO
</subsectionHeader>
<bodyText confidence="0.999988818181818">
In the construction of NFO transitions, we ex-
haustively use the ROTATE’s to make L(Q) com-
plete ordered. We also observed that the tran-
sition LEFT-ARC, RIGHT-ARC and SHIFT only
change the relative order between the first element
of L(Q) and the rest elements. Therefore we ex-
plored an approximate procedure to determine the
ROTATE’s, based on the observation. We call it ap-
proximate NFO (ANFO). Using notation defined
in Section 3.5, the approximate procedure goes as
follows. When it comes to the determination of
</bodyText>
<equation confidence="0.98464525">
w1 ··· wi
wi+1 ··· w2i
w2i+1
w2i+2
</equation>
<page confidence="0.993055">
451
</page>
<figureCaption confidence="0.975087">
Figure 5: A graph that can be parsed
</figureCaption>
<bodyText confidence="0.9986211">
with 53 with a transition sequence
SSSSR3SR3APAPR2R3SR3SR3APAPAPAPAP,
where S stands for SHIFT, R for ROTATE, A for
LEFT-ARC, and P for POP. But the approximate
procedure fails to find the oracle, since R2R3 in
bold in the sequence are not to be applied.
the ROTATE sequence, let k be the largest m such
that 0 &lt; m &lt; min{K, l} and L(j,,t) strictly pre-
cedes L(j1) by the lexicographical order (here we
assume L(j0) strictly precedes any L(j), j E Q).
If k &gt; 0, we set tz to ROTATEk; else we set tz to
SHIFT. The approximation assumes L(Q) is com-
pletely ordered except the first element, and insert
the first element to its proper place each time.
Definition 2. We define ˆ!9K as the graphs the ora-
cle of which can be extracted by 5K with the ap-
proximation procedure.
It can be inferred similarly that Theorem 1 and
Theorem 2 also hold for ˆ!9’s. However, the ˆ!9K is
not equal to !9K in non-trivial cases.
Theorem 4. ˆ!9z C !9z, Vi &gt; 3.
Proof. It is trivial that ˆ!9z C !9z. An example graph
that is in !93 but not in ˆ!93 is shown in Figure 5,
examples for arbitrary i &gt; 3 can be constructed
similarly.
The above theorem indicates the inadequacy of
the ANFO deriving procedure. Nevertheless, em-
pirical evaluation (Section 4.2) shows that the cov-
erage of AFO and ANFO deriving procedures are
almost identical when applying to linguistic data.
</bodyText>
<subsectionHeader confidence="0.99444">
3.7 Statistical Parsing
</subsectionHeader>
<bodyText confidence="0.999969272727273">
When we parse a sentence w1w2 · · · w,, we start
with the initial configuration c0 = cs(x), and
choose next transition tz = C(cz−1) iteratively ac-
cording to a discriminative classifier trained on or-
acle sequences. To build a parser, we use a struc-
tured classifier to approximate the oracle, and ap-
ply the Passive-Aggressive (PA) algorithm (Cram-
mer et al., 2006) for parameter estimation. The
PA algorithm is similar to the Perceptron algo-
rithm, the difference from which is the update of
weight vector. We also use parameter averaging
and early update to achieve better training. Devel-
oping features has been shown crucial to advanc-
ing the state-of-the-art in dependency tree parsing
(Koo and Collins, 2010; Zhang and Nivre, 2011).
To build accurate deep dependency parsers, we
utilize a large set of features for disambiguation.
See the notes included in the supplymentary ma-
terial for details. To improve the performance, we
also apply the technique of beam search, which
keep a beam of transition sequences with highest
scores when parsing.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99175">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999978375">
CTB is a segmented, part-of-speech (POS) tagged,
and fully bracketed corpus in the constituency for-
malism, and very popular to evaluate fundamen-
tal NLP tasks, including word segmentation (Sun
and Xu, 2011), POS tagging (Sun and Uszkoreit,
2012), and syntactic parsing (Zhang and Clark,
2009; Sun and Wan, 2013). We use CTB 6.0 and
define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. We use
gold-standard word segmentation and POS tag-
ing results as inputs. All transition-based parsing
models are trained with beam 16 and iteration 30.
Overall precision/recall/f-score with respect to de-
pendency tokens is reported. To evaluate the abil-
ity to recover non-local dependencies, the recall of
such dependencies are reported too.
</bodyText>
<subsectionHeader confidence="0.989102">
4.2 Coverage and Accuracy
</subsectionHeader>
<bodyText confidence="0.999977388888889">
There is a dual effect of the increase of the param-
eter k to our transition-based dependency parser.
On one hand, the higher k is, the more expres-
sivity the corresponding transition system has. A
system with higher k covers more structures and
allows to use more data for training. On the other
hand, higher k brings more ambiguities to the cor-
responding parser, and the parsing performance
may thus suffer. Note that the ambiguity exists not
only in each step for transition decision, but also
in selecting the training oracle.
The left-most columns of Tab. 3 shows the cov-
erage of K-permutation transition system with re-
spect to different K and different oracle deriving
algorithms. Readers may be surprised that the
coverage of NFO and ANFO deriving procedures
is the same. Actually, all the covered graphs by
the two oracle deriving procedures are exactly the
</bodyText>
<equation confidence="0.84942">
w1 w2 w3 w4 w5 w6 w7 w8 w9
</equation>
<page confidence="0.975846">
452
</page>
<table confidence="0.9892362">
System NFO ANFO UP UR UF LP LR LF URL LRL URNL LRNL
S2 76.5 76.5 85.88 81.00 83.37 83.98 79.21 81.53 81.93 80.34 58.88 52.17
S3 89.0 89.0 86.02 81.72 83.82 84.07 79.86 81.91 82.61 80.94 60.46 54.28
S4 95.6 95.6 86.28 82.06 84.12 84.35 80.22 82.23 82.92 81.29 61.48 54.77
S5 98.4 98.4 86.44 82.21 84.27 84.51 80.37 82.39 83.15 81.51 59.80 53.30
</table>
<tableCaption confidence="0.999822">
Table 3: Coverage and accuracy of the GR parser on the development data.
</tableCaption>
<bodyText confidence="0.999938840909091">
same, except for S3. Only 1 from 22277 sen-
tences can find a NFO but not an ANFO. This
number demonstrates the effectiveness of ANFO.
In the following experiments, we use the ANFO’s
to train our parser.
Applied to our data, S2, i.e. the exact system in-
troduced by Titov et al. (2009), only covers 76.5%
GR graphs. This is very different from the re-
sult obtained on the CoNLL shared task data for
English semantic role labeling (SRL). According
to (Titov et al., 2009), 99% semantic-role-labelled
graphs can be generated by S2. We think there are
two main reasons accounting for the differences,
and highlight the importance of the expressiveness
of transition systems to solve deep dependency
parsing problems. First, the SRL task only focuses
on finding arguments and adjuncts of verbal (and
nominal) predicates, while dependencies headed
by other words are not contained in its graph rep-
resentation. On contrast, a deep dependency struc-
ture, like GR graph, approximates deep syntactic
or semantic information of a sentence as a whole,
and therefore is more dense. As a result, permuta-
tion system with a very low k is incapable to han-
dle more cases. Another reason is about the Chi-
nese language. Some language-specific properties
result in complex crossing arcs. For example, se-
rial verb constructions are widely used in Chinese
to describe several separate events without con-
junctions. The verbal heads in such constructions
share subjects and adjuncts, both of which are be-
fore the heads. The distributive dependencies be-
tween verbal heads and subjects/adjuncts usually
produce crossing arcs (see Fig. 6). To test our as-
sumption, we evaluate the coverage of S2 over the
functor-argument dependency graphs provided by
the English and Chinese CCGBank (Hockenmaier
and Steedman, 2007; Tse and Curran, 2010). The
result is 96.9% vs. 89.0%, which confirms our
linguistic intuition under another grammar formal-
ism.
Tab. 3 summarizes the performance of the
transition-based parser with different configura-
tions to reveal how well data-driven parsing can
</bodyText>
<figure confidence="0.58494">
subject adjunct verb1 verb2
</figure>
<figureCaption confidence="0.989645">
Figure 6: A simplified example to illustrate cross-
ing arcs in serial verbal constructions.
</figureCaption>
<bodyText confidence="0.972859461538461">
be performed in realistic situations. We can see
that with the increase of K, the overall parsing ac-
curacy incrementally goes up. The high complex-
ity of Chinese deep dependency structures demon-
strates the importance of the expressiveness of a
transition system, while the improved numeric ac-
curacies practically certify the benefits. The two
points merit further exploration to more expressive
transition systems for deep dependency parsing, at
least for Chinese. The labeled evaluation scores
on the final test data are presented in Tab. 4.
Test UP UR UF LRL LRNL
S5 83.93 79.82 81.82 80.94 54.38
</bodyText>
<tableCaption confidence="0.988239">
Table 4: Performance on the test data.
</tableCaption>
<subsectionHeader confidence="0.991237">
4.3 Precision vs. Recall
</subsectionHeader>
<bodyText confidence="0.99995675">
A noteworthy thing about the overall performance
is that the precision is promising but the recall is
too low behind. This difference is consistent with
the result obtained by a shift-reduce CCG parser
(Zhang and Clark, 2011). The functor-argument
dependencies generated by that parser also has a
relatively high precision but considerably low re-
call. There are two similarities between our parser
and theirs: 1) both parsers produce dependency
graphs rather trees; 2) both parser employ a beam
decoder that does not guarantee global optimality.
To build NLP application, e.g. information extrac-
tion, systems upon GR parsing, such property mer-
its attention. A good trade-off between the preci-
sion and the recall may have a great impact on final
results.
</bodyText>
<page confidence="0.998238">
453
</page>
<subsectionHeader confidence="0.998394">
4.4 Local vs. Non-local
</subsectionHeader>
<bodyText confidence="0.999916615384615">
Although the micro accuracy of all dependencies
are considerably good, the ability of current state-
of-the-art statistical parsers to find difficult non-
local materials is far from satisfactory, even for
English (Rimell et al., 2009; Bender et al., 2011).
We report the accuracy in terms of local and non-
local dependencies respectively to show the diffi-
culty of the recovery of non-local dependencies.
The last four columns of Tab. 3 demonstrates the
labeled/unlabeled recall of local (URL/LRL) and
non-local dependencies (URNL/LRNL). We can
clearly see that non-local dependency recovery is
extremely difficult for Chinese parsing.
</bodyText>
<subsectionHeader confidence="0.993269">
4.5 Deep vs. Deep
</subsectionHeader>
<bodyText confidence="0.999965928571429">
CCG and HPSG parsers also favor the dependency-
based metrics for evaluation (Clark and Curran,
2007b; Miyao and Tsujii, 2008). Previous work
on Chinese CCG and HPSG parsing unanimously
agrees that obtaining the deep analysis of Chinese
is more challenging (Yu et al., 2011; Tse and Cur-
ran, 2012). The successful C&amp;C and Enju parsers
provide very inaccurate results for Chinese texts.
Though the numbers profiling the qualities of deep
dependency structures under different formalisms
are not directly comparable, all empirical eval-
uation indicates that the state-of-the-art of deep
linguistic processing for Chinese lag behind very
much.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999765">
Wide-coverage in-depth and accurate linguistic
processing is desirable for many practical NLP ap-
plications, such as machine translation (Wu et al.,
2010) and information extraction (Miyao et al.,
2008). Parsing in deep formalisms, e.g. CCG,
HPSG, LFG and TAG, provides valuable, richer
linguistic information, and researchers thus draw
more and more attention to it. Very recently, study
on deep linguistic processing for Chinese has been
initialized. Our work is one of them.
To quickly construct deep annotations, corpus-
driven grammar engineering has been studied.
Phrase structure trees in CTB have been semi-
automatically converted to deep derivations in the
CCG (Tse and Curran, 2010), LFG (Guo et al.,
2007), TAG (Xia, 2001) and HPSG (Yu et al.,
2010) formalisms. Our GR extraction work is sim-
ilar, but grounded in GB, which is more consistent
with the construction of the original annotations.
Based on converted fine-grained linguistic an-
notations, successful English deep parsers, such as
C&amp;C (Clark and Curran, 2007b) and Enju (Miyao
and Tsujii, 2008), have been evaluated (Yu et al.,
2011; Tse and Curran, 2012). We also borrow
many ideas from recent advances in deep syntac-
tic or semantic parsing for English. In particular,
Sagae and Tsujii (2008)’s and Titov et al. (2009)’s
studies on transition-based deep dependency pars-
ing motivated our work very much. However, sim-
ple adoption of their systems does not resolve Chi-
nese GR parsing well because the GR graphs are
much more complicated. Our investigation on the
K-permutation transition system advances the ca-
pacity of existing methods.
</bodyText>
<sectionHeader confidence="0.998003" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999912047619048">
Recent years witnessed rapid progress made on
deep linguistic processing for English, and ini-
tial attempts for Chinese. Our work stands in
between traditional dependency tree parsing and
deep linguistic processing. We introduced a sys-
tem for automatically extracting grammatical rela-
tions of Chinese sentences from GB phrase struc-
ture trees. The present work remedies the re-
source gap by facilitating the accurate extraction
of GR annotations from GB trees. Manual evalua-
tion demonstrate the effectiveness of our method.
With the availability of high-quality GR resources,
transition-based methods for GR parsing was stud-
ied. A new formal system, namely K-permutation
system, is well theoretically discussed and prac-
tically implemented as the core module of a deep
dependency parser. Empirical evaluation and anal-
ysis were presented to give better understanding of
the Chinese GR parsing problem. Detailed anal-
ysis reveals some important directions for future
investigation.
</bodyText>
<sectionHeader confidence="0.754326" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.991147666666667">
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&amp;D Program (2012AA011101).
</bodyText>
<sectionHeader confidence="0.87937" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.9803805">
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 397–408.
Association for Computational Linguistics, Edinburgh,
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1037.
</bodyText>
<page confidence="0.999036">
454
</page>
<reference confidence="0.996205496124031">
J. Bresnan and R. M. Kaplan. 1982. Introduction: Grammars
as mental representations of language. In J. Bresnan, edi-
tor, The Mental Representation of Grammatical Relations,
pages xvii–lii. MIT Press, Cambridge, MA.
Ted Briscoe and John Carroll. 2006. Evaluating the ac-
curacy of an unlexicalized statistical parser on the parc
depbank. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 41–48. Associ-
ation for Computational Linguistics, Sydney, Australia.
URL http://www.aclweb.org/anthology/P/
P06/P06-2006.
Andrew Carnie. 2007. Syntax: A Generative Introduction.
Blackwell Publishing, Blackwell Publishing 350 Main
Street, Malden, MA 02148-5020, USA, second edition.
Noam Chomsky. 1981. Lectures on Government and Binding.
Foris Publications, Dordecht.
Stephen Clark and James Curran. 2007a. Formalism-
independent parser evaluation with ccg and depbank. In
Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 248–255. Associa-
tion for Computational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/anthology/
P07-1032.
Stephen Clark and James R. Curran. 2007b. Wide-coverage
efficient statistical parsing with CCG and log-linear mod-
els. Comput. Linguist., 33(4):493–552. URL http://
dx.doi.org/10.1162/coli.2007.33.4.493.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JOURNAL OF MACHINE
LEARNING RESEARCH, 7:551–585.
M. Dalrymple. 2001. Lexical-Functional Grammar, vol-
ume 34 of Syntax and Semantics. Academic Press, New
York.
Yuqing Guo, Josef van Genabith, and Haifeng Wang. 2007.
Treebank-based acquisition of lfg resources for Chinese.
In Proceedings of the LFG07 Conference. CSLI Publica-
tions, California, USA.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
corpus of CCG derivations and dependency structures ex-
tracted from the penn treebank. Computational Linguis-
tics, 33(3):355–396.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic pars-
ing. In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages 97–
104. Association for Computational Linguistics, Boston,
Massachusetts, USA.
Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700
dependency bank. In In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Corpora
(LINC-03), pages 1–8.
Terry Koo and Michael Collins. 2010. Efficient third-order
dependency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 1–11. Association for Computational Linguistics,
Uppsala, Sweden. URL http://www.aclweb.org/
anthology/P10-1001.
Ryan McDonald. 2006. Discriminative learning and span-
ning tree algorithms for dependency parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Yusuke Miyao, Rune Sastre, Kenji Sagae, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2008. Task-oriented evalu-
ation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46–54. Associ-
ation for Computational Linguistics, Columbus, Ohio.
URL http://www.aclweb.org/anthology/P/
P08/P08-1006.
Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2007. To-
wards framework-independent evaluation of deep linguis-
tic parsers. In Ann Copestake, editor, Proceedings of
the GEAF 2007 Workshop, CSLI Studies in Computa-
tional Linguistics Online, page 21 pages. CSLI Publica-
tions. URL http://www.cs.cmu.edu/˜sagae/
docs/geaf07miyaoetal.pdf.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Comput. Lin-
guist., 34(1):35–80. URLhttp://dx.doi.org/10.
1162/coli.2008.34.1.35.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Comput. Linguist., 34:513–
553. URL http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 351–
359. Association for Computational Linguistics, Sun-
tec, Singapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1040.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-
based dependency parsing. In Hwee Tou Ng and Ellen
Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Con-
ference on Computational Natural Language Learning
(CoNLL-2004), pages 49–56. Association for Computa-
tional Linguistics, Boston, Massachusetts, USA.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The University of Chicago Press,
Chicago.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Un-
bounded dependency recovery for parser evaluation. In
Proceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 813–821.
Association for Computational Linguistics, Singapore.
URL http://www.aclweb.org/anthology/D/
D09/D09-1085.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce de-
pendency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguistics,
pages 753–760. Coling 2008 Organizing Committee,
Manchester, UK. URL http://www.aclweb.org/
anthology/C08-1095.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradig-
matic and syntagmatic lexical relations: Towards accu-
rate Chinese part-of-speech tagging. In Proceedings of the
50th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics.
Weiwei Sun and Xiaojun Wan. 2013. Data-driven, pcfg-based
and pseudo-pcfg-based models for Chinese dependency
parsing. Transactions of the Association for Computa-
tional Linguistics (TACL).
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970–979.
Association for Computational Linguistics, Edinburgh,
</reference>
<page confidence="0.987536">
455
</page>
<reference confidence="0.999809163043479">
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1090.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of the 21st international jont con-
ference on Artifical intelligence, pages 1562–1567. Mor-
gan Kaufmann Publishers Inc., San Francisco, CA, USA.
URL http://dl.acm.org/citation.cfm?id=
1661445.1661696.
Andre Torres Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342–350. Asso-
ciation for Computational Linguistics, Suntec, Singapore.
URL http://www.aclweb.org/anthology/P/
P09/P09-1039.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the penn Chi-
nese treebank. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling 2010),
pages 1083–1091. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-1122.
Daniel Tse and James R. Curran. 2012. The challenges
of parsing Chinese with combinatory categorial gram-
mar. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
pages 295–304. Association for Computational Linguis-
tics, Montr´eal, Canada. URL http://www.aclweb.
org/anthology/N12-1030.
Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages
325–334. Association for Computational Linguistics, Up-
psala, Sweden. URL http://www.aclweb.org/
anthology/P10-1034.
Fei Xia. 2001. Automatic grammar generation from two dif-
ferent perspectives. Ph.D. thesis, University of Pennsylva-
nia.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005. The penn Chinese treebank: Phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11:207–238. URL http://portal.acm.org/
citation.cfm?id=1064781.1064785.
Nianwen Xue. 2007. Tapping the implicit information for the
PS to DS conversion of the Chinese treebank. In Proceed-
ings of the Sixth International Workshop on Treebanks and
Linguistics Theories.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
The 8th International Workshop of Parsing Technologies
(IWPT2003), pages 195–206.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli Wang,
and Junichi Tsujii. 2011. Analysis of the difficul-
ties in Chinese deep parsing. In Proceedings of the
12th International Conference on Parsing Technologies,
pages 48–57. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.org/
anthology/W11-2907.
Kun Yu, Miyao Yusuke, Xiangli Wang, Takuya Matsuzaki,
and Junichi Tsujii. 2010. Semi-automatically devel-
oping Chinese hpsg grammar from the penn Chinese
treebank for deep parsing. In Coling 2010: Posters,
pages 1417–1425. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-2162.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global discrim-
inative model. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT’09),
pages 162–171. Association for Computational Linguis-
tics, Paris, France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 683–692.
Association for Computational Linguistics, Portland,
Oregon, USA. URL http://www.aclweb.org/
anthology/P11-1069.
Yue Zhang and Joakim Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technolo-
gies, pages 188–193. Association for Computational Lin-
guistics, Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
</reference>
<page confidence="0.999081">
456
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.766101">
<title confidence="0.9584175">Grammatical Relations in GB-Ground Extraction and Data-Driven Parsing</title>
<author confidence="0.873734">Yantao Du Sun</author>
<author confidence="0.873734">Xin Kou</author>
<author confidence="0.873734">Shuoyang Ding</author>
<author confidence="0.873734">Xiaojun</author>
<affiliation confidence="0.96432">Institute of Computer Science and Technology, Peking</affiliation>
<address confidence="0.951273">The MOE Key Laboratory of Computational Linguistics, Peking</address>
<abstract confidence="0.996883222222222">This paper is concerned with building linguistic resources and statistical parsers for deep grammatical relation (GR) analysis of Chinese texts. A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs. The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation. Based on the converted corpus, we study transition-based, datadriven models for GR parsing. We present a novel transition system which suits GR graphs better than existing systems. The key idea is to introduce a new of transition that reorders top in the memory module. Evaluation gauges how successful GR parsing for Chinese can be by applying datadriven models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bresnan</author>
<author>R M Kaplan</author>
</authors>
<title>Introduction: Grammars as mental representations of language. In</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>pages xvii–lii.</pages>
<editor>J. Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1651" citStr="Bresnan and Kaplan, 1982" startWordPosition="228" endWordPosition="232"> reorders top k elements in the memory module. Evaluation gauges how successful GR parsing for Chinese can be by applying datadriven models. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. They are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc. GRs have assumed an important role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to tra</context>
</contexts>
<marker>Bresnan, Kaplan, 1982</marker>
<rawString>J. Bresnan and R. M. Kaplan. 1982. Introduction: Grammars as mental representations of language. In J. Bresnan, editor, The Mental Representation of Grammatical Relations, pages xvii–lii. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the parc depbank.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>41--48</pages>
<location>Sydney, Australia. URL http://www.aclweb.org/anthology/P/</location>
<contexts>
<context position="2006" citStr="Briscoe and Carroll, 2006" startWordPosition="281" endWordPosition="284">tc. GRs have assumed an important role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed </context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the parc depbank. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41–48. Association for Computational Linguistics, Sydney, Australia. URL http://www.aclweb.org/anthology/P/ P06/P06-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carnie</author>
</authors>
<title>Syntax: A Generative Introduction.</title>
<date>2007</date>
<volume>350</volume>
<publisher>Blackwell Publishing, Blackwell Publishing</publisher>
<location>Main Street, Malden, MA 02148-5020, USA,</location>
<note>second edition.</note>
<contexts>
<context position="2316" citStr="Carnie, 2007" startWordPosition="330" endWordPosition="331">Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not only local but also various long-distance dependencies, such as coordinations, control/raising constructions, topicalization, relative clauses and many other complicated linguistic phenomena that goes beyond shallow syntax (see Fig. 1 for example.). Manual evaluation highlights the rel</context>
</contexts>
<marker>Carnie, 2007</marker>
<rawString>Andrew Carnie. 2007. Syntax: A Generative Introduction. Blackwell Publishing, Blackwell Publishing 350 Main Street, Malden, MA 02148-5020, USA, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding. Foris Publications,</booktitle>
<location>Dordecht.</location>
<contexts>
<context position="2301" citStr="Chomsky, 1981" startWordPosition="328" endWordPosition="329">rase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not only local but also various long-distance dependencies, such as coordinations, control/raising constructions, topicalization, relative clauses and many other complicated linguistic phenomena that goes beyond shallow syntax (see Fig. 1 for example.). Manual evaluation hig</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Noam Chomsky. 1981. Lectures on Government and Binding. Foris Publications, Dordecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
</authors>
<title>Formalismindependent parser evaluation with ccg and depbank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>248--255</pages>
<location>Prague, Czech Republic. URL http://www.aclweb.org/anthology/</location>
<contexts>
<context position="2030" citStr="Clark and Curran, 2007" startWordPosition="285" endWordPosition="289">ortant role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not </context>
<context position="25350" citStr="Clark and Curran, 2007" startWordPosition="4186" endWordPosition="4189">sers to find difficult nonlocal materials is far from satisfactory, even for English (Rimell et al., 2009; Bender et al., 2011). We report the accuracy in terms of local and nonlocal dependencies respectively to show the difficulty of the recovery of non-local dependencies. The last four columns of Tab. 3 demonstrates the labeled/unlabeled recall of local (URL/LRL) and non-local dependencies (URNL/LRNL). We can clearly see that non-local dependency recovery is extremely difficult for Chinese parsing. 4.5 Deep vs. Deep CCG and HPSG parsers also favor the dependencybased metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). Previous work on Chinese CCG and HPSG parsing unanimously agrees that obtaining the deep analysis of Chinese is more challenging (Yu et al., 2011; Tse and Curran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. 5 Related Work Wide-coverage in-depth and accurate linguistic</context>
<context position="26928" citStr="Clark and Curran, 2007" startWordPosition="4429" endWordPosition="4432">stic processing for Chinese has been initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. 6 Conclusion Recent ye</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James Curran. 2007a. Formalismindependent parser evaluation with ccg and depbank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255. Association for Computational Linguistics, Prague, Czech Republic. URL http://www.aclweb.org/anthology/ P07-1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>4</issue>
<note>URL http:// dx.doi.org/10.1162/coli.2007.33.4.493.</note>
<contexts>
<context position="2030" citStr="Clark and Curran, 2007" startWordPosition="285" endWordPosition="289">ortant role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not </context>
<context position="25350" citStr="Clark and Curran, 2007" startWordPosition="4186" endWordPosition="4189">sers to find difficult nonlocal materials is far from satisfactory, even for English (Rimell et al., 2009; Bender et al., 2011). We report the accuracy in terms of local and nonlocal dependencies respectively to show the difficulty of the recovery of non-local dependencies. The last four columns of Tab. 3 demonstrates the labeled/unlabeled recall of local (URL/LRL) and non-local dependencies (URNL/LRNL). We can clearly see that non-local dependency recovery is extremely difficult for Chinese parsing. 4.5 Deep vs. Deep CCG and HPSG parsers also favor the dependencybased metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). Previous work on Chinese CCG and HPSG parsing unanimously agrees that obtaining the deep analysis of Chinese is more challenging (Yu et al., 2011; Tse and Curran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. 5 Related Work Wide-coverage in-depth and accurate linguistic</context>
<context position="26928" citStr="Clark and Curran, 2007" startWordPosition="4429" endWordPosition="4432">stic processing for Chinese has been initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. 6 Conclusion Recent ye</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007b. Wide-coverage efficient statistical parsing with CCG and log-linear models. Comput. Linguist., 33(4):493–552. URL http:// dx.doi.org/10.1162/coli.2007.33.4.493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>JOURNAL OF MACHINE LEARNING RESEARCH,</journal>
<pages>7--551</pages>
<contexts>
<context position="18173" citStr="Crammer et al., 2006" startWordPosition="3016" endWordPosition="3020"> above theorem indicates the inadequacy of the ANFO deriving procedure. Nevertheless, empirical evaluation (Section 4.2) shows that the coverage of AFO and ANFO deriving procedures are almost identical when applying to linguistic data. 3.7 Statistical Parsing When we parse a sentence w1w2 · · · w,, we start with the initial configuration c0 = cs(x), and choose next transition tz = C(cz−1) iteratively according to a discriminative classifier trained on oracle sequences. To build a parser, we use a structured classifier to approximate the oracle, and apply the Passive-Aggressive (PA) algorithm (Crammer et al., 2006) for parameter estimation. The PA algorithm is similar to the Perceptron algorithm, the difference from which is the update of weight vector. We also use parameter averaging and early update to achieve better training. Developing features has been shown crucial to advancing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam s</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. JOURNAL OF MACHINE LEARNING RESEARCH, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dalrymple</author>
</authors>
<title>Lexical-Functional Grammar,</title>
<date>2001</date>
<volume>34</volume>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="1669" citStr="Dalrymple, 2001" startWordPosition="233" endWordPosition="234">n the memory module. Evaluation gauges how successful GR parsing for Chinese can be by applying datadriven models. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. They are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc. GRs have assumed an important role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Governmen</context>
</contexts>
<marker>Dalrymple, 2001</marker>
<rawString>M. Dalrymple. 2001. Lexical-Functional Grammar, volume 34 of Syntax and Semantics. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Josef van Genabith</author>
<author>Haifeng Wang</author>
</authors>
<title>Treebank-based acquisition of lfg resources for Chinese.</title>
<date>2007</date>
<booktitle>In Proceedings of the LFG07 Conference.</booktitle>
<publisher>CSLI Publications,</publisher>
<location>California, USA.</location>
<marker>Guo, van Genabith, Wang, 2007</marker>
<rawString>Yuqing Guo, Josef van Genabith, and Haifeng Wang. 2007. Treebank-based acquisition of lfg resources for Chinese. In Proceedings of the LFG07 Conference. CSLI Publications, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the penn treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="22773" citStr="Hockenmaier and Steedman, 2007" startWordPosition="3780" endWordPosition="3783">other reason is about the Chinese language. Some language-specific properties result in complex crossing arcs. For example, serial verb constructions are widely used in Chinese to describe several separate events without conjunctions. The verbal heads in such constructions share subjects and adjuncts, both of which are before the heads. The distributive dependencies between verbal heads and subjects/adjuncts usually produce crossing arcs (see Fig. 6). To test our assumption, we evaluate the coverage of S2 over the functor-argument dependency graphs provided by the English and Chinese CCGBank (Hockenmaier and Steedman, 2007; Tse and Curran, 2010). The result is 96.9% vs. 89.0%, which confirms our linguistic intuition under another grammar formalism. Tab. 3 summarizes the performance of the transition-based parser with different configurations to reveal how well data-driven parsing can subject adjunct verb1 verb2 Figure 6: A simplified example to illustrate crossing arcs in serial verbal constructions. be performed in realistic situations. We can see that with the increase of K, the overall parsing accuracy incrementally goes up. The high complexity of Chinese deep dependency structures demonstrates the importanc</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the penn treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell Alex Vasserman</author>
<author>Richard Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 97– 104. Association for Computational Linguistics,</booktitle>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="1979" citStr="Kaplan et al., 2004" startWordPosition="277" endWordPosition="280">ct/indirect object, etc. GRs have assumed an important role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are repre</context>
</contexts>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>Ron Kaplan, Stefan Riezler, Tracy H King, John T Maxwell III, Alex Vasserman, and Richard Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 97– 104. Association for Computational Linguistics, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Holloway King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC 700 dependency bank. In</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03),</booktitle>
<pages>1--8</pages>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700 dependency bank. In In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Uppsala,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="18518" citStr="Koo and Collins, 2010" startWordPosition="3072" endWordPosition="3075">cs(x), and choose next transition tz = C(cz−1) iteratively according to a discriminative classifier trained on oracle sequences. To build a parser, we use a structured classifier to approximate the oracle, and apply the Passive-Aggressive (PA) algorithm (Crammer et al., 2006) for parameter estimation. The PA algorithm is similar to the Perceptron algorithm, the difference from which is the update of weight vector. We also use parameter averaging and early update to achieve better training. Developing features has been shown crucial to advancing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. 4 Experiments 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun a</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11. Association for Computational Linguistics, Uppsala, Sweden. URL http://www.aclweb.org/ anthology/P10-1001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative learning and spanning tree algorithms for dependency parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="9269" citStr="McDonald, 2006" startWordPosition="1375" endWordPosition="1376"> shallow constituency and dependency parsers. With high-quality GR resources at hand, it is possible to study statistical approaches to automatically parse GR graphs. In this section, we investigate the feasibility of applying a datadriven, grammar-free approach to build GRs directly. In particular, transition-based dependency parsing method is studied. 3.1 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transit</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative learning and spanning tree algorithms for dependency parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Rune Sastre</author>
<author>Kenji Sagae</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>46--54</pages>
<location>Columbus, Ohio. URL http://www.aclweb.org/anthology/P/</location>
<contexts>
<context position="26105" citStr="Miyao et al., 2008" startWordPosition="4299" endWordPosition="4302"> is more challenging (Yu et al., 2011; Tse and Curran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. 5 Related Work Wide-coverage in-depth and accurate linguistic processing is desirable for many practical NLP applications, such as machine translation (Wu et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep formalisms, e.g. CCG, HPSG, LFG and TAG, provides valuable, richer linguistic information, and researchers thus draw more and more attention to it. Very recently, study on deep linguistic processing for Chinese has been initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is simila</context>
</contexts>
<marker>Miyao, Sastre, Sagae, Matsuzaki, Tsujii, 2008</marker>
<rawString>Yusuke Miyao, Rune Sastre, Kenji Sagae, Takuya Matsuzaki, and Jun’ichi Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In Proceedings of ACL-08: HLT, pages 46–54. Association for Computational Linguistics, Columbus, Ohio. URL http://www.aclweb.org/anthology/P/ P08/P08-1006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Towards framework-independent evaluation of deep linguistic parsers.</title>
<date>2007</date>
<booktitle>Proceedings of the GEAF 2007 Workshop, CSLI Studies in Computational Linguistics Online,</booktitle>
<pages>21</pages>
<editor>In Ann Copestake, editor,</editor>
<publisher>CSLI Publications. URL</publisher>
<note>http://www.cs.cmu.edu/˜sagae/ docs/geaf07miyaoetal.pdf.</note>
<contexts>
<context position="2052" citStr="Miyao et al., 2007" startWordPosition="290" endWordPosition="293"> theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not only local but also va</context>
</contexts>
<marker>Miyao, Sagae, Tsujii, 2007</marker>
<rawString>Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2007. Towards framework-independent evaluation of deep linguistic parsers. In Ann Copestake, editor, Proceedings of the GEAF 2007 Workshop, CSLI Studies in Computational Linguistics Online, page 21 pages. CSLI Publications. URL http://www.cs.cmu.edu/˜sagae/ docs/geaf07miyaoetal.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic hpsg parsing.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>10--1162</pages>
<contexts>
<context position="25376" citStr="Miyao and Tsujii, 2008" startWordPosition="4190" endWordPosition="4193">nlocal materials is far from satisfactory, even for English (Rimell et al., 2009; Bender et al., 2011). We report the accuracy in terms of local and nonlocal dependencies respectively to show the difficulty of the recovery of non-local dependencies. The last four columns of Tab. 3 demonstrates the labeled/unlabeled recall of local (URL/LRL) and non-local dependencies (URNL/LRNL). We can clearly see that non-local dependency recovery is extremely difficult for Chinese parsing. 4.5 Deep vs. Deep CCG and HPSG parsers also favor the dependencybased metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). Previous work on Chinese CCG and HPSG parsing unanimously agrees that obtaining the deep analysis of Chinese is more challenging (Yu et al., 2011; Tse and Curran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. 5 Related Work Wide-coverage in-depth and accurate linguistic processing is desirable f</context>
<context position="26964" citStr="Miyao and Tsujii, 2008" startWordPosition="4435" endWordPosition="4438">n initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. 6 Conclusion Recent years witnessed rapid progress made on</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. Comput. Linguist., 34(1):35–80. URLhttp://dx.doi.org/10. 1162/coli.2008.34.1.35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<booktitle>Comput. Linguist., 34:513– 553. URL http://dx.doi.org/10.1162/coli.</booktitle>
<pages>07--056</pages>
<contexts>
<context position="9237" citStr="Nivre, 2008" startWordPosition="1371" endWordPosition="1372">l approaches to build accurate shallow constituency and dependency parsers. With high-quality GR resources at hand, it is possible to study statistical approaches to automatically parse GR graphs. In this section, we investigate the feasibility of applying a datadriven, grammar-free approach to build GRs directly. In particular, transition-based dependency parsing method is studied. 3.1 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) ind</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Comput. Linguist., 34:513– 553. URL http://dx.doi.org/10.1162/coli. 07-056-R1-07-027.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>351--359</pages>
<contexts>
<context position="11886" citStr="Nivre (2009)" startWordPosition="1838" endWordPosition="1839">actions are performed sequentially to consume words from the queue and update the partial parsing results. 3.3 Online Reordering Among existing systems, Sagae and Tsujii’s is designed for projective graphs (denoted by G1 in Definition 1), and Titov et al.’s handles only a specific subset of non-projective graphs as well as projective graphs (G2). Applied to our data, only 51.0% and 76.5% of the extracted graphs are parsable with their systems. Obviously, it is necessary to investigate new transition systems for the parsing task in our study. To deal with crossing arcs, Titov et al. (2009) and Nivre (2009) designed a SWAP transition that switches the position of the two topmost nodes on the stack. Inspired by their work, we extend this approach to parse more general graphs. The basic idea is to provide our new system with an ability to reorder more nodes during decoding in an online fashion, which we refer to as online reordering. 3.4 K-Permutation System We define a K-permutation transition system SK = (C, T, cs, Ct), where a configuration c = 450 (Q, β, A) E C contains a stack Q of nodes besides β and A. We set the initial configuration for a sentence x = w1, ... , wn to be cs(x) = ([], [1, .</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351– 359. Association for Computational Linguistics, Suntec, Singapore. URL http://www.aclweb.org/ anthology/P/P09/P09-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memorybased dependency parsing.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng</booktitle>
<pages>49--56</pages>
<editor>and Ellen Riloff, editors,</editor>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="9621" citStr="Nivre et al., 2004" startWordPosition="1425" endWordPosition="1428">studied. 3.1 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 Transition Systems Following (Nivre, 2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs, Ct), where 1. C is a set of configurations, each of which contains a buffer β</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memorybased dependency parsing. In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 49–56. Association for Computational Linguistics, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="1740" citStr="Pollard and Sag, 1994" startWordPosition="241" endWordPosition="244">for Chinese can be by applying datadriven models. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. They are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc. GRs have assumed an important role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structu</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. The University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded dependency recovery for parser evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>813--821</pages>
<contexts>
<context position="24833" citStr="Rimell et al., 2009" startWordPosition="4106" endWordPosition="4109">rser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Local vs. Non-local Although the micro accuracy of all dependencies are considerably good, the ability of current stateof-the-art statistical parsers to find difficult nonlocal materials is far from satisfactory, even for English (Rimell et al., 2009; Bender et al., 2011). We report the accuracy in terms of local and nonlocal dependencies respectively to show the difficulty of the recovery of non-local dependencies. The last four columns of Tab. 3 demonstrates the labeled/unlabeled recall of local (URL/LRL) and non-local dependencies (URNL/LRNL). We can clearly see that non-local dependency recovery is extremely difficult for Chinese parsing. 4.5 Deep vs. Deep CCG and HPSG parsers also favor the dependencybased metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). Previous work on Chinese CCG and HPSG parsing unanimous</context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency recovery for parser evaluation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813–821. Association for Computational Linguistics, Singapore. URL http://www.aclweb.org/anthology/D/ D09/D09-1085.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Shift-reduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>753--760</pages>
<location>Manchester, UK.</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="3534" citStr="Sagae and Tsujii (2008)" startWordPosition="504" endWordPosition="507">e reliability of our linguistically-motivated GR extraction algorithm: The overall dependency-based precision and recall are 99.17 and 98.87. The automatically-converted corpus would be of use for a wide variety of NLP tasks. Recent years have seen the introduction of a number of treebank-guided statistical parsers capable of generating considerably accurate parses for Chinese. With the high-quality GR resource at hand, we study data-driven GR parsing. Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually studied two transition systems that can generate more general graphs rather than trees. Inspired by their work, we study transition-based models for building deep dependency structures. The existence of a large number of crossing arcs in GR graphs makes left-to-right, incremental graph spanning computationally hard. Applied to our data, the two existing systems cover only 51.0% and 76.5% GR graphs respectively. To better suit 446 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 446–456, Baltimore, Maryland, USA</context>
<context position="9809" citStr="Sagae and Tsujii (2008)" startWordPosition="1454" endWordPosition="1458">tionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 Transition Systems Following (Nivre, 2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs, Ct), where 1. C is a set of configurations, each of which contains a buffer β of (remaining) words and a set A of dependency arcs, Transitions SHIFT (σ,j|β,A) ⇒ (σ|j,β,A) LEFT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(j,l,i)}) RIGHT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(i,l,j)</context>
<context position="14166" citStr="Sagae and Tsujii, 2008" startWordPosition="2291" endWordPosition="2294">o the C operation. Definition 1. If a graph G can be parsed with transition system SK, we say G is a K-perm graph. We use !gK to denote the set of all k-perm graphs. Specially, !g0 = 0, !g1 is the set of all projective graphs, and !g∞ = U∞k=0 !gk. Theorem 1. !gi C !gi+1, bi &gt; 0. Proof. It is obvious that !gi C !gi+1 and !g0 C !g1. Fig. 4 gives an example which is in !gi+1 but not in !gi for all i &gt; 0, indicating !gi =� !gi+1. Theorem 2. !g∞ is the set of all graphs without self-loop. Proof. It follows immediately from the fact that G E !g|V |,bG = (V, E). The transition systems introduced in (Sagae and Tsujii, 2008) and (Titov et al., 2009) can be viewed as S11 and S2. 1Though Sagae and Tsujii (2008) introduced additional constraints to exclude cyclic path, the fundamental transition mechanism of their system is the same to S1. Figure 4: A graph which is in !gi+1, but not in !gi. 3.5 Normal Form Oracle The K-permutation transition system may allow multiple oracle transition sequences on one graph, but trying to sum all the possible oracles is usually computational expensive. Here we give a construction procedure which is guaranteed to find an oracle sequence if one exits. We refer it as normal form oracl</context>
<context position="27163" citStr="Sagae and Tsujii (2008)" startWordPosition="4469" endWordPosition="4472">o deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. 6 Conclusion Recent years witnessed rapid progress made on deep linguistic processing for English, and initial attempts for Chinese. Our work stands in between traditional dependency tree parsing and deep linguistic processing. We introduced a system for au</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce dependency DAG parsing. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 753–760. Coling 2008 Organizing Committee, Manchester, UK. URL http://www.aclweb.org/ anthology/C08-1095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19137" citStr="Sun and Uszkoreit, 2012" startWordPosition="3169" endWordPosition="3172"> 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. 4 Experiments 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. 4.2 Coverage and Accuracy There is a dual effect of the increase of the parameter k </context>
</contexts>
<marker>Sun, Uszkoreit, 2012</marker>
<rawString>Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Xiaojun Wan</author>
</authors>
<title>Data-driven, pcfg-based and pseudo-pcfg-based models for Chinese dependency parsing.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL).</journal>
<contexts>
<context position="19203" citStr="Sun and Wan, 2013" startWordPosition="3180" endWordPosition="3183">, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. 4 Experiments 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. 4.2 Coverage and Accuracy There is a dual effect of the increase of the parameter k to our transition-based dependency parser. On one hand, the higher</context>
</contexts>
<marker>Sun, Wan, 2013</marker>
<rawString>Weiwei Sun and Xiaojun Wan. 2013. Data-driven, pcfg-based and pseudo-pcfg-based models for Chinese dependency parsing. Transactions of the Association for Computational Linguistics (TACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Edinburgh,</location>
<contexts>
<context position="19098" citStr="Sun and Xu, 2011" startWordPosition="3163" endWordPosition="3166">y tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. 4 Experiments 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. 4.2 Coverage and Accuracy There is a dual eff</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 970–979. Association for Computational Linguistics, Edinburgh,</rawString>
</citation>
<citation valid="false">
<authors>
<author>UK Scotland</author>
</authors>
<pages>11--1090</pages>
<note>URL http://www.aclweb.org/</note>
<marker>Scotland, </marker>
<rawString>Scotland, UK. URL http://www.aclweb.org/ anthology/D11-1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence,</booktitle>
<pages>1562--1567</pages>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<note>URL http://dl.acm.org/citation.cfm?id=</note>
<contexts>
<context position="3558" citStr="Titov et al. (2009)" startWordPosition="509" endWordPosition="512">tically-motivated GR extraction algorithm: The overall dependency-based precision and recall are 99.17 and 98.87. The automatically-converted corpus would be of use for a wide variety of NLP tasks. Recent years have seen the introduction of a number of treebank-guided statistical parsers capable of generating considerably accurate parses for Chinese. With the high-quality GR resource at hand, we study data-driven GR parsing. Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually studied two transition systems that can generate more general graphs rather than trees. Inspired by their work, we study transition-based models for building deep dependency structures. The existence of a large number of crossing arcs in GR graphs makes left-to-right, incremental graph spanning computationally hard. Applied to our data, the two existing systems cover only 51.0% and 76.5% GR graphs respectively. To better suit 446 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 446–456, Baltimore, Maryland, USA, June 23-25 2014. c�201</context>
<context position="9833" citStr="Titov et al. (2009)" startWordPosition="1460" endWordPosition="1463">oto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 Transition Systems Following (Nivre, 2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs, Ct), where 1. C is a set of configurations, each of which contains a buffer β of (remaining) words and a set A of dependency arcs, Transitions SHIFT (σ,j|β,A) ⇒ (σ|j,β,A) LEFT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(j,l,i)}) RIGHT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(i,l,j)}) POP (σ|i, β, A) ⇒ (σ,</context>
<context position="11869" citStr="Titov et al. (2009)" startWordPosition="1833" endWordPosition="1836">. A set of SHIFT/REDUCE actions are performed sequentially to consume words from the queue and update the partial parsing results. 3.3 Online Reordering Among existing systems, Sagae and Tsujii’s is designed for projective graphs (denoted by G1 in Definition 1), and Titov et al.’s handles only a specific subset of non-projective graphs as well as projective graphs (G2). Applied to our data, only 51.0% and 76.5% of the extracted graphs are parsable with their systems. Obviously, it is necessary to investigate new transition systems for the parsing task in our study. To deal with crossing arcs, Titov et al. (2009) and Nivre (2009) designed a SWAP transition that switches the position of the two topmost nodes on the stack. Inspired by their work, we extend this approach to parse more general graphs. The basic idea is to provide our new system with an ability to reorder more nodes during decoding in an online fashion, which we refer to as online reordering. 3.4 K-Permutation System We define a K-permutation transition system SK = (C, T, cs, Ct), where a configuration c = 450 (Q, β, A) E C contains a stack Q of nodes besides β and A. We set the initial configuration for a sentence x = w1, ... , wn to be c</context>
<context position="14191" citStr="Titov et al., 2009" startWordPosition="2296" endWordPosition="2299"> 1. If a graph G can be parsed with transition system SK, we say G is a K-perm graph. We use !gK to denote the set of all k-perm graphs. Specially, !g0 = 0, !g1 is the set of all projective graphs, and !g∞ = U∞k=0 !gk. Theorem 1. !gi C !gi+1, bi &gt; 0. Proof. It is obvious that !gi C !gi+1 and !g0 C !g1. Fig. 4 gives an example which is in !gi+1 but not in !gi for all i &gt; 0, indicating !gi =� !gi+1. Theorem 2. !g∞ is the set of all graphs without self-loop. Proof. It follows immediately from the fact that G E !g|V |,bG = (V, E). The transition systems introduced in (Sagae and Tsujii, 2008) and (Titov et al., 2009) can be viewed as S11 and S2. 1Though Sagae and Tsujii (2008) introduced additional constraints to exclude cyclic path, the fundamental transition mechanism of their system is the same to S1. Figure 4: A graph which is in !gi+1, but not in !gi. 3.5 Normal Form Oracle The K-permutation transition system may allow multiple oracle transition sequences on one graph, but trying to sum all the possible oracles is usually computational expensive. Here we give a construction procedure which is guaranteed to find an oracle sequence if one exits. We refer it as normal form oracle (NFO). Let L(j) be the </context>
<context position="21273" citStr="Titov et al. (2009)" startWordPosition="3542" endWordPosition="3545">37 83.98 79.21 81.53 81.93 80.34 58.88 52.17 S3 89.0 89.0 86.02 81.72 83.82 84.07 79.86 81.91 82.61 80.94 60.46 54.28 S4 95.6 95.6 86.28 82.06 84.12 84.35 80.22 82.23 82.92 81.29 61.48 54.77 S5 98.4 98.4 86.44 82.21 84.27 84.51 80.37 82.39 83.15 81.51 59.80 53.30 Table 3: Coverage and accuracy of the GR parser on the development data. same, except for S3. Only 1 from 22277 sentences can find a NFO but not an ANFO. This number demonstrates the effectiveness of ANFO. In the following experiments, we use the ANFO’s to train our parser. Applied to our data, S2, i.e. the exact system introduced by Titov et al. (2009), only covers 76.5% GR graphs. This is very different from the result obtained on the CoNLL shared task data for English semantic role labeling (SRL). According to (Titov et al., 2009), 99% semantic-role-labelled graphs can be generated by S2. We think there are two main reasons accounting for the differences, and highlight the importance of the expressiveness of transition systems to solve deep dependency parsing problems. First, the SRL task only focuses on finding arguments and adjuncts of verbal (and nominal) predicates, while dependencies headed by other words are not contained in its gra</context>
<context position="27189" citStr="Titov et al. (2009)" startWordPosition="4474" endWordPosition="4477">(Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. 6 Conclusion Recent years witnessed rapid progress made on deep linguistic processing for English, and initial attempts for Chinese. Our work stands in between traditional dependency tree parsing and deep linguistic processing. We introduced a system for automatically extracting gra</context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the 21st international jont conference on Artifical intelligence, pages 1562–1567. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. URL http://dl.acm.org/citation.cfm?id= 1661445.1661696.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andre Torres Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>342--350</pages>
<contexts>
<context position="9299" citStr="Martins et al., 2009" startWordPosition="1379" endWordPosition="1382">nd dependency parsers. With high-quality GR resources at hand, it is possible to study statistical approaches to automatically parse GR graphs. In this section, we investigate the feasibility of applying a datadriven, grammar-free approach to build GRs directly. In particular, transition-based dependency parsing method is studied. 3.1 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andre Torres Martins, Noah Smith, and Eric Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 342–350. Association for Computational Linguistics, Suntec, Singapore. URL http://www.aclweb.org/anthology/P/ P09/P09-1039.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Tse</author>
<author>James R Curran</author>
</authors>
<title>Chinese CCGbank: extracting CCG derivations from the penn Chinese treebank.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1083--1091</pages>
<location>Beijing,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="6586" citStr="Tse and Curran, 2010" startWordPosition="975" endWordPosition="978">our knowledge, this work provides the first result of extensive experiments of parsing Chinese with GRs. We release our GR processing kit and goldstandard annotations for research purposes. These resources can be downloaded at http://www. icst.pku.edu.cn/lcwm/omg. 2 GB-grounded GR Extraction In this section, we discuss the construction of the GR annotations. Basically, the annotations are automatically converted from a GB-grounded phrasestructure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep grammar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB, which is the linguistic basis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, extraction, topicalization, and many other complicated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. 2.1 Linguistic Basis GRs are encoded in differe</context>
<context position="22796" citStr="Tse and Curran, 2010" startWordPosition="3784" endWordPosition="3787">e language. Some language-specific properties result in complex crossing arcs. For example, serial verb constructions are widely used in Chinese to describe several separate events without conjunctions. The verbal heads in such constructions share subjects and adjuncts, both of which are before the heads. The distributive dependencies between verbal heads and subjects/adjuncts usually produce crossing arcs (see Fig. 6). To test our assumption, we evaluate the coverage of S2 over the functor-argument dependency graphs provided by the English and Chinese CCGBank (Hockenmaier and Steedman, 2007; Tse and Curran, 2010). The result is 96.9% vs. 89.0%, which confirms our linguistic intuition under another grammar formalism. Tab. 3 summarizes the performance of the transition-based parser with different configurations to reveal how well data-driven parsing can subject adjunct verb1 verb2 Figure 6: A simplified example to illustrate crossing arcs in serial verbal constructions. be performed in realistic situations. We can see that with the increase of K, the overall parsing accuracy incrementally goes up. The high complexity of Chinese deep dependency structures demonstrates the importance of the expressiveness</context>
<context position="26592" citStr="Tse and Curran, 2010" startWordPosition="4374" endWordPosition="4377">ble for many practical NLP applications, such as machine translation (Wu et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep formalisms, e.g. CCG, HPSG, LFG and TAG, provides valuable, richer linguistic information, and researchers thus draw more and more attention to it. Very recently, study on deep linguistic processing for Chinese has been initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s </context>
</contexts>
<marker>Tse, Curran, 2010</marker>
<rawString>Daniel Tse and James R. Curran. 2010. Chinese CCGbank: extracting CCG derivations from the penn Chinese treebank. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1083–1091. Coling 2010 Organizing Committee, Beijing, China. URL http://www.aclweb.org/ anthology/C10-1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Tse</author>
<author>James R Curran</author>
</authors>
<title>The challenges of parsing Chinese with combinatory categorial grammar.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>295--304</pages>
<location>Montr´eal, Canada. URL http://www.aclweb.</location>
<contexts>
<context position="25546" citStr="Tse and Curran, 2012" startWordPosition="4218" endWordPosition="4222">espectively to show the difficulty of the recovery of non-local dependencies. The last four columns of Tab. 3 demonstrates the labeled/unlabeled recall of local (URL/LRL) and non-local dependencies (URNL/LRNL). We can clearly see that non-local dependency recovery is extremely difficult for Chinese parsing. 4.5 Deep vs. Deep CCG and HPSG parsers also favor the dependencybased metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). Previous work on Chinese CCG and HPSG parsing unanimously agrees that obtaining the deep analysis of Chinese is more challenging (Yu et al., 2011; Tse and Curran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. 5 Related Work Wide-coverage in-depth and accurate linguistic processing is desirable for many practical NLP applications, such as machine translation (Wu et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep formalisms, e.g. CCG, H</context>
<context position="27025" citStr="Tse and Curran, 2012" startWordPosition="4446" endWordPosition="4449">ep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. 6 Conclusion Recent years witnessed rapid progress made on deep linguistic processing for English, and initial attempts</context>
</contexts>
<marker>Tse, Curran, 2012</marker>
<rawString>Daniel Tse and James R. Curran. 2012. The challenges of parsing Chinese with combinatory categorial grammar. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 295–304. Association for Computational Linguistics, Montr´eal, Canada. URL http://www.aclweb. org/anthology/N12-1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Fine-grained tree-to-string translation rule extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>325--334</pages>
<location>Uppsala,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="26057" citStr="Wu et al., 2010" startWordPosition="4292" endWordPosition="4295">s that obtaining the deep analysis of Chinese is more challenging (Yu et al., 2011; Tse and Curran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. 5 Related Work Wide-coverage in-depth and accurate linguistic processing is desirable for many practical NLP applications, such as machine translation (Wu et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep formalisms, e.g. CCG, HPSG, LFG and TAG, provides valuable, richer linguistic information, and researchers thus draw more and more attention to it. Very recently, study on deep linguistic processing for Chinese has been initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 20</context>
</contexts>
<marker>Wu, Matsuzaki, Tsujii, 2010</marker>
<rawString>Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii. 2010. Fine-grained tree-to-string translation rule extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325–334. Association for Computational Linguistics, Uppsala, Sweden. URL http://www.aclweb.org/ anthology/P10-1034.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Automatic grammar generation from two different perspectives.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6633" citStr="Xia, 2001" startWordPosition="987" endWordPosition="988">ensive experiments of parsing Chinese with GRs. We release our GR processing kit and goldstandard annotations for research purposes. These resources can be downloaded at http://www. icst.pku.edu.cn/lcwm/omg. 2 GB-grounded GR Extraction In this section, we discuss the construction of the GR annotations. Basically, the annotations are automatically converted from a GB-grounded phrasestructure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep grammar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB, which is the linguistic basis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, extraction, topicalization, and many other complicated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. 2.1 Linguistic Basis GRs are encoded in different ways in different languages. In some languag</context>
<context position="26633" citStr="Xia, 2001" startWordPosition="4384" endWordPosition="4385">hine translation (Wu et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep formalisms, e.g. CCG, HPSG, LFG and TAG, provides valuable, richer linguistic information, and researchers thus draw more and more attention to it. Very recently, study on deep linguistic processing for Chinese has been initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependen</context>
</contexts>
<marker>Xia, 2001</marker>
<rawString>Fei Xia. 2001. Automatic grammar generation from two different perspectives. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-dong Chiou</author>
<author>Marta Palmer</author>
</authors>
<title>The penn Chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering,</title>
<date>2005</date>
<note>URL http://portal.acm.org/ citation.cfm?id=1064781.1064785.</note>
<contexts>
<context position="2398" citStr="Xue et al., 2005" startWordPosition="340" endWordPosition="343">n particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗Email correspondence. In this paper, we address the question of analyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are explicitly represented. Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not only local but also various long-distance dependencies, such as coordinations, control/raising constructions, topicalization, relative clauses and many other complicated linguistic phenomena that goes beyond shallow syntax (see Fig. 1 for example.). Manual evaluation highlights the reliability of our linguistically-motivated GR extraction algorithm: The overall depe</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The penn Chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11:207–238. URL http://portal.acm.org/ citation.cfm?id=1064781.1064785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Tapping the implicit information for the PS to DS conversion of the Chinese treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixth International Workshop on Treebanks and Linguistics Theories.</booktitle>
<marker>Xue, 2007</marker>
<rawString>Nianwen Xue. 2007. Tapping the implicit information for the PS to DS conversion of the Chinese treebank. In Proceedings of the Sixth International Workshop on Treebanks and Linguistics Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In The 8th International Workshop of Parsing Technologies (IWPT2003),</booktitle>
<pages>195--206</pages>
<contexts>
<context position="9223" citStr="Yamada and Matsumoto, 2003" startWordPosition="1367" endWordPosition="1370">the blossoming of statistical approaches to build accurate shallow constituency and dependency parsers. With high-quality GR resources at hand, it is possible to study statistical approaches to automatically parse GR graphs. In this section, we investigate the feasibility of applying a datadriven, grammar-free approach to build GRs directly. In particular, transition-based dependency parsing method is studied. 3.1 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In The 8th International Workshop of Parsing Technologies (IWPT2003), pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Yusuke Miyao</author>
<author>Takuya Matsuzaki</author>
<author>Xiangli Wang</author>
<author>Junichi Tsujii</author>
</authors>
<title>Analysis of the difficulties in Chinese deep parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>48--57</pages>
<location>Dublin,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="25523" citStr="Yu et al., 2011" startWordPosition="4214" endWordPosition="4217">al dependencies respectively to show the difficulty of the recovery of non-local dependencies. The last four columns of Tab. 3 demonstrates the labeled/unlabeled recall of local (URL/LRL) and non-local dependencies (URNL/LRNL). We can clearly see that non-local dependency recovery is extremely difficult for Chinese parsing. 4.5 Deep vs. Deep CCG and HPSG parsers also favor the dependencybased metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). Previous work on Chinese CCG and HPSG parsing unanimously agrees that obtaining the deep analysis of Chinese is more challenging (Yu et al., 2011; Tse and Curran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. 5 Related Work Wide-coverage in-depth and accurate linguistic processing is desirable for many practical NLP applications, such as machine translation (Wu et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep </context>
<context position="27002" citStr="Yu et al., 2011" startWordPosition="4442" endWordPosition="4445">ckly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. 6 Conclusion Recent years witnessed rapid progress made on deep linguistic processing for Englis</context>
</contexts>
<marker>Yu, Miyao, Matsuzaki, Wang, Tsujii, 2011</marker>
<rawString>Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli Wang, and Junichi Tsujii. 2011. Analysis of the difficulties in Chinese deep parsing. In Proceedings of the 12th International Conference on Parsing Technologies, pages 48–57. Association for Computational Linguistics, Dublin, Ireland. URL http://www.aclweb.org/ anthology/W11-2907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Miyao Yusuke</author>
<author>Xiangli Wang</author>
<author>Takuya Matsuzaki</author>
<author>Junichi Tsujii</author>
</authors>
<title>Semi-automatically developing Chinese hpsg grammar from the penn Chinese treebank for deep parsing.</title>
<date>2010</date>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>1417--1425</pages>
<location>Beijing,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="6603" citStr="Yu et al., 2010" startWordPosition="979" endWordPosition="982">rk provides the first result of extensive experiments of parsing Chinese with GRs. We release our GR processing kit and goldstandard annotations for research purposes. These resources can be downloaded at http://www. icst.pku.edu.cn/lcwm/omg. 2 GB-grounded GR Extraction In this section, we discuss the construction of the GR annotations. Basically, the annotations are automatically converted from a GB-grounded phrasestructure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep grammar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB, which is the linguistic basis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, extraction, topicalization, and many other complicated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. 2.1 Linguistic Basis GRs are encoded in different ways in differ</context>
<context position="26660" citStr="Yu et al., 2010" startWordPosition="4388" endWordPosition="4391">et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep formalisms, e.g. CCG, HPSG, LFG and TAG, provides valuable, richer linguistic information, and researchers thus draw more and more attention to it. Very recently, study on deep linguistic processing for Chinese has been initialized. Our work is one of them. To quickly construct deep annotations, corpusdriven grammar engineering has been studied. Phrase structure trees in CTB have been semiautomatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is similar, but grounded in GB, which is more consistent with the construction of the original annotations. Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our wo</context>
</contexts>
<marker>Yu, Yusuke, Wang, Matsuzaki, Tsujii, 2010</marker>
<rawString>Kun Yu, Miyao Yusuke, Xiangli Wang, Takuya Matsuzaki, and Junichi Tsujii. 2010. Semi-automatically developing Chinese hpsg grammar from the penn Chinese treebank for deep parsing. In Coling 2010: Posters, pages 1417–1425. Coling 2010 Organizing Committee, Beijing, China. URL http://www.aclweb.org/ anthology/C10-2162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the Chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>162--171</pages>
<location>Paris,</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="19183" citStr="Zhang and Clark, 2009" startWordPosition="3176" endWordPosition="3179">deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. 4 Experiments 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. 4.2 Coverage and Accuracy There is a dual effect of the increase of the parameter k to our transition-based dependency parser. On </context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the Chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 162–171. Association for Computational Linguistics, Paris, France. URL http://www.aclweb.org/ anthology/W09-3825.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce CCG parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>683--692</pages>
<location>Portland, Oregon, USA.</location>
<note>URL http://www.aclweb.org/</note>
<contexts>
<context position="24046" citStr="Zhang and Clark, 2011" startWordPosition="3983" endWordPosition="3986">le the improved numeric accuracies practically certify the benefits. The two points merit further exploration to more expressive transition systems for deep dependency parsing, at least for Chinese. The labeled evaluation scores on the final test data are presented in Tab. 4. Test UP UR UF LRL LRNL S5 83.93 79.82 81.82 80.94 54.38 Table 4: Performance on the test data. 4.3 Precision vs. Recall A noteworthy thing about the overall performance is that the precision is promising but the recall is too low behind. This difference is consistent with the result obtained by a shift-reduce CCG parser (Zhang and Clark, 2011). The functor-argument dependencies generated by that parser also has a relatively high precision but considerably low recall. There are two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Local vs. Non-local Although the micro accuracy of all dependen</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Shift-reduce CCG parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 683–692. Association for Computational Linguistics, Portland, Oregon, USA. URL http://www.aclweb.org/ anthology/P11-1069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<location>Portland, Oregon, USA.</location>
<note>URL http://www.</note>
<contexts>
<context position="18542" citStr="Zhang and Nivre, 2011" startWordPosition="3076" endWordPosition="3079">transition tz = C(cz−1) iteratively according to a discriminative classifier trained on oracle sequences. To build a parser, we use a structured classifier to approximate the oracle, and apply the Passive-Aggressive (PA) algorithm (Crammer et al., 2006) for parameter estimation. The PA algorithm is similar to the Perceptron algorithm, the difference from which is the update of weight vector. We also use parameter averaging and early update to achieve better training. Developing features has been shown crucial to advancing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. 4 Experiments 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193. Association for Computational Linguistics, Portland, Oregon, USA. URL http://www. aclweb.org/anthology/P11-2033.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>