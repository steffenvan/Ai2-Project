<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013860">
<title confidence="0.989487">
Samsung: Align-and-Differentiate Approach to Semantic Textual Similarity
</title>
<author confidence="0.98176">
Lushan Han, Justin Martineau, Doreen Cheng and Christopher Thomas
</author>
<affiliation confidence="0.931671">
Samsung Research America
</affiliation>
<address confidence="0.912304">
665 Clyde Avenue
Mountain View, CA 94043, USA
</address>
<email confidence="0.991199">
{lushan.han, justin.m, doreen.c, c2.thomas}@samsung.com
</email>
<sectionHeader confidence="0.998543" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941928571428">
This paper describes our Align-and-
Differentiate approach to the SemEval
2015 Task 2 competition for English Seman-
tic Textual Similarity (STS) systems. Our
submission achieved the top place on two
of the five evaluation datasets. Our team
placed 3rd among 28 participating teams,
and our three runs ranked 4th, 6th and 7th
among the 73 runs submitted by the 28
teams. Our approach improves upon the
UMBC PairingWords system by semantically
differentiating distributionally similar terms.
This novel addition improves results by 2.5
points on the Pearson correlation measure.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988156">
Since its inception in 2012, the annual Semantic
Textual Similarity (STS) task has attracted and in-
creasing amount of interest in the NLP community.
The task is to measure the semantic similarity be-
tween two sentences using a scale ranging from 0
to 5 (Agirre et al., 2012; Agirre et al., 2013; Agir-
rea et al., 2014). In this task, 0 means unrelated
and 5 means complete semantic equivalence. For
example, the sentence “China’s new PM rejects US
hacking claims” is semantically equivalent to the
sentence “China Premier Li rejects ‘groundless’ US
hacking accusations” even though there are many
word level differences between the two sentences.
Improvements in the STS task can advance or
benefit many research areas, such as paraphrase
recognition (Dolan et al., 2004), automatic ma-
chine translation evaluation (Kauchak and Barzi-
lay, 2006), ontology mapping and schema matching
(Han, 2014), Twitter search (Sriram et al., 2010), im-
age retrieval by captions (Coelho et al., 2004) and
information retrieval in general.
Measuring semantic similarity is difficult because
it is relatively easy to express the same idea in very
different ways. Both word choice and word order
can have a great impact on the semantics of a sen-
tence, or not at all. For example, the sentences “A
woman is playing piano on the street” and “A lady
is playing violin on the street” have a semantic sim-
ilarity score of only 2, because pianos are not vio-
lins so the two events in the sentences must be dif-
ferent. This is problematic because common solu-
tions, such as bag-of-words representations, parse
trees, and word alignments measure word choice and
word order. We improve upon existing word choice
approaches with better measures to semantically dif-
ferentiate distributionally similar terms, and by us-
ing these measures to also improve the word align-
ment.
Our solution is an Align-and-Differentiate ap-
proach, in which we greedily align words between
sentences, before penalizing non-matching words
in the differentiate-phase. Our system improves
upon the successful UMBC PairingWords system
by about 2 points of Pearson’s Correlation measure.
The success of the PairingWords system is largely
due to their high-quality distributional word similar-
ity model1 described in (Han et al., 2013). The dis-
tributional similarity model can tell that “woman”
and “lady” in the above example are highly similar,
which is usually correct, but it also says that “pi-
</bodyText>
<footnote confidence="0.997967">
1See http://swoogle.umbc.edu/SimService/
for a demo.
</footnote>
<page confidence="0.935806">
172
</page>
<bodyText confidence="0.93234">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 172–177,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
ano” and “violin” are very similar, which in many
contexts is incorrect. While distributional similarity
measures can be criticized for producing high simi-
larity scores for antonyms and contrasting words, we
find that this property is actually advantageous when
performing word alignment between two sentences.
We take advantage of this property by first aligning
with distributional similarity, and then differentiate
by penalizing alignments of words that are semanti-
cally disjoint (Ex: antonyms). This technique to first
align and then differentiate is our key improvement.
The remainder of the paper proceeds as follows.
Section 2 briefly revisits the UMBC PairingWords
system. Section 3 presents our new Align-and-
Differentiate approach. Section 4 presents and dis-
cusses our results.
</bodyText>
<sectionHeader confidence="0.995627" genericHeader="method">
2 UMBC PairingWords System
</sectionHeader>
<bodyText confidence="0.9916416">
The PairingWords system (Han et al., 2013) uses
a state-of-the-art word similarity measure to align
words in the sentence pair and computes the STS
score using a simple metric that combines individ-
ual term alignment scores.
</bodyText>
<figureCaption confidence="0.998062">
Figure 1: Overview of UMBC PairingWords system.
</figureCaption>
<subsectionHeader confidence="0.996505">
2.1 Precompute Word Similarities
</subsectionHeader>
<bodyText confidence="0.997792">
First, a distributional model was built on an En-
glish corpus2 of three-billion words and separated
</bodyText>
<footnote confidence="0.9964885">
2The UMBC WebBase corpus is available for download at
http://ebiq.org/r/351
</footnote>
<bodyText confidence="0.9997720625">
into paragraphs. Words are POS tagged and lem-
matized. A small context window of ±4 words is
used to count word co-occurrences. The vocabulary
has a size of 29,000 terms, which includes primar-
ily open-class words (i.e. nouns, verbs, adjectives
and adverbs). Singular Value Decomposition (SVD)
(Landauer and Dumais, 1997; Burgess et al., 1998)
has been used to reduce the 29K word vectors to 300
dimensions. The distributional similarity between
two words is measured by the cosine similarity of
their corresponding reduced word vectors. The dis-
tributional similarity is then enhanced with WordNet
(Fellbaum, 1998) relations in eight categories (See
(Han et al., 2013)). Finally it is wrapped with sur-
face similarity modules to handle the matching of
out-of-vocabulary words.
</bodyText>
<subsectionHeader confidence="0.994276">
2.2 NLP Pipeline
</subsectionHeader>
<bodyText confidence="0.9999005">
The Stanford POS tagger is applied to tag and lem-
matize the input sentences. A predefined vocab-
ulary, POS tags, and regular expressions are used
to recognize multi-word terms including noun and
verb phrases, proper nouns, numbers and time.
Stop words are ignored. The stop word list was
augmented with adverbs that occurred more than
500, 000 times in the corpus.
</bodyText>
<subsectionHeader confidence="0.999377">
2.3 Word Alignment Between Two Sentences
</subsectionHeader>
<bodyText confidence="0.99999175">
The alignment function g for a target word w in one
sentence 5 is simply defined as its most similar word
w&apos; in the other sentence 5&apos; with respect to the afore-
mentioned word similarity measure. See Equation 1.
</bodyText>
<equation confidence="0.922779">
g(w) = argmax sim(w, w&apos;) (1)
WIES/
</equation>
<subsectionHeader confidence="0.986542">
2.4 Score
</subsectionHeader>
<bodyText confidence="0.999985363636363">
The PairingWords systems yield an STS score in the
range [0, 1] with a linearly scaled definition corre-
sponding to the standard STS score. This score is
computed using the word level semantic similarity
of the aligned words. The PairingWords system uses
a similarity threshold to decide whether a term can
be aligned. If a term cannot be aligned then a penalty
is imposed. Therefore, the PairingWords STS score
is the result of subtracting the penalty score P from
the overall term alignment score T, which is defined
in Equation 2.
</bodyText>
<page confidence="0.994366">
173
</page>
<bodyText confidence="0.9996625">
where S1 and S2 are the sets of words/terms in two
input sentences.
</bodyText>
<sectionHeader confidence="0.990279" genericHeader="method">
3 Align-and-Differentiate Approach
</sectionHeader>
<bodyText confidence="0.999751666666667">
Our system extends the UMBC PairingWords sys-
tem by differentiating distributionally similar terms,
resulting in a conceptually new framework to tackle
the STS challenge. Figure 2 illustrates our system.
After preprocessing there are four main algorithms:
align, differentiate, score, and rescore.
</bodyText>
<figureCaption confidence="0.833080666666667">
Figure 2: Our system overview. Blue components (Dif-
ferentiate and Rescore) mark the most novel additions to
the PairingWords system.
</figureCaption>
<subsectionHeader confidence="0.999478">
3.1 Precompute Word Similarities
</subsectionHeader>
<bodyText confidence="0.986131">
We reused the distributional model built for the
UMBC PairingWords system.
</bodyText>
<subsectionHeader confidence="0.995182">
3.2 NLP Pipeline
</subsectionHeader>
<bodyText confidence="0.999256">
In addition to the basic NLP techniques used by the
PairingWords in Section 2.2 we use the Stanford de-
pendency parser to translate the input sentences into
their dependency graph representation.
</bodyText>
<subsectionHeader confidence="0.998711">
3.3 Word Alignment Between Two Sentences
</subsectionHeader>
<bodyText confidence="0.999592902439025">
For alignment we upgraded the PairingWords ap-
proach (see Equation 1) with candidate disambigua-
tion. If multiple candidates (ambiguity) exist, we
use their neighboring words in the sentences and de-
pendency graphs to carry out disambiguation. For
two mapping candidates, we found their neighbor-
ing words in terms of dependency relations. Then
we choose the candidate with the highest neighbor
similarity. This alignment method is directional. In
domains for which we have high confidence that
the dependency parser will correctly parse both sen-
tences, we require mutual agreement in both direc-
tions. Mutual alignment is computed by finding g
such that g(w) = w&apos; and g(w&apos;) = w.
The similarity function sim(w, w&apos;) is the word
similarity function described in Section 2.1.
Following the PairingWords system, we use a
similarity threshold of .05 to determine whether a
vocabulary word3 has at least some minimum sim-
ilarity with any of the words in the other sentence.
We call a word Out Of Context (OOC) if the thresh-
old is not satisfied. The appearance of OOC words
could be an indicator of different sentence seman-
tics, as illustrated in the example “A beautiful red
car” vs. “A beautiful red rose” where “car” is an
OOC word with respect to the other sentence. The
impact of OOC words to semantic equivalence is
disproportionately high. Therefore, we penalize se-
mantic similarity scores in proportion to the number
of OOC words.
However, we observed that if OOC words oc-
cur because there are additional details, then these
words should not be penalized. For example, in the
two sentences “Matt Smith to leave Doctor Who af-
ter 4 years” and “Matt Smith quits Doctor Who”, the
word ‘year’ is an OOC word that does not signifi-
cantly reduce the semantic equivalence. We found
that many of these extraneous and benign OOC
words do not represent physical objects, i.e. some-
thing that can be touched. Hence, we chose to
only penalize OOC words that are physical objects.
</bodyText>
<footnote confidence="0.9522115">
3A vocabulary word means a word in our vocabulary of 29k
words
</footnote>
<equation confidence="0.6503255">
� sim(t, g(t)) � sim(t, g(t))
tES1 2 IS11 + tES2
T= (2)
2 � �S2�
</equation>
<page confidence="0.987699">
174
</page>
<bodyText confidence="0.997724">
WordNet has a synset physical objects and we use
its descendants to collect the set of physical objects.
</bodyText>
<subsectionHeader confidence="0.882425">
3.4 Differentiate
</subsectionHeader>
<bodyText confidence="0.999976444444445">
This subsection defines and then describes how we
identify Disjoint Similar Concepts.
The semantic similarity of two words is the degree
of semantic equivalence between the two words. We
may also say, it is the ability to substitute one term
for the other without changing the meaning of a sen-
tence.
Many distributionally similar terms are not se-
mantically similar. Examples include “good” vs
“bad”, “cat” vs “dog”, “Thuesday” vs “Monday”,
“France” vs “England” and etc. Existing research
on distributional models has mainly been focused on
studying antonyms or contrasting words (Moham-
mad et al., 2008; Scheible et al., 2013; Mohammad
et al., 2013). However, as shown by the above exam-
ples, the scope of distributionally similar but not se-
mantically similar terms goes far beyond antonyms.
Hereafter, we refer to this new category of terms as
</bodyText>
<subsubsectionHeader confidence="0.50741">
Disjoint Similar Concepts (DSCs).
</subsubsectionHeader>
<bodyText confidence="0.999784458333334">
To the best of our knowledge, collecting Disjoint
Similar Concepts is a novel research problem. Gen-
eral statistical methods are not easily available, but
we can extract such information from human-crafted
ontologies, such as WordNet. For this work, we
identify Disjoint Similar Concepts as siblings under
a common parent in an ontology, such as WordNet.
For example, in the electronics domain, we can as-
sert that smart phone and tablet are DSCs if they are
siblings with the same parent electronics in the on-
tology.
We use a semi-automatic method to produce sev-
eral sets of potential DSCs for our STS system. The
sets include animals, countries, vehicles, weekdays,
colors and etc. First, we decide what types of DSCs
are likely to appear in a dataset. For example, an-
imals and vehicles will likely appear in the images
training dataset.
We penalize each aligned word pair that has Dis-
joint Similar Concepts. If both words are antonyms
then they are DSCs. If both words share the same
hypernym in WordNet, and that hypernym is a po-
tential DSC, then they are DSCs. Otherwise, the
concepts are considered semantically similar.
</bodyText>
<subsectionHeader confidence="0.886419">
3.5 Score
</subsectionHeader>
<bodyText confidence="0.998757">
We create a base similarity score Ei, and then ap-
ply penalties for OOC words Oi and Disjoint Similar
Concepts Di.
</bodyText>
<equation confidence="0.99974">
Ti = i E 11, 21 (3)
2 � �Si�
Ei =
(t,g(t))ESSi
�Oi = α(t) i E 11, 21 (5)
tEOOCi
�Di = β((t, g(t))) i E 11, 21 (6)
(t,g(t))EDSCi
STS = T1 + T2 (7)
</equation>
<bodyText confidence="0.999970142857143">
Our primary method of producing the STS score
is shown in Equations 3 to 7. The method is based
on the directional alignment function described in
Section 3.3. Ei is the base score where i indicates
the alignment direction and SSi represents the col-
lection of pairs of semantically similar terms for di-
rection i. Oi is the sum of penalties applied to OOC
terms for direction i. In our current system, the func-
tion α(t) has a constant value 1.0. Di is the sum of
penalties applied to Disjoint Similar Concepts for di-
rection i. We normally set β((t, g(t))) to 0.5 but we
can also tune β coefficient depending on different
types of Disjoint Similar Concepts (e.g.animal and
color), if a training dataset is available.
</bodyText>
<subsectionHeader confidence="0.997563">
3.6 Rescore by Learning STS Offset Scores
</subsectionHeader>
<bodyText confidence="0.999269785714286">
We learn an offset score to account for and correct
systemic biases in the Align and Differentiate algo-
rithm using supervised machine learning. For do-
mains with labeled data we used bag-of-words Sup-
port Vector Machines (SVMs) in regression mode,
with a linear kernel, to compute an offset score
measuring the difference between our Equation 7
STS score and the gold standard training STS score.
We add this offset score to the Equation 7 STS
score. This process improved our Pearson Corre-
lation scores from .7936 to .8162 on the 2014 STS
data in a ten fold cross-validation setting.
The SVM was trained on a length normalized
bag-of-words with additional non-normalized meta
</bodyText>
<equation confidence="0.5023465">
Ei − Oi − Di
sim(t, g(t)) i E 11, 21 (4)
</equation>
<page confidence="0.718924">
175
</page>
<table confidence="0.999766">
Dataset alpha beta delta
headlines (750 pairs) 0.8342 (2) 0.8342 0.8417 (1)
images (750 pairs) 0.8701 (2) 0.8713 (1) 0.8634
students (750 pairs) 0.7827 (2) 0.7819 0.7825
forums (375 pairs) 0.6589 0.6586 0.6639
belief (375 pairs) 0.7029 0.6995 0.6952
weighted mean 0.7920 (4) 0.7916 (7) 0.7918 (6)
</table>
<tableCaption confidence="0.988438">
Table 1: Pearson correlation and STS 2015 Competition
Rank of our three runs on test sets.
</tableCaption>
<bodyText confidence="0.9988225">
features for (1) the length difference between sen-
tence pairs, (2) the percentage of exact word to word
matches between both sentences, and (3) the STS
score produced in Equation 7. The bag-of-words
feature values were calculated by taking the abso-
lute value of the difference between the number of
times a word occurred in the first sentence versus
the number of occurrences in the paired sentence.
The bag-of-words was created with both words and
bi-gram word sequences.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999905148148148">
Table 1 shows the official results of our three runs,
alpha, beta and delta, in the 2015 STS task. Each en-
try supplies a run’s Pearson correlation on a dataset
and the rank of the run among all 73 runs submitted
by the 28 teams. The last row shows the weighted
mean and the overall ranks of our three runs.
The alpha run was produced by applying the
align-and-differentiate algorithm to the five datasets
with the same parameter settings. The beta run was
produced without penalizing OOC terms, except for
the images dataset. The result for penalizing OOC
terms are slightly better, but are just shy of a 95%
confidence interval (using paired T-tests). On the
images dataset, we exploited dependency structure
in the align and differentiate algorithm. We use
the supervised ML model to rescuer our STS scores
only for the delta run on the Headlines and Images
datasets.
Our results on the forums and beliefs datasets
were surprisingly much lower than other datasets
due to the PairingWords system’s poor baseline per-
formance on these datasets as shown in Table 2. We
speculate that this drop in performance is caused by
the PairingWords system ignoring words that are not
nouns, verbs, adjectives and limited adverbs. These
include common meaningful words such as “how”
and “why” in both datasets.
</bodyText>
<table confidence="0.966817333333333">
System headline image student forum belief mean
UMBC .8059 .8431 .7588 .6646 .6996 .7725
alpha .8342 .8701 .7827 .6589 .7029 .7920
</table>
<tableCaption confidence="0.989526">
Table 2: Our approach improves results by 2.5% in Pear-
son’s correlation.
</tableCaption>
<bodyText confidence="0.999384">
Our approach of semantically differentiating dis-
tributionally similar terms, as shown in Table 2 is a
statistically significant improvement at the 95% con-
fidence interval.
</bodyText>
<sectionHeader confidence="0.998093" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998269">
We thank Ebiquity lab, CSEE department, Univer-
sity of Maryland, Baltimore County for providing
their 2013 STS code.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99764984375">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: a pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics, pages 385–393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Eneko Agirrea, Carmen Baneab, Claire Cardiec, Daniel
Cerd, Mona Diabe, Aitor Gonzalez-Agirrea, Weiwei
Guof, Rada Mihalceab, German Rigaua, and Janyce
Wiebeg. 2014. Semeval-2014 task 10: Multilingual
semantic textual similarity. SemEval 2014, page 81.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: Words, sentences, discourse. Dis-
course Processes, 25:211–257.
T.A.S. Coelho, P´avel Pereira Calado, Lamarque
Vieira Souza, Berthier Ribeiro-Neto, and Richard
Muntz. 2004. Image retrieval using multiple evidence
ranking. IEEE Trans. on Knowl. and Data Eng.,
16(4):408–417.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, COLING ’04.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press, May.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013.
</reference>
<page confidence="0.986649">
176
</page>
<reference confidence="0.998618205882353">
UMBC EBIQUITY-CORE: Semantic Textual Sim-
ilarity Systems. In Proceedings of the Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics, June.
Lushan Han. 2014. Schema Free Querying of Semantic
Data. Ph.D. thesis, University of Maryland, Baltimore
County, August.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In HLT-NAACL ’06,
pages 455–462.
T. Landauer and S. Dumais. 1997. A solution to plato’s
problem: The latent semantic analysis theory of the ac-
quisition, induction, and representation of knowledge.
In Psychological Review, 104, pages 211–240.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word-pair antonymy. In Proc. Conf. on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-2008), October.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing Lexical Contrast.
Computational Linguistics, 39(July 2012):555–590.
S. Scheible, S. Schulte im Walde, and S. Springorum.
2013. Uncovering distributional differences between
synonyms and antonyms in a word space model. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 489–
497.
Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Fer-
hatosmanoglu, and Murat Demirbas. 2010. Short text
classification in twitter to improve information filter-
ing. In Proceedings of the 33rd international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 841–842.
</reference>
<page confidence="0.997688">
177
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.791693">
<title confidence="0.999262">Samsung: Align-and-Differentiate Approach to Semantic Textual Similarity</title>
<author confidence="0.99112">Lushan Han</author>
<author confidence="0.99112">Justin Martineau</author>
<author confidence="0.99112">Doreen Cheng</author>
<author confidence="0.99112">Christopher</author>
<affiliation confidence="0.999103">Samsung Research</affiliation>
<address confidence="0.99891">665 Clyde Mountain View, CA 94043,</address>
<email confidence="0.966328">justin.m,doreen.c,</email>
<abstract confidence="0.999234923076923">This paper describes our Align-and- Differentiate approach to the SemEval 2015 Task 2 competition for English Semantic Textual Similarity (STS) systems. Our submission achieved the top place on two of the five evaluation datasets. Our team placed 3rd among 28 participating teams, and our three runs ranked 4th, 6th and 7th among the 73 runs submitted by the 28 teams. Our approach improves upon the by semantically differentiating distributionally similar terms.</abstract>
<note confidence="0.844534">This novel addition improves results by 2.5 points on the Pearson correlation measure.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: a pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="1136" citStr="Agirre et al., 2012" startWordPosition="166" endWordPosition="169">ed 3rd among 28 participating teams, and our three runs ranked 4th, 6th and 7th among the 73 runs submitted by the 28 teams. Our approach improves upon the UMBC PairingWords system by semantically differentiating distributionally similar terms. This novel addition improves results by 2.5 points on the Pearson correlation measure. 1 Introduction Since its inception in 2012, the annual Semantic Textual Similarity (STS) task has attracted and increasing amount of interest in the NLP community. The task is to measure the semantic similarity between two sentences using a scale ranging from 0 to 5 (Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014). In this task, 0 means unrelated and 5 means complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema ma</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: a pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="1157" citStr="Agirre et al., 2013" startWordPosition="170" endWordPosition="173">cipating teams, and our three runs ranked 4th, 6th and 7th among the 73 runs submitted by the 28 teams. Our approach improves upon the UMBC PairingWords system by semantically differentiating distributionally similar terms. This novel addition improves results by 2.5 points on the Pearson correlation measure. 1 Introduction Since its inception in 2012, the annual Semantic Textual Similarity (STS) task has attracted and increasing amount of interest in the NLP community. The task is to measure the semantic similarity between two sentences using a scale ranging from 0 to 5 (Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014). In this task, 0 means unrelated and 5 means complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema matching (Han, 2014), T</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirrea</author>
<author>Carmen Baneab</author>
<author>Claire Cardiec</author>
<author>Daniel Cerd</author>
</authors>
<title>Mona Diabe, Aitor Gonzalez-Agirrea, Weiwei Guof, Rada Mihalceab, German Rigaua, and Janyce Wiebeg.</title>
<date>2014</date>
<booktitle>Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval 2014,</booktitle>
<pages>81</pages>
<contexts>
<context position="1180" citStr="Agirrea et al., 2014" startWordPosition="174" endWordPosition="178">ur three runs ranked 4th, 6th and 7th among the 73 runs submitted by the 28 teams. Our approach improves upon the UMBC PairingWords system by semantically differentiating distributionally similar terms. This novel addition improves results by 2.5 points on the Pearson correlation measure. 1 Introduction Since its inception in 2012, the annual Semantic Textual Similarity (STS) task has attracted and increasing amount of interest in the NLP community. The task is to measure the semantic similarity between two sentences using a scale ranging from 0 to 5 (Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014). In this task, 0 means unrelated and 5 means complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema matching (Han, 2014), Twitter search (Sriram e</context>
</contexts>
<marker>Agirrea, Baneab, Cardiec, Cerd, 2014</marker>
<rawString>Eneko Agirrea, Carmen Baneab, Claire Cardiec, Daniel Cerd, Mona Diabe, Aitor Gonzalez-Agirrea, Weiwei Guof, Rada Mihalceab, German Rigaua, and Janyce Wiebeg. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval 2014, page 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burgess</author>
<author>K Livesay</author>
<author>K Lund</author>
</authors>
<date>1998</date>
<booktitle>Explorations in context space: Words, sentences, discourse. Discourse Processes,</booktitle>
<pages>25--211</pages>
<contexts>
<context position="5168" citStr="Burgess et al., 1998" startWordPosition="787" endWordPosition="790"> term alignment scores. Figure 1: Overview of UMBC PairingWords system. 2.1 Precompute Word Similarities First, a distributional model was built on an English corpus2 of three-billion words and separated 2The UMBC WebBase corpus is available for download at http://ebiq.org/r/351 into paragraphs. Words are POS tagged and lemmatized. A small context window of ±4 words is used to count word co-occurrences. The vocabulary has a size of 29,000 terms, which includes primarily open-class words (i.e. nouns, verbs, adjectives and adverbs). Singular Value Decomposition (SVD) (Landauer and Dumais, 1997; Burgess et al., 1998) has been used to reduce the 29K word vectors to 300 dimensions. The distributional similarity between two words is measured by the cosine similarity of their corresponding reduced word vectors. The distributional similarity is then enhanced with WordNet (Fellbaum, 1998) relations in eight categories (See (Han et al., 2013)). Finally it is wrapped with surface similarity modules to handle the matching of out-of-vocabulary words. 2.2 NLP Pipeline The Stanford POS tagger is applied to tag and lemmatize the input sentences. A predefined vocabulary, POS tags, and regular expressions are used to re</context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>C. Burgess, K. Livesay, and K. Lund. 1998. Explorations in context space: Words, sentences, discourse. Discourse Processes, 25:211–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A S Coelho</author>
<author>P´avel Pereira Calado</author>
<author>Lamarque Vieira Souza</author>
<author>Berthier Ribeiro-Neto</author>
<author>Richard Muntz</author>
</authors>
<title>Image retrieval using multiple evidence ranking.</title>
<date>2004</date>
<journal>IEEE Trans. on Knowl. and Data Eng.,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="1843" citStr="Coelho et al., 2004" startWordPosition="276" endWordPosition="279">ns complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema matching (Han, 2014), Twitter search (Sriram et al., 2010), image retrieval by captions (Coelho et al., 2004) and information retrieval in general. Measuring semantic similarity is difficult because it is relatively easy to express the same idea in very different ways. Both word choice and word order can have a great impact on the semantics of a sentence, or not at all. For example, the sentences “A woman is playing piano on the street” and “A lady is playing violin on the street” have a semantic similarity score of only 2, because pianos are not violins so the two events in the sentences must be different. This is problematic because common solutions, such as bag-of-words representations, parse tree</context>
</contexts>
<marker>Coelho, Calado, Souza, Ribeiro-Neto, Muntz, 2004</marker>
<rawString>T.A.S. Coelho, P´avel Pereira Calado, Lamarque Vieira Souza, Berthier Ribeiro-Neto, and Richard Muntz. 2004. Image retrieval using multiple evidence ranking. IEEE Trans. on Knowl. and Data Eng., 16(4):408–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04.</booktitle>
<contexts>
<context position="1633" citStr="Dolan et al., 2004" startWordPosition="244" endWordPosition="247">he task is to measure the semantic similarity between two sentences using a scale ranging from 0 to 5 (Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014). In this task, 0 means unrelated and 5 means complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema matching (Han, 2014), Twitter search (Sriram et al., 2010), image retrieval by captions (Coelho et al., 2004) and information retrieval in general. Measuring semantic similarity is difficult because it is relatively easy to express the same idea in very different ways. Both word choice and word order can have a great impact on the semantics of a sentence, or not at all. For example, the sentences “A woman is playing piano on the street” and “A lady is playing violin on the street” have a semant</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="5439" citStr="Fellbaum, 1998" startWordPosition="830" endWordPosition="831">51 into paragraphs. Words are POS tagged and lemmatized. A small context window of ±4 words is used to count word co-occurrences. The vocabulary has a size of 29,000 terms, which includes primarily open-class words (i.e. nouns, verbs, adjectives and adverbs). Singular Value Decomposition (SVD) (Landauer and Dumais, 1997; Burgess et al., 1998) has been used to reduce the 29K word vectors to 300 dimensions. The distributional similarity between two words is measured by the cosine similarity of their corresponding reduced word vectors. The distributional similarity is then enhanced with WordNet (Fellbaum, 1998) relations in eight categories (See (Han et al., 2013)). Finally it is wrapped with surface similarity modules to handle the matching of out-of-vocabulary words. 2.2 NLP Pipeline The Stanford POS tagger is applied to tag and lemmatize the input sentences. A predefined vocabulary, POS tags, and regular expressions are used to recognize multi-word terms including noun and verb phrases, proper nouns, numbers and time. Stop words are ignored. The stop word list was augmented with adverbs that occurred more than 500, 000 times in the corpus. 2.3 Word Alignment Between Two Sentences The alignment fu</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Johnathan Weese</author>
</authors>
<date>2013</date>
<contexts>
<context position="3129" citStr="Han et al., 2013" startWordPosition="483" endWordPosition="486"> upon existing word choice approaches with better measures to semantically differentiate distributionally similar terms, and by using these measures to also improve the word alignment. Our solution is an Align-and-Differentiate approach, in which we greedily align words between sentences, before penalizing non-matching words in the differentiate-phase. Our system improves upon the successful UMBC PairingWords system by about 2 points of Pearson’s Correlation measure. The success of the PairingWords system is largely due to their high-quality distributional word similarity model1 described in (Han et al., 2013). The distributional similarity model can tell that “woman” and “lady” in the above example are highly similar, which is usually correct, but it also says that “pi1See http://swoogle.umbc.edu/SimService/ for a demo. 172 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 172–177, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics ano” and “violin” are very similar, which in many contexts is incorrect. While distributional similarity measures can be criticized for producing high similarity scores for antonyms and contrasting</context>
<context position="4389" citStr="Han et al., 2013" startWordPosition="667" endWordPosition="670">lly advantageous when performing word alignment between two sentences. We take advantage of this property by first aligning with distributional similarity, and then differentiate by penalizing alignments of words that are semantically disjoint (Ex: antonyms). This technique to first align and then differentiate is our key improvement. The remainder of the paper proceeds as follows. Section 2 briefly revisits the UMBC PairingWords system. Section 3 presents our new Align-andDifferentiate approach. Section 4 presents and discusses our results. 2 UMBC PairingWords System The PairingWords system (Han et al., 2013) uses a state-of-the-art word similarity measure to align words in the sentence pair and computes the STS score using a simple metric that combines individual term alignment scores. Figure 1: Overview of UMBC PairingWords system. 2.1 Precompute Word Similarities First, a distributional model was built on an English corpus2 of three-billion words and separated 2The UMBC WebBase corpus is available for download at http://ebiq.org/r/351 into paragraphs. Words are POS tagged and lemmatized. A small context window of ±4 words is used to count word co-occurrences. The vocabulary has a size of 29,000</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Johnathan Weese. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>UMBC EBIQUITY-CORE</author>
</authors>
<title>Semantic Textual Similarity Systems.</title>
<date></date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics,</booktitle>
<marker>EBIQUITY-CORE, </marker>
<rawString>UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
</authors>
<title>Schema Free Querying of Semantic Data.</title>
<date>2014</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland, Baltimore County,</institution>
<contexts>
<context position="1754" citStr="Han, 2014" startWordPosition="263" endWordPosition="264"> et al., 2013; Agirrea et al., 2014). In this task, 0 means unrelated and 5 means complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema matching (Han, 2014), Twitter search (Sriram et al., 2010), image retrieval by captions (Coelho et al., 2004) and information retrieval in general. Measuring semantic similarity is difficult because it is relatively easy to express the same idea in very different ways. Both word choice and word order can have a great impact on the semantics of a sentence, or not at all. For example, the sentences “A woman is playing piano on the street” and “A lady is playing violin on the street” have a semantic similarity score of only 2, because pianos are not violins so the two events in the sentences must be different. This </context>
</contexts>
<marker>Han, 2014</marker>
<rawString>Lushan Han. 2014. Schema Free Querying of Semantic Data. Ph.D. thesis, University of Maryland, Baltimore County, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In HLT-NAACL ’06,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="1704" citStr="Kauchak and Barzilay, 2006" startWordPosition="253" endWordPosition="257">nces using a scale ranging from 0 to 5 (Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014). In this task, 0 means unrelated and 5 means complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema matching (Han, 2014), Twitter search (Sriram et al., 2010), image retrieval by captions (Coelho et al., 2004) and information retrieval in general. Measuring semantic similarity is difficult because it is relatively easy to express the same idea in very different ways. Both word choice and word order can have a great impact on the semantics of a sentence, or not at all. For example, the sentences “A woman is playing piano on the street” and “A lady is playing violin on the street” have a semantic similarity score of only 2, because pianos are not violins so the tw</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In HLT-NAACL ’06, pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>In Psychological Review,</journal>
<volume>104</volume>
<pages>211--240</pages>
<contexts>
<context position="5145" citStr="Landauer and Dumais, 1997" startWordPosition="783" endWordPosition="786">ic that combines individual term alignment scores. Figure 1: Overview of UMBC PairingWords system. 2.1 Precompute Word Similarities First, a distributional model was built on an English corpus2 of three-billion words and separated 2The UMBC WebBase corpus is available for download at http://ebiq.org/r/351 into paragraphs. Words are POS tagged and lemmatized. A small context window of ±4 words is used to count word co-occurrences. The vocabulary has a size of 29,000 terms, which includes primarily open-class words (i.e. nouns, verbs, adjectives and adverbs). Singular Value Decomposition (SVD) (Landauer and Dumais, 1997; Burgess et al., 1998) has been used to reduce the 29K word vectors to 300 dimensions. The distributional similarity between two words is measured by the cosine similarity of their corresponding reduced word vectors. The distributional similarity is then enhanced with WordNet (Fellbaum, 1998) relations in eight categories (See (Han et al., 2013)). Finally it is wrapped with surface similarity modules to handle the matching of out-of-vocabulary words. 2.2 NLP Pipeline The Stanford POS tagger is applied to tag and lemmatize the input sentences. A predefined vocabulary, POS tags, and regular exp</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. In Psychological Review, 104, pages 211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word-pair antonymy.</title>
<date>2008</date>
<booktitle>In Proc. Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-2008),</booktitle>
<contexts>
<context position="10526" citStr="Mohammad et al., 2008" startWordPosition="1669" endWordPosition="1673"> 3.4 Differentiate This subsection defines and then describes how we identify Disjoint Similar Concepts. The semantic similarity of two words is the degree of semantic equivalence between the two words. We may also say, it is the ability to substitute one term for the other without changing the meaning of a sentence. Many distributionally similar terms are not semantically similar. Examples include “good” vs “bad”, “cat” vs “dog”, “Thuesday” vs “Monday”, “France” vs “England” and etc. Existing research on distributional models has mainly been focused on studying antonyms or contrasting words (Mohammad et al., 2008; Scheible et al., 2013; Mohammad et al., 2013). However, as shown by the above examples, the scope of distributionally similar but not semantically similar terms goes far beyond antonyms. Hereafter, we refer to this new category of terms as Disjoint Similar Concepts (DSCs). To the best of our knowledge, collecting Disjoint Similar Concepts is a novel research problem. General statistical methods are not easily available, but we can extract such information from human-crafted ontologies, such as WordNet. For this work, we identify Disjoint Similar Concepts as siblings under a common parent in </context>
</contexts>
<marker>Mohammad, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008. Computing word-pair antonymy. In Proc. Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-2008), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Bonnie J Dorr</author>
<author>Graeme Hirst</author>
<author>Peter D Turney</author>
</authors>
<date>2013</date>
<booktitle>Computing Lexical Contrast. Computational Linguistics, 39(July</booktitle>
<pages>2012--555</pages>
<contexts>
<context position="10573" citStr="Mohammad et al., 2013" startWordPosition="1678" endWordPosition="1681"> then describes how we identify Disjoint Similar Concepts. The semantic similarity of two words is the degree of semantic equivalence between the two words. We may also say, it is the ability to substitute one term for the other without changing the meaning of a sentence. Many distributionally similar terms are not semantically similar. Examples include “good” vs “bad”, “cat” vs “dog”, “Thuesday” vs “Monday”, “France” vs “England” and etc. Existing research on distributional models has mainly been focused on studying antonyms or contrasting words (Mohammad et al., 2008; Scheible et al., 2013; Mohammad et al., 2013). However, as shown by the above examples, the scope of distributionally similar but not semantically similar terms goes far beyond antonyms. Hereafter, we refer to this new category of terms as Disjoint Similar Concepts (DSCs). To the best of our knowledge, collecting Disjoint Similar Concepts is a novel research problem. General statistical methods are not easily available, but we can extract such information from human-crafted ontologies, such as WordNet. For this work, we identify Disjoint Similar Concepts as siblings under a common parent in an ontology, such as WordNet. For example, in t</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, Turney, 2013</marker>
<rawString>Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney. 2013. Computing Lexical Contrast. Computational Linguistics, 39(July 2012):555–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Scheible</author>
<author>S Schulte im Walde</author>
<author>S Springorum</author>
</authors>
<title>Uncovering distributional differences between synonyms and antonyms in a word space model.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>489--497</pages>
<contexts>
<context position="10549" citStr="Scheible et al., 2013" startWordPosition="1674" endWordPosition="1677"> subsection defines and then describes how we identify Disjoint Similar Concepts. The semantic similarity of two words is the degree of semantic equivalence between the two words. We may also say, it is the ability to substitute one term for the other without changing the meaning of a sentence. Many distributionally similar terms are not semantically similar. Examples include “good” vs “bad”, “cat” vs “dog”, “Thuesday” vs “Monday”, “France” vs “England” and etc. Existing research on distributional models has mainly been focused on studying antonyms or contrasting words (Mohammad et al., 2008; Scheible et al., 2013; Mohammad et al., 2013). However, as shown by the above examples, the scope of distributionally similar but not semantically similar terms goes far beyond antonyms. Hereafter, we refer to this new category of terms as Disjoint Similar Concepts (DSCs). To the best of our knowledge, collecting Disjoint Similar Concepts is a novel research problem. General statistical methods are not easily available, but we can extract such information from human-crafted ontologies, such as WordNet. For this work, we identify Disjoint Similar Concepts as siblings under a common parent in an ontology, such as Wo</context>
</contexts>
<marker>Scheible, Walde, Springorum, 2013</marker>
<rawString>S. Scheible, S. Schulte im Walde, and S. Springorum. 2013. Uncovering distributional differences between synonyms and antonyms in a word space model. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 489– 497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bharath Sriram</author>
<author>Dave Fuhry</author>
<author>Engin Demir</author>
<author>Hakan Ferhatosmanoglu</author>
<author>Murat Demirbas</author>
</authors>
<title>Short text classification in twitter to improve information filtering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>841--842</pages>
<contexts>
<context position="1792" citStr="Sriram et al., 2010" startWordPosition="267" endWordPosition="270">., 2014). In this task, 0 means unrelated and 5 means complete semantic equivalence. For example, the sentence “China’s new PM rejects US hacking claims” is semantically equivalent to the sentence “China Premier Li rejects ‘groundless’ US hacking accusations” even though there are many word level differences between the two sentences. Improvements in the STS task can advance or benefit many research areas, such as paraphrase recognition (Dolan et al., 2004), automatic machine translation evaluation (Kauchak and Barzilay, 2006), ontology mapping and schema matching (Han, 2014), Twitter search (Sriram et al., 2010), image retrieval by captions (Coelho et al., 2004) and information retrieval in general. Measuring semantic similarity is difficult because it is relatively easy to express the same idea in very different ways. Both word choice and word order can have a great impact on the semantics of a sentence, or not at all. For example, the sentences “A woman is playing piano on the street” and “A lady is playing violin on the street” have a semantic similarity score of only 2, because pianos are not violins so the two events in the sentences must be different. This is problematic because common solution</context>
</contexts>
<marker>Sriram, Fuhry, Demir, Ferhatosmanoglu, Demirbas, 2010</marker>
<rawString>Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Ferhatosmanoglu, and Murat Demirbas. 2010. Short text classification in twitter to improve information filtering. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 841–842.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>