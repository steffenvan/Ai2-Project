<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000742">
<title confidence="0.9969005">
Building a Discourse-Tagged Corpus in the Framework of
Rhetorical Structure Theory
</title>
<author confidence="0.995899">
Lynn Carlson
</author>
<affiliation confidence="0.755651">
Department of Defense
Ft. George G. Meade
</affiliation>
<address confidence="0.8878">
MD 20755
</address>
<email confidence="0.992078">
lmcarlnord@aol.com
</email>
<author confidence="0.993855">
Daniel Marcu
</author>
<affiliation confidence="0.849555333333333">
Information Sciences Institute
University of S. California
Marina del Rey, CA 90292
</affiliation>
<email confidence="0.995622">
marcu@isi.edu
</email>
<author confidence="0.954118">
Mary Ellen Okurowski
</author>
<affiliation confidence="0.736722">
Department of Defense
Ft. George G. Meade
</affiliation>
<address confidence="0.875918">
MD 20755
</address>
<email confidence="0.988652">
meokuro@romulus.ncsc.mil
</email>
<sectionHeader confidence="0.993573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998219357142857">
We describe our experience in
developing a discourse-annotated
corpus for community-wide use.
Working in the framework of
Rhetorical Structure Theory, we were
able to create a large annotated
resource with very high consistency,
using a well-defined methodology and
protocol. This resource is made
publicly available through the
Linguistic Data Consortium to enable
researchers to develop empirically
grounded, discourse-specific
applications.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945021739131">
The advent of large-scale collections of
annotated data has marked a paradigm shift in
the research community for natural language
processing. These corpora, now also common in
many languages, have accelerated development
efforts and energized the community.
Annotation ranges from broad characterization
of document-level information, such as topic or
relevance judgments (Voorhees and Harman,
1999; Wayne, 2000) to discrete analysis of a
wide range of linguistic phenomena. However,
rich theoretical approaches to discourse/text
analysis (Van Dijk and Kintsch, 1983; Meyer,
1985; Grosz and Sidner, 1986; Mann and
Thompson, 1988) have yet to be applied on a
large scale. So far, the annotation of discourse
structure of documents has been applied
primarily to identifying topical segments
(Hearst, 1997), inter-sentential relations
(Nomoto and Matsumoto, 1999; Ts’ou et al.,
2000), and hierarchical analyses of small
corpora (Moser and Moore, 1995; Marcu et al.,
1999).
In this paper, we recount our experience in
developing a large resource with discourse-level
annotation for NLP research. Our main goal in
undertaking this effort was to create a reference
corpus for community-wide use. Two essential
considerations from the outset were that the
corpus needed to be consistently annotated, and
that it would be made publicly available through
the Linguistic Data Consortium for a nominal
fee to cover distribution costs. The paper
describes the challenges we faced in building a
corpus of this level of complexity and scope –
including selection of theoretical approach,
annotation methodology, training, and quality
assurance. The resulting corpus contains 385
documents of American English selected from
the Penn Treebank (Marcus et al., 1993),
annotated in the framework of Rhetorical
Structure Theory. We believe this resource
holds great promise as a rich new source of text-
level information to support multiple lines of
research for language understanding
applications.
</bodyText>
<sectionHeader confidence="0.995651" genericHeader="introduction">
2 Framework
</sectionHeader>
<bodyText confidence="0.999680296296296">
Two principle goals underpin the creation of this
discourse-tagged corpus: 1) The corpus should
be grounded in a particular theoretical approach,
and 2) it should be sufficiently large enough to
offer potential for wide-scale use – including
linguistic analysis, training of statistical models
of discourse, and other computational linguistic
applications. These goals necessitated a number
of constraints to our approach. The theoretical
framework had to be practical and repeatable
over a large set of documents in a reasonable
amount of time, with a significant level of
consistency across annotators. Thus, our
approach contributes to the community quite
differently from detailed analyses of specific
discourse phenomena in depth, such as
anaphoric relations (Garside et al., 1997) or
style types (Leech et al., 1997); analysis of a
single text from multiple perspectives (Mann
and Thompson, 1992); or illustrations of a
theoretical model on a single representative text
(Britton and Black, 1985; Van Dijk and Kintsch,
1983).
Our annotation work is grounded in the
Rhetorical Structure Theory (RST) framework
(Mann and Thompson, 1988). We decided to
use RST for three reasons:
</bodyText>
<listItem confidence="0.995611">
• It is a framework that yields rich annotations
that uniformly capture intentional, semantic,
and textual features that are specific to a
given text.
• Previous research on annotating texts with
rhetorical structure trees (Marcu et al.,
1999) has shown that texts can be annotated
by multiple judges at relatively high levels
of agreement. We aimed to produce
annotation protocols that would yield even
higher agreement figures.
• Previous research has shown that RST trees
</listItem>
<bodyText confidence="0.7743424">
can play a crucial role in building natural
language generation systems (Novy, 1993;
Moore and Paris, 1993; Moore, 1995) and
text summarization systems (Marcu, 2000);
can be used to increase the naturalness of
machine translation outputs (Marcu et al.
2000); and can be used to build essay-
scoring systems that provide students with
discourse-based feedback (Burstein et al.,
2001). We suspect that RST trees can be
exploited successfully in the context of
other applications as well.
In the RST framework, the discourse
structure of a text can be represented as a tree
defined in terms of four aspects:
</bodyText>
<listItem confidence="0.968977888888889">
• The leaves of the tree correspond to text
fragments that represent the minimal units
of the discourse, called elementary
discourse units
• The internal nodes of the tree correspond to
contiguous text spans
• Each node is characterized by its nuclearity
– a nucleus indicates a more essential unit of
information, while a satellite indicates a
</listItem>
<bodyText confidence="0.6677645">
supporting or background unit of
information.
</bodyText>
<listItem confidence="0.9382688">
• Each node is characterized by a rhetorical
relation that holds between two or more
non-overlapping, adjacent text spans.
Relations can be of intentional, semantic, or
textual nature.
</listItem>
<bodyText confidence="0.9993445">
Below, we describe the protocol that we used
to build consistent RST annotations.
</bodyText>
<subsectionHeader confidence="0.999504">
2.1 Segmenting Texts into Units
</subsectionHeader>
<bodyText confidence="0.99996875">
The first step in characterizing the discourse
structure of a text in our protocol is to determine
the elementary discourse units (EDUs), which
are the minimal building blocks of a discourse
tree. Mann and Thompson (1988, p. 244) state
that “RST provides a general way to describe
the relations among clauses in a text, whether or
not they are grammatically or lexically
signalled.” Yet, applying this intuitive notion to
the task of producing a large, consistently
annotated corpus is extremely difficult, because
the boundary between discourse and syntax can
be very blurry. The examples below, which
range from two distinct sentences to a single
clause, all convey essentially the same meaning,
packaged in different ways:
</bodyText>
<listItem confidence="0.9972288125">
1. [Xerox Corp.’s third-quarter net income
grew 6.2% on 7.3% higher revenue.] [This
earned mixed reviews from Wall Street
analysts.]
2. [Xerox Corp’s third-quarter net income
grew 6.2% on 7.3% higher revenue,] [which
earned mixed reviews from Wall Street
analysts.]
3. [Xerox Corp’s third-quarter net income
grew 6.2% on 7.3% higher revenue,]
[earning mixed reviews from Wall Street
analysts.]
4. [The 6.2% growth of Xerox Corp.’s third-
quarter net income on 7.3% higher revenue
earned mixed reviews from Wall Street
analysts.]
</listItem>
<bodyText confidence="0.999756878787879">
In Example 1, there is a consequential
relation between the first and second sentences.
Ideally, we would like to capture that kind of
rhetorical information regardless of the syntactic
form in which it is conveyed. However, as
examples 2-4 illustrate, separating rhetorical
from syntactic analysis is not always easy. It is
inevitable that any decision on how to bracket
elementary discourse units necessarily involves
some compromises.
Reseachers in the field have proposed a
number of competing hypotheses about what
constitutes an elementary discourse unit. While
some take the elementary units to be clauses
(Grimes, 1975; Givon, 1983; Longacre, 1983),
others take them to be prosodic units
(Hirschberg and Litman, 1993), turns of talk
(Sacks, 1974), sentences (Polanyi, 1988),
intentionally defined discourse segments (Grosz
and Sidner, 1986), or the “contextually indexed
representation of information conveyed by a
semiotic gesture, asserting a single state of
affairs or partial state of affairs in a discourse
world,” (Polanyi, 1996, p.5). Regardless of their
theoretical stance, all agree that the elementary
discourse units are non-overlapping spans of
text.
Our goal was to find a balance between
granularity of tagging and ability to identify
units consistently on a large scale. In the end,
we chose the clause as the elementary unit of
discourse, using lexical and syntactic clues to
help determine boundaries:
</bodyText>
<listItem confidence="0.955449214285714">
5. [Although Mr. Freeman is retiring,] [he will
continue to work as a consultant for
American Express on a project basis.]wsj_1317
6. [Bond Corp., a brewing, property, media
and resources company, is selling many of
its assets] [to reduce its debts.]wsj_0630
However, clauses that are subjects, objects,
or complements of a main verb are not treated as
EDUs:
7. [Making computers smaller often means
sacrificing memory.]wsj_2387
8. [Insurers could see claims totaling nearly
$1 billion from the San Francisco
earthquake.]wsj_0675
</listItem>
<bodyText confidence="0.953025333333333">
Relative clauses, nominal postmodifiers, or
clauses that break up other legitimate EDUs, are
treated as embedded discourse units:
</bodyText>
<listItem confidence="0.9884084">
9. [The results underscore Sears’s difficulties]
[in implementing the “everyday low
pricing” strategy...]wsj_1105
10. [The Bush Administration,] [trying to blunt
growing demands from Western Europe for
</listItem>
<bodyText confidence="0.99135">
a relaxation of controls on exports to the
Soviet bloc,] [is questioning...]wsj_2326
Finally, a small number of phrasal EDUs are
allowed, provided that the phrase begins with a
strong discourse marker, such as because, in
spite of, as a result of, according to. We opted
for consistency in segmenting, sacrificing some
potentially discourse-relevant phrases in the
process.
</bodyText>
<subsectionHeader confidence="0.99978">
2.2 Building up the Discourse Structure
</subsectionHeader>
<bodyText confidence="0.985697261904762">
Once the elementary units of discourse have
been determined, adjacent spans are linked
together via rhetorical relations creating a
hierarchical structure. Relations may be
mononuclear or multinuclear. Mononuclear
relations hold between two spans and reflect the
situation in which one span, the nucleus, is more
salient to the discourse structure, while the other
span, the satellite, represents supporting
information. Multinuclear relations hold among
two or more spans of equal weight in the
discourse structure. A total of 53 mononuclear
and 25 multinuclear relations were used for the
tagging of the RST Corpus. The final inventory
of rhetorical relations is data driven, and is
based on extensive analysis of the corpus.
Although this inventory is highly detailed,
annotators strongly preferred keeping a higher
level of granularity in the selections available to
them during the tagging process. More extensive
analysis of the final tagged corpus will
demonstrate the extent to which individual
relations that are similar in semantic content
were distinguished consistently during the
tagging process.
The 78 relations used in annotating the
corpus can be partitioned into 16 classes that
share some type of rhetorical meaning:
Attribution, Background, Cause, Comparison,
Condition, Contrast, Elaboration, Enablement,
Evaluation, Explanation, Joint, Manner-Means,
Topic-Comment, Summary, Temporal, Topic-
Change. For example, the class Explanation
includes the relations evidence, explanation-
argumentative, and reason, while Topic-
Comment includes problem-solution, question-
answer, statement-response, topic-comment, and
comment-topic. In addition, three relations are
used to impose structure on the tree: textual-
organization, span, and same-unit (used to link
parts of units separated by embedded units or
spans).
</bodyText>
<sectionHeader confidence="0.985117" genericHeader="method">
3 Discourse Annotation Task
</sectionHeader>
<bodyText confidence="0.999984454545454">
Our methodology for annotating the RST
Corpus builds on prior corpus work in the
Rhetorical Structure Theory framework by
Marcu et al. (1999). Because the goal of this
effort was to build a high-quality, consistently
annotated reference corpus, the task required
that we employ people as annotators whose
primary professional experience was in the area
of language analysis and reporting, provide
extensive annotator training, and specify a
rigorous set of annotation guidelines.
</bodyText>
<subsectionHeader confidence="0.99981">
3.1 Annotator Profile and Training
</subsectionHeader>
<bodyText confidence="0.999955666666667">
The annotators hired to build the corpus were all
professional language analysts with prior
experience in other types of data annotation.
They underwent extensive hands-on training,
which took place roughly in three phases.
During the orientation phase, the annotators
were introduced to the principles of Rhetorical
Structure Theory and the discourse-tagging tool
used for the project (Marcu et al., 1999). The
tool enables an annotator to segment a text into
units, and then build up a hierarchical structure
of the discourse. In this stage of the training, the
focus was on segmenting hard copy texts into
EDUs, and learning the mechanics of the tool.
In the second phase, annotators began to
explore interpretations of discourse structure, by
independently tagging a short document, based
on an initial set of tagging guidelines, and then
meeting as a group to compare results. The
initial focus was on resolving segmentation
differences, but over time this shifted to
addressing issues of relations and nuclearity.
These exploratory sessions led to enhancements
in the tagging guidelines. To reinforce new
rules, annotators re-tagged the document.
During this process, we regularly tracked inter-
annotator agreement (see Section 4.2). In the
final phase, the annotation team concentrated on
ways to reduce differences by adopting some
heuristics for handling higher levels of the
discourse structure. Wiebe et al. (1999) present
a method for automatically formulating a single
best tag when multiple judges disagree on
selecting between binary features. Because our
annotators had to select among multiple choices
at each stage of the discourse annotation
process, and because decisions made at one
stage influenced the decisions made during
subsequent stages, we could not apply Wiebe et
al.’s method. Our methodology for determining
the “best” guidelines was much more of a
consensus-building process, taking into
consideration multiple factors at each step. The
final tagging manual, over 80 pages in length,
contains extensive examples from the corpus to
illustrate text segmentation, nuclearity, selection
of relations, and discourse cues. The manual can
be downloaded from the following web site:
http://www.isi.edu/~marcu/discourse.
The actual tagging of the corpus progressed
in three developmental phases. During the initial
phase of about four months, the team created a
preliminary corpus of 100 tagged documents.
This was followed by a one-month reassessment
phase, during which we measured consistency
across the group on a select set of documents,
and refined the annotation rules. At this point,
we decided to proceed by pre-segmenting all of
the texts on hard copy, to ensure a higher overall
quality to the final corpus. Each text was pre-
segmented by two annotators; discrepancies
were resolved by the author of the tagging
guidelines. In the final phase (about six months)
all 100 documents were re-tagged with the new
approach and guidelines. The remainder of the
corpus was tagged in this manner.
</bodyText>
<subsectionHeader confidence="0.999824">
3.2 Tagging Strategies
</subsectionHeader>
<bodyText confidence="0.999931941176471">
Annotators developed different strategies for
analyzing a document and building up the
corresponding discourse tree. There were two
basic orientations for document analysis – hard
copy or graphical visualization with the tool.
Hard copy analysis ranged from jotting of notes
in the margins to marking up the document into
discourse segments. Those who preferred a
graphical orientation performed their analysis
simultaneously with building the discourse
structure, and were more likely to build the
discourse tree in chunks, rather than
incrementally.
We observed a variety of annotation styles
for the actual building of a discourse tree. Two
of the more representative styles are illustrated
below.
</bodyText>
<footnote confidence="0.7798795">
1. The annotator segments the text one unit at
a time, then incrementally builds up the
</footnote>
<bodyText confidence="0.94857072972973">
discourse tree by immediately attaching the
current node to a previous node. When
building the tree in this fashion, the
annotator must anticipate the upcoming
discourse structure, possibly for a large
span. Yet, often an appropriate choice of
relation for an unseen segment may not be
obvious from the current (rightmost) unit
that needs to be attached. That is why
annotators typically used this approach on
short documents, but resorted to other
strategies for longer documents.
2. The annotator segments multiple units at a
time, then builds discourse sub-trees for
each sentence. Adjacent sentences are then
linked, and larger sub-trees begin to
emerge. The final tree is produced by
linking major chunks of the discourse
Corp.]18 [This is in part because of the effect]19
[of having to average the number of shares
outstanding,]20 [she said.]21 [In addition,]22 [Mrs.
Lidgerwood said,]23 [Norfolk is likely to draw
down its cash initially]24 [to finance the
purchases]25 [and thus forfeit some interest
income.]26 wsj_1111
The discourse sub-tree for this text fragment
is given in Figure 1. Using Style 1 the annotator,
upon segmenting unit [17], must anticipate the
upcoming example relation, which spans units
[17-26]. However, even if the annotator selects
an incorrect relation at that point, the tool allows
great flexibility in changing the structure of the
tree later on.
Using Style 2, the annotator segments each
sentence, and builds up corresponding sub-trees
for spans [16], [17-18], [19-21] and [22-26]. The
*elaboration-object-attribute-embedded
</bodyText>
<figureCaption confidence="0.998848">
Figure 1: Discourse sub-tree for multiple sentences
</figureCaption>
<figure confidence="0.994569071428572">
(16)
example
17-26 elaboration-additional
17-18 attribution 19-21 attribution
(26)
same-unit
(17) 24-25
(18) 19-20 (21) 22-23
*
+
purpose
+attribution-embedded (19) (20) (22) (23) (24) (25)
17-21 explanation-argumentative 22-26
consequence-s
</figure>
<bodyText confidence="0.999569375">
structure. This strategy allows the annotator
to see the emerging discourse structure more
globally; thus, it was the preferred approach
for longer documents.
Consider the text fragment below, consisting
of four sentences, and 11 EDUs:
[Still, analysts don’t expect the buy-back to
significantly affect per-share earnings in the
short term.]16 [The impact won’t be that great,]17
[said Graeme Lidgerwood of First Boston
second and third sub-trees are then linked via an
explanation-argumentative relation, after which,
the fourth sub-tree is linked via an elaboration-
additional relation. The resulting span [17-26] is
finally attached to node [16] as an example
satellite.
</bodyText>
<sectionHeader confidence="0.988263" genericHeader="method">
4 Quality Assurance
</sectionHeader>
<bodyText confidence="0.9886715">
A number of steps were taken to ensure the
quality of the final discourse corpus. These
</bodyText>
<tableCaption confidence="0.999665">
Table 1: Inter-annotator agreement – periodic results for three taggers
</tableCaption>
<table confidence="0.9984225">
Taggers Units Spans Nuclearity Relations Fewer- No. of Avg. No.
Relations Docs EDUs
A, B, E 0.874407 0.772147 0.705330 0.601673 0.644851 4 128.750000
(Apr 00)
A, B, E 0.952721 0.844141 0.782589 0.708932 0.739616 5 38.400002
(Jun 00)
A, E 0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668
(Nov 00)
B, E 0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713
(Nov 00)
A, B 1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001
(Nov 00)
A, B, E 0.971613 0.899971 0.855867 0.755539 0.782312 5 68.599998
(Jan 01)
</table>
<bodyText confidence="0.997092333333333">
involved two types of tasks: checking the
validity of the trees and tracking inter-annotator
consistency.
</bodyText>
<subsectionHeader confidence="0.996027">
4.1 Tree Validation Procedures
</subsectionHeader>
<bodyText confidence="0.999988384615385">
Annotators reviewed each tree for syntactic and
semantic validity. Syntactic checking involved
ensuring that the tree had a single root node and
comparing the tree to the document to check for
missing sentences or fragments from the end of
the text. Semantic checking involved reviewing
nuclearity assignments, as well as choice of
relation and level of attachment in the tree. All
trees were checked with a discourse parser and
tree traversal program which often identified
errors undetected by the manual validation
process. In the end, all of the trees worked
successfully with these programs.
</bodyText>
<subsectionHeader confidence="0.99955">
4.2 Measuring Consistency
</subsectionHeader>
<bodyText confidence="0.999778632653061">
We tracked inter-annotator agreement during
each phase of the project, using a method
developed by Marcu et al. (1999) for computing
kappa statistics over hierarchical structures. The
kappa coefficient (Siegel and Castellan, 1988)
has been used extensively in previous empirical
studies of discourse (Carletta et al., 1997;
Flammia and Zue, 1995; Passonneau and
Litman, 1997). It measures pairwise agreement
among a set of coders who make category
judgments, correcting for chance expected
agreement. The method described in Marcu et
al. (1999) maps hierarchical structures into sets
of units that are labeled with categorial
judgments. The strengths and shortcomings of
the approach are also discussed in detail there.
Researchers in content analysis (Krippendorff,
1980) suggest that values of kappa &gt; 0.8 reflect
very high agreement, while values between 0.6
and 0.8 reflect good agreement.
Table 1 shows average kappa statistics
reflecting the agreement of three annotators at
various stages of the tasks on selected
documents. Different sets of documents were
chosen for each stage, with no overlap in
documents. The statistics measure annotation
reliability at four levels: elementary discourse
units, hierarchical spans, hierarchical nuclearity
and hierarchical relation assignments.
At the unit level, the initial (April 00) scores
and final (January 01) scores represent
agreement on blind segmentation, and are
shown in boldface. The interim June and
November scores represent agreement on hard
copy pre-segmented texts. Notice that even with
pre-segmenting, the agreement on units is not
100% perfect, because of human errors that
occur in segmenting with the tool. As Table 1
shows, all levels demonstrate a marked
improvement from April to November (when
the final corpus was completed), ranging from
about 0.77 to 0.92 at the span level, from 0.70 to
0.88 at the nuclearity level, and from 0.60 to
0.79 at the relation level. In particular, when
relations are combined into the 16 rhetorically-
related classes discussed in Section 2.2, the
November results of the annotation process are
extremely good. The Fewer-Relations column
shows the improvement in scores on assigning
</bodyText>
<tableCaption confidence="0.997353">
Table 2: Inter-annotator agreement – final results fox six taggers
</tableCaption>
<table confidence="0.999121125">
Taggers Units Spans Nuclearity Relations Fewer- No. of Avg. No.
Relations Docs EDUs
B, E 0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713
A, E 0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668
A, B 1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001
A, C 0.950962 0.840187 0.782688 0.676564 0.711109 4 116.500000
A, F 0.952342 0.777553 0.694634 0.597302 0.624908 4 26.500000
A, D 1.000000 0.868280 0.801544 0.720692 0.769894 4 23.250000
</table>
<bodyText confidence="0.9995546">
relations when they are grouped in this manner,
with November results ranging from 0.78 to
0.82. In order to see how much of the
improvement had to do with pre-segmenting, we
asked the same three annotators to annotate five
previously unseen documents in January,
without reference to a pre-segmented document.
The results of this experiment are given in the
last row of Table 1, and they reflect only a small
overall decline in performance from the
November results. These scores reflect very
strong agreement and represent a significant
improvement over previously reported results on
annotating multiple texts in the RST framework
(Marcu et al., 1999).
Table 2 reports final results for all pairs of
taggers who double-annotated four or more
documents, representing 30 out of the 53
documents that were double-tagged. Results are
based on pre-segmented documents.
Our team was able to reach a significant
level of consistency, even though they faced a
number of challenges which reflect differences
in the agreement scores at the various levels.
While operating under the constraints typical of
any theoretical approach in an applied
environment, the annotators faced a task in
which the complexity increased as support from
the guidelines tended to decrease. Thus, while
rules for segmenting were fairly precise,
annotators relied on heuristics requiring more
human judgment to assign relations and
nuclearity. Another factor is that the cognitive
challenge of the task increases as the tree takes
shape. It is relatively straightforward for the
annotator to make a decision on assignment of
nuclearity and relation at the inter-clausal level,
but this becomes more complex at the inter-
sentential level, and extremely difficult when
linking large segments.
This tension between task complexity and
guideline under-specification resulted from the
practical application of a theoretical model on a
broad scale. While other discourse theoretical
approaches posit distinctly different treatments
for various levels of the discourse (Van Dijk and
Kintsch, 1983; Meyer, 1985), RST relies on a
standard methodology to analyze the document
at all levels. The RST relation set is rich and the
concept of nuclearity, somewhat interpretive.
This gave our annotators more leeway in
interpreting the higher levels of the discourse
structure, thus introducing some stylistic
differences, which may prove an interesting
avenue of future research.
</bodyText>
<sectionHeader confidence="0.968411" genericHeader="method">
5 Corpus Details
</sectionHeader>
<bodyText confidence="0.999714653846154">
The RST Corpus consists of 385 Wall Street
Journal articles from the Penn Treebank,
representing over 176,000 words of text. In
order to measure inter-annotator consistency, 53
of the documents (13.8%) were double-tagged.
The documents range in size from 31 to 2124
words, with an average of 458.14 words per
document. The final tagged corpus contains
21,789 EDUs with an average of 56.59 EDUs
per document. The average number of words per
EDU is 8.1.
The articles range over a variety of topics,
including financial reports, general interest
stories, business-related news, cultural reviews,
editorials, and letters to the editor. In selecting
these documents, we partnered with the
Linguistic Data Consortium to select Penn
Treebank texts for which the syntactic
bracketing was known to be of high caliber.
Thus, the RST Corpus provides an additional
level of linguistic annotation to supplement
existing annotated resources.
For details on obtaining the corpus,
annotation software, tagging guidelines, and
related documentation and resources, see:
http://www.isi.edu/~marcu/discourse.
</bodyText>
<sectionHeader confidence="0.997424" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9997935">
A growing number of groups have developed or
are developing discourse-annotated corpora for
text. These can be characterized both in terms of
the kinds of features annotated as well as by the
scope of the annotation. Features may include
specific discourse cues or markers, coreference
links, identification of rhetorical relations, etc.
The scope of the annotation refers to the levels
of analysis within the document, and can be
characterized as follows:
</bodyText>
<listItem confidence="0.981665166666667">
• sentential: annotation of features at the
intra-sentential or inter-sentential level, at a
single level of depth (Sundheim, 1995;
Tsou et al., 2000; Nomoto and Matsumoto,
1999; Rebeyrolle, 2000).
• hierarchical: annotation of features at
multiple levels, building upon lower levels
of analysis at the clause or sentence level
(Moser and Moore, 1995; Marcu, et al.
1999)
• document-level: broad characterization of
document structure such as identification of
</listItem>
<bodyText confidence="0.980154708333334">
topical segments (Hearst, 1997), linking of
large text segments via specific relations
(Ferrari, 1998; Rebeyrolle, 2000), or
defining text objects with a text architecture
(Pery-Woodley and Rebeyrolle, 1998).
Developing corpora with these kinds of rich
annotation is a labor-intensive effort. Building
the RST Corpus involved more than a dozen
people on a full or part-time basis over a one-
year time frame (Jan. – Dec. 2000). Annotation
of a single document could take anywhere from
30 minutes to several hours, depending on the
length and topic. Re-tagging of a large number
of documents after major enhancements to the
annotation guidelines was also time consuming.
In addition, limitations of the theoretical
approach became more apparent over time.
Because the RST theory does not differentiate
between different levels of the tree structure, a
fairly fine-grained set of relations operates
between EDUs and EDU clusters at the macro-
level. The procedural knowledge available at the
EDU level is likely to need further refinement
for higher-level text spans along the lines of
other work which posits a few macro-level
relations for text segments, such as Ferrari
(1998) or Meyer (1985). Moreover, using the
RST approach, the resultant tree structure, like a
traditional outline, imposed constraints that
other discourse representations (e.g., graph)
would not. In combination with the tree
structure, the concept of nuclearity also guided
an annotator to capture one of a number of
possible stylistic interpretations. We ourselves
are eager to explore these aspects of the RST,
and expect new insights to appear through
analysis of the corpus.
We anticipate that the RST Corpus will be
multifunctional and support a wide range of
language engineering applications. The added
value of multiple layers of overt linguistic
phenomena enhancing the Penn Treebank
information can be exploited to advance the
study of discourse, to enhance language
technologies such as text summarization,
machine translation or information retrieval, or
to be a testbed for new and creative natural
language processing techniques.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998689210227273">
Bruce Britton and John Black. 1985.
Understanding Expository Text. Hillsdale, NJ:
Lawrence Erlbaum Associates.
Jill Burstein, Daniel Marcu, Slava Andreyev,
and Martin Chodorow. 2001. Towards
automatic identification of discourse elements in
essays. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, Toulouse, France.
Jean Carletta, Amy Isard, Stephen Isard,
Jacqueline Kowtko, Gwyneth Doherty-Sneddon,
and Anne Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Linguistics 23(1): 13-32.
Giacomo Ferrari. 1998. Preliminary steps
toward the creation of a discourse and text
resource. In Proceedings of the First
International Conference on Language
Resources and Evaluation (LREC 1998),
Granada, Spain, 999-1001.
Giovanni Flammia and Victor Zue. 1995.
Empirical evaluation of human performance and
agreement in parsing discourse constituents in
spoken dialogue. In Proceedings of the 4th
European Conference on Speech
Communication and Technology, Madrid, Spain,
vol. 3, 1965-1968.
Roger Garside, Steve Fligelstone and Simon
Botley. 1997. Discourse Annotation: Anaphoric
Relations in Corpora. In Corpus annotation:
Linguistic information from computer text
corpora, edited by R. Garside, G. Leech, and T.
McEnery. London: Longman, 66-84.
Talmy Givon. 1983. Topic continuity in
discourse. In Topic Continuity in Discourse: a
Quantitative Cross-Language Study.
Amsterdam/Philadelphia: John Benjamins, 1-41.
Joseph Evans Grimes. 1975. The Thread of
Discourse. The Hague, Paris: Mouton.
Barbara Grosz and Candice Sidner. 1986.
Attentions, intentions, and the structure of
discourse. Computational Linguistics, 12(3):
175-204.
Marti Hearst. 1997. TextTiling: Segmenting
text into multi-paragraph subtopic passages.
Computational Linguistics 23(1): 33-64.
Julia Hirschberg and Diane Litman. 1993.
Empirical studies on the disambiguation of cue
phrases. Computational Linguistics 19(3): 501-
530.
Eduard Hovy. 1993. Automated discourse
generation using discourse structure relations.
Artificial Intelligence 63(1-2): 341-386.
Klaus Krippendorff. 1980. Content Analysis:
An Introduction to its Methodology. Beverly
Hills, CA: Sage Publications.
Geoffrey Leech, Tony McEnery, and Martin
Wynne. 1997. Further levels of annotation. In
Corpus Annotation: Linguistic Information from
Computer Text Corpora, edited by R. Garside,
G. Leech, and T. McEnery. London: Longman,
85-101.
Robert Longacre. 1983. The Grammar of
Discourse. New York: Plenum Press.
William Mann and Sandra Thompson. 1988.
Rhetorical structure theory. Toward a functional
theory of text organization. Text, 8(3): 243-281.
William Mann and Sandra Thompson, eds.
1992. Discourse Description: Diverse Linguistic
Analyses of a Fund-raising Text.
Amsterdam/Philadelphia: John Benjamins.
Daniel Marcu. 2000. The Theory and
Practice of Discourse Parsing and
Summarization. Cambridge, MA: The MIT
Press.
Daniel Marcu, Estibaliz Amorrortu, and
Magdelena Romera. 1999. Experiments in
constructing a corpus of discourse trees. In
Proceedings of the ACL Workshop on Standards
and Tools for Discourse Tagging, College Park,
MD, 48-57.
Daniel Marcu, Lynn Carlson, and Maki
Watanabe. 2000. The automatic translation of
discourse structures. Proceedings of the First
Annual Meeting of the North American Chapter
of the Association for Computational
Linguistics, Seattle, WA, 9-17.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: the Penn
Treebank, Computational Linguistics 19(2),
313-330.
Bonnie Meyer. 1985. Prose Analysis:
Purposes, Procedures, and Problems. In
Understanding Expository Text, edited by B.
Britton and J. Black. Hillsdale, NJ: Lawrence
Erlbaum Associates, 11-64.
Johanna Moore. 1995. Participating in
Explanatory Dialogues: Interpreting and
Responding to Questions in Context.
Cambridge, MA: MIT Press.
Johanna Moore and Cecile Paris. 1993.
Planning text for advisory dialogues: capturing
intentional and rhetorical information.
Computational Linguistics 19(4): 651-694.
Megan Moser and Johanna Moore. 1995.
Investigating cue selection and placement in
tutorial discourse. Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics, Cambridge, MA,
130-135.
Tadashi Nomoto and Yuji Matsumoto. 1999.
Learning discourse relations with active data
selection. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
College Park, MD, 158-167.
Rebecca Passonneau and Diane Litman.
1997. Discourse segmentation by human and
automatic means. Computational Linguistics
23(1): 103-140.
Marie-Paule Pery-Woodley and Josette
Rebeyrolle. 1998. Domain and genre in
sublanguage text: definitional microtexts in
three corpora. In Proceedings of the First
International Conference on Language
Resources and Evaluation (LREC-1998),
Granada, Spain, 987-992.
Livia Polanyi. 1988. A formal model of the
structure of discourse. Journal of Pragmatics
12: 601-638.
Livia Polanyi. 1996. The linguistic structure
of discourse. Center for the Study of Language
and Information. CSLI-96-200.
Josette Rebeyrolle. 2000. Utilisation de
contextes définitoires pour l’acquisition de
connaissances à partir de textes. In Actes
Journées Francophones d’Ingénierie de la
Connaissance (IC’2000), Toulouse, IRIT, 105-
114.
Harvey Sacks, Emmanuel Schegloff, and
Gail Jefferson. 1974. A simple systematics for
the organization of turntaking in conversation.
Language 50: 696-735.
Sidney Siegal and N.J. Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences. New York: McGraw-Hill.
Beth Sundheim. 1995. Overview of results of
the MUC-6 evaluation. In Proceedings of the
Sixth Message Understanding Conference
(MUC-6), Columbia, MD, 13-31.
Benjamin K. T’sou, Tom B.Y. Lai, Samuel
W.K. Chan, Weijun Gao, and Xuegang Zhan.
2000. Enhancement of Chinese discourse
marker tagger with C.4.5. In Proceedings of the
Second Chinese Language Processing
Workshop, Hong Kong, 38-45.
Teun A. Van Dijk and Walter Kintsch. 1983.
Strategies of Discourse Comprehension. New
York: Academic Press.
Ellen Voorhees and Donna Harman. 1999.
The Eighth Text Retrieval Conference (TREC-
8). NIST Special Publication 500-246.
Charles Wayne. 2000. Multilingual topic
detection and tracking: successful research
enabled by corpora and evaluation. In
Proceedings of the Second International
Conference on Language Resources and
Evaluation (LREC-2000), Athens, Greece,
1487-1493.
Janyce Wiebe, Rebecca Bruce, and Thomas
O’Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics.
College Park, MD, 246-253.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9865465">Building a Discourse-Tagged Corpus in the Framework Rhetorical Structure Theory</title>
<author confidence="0.992547">Lynn</author>
<affiliation confidence="0.791709">Department of Ft. George G. MD</affiliation>
<email confidence="0.997356">lmcarlnord@aol.com</email>
<author confidence="0.972821">Daniel</author>
<affiliation confidence="0.9997545">Information Sciences University of S.</affiliation>
<author confidence="0.839971">Marina del Rey</author>
<author confidence="0.839971">CA</author>
<email confidence="0.986455">marcu@isi.edu</email>
<author confidence="0.999963">Mary Ellen</author>
<affiliation confidence="0.757696666666667">Department of Ft. George G. MD</affiliation>
<email confidence="0.951029">meokuro@romulus.ncsc.mil</email>
<abstract confidence="0.999374890625">We describe our experience in developing a discourse-annotated corpus for community-wide use. Working in the framework of Rhetorical Structure Theory, we were able to create a large annotated resource with very high consistency, using a well-defined methodology and protocol. This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, applications. The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selection of theoretical approach, annotation methodology, training, and quality assurance. The resulting corpus contains 385 documents of American English selected from the Penn Treebank (Marcus et al., 1993), annotated in the framework of Rhetorical Structure Theory. We believe this resource holds great promise as a rich new source of textlevel information to support multiple lines of research for language understanding applications. Two principle goals underpin the creation of this discourse-tagged corpus: 1) The corpus should be grounded in a particular theoretical approach, and 2) it should be sufficiently large enough to offer potential for wide-scale use – including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications. These goals necessitated a number of constraints to our approach. The theoretical framework had to be practical and repeatable over a large set of documents in a reasonable amount of time, with a significant level of consistency across annotators. Thus, our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth, such as anaphoric relations (Garside et al., 1997) or style types (Leech et al., 1997); analysis of a single text from multiple perspectives (Mann and Thompson, 1992); or illustrations of a theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, 1983). Our annotation work is grounded in the Rhetorical Structure Theory (RST) framework (Mann and Thompson, 1988). We decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units the discourse, called discourse units • The internal nodes of the tree correspond to text Each node is characterized by its – a nucleus indicates a more essential unit of information, while a satellite indicates a supporting or background unit of information. Each node is characterized by a holds between two or more non-overlapping, adjacent text spans. Relations can be of intentional, semantic, or textual nature. Below, we describe the protocol that we used to build consistent RST annotations. 2.1 Segmenting Texts into Units The first step in characterizing the discourse structure of a text in our protocol is to determine the elementary discourse units (EDUs), which are the minimal building blocks of a discourse tree. Mann and Thompson (1988, p. 244) state that “RST provides a general way to describe the relations among clauses in a text, whether or not they are grammatically or lexically signalled.” Yet, applying this intuitive notion to the task of producing a large, consistently annotated corpus is extremely difficult, because the boundary between discourse and syntax can be very blurry. The examples below, which range from two distinct sentences to a single clause, all convey essentially the same meaning, packaged in different ways: 1. [Xerox Corp.’s third-quarter net income grew 6.2% on 7.3% higher revenue.] [This earned mixed reviews from Wall Street analysts.] 2. [Xerox Corp’s third-quarter net income grew 6.2% on 7.3% higher revenue,] [which earned mixed reviews from Wall Street analysts.] 3. [Xerox Corp’s third-quarter net income grew 6.2% on 7.3% higher revenue,] [earning mixed reviews from Wall Street analysts.] 4. [The 6.2% growth of Xerox Corp.’s thirdquarter net income on 7.3% higher revenue earned mixed reviews from Wall Street analysts.] In Example 1, there is a consequential relation between the first and second sentences. Ideally, we would like to capture that kind of rhetorical information regardless of the syntactic form in which it is conveyed. However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy. It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ability to identify units consistently on a large scale. In the end, we chose the clause as the elementary unit of discourse, using lexical and syntactic clues to help determine boundaries: 5. Freeman is retiring,] [he will continue to work as a consultant for Express on a project 6. [Bond Corp., a brewing, property, media and resources company, is selling many of assets] reduce However, clauses that are subjects, objects, or complements of a main verb are not treated as EDUs: 7. computers smaller means [Insurers could see claims nearly $1 billion from the San Francisco Relative clauses, nominal postmodifiers, or clauses that break up other legitimate EDUs, are treated as embedded discourse units: 9. [The results underscore Sears’s difficulties] implementing the “everyday low [The Bush Administration,] to blunt growing demands from Western Europe for a relaxation of controls on exports to the [is Finally, a small number of phrasal EDUs are allowed, provided that the phrase begins with a discourse marker, such as in of, as a result of, according We opted for consistency in segmenting, sacrificing some potentially discourse-relevant phrases in the process. 2.2 Building up the Discourse Structure Once the elementary units of discourse have been determined, adjacent spans are linked together via rhetorical relations creating a hierarchical structure. Relations may be mononuclear or multinuclear. Mononuclear relations hold between two spans and reflect the in which one span, the more salient to the discourse structure, while the other the supporting information. Multinuclear relations hold among two or more spans of equal weight in the discourse structure. A total of 53 mononuclear and 25 multinuclear relations were used for the tagging of the RST Corpus. The final inventory of rhetorical relations is data driven, and is based on extensive analysis of the corpus. Although this inventory is highly detailed, annotators strongly preferred keeping a higher level of granularity in the selections available to them during the tagging process. More extensive analysis of the final tagged corpus will demonstrate the extent to which individual relations that are similar in semantic content were distinguished consistently during the tagging process. The 78 relations used in annotating the corpus can be partitioned into 16 classes that share some type of rhetorical meaning:</abstract>
<keyword confidence="0.8055705">Cause, Comparison, Condition, Contrast, Elaboration, Enablement, Manner-Means, Topic-</keyword>
<abstract confidence="0.992799229050279">example, the class the relations explanation- Topicquestionstatement-response, topic-comment, addition, three relations are to impose structure on the tree: textualspan, to link parts of units separated by embedded units or spans). Annotation Task Our methodology for annotating the RST Corpus builds on prior corpus work in the Rhetorical Structure Theory framework by Marcu et al. (1999). Because the goal of this effort was to build a high-quality, consistently annotated reference corpus, the task required that we employ people as annotators whose primary professional experience was in the area of language analysis and reporting, provide extensive annotator training, and specify a rigorous set of annotation guidelines. 3.1 Annotator Profile and Training The annotators hired to build the corpus were all professional language analysts with prior experience in other types of data annotation. They underwent extensive hands-on training, which took place roughly in three phases. During the orientation phase, the annotators were introduced to the principles of Rhetorical Structure Theory and the discourse-tagging tool used for the project (Marcu et al., 1999). The tool enables an annotator to segment a text into units, and then build up a hierarchical structure of the discourse. In this stage of the training, the focus was on segmenting hard copy texts into EDUs, and learning the mechanics of the tool. In the second phase, annotators began to explore interpretations of discourse structure, by independently tagging a short document, based on an initial set of tagging guidelines, and then meeting as a group to compare results. The initial focus was on resolving segmentation differences, but over time this shifted to addressing issues of relations and nuclearity. These exploratory sessions led to enhancements in the tagging guidelines. To reinforce new rules, annotators re-tagged the document. During this process, we regularly tracked interannotator agreement (see Section 4.2). In the final phase, the annotation team concentrated on ways to reduce differences by adopting some heuristics for handling higher levels of the discourse structure. Wiebe et al. (1999) present a method for automatically formulating a single best tag when multiple judges disagree on selecting between binary features. Because our annotators had to select among multiple choices at each stage of the discourse annotation process, and because decisions made at one stage influenced the decisions made during subsequent stages, we could not apply Wiebe et al.’s method. Our methodology for determining the “best” guidelines was much more of a consensus-building process, taking into consideration multiple factors at each step. The final tagging manual, over 80 pages in length, contains extensive examples from the corpus to illustrate text segmentation, nuclearity, selection of relations, and discourse cues. The manual can be downloaded from the following web site: http://www.isi.edu/~marcu/discourse. The actual tagging of the corpus progressed in three developmental phases. During the initial phase of about four months, the team created a preliminary corpus of 100 tagged documents. This was followed by a one-month reassessment phase, during which we measured consistency across the group on a select set of documents, and refined the annotation rules. At this point, we decided to proceed by pre-segmenting all of the texts on hard copy, to ensure a higher overall quality to the final corpus. Each text was presegmented by two annotators; discrepancies were resolved by the author of the tagging guidelines. In the final phase (about six months) all 100 documents were re-tagged with the new approach and guidelines. The remainder of the corpus was tagged in this manner. 3.2 Tagging Strategies Annotators developed different strategies for analyzing a document and building up the corresponding discourse tree. There were two basic orientations for document analysis – hard copy or graphical visualization with the tool. Hard copy analysis ranged from jotting of notes in the margins to marking up the document into discourse segments. Those who preferred a graphical orientation performed their analysis simultaneously with building the discourse structure, and were more likely to build the discourse tree in chunks, rather than incrementally. We observed a variety of annotation styles for the actual building of a discourse tree. Two of the more representative styles are illustrated below. annotator segments the text one unit at a time, then incrementally builds up the discourse tree by immediately attaching the node to a previous When building the tree in this fashion, the annotator must anticipate the upcoming discourse structure, possibly for a large span. Yet, often an appropriate choice of relation for an unseen segment may not be obvious from the current (rightmost) unit that needs to be attached. That is why annotators typically used this approach on short documents, but resorted to other strategies for longer documents. annotator segments multiple units at a time, then builds discourse sub-trees for each sentence. Adjacent sentences are then linked, and larger sub-trees begin to emerge. The final tree is produced by linking major chunks of the discourse [This is in part because of the [of having to average the number of shares [she [In [Mrs. [Norfolk is likely to draw its cash [to finance the [and thus forfeit some interest wsj_1111 The discourse sub-tree for this text fragment is given in Figure 1. Using Style 1 the annotator, upon segmenting unit [17], must anticipate the which spans units [17-26]. However, even if the annotator selects an incorrect relation at that point, the tool allows great flexibility in changing the structure of the tree later on. Using Style 2, the annotator segments each sentence, and builds up corresponding sub-trees for spans [16], [17-18], [19-21] and [22-26]. The *elaboration-object-attribute-embedded Figure 1: Discourse sub-tree for multiple sentences (16) example (26) same-unit (17) 24-25 (18) 19-20 (21) 22-23 * + purpose (20) (22) (23) (24) (25) consequence-s strategy allows the annotator to see the emerging discourse structure more globally; thus, it was the preferred approach for longer documents. Consider the text fragment below, consisting of four sentences, and 11 EDUs: [Still, analysts don’t expect the buy-back to significantly affect per-share earnings in the [The impact won’t be that [said Graeme Lidgerwood of First Boston second and third sub-trees are then linked via an after which, fourth sub-tree is linked via an elaboration- The resulting span [17-26] is attached to node [16] as an satellite. Assurance A number of steps were taken to ensure the quality of the final discourse corpus. These Table 1: Inter-annotator agreement – periodic results for three taggers Taggers Units Spans Nuclearity Relations Relations No. Avg.</abstract>
<note confidence="0.795935916666667">Docs EDUs A, B, 0.874407 0.772147 0.705330 0.601673 0.644851 4 128.750000 (Apr 00) A, B, 0.952721 0.844141 0.782589 0.708932 0.739616 5 38.400002 (Jun 00) A, 0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668 (Nov 00) B, 0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713 (Nov 00) A, 1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001 (Nov 00) A, B, 0.971613 0.899971 0.855867 0.755539 0.782312 5 68.599998</note>
<date confidence="0.454759">(Jan 01)</date>
<abstract confidence="0.995049173913044">involved two types of tasks: checking the validity of the trees and tracking inter-annotator consistency. 4.1 Tree Validation Procedures Annotators reviewed each tree for syntactic and semantic validity. Syntactic checking involved ensuring that the tree had a single root node and comparing the tree to the document to check for missing sentences or fragments from the end of the text. Semantic checking involved reviewing nuclearity assignments, as well as choice of relation and level of attachment in the tree. All trees were checked with a discourse parser and tree traversal program which often identified errors undetected by the manual validation process. In the end, all of the trees worked successfully with these programs. 4.2 Measuring Consistency We tracked inter-annotator agreement during each phase of the project, using a method developed by Marcu et al. (1999) for computing kappa statistics over hierarchical structures. The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997). It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement. The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments. The strengths and shortcomings of the approach are also discussed in detail there. Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa &gt; 0.8 reflect very high agreement, while values between 0.6 and 0.8 reflect good agreement. Table 1 shows average kappa statistics reflecting the agreement of three annotators at various stages of the tasks on selected documents. Different sets of documents were chosen for each stage, with no overlap in documents. The statistics measure annotation reliability at four levels: elementary discourse units, hierarchical spans, hierarchical nuclearity and hierarchical relation assignments. At the unit level, the initial (April 00) scores and final (January 01) scores represent agreement on blind segmentation, and are shown in boldface. The interim June and November scores represent agreement on hard copy pre-segmented texts. Notice that even with pre-segmenting, the agreement on units is not 100% perfect, because of human errors that occur in segmenting with the tool. As Table 1 shows, all levels demonstrate a marked improvement from April to November (when the final corpus was completed), ranging from about 0.77 to 0.92 at the span level, from 0.70 to 0.88 at the nuclearity level, and from 0.60 to 0.79 at the relation level. In particular, when relations are combined into the 16 rhetoricallyrelated classes discussed in Section 2.2, the November results of the annotation process are extremely good. The Fewer-Relations column shows the improvement in scores on assigning Table 2: Inter-annotator agreement – final results fox six taggers Taggers Units Spans Nuclearity Relations Fewer- Relations No. of Docs Avg. No. EDUs</abstract>
<note confidence="0.787894333333333">B, E 0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713 A, E 0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668 A, B 1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001 A, C 0.950962 0.840187 0.782688 0.676564 0.711109 4 116.500000 A, F 0.952342 0.777553 0.694634 0.597302 0.624908 4 26.500000 A, D 1.000000 0.868280 0.801544 0.720692 0.769894 4 23.250000</note>
<abstract confidence="0.999432454545455">relations when they are grouped in this manner, with November results ranging from 0.78 to 0.82. In order to see how much of the improvement had to do with pre-segmenting, we asked the same three annotators to annotate five previously unseen documents in January, without reference to a pre-segmented document. The results of this experiment are given in the last row of Table 1, and they reflect only a small overall decline in performance from the November results. These scores reflect very strong agreement and represent a significant improvement over previously reported results on annotating multiple texts in the RST framework (Marcu et al., 1999). Table 2 reports final results for all pairs of taggers who double-annotated four or more documents, representing 30 out of the 53 documents that were double-tagged. Results are based on pre-segmented documents. Our team was able to reach a significant level of consistency, even though they faced a number of challenges which reflect differences in the agreement scores at the various levels. While operating under the constraints typical of any theoretical approach in an applied environment, the annotators faced a task in which the complexity increased as support from the guidelines tended to decrease. Thus, while rules for segmenting were fairly precise, annotators relied on heuristics requiring more human judgment to assign relations and nuclearity. Another factor is that the cognitive challenge of the task increases as the tree takes shape. It is relatively straightforward for the annotator to make a decision on assignment of nuclearity and relation at the inter-clausal level, but this becomes more complex at the intersentential level, and extremely difficult when linking large segments. This tension between task complexity and guideline under-specification resulted from the practical application of a theoretical model on a broad scale. While other discourse theoretical approaches posit distinctly different treatments for various levels of the discourse (Van Dijk and Kintsch, 1983; Meyer, 1985), RST relies on a standard methodology to analyze the document at all levels. The RST relation set is rich and the concept of nuclearity, somewhat interpretive. This gave our annotators more leeway in interpreting the higher levels of the discourse structure, thus introducing some stylistic differences, which may prove an interesting avenue of future research.</abstract>
<note confidence="0.808509">Details The RST Corpus consists of 385 Wall Street Journal articles from the Penn Treebank, representing over 176,000 words of text. In order to measure inter-annotator consistency, 53 of the documents (13.8%) were double-tagged. The documents range in size from 31 to 2124 words, with an average of 458.14 words per</note>
<abstract confidence="0.994747584269663">document. The final tagged corpus contains 21,789 EDUs with an average of 56.59 EDUs per document. The average number of words per EDU is 8.1. The articles range over a variety of topics, including financial reports, general interest stories, business-related news, cultural reviews, editorials, and letters to the editor. In selecting these documents, we partnered with the Linguistic Data Consortium to select Penn Treebank texts for which the syntactic bracketing was known to be of high caliber. Thus, the RST Corpus provides an additional level of linguistic annotation to supplement existing annotated resources. For details on obtaining the corpus, annotation software, tagging guidelines, and related documentation and resources, http://www.isi.edu/~marcu/discourse. A growing number of groups have developed or are developing discourse-annotated corpora for text. These can be characterized both in terms of the kinds of features annotated as well as by the scope of the annotation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: sentential: of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). hierarchical: of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) document-level: characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000). Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic. Re-tagging of a large number of documents after major enhancements to the annotation guidelines was also time consuming. In addition, limitations of the theoretical approach became more apparent over time. Because the RST theory does not differentiate between different levels of the tree structure, a fairly fine-grained set of relations operates between EDUs and EDU clusters at the macrolevel. The procedural knowledge available at the EDU level is likely to need further refinement for higher-level text spans along the lines of other work which posits a few macro-level relations for text segments, such as Ferrari (1998) or Meyer (1985). Moreover, using the RST approach, the resultant tree structure, like a traditional outline, imposed constraints that other discourse representations (e.g., graph) would not. In combination with the tree structure, the concept of nuclearity also guided an annotator to capture one of a number of possible stylistic interpretations. We ourselves are eager to explore these aspects of the RST, and expect new insights to appear through analysis of the corpus. We anticipate that the RST Corpus will be multifunctional and support a wide range of language engineering applications. The added value of multiple layers of overt linguistic phenomena enhancing the Penn Treebank information can be exploited to advance the study of discourse, to enhance language technologies such as text summarization, machine translation or information retrieval, or to be a testbed for new and creative natural language processing techniques.</abstract>
<note confidence="0.2941236">References Bruce Britton and John Black. 1985. Expository Text. NJ: Lawrence Erlbaum Associates. Jill Burstein, Daniel Marcu, Slava Andreyev, and Martin Chodorow. 2001. Towards automatic identification of discourse elements in In of the Annual Meeting of the Association for Computational France.</note>
<author confidence="0.935148">Jean Carletta</author>
<author confidence="0.935148">Amy Isard</author>
<author confidence="0.935148">Stephen Isard</author>
<author confidence="0.935148">Jacqueline Kowtko</author>
<author confidence="0.935148">Gwyneth Doherty-Sneddon</author>
<abstract confidence="0.896333142857143">and Anne Anderson. 1997. The reliability of a dialogue structure coding Linguistics 13-32. Giacomo Ferrari. 1998. Preliminary steps toward the creation of a discourse and text In of the First International Conference on Language</abstract>
<note confidence="0.666062777777778">Resources and Evaluation (LREC 1998), Granada, Spain, 999-1001. Giovanni Flammia and Victor Zue. 1995. Empirical evaluation of human performance and agreement in parsing discourse constituents in dialogue. In of the European Conference on Speech and Technology, Spain, vol. 3, 1965-1968. Roger Garside, Steve Fligelstone and Simon Botley. 1997. Discourse Annotation: Anaphoric in Corpora. In annotation: Linguistic information from computer text by R. Garside, G. Leech, and T. McEnery. London: Longman, 66-84. Talmy Givon. 1983. Topic continuity in In Continuity in Discourse: a Cross-Language Amsterdam/Philadelphia: John Benjamins, 1-41. Evans Grimes. The Thread of Hague, Paris: Mouton. Barbara Grosz and Candice Sidner. 1986. Attentions, intentions, and the structure of Linguistics, 175-204. Marti Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Linguistics 33-64. Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue Linguistics 501- 530. Eduard Hovy. 1993. Automated discourse generation using discourse structure relations. Intelligence 341-386. Krippendorff. 1980. Analysis:</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bruce Britton</author>
<author>John Black</author>
</authors>
<title>Understanding Expository Text. Hillsdale, NJ: Lawrence Erlbaum Associates.</title>
<date>1985</date>
<contexts>
<context position="3834" citStr="Britton and Black, 1985" startWordPosition="553" endWordPosition="556">ated a number of constraints to our approach. The theoretical framework had to be practical and repeatable over a large set of documents in a reasonable amount of time, with a significant level of consistency across annotators. Thus, our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth, such as anaphoric relations (Garside et al., 1997) or style types (Leech et al., 1997); analysis of a single text from multiple perspectives (Mann and Thompson, 1992); or illustrations of a theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, 1983). Our annotation work is grounded in the Rhetorical Structure Theory (RST) framework (Mann and Thompson, 1988). We decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreeme</context>
</contexts>
<marker>Britton, Black, 1985</marker>
<rawString>Bruce Britton and John Black. 1985. Understanding Expository Text. Hillsdale, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Slava Andreyev</author>
<author>Martin Chodorow</author>
</authors>
<title>Towards automatic identification of discourse elements in essays.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="4871" citStr="Burstein et al., 2001" startWordPosition="716" endWordPosition="719">al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units • The internal nodes of the tree correspond to contiguous text spans • Each node is characterized by its nuclearity – a nucleus indicates a more essential unit of information, while a satellite indicates a supporting or background unit of</context>
</contexts>
<marker>Burstein, Marcu, Andreyev, Chodorow, 2001</marker>
<rawString>Jill Burstein, Daniel Marcu, Slava Andreyev, and Martin Chodorow. 2001. Towards automatic identification of discourse elements in essays. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Amy Isard</author>
<author>Stephen Isard</author>
<author>Jacqueline Kowtko</author>
<author>Gwyneth Doherty-Sneddon</author>
<author>Anne Anderson</author>
</authors>
<title>The reliability of a dialogue structure coding scheme.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<issue>1</issue>
<pages>13--32</pages>
<contexts>
<context position="20336" citStr="Carletta et al., 1997" startWordPosition="3034" endWordPosition="3037">oice of relation and level of attachment in the tree. All trees were checked with a discourse parser and tree traversal program which often identified errors undetected by the manual validation process. In the end, all of the trees worked successfully with these programs. 4.2 Measuring Consistency We tracked inter-annotator agreement during each phase of the project, using a method developed by Marcu et al. (1999) for computing kappa statistics over hierarchical structures. The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997). It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement. The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments. The strengths and shortcomings of the approach are also discussed in detail there. Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa &gt; 0.8 reflect very high agreement, while values between 0.6 and 0.8 reflect good agreement. Table 1 shows average kappa </context>
</contexts>
<marker>Carletta, Isard, Isard, Kowtko, Doherty-Sneddon, Anderson, 1997</marker>
<rawString>Jean Carletta, Amy Isard, Stephen Isard, Jacqueline Kowtko, Gwyneth Doherty-Sneddon, and Anne Anderson. 1997. The reliability of a dialogue structure coding scheme. Computational Linguistics 23(1): 13-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giacomo Ferrari</author>
</authors>
<title>Preliminary steps toward the creation of a discourse and text resource.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>999--1001</pages>
<location>Granada,</location>
<contexts>
<context position="27300" citStr="Ferrari, 1998" startWordPosition="4087" endWordPosition="4088">the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000). Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic. Re-tagging of a large number of documents after major enhancements to the annotation guidelines was also time consuming. In addition, limitations of the t</context>
</contexts>
<marker>Ferrari, 1998</marker>
<rawString>Giacomo Ferrari. 1998. Preliminary steps toward the creation of a discourse and text resource. In Proceedings of the First International Conference on Language Resources and Evaluation (LREC 1998), Granada, Spain, 999-1001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Flammia</author>
<author>Victor Zue</author>
</authors>
<title>Empirical evaluation of human performance and agreement in parsing discourse constituents in spoken dialogue.</title>
<date>1995</date>
<booktitle>In Proceedings of the 4th European Conference on Speech Communication and Technology,</booktitle>
<volume>3</volume>
<pages>1965--1968</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="20359" citStr="Flammia and Zue, 1995" startWordPosition="3038" endWordPosition="3041">vel of attachment in the tree. All trees were checked with a discourse parser and tree traversal program which often identified errors undetected by the manual validation process. In the end, all of the trees worked successfully with these programs. 4.2 Measuring Consistency We tracked inter-annotator agreement during each phase of the project, using a method developed by Marcu et al. (1999) for computing kappa statistics over hierarchical structures. The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997). It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement. The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments. The strengths and shortcomings of the approach are also discussed in detail there. Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa &gt; 0.8 reflect very high agreement, while values between 0.6 and 0.8 reflect good agreement. Table 1 shows average kappa statistics reflecting t</context>
</contexts>
<marker>Flammia, Zue, 1995</marker>
<rawString>Giovanni Flammia and Victor Zue. 1995. Empirical evaluation of human performance and agreement in parsing discourse constituents in spoken dialogue. In Proceedings of the 4th European Conference on Speech Communication and Technology, Madrid, Spain, vol. 3, 1965-1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Garside</author>
<author>Steve Fligelstone</author>
<author>Simon Botley</author>
</authors>
<title>Discourse Annotation: Anaphoric Relations in Corpora. In Corpus annotation: Linguistic information from computer text corpora, edited by</title>
<date>1997</date>
<pages>66--84</pages>
<publisher>Longman,</publisher>
<location>London:</location>
<contexts>
<context position="3620" citStr="Garside et al., 1997" startWordPosition="519" endWordPosition="522"> sufficiently large enough to offer potential for wide-scale use – including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications. These goals necessitated a number of constraints to our approach. The theoretical framework had to be practical and repeatable over a large set of documents in a reasonable amount of time, with a significant level of consistency across annotators. Thus, our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth, such as anaphoric relations (Garside et al., 1997) or style types (Leech et al., 1997); analysis of a single text from multiple perspectives (Mann and Thompson, 1992); or illustrations of a theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, 1983). Our annotation work is grounded in the Rhetorical Structure Theory (RST) framework (Mann and Thompson, 1988). We decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetoric</context>
</contexts>
<marker>Garside, Fligelstone, Botley, 1997</marker>
<rawString>Roger Garside, Steve Fligelstone and Simon Botley. 1997. Discourse Annotation: Anaphoric Relations in Corpora. In Corpus annotation: Linguistic information from computer text corpora, edited by R. Garside, G. Leech, and T. McEnery. London: Longman, 66-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Talmy Givon</author>
</authors>
<title>Topic continuity in discourse.</title>
<date>1983</date>
<booktitle>In Topic Continuity in Discourse:</booktitle>
<publisher>a</publisher>
<contexts>
<context position="7676" citStr="Givon, 1983" startWordPosition="1159" endWordPosition="1160">sequential relation between the first and second sentences. Ideally, we would like to capture that kind of rhetorical information regardless of the syntactic form in which it is conveyed. However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy. It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ab</context>
</contexts>
<marker>Givon, 1983</marker>
<rawString>Talmy Givon. 1983. Topic continuity in discourse. In Topic Continuity in Discourse: a</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quantitative Cross-Language Study</author>
</authors>
<title>Amsterdam/Philadelphia: John Benjamins, 1-41. Joseph Evans Grimes.</title>
<date>1975</date>
<location>Paris: Mouton.</location>
<marker>Study, 1975</marker>
<rawString>Quantitative Cross-Language Study. Amsterdam/Philadelphia: John Benjamins, 1-41. Joseph Evans Grimes. 1975. The Thread of Discourse. The Hague, Paris: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candice Sidner</author>
</authors>
<title>Attentions, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="1446" citStr="Grosz and Sidner, 1986" startWordPosition="194" endWordPosition="197">s. 1 Introduction The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential</context>
<context position="7885" citStr="Grosz and Sidner, 1986" startWordPosition="1186" endWordPosition="1189">as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy. It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ability to identify units consistently on a large scale. In the end, we chose the clause as the elementary unit of discourse, using lexical and syntactic clues to help determine boundaries: 5. [Although Mr. Free</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candice Sidner. 1986. Attentions, intentions, and the structure of discourse. Computational Linguistics, 12(3): 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<issue>1</issue>
<pages>33--64</pages>
<contexts>
<context position="1646" citStr="Hearst, 1997" startWordPosition="228" endWordPosition="229">, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover</context>
<context position="27230" citStr="Hearst, 1997" startWordPosition="4077" endWordPosition="4078">. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000). Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic. Re-tagging of a large number of documents after major enhancements to the annotation</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics 23(1): 33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>3</issue>
<pages>501--530</pages>
<contexts>
<context position="7762" citStr="Hirschberg and Litman, 1993" startWordPosition="1170" endWordPosition="1173">we would like to capture that kind of rhetorical information regardless of the syntactic form in which it is conveyed. However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy. It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ability to identify units consistently on a large scale. In the end, we chose the clause</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics 19(3): 501-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Artificial Intelligence</journal>
<volume>63</volume>
<issue>1</issue>
<pages>341--386</pages>
<marker>Hovy, 1993</marker>
<rawString>Eduard Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelligence 63(1-2): 341-386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology.</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<location>Beverly Hills, CA:</location>
<contexts>
<context position="20786" citStr="Krippendorff, 1980" startWordPosition="3102" endWordPosition="3103">r hierarchical structures. The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997). It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement. The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments. The strengths and shortcomings of the approach are also discussed in detail there. Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa &gt; 0.8 reflect very high agreement, while values between 0.6 and 0.8 reflect good agreement. Table 1 shows average kappa statistics reflecting the agreement of three annotators at various stages of the tasks on selected documents. Different sets of documents were chosen for each stage, with no overlap in documents. The statistics measure annotation reliability at four levels: elementary discourse units, hierarchical spans, hierarchical nuclearity and hierarchical relation assignments. At the unit level, the initial (April 00) scores and final (January 01) scores re</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Beverly Hills, CA: Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Tony McEnery</author>
<author>Martin Wynne</author>
</authors>
<title>Further levels of annotation. In</title>
<date>1997</date>
<contexts>
<context position="3656" citStr="Leech et al., 1997" startWordPosition="526" endWordPosition="529">tential for wide-scale use – including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications. These goals necessitated a number of constraints to our approach. The theoretical framework had to be practical and repeatable over a large set of documents in a reasonable amount of time, with a significant level of consistency across annotators. Thus, our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth, such as anaphoric relations (Garside et al., 1997) or style types (Leech et al., 1997); analysis of a single text from multiple perspectives (Mann and Thompson, 1992); or illustrations of a theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, 1983). Our annotation work is grounded in the Rhetorical Structure Theory (RST) framework (Mann and Thompson, 1988). We decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 19</context>
</contexts>
<marker>Leech, McEnery, Wynne, 1997</marker>
<rawString>Geoffrey Leech, Tony McEnery, and Martin Wynne. 1997. Further levels of annotation. In</rawString>
</citation>
<citation valid="false">
<authors>
<author>Corpus</author>
</authors>
<title>Annotation: Linguistic Information from Computer Text Corpora, edited by</title>
<pages>85--101</pages>
<publisher>Longman,</publisher>
<location>London:</location>
<marker>Corpus, </marker>
<rawString>Corpus Annotation: Linguistic Information from Computer Text Corpora, edited by R. Garside, G. Leech, and T. McEnery. London: Longman, 85-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Longacre</author>
</authors>
<title>The Grammar of Discourse.</title>
<date>1983</date>
<publisher>Plenum Press.</publisher>
<location>New York:</location>
<contexts>
<context position="7693" citStr="Longacre, 1983" startWordPosition="1161" endWordPosition="1162">lation between the first and second sentences. Ideally, we would like to capture that kind of rhetorical information regardless of the syntactic form in which it is conveyed. However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy. It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ability to identify</context>
</contexts>
<marker>Longacre, 1983</marker>
<rawString>Robert Longacre. 1983. The Grammar of Discourse. New York: Plenum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory. Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<contexts>
<context position="1472" citStr="Mann and Thompson, 1988" startWordPosition="198" endWordPosition="201">vent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the o</context>
<context position="3973" citStr="Mann and Thompson, 1988" startWordPosition="574" endWordPosition="577">n a reasonable amount of time, with a significant level of consistency across annotators. Thus, our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth, such as anaphoric relations (Garside et al., 1997) or style types (Leech et al., 1997); analysis of a single text from multiple perspectives (Mann and Thompson, 1992); or illustrations of a theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, 1983). Our annotation work is grounded in the Rhetorical Structure Theory (RST) framework (Mann and Thompson, 1988). We decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 199</context>
<context position="6004" citStr="Mann and Thompson (1988" startWordPosition="900" endWordPosition="903">ential unit of information, while a satellite indicates a supporting or background unit of information. • Each node is characterized by a rhetorical relation that holds between two or more non-overlapping, adjacent text spans. Relations can be of intentional, semantic, or textual nature. Below, we describe the protocol that we used to build consistent RST annotations. 2.1 Segmenting Texts into Units The first step in characterizing the discourse structure of a text in our protocol is to determine the elementary discourse units (EDUs), which are the minimal building blocks of a discourse tree. Mann and Thompson (1988, p. 244) state that “RST provides a general way to describe the relations among clauses in a text, whether or not they are grammatically or lexically signalled.” Yet, applying this intuitive notion to the task of producing a large, consistently annotated corpus is extremely difficult, because the boundary between discourse and syntax can be very blurry. The examples below, which range from two distinct sentences to a single clause, all convey essentially the same meaning, packaged in different ways: 1. [Xerox Corp.’s third-quarter net income grew 6.2% on 7.3% higher revenue.] [This earned mix</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William Mann and Sandra Thompson. 1988. Rhetorical structure theory. Toward a functional theory of text organization. Text, 8(3): 243-281.</rawString>
</citation>
<citation valid="true">
<title>Discourse Description: Diverse Linguistic Analyses of a Fund-raising Text. Amsterdam/Philadelphia:</title>
<date>1992</date>
<editor>William Mann and Sandra Thompson, eds.</editor>
<publisher>John Benjamins.</publisher>
<marker>1992</marker>
<rawString>William Mann and Sandra Thompson, eds. 1992. Discourse Description: Diverse Linguistic Analyses of a Fund-raising Text. Amsterdam/Philadelphia: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="4656" citStr="Marcu, 2000" startWordPosition="684" endWordPosition="685">lds rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units • The internal nodes of</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Estibaliz Amorrortu</author>
<author>Magdelena Romera</author>
</authors>
<title>Experiments in constructing a corpus of discourse trees.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop on Standards and Tools for Discourse Tagging,</booktitle>
<pages>48--57</pages>
<location>College Park, MD,</location>
<contexts>
<context position="1811" citStr="Marcu et al., 1999" startWordPosition="250" endWordPosition="253">or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selection of theoretical appr</context>
<context position="4259" citStr="Marcu et al., 1999" startWordPosition="620" endWordPosition="623">ch et al., 1997); analysis of a single text from multiple perspectives (Mann and Thompson, 1992); or illustrations of a theoretical model on a single representative text (Britton and Black, 1985; Van Dijk and Kintsch, 1983). Our annotation work is grounded in the Rhetorical Structure Theory (RST) framework (Mann and Thompson, 1988). We decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein e</context>
<context position="11730" citStr="Marcu et al. (1999)" startWordPosition="1745" endWordPosition="1748"> Topic-Comment, Summary, Temporal, TopicChange. For example, the class Explanation includes the relations evidence, explanationargumentative, and reason, while TopicComment includes problem-solution, questionanswer, statement-response, topic-comment, and comment-topic. In addition, three relations are used to impose structure on the tree: textualorganization, span, and same-unit (used to link parts of units separated by embedded units or spans). 3 Discourse Annotation Task Our methodology for annotating the RST Corpus builds on prior corpus work in the Rhetorical Structure Theory framework by Marcu et al. (1999). Because the goal of this effort was to build a high-quality, consistently annotated reference corpus, the task required that we employ people as annotators whose primary professional experience was in the area of language analysis and reporting, provide extensive annotator training, and specify a rigorous set of annotation guidelines. 3.1 Annotator Profile and Training The annotators hired to build the corpus were all professional language analysts with prior experience in other types of data annotation. They underwent extensive hands-on training, which took place roughly in three phases. Du</context>
<context position="20132" citStr="Marcu et al. (1999)" startWordPosition="3006" endWordPosition="3009">a single root node and comparing the tree to the document to check for missing sentences or fragments from the end of the text. Semantic checking involved reviewing nuclearity assignments, as well as choice of relation and level of attachment in the tree. All trees were checked with a discourse parser and tree traversal program which often identified errors undetected by the manual validation process. In the end, all of the trees worked successfully with these programs. 4.2 Measuring Consistency We tracked inter-annotator agreement during each phase of the project, using a method developed by Marcu et al. (1999) for computing kappa statistics over hierarchical structures. The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997). It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement. The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments. The strengths and shortcomings of the approach are also discussed in detail there</context>
<context position="23380" citStr="Marcu et al., 1999" startWordPosition="3500" endWordPosition="3503">this manner, with November results ranging from 0.78 to 0.82. In order to see how much of the improvement had to do with pre-segmenting, we asked the same three annotators to annotate five previously unseen documents in January, without reference to a pre-segmented document. The results of this experiment are given in the last row of Table 1, and they reflect only a small overall decline in performance from the November results. These scores reflect very strong agreement and represent a significant improvement over previously reported results on annotating multiple texts in the RST framework (Marcu et al., 1999). Table 2 reports final results for all pairs of taggers who double-annotated four or more documents, representing 30 out of the 53 documents that were double-tagged. Results are based on pre-segmented documents. Our team was able to reach a significant level of consistency, even though they faced a number of challenges which reflect differences in the agreement scores at the various levels. While operating under the constraints typical of any theoretical approach in an applied environment, the annotators faced a task in which the complexity increased as support from the guidelines tended to d</context>
<context position="27109" citStr="Marcu, et al. 1999" startWordPosition="4060" endWordPosition="4063">tation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000). Annotation of a single document could take anywhere from 30 minutes to several hours</context>
</contexts>
<marker>Marcu, Amorrortu, Romera, 1999</marker>
<rawString>Daniel Marcu, Estibaliz Amorrortu, and Magdelena Romera. 1999. Experiments in constructing a corpus of discourse trees. In Proceedings of the ACL Workshop on Standards and Tools for Discourse Tagging, College Park, MD, 48-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Lynn Carlson</author>
<author>Maki Watanabe</author>
</authors>
<title>The automatic translation of discourse structures.</title>
<date>2000</date>
<booktitle>Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>9--17</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="4748" citStr="Marcu et al. 2000" startWordPosition="697" endWordPosition="700">s that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units • The internal nodes of the tree correspond to contiguous text spans • Each node is characterized by its nuclearity</context>
</contexts>
<marker>Marcu, Carlson, Watanabe, 2000</marker>
<rawString>Daniel Marcu, Lynn Carlson, and Maki Watanabe. 2000. The automatic translation of discourse structures. Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics, Seattle, WA, 9-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="2591" citStr="Marcus et al., 1993" startWordPosition="368" endWordPosition="371">fort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selection of theoretical approach, annotation methodology, training, and quality assurance. The resulting corpus contains 385 documents of American English selected from the Penn Treebank (Marcus et al., 1993), annotated in the framework of Rhetorical Structure Theory. We believe this resource holds great promise as a rich new source of textlevel information to support multiple lines of research for language understanding applications. 2 Framework Two principle goals underpin the creation of this discourse-tagged corpus: 1) The corpus should be grounded in a particular theoretical approach, and 2) it should be sufficiently large enough to offer potential for wide-scale use – including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications. </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank, Computational Linguistics 19(2), 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Meyer</author>
</authors>
<title>Prose Analysis: Purposes, Procedures, and Problems. In Understanding Expository Text,</title>
<date>1985</date>
<journal>Lawrence Erlbaum Associates,</journal>
<pages>11--64</pages>
<location>Hillsdale, NJ:</location>
<note>edited by</note>
<contexts>
<context position="1422" citStr="Meyer, 1985" startWordPosition="192" endWordPosition="193">c applications. 1 Introduction The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community</context>
<context position="24799" citStr="Meyer, 1985" startWordPosition="3714" endWordPosition="3715"> of the task increases as the tree takes shape. It is relatively straightforward for the annotator to make a decision on assignment of nuclearity and relation at the inter-clausal level, but this becomes more complex at the intersentential level, and extremely difficult when linking large segments. This tension between task complexity and guideline under-specification resulted from the practical application of a theoretical model on a broad scale. While other discourse theoretical approaches posit distinctly different treatments for various levels of the discourse (Van Dijk and Kintsch, 1983; Meyer, 1985), RST relies on a standard methodology to analyze the document at all levels. The RST relation set is rich and the concept of nuclearity, somewhat interpretive. This gave our annotators more leeway in interpreting the higher levels of the discourse structure, thus introducing some stylistic differences, which may prove an interesting avenue of future research. 5 Corpus Details The RST Corpus consists of 385 Wall Street Journal articles from the Penn Treebank, representing over 176,000 words of text. In order to measure inter-annotator consistency, 53 of the documents (13.8%) were double-tagged</context>
<context position="28388" citStr="Meyer (1985)" startWordPosition="4259" endWordPosition="4260">f documents after major enhancements to the annotation guidelines was also time consuming. In addition, limitations of the theoretical approach became more apparent over time. Because the RST theory does not differentiate between different levels of the tree structure, a fairly fine-grained set of relations operates between EDUs and EDU clusters at the macrolevel. The procedural knowledge available at the EDU level is likely to need further refinement for higher-level text spans along the lines of other work which posits a few macro-level relations for text segments, such as Ferrari (1998) or Meyer (1985). Moreover, using the RST approach, the resultant tree structure, like a traditional outline, imposed constraints that other discourse representations (e.g., graph) would not. In combination with the tree structure, the concept of nuclearity also guided an annotator to capture one of a number of possible stylistic interpretations. We ourselves are eager to explore these aspects of the RST, and expect new insights to appear through analysis of the corpus. We anticipate that the RST Corpus will be multifunctional and support a wide range of language engineering applications. The added value of m</context>
</contexts>
<marker>Meyer, 1985</marker>
<rawString>Bonnie Meyer. 1985. Prose Analysis: Purposes, Procedures, and Problems. In Understanding Expository Text, edited by B. Britton and J. Black. Hillsdale, NJ: Lawrence Erlbaum Associates, 11-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
</authors>
<title>Participating in Explanatory Dialogues: Interpreting and Responding to Questions in Context. Cambridge,</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<location>MA:</location>
<contexts>
<context position="1790" citStr="Moore, 1995" startWordPosition="248" endWordPosition="249">uch as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selectio</context>
<context position="4611" citStr="Moore, 1995" startWordPosition="678" endWordPosition="679">r three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called eleme</context>
<context position="27088" citStr="Moore, 1995" startWordPosition="4058" endWordPosition="4059">e of the annotation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000). Annotation of a single document could take anywhere from 30 min</context>
</contexts>
<marker>Moore, 1995</marker>
<rawString>Johanna Moore. 1995. Participating in Explanatory Dialogues: Interpreting and Responding to Questions in Context. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
<author>Cecile Paris</author>
</authors>
<title>Planning text for advisory dialogues: capturing intentional and rhetorical information.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>4</issue>
<pages>651--694</pages>
<contexts>
<context position="4597" citStr="Moore and Paris, 1993" startWordPosition="674" endWordPosition="677">e decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Novy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse</context>
</contexts>
<marker>Moore, Paris, 1993</marker>
<rawString>Johanna Moore and Cecile Paris. 1993. Planning text for advisory dialogues: capturing intentional and rhetorical information. Computational Linguistics 19(4): 651-694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan Moser</author>
<author>Johanna Moore</author>
</authors>
<title>Investigating cue selection and placement in tutorial discourse.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>130--135</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="1790" citStr="Moser and Moore, 1995" startWordPosition="246" endWordPosition="249">rmation, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selectio</context>
<context position="27088" citStr="Moser and Moore, 1995" startWordPosition="4056" endWordPosition="4059">y the scope of the annotation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000). Annotation of a single document could take anywhere from 30 min</context>
</contexts>
<marker>Moser, Moore, 1995</marker>
<rawString>Megan Moser and Johanna Moore. 1995. Investigating cue selection and placement in tutorial discourse. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, MA, 130-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Learning discourse relations with active data selection.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>158--167</pages>
<location>College Park, MD,</location>
<contexts>
<context position="1702" citStr="Nomoto and Matsumoto, 1999" startWordPosition="232" endWordPosition="235">energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges </context>
<context position="26915" citStr="Nomoto and Matsumoto, 1999" startWordPosition="4030" endWordPosition="4033">g number of groups have developed or are developing discourse-annotated corpora for text. These can be characterized both in terms of the kinds of features annotated as well as by the scope of the annotation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus</context>
</contexts>
<marker>Nomoto, Matsumoto, 1999</marker>
<rawString>Tadashi Nomoto and Yuji Matsumoto. 1999. Learning discourse relations with active data selection. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, College Park, MD, 158-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Diane Litman</author>
</authors>
<title>Discourse segmentation by human and automatic means.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<issue>1</issue>
<pages>103--140</pages>
<contexts>
<context position="20389" citStr="Passonneau and Litman, 1997" startWordPosition="3042" endWordPosition="3045">e tree. All trees were checked with a discourse parser and tree traversal program which often identified errors undetected by the manual validation process. In the end, all of the trees worked successfully with these programs. 4.2 Measuring Consistency We tracked inter-annotator agreement during each phase of the project, using a method developed by Marcu et al. (1999) for computing kappa statistics over hierarchical structures. The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997). It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement. The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments. The strengths and shortcomings of the approach are also discussed in detail there. Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa &gt; 0.8 reflect very high agreement, while values between 0.6 and 0.8 reflect good agreement. Table 1 shows average kappa statistics reflecting the agreement of three annotato</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca Passonneau and Diane Litman. 1997. Discourse segmentation by human and automatic means. Computational Linguistics 23(1): 103-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Paule Pery-Woodley</author>
<author>Josette Rebeyrolle</author>
</authors>
<title>Domain and genre in sublanguage text: definitional microtexts in three corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation (LREC-1998),</booktitle>
<pages>987--992</pages>
<location>Granada,</location>
<contexts>
<context position="27406" citStr="Pery-Woodley and Rebeyrolle, 1998" startWordPosition="4099" endWordPosition="4102">es at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000). Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic. Re-tagging of a large number of documents after major enhancements to the annotation guidelines was also time consuming. In addition, limitations of the theoretical approach became more apparent over time. Because the RST theory does not differentiate between </context>
</contexts>
<marker>Pery-Woodley, Rebeyrolle, 1998</marker>
<rawString>Marie-Paule Pery-Woodley and Josette Rebeyrolle. 1998. Domain and genre in sublanguage text: definitional microtexts in three corpora. In Proceedings of the First International Conference on Language Resources and Evaluation (LREC-1998), Granada, Spain, 987-992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
</authors>
<title>A formal model of the structure of discourse.</title>
<date>1988</date>
<journal>Journal of Pragmatics</journal>
<volume>12</volume>
<pages>601--638</pages>
<contexts>
<context position="7818" citStr="Polanyi, 1988" startWordPosition="1180" endWordPosition="1181">s of the syntactic form in which it is conveyed. However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy. It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ability to identify units consistently on a large scale. In the end, we chose the clause as the elementary unit of discourse, using lexical and </context>
</contexts>
<marker>Polanyi, 1988</marker>
<rawString>Livia Polanyi. 1988. A formal model of the structure of discourse. Journal of Pragmatics 12: 601-638.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
</authors>
<title>The linguistic structure of discourse. Center for the Study of Language and Information.</title>
<date>1996</date>
<pages>96--200</pages>
<contexts>
<context position="8079" citStr="Polanyi, 1996" startWordPosition="1217" endWordPosition="1218">romises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ability to identify units consistently on a large scale. In the end, we chose the clause as the elementary unit of discourse, using lexical and syntactic clues to help determine boundaries: 5. [Although Mr. Freeman is retiring,] [he will continue to work as a consultant for American Express on a project basis.]wsj_1317 6. [Bond Corp., a brewing, property, media and resources company, is selling many of</context>
</contexts>
<marker>Polanyi, 1996</marker>
<rawString>Livia Polanyi. 1996. The linguistic structure of discourse. Center for the Study of Language and Information. CSLI-96-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josette Rebeyrolle</author>
</authors>
<title>Utilisation de contextes définitoires pour l’acquisition de connaissances à partir de textes.</title>
<date>2000</date>
<booktitle>In Actes Journées Francophones d’Ingénierie de la Connaissance (IC’2000),</booktitle>
<pages>105--114</pages>
<location>Toulouse, IRIT,</location>
<contexts>
<context position="26934" citStr="Rebeyrolle, 2000" startWordPosition="4034" endWordPosition="4035">loped or are developing discourse-annotated corpora for text. These can be characterized both in terms of the kinds of features annotated as well as by the scope of the annotation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a labor-intensive effort. Building the RST Corpus involved more than</context>
</contexts>
<marker>Rebeyrolle, 2000</marker>
<rawString>Josette Rebeyrolle. 2000. Utilisation de contextes définitoires pour l’acquisition de connaissances à partir de textes. In Actes Journées Francophones d’Ingénierie de la Connaissance (IC’2000), Toulouse, IRIT, 105-114.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Harvey Sacks</author>
</authors>
<title>Emmanuel Schegloff,</title>
<location>and</location>
<marker>Sacks, </marker>
<rawString>Harvey Sacks, Emmanuel Schegloff, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gail Jefferson</author>
</authors>
<title>A simple systematics for the organization of turntaking in conversation.</title>
<date>1974</date>
<journal>Language</journal>
<volume>50</volume>
<pages>696--735</pages>
<marker>Jefferson, 1974</marker>
<rawString>Gail Jefferson. 1974. A simple systematics for the organization of turntaking in conversation. Language 50: 696-735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegal</author>
<author>N J Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<marker>Siegal, Castellan, 1988</marker>
<rawString>Sidney Siegal and N.J. Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences. New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Sundheim</author>
</authors>
<title>Overview of results of the MUC-6 evaluation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>13--31</pages>
<location>Columbia, MD,</location>
<contexts>
<context position="26868" citStr="Sundheim, 1995" startWordPosition="4024" endWordPosition="4025">cu/discourse. 6 Discussion A growing number of groups have developed or are developing discourse-annotated corpora for text. These can be characterized both in terms of the kinds of features annotated as well as by the scope of the annotation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a </context>
</contexts>
<marker>Sundheim, 1995</marker>
<rawString>Beth Sundheim. 1995. Overview of results of the MUC-6 evaluation. In Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, MD, 13-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin K T’sou</author>
<author>Tom B Y Lai</author>
<author>Samuel W K Chan</author>
<author>Weijun Gao</author>
<author>Xuegang Zhan</author>
</authors>
<title>Enhancement of Chinese discourse marker tagger with C.4.5.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Chinese Language Processing Workshop, Hong Kong,</booktitle>
<pages>38--45</pages>
<marker>T’sou, Lai, Chan, Gao, Zhan, 2000</marker>
<rawString>Benjamin K. T’sou, Tom B.Y. Lai, Samuel W.K. Chan, Weijun Gao, and Xuegang Zhan. 2000. Enhancement of Chinese discourse marker tagger with C.4.5. In Proceedings of the Second Chinese Language Processing Workshop, Hong Kong, 38-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teun A Van Dijk</author>
<author>Walter Kintsch</author>
</authors>
<date>1983</date>
<booktitle>Strategies of Discourse Comprehension.</booktitle>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>Van Dijk, Kintsch, 1983</marker>
<rawString>Teun A. Van Dijk and Walter Kintsch. 1983. Strategies of Discourse Comprehension. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
<author>Donna Harman</author>
</authors>
<date>1999</date>
<booktitle>The Eighth Text Retrieval Conference (TREC8). NIST Special Publication</booktitle>
<pages>500--246</pages>
<contexts>
<context position="1241" citStr="Voorhees and Harman, 1999" startWordPosition="164" endWordPosition="167">g a well-defined methodology and protocol. This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, discourse-specific applications. 1 Introduction The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount ou</context>
</contexts>
<marker>Voorhees, Harman, 1999</marker>
<rawString>Ellen Voorhees and Donna Harman. 1999. The Eighth Text Retrieval Conference (TREC8). NIST Special Publication 500-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Wayne</author>
</authors>
<title>Multilingual topic detection and tracking: successful research enabled by corpora and evaluation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC-2000),</booktitle>
<pages>1487--1493</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="1255" citStr="Wayne, 2000" startWordPosition="168" endWordPosition="169">y and protocol. This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, discourse-specific applications. 1 Introduction The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience i</context>
</contexts>
<marker>Wayne, 2000</marker>
<rawString>Charles Wayne. 2000. Multilingual topic detection and tracking: successful research enabled by corpora and evaluation. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC-2000), Athens, Greece, 1487-1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rebecca Bruce</author>
<author>Thomas O’Hara</author>
</authors>
<title>Development and use of a goldstandard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. College Park, MD,</booktitle>
<pages>246--253</pages>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Janyce Wiebe, Rebecca Bruce, and Thomas O’Hara. 1999. Development and use of a goldstandard data set for subjectivity classifications. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. College Park, MD, 246-253.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>