<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001185">
<title confidence="0.993448">
Comparing Information Extraction Pattern Models
</title>
<author confidence="0.998193">
Mark Stevenson and Mark A. Greenwood
</author>
<affiliation confidence="0.9979855">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.99159">
Sheffield, S1 4DP, UK
</address>
<email confidence="0.999742">
{marks,m.greenwood}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986795">
Several recently reported techniques for
the automatic acquisition of Information
Extraction (IE) systems have used depen-
dency trees as the basis of their extrac-
tion pattern representation. These ap-
proaches have used a variety of pattern
models (schemes for representing IE pat-
terns based on particular parts of the de-
pendency analysis). An appropriate model
should be expressive enough to represent
the information which is to be extracted
from text without being overly compli-
cated. Four previously reported pattern
models are evaluated using existing IE
evaluation corpora and three dependency
parsers. It was found that one model,
linked chains, could represent around 95%
of the information of interest without gen-
erating an unwieldy number of possible
patterns.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934884615385">
A common approach to Information Extraction
(IE) is to use patterns which match against text
and identify items of interest. Patterns are applied
to text which has undergone various levels of lin-
guistic analysis, such as phrase chunking (Soder-
land, 1999) and full syntactic parsing (Gaizauskas
et al., 1996). The approaches use different defini-
tions of what constitutes a valid pattern. For exam-
ple, the AutoSlog system (Riloff, 1993) uses pat-
terns which match certain grammatical categories,
mainly nouns and verbs, in phrase chunked text
while Yangarber et al. (2000) use subject-verb-
object tuples derived from a dependency parse. An
appropriate pattern language must encode enough
information about the text to be able to accurately
identify the items of interest. However, it should
not contain so much information as to be complex
and impractical to apply.
Several recent approaches to IE have used pat-
terns based on a dependency analysis of the input
text (Yangarber, 2003; Sudo et al., 2001; Sudo et
al., 2003; Bunescu and Mooney, 2005; Stevenson
and Greenwood, 2005). These approaches have
used a variety of pattern models (schemes for rep-
resenting IE patterns based on particular parts of
the dependency tree). For example, Yangarber
(2003) uses just subject-verb-object tuples while
Sudo et al. (2003) allow any subpart of the tree to
act as an extraction pattern. The set of patterns al-
lowed by the first model is a proper subset of the
second and therefore captures less of the informa-
tion contained in the dependency tree. Little anal-
ysis has been carried out into the appropriateness
of each model. Sudo et al. (2003) compared three
models in terms of their ability to identify event
participants.
The choice of pattern model has an effect on
the number of potential patterns. This has impli-
cations on the practical application for each ap-
proach, particularly when used for automatic ac-
quisition of IE systems using learning methods
(Yangarber et al., 2000; Sudo et al., 2003; Bunescu
and Mooney, 2005). This paper evaluates the ap-
propriateness of four pattern models in terms of
the competing aims of expressive completeness
(ability to represent information in text) and com-
plexity (number of possible patterns). Each model
is examined by comparing it against a corpus an-
notated with events and determining the propor-
tion of those which it is capable of representing.
The remainder of this paper is organised as fol-
lows: a variety of dependency-tree-based IE pat-
</bodyText>
<page confidence="0.984578">
12
</page>
<note confidence="0.9628965">
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 12–19,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999614">
Figure 1: An example dependency tree.
</figureCaption>
<bodyText confidence="0.999923">
tern models are introduced (Sections 2 and 3).
Section 4 describes experiments comparing each
model and the results are discussed in Section 5.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="method">
2 Pattern Models
</sectionHeader>
<bodyText confidence="0.99981056">
In dependency analysis (Mel’ˇcuk, 1987) the syn-
tax of a sentence is represented by a set of directed
binary links between a word (the head) and one of
its modifiers. These links may be labelled to in-
dicate the grammatical relation between the head
and modifier (e.g. subject, object). In general
cyclical paths are disallowed so that the analysis
forms a tree structure. An example dependency
analysis for the sentence “Acme Inc. hired Mr
Smith as their new CEO, replacing Mr Bloggs.”
is shown Figure 1.
The remainder of this section outlines four mod-
els for representing extraction patterns which can
be derived from dependency trees.
Predicate-Argument Model (SVO): A simple
approach, used by Yangarber (2003) and Steven-
son and Greenwood (2005), is to use subject-verb-
object tuples from the dependency parse as extrac-
tion patterns. These consist of a verb and its sub-
ject and/or direct object1. An SVO pattern is ex-
tracted for each verb in a sentence. Figure 2 shows
the two SVO patterns2 which are produced for the
dependency tree shown in Figure 1.
This model may be motivated by the assump-
tion that many IE scenarios involve the extraction
</bodyText>
<footnote confidence="0.978124">
1Yangarber et al. (2000) and Sudo et al. (2003) used a
slightly extended version of this model in which the pattern
also included certain phrases which referred to either the sub-
ject or object.
2The formalism used for representing dependency pat-
terns is similar to the one introduced by Sudo et al. (2003).
Each node in the tree is represented in the format a[b/c]
(e.g. subj[N/bomber]) where c is the lexical item
(bomber), b its grammatical tag (N) and a the dependency
relation between this node and its parent (subj). The rela-
tionship between nodes is represented as X(A+B+C) which
indicates that nodes A, B and C are direct descendents of node
X.
</footnote>
<bodyText confidence="0.999013019607843">
of participants in specific events. For example,
the MUC-6 (MUC, 1995) management succession
scenario concerns the identification of individuals
who are changing job. These events are often de-
scribed using a simple predicate argument struc-
ture, e.g. “Acme Inc. fired Smith”. However,
the SVO model cannot represent information de-
scribed using other linguistic constructions such as
nominalisations or prepositional phrases. For ex-
ample, in the MUC6 texts it is common for job ti-
tles to be mentioned within prepositional phrases,
e.g. “Smith joined Acme Inc. as CEO”.
Chains: A pattern is defined as a path between
a verb node and any other node in the dependency
tree passing through zero or more intermediate
nodes (Sudo et al., 2001). Figure 2 shows the eight
chains which can be extracted from the tree in Fig-
ure 1.
Chains provide a mechanism for encoding in-
formation beyond the direct arguments of predi-
cates and includes areas of the dependency tree ig-
nored by the SVO model. For example, they can
represent information expressed as a nominalisa-
tion or within a prepositional phrase, e.g. “The
resignation of Smith from the board of Acme ...”
However, a potential shortcoming of this model is
that it cannot represent the link between arguments
of a verb. Patterns in the chain model format are
unable to represent even the simplest of sentences
containing a transitive verb, e.g. “Smith left Acme
Inc.”.
Linked Chains: The linked chains model
(Greenwood et al., 2005) represents extraction
patterns as a pair of chains which share the same
verb but no direct descendants. This model gen-
erates 14 patterns for the verb hire in Figure 1,
examples of which are shown in Figure 2. This
pattern representation encodes most of the infor-
mation in the sentence with the advantage of being
able to link together event participants which nei-
ther of the SVO or chain model can, for example
the relation between “Smith” and “Bloggs”.
Subtrees: The final model to be considered is
the subtree model (Sudo et al., 2003). In this
model any subtree of a dependency tree can be
used as an extraction pattern, where a subtree is
any set of nodes in the tree which are connected to
one another. Single nodes are not considered to be
subtrees. The subtree model is a richer representa-
tion than those discussed so far and can represent
any part of a dependency tree. Each of the previ-
</bodyText>
<page confidence="0.996911">
13
</page>
<figure confidence="0.994024461538462">
SVO Chains
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith]) [V/hire](subj[N/Acme Inc.])
[V/replace](obj[N/Mr Bloggs]) [V/hire](obj[N/Mr Smith])
[V/hire](obj[N/Mr Smith](as[N/CEO]))
[V/hire](obj[N/Mr Smith](as[N/CEO](gen[N/their])))
[V/hire](obj[N/Mr Smith](as[N/CEO](mod[A/new])))
[V/hire](vpsc mod[V/replace])
[V/hire](vpsc mod[V/replace](obj[N/Mr Bloggs]))
[V/replace](obj[N/Mr Bloggs])
Linked Chains
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith])
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith](as[N/CEO]))
[V/hire](obj[N/Mr Smith]+vpsc mod[V/replace](obj[N/Mr Bloggs]))
</figure>
<figureCaption confidence="0.999798">
Figure 2: Example patterns for three models
</figureCaption>
<bodyText confidence="0.999965">
ous models form a proper subset of the subtrees.
By choosing an appropriate subtree it is possible
to link together any pair of nodes in a tree and
consequently this model can represent the relation
between any set of items in the sentence.
</bodyText>
<sectionHeader confidence="0.973952" genericHeader="method">
3 Pattern Enumeration and Complexity
</sectionHeader>
<bodyText confidence="0.948740777777778">
In addition to encoding different parts of the de-
pendency analysis, each pattern model will also
generate a different number of potential patterns.
A dependency tree, T, can be viewed as a set
of N connected nodes. Assume that V , such that
V C_ N, is the set of nodes in the dependency tree
labelled as a verb.
Predicate-Argument Model (SVO): The num-
ber of SVO patterns extracted from T is:
</bodyText>
<equation confidence="0.844306">
Nsvo (T) = |V  |(1)
</equation>
<bodyText confidence="0.8355086">
Chain Model: A chain can be created between
any verb and a node it dominates (directly or indi-
rectly). Now assume that d(v) denotes the count
of a node v and all its descendents then the number
of chains is given by:
</bodyText>
<equation confidence="0.9815445">
�Nchains (T) = ( d (v) − 1) (2)
v∈V
</equation>
<bodyText confidence="0.78881375">
Linked Chains: Let C(v) denote the set of di-
rect child nodes of node v and vi denote the i-th
child, so C(v) = {v1, v2, ...v|C(v) |}. The number
of possible linked chains in T is given by:
</bodyText>
<equation confidence="0.594006">
d(vi) d(vj)
</equation>
<bodyText confidence="0.8914204">
(3)
Subtrees: Now assume that sub(n) is a func-
tion denoting the number of subtrees, including
single nodes, rooted at node n. This can be de-
fined recursively as follows:
</bodyText>
<equation confidence="0.966615">
�sub(n) − |N |(5)
</equation>
<bodyText confidence="0.999995724137931">
The dependency tree shown in Figure 1 gener-
ates 2, 8, 14 and 42 possible SVO, chain, linked
chain and subtree patterns respectively. The num-
ber of SVO patterns is constant on the number of
verbs in the tree. The number of chains is gener-
ally a linear function on the size of the tree but,
in the worst case, can be polynomial. The linked
chain model generates a polynomial number of
patterns while the subtree model is exponential.
There is a clear tradeoff between the complex-
ity of pattern representations and the practicality
of computation using them. Some pattern rep-
resentations are more expressive, in terms of the
amount of information from the dependency tree
they make use of, than others (Section 2) and are
therefore more likely to produce accurate extrac-
tion patterns. However, the more expressive mod-
els will add extra complexities during computation
since a greater number of patterns will be gen-
erated. This complexity, both in the number of
patterns produced and the computational effort re-
quired to produce them, limits the algorithms that
can reasonably be applied to learn useful extrac-
tion patterns.
For a pattern model to be suitable for an ex-
traction task it needs to be expressive enough to
encode enough information from the dependency
parse to accurately identify the items which need
to be extracted. However, we also aim for the
</bodyText>
<equation confidence="0.97079">
{ 1 if n is a leaf node
|C(n)|
sub(n) =
�
i=1
(sub (ni) + 1) otherwise
(4)
</equation>
<bodyText confidence="0.5800335">
The total number of subtrees in a tree is given
by:
</bodyText>
<figure confidence="0.698549">
�Nlinked chains (T) =
|C(v)|
�
v∈V i=1
|C(v)|
�
j=i+1
Nsubtree (T) = 1:
n∈N
</figure>
<page confidence="0.984852">
14
</page>
<bodyText confidence="0.999938">
model to be as computationally tractable as pos-
sible. The ideal model will then be one with suffi-
cient expressive power while at the same time not
including extra information which would make its
use less practical.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999992">
We carried out experiments to determine how suit-
able the pattern representations detailed in Section
2 are for encoding the information of interest to
IE systems. We chose a set of IE corpora anno-
tated with the information to be extracted (detailed
in Section 4.1), generated sets of patterns using a
variety of dependency parsers (Section 4.2) which
were then examined to discover how much of the
target information they contain (Section 4.3).
</bodyText>
<subsectionHeader confidence="0.923728">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999928333333334">
Corpora representing different genres of text were
chosen for these experiments; one containing
newspaper text and another composed of biomed-
ical abstracts. The first corpus consisted of Wall
Street Journal texts from the Sixth Message Un-
derstanding Conference (MUC, 1995) IE evalu-
ation. These are reliably annotated with details
about the movement of executives between jobs.
We make use of a version of the corpus pro-
duced by Soderland (1999) in which events de-
scribed within a single sentence were annotated.
Events in this corpus identify relations between
up to four entities: PersonIn (the person start-
ing a new job), PersonOut (person leaving a
job), Post (the job title) and Organisation
(the employer). These events were broken down
into a set of binary relationships. For exam-
ple, the sentence “Smith was recently made chair-
man of Acme.” contains information about the
new employee (Smith), post (chairman) and or-
ganisation (Acme). Events are represented as a
set of binary relationships, Smith-chairman,
chairman-Acme and Smith-Acme for this
example.
The second corpus uses documents taken from
the biomedical domain, specifically the train-
ing corpus used in the LLL-05 challenge task
(N´edellec, 2005), and a pair of corpora (Craven
and Kumlien, 1999) which were derived from the
Yeast Proteome Database (YPD) (Hodges et al.,
1999) and the Online Mendelian Inheritance in
Man database (OMIM) (Hamosh et al., 2002).
Each of these corpora are annotated with binary
relations between pairs of entities. The LLL-05
corpora contains interactions between genes and
proteins. For example the sentence “Expression
of the sigma(K)-dependent cwlH gene depended
on gerE” contains relations between sigma(K) and
cwlH and between gerE and cwlH. The YPD cor-
pus is concerned with the subcellular compart-
ments in which particular yeast proteins localize.
An example sentence “Uba2p is located largely in
the nucleus” relates Uba2p and the nucleus. The
relations in the OMIM corpora are between genes
and diseases, for example “Most sporadic colorec-
tal cancers also have two APC mutations” con-
tains a relation between APC and colorectal can-
cer.
The MUC6 corpus contains a total of six pos-
sible binary relations. Each of the three biomedi-
cal corpora contain a single relation type, giving a
total of nine binary relations for the experiments.
There are 3911 instances of binary relations in all
corpora.
</bodyText>
<subsectionHeader confidence="0.994986">
4.2 Generating Dependency Patterns
</subsectionHeader>
<bodyText confidence="0.99988064">
Three dependency parsers were used for these ex-
periments: MINIPAR3 (Lin, 1999), the Machinese
Syntax4 parser from Connexor Oy (Tapanainen
and J¨arvinen, 1997) and the Stanford5 parser
(Klein and Manning, 2003). These three parsers
represent a cross-section of approaches to produc-
ing dependency analyses: MINIPAR uses a con-
stituency grammar internally before converting
the result to a dependency tree, Machinese Syn-
tax uses a functional dependency grammar, and
the Stanford Parser is a lexicalized probabilistic
parser.
Before these parsers were applied to the various
corpora the named entities participating in rela-
tions are replaced by a token indicating their class.
For example, in the MUC6 corpus “Acme hired
Smith” would become “Organisation hired
PersonIn”. Each parser was adapted to deal
with these tokens correctly. The parsers were ap-
plied to each corpus and patterns extracted from
the dependency trees generated.
The analyses produced by the parsers were post-
processed to make the most of the information
they contain and ensure consistent structures from
which patterns could be extracted. It was found
</bodyText>
<footnote confidence="0.999970666666667">
3http://www.cs.ualberta.ca/˜lindek/
4http://www.connexor.com/software/syntax/
5http://www-nlp.stanford.edu/software/
</footnote>
<page confidence="0.975028">
15
</page>
<table confidence="0.9995295">
Parser SVO Chains Linked chains Subtrees
MINIPAR 2,980 52,659 149,504 353,778,240,702,149,000
Machinese Syntax 2,382 67,690 265,631 4,641,825,924
Stanford 2,950 76,620 478,643 1,696,259,251,073
</table>
<tableCaption confidence="0.999833">
Table 1: Number of patterns produced for each pattern model by different parsers
</tableCaption>
<bodyText confidence="0.999956880952381">
that the parsers were often unable to generate a de-
pendency tree which included the whole sentence
and instead generate an analysis consisting of sen-
tence fragments represented as separate tree struc-
tures. Some fragments did not include a verb so
no patterns could be extracted. To take account of
this we allowed the root node of any tree fragment
to take the place of a verb in a pattern (see Sec-
tion 2). This leads to the generation of more chain
and linked chain patterns but has no effect on the
number of SVO patterns or subtrees.
Table 1 shows the number of patterns generated
from the dependency trees produced by each of the
parsers. The number of subtrees generated from
the MINIPAR parses is several orders of magnitude
higher than the others because MINIPAR allows
certain nodes to be the modifier of two separate
nodes to deal with phenomena such as conjunc-
tion, anaphora and VP-coordination. For exam-
ple, in the sentence “The bomb caused widespread
damage and killed three people” the bomb is the
subject of both the verbs cause and kill. We made
use of this information by duplicating any nodes
(and their descendants) with more than one head.6
Overall the figures in Table 1 are consistent with
the analysis in Section 3 but there is great variation
in the number of patterns produced by the differ-
ent parsers. For example, the Stanford parser pro-
duces more chains and linked chains than the other
parsers. (If we did not duplicate portions of the
MINIPAR parses then the Stanford parser would
also generate the most subtrees.) We found that
the Stanford parser was the most likely to gen-
erate a single dependency tree for each sentence
while the other two produced a set of tree frag-
ments. A single dependency analysis contains a
greater number of patterns, and possible subtrees,
than a fragmented analysis. One reason for this
may be that the Stanford parser is unique in allow-
ing the use of an underspecified dependency rela-
tion, dep, which can be applied when the role of
the dependency is unclear. This allows the Stan-
</bodyText>
<footnote confidence="0.921754">
6One dependency tree produced by MINIPAR, expanded in
this way, contained approximately 1 x 1064 subtrees. These
are not included in the total number of subtrees for the MINI-
PAR parses shown in the table.
</footnote>
<bodyText confidence="0.985965">
ford parser to generate analyses which span more
of the sentence than the other two.
</bodyText>
<subsectionHeader confidence="0.999041">
4.3 Evaluating Pattern Models
</subsectionHeader>
<bodyText confidence="0.999741428571429">
Patterns from each of the four models are exam-
ined to check whether they cover the information
which should be extracted. In this context “cover”
means that the pattern contains both elements
of the relation. For example, an SVO pattern
extracted from the dependency parse of “Smith
was recently made chairman of Acme.” would be
</bodyText>
<equation confidence="0.510366">
[V/make](subj[N/Smith]+obj[N/chairman])
</equation>
<bodyText confidence="0.999979857142857">
which covers the relation between Smith and
chairman but not the relations between Smith
and Acme or chairman and Acme. The coverage
of each model is computed as the percentage of
relations in the corpus for which at least one of
the patterns contains both of the participating
entities. Coverage is related to the more familiar
IE evaluation metric of recall since the coverage
of a pattern model places an upper bound on the
recall of any system using that model. The aim
of this work is to determine the proportion of
the relations in a corpus that can be represented
using the various pattern models rather than their
performance in an IE system and, consequently,
we choose to evaluate models in terms of their
coverage rather than precision and recall.7
For practical applications parsers are required
to generate the dependency analysis but these may
not always provide a complete analysis for every
sentence. The coverage of each model is influ-
enced by the ability of the parser to produce a tree
which connects the elements of the event to be ex-
tracted. To account for this we compute the cov-
erage of each model relative to a particular parser.
The subtree model covers all events whose enti-
ties are included in the dependency tree and, con-
sequently, the coverage of this model represents
the maximum number of events that the model can
</bodyText>
<footnote confidence="0.735113571428571">
7The subtree model can be used to cover any set of items
in a dependency tree. So, given accurate dependency anal-
yses, this model will cover all events. The coverage of the
subtree model can be determined by checking if the elements
of the event are connected in the dependency analysis of the
sentence and, for simplicity, we chose to do this rather than
enumerating all subtrees.
</footnote>
<page confidence="0.997626">
16
</page>
<bodyText confidence="0.999784875">
represent for a given dependency tree. The cover-
age of other models relative to a dependency anal-
ysis can be computed by dividing the number of
events it covers by the number covered by the sub-
tree model (i.e. the maximum which can be cov-
ered). This measure is refered to as the bounded
coverage of the model. Bounded coverage for the
subtree model is always 100%.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.996107853658536">
Coverage and bounded-coverage results for each
pattern representation and parser combination are
given in Table 2. The table lists the corpus, the
total number of instances within that corpus and
the results for each of the four pattern models. Re-
sults for the subtree model lists the coverage and
raw count, the bounded-coverage for this model
will always be 100% and is not listed. Results
for the other three models show the coverage and
raw count along with the bounded coverage. The
coverage of each parser and pattern representa-
tion (combined across both corpora) are also sum-
marised in Figure 3.
The simplest representation, SVO, does not per-
form well in this evaluation. The highest bounded-
coverage score is 15.1% (MUC6 corpus, Stanford
parser) but the combined average over all corpora
is less than 6% for any parser. This suggests
that the SVO representation is simply not expres-
sive enough for IE. Previous work which has used
this representation have used indirect evaluation:
document and sentence filtering (Yangarber, 2003;
Stevenson and Greenwood, 2005). While the SVO
representation may be expressive enough to allow
a classifier to distinguish documents or sentences
which are relevant to a particular extraction task it
seems too limited to be used for relation extrac-
tion. The SVO representation performs notice-
ably worse on the biomedical text. Our analysis
suggests that this is because the items of interest
are commonly described in ways which the SVO
model is unable to represent.
The more complex chain model covers a greater
percentage of the relations. However its bounded-
coverage is still less than half of the relations in ei-
ther the MUC6 corpus or the biomedical texts. Us-
ing the chain model the best coverage which can
be achieved over any corpus is 41.07% (MUC6
corpus, MINIPAR and Stanford parser) which is
unlikely to be sufficient to create an IE system.
Results for the linked chain representation are
</bodyText>
<subsectionHeader confidence="0.384839">
MINIPAR Machinese Syntax Stanford
</subsectionHeader>
<figureCaption confidence="0.6740375">
Figure 3: Coverage of various pattern representa-
tion models for each of the three parsers.
</figureCaption>
<bodyText confidence="0.999756161290322">
much more promising covering around 70% of all
relations using the MINIPAR and Machinese Syn-
tax parsers and over 90.64% using the Stanford
parser. For all three parsers this model achieves
a bounded-coverage of close to 95%, indicating
that this model can represent the majority of re-
lations which are included in a dependency tree.
The subtree representation covers slight more of
the relations than linked chains: around 75% us-
ing the MINIPAR or Machinese Syntax parsers and
96.62% using the Stanford parser.
A one-way repeated measures ANOVA was car-
ried out to analyse the differences between the re-
sults for each model shown in Table 2. It was
found that the differences between the SVO, chain,
linked chain and subtree models are significant
(p &lt; 0.01). A Tukey test was then applied to iden-
tify which of the individual differences between
pairs of models were significant. Differences be-
tween two pairs of models were not found to be
significant (p &lt; 0.01): SVO and chains; linked
chains and subtrees.
These results suggest that the linked chains and
subtree models can represent significantly more of
the relations which occur in IE scenarios than ei-
ther the SVO or chain models. However, there is
little to be gained from using the subtree model
since accuracy of the linked chain model is com-
parable and the number of patterns generated is
bounded by a polynomial rather than exponential
function.
</bodyText>
<subsectionHeader confidence="0.998824">
5.1 Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.9999472">
Examination of the relations which were cov-
ered by the subtree model but not by linked
chains suggested that there are certain construc-
tions which cause difficulties. One such construc-
tion is the appositive, e.g. the relation between
</bodyText>
<figure confidence="0.996965769230769">
Coverage
100%
90%
80%
50%
40%
30%
20%
70%
60%
10%
0%
SVO Chains Linked Chains Subtrees
</figure>
<page confidence="0.98814">
17
</page>
<table confidence="0.999805916666667">
Parser Corpus # of SVO %B-C Chains %B-C Linked Chains %B-C Subtrees
Relations %C %C %C %C
MINIPAR MUC6 1322 7.49 (99) 9.07 41.07 (543) 49.73 81.92 (1083) 99.18 82.60 (1092)
Biomed 2589 0.93 (24) 1.30 17.38 (450) 24.44 65.31 (1691) 91.85 71.11 (1841)
Combined 3911 3.14 (123) 4.19 25.39 (993) 33.86 70.93 (2774) 94.58 74.99 (2933)
Machinese MUC6 1322 2.12 (28) 2.75 35.70 (472) 46.41 76.32 (1009) 99.21 76.93 (1017)
Syntax
Biomed 2589 0.19 (5) 0.27 14.56 (377) 20.47 65.47 (1695) 92.02 71.15 (1842)
Combined 3911 0.84 (33) 1.15 21.71 (849) 29.70 69.14 (2704) 94.58 73.10 (2859)
Stanford MUC6 1322 15.05 (199) 15.10 41.07 (543) 41.20 94.78 (1253) 95.07 99.70 (1318)
Biomed 2589 0.46 (12) 0.49 16.53 (428) 17.39 88.52 (2292) 93.13 95.06 (2461)
Combined 3911 5.40 (211) 5.58 24.83 (971) 25.69 90.64 (3545) 93.81 96.62 (3779)
</table>
<tableCaption confidence="0.999609">
Table 2: Evaluation results for the three different parsers.
</tableCaption>
<bodyText confidence="0.999470877192983">
PersonOut and Organisation in the frag-
ment “Organisation’s Post, PersonOut,
resigned yesterday morning”. Certain nominal-
isations may also cause problems for the linked
chains representation, e.g. in biomedical text
the relation between Agent and Target in the
nominalisation “the Agent-dependent assembly
of Target” cannot be represented by a linked
chain. In both cases the problem is caused by the
fact that the dependency tree generated includes
the two named entities in part of the tree domi-
nated by a node marked as a noun. Since each
linked chain must be anchored at a verb (or the
root of a tree fragment) and the two chains can-
not share part of their path, these relations are not
covered. It would be possible to create another
representation which allowed these relations to be
captured but it would generate more patterns than
the linked chain model.
Our results also reveal that the choice of depen-
dency parser effects the coverage of each model
(see Figure 3). The subtree model coverage scores
for each parser shown in Table 3 represent the per-
centage of sentences for which an analysis was
generated that included both items from the bi-
nary relations. These figures are noticably higher
for the Stanford parser. We previously mentioned
(Section 4.2) that this parser allows the use of an
underspecified dependency relation and suggested
that this may be a reason for the higher cover-
age. The use of underspecified dependency re-
lations may not be useful for all applications but
is unlikely to cause problems for systems which
learn IE patterns provided the trees generated by
the parser are consistent. Differences between the
results produced by the three parsers suggest that
it is important to fully evaluate their suitability for
a particular purpose.
These experiments also provide insights into the
more general question of how suitable dependency
trees are as a basis for extraction patterns. De-
pendency analysis has the advantage of generat-
ing analyses which abstract away from the sur-
face realisation of text to a greater extent than
phrase structure grammars tend to. This leads to
the semantic information being more accessible in
the representation of the text which can be use-
ful for IE. For practical applications this approach
relies on the ability to accurately generate depen-
dency analyses. The results presented here sug-
gest that the Stanford parser (Klein and Manning,
2003) is capable of generating analyses for almost
all sentences within corpora from two very differ-
ent domains. Bunescu and Mooney (2005) have
also demonstrated that dependency graphs can be
produced using Combinatory Categorial Grammar
(CCG) and context-free grammar (CFG) parsers.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999874428571429">
This paper compares four IE pattern models:
SVO, chains, linked chains and subtrees. Us-
ing texts from the management succession and
biomedical domains it was found that the linked
chains model can represent around 95% of the
possible relations contained in the text, given a de-
pendency parse. Subtrees can represent all the re-
lations contained within dependency trees but their
use is less practical because enumerating all pos-
sible subtrees is a more complex problem and the
large number of resulting patterns could limit the
learning algorithms that can be applied. This re-
sult should be borne in mind during the design of
IE systems.
</bodyText>
<sectionHeader confidence="0.997736" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996965">
The authors are grateful to Mike Stannet for pro-
viding the method for counting subtrees intro-
duced in Section 3 and to Connexor Oy for use
of the Machinese Syntax parser. The research
</bodyText>
<page confidence="0.997012">
18
</page>
<bodyText confidence="0.9999212">
described in this paper was funded by the En-
gineering and Physical Sciences Research Coun-
cil via the RESuLT project (GR/T06391) and par-
tially funded by the IST 6th Framework project X-
Media (FP6-26978).
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871602272727">
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
724–731, Vancouver, B.C.
Mark Craven and Johan Kumlien. 1999. Construct-
ing Biological Knowledge Bases by Extracting In-
formation from Text Sources. In Proceedings of the
Seventh International Conference on Intelligent Sys-
tems for Molecular Biology, pages 77–86, Heidel-
berg, Germany. AAAI Press.
Robert Gaizauskas, Takahiro Wakao, Kevin
Humphreys, Hamish Cunningham, and Yorick
Wilks. 1996. Description of the LaSIE system
as used for MUC-6. In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
pages 207–220, San Francisco, CA.
Mark A. Greenwood, Mark Stevenson, Yikun Guo,
Henk Harkema, and Angus Roberts. 2005. Au-
tomatically Acquiring a Linguistically Motivated
Genic Interaction Extraction System. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany.
Ada Hamosh, Alan F. Scott, Joanna Amberger, Carol
Bocchini, David Valle, and Victor A. McKusick.
2002. Online Mendelian Inheritance in Man
(OMIM), a knowledgebase of human genes and ge-
netic disorders. Nucleic Acids Research, 30(1):52–
55.
Peter E. Hodges, Andrew H. Z. McKee, Brian P. Davis,
William E. Payne, and James I. Garrels. 1999. The
Yeast Proteome Database (YPD): a model for the or-
ganization and presentation of genome-wide func-
tional data. Nucleic Acids Research, 27(1):69–73.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 423–430, Sap-
poro, Japan.
Dekang Lin. 1999. MINIPAR: A Minimalist Parser.
In Maryland Linguistics Colloquium, University of
Maryland, College Park.
Igor Mel’ˇcuk. 1987. Dependency Syntax: Theory and
Practice. SUNY Press, New York.
MUC. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6), San Mateo, CA.
Morgan Kaufmann.
Claire N´edellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany, August.
Ellen Riloff. 1993. Automatically constructing a dic-
tionary for information extraction tasks. pages 811–
816.
Stephen Soderland. 1999. Learning Information Ex-
traction Rules for Semi-structured and free text. Ma-
chine Learning, 31(1-3):233–272.
Mark Stevenson and Mark A. Greenwood. 2005. A
Semantic Approach to IE Pattern Induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 379–386,
Ann Arbor, MI.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-03),
pages 224–231, Sapporo, Japan.
Pasi Tapanainen and Timo J¨arvinen. 1997. A Non-
Projective Dependency Parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, pages 64–74, Washington, DC.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic Acquisition of
Domain Knowledge for Information Extraction. In
Proceedings ofthe 18th International Conference on
Computational Linguistics (COLING 2000), pages
940–946, Saarbr¨ucken, Germany.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 343–350, Sap-
poro, Japan.
</reference>
<page confidence="0.999331">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563422">
<title confidence="0.99976">Comparing Information Extraction Pattern Models</title>
<author confidence="0.999925">Mark Stevenson</author>
<author confidence="0.999925">A Mark</author>
<affiliation confidence="0.995876">Department of Computer University of</affiliation>
<address confidence="0.592868">Sheffield, S1 4DP,</address>
<abstract confidence="0.997609523809524">Several recently reported techniques for the automatic acquisition of Information Extraction (IE) systems have used dependency trees as the basis of their extraction pattern representation. These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency analysis). An appropriate model should be expressive enough to represent the information which is to be extracted from text without being overly complicated. Four previously reported pattern models are evaluated using existing IE evaluation corpora and three dependency parsers. It was found that one model, linked chains, could represent around 95% of the information of interest without generating an unwieldy number of possible patterns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<location>Vancouver, B.C.</location>
<contexts>
<context position="2031" citStr="Bunescu and Mooney, 2005" startWordPosition="307" endWordPosition="310">m (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency tree). For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al. (2003) allow any subpart of the tree to act as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into the appropriateness of each model. Sudo et al. (2003) compared t</context>
<context position="28209" citStr="Bunescu and Mooney (2005)" startWordPosition="4624" endWordPosition="4627">ependency analysis has the advantage of generating analyses which abstract away from the surface realisation of text to a greater extent than phrase structure grammars tend to. This leads to the semantic information being more accessible in the representation of the text which can be useful for IE. For practical applications this approach relies on the ability to accurately generate dependency analyses. The results presented here suggest that the Stanford parser (Klein and Manning, 2003) is capable of generating analyses for almost all sentences within corpora from two very different domains. Bunescu and Mooney (2005) have also demonstrated that dependency graphs can be produced using Combinatory Categorial Grammar (CCG) and context-free grammar (CFG) parsers. 6 Conclusions This paper compares four IE pattern models: SVO, chains, linked chains and subtrees. Using texts from the management succession and biomedical domains it was found that the linked chains model can represent around 95% of the possible relations contained in the text, given a dependency parse. Subtrees can represent all the relations contained within dependency trees but their use is less practical because enumerating all possible subtree</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724–731, Vancouver, B.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing Biological Knowledge Bases by Extracting Information from Text Sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,</booktitle>
<pages>77--86</pages>
<publisher>AAAI Press.</publisher>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="13443" citStr="Craven and Kumlien, 1999" startWordPosition="2194" endWordPosition="2197">son leaving a job), Post (the job title) and Organisation (the employer). These events were broken down into a set of binary relationships. For example, the sentence “Smith was recently made chairman of Acme.” contains information about the new employee (Smith), post (chairman) and organisation (Acme). Events are represented as a set of binary relationships, Smith-chairman, chairman-Acme and Smith-Acme for this example. The second corpus uses documents taken from the biomedical domain, specifically the training corpus used in the LLL-05 challenge task (N´edellec, 2005), and a pair of corpora (Craven and Kumlien, 1999) which were derived from the Yeast Proteome Database (YPD) (Hodges et al., 1999) and the Online Mendelian Inheritance in Man database (OMIM) (Hamosh et al., 2002). Each of these corpora are annotated with binary relations between pairs of entities. The LLL-05 corpora contains interactions between genes and proteins. For example the sentence “Expression of the sigma(K)-dependent cwlH gene depended on gerE” contains relations between sigma(K) and cwlH and between gerE and cwlH. The YPD corpus is concerned with the subcellular compartments in which particular yeast proteins localize. An example s</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing Biological Knowledge Bases by Extracting Information from Text Sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology, pages 77–86, Heidelberg, Germany. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gaizauskas</author>
<author>Takahiro Wakao</author>
<author>Kevin Humphreys</author>
<author>Hamish Cunningham</author>
<author>Yorick Wilks</author>
</authors>
<title>Description of the LaSIE system as used for MUC-6.</title>
<date>1996</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>207--220</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="1296" citStr="Gaizauskas et al., 1996" startWordPosition="188" endWordPosition="191">verly complicated. Four previously reported pattern models are evaluated using existing IE evaluation corpora and three dependency parsers. It was found that one model, linked chains, could represent around 95% of the information of interest without generating an unwieldy number of possible patterns. 1 Introduction A common approach to Information Extraction (IE) is to use patterns which match against text and identify items of interest. Patterns are applied to text which has undergone various levels of linguistic analysis, such as phrase chunking (Soderland, 1999) and full syntactic parsing (Gaizauskas et al., 1996). The approaches use different definitions of what constitutes a valid pattern. For example, the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used pat</context>
</contexts>
<marker>Gaizauskas, Wakao, Humphreys, Cunningham, Wilks, 1996</marker>
<rawString>Robert Gaizauskas, Takahiro Wakao, Kevin Humphreys, Hamish Cunningham, and Yorick Wilks. 1996. Description of the LaSIE system as used for MUC-6. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 207–220, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
<author>Mark Stevenson</author>
<author>Yikun Guo</author>
<author>Henk Harkema</author>
<author>Angus Roberts</author>
</authors>
<title>Automatically Acquiring a Linguistically Motivated Genic Interaction Extraction System.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th Learning Language in Logic Workshop (LLL05),</booktitle>
<location>Bonn, Germany.</location>
<contexts>
<context position="7095" citStr="Greenwood et al., 2005" startWordPosition="1147" endWordPosition="1150">encoding information beyond the direct arguments of predicates and includes areas of the dependency tree ignored by the SVO model. For example, they can represent information expressed as a nominalisation or within a prepositional phrase, e.g. “The resignation of Smith from the board of Acme ...” However, a potential shortcoming of this model is that it cannot represent the link between arguments of a verb. Patterns in the chain model format are unable to represent even the simplest of sentences containing a transitive verb, e.g. “Smith left Acme Inc.”. Linked Chains: The linked chains model (Greenwood et al., 2005) represents extraction patterns as a pair of chains which share the same verb but no direct descendants. This model generates 14 patterns for the verb hire in Figure 1, examples of which are shown in Figure 2. This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between “Smith” and “Bloggs”. Subtrees: The final model to be considered is the subtree model (Sudo et al., 2003). In this model any subtree of a dependency tree can be used a</context>
</contexts>
<marker>Greenwood, Stevenson, Guo, Harkema, Roberts, 2005</marker>
<rawString>Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk Harkema, and Angus Roberts. 2005. Automatically Acquiring a Linguistically Motivated Genic Interaction Extraction System. In Proceedings of the 4th Learning Language in Logic Workshop (LLL05), Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ada Hamosh</author>
<author>Alan F Scott</author>
<author>Joanna Amberger</author>
<author>Carol Bocchini</author>
<author>David Valle</author>
<author>Victor A McKusick</author>
</authors>
<title>Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders.</title>
<date>2002</date>
<journal>Nucleic Acids Research,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>55</pages>
<contexts>
<context position="13605" citStr="Hamosh et al., 2002" startWordPosition="2220" endWordPosition="2223">Smith was recently made chairman of Acme.” contains information about the new employee (Smith), post (chairman) and organisation (Acme). Events are represented as a set of binary relationships, Smith-chairman, chairman-Acme and Smith-Acme for this example. The second corpus uses documents taken from the biomedical domain, specifically the training corpus used in the LLL-05 challenge task (N´edellec, 2005), and a pair of corpora (Craven and Kumlien, 1999) which were derived from the Yeast Proteome Database (YPD) (Hodges et al., 1999) and the Online Mendelian Inheritance in Man database (OMIM) (Hamosh et al., 2002). Each of these corpora are annotated with binary relations between pairs of entities. The LLL-05 corpora contains interactions between genes and proteins. For example the sentence “Expression of the sigma(K)-dependent cwlH gene depended on gerE” contains relations between sigma(K) and cwlH and between gerE and cwlH. The YPD corpus is concerned with the subcellular compartments in which particular yeast proteins localize. An example sentence “Uba2p is located largely in the nucleus” relates Uba2p and the nucleus. The relations in the OMIM corpora are between genes and diseases, for example “Mo</context>
</contexts>
<marker>Hamosh, Scott, Amberger, Bocchini, Valle, McKusick, 2002</marker>
<rawString>Ada Hamosh, Alan F. Scott, Joanna Amberger, Carol Bocchini, David Valle, and Victor A. McKusick. 2002. Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders. Nucleic Acids Research, 30(1):52– 55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Hodges</author>
<author>Andrew H Z McKee</author>
<author>Brian P Davis</author>
<author>William E Payne</author>
<author>James I Garrels</author>
</authors>
<title>The Yeast Proteome Database (YPD): a model for the organization and presentation of genome-wide functional data.</title>
<date>1999</date>
<journal>Nucleic Acids Research,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="13523" citStr="Hodges et al., 1999" startWordPosition="2207" endWordPosition="2210">s were broken down into a set of binary relationships. For example, the sentence “Smith was recently made chairman of Acme.” contains information about the new employee (Smith), post (chairman) and organisation (Acme). Events are represented as a set of binary relationships, Smith-chairman, chairman-Acme and Smith-Acme for this example. The second corpus uses documents taken from the biomedical domain, specifically the training corpus used in the LLL-05 challenge task (N´edellec, 2005), and a pair of corpora (Craven and Kumlien, 1999) which were derived from the Yeast Proteome Database (YPD) (Hodges et al., 1999) and the Online Mendelian Inheritance in Man database (OMIM) (Hamosh et al., 2002). Each of these corpora are annotated with binary relations between pairs of entities. The LLL-05 corpora contains interactions between genes and proteins. For example the sentence “Expression of the sigma(K)-dependent cwlH gene depended on gerE” contains relations between sigma(K) and cwlH and between gerE and cwlH. The YPD corpus is concerned with the subcellular compartments in which particular yeast proteins localize. An example sentence “Uba2p is located largely in the nucleus” relates Uba2p and the nucleus.</context>
</contexts>
<marker>Hodges, McKee, Davis, Payne, Garrels, 1999</marker>
<rawString>Peter E. Hodges, Andrew H. Z. McKee, Brian P. Davis, William E. Payne, and James I. Garrels. 1999. The Yeast Proteome Database (YPD): a model for the organization and presentation of genome-wide functional data. Nucleic Acids Research, 27(1):69–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="14822" citStr="Klein and Manning, 2003" startWordPosition="2410" endWordPosition="2413">ple “Most sporadic colorectal cancers also have two APC mutations” contains a relation between APC and colorectal cancer. The MUC6 corpus contains a total of six possible binary relations. Each of the three biomedical corpora contain a single relation type, giving a total of nine binary relations for the experiments. There are 3911 instances of binary relations in all corpora. 4.2 Generating Dependency Patterns Three dependency parsers were used for these experiments: MINIPAR3 (Lin, 1999), the Machinese Syntax4 parser from Connexor Oy (Tapanainen and J¨arvinen, 1997) and the Stanford5 parser (Klein and Manning, 2003). These three parsers represent a cross-section of approaches to producing dependency analyses: MINIPAR uses a constituency grammar internally before converting the result to a dependency tree, Machinese Syntax uses a functional dependency grammar, and the Stanford Parser is a lexicalized probabilistic parser. Before these parsers were applied to the various corpora the named entities participating in relations are replaced by a token indicating their class. For example, in the MUC6 corpus “Acme hired Smith” would become “Organisation hired PersonIn”. Each parser was adapted to deal with these</context>
<context position="28076" citStr="Klein and Manning, 2003" startWordPosition="4603" endWordPosition="4606">ents also provide insights into the more general question of how suitable dependency trees are as a basis for extraction patterns. Dependency analysis has the advantage of generating analyses which abstract away from the surface realisation of text to a greater extent than phrase structure grammars tend to. This leads to the semantic information being more accessible in the representation of the text which can be useful for IE. For practical applications this approach relies on the ability to accurately generate dependency analyses. The results presented here suggest that the Stanford parser (Klein and Manning, 2003) is capable of generating analyses for almost all sentences within corpora from two very different domains. Bunescu and Mooney (2005) have also demonstrated that dependency graphs can be produced using Combinatory Categorial Grammar (CCG) and context-free grammar (CFG) parsers. 6 Conclusions This paper compares four IE pattern models: SVO, chains, linked chains and subtrees. Using texts from the management succession and biomedical domains it was found that the linked chains model can represent around 95% of the possible relations contained in the text, given a dependency parse. Subtrees can r</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>MINIPAR: A Minimalist Parser. In</title>
<date>1999</date>
<institution>Maryland Linguistics Colloquium, University of Maryland, College Park.</institution>
<contexts>
<context position="14691" citStr="Lin, 1999" startWordPosition="2393" endWordPosition="2394">he nucleus” relates Uba2p and the nucleus. The relations in the OMIM corpora are between genes and diseases, for example “Most sporadic colorectal cancers also have two APC mutations” contains a relation between APC and colorectal cancer. The MUC6 corpus contains a total of six possible binary relations. Each of the three biomedical corpora contain a single relation type, giving a total of nine binary relations for the experiments. There are 3911 instances of binary relations in all corpora. 4.2 Generating Dependency Patterns Three dependency parsers were used for these experiments: MINIPAR3 (Lin, 1999), the Machinese Syntax4 parser from Connexor Oy (Tapanainen and J¨arvinen, 1997) and the Stanford5 parser (Klein and Manning, 2003). These three parsers represent a cross-section of approaches to producing dependency analyses: MINIPAR uses a constituency grammar internally before converting the result to a dependency tree, Machinese Syntax uses a functional dependency grammar, and the Stanford Parser is a lexicalized probabilistic parser. Before these parsers were applied to the various corpora the named entities participating in relations are replaced by a token indicating their class. For ex</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. MINIPAR: A Minimalist Parser. In Maryland Linguistics Colloquium, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1987</date>
<publisher>SUNY Press,</publisher>
<location>New York.</location>
<marker>Mel’ˇcuk, 1987</marker>
<rawString>Igor Mel’ˇcuk. 1987. Dependency Syntax: Theory and Practice. SUNY Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC</author>
</authors>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="5692" citStr="MUC, 1995" startWordPosition="917" endWordPosition="918">ern also included certain phrases which referred to either the subject or object. 2The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003). Each node in the tree is represented in the format a[b/c] (e.g. subj[N/bomber]) where c is the lexical item (bomber), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj). The relationship between nodes is represented as X(A+B+C) which indicates that nodes A, B and C are direct descendents of node X. of participants in specific events. For example, the MUC-6 (MUC, 1995) management succession scenario concerns the identification of individuals who are changing job. These events are often described using a simple predicate argument structure, e.g. “Acme Inc. fired Smith”. However, the SVO model cannot represent information described using other linguistic constructions such as nominalisations or prepositional phrases. For example, in the MUC6 texts it is common for job titles to be mentioned within prepositional phrases, e.g. “Smith joined Acme Inc. as CEO”. Chains: A pattern is defined as a path between a verb node and any other node in the dependency tree pa</context>
<context position="12452" citStr="MUC, 1995" startWordPosition="2038" endWordPosition="2039"> the information of interest to IE systems. We chose a set of IE corpora annotated with the information to be extracted (detailed in Section 4.1), generated sets of patterns using a variety of dependency parsers (Section 4.2) which were then examined to discover how much of the target information they contain (Section 4.3). 4.1 Corpora Corpora representing different genres of text were chosen for these experiments; one containing newspaper text and another composed of biomedical abstracts. The first corpus consisted of Wall Street Journal texts from the Sixth Message Understanding Conference (MUC, 1995) IE evaluation. These are reliably annotated with details about the movement of executives between jobs. We make use of a version of the corpus produced by Soderland (1999) in which events described within a single sentence were annotated. Events in this corpus identify relations between up to four entities: PersonIn (the person starting a new job), PersonOut (person leaving a job), Post (the job title) and Organisation (the employer). These events were broken down into a set of binary relationships. For example, the sentence “Smith was recently made chairman of Acme.” contains information abo</context>
</contexts>
<marker>MUC, 1995</marker>
<rawString>MUC. 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6), San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire N´edellec</author>
</authors>
<title>Learning Language in Logic -Genic Interaction Extraction Challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th Learning Language in Logic Workshop (LLL05),</booktitle>
<location>Bonn, Germany,</location>
<marker>N´edellec, 2005</marker>
<rawString>Claire N´edellec. 2005. Learning Language in Logic -Genic Interaction Extraction Challenge. In Proceedings of the 4th Learning Language in Logic Workshop (LLL05), Bonn, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<pages>811--816</pages>
<contexts>
<context position="1423" citStr="Riloff, 1993" startWordPosition="210" endWordPosition="211">. It was found that one model, linked chains, could represent around 95% of the information of interest without generating an unwieldy number of possible patterns. 1 Introduction A common approach to Information Extraction (IE) is to use patterns which match against text and identify items of interest. Patterns are applied to text which has undergone various levels of linguistic analysis, such as phrase chunking (Soderland, 1999) and full syntactic parsing (Gaizauskas et al., 1996). The approaches use different definitions of what constitutes a valid pattern. For example, the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Moon</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. pages 811– 816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
</authors>
<title>Learning Information Extraction Rules for Semi-structured and free text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>31--1</pages>
<contexts>
<context position="1243" citStr="Soderland, 1999" startWordPosition="181" endWordPosition="183"> is to be extracted from text without being overly complicated. Four previously reported pattern models are evaluated using existing IE evaluation corpora and three dependency parsers. It was found that one model, linked chains, could represent around 95% of the information of interest without generating an unwieldy number of possible patterns. 1 Introduction A common approach to Information Extraction (IE) is to use patterns which match against text and identify items of interest. Patterns are applied to text which has undergone various levels of linguistic analysis, such as phrase chunking (Soderland, 1999) and full syntactic parsing (Gaizauskas et al., 1996). The approaches use different definitions of what constitutes a valid pattern. For example, the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to</context>
<context position="12624" citStr="Soderland (1999)" startWordPosition="2068" endWordPosition="2069">atterns using a variety of dependency parsers (Section 4.2) which were then examined to discover how much of the target information they contain (Section 4.3). 4.1 Corpora Corpora representing different genres of text were chosen for these experiments; one containing newspaper text and another composed of biomedical abstracts. The first corpus consisted of Wall Street Journal texts from the Sixth Message Understanding Conference (MUC, 1995) IE evaluation. These are reliably annotated with details about the movement of executives between jobs. We make use of a version of the corpus produced by Soderland (1999) in which events described within a single sentence were annotated. Events in this corpus identify relations between up to four entities: PersonIn (the person starting a new job), PersonOut (person leaving a job), Post (the job title) and Organisation (the employer). These events were broken down into a set of binary relationships. For example, the sentence “Smith was recently made chairman of Acme.” contains information about the new employee (Smith), post (chairman) and organisation (Acme). Events are represented as a set of binary relationships, Smith-chairman, chairman-Acme and Smith-Acme </context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>Stephen Soderland. 1999. Learning Information Extraction Rules for Semi-structured and free text. Machine Learning, 31(1-3):233–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Mark A Greenwood</author>
</authors>
<title>A Semantic Approach to IE Pattern Induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>379--386</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2063" citStr="Stevenson and Greenwood, 2005" startWordPosition="311" endWordPosition="314">erns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency tree). For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al. (2003) allow any subpart of the tree to act as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into the appropriateness of each model. Sudo et al. (2003) compared three models in terms of their ab</context>
<context position="4571" citStr="Stevenson and Greenwood (2005)" startWordPosition="718" endWordPosition="722">tween a word (the head) and one of its modifiers. These links may be labelled to indicate the grammatical relation between the head and modifier (e.g. subject, object). In general cyclical paths are disallowed so that the analysis forms a tree structure. An example dependency analysis for the sentence “Acme Inc. hired Mr Smith as their new CEO, replacing Mr Bloggs.” is shown Figure 1. The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verbobject tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object1. An SVO pattern is extracted for each verb in a sentence. Figure 2 shows the two SVO patterns2 which are produced for the dependency tree shown in Figure 1. This model may be motivated by the assumption that many IE scenarios involve the extraction 1Yangarber et al. (2000) and Sudo et al. (2003) used a slightly extended version of this model in which the pattern also included certain phrases which referred to either the subject or object. 2The fo</context>
<context position="22031" citStr="Stevenson and Greenwood, 2005" startWordPosition="3599" endWordPosition="3602">d raw count along with the bounded coverage. The coverage of each parser and pattern representation (combined across both corpora) are also summarised in Figure 3. The simplest representation, SVO, does not perform well in this evaluation. The highest boundedcoverage score is 15.1% (MUC6 corpus, Stanford parser) but the combined average over all corpora is less than 6% for any parser. This suggests that the SVO representation is simply not expressive enough for IE. Previous work which has used this representation have used indirect evaluation: document and sentence filtering (Yangarber, 2003; Stevenson and Greenwood, 2005). While the SVO representation may be expressive enough to allow a classifier to distinguish documents or sentences which are relevant to a particular extraction task it seems too limited to be used for relation extraction. The SVO representation performs noticeably worse on the biomedical text. Our analysis suggests that this is because the items of interest are commonly described in ways which the SVO model is unable to represent. The more complex chain model covers a greater percentage of the relations. However its boundedcoverage is still less than half of the relations in either the MUC6 </context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>Mark Stevenson and Mark A. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 379–386, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Automatic Pattern Acquisition for Japanese Information Extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT2001).</booktitle>
<contexts>
<context position="1986" citStr="Sudo et al., 2001" startWordPosition="299" endWordPosition="302">ttern. For example, the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency tree). For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al. (2003) allow any subpart of the tree to act as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into the appropriateness</context>
<context position="6357" citStr="Sudo et al., 2001" startWordPosition="1022" endWordPosition="1025">entification of individuals who are changing job. These events are often described using a simple predicate argument structure, e.g. “Acme Inc. fired Smith”. However, the SVO model cannot represent information described using other linguistic constructions such as nominalisations or prepositional phrases. For example, in the MUC6 texts it is common for job titles to be mentioned within prepositional phrases, e.g. “Smith joined Acme Inc. as CEO”. Chains: A pattern is defined as a path between a verb node and any other node in the dependency tree passing through zero or more intermediate nodes (Sudo et al., 2001). Figure 2 shows the eight chains which can be extracted from the tree in Figure 1. Chains provide a mechanism for encoding information beyond the direct arguments of predicates and includes areas of the dependency tree ignored by the SVO model. For example, they can represent information expressed as a nominalisation or within a prepositional phrase, e.g. “The resignation of Smith from the board of Acme ...” However, a potential shortcoming of this model is that it cannot represent the link between arguments of a verb. Patterns in the chain model format are unable to represent even the simple</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2001</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2001. Automatic Pattern Acquisition for Japanese Information Extraction. In Proceedings of the Human Language Technology Conference (HLT2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>224--231</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2005" citStr="Sudo et al., 2003" startWordPosition="303" endWordPosition="306"> the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency tree). For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al. (2003) allow any subpart of the tree to act as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into the appropriateness of each model. Sud</context>
<context position="5017" citStr="Sudo et al. (2003)" startWordPosition="800" endWordPosition="803">traction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verbobject tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object1. An SVO pattern is extracted for each verb in a sentence. Figure 2 shows the two SVO patterns2 which are produced for the dependency tree shown in Figure 1. This model may be motivated by the assumption that many IE scenarios involve the extraction 1Yangarber et al. (2000) and Sudo et al. (2003) used a slightly extended version of this model in which the pattern also included certain phrases which referred to either the subject or object. 2The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003). Each node in the tree is represented in the format a[b/c] (e.g. subj[N/bomber]) where c is the lexical item (bomber), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj). The relationship between nodes is represented as X(A+B+C) which indicates that nodes A, B and C are direct descendents of nod</context>
<context position="7633" citStr="Sudo et al., 2003" startWordPosition="1241" endWordPosition="1244">left Acme Inc.”. Linked Chains: The linked chains model (Greenwood et al., 2005) represents extraction patterns as a pair of chains which share the same verb but no direct descendants. This model generates 14 patterns for the verb hire in Figure 1, examples of which are shown in Figure 2. This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between “Smith” and “Bloggs”. Subtrees: The final model to be considered is the subtree model (Sudo et al., 2003). In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another. Single nodes are not considered to be subtrees. The subtree model is a richer representation than those discussed so far and can represent any part of a dependency tree. Each of the previ13 SVO Chains [V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith]) [V/hire](subj[N/Acme Inc.]) [V/replace](obj[N/Mr Bloggs]) [V/hire](obj[N/Mr Smith]) [V/hire](obj[N/Mr Smith](as[N/CEO])) [V/hire](obj[N/Mr Smith](as[N/CEO](gen[N/their]))) [V/hire](obj</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 224–231, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>A NonProjective Dependency Parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--74</pages>
<location>Washington, DC.</location>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo J¨arvinen. 1997. A NonProjective Dependency Parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–74, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Automatic Acquisition of Domain Knowledge for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings ofthe 18th International Conference on Computational Linguistics (COLING</booktitle>
<pages>940--946</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="1558" citStr="Yangarber et al. (2000)" startWordPosition="229" endWordPosition="232"> unwieldy number of possible patterns. 1 Introduction A common approach to Information Extraction (IE) is to use patterns which match against text and identify items of interest. Patterns are applied to text which has undergone various levels of linguistic analysis, such as phrase chunking (Soderland, 1999) and full syntactic parsing (Gaizauskas et al., 1996). The approaches use different definitions of what constitutes a valid pattern. For example, the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns </context>
<context position="2959" citStr="Yangarber et al., 2000" startWordPosition="461" endWordPosition="464"> as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into the appropriateness of each model. Sudo et al. (2003) compared three models in terms of their ability to identify event participants. The choice of pattern model has an effect on the number of potential patterns. This has implications on the practical application for each approach, particularly when used for automatic acquisition of IE systems using learning methods (Yangarber et al., 2000; Sudo et al., 2003; Bunescu and Mooney, 2005). This paper evaluates the appropriateness of four pattern models in terms of the competing aims of expressive completeness (ability to represent information in text) and complexity (number of possible patterns). Each model is examined by comparing it against a corpus annotated with events and determining the proportion of those which it is capable of representing. The remainder of this paper is organised as follows: a variety of dependency-tree-based IE pat12 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 12–19, S</context>
<context position="4994" citStr="Yangarber et al. (2000)" startWordPosition="795" endWordPosition="798">r models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verbobject tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object1. An SVO pattern is extracted for each verb in a sentence. Figure 2 shows the two SVO patterns2 which are produced for the dependency tree shown in Figure 1. This model may be motivated by the assumption that many IE scenarios involve the extraction 1Yangarber et al. (2000) and Sudo et al. (2003) used a slightly extended version of this model in which the pattern also included certain phrases which referred to either the subject or object. 2The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003). Each node in the tree is represented in the format a[b/c] (e.g. subj[N/bomber]) where c is the lexical item (bomber), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj). The relationship between nodes is represented as X(A+B+C) which indicates that nodes A, B and C are di</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proceedings ofthe 18th International Conference on Computational Linguistics (COLING 2000), pages 940–946, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Counter-training in the Discovery of Semantic Patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>343--350</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1967" citStr="Yangarber, 2003" startWordPosition="297" endWordPosition="298">itutes a valid pattern. For example, the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency tree). For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al. (2003) allow any subpart of the tree to act as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into </context>
<context position="4536" citStr="Yangarber (2003)" startWordPosition="715" endWordPosition="716">ected binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the grammatical relation between the head and modifier (e.g. subject, object). In general cyclical paths are disallowed so that the analysis forms a tree structure. An example dependency analysis for the sentence “Acme Inc. hired Mr Smith as their new CEO, replacing Mr Bloggs.” is shown Figure 1. The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verbobject tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object1. An SVO pattern is extracted for each verb in a sentence. Figure 2 shows the two SVO patterns2 which are produced for the dependency tree shown in Figure 1. This model may be motivated by the assumption that many IE scenarios involve the extraction 1Yangarber et al. (2000) and Sudo et al. (2003) used a slightly extended version of this model in which the pattern also included certain phrases which referred to ei</context>
<context position="21999" citStr="Yangarber, 2003" startWordPosition="3597" endWordPosition="3598">w the coverage and raw count along with the bounded coverage. The coverage of each parser and pattern representation (combined across both corpora) are also summarised in Figure 3. The simplest representation, SVO, does not perform well in this evaluation. The highest boundedcoverage score is 15.1% (MUC6 corpus, Stanford parser) but the combined average over all corpora is less than 6% for any parser. This suggests that the SVO representation is simply not expressive enough for IE. Previous work which has used this representation have used indirect evaluation: document and sentence filtering (Yangarber, 2003; Stevenson and Greenwood, 2005). While the SVO representation may be expressive enough to allow a classifier to distinguish documents or sentences which are relevant to a particular extraction task it seems too limited to be used for relation extraction. The SVO representation performs noticeably worse on the biomedical text. Our analysis suggests that this is because the items of interest are commonly described in ways which the SVO model is unable to represent. The more complex chain model covers a greater percentage of the relations. However its boundedcoverage is still less than half of t</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>Roman Yangarber. 2003. Counter-training in the Discovery of Semantic Patterns. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 343–350, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>