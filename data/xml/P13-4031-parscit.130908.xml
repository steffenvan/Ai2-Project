<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001035">
<title confidence="0.985494">
tSEARCH: Flexible and Fast Search over Automatic Translations for
Improved Quality/Error Analysis
</title>
<author confidence="0.790407">
Meritxell Gonz`alez and Laura Mascarell and Lluis M`arquez
</author>
<affiliation confidence="0.755804">
TALP Research Center
</affiliation>
<address confidence="0.606629">
Universitat Polit`ecnica de Catalunya
</address>
<email confidence="0.999205">
{mgonzalez,lmascarell,lluism}@lsi.upc.edu
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966619047619">
This work presents tSEARCH, a web-based
application that provides mechanisms for
doing complex searches over a collection
of translation cases evaluated with a large
set of diverse measures. tSEARCH uses the
evaluation results obtained with the ASIYA
toolkit for MT evaluation and it is connected
to its on-line GUI, which makes possible
a graphical visualization and interactive ac-
cess to the evaluation results. The search
engine offers a flexible query language al-
lowing to find translation examples match-
ing a combination of numerical and struc-
tural features associated to the calculation of
the quality metrics. Its database design per-
mits a fast response time for all queries sup-
ported on realistic-size test beds. In sum-
mary, tSEARCH, used with ASIYA, offers
developers of MT systems and evaluation
metrics a powerful tool for helping transla-
tion and error analysis.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998647">
In Machine Translation (MT) system develop-
ment, a qualitative analysis of the translations is a
fundamental step in order to spot the limitations of
a system, compare the linguistic abilities of differ-
ent systems or tune the parameters during system
refinement. This is especially true in statistical
MT systems, where usually no special structured
knowledge is used other than parallel data and lan-
guage models, but also on systems that need to
reason over linguistic structures. The need for an-
alyzing and comparing automatic translations with
respect to evaluation metrics is also paramount
for developers of translation quality metrics, who
need elements of analysis to better understand the
behavior of their evaluation measures.
This paper presents tSEARCH, a web applica-
tion that aims to alleviate the burden of manual
analysis that developers have to conduct to as-
sess the translation quality aspects involved in the
above mentioned situations. As a toy example,
consider for instance an evaluation setting with
two systems, s1 and s2, and two evaluation met-
rics m1 and m2. Assume also that m1 scores s1 to
be better than s2 in a particular test set, while m2
predicts just the contrary. In order to analyze this
contradictory evaluation one might be interested in
inspecting from the test set the particular transla-
tion examples that contribute to these results, i.e.,
text segments t for which the translation provided
by s1 is scored better by m1 than the translation
provided by s2 and the opposite behavior regard-
ing metric m2. tSEARCH allows to retrieve (vi-
sualize and export) these sentences with a simple
query in a fast time response. The search can be
further constrained, by requiring certain margins
on the differences, by including other systems or
metrics, or by requiring some specific syntactic or
semantic constructs to appear in the examples.
tSEARCH is build on top of ASIYA (Gim´enez
and M`arquez, 2010), an open-source toolkit for
MT evaluation; and it can be used along with
the ASIYA ON-LINE INTERFACE (Gonz`alez et al.,
2012), which provides an interactive environment
to examine the sentences. ASIYA allows to ana-
lyze a wide range of linguistic aspects of candi-
date and reference translations using a large set
of automatic and heterogeneous evaluation met-
rics. In particular, it offers a especially rich set
of measures that use syntactic and semantic infor-
mation. The intermediate structures generated by
the parsers, and used to compute the scoring mea-
sures, could be priceless for MT developers, who
can use them to compare the structures of several
translations and see how they affect the perfor-
mance of the metrics, providing more understand-
ing in order to interpret the actual performance of
the automatic translation systems.
tSEARCH consists of: 1) a database that stores
</bodyText>
<page confidence="0.977774">
181
</page>
<bodyText confidence="0.926554789473684">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 181–186,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
the resources generated by ASIYA, 2) a query lan-
guage and a search engine able to look through
the information gathered in the database, and 3) a
graphical user interface that assists the user to
write a query, returns the set of sentences that ful-
fill the conditions, and allows to export these re-
sults in XML format. The application is publicly
accessible on-line1, and a brief explanation of its
most important features is given in the demonstra-
tive video.
In the following, Section 2 gives an overview
of the ASIYA toolkit and the information gathered
from the evaluation output. Section 3 and Sec-
tion 4 describe in depth the tSEARCH application
and the on-line interface, respectively. Finally,
Section 5 reviews similar applications in compari-
son to the functionalities addressed by tSEARCH.
</bodyText>
<sectionHeader confidence="0.962054" genericHeader="method">
2 NIT Evaluation with the ASIYA Toolkit
</sectionHeader>
<bodyText confidence="0.999812">
Currently, ASIYA contains more than 800 variants
of MT metrics to measure the similarity between
two translations at several linguistic dimensions.
Moreover, the scores can be calculated at three
granularity levels: system (entire test-set), docu-
ment and sentence (or segment).
As shown in Figure 1, ASIYA requires the user
to provide a test suite. Then, the input files are
processed in order to calculate the annotations, the
parsing trees and the final metric scores. Sev-
eral external components are used for both, met-
ric computation and automatic linguistic analysis2.
The use of these tools depends on the languages
supported and the type of measures that one needs
to obtain. Hence, for instance, lexical-based
measures are computed using the last version
of most popular metrics, such as BLEU, NIST,
METEOR or ROUGE. The syntax-wise measures
need the output of taggers, lemmatizers, parsers
</bodyText>
<footnote confidence="0.469744">
1http://asiya.lsi.upc.edu/demo
2A complete list of external components can be found in
the Technical Manual at the ASIYA web-site
</footnote>
<bodyText confidence="0.9925343125">
and other analyzers. In those cases, ASIYA uses
the SVMTool (Gim´enez and M`arquez, 2004),
BIOS (Surdeanu et al., 2005), the Charniak-
Johnson and Berkeley constituent parsers (Char-
niak and Johnson, 2005; Petrov and Klein, 2007),
and the MALT dependency parser (Nivre et al.,
2007), among others.
In the tSEARCH platform, the system manages
the communication with an instance of the ASIYA
toolkit running on the server. For every test suite,
the system maintains a synchronized representa-
tion of the input data, the evaluation results and the
linguistic information generated. Then, the system
updates a database where the test suites are stored
for further analysis using the tSEARCH tool, as de-
scribed next.
</bodyText>
<sectionHeader confidence="0.994929" genericHeader="method">
3 The tSEARCH Tool
</sectionHeader>
<bodyText confidence="0.999959391304348">
tSEARCH offers a graphical search engine to ana-
lyze a given test suite. The system core retrieves
all translation examples that satisfy certain prop-
erties related to either the evaluation scores or the
linguistic structures. The query language designed
is simple and flexible, and it allows to combine
many properties to build sophisticated searches.
The tSEARCH architecture consists of the three
components illustrated in Figure 2: the web-based
interface, the storage system based on NoSQL
technology and the tSEARCH core, composed of
a query parser and a search engine.
The databases (Section 3.1) are fed through the
tSearch Data Loader API used by ASIYA. At
run-time, during the calculation of the measures,
ASIYA inserts all the information being calcu-
lated (metrics and parses) and a number of pre-
calculated variables (e.g., average, mean and per-
centiles). These operations are made in parallel,
which makes the overhead of filling the database
marginal.
The query parser (Section 3.2) receives the
query from the on-line interface and converts it
</bodyText>
<figureCaption confidence="0.997401">
Figure 1: ASIYA processes and data files Figure 2: tSEARCH architecture
</figureCaption>
<page confidence="0.766679">
182
</page>
<figure confidence="0.829341">
(a) Scores Column Family (b) Statistics Column Family
(c) Linguistic Elements Column Family
</figure>
<figureCaption confidence="0.999684">
Figure 3: tSEARCH data model
</figureCaption>
<bodyText confidence="0.9997372">
into a binary tree structure where each leaf is a sin-
gle part of an operation and each node combines
the partial results of the children. The search en-
gine obtains the final results by processing the tree
bottom-up until the root is reached.
</bodyText>
<subsectionHeader confidence="0.998862">
3.1 Data Representation, Storage and Access
</subsectionHeader>
<bodyText confidence="0.99793662962963">
The amount of data generated by ASIYA can be
very large for test sets with thousands of sen-
tences. In order to handle the high volume of
information, we decided to use the Apache Cas-
sandra database3, a NoSQL (also known as not
only SQL) solution that deals successfully with
this problem.
It is important to remark that there is no similar-
ity between NoSQL and the traditional relational
database management system model (RDBMS).
Actually, RDBMS uses SQL as its query language
and requires a relational model, whereas NoSQL
databases do not. Besides, the tSEARCH query
language can be complex, with several conditions,
which makes RDBMS perform poorly due the
complexity of the tables. In contrast, NoSQL-
databases use big-tables having many querying
information precalculated as key values, which
yields for direct access to the results.
The Cassandra data model is based on column
families (CF). A CF consists of a set of rows that
are uniquely identified by its key and have a set
of columns as values. So far, the tSEARCH data
model has the three CFs shown in Figure 3. The
scores CF in Figure 3(a) stores information related
to metrics and score values. Each row slot contains
the list of segments that matches the column key.
</bodyText>
<footnote confidence="0.805549">
3http://cassandra.apache.org/
</footnote>
<bodyText confidence="0.999735941176471">
The statistics CF in Figure 3(b) stores basic statis-
tics, such as the minimum, maximum, average,
median and percentiles values for every evaluation
metric. The CF having the linguistic elements in
Figure 3(c) stores the results of the parsers, such
as part-of-speech, grammatical categories and de-
pendency relationships.
One of the goals of NoSQL databases is to ob-
tain the information required in the minimum ac-
cess time. Therefore, the data is stored in the
way required by the tSEARCH application. For
instance, the query BLEU &gt; 0.4 looks for all
segments in the test suite having a BLEU score
greater than 0.4. Thus, in order to get the query
result in constant time, we use the metric identi-
fier as a part of the key for the scores CF, and the
score 0.4 as the column key.
</bodyText>
<subsectionHeader confidence="0.998759">
3.2 The Query Language and Parser
</subsectionHeader>
<bodyText confidence="0.999987882352941">
The Query Parser module is one of the key ingre-
dients in the tSEARCH application because it de-
termines the query grammar and the allowed op-
erations, and it provides a parsing method to an-
alyze any query and produce a machine-readable
version of its semantics. It is also necessary in or-
der to validate the query.
There are several types of queries, depending on
the operations used: arithmetic comparisons, sta-
tistical functions (e.g., average, quartiles), range
of values, linguistic elements and logical opera-
tors. Furthermore, the queries can be applied at
segment-, document- and/or system-level, and it
is even possible to create any group of systems
or metrics. This is useful, for instance, in or-
der to limit the search to certain type of systems
(e.g., rule-based vs. statistical) and specific met-
</bodyText>
<page confidence="0.997898">
183
</page>
<figureCaption confidence="0.998395">
Figure 4: (top) Query operations and functions, (bottom) Queries for group of systems and metrics
</figureCaption>
<bodyText confidence="0.903708333333333">
rics (e.g., lexical vs. syntactic). All possible query
types are described in the following subsections
(3.2.1 to 3.2.3) and listed in Figure 4.
</bodyText>
<subsectionHeader confidence="0.943977">
3.2.1 Segment-level and Metric-based
Queries
</subsectionHeader>
<bodyText confidence="0.999988295454545">
The most basic queries are those related to
segment level scores, i.e., obtain all segments
scored above/below a value for a concrete met-
ric. The common comparison operators are sup-
ported, such as for instance, BLEU &gt; 0.4 and
BLEU gt 0.4, that are both correct and equiva-
lent queries.
Basic statistics are also calculated at run-time,
which allows to use statistic variables as values,
e.g., obtain the segments scored in the fourth quar-
tile of BLEU. The maximum, minimum, average,
median and percentile values of each metric are
precalculated and saved into the MAX, MIN, AVG,
MEDIAN and PERC variables, respectively. The
thresholds and quartiles (TH,Q) are calculated at
run-time based on percentiles. MIN and MAX can
also be used and allow to get all segments in the
test set (i.e.,BLEU ge MIN).
The threshold function implies a percentage.
The query BLEU &gt; TH(20) gets all segments
that have a BLEU score greater than the score
value of the bottom 20% of the sentences.
It is also possible to specify an interval of values
using the operator IN[x,y]. The use of paren-
thesis is allowed in order to exclude the bound-
aries. The arguments for this operator can be
either numerical values or the predefined func-
tions for quartiles and percentiles. Therefore,
the following example BLEU IN [TH(20),
TH(30)] returns all segments with a BLEU score
in the range between the threshold of the 20% (in-
cluded) and the 30% (excluded).
The quartile function Q(X) takes a value be-
tween 1 and 4 and returns all segments that
have their score in that quartile. In contrast,
the percentile function generalizes the previous:
PERC(n,M), where 1 &lt; M &lt;= 100;1 &lt;= n &lt;=
M, returns all the segments with a score in the nth
part, when the range of scores is divided in M parts
of equal size.
Finally, a query can be composed of more than
one criterion. To do so, the logical operators AND
and OR are used to specify intersection and union,
respectively.
</bodyText>
<subsectionHeader confidence="0.532207">
3.2.2 System- and Document-level Queries
</subsectionHeader>
<bodyText confidence="0.999919">
The queries described next implement the search
procedures for more sophisticated queries involv-
ing system and document level properties, and
also the linguistic information used in the calcu-
</bodyText>
<page confidence="0.997597">
184
</page>
<bodyText confidence="0.99996525">
lation of the evaluation measures. The purpose of
this functionality is to answer questions related to
groups of systems and/or metrics.
As explained in the introduction, one may
want to find the segments with good scores
for lexical metrics and, simultaneously, bad
scores for syntactic-based ones, or viceversa.
The following query illustrates how to do
</bodyText>
<listItem confidence="0.3067255">
it: ((srb[LEX] &gt; AVG) OR (s3[LEX]
&lt; AVG)) AND ((srb[SYN] &lt; AVG) OR
(s3[SYN] &gt; AVG) ), where srb = {s1, s2}
is the definition of a group of the rule-based
</listItem>
<bodyText confidence="0.975071909090909">
systems s1 and s2, s3 is another transla-
tion system, and LEX={BLEU,NIST} and
SYN={CP-Op(*),SP-Oc(*)} are two groups
of lexical- and syntactic-based measures, respec-
tively. The output of this kind of queries can help
developers to inspect the differences between the
systems that meet these criteria.
Concerning queries at document level, its struc-
ture is the same but applied at document scope.
They may help to find divergences when translat-
ing documents from different domains.
</bodyText>
<subsectionHeader confidence="0.969984">
3.2.3 Linguistic Element-based Queries
</subsectionHeader>
<bodyText confidence="0.999370555555556">
The last functionality in tSEARCH allows search-
ing the segments that contain specific linguistic
elements (LE), estimated with any of the ana-
lyzers used to calculate the linguistic structures.
Linguistic-wise queries will allow the user to
find segments which match the criteria for any
linguistic feature calculated by ASIYA: part-of-
speech, lemmas, named entities, grammatical cat-
egories, dependency relations, semantic roles and
discourse structures.
We have implemented queries that match
n-grams of lemmas (lemma), parts-of-speech
(pos) and items of shallow (SP) or constituent
parsing (CP), dependency relations (DP) and se-
mantic roles SR, such as LE[lemma(be),
pos(NN,adj), SP(NP,ADJP,VP),
CP(VP,PP)]. The DP function allows also
specifying a compositional criterion (i.e., the
categories of two words and their dependency
relationship) and even a chain of relations, e.g.,
LE[DP(N,nsubj,V,dep,V)]. In turn, the
SR function obtains the segments that match a
verb and its list of arguments, e.g., LE[SR(ask,
A0, A1)].
The asterisk symbol can be used to substi-
tute any LE-item, e.g., LE[SP(NP,*,PP),
DP(*,*,V)]. When combined with semantic
roles, one asterisk substitutes any verb that has all
the arguments specified, e.g., LE[SR(*, A0,
A1)], whereas two asterisks in a row allow
arguments to belong to different verbs in the
same sentence. For instance, LE[SR(**, A1,
AM-TMP)] matches the sentence Those who pre-
fer to save money, may try to wait a few more days,
where the verb wait has the argument AM-TMP
and the verb prefer has the argument A1.
</bodyText>
<sectionHeader confidence="0.9939745" genericHeader="method">
4 On-line Interface and Export of the
Results
</sectionHeader>
<bodyText confidence="0.999882285714286">
tSEARCH is fully accessible on-line through the
ASIYA ON-LINE INTERFACE. The web applica-
tion runs ASIYA remotely, calculates the scores
and fills the tSEARCH database. It also offers the
chance to upload the results of a test suite previ-
ously processed. This way it feeds the database
directly, without the need to run ASIYA.
Anyhow, once the tSEARCH interface is already
accessible, one can see a tools icon on the right
of the search box. It shows the toolbar with all
available metrics, functions and operations. The
search box allows to query the database using the
query language described in Section 3.2.
After typing a query, the user can navigate the
results using three different views that organize
them according to the user preferences: 1) All
segments shows all segments and metrics men-
tioned in the query, the segments can be sorted
by the score, in ascendent or descendent order,
just tapping on the metric name; 2) Grouped by
system groups the segments by system and, for
</bodyText>
<figureCaption confidence="0.997202">
Figure 5: The tSEARCH Interface
</figureCaption>
<page confidence="0.995469">
185
</page>
<bodyText confidence="0.999988277777778">
each system, by document; 3) Grouped by segment
displays the segment organization, which allows
an easy comparison between several translations.
Each group contains all the information related to
a segment number, such as the source and the ref-
erence sentences along with the candidate transla-
tions that matched the query.
Additionally, moving the mouse over the seg-
ments displays a floating box as illustrated in Fig-
ure 5. It shows some relevant information, such
as the source and references segments, the system
that generated the translation, the document which
the segment belongs to, and the scores.
Finally, all output data obtained during the
search can be exported as an XML file. It is possi-
ble to export all segments, or the results structured
by system, by segment, or more specific informa-
tion from the views.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
5 Related Work and Conclusions
</sectionHeader>
<bodyText confidence="0.999825375">
The ultimate goal of tSEARCH is to provide the
community with a user-friendly tool that facilitates
the qualitative analysis of automatic translations.
Currently, there are no freely available automatic
tools for aiding MT evaluation tasks. For this rea-
son, we believe that tSEARCH can be a useful tool
for MT system and evaluation metric developers.
So far, related works in the field address (semi)-
automatic error analysis from different perspec-
tives. A framework for error analysis and classifi-
cation was proposed in (Vilar et al., 2006), which
has inspired more recent works in the area, such
as (Fishel et al., 2011). They propose a method
for automatic identification of various error types.
The methodology proposed is language indepen-
dent and tackles lexical information. Nonetheless,
it can also take into account language-dependent
information if linguistic analyzers are available.
The user interface presented in (Berka et al., 2012)
provides also automatic error detection and clas-
sification. It is the result of merging the Hjer-
son tool (Popovi´c, 2011) and Addicter (Zeman et
al., 2011). This web application shows alignments
and different types of errors colored.
In contrast, the ASIYA interface and the
tSEARCH tool together facilitate the qualitative
analysis of the evaluation results yet providing
a framework to obtain multiple evaluation met-
rics and linguistic analysis of the translations.
They also provide the mechanisms to search and
find relevant translation examples using a flexible
query language, and to export the results.
</bodyText>
<sectionHeader confidence="0.996523" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.969650125">
This research has been partially funded by
the Spanish Ministry of Education and Science
(OpenMT-2, TIN2009-14675-C03), the European
Community’s Seventh Framework Programme un-
der grant agreement number 247762 (FAUST,
FP7-ICT-2009-4-247762) and the EAMT Spon-
sorhip of Activities: Small research and develop-
ment project, 2012.
</bodyText>
<sectionHeader confidence="0.99831" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999575219512195">
Jan Berka, Ondrej Bojar, Mark Fishel, Maja Popovic,
and Daniel Zeman. 2012. Automatic MT error anal-
ysis: Hjerson helping Addicter. In Proc. 8th LREC.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proc. 43rd Meeting of the ACL.
Mark Fishel, Ondˇrej Bojar, Daniel Zeman, and Jan
Berka. 2011. Automatic Translation Error Analy-
sis. In Proc. 14th Text, Speech and Dialogue (TSD).
Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proc. 4th Intl. Conf. LREC.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94.
Meritxell Gonz`alez, Jes´us Gim´enez, and Llu´ıs
M`arquez. 2012. A Graphical Interface for MT Eval-
uation and Error Analysis. In Proc. 50th Meeting of
the ACL. System Demonstration.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering, 13.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. HLT.
Maja Popovi´c. 2011. Hjerson: An Open Source
Tool for Automatic Error Classification of Machine
Translation Output. The Prague Bulletin of Mathe-
matical Linguistics, 96.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proc. 9th INTERSPEECH.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proc. 5th LREC.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondrej Bo-
jar. 2011. Addicter: What Is Wrong with My Trans-
lations? The Prague Bulletin of Mathematical Lin-
guistics, 96.
</reference>
<page confidence="0.998772">
186
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911197">
<title confidence="0.987801">Flexible and Fast Search over Automatic Translations Improved Quality/Error Analysis</title>
<author confidence="0.98255">Gonz`alez Mascarell</author>
<affiliation confidence="0.9874265">TALP Research Universitat Polit`ecnica de</affiliation>
<abstract confidence="0.998487045454545">work presents a web-based application that provides mechanisms for doing complex searches over a collection of translation cases evaluated with a large of diverse measures. the results obtained with the toolkit for MT evaluation and it is connected to its on-line GUI, which makes possible a graphical visualization and interactive access to the evaluation results. The search engine offers a flexible query language allowing to find translation examples matching a combination of numerical and structural features associated to the calculation of the quality metrics. Its database design permits a fast response time for all queries supported on realistic-size test beds. In sumused with offers developers of MT systems and evaluation metrics a powerful tool for helping translation and error analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Berka</author>
<author>Ondrej Bojar</author>
<author>Mark Fishel</author>
<author>Maja Popovic</author>
<author>Daniel Zeman</author>
</authors>
<title>Automatic MT error analysis: Hjerson helping Addicter.</title>
<date>2012</date>
<booktitle>In Proc. 8th LREC.</booktitle>
<contexts>
<context position="19086" citStr="Berka et al., 2012" startWordPosition="3076" endWordPosition="3079">ion metric developers. So far, related works in the field address (semi)- automatic error analysis from different perspectives. A framework for error analysis and classification was proposed in (Vilar et al., 2006), which has inspired more recent works in the area, such as (Fishel et al., 2011). They propose a method for automatic identification of various error types. The methodology proposed is language independent and tackles lexical information. Nonetheless, it can also take into account language-dependent information if linguistic analyzers are available. The user interface presented in (Berka et al., 2012) provides also automatic error detection and classification. It is the result of merging the Hjerson tool (Popovi´c, 2011) and Addicter (Zeman et al., 2011). This web application shows alignments and different types of errors colored. In contrast, the ASIYA interface and the tSEARCH tool together facilitate the qualitative analysis of the evaluation results yet providing a framework to obtain multiple evaluation metrics and linguistic analysis of the translations. They also provide the mechanisms to search and find relevant translation examples using a flexible query language, and to export th</context>
</contexts>
<marker>Berka, Bojar, Fishel, Popovic, Zeman, 2012</marker>
<rawString>Jan Berka, Ondrej Bojar, Mark Fishel, Maja Popovic, and Daniel Zeman. 2012. Automatic MT error analysis: Hjerson helping Addicter. In Proc. 8th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine N-best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Meeting of the ACL.</booktitle>
<contexts>
<context position="6229" citStr="Charniak and Johnson, 2005" startWordPosition="974" endWordPosition="978"> on the languages supported and the type of measures that one needs to obtain. Hence, for instance, lexical-based measures are computed using the last version of most popular metrics, such as BLEU, NIST, METEOR or ROUGE. The syntax-wise measures need the output of taggers, lemmatizers, parsers 1http://asiya.lsi.upc.edu/demo 2A complete list of external components can be found in the Technical Manual at the ASIYA web-site and other analyzers. In those cases, ASIYA uses the SVMTool (Gim´enez and M`arquez, 2004), BIOS (Surdeanu et al., 2005), the CharniakJohnson and Berkeley constituent parsers (Charniak and Johnson, 2005; Petrov and Klein, 2007), and the MALT dependency parser (Nivre et al., 2007), among others. In the tSEARCH platform, the system manages the communication with an instance of the ASIYA toolkit running on the server. For every test suite, the system maintains a synchronized representation of the input data, the evaluation results and the linguistic information generated. Then, the system updates a database where the test suites are stored for further analysis using the tSEARCH tool, as described next. 3 The tSEARCH Tool tSEARCH offers a graphical search engine to analyze a given test suite. Th</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-Fine N-best Parsing and MaxEnt Discriminative Reranking. In Proc. 43rd Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Fishel</author>
<author>Ondˇrej Bojar</author>
<author>Daniel Zeman</author>
<author>Jan Berka</author>
</authors>
<title>Automatic Translation Error Analysis.</title>
<date>2011</date>
<booktitle>In Proc. 14th Text, Speech and Dialogue (TSD).</booktitle>
<contexts>
<context position="18762" citStr="Fishel et al., 2011" startWordPosition="3031" endWordPosition="3034"> ultimate goal of tSEARCH is to provide the community with a user-friendly tool that facilitates the qualitative analysis of automatic translations. Currently, there are no freely available automatic tools for aiding MT evaluation tasks. For this reason, we believe that tSEARCH can be a useful tool for MT system and evaluation metric developers. So far, related works in the field address (semi)- automatic error analysis from different perspectives. A framework for error analysis and classification was proposed in (Vilar et al., 2006), which has inspired more recent works in the area, such as (Fishel et al., 2011). They propose a method for automatic identification of various error types. The methodology proposed is language independent and tackles lexical information. Nonetheless, it can also take into account language-dependent information if linguistic analyzers are available. The user interface presented in (Berka et al., 2012) provides also automatic error detection and classification. It is the result of merging the Hjerson tool (Popovi´c, 2011) and Addicter (Zeman et al., 2011). This web application shows alignments and different types of errors colored. In contrast, the ASIYA interface and the </context>
</contexts>
<marker>Fishel, Bojar, Zeman, Berka, 2011</marker>
<rawString>Mark Fishel, Ondˇrej Bojar, Daniel Zeman, and Jan Berka. 2011. Automatic Translation Error Analysis. In Proc. 14th Text, Speech and Dialogue (TSD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proc. 4th Intl. Conf. LREC.</booktitle>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. In Proc. 4th Intl. Conf. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague</title>
<date>2010</date>
<journal>Bulletin of Mathematical Linguistics,</journal>
<volume>94</volume>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics, 94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meritxell Gonz`alez</author>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>A Graphical Interface for MT Evaluation and Error Analysis.</title>
<date>2012</date>
<booktitle>In Proc. 50th Meeting of the ACL. System Demonstration.</booktitle>
<marker>Gonz`alez, Gim´enez, M`arquez, 2012</marker>
<rawString>Meritxell Gonz`alez, Jes´us Gim´enez, and Llu´ıs M`arquez. 2012. A Graphical Interface for MT Evaluation and Error Analysis. In Proc. 50th Meeting of the ACL. System Demonstration.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<contexts>
<context position="6307" citStr="Nivre et al., 2007" startWordPosition="988" endWordPosition="991"> for instance, lexical-based measures are computed using the last version of most popular metrics, such as BLEU, NIST, METEOR or ROUGE. The syntax-wise measures need the output of taggers, lemmatizers, parsers 1http://asiya.lsi.upc.edu/demo 2A complete list of external components can be found in the Technical Manual at the ASIYA web-site and other analyzers. In those cases, ASIYA uses the SVMTool (Gim´enez and M`arquez, 2004), BIOS (Surdeanu et al., 2005), the CharniakJohnson and Berkeley constituent parsers (Charniak and Johnson, 2005; Petrov and Klein, 2007), and the MALT dependency parser (Nivre et al., 2007), among others. In the tSEARCH platform, the system manages the communication with an instance of the ASIYA toolkit running on the server. For every test suite, the system maintains a synchronized representation of the input data, the evaluation results and the linguistic information generated. Then, the system updates a database where the test suites are stored for further analysis using the tSEARCH tool, as described next. 3 The tSEARCH Tool tSEARCH offers a graphical search engine to analyze a given test suite. The system core retrieves all translation examples that satisfy certain properti</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In</title>
<date>2007</date>
<booktitle>Proc. HLT.</booktitle>
<contexts>
<context position="6254" citStr="Petrov and Klein, 2007" startWordPosition="979" endWordPosition="982">and the type of measures that one needs to obtain. Hence, for instance, lexical-based measures are computed using the last version of most popular metrics, such as BLEU, NIST, METEOR or ROUGE. The syntax-wise measures need the output of taggers, lemmatizers, parsers 1http://asiya.lsi.upc.edu/demo 2A complete list of external components can be found in the Technical Manual at the ASIYA web-site and other analyzers. In those cases, ASIYA uses the SVMTool (Gim´enez and M`arquez, 2004), BIOS (Surdeanu et al., 2005), the CharniakJohnson and Berkeley constituent parsers (Charniak and Johnson, 2005; Petrov and Klein, 2007), and the MALT dependency parser (Nivre et al., 2007), among others. In the tSEARCH platform, the system manages the communication with an instance of the ASIYA toolkit running on the server. For every test suite, the system maintains a synchronized representation of the input data, the evaluation results and the linguistic information generated. Then, the system updates a database where the test suites are stored for further analysis using the tSEARCH tool, as described next. 3 The tSEARCH Tool tSEARCH offers a graphical search engine to analyze a given test suite. The system core retrieves a</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proc. HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
</authors>
<title>Hjerson: An Open Source Tool for Automatic Error Classification of Machine Translation Output. The Prague Bulletin of</title>
<date>2011</date>
<journal>Mathematical Linguistics,</journal>
<volume>96</volume>
<marker>Popovi´c, 2011</marker>
<rawString>Maja Popovi´c. 2011. Hjerson: An Open Source Tool for Automatic Error Classification of Machine Translation Output. The Prague Bulletin of Mathematical Linguistics, 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Eli Comelles</author>
</authors>
<title>Named Entity Recognition from Spontaneous OpenDomain Speech.</title>
<date>2005</date>
<booktitle>In Proc. 9th INTERSPEECH.</booktitle>
<contexts>
<context position="6147" citStr="Surdeanu et al., 2005" startWordPosition="963" endWordPosition="966">computation and automatic linguistic analysis2. The use of these tools depends on the languages supported and the type of measures that one needs to obtain. Hence, for instance, lexical-based measures are computed using the last version of most popular metrics, such as BLEU, NIST, METEOR or ROUGE. The syntax-wise measures need the output of taggers, lemmatizers, parsers 1http://asiya.lsi.upc.edu/demo 2A complete list of external components can be found in the Technical Manual at the ASIYA web-site and other analyzers. In those cases, ASIYA uses the SVMTool (Gim´enez and M`arquez, 2004), BIOS (Surdeanu et al., 2005), the CharniakJohnson and Berkeley constituent parsers (Charniak and Johnson, 2005; Petrov and Klein, 2007), and the MALT dependency parser (Nivre et al., 2007), among others. In the tSEARCH platform, the system manages the communication with an instance of the ASIYA toolkit running on the server. For every test suite, the system maintains a synchronized representation of the input data, the evaluation results and the linguistic information generated. Then, the system updates a database where the test suites are stored for further analysis using the tSEARCH tool, as described next. 3 The tSEAR</context>
</contexts>
<marker>Surdeanu, Turmo, Comelles, 2005</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005. Named Entity Recognition from Spontaneous OpenDomain Speech. In Proc. 9th INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error Analysis of Machine Translation Output.</title>
<date>2006</date>
<booktitle>In Proc. 5th LREC.</booktitle>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error Analysis of Machine Translation Output. In Proc. 5th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Mark Fishel</author>
<author>Jan Berka</author>
<author>Ondrej Bojar</author>
</authors>
<title>Addicter: What Is Wrong with My Translations? The Prague Bulletin of</title>
<date>2011</date>
<journal>Mathematical Linguistics,</journal>
<volume>96</volume>
<contexts>
<context position="19242" citStr="Zeman et al., 2011" startWordPosition="3102" endWordPosition="3105">is and classification was proposed in (Vilar et al., 2006), which has inspired more recent works in the area, such as (Fishel et al., 2011). They propose a method for automatic identification of various error types. The methodology proposed is language independent and tackles lexical information. Nonetheless, it can also take into account language-dependent information if linguistic analyzers are available. The user interface presented in (Berka et al., 2012) provides also automatic error detection and classification. It is the result of merging the Hjerson tool (Popovi´c, 2011) and Addicter (Zeman et al., 2011). This web application shows alignments and different types of errors colored. In contrast, the ASIYA interface and the tSEARCH tool together facilitate the qualitative analysis of the evaluation results yet providing a framework to obtain multiple evaluation metrics and linguistic analysis of the translations. They also provide the mechanisms to search and find relevant translation examples using a flexible query language, and to export the results. Acknowledgments This research has been partially funded by the Spanish Ministry of Education and Science (OpenMT-2, TIN2009-14675-C03), the Europ</context>
</contexts>
<marker>Zeman, Fishel, Berka, Bojar, 2011</marker>
<rawString>Daniel Zeman, Mark Fishel, Jan Berka, and Ondrej Bojar. 2011. Addicter: What Is Wrong with My Translations? The Prague Bulletin of Mathematical Linguistics, 96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>