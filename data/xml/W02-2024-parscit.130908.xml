<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018022">
<title confidence="0.9866835">
Introduction to the CoNLL-2002 Shared Task:
Language-Independent Named Entity Recognition
</title>
<author confidence="0.88541">
Erik F. Tjong Kim Sang
</author>
<affiliation confidence="0.8380835">
CNTS - Language Technology Group
University of Antwerp
</affiliation>
<email confidence="0.994837">
erikt@uia.ua.ac.be
</email>
<sectionHeader confidence="0.987346" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999925">
We describe the CoNLL-2002 shared task:
language-independent named entity recogni-
tion. We give background information on the
data sets and the evaluation method, present a
general overview of the systems that have taken
part in the task and discuss their performance.
</bodyText>
<sectionHeader confidence="0.993743" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99954076">
Named entities are phrases that contain the
names of persons, organizations, locations,
times and quantities. Example:
[PER Wolff ] , currently a journalist in
[LOC Argentina ] , played with [PER
Del Bosque ] in the final years of the
seventies in [ORG Real Madrid ] .
This sentence contains four named entities:
Wolï¿½ and Del Bosque are persons, Argentina
is a location and Real Madrid is a organiza-
tion. The shared task of CoNLL-2002 concerns
language-independent named entity recogni-
tion. We will concentrate on four types of
named entities: persons, locations, organiza-
tions and names of miscellaneous entities that
do not belong to the previous three groups. The
participants of the shared task have been offered
training and test data for two European lan-
guages: Spanish and Dutch. They have used the
data for developing a named-entity recognition
system that includes a machine learning compo-
nent. The organizers of the shared task were es-
pecially interested in approaches that make use
of additional nonannotated data for improving
their performance.
</bodyText>
<sectionHeader confidence="0.631917" genericHeader="introduction">
2 Data and Evaluation
</sectionHeader>
<bodyText confidence="0.997327523809524">
The CoNLL-2002 named entity data consists of
six files covering two languages: Spanish and
Dutch&apos;. Each of the languages has a training
file, a development file and a test file. The
learning methods will be trained with the train-
ing data. The development data can be used
for tuning the parameters of the learning meth-
ods. When the best parameters are found, the
method can be trained on the training data and
tested on the test data. The results of the dif-
ferent learning methods on the test sets will be
compared in the evaluation of the shared task.
The split between development data and test
data has been chosen to avoid that systems are
being tuned to the test data.
All data files contain one word per line with
empty lines representing sentence boundaries.
Additionally each line contains a tag which
states whether the word is inside a named entity
or not. The tag also encodes the type of named
entity. Here is a part of the example sentence:
</bodyText>
<equation confidence="0.771889333333333">
Wolff B-PER
, O
currently O
a O
journalist O
in O
Argentina B-LOC
, O
played O
with O
Del B-PER
Bosque I-PER
</equation>
<bodyText confidence="0.724841">
Words tagged with O are outside of named
entities. The B-XXX tag is used for the first
word in a named entity of type XXX and I-
XXX is used for all other words in named en-
tities of type XXX. The data contains enti-
</bodyText>
<footnote confidence="0.949088">
&apos;The data files are available from http://lcg-www.
uia.ac.be/conll2002/ner/
</footnote>
<bodyText confidence="0.999867020833333">
ties of four types: persons (PER), organiza-
tions (ORG), locations (LOC) and miscella-
neous names (MISC). The tagging scheme is a
variant of the IOB scheme originally put for-
ward by Ramshaw and Marcus (1995). We as-
sume that named entities are non-recursive and
non-overlapping. In case a named entity is em-
bedded in another named entity usually only
the top level entity will be marked.
The Spanish data is a collection of news wire
articles made available by the Spanish EFE
News Agency. The articles are from May 2000.
The annotation was carried out by the TALP
Research Center2 of the Technical University of
Catalonia (UPC) and the Center of Language
and Computation (CLiC3) of the University of
Barcelona (UB), and funded by the European
Commission through the NAMIC project (IST-
1999-12392). The data contains words and en-
tity tags only. The training, development and
test data files contain 273037, 54837 and 53049
lines respectively.
The Dutch data consist of four editions of the
Belgian newspaper &amp;quot;De Morgen&amp;quot; of 2000 (June
2, July 1, August 1 and September 1). The data
was annotated as a part of the Atranos project4
at the University of Antwerp in Belgium, Eu-
rope. The annotator has followed the MITRE
and SAIC guidelines for named entity recogni-
tion (Chinchor et al., 1999) as well as possible.
The data consists of words, entity tags and part-
of-speech tags which have been derived by a
Dutch part-of-speech tagger (Daelemans et al.,
1996). Additionally the article boundaries in
the text have been marked explicitly with lines
containing the tag -DOCSTART-. The training,
development and test data files contain 218737,
40656 and 74189 lines respectively.
The performance in this task is mea-
sured with F,3=1 rate which is equal to
(32+1)*precision*recall / (&apos;32*precision+recall)
with 3=1 (van Rijsbergen, 1975). Precision is
the percentage of named entities found by the
learning system that are correct. Recall is the
percentage of named entities present in the cor-
pus that are found by the system. A named
entity is correct only if it is an exact match of
the corresponding entity in the data file.
</bodyText>
<footnote confidence="0.999100333333333">
2http://www.talp.upc.es/
3http://clic.fil.ub.es/
4http://atranos.esat.kuleuven.ac.be/
</footnote>
<sectionHeader confidence="0.995475" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999847068493151">
Twelve systems have participated in this shared
task. The results for the test sets for Spanish
and Dutch can be found in Table 1. A baseline
rate was computed for both sets. It was pro-
duced by a system which only identified entities
which had a unique class in the training data. If
a phrase was part of more than one entity, the
system would select the longest one. All sys-
tems that participated in the shared task have
outperformed the baseline system.
McNamee and Mayfield (2002) have applied
support vector machines to the data of the
shared task. Their system used many binary
features for representing words (almost 9000).
They have evaluated different parameter set-
tings of the system and have selected a cascaded
approach in which first entity boundaries were
predicted and then entity classes (Spanish test
set: F,3=1=60.97; Dutch test set: F,3=1=59.52).
Black and Vasilakopoulos (2002) have evalu-
ated two approaches to the shared task. The
first was a transformation-based method which
generated in rules in a single pass rather than
in many passes. The second method was a
decision tree method. They found that the
transformation-based method consistently out-
performed the decision trees (Spanish test set:
F,3=1=67.49; Dutch test set: F,3=1=56.43)
Tsukamoto, Mitsuishi and Sassano (2002)
used a stacked AdaBoost classifier for finding
named entities. They found that cascading clas-
sifiers helped improved performance. Their fi-
nal system consisted of a cascade of five learners
each of which performed 10,000 boosting rounds
(Spanish test set: F,3=1=71.49; Dutch test set:
F,3=1=60.93)
Malouf (2002) tested different models with
the shared task data: a statistical baseline
model, a Hidden Markov Model and maximum
entropy models with different features. The lat-
ter proved to perform best. The maximum en-
tropy models benefited from extra feature which
encoded capitalization information, positional
information and information about the current
word being part of a person name earlier in
the text. However, incorporating a list of per-
son names in the training process did not help
(Spanish test set: F,3=1=73.66; Dutch test set:
F,3=1=68.08)
Jansche (2002) employed a first-order Markov
model as a named entity recognizer. His system
used two separate passes, one for extracting en-
tity boundaries and one for classifying entities.
He evaluated different features in both subpro-
cesses. The categorization process was trained
separately from the extraction process but that
did not seem to have harmed overall perfor-
mance (Spanish test set: F,3=1=73.89; Dutch
test set: F,3=1=69.68)
Patrick, Whitelaw and Munro (2002) present
SLINERC, a language-independent named en-
tity recognizer. The system uses tries as well
as character n-grams for encoding word-internal
and contextual information. Additionally, it re-
lies on lists of entities which have been com-
piled from the training data. The overall sys-
tem consists of six stages, three regarding entity
recognition and three for entity categorization.
Stages use the output of previous stages for ob-
taining an improved performance (Spanish test
set: F,3=1=73.92; Dutch test set: F,3=1=71.36)
Tjong Kim Sang (2002) has applied a
memory-based learner to the data of the shared
task. He used a two-stage processing strategy
as well: first identifying entities and then clas-
sifying them. Apart from the base classifier,
his system made use of three extra techniques
for boosting performance: cascading classifiers
(stacking), feature selection and system combi-
nation. Each of these techniques were shown to
be useful (Spanish test set: F,3=1=75.78; Dutch
test set: F,3=1=70.67).
Burger, Henderson and Morgan (2002) have
evaluated three approaches to finding named
entities. They started with a baseline system
which consisted of an HMM-based phrase tag-
ger. They gave the tagger access to a list
of approximately 250,000 named entities and
the performance improved. After this several
smoothed word classes derived from the avail-
able data were incorporated into the training
process. The system performed better with the
derived word lists than with the external named
entity lists (Spanish test set: F,3=1=75.78;
Dutch test set: F,3=1=72.57).
Cucerzan and Yarowsky (2002) approached
the shared task by using word-internal and con-
textual information stored in character-based
tries. Their system obtained good results by us-
ing part-of-speech tag information and employ-
ing the one sense per discourse principle. The
authors expect a performance increase when the
system has access to external entity lists but
have not presented the results of this in detail
(Spanish test set: F,3=1=77.15; Dutch test set:
F/3=1=72.31).
Wu, Ngai, Carpuat, Larsen and Yang (2002)
have applied AdaBoost.MH to the shared
task data and compared the performance with
that of a maximum entropy-based named
entity tagger. Their system used lexical
and part-of-speech information, contextual and
word-internal clues, capitalization information,
knowledge about entity classes of previous oc-
currences of words and a small external list
of named entity words. The boosting tech-
niques operated on decision stumps, decision
trees of depth one. They outperformed the
maximum entropy-based named entity tagger
(Spanish test set: F,3=1=76.61; Dutch test set:
F,3=1=75.36).
Florian (2002) employed three stacked
learners for named entity recognition:
transformation-based learning for obtain-
ing base-level non-typed named entities, Snow
for improving the quality of these entities
and the forward-backward algorithm for find-
ing categories for the named entities. The
combination of the three algorithms showed
a substantially improved performance when
compared with a single algorithm and an
algorithm pair (Spanish test set: F,3=1=79.05;
Dutch test set: F,3=1=74.99).
Carreras, Marquez and Padro (2002) have ap-
proached the shared task by using AdaBoost
applied to fixed-depth decision trees. Their sys-
tem used many different input features contex-
tual information, word-internal clues, previous
entity classes, part-of-speech tags (Dutch only)
and external word lists (Spanish only). It pro-
cessed the data in two stages: first entity recog-
nition and then classification. Their system
obtained the best results in this shared task
for both the Spanish and Dutch test data sets
(Spanish test set: F,3=1=81.39; Dutch test set:
F,3=1=77.05).
</bodyText>
<sectionHeader confidence="0.948983" genericHeader="method">
4 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999827428571429">
We have described the CoNLL-2002 shared
task: language-independent named entity
recognition. Twelve different systems have been
applied to data covering two Western European
languages: Spanish and Dutch. A boosted deci-
sion tree method obtained the best performance
on both data sets (Carreras et al., 2002).
</bodyText>
<sectionHeader confidence="0.973791" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.995619">
Tjong Kim Sang is supported by IWT STWW
as a researcher in the ATRANOS project.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994045869565218">
William J. Black and Argyrios Vasilakopoulos. 2002.
Language-independent named entity classification
by modified transformation-based learning and by
decision tree induction. In Proceedings of CoNLL-
2002. Taipei, Taiwan.
John D. Burger, John C. Henderson, and William T.
Morgan. 2002. Statistical named entity recog-
nizer adaptation. In Proceedings of CoNLL-2002.
Taipei, Taiwan.
Xavier Carreras, Lluis Marques, and Lluis Padro.
2002. Named entity extraction using adaboost.
In Proceedings of CoNLL-2002. Taipei, Taiwan.
Nancy Chinchor, Erica Brown, Lisa Ferro, and Patty
Robinson. 1999. 1999 Named Entity Recognition
Task Definition. MITRE and SAIC.
Silviu Cucerzan and David Yarowsky. 2002. Lan-
guage independent ner using a unified model of
internal and contextual evidence. In Proceedings
of CoNLL-2002. Taipei, Taiwan.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part
of speech tagger-generator. In Proceedings of the
Fourth Workshop on Very Large Corpora, pages
14{27. Copenhagen, Denmark.
Radu Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceedings
of CoNLL-2002. Taipei, Taiwan.
Martin Jansche. 2002. Named entity extraction
with conditional markov models and classifiers.
In Proceedings of CoNLL-2002. Taipei, Taiwan.
Robert Malouf. 2002. Markov models for language-
independent named entity recognition. In Pro-
ceedings of CoNLL-2002. Taipei, Taiwan.
Paul McNamee and James Mayfield. 2002. Entity
extraction without language-specific resources. In
Proceedings of CoNLL-2002. Taipei, Taiwan.
Jon Patrick, Casey Whitelaw, and Robert Munro.
2002. Slinerc: The sydney language-independent
named entity recogniser and classifier. In Proceed-
ings of CoNLL-2002. Taipei, Taiwan.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learn-
ing. In Proceedings of the Third ACL Workshop
on Very Large Corpora, pages 82{94. Cambridge,
MA, USA.
Erik F. Tjong Kim Sang. 2002. Memory-based
</reference>
<table confidence="0.999443892857143">
Spanish test precision recall F,3=1
Carreras et.al. 81.38% 81.40% 81.39
Florian 78.70% 79.40% 79.05
Cucerzan et.al. 78.19% 76.14% 77.15
Wu et.al. 75.85% 77.38% 76.61
Burger et.al. 74.19% 77.44% 75.78
Tjong Kim Sang 76.00% 75.55% 75.78
Patrick et.al. 74.32% 73.52% 73.92
Jansche 74.03% 73.76% 73.89
Malouf 73.93% 73.39% 73.66
Tsukamoto 69.04% 74.12% 71.49
Black et.al. 68.78% 66.24% 67.49
McNamee et.al. 56.28% 66.51% 60.97
baselines 26.27% 56.48% 35.86
Dutch test precision recall F,3=1
Carreras et.al. 77.83% 76.29% 77.05
Wu et.al. 76.95% 73.83% 75.36
Florian 75.10% 74.89% 74.99
Burger et.al. 72.69% 72.45% 72.57
Cucerzan et.al. 73.03% 71.62% 72.31
Patrick et.al. 74.01% 68.90% 71.36
Tjong Kim Sang 72.56% 68.88% 70.67
Jansche 70.11% 69.26% 69.68
Malouf 70.88% 65.50% 68.08
Tsukamoto 57.33% 65.02% 60.93
McNamee et.al. 56.22% 63.24% 59.52
Black et.al. 62.12% 51.69% 56.43
baselines 64.38% 45.19% 53.10
</table>
<tableCaption confidence="0.994915">
Table 1: Overall precision, recall and F,3=1 rates
</tableCaption>
<reference confidence="0.9103862">
obtained by the twelve participating systems on
the test data sets for the two languages in the
CoNLL-2002 shared task.
named entity recognition. In Proceedings of
CoNLL-2002. Taipei, Taiwan.
Koji Tsukamoto, Yutaka Mitsuishi, and Manabu
Sassano. 2002. Learning with multiple stacking
for named entity recognition. In Proceedings of
CoNLL-2002. Taipei, Taiwan.
C.J. van Rijsbergen. 1975. Information Retrieval.
Buttersworth.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe
Larsen, and Yongsheng Yang. 2002. Boosting
for named entity recognition. In Proceedings of
CoNLL-2002. Taipei, Taiwan.
</reference>
<bodyText confidence="0.9697865">
SDue to some harmful annotation errors in the train-
ing data, the baseline system performs less well than
expected. Without the errors, the baseline Fp-i rates
would have been 62.49 for Spanish and 57.59 for Dutch.
</bodyText>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William J Black</author>
<author>Argyrios Vasilakopoulos</author>
</authors>
<title>Language-independent named entity classification by modified transformation-based learning and by decision tree induction.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL2002.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="6015" citStr="Black and Vasilakopoulos (2002)" startWordPosition="992" endWordPosition="995"> training data. If a phrase was part of more than one entity, the system would select the longest one. All systems that participated in the shared task have outperformed the baseline system. McNamee and Mayfield (2002) have applied support vector machines to the data of the shared task. Their system used many binary features for representing words (almost 9000). They have evaluated different parameter settings of the system and have selected a cascaded approach in which first entity boundaries were predicted and then entity classes (Spanish test set: F,3=1=60.97; Dutch test set: F,3=1=59.52). Black and Vasilakopoulos (2002) have evaluated two approaches to the shared task. The first was a transformation-based method which generated in rules in a single pass rather than in many passes. The second method was a decision tree method. They found that the transformation-based method consistently outperformed the decision trees (Spanish test set: F,3=1=67.49; Dutch test set: F,3=1=56.43) Tsukamoto, Mitsuishi and Sassano (2002) used a stacked AdaBoost classifier for finding named entities. They found that cascading classifiers helped improved performance. Their final system consisted of a cascade of five learners each o</context>
</contexts>
<marker>Black, Vasilakopoulos, 2002</marker>
<rawString>William J. Black and Argyrios Vasilakopoulos. 2002. Language-independent named entity classification by modified transformation-based learning and by decision tree induction. In Proceedings of CoNLL2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John C Henderson</author>
<author>William T Morgan</author>
</authors>
<title>Statistical named entity recognizer adaptation.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Burger, Henderson, Morgan, 2002</marker>
<rawString>John D. Burger, John C. Henderson, and William T. Morgan. 2002. Statistical named entity recognizer adaptation. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marques</author>
<author>Lluis Padro</author>
</authors>
<title>Named entity extraction using adaboost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Carreras, Marques, Padro, 2002</marker>
<rawString>Xavier Carreras, Lluis Marques, and Lluis Padro. 2002. Named entity extraction using adaboost. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>Erica Brown</author>
<author>Lisa Ferro</author>
<author>Patty Robinson</author>
</authors>
<title>Named Entity Recognition Task Definition. MITRE and SAIC.</title>
<date>1999</date>
<contexts>
<context position="4193" citStr="Chinchor et al., 1999" startWordPosition="703" endWordPosition="706">utation (CLiC3) of the University of Barcelona (UB), and funded by the European Commission through the NAMIC project (IST1999-12392). The data contains words and entity tags only. The training, development and test data files contain 273037, 54837 and 53049 lines respectively. The Dutch data consist of four editions of the Belgian newspaper &amp;quot;De Morgen&amp;quot; of 2000 (June 2, July 1, August 1 and September 1). The data was annotated as a part of the Atranos project4 at the University of Antwerp in Belgium, Europe. The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al., 1999) as well as possible. The data consists of words, entity tags and partof-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al., 1996). Additionally the article boundaries in the text have been marked explicitly with lines containing the tag -DOCSTART-. The training, development and test data files contain 218737, 40656 and 74189 lines respectively. The performance in this task is measured with F,3=1 rate which is equal to (32+1)*precision*recall / (&apos;32*precision+recall) with 3=1 (van Rijsbergen, 1975). Precision is the percentage of named entities found by the </context>
</contexts>
<marker>Chinchor, Brown, Ferro, Robinson, 1999</marker>
<rawString>Nancy Chinchor, Erica Brown, Lisa Ferro, and Patty Robinson. 1999. 1999 Named Entity Recognition Task Definition. MITRE and SAIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Language independent ner using a unified model of internal and contextual evidence.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="9343" citStr="Cucerzan and Yarowsky (2002)" startWordPosition="1496" endWordPosition="1499">5.78; Dutch test set: F,3=1=70.67). Burger, Henderson and Morgan (2002) have evaluated three approaches to finding named entities. They started with a baseline system which consisted of an HMM-based phrase tagger. They gave the tagger access to a list of approximately 250,000 named entities and the performance improved. After this several smoothed word classes derived from the available data were incorporated into the training process. The system performed better with the derived word lists than with the external named entity lists (Spanish test set: F,3=1=75.78; Dutch test set: F,3=1=72.57). Cucerzan and Yarowsky (2002) approached the shared task by using word-internal and contextual information stored in character-based tries. Their system obtained good results by using part-of-speech tag information and employing the one sense per discourse principle. The authors expect a performance increase when the system has access to external entity lists but have not presented the results of this in detail (Spanish test set: F,3=1=77.15; Dutch test set: F/3=1=72.31). Wu, Ngai, Carpuat, Larsen and Yang (2002) have applied AdaBoost.MH to the shared task data and compared the performance with that of a maximum entropy-b</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 2002. Language independent ner using a unified model of internal and contextual evidence. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gillis</author>
</authors>
<title>Mbt: A memory-based part of speech tagger-generator.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="4359" citStr="Daelemans et al., 1996" startWordPosition="731" endWordPosition="734">ity tags only. The training, development and test data files contain 273037, 54837 and 53049 lines respectively. The Dutch data consist of four editions of the Belgian newspaper &amp;quot;De Morgen&amp;quot; of 2000 (June 2, July 1, August 1 and September 1). The data was annotated as a part of the Atranos project4 at the University of Antwerp in Belgium, Europe. The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al., 1999) as well as possible. The data consists of words, entity tags and partof-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al., 1996). Additionally the article boundaries in the text have been marked explicitly with lines containing the tag -DOCSTART-. The training, development and test data files contain 218737, 40656 and 74189 lines respectively. The performance in this task is measured with F,3=1 rate which is equal to (32+1)*precision*recall / (&apos;32*precision+recall) with 3=1 (van Rijsbergen, 1975). Precision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it </context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. 1996. Mbt: A memory-based part of speech tagger-generator. In Proceedings of the Fourth Workshop on Very Large Corpora, pages 14{27. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
</authors>
<title>Named entity recognition as a house of cards: Classifier stacking.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="10426" citStr="Florian (2002)" startWordPosition="1657" endWordPosition="1658">rsen and Yang (2002) have applied AdaBoost.MH to the shared task data and compared the performance with that of a maximum entropy-based named entity tagger. Their system used lexical and part-of-speech information, contextual and word-internal clues, capitalization information, knowledge about entity classes of previous occurrences of words and a small external list of named entity words. The boosting techniques operated on decision stumps, decision trees of depth one. They outperformed the maximum entropy-based named entity tagger (Spanish test set: F,3=1=76.61; Dutch test set: F,3=1=75.36). Florian (2002) employed three stacked learners for named entity recognition: transformation-based learning for obtaining base-level non-typed named entities, Snow for improving the quality of these entities and the forward-backward algorithm for finding categories for the named entities. The combination of the three algorithms showed a substantially improved performance when compared with a single algorithm and an algorithm pair (Spanish test set: F,3=1=79.05; Dutch test set: F,3=1=74.99). Carreras, Marquez and Padro (2002) have approached the shared task by using AdaBoost applied to fixed-depth decision tr</context>
</contexts>
<marker>Florian, 2002</marker>
<rawString>Radu Florian. 2002. Named entity recognition as a house of cards: Classifier stacking. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jansche</author>
</authors>
<title>Named entity extraction with conditional markov models and classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="7288" citStr="Jansche (2002)" startWordPosition="1187" endWordPosition="1188"> F,3=1=71.49; Dutch test set: F,3=1=60.93) Malouf (2002) tested different models with the shared task data: a statistical baseline model, a Hidden Markov Model and maximum entropy models with different features. The latter proved to perform best. The maximum entropy models benefited from extra feature which encoded capitalization information, positional information and information about the current word being part of a person name earlier in the text. However, incorporating a list of person names in the training process did not help (Spanish test set: F,3=1=73.66; Dutch test set: F,3=1=68.08) Jansche (2002) employed a first-order Markov model as a named entity recognizer. His system used two separate passes, one for extracting entity boundaries and one for classifying entities. He evaluated different features in both subprocesses. The categorization process was trained separately from the extraction process but that did not seem to have harmed overall performance (Spanish test set: F,3=1=73.89; Dutch test set: F,3=1=69.68) Patrick, Whitelaw and Munro (2002) present SLINERC, a language-independent named entity recognizer. The system uses tries as well as character n-grams for encoding word-intern</context>
</contexts>
<marker>Jansche, 2002</marker>
<rawString>Martin Jansche. 2002. Named entity extraction with conditional markov models and classifiers. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>Markov models for languageindependent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="6730" citStr="Malouf (2002)" startWordPosition="1101" endWordPosition="1102">enerated in rules in a single pass rather than in many passes. The second method was a decision tree method. They found that the transformation-based method consistently outperformed the decision trees (Spanish test set: F,3=1=67.49; Dutch test set: F,3=1=56.43) Tsukamoto, Mitsuishi and Sassano (2002) used a stacked AdaBoost classifier for finding named entities. They found that cascading classifiers helped improved performance. Their final system consisted of a cascade of five learners each of which performed 10,000 boosting rounds (Spanish test set: F,3=1=71.49; Dutch test set: F,3=1=60.93) Malouf (2002) tested different models with the shared task data: a statistical baseline model, a Hidden Markov Model and maximum entropy models with different features. The latter proved to perform best. The maximum entropy models benefited from extra feature which encoded capitalization information, positional information and information about the current word being part of a person name earlier in the text. However, incorporating a list of person names in the training process did not help (Spanish test set: F,3=1=73.66; Dutch test set: F,3=1=68.08) Jansche (2002) employed a first-order Markov model as a </context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. Markov models for languageindependent named entity recognition. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>James Mayfield</author>
</authors>
<title>Entity extraction without language-specific resources.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5602" citStr="McNamee and Mayfield (2002)" startWordPosition="930" endWordPosition="933">h of the corresponding entity in the data file. 2http://www.talp.upc.es/ 3http://clic.fil.ub.es/ 4http://atranos.esat.kuleuven.ac.be/ 3 Results Twelve systems have participated in this shared task. The results for the test sets for Spanish and Dutch can be found in Table 1. A baseline rate was computed for both sets. It was produced by a system which only identified entities which had a unique class in the training data. If a phrase was part of more than one entity, the system would select the longest one. All systems that participated in the shared task have outperformed the baseline system. McNamee and Mayfield (2002) have applied support vector machines to the data of the shared task. Their system used many binary features for representing words (almost 9000). They have evaluated different parameter settings of the system and have selected a cascaded approach in which first entity boundaries were predicted and then entity classes (Spanish test set: F,3=1=60.97; Dutch test set: F,3=1=59.52). Black and Vasilakopoulos (2002) have evaluated two approaches to the shared task. The first was a transformation-based method which generated in rules in a single pass rather than in many passes. The second method was </context>
</contexts>
<marker>McNamee, Mayfield, 2002</marker>
<rawString>Paul McNamee and James Mayfield. 2002. Entity extraction without language-specific resources. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Patrick</author>
<author>Casey Whitelaw</author>
<author>Robert Munro</author>
</authors>
<title>Slinerc: The sydney language-independent named entity recogniser and classifier.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Patrick, Whitelaw, Munro, 2002</marker>
<rawString>Jon Patrick, Casey Whitelaw, and Robert Munro. 2002. Slinerc: The sydney language-independent named entity recogniser and classifier. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3115" citStr="Ramshaw and Marcus (1995)" startWordPosition="520" endWordPosition="523"> the example sentence: Wolff B-PER , O currently O a O journalist O in O Argentina B-LOC , O played O with O Del B-PER Bosque I-PER Words tagged with O are outside of named entities. The B-XXX tag is used for the first word in a named entity of type XXX and IXXX is used for all other words in named entities of type XXX. The data contains enti&apos;The data files are available from http://lcg-www. uia.ac.be/conll2002/ner/ ties of four types: persons (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC). The tagging scheme is a variant of the IOB scheme originally put forward by Ramshaw and Marcus (1995). We assume that named entities are non-recursive and non-overlapping. In case a named entity is embedded in another named entity usually only the top level entity will be marked. The Spanish data is a collection of news wire articles made available by the Spanish EFE News Agency. The articles are from May 2000. The annotation was carried out by the TALP Research Center2 of the Technical University of Catalonia (UPC) and the Center of Language and Computation (CLiC3) of the University of Barcelona (UB), and funded by the European Commission through the NAMIC project (IST1999-12392). The data c</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora, pages 82{94. Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Memory-based obtained by the twelve participating systems on the test data sets for the two languages in the CoNLL-2002 shared task. named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="8289" citStr="Sang (2002)" startWordPosition="1338" endWordPosition="1339">3.89; Dutch test set: F,3=1=69.68) Patrick, Whitelaw and Munro (2002) present SLINERC, a language-independent named entity recognizer. The system uses tries as well as character n-grams for encoding word-internal and contextual information. Additionally, it relies on lists of entities which have been compiled from the training data. The overall system consists of six stages, three regarding entity recognition and three for entity categorization. Stages use the output of previous stages for obtaining an improved performance (Spanish test set: F,3=1=73.92; Dutch test set: F,3=1=71.36) Tjong Kim Sang (2002) has applied a memory-based learner to the data of the shared task. He used a two-stage processing strategy as well: first identifying entities and then classifying them. Apart from the base classifier, his system made use of three extra techniques for boosting performance: cascading classifiers (stacking), feature selection and system combination. Each of these techniques were shown to be useful (Spanish test set: F,3=1=75.78; Dutch test set: F,3=1=70.67). Burger, Henderson and Morgan (2002) have evaluated three approaches to finding named entities. They started with a baseline system which c</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Memory-based obtained by the twelve participating systems on the test data sets for the two languages in the CoNLL-2002 shared task. named entity recognition. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Tsukamoto</author>
<author>Yutaka Mitsuishi</author>
<author>Manabu Sassano</author>
</authors>
<title>Learning with multiple stacking for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Tsukamoto, Mitsuishi, Sassano, 2002</marker>
<rawString>Koji Tsukamoto, Yutaka Mitsuishi, and Manabu Sassano. 2002. Learning with multiple stacking for named entity recognition. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1975</date>
<journal>Information Retrieval. Buttersworth.</journal>
<marker>van Rijsbergen, 1975</marker>
<rawString>C.J. van Rijsbergen. 1975. Information Retrieval. Buttersworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Grace Ngai</author>
<author>Marine Carpuat</author>
<author>Jeppe Larsen</author>
<author>Yongsheng Yang</author>
</authors>
<title>Boosting for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Wu, Ngai, Carpuat, Larsen, Yang, 2002</marker>
<rawString>Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang. 2002. Boosting for named entity recognition. In Proceedings of CoNLL-2002. Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>