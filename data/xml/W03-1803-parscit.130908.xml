<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.99638">
Noun-Noun Compound Machine Translation:
A Feasibility Study on Shallow Processing
</title>
<author confidence="0.742113">
TakaakiTanaka
</author>
<affiliation confidence="0.589135">
Communication Science Laboratories
Nippon Telephone and Telegraph Corporation
Kyoto, Japan
</affiliation>
<email confidence="0.989936">
takaaki@cslab.kecl.ntt.co.jp
</email>
<author confidence="0.848504">
Timothy Baldwin
</author>
<affiliation confidence="0.734772">
CSLI
Stanford University
</affiliation>
<address confidence="0.832629">
Stanford, CA 94305 USA
</address>
<email confidence="0.995854">
tbaldwin@csli.stanford.edu
</email>
<sectionHeader confidence="0.993786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998125">
The translation of compound nouns is a ma-
jor issue in machine translation due to their
frequency of occurrence and high produc-
tivity. Various shallow methods have been
proposed to translate compound nouns, no-
table amongst which are memory-based
machine translation and word-to-word com-
positional machine translation. This paper
describes the results of a feasibility study
on the ability of these methods to trans-
late Japanese and English noun-noun com-
pounds.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961014285714">
Multiword expressions are problematic in machine
translation (MT) due to the idiomaticity and overgen-
eration problems (Sag et al., 2002). Idiomaticity is
the problem of compositional semantic unpredictabil-
ity and/or syntactic markedness, as seen in expres-
sions such as kick the bucket (= die) and by and large,
respectively. Overgeneration occurs as a result of a
system failing to capture idiosyncratic lexical affini-
ties between words, such as the blocking of seemingly
equivalent word combinations (e.g. many thanks vs.
*several thanks). In this paper, we target the particu-
lar task of the Japanese English machine translation
of noun-noun compounds to outline the various tech-
niques that have been proposed to tackle idiomaticity
and overgeneration, and carry out detailed analysis of
their viability over naturally-occurring data.
Noun-noun (NN) compounds (e.g. web server, car
park) characteristically occur with high frequency and
high lexical and semantic variability. A summary ex-
amination of the 90m-word written component of the
British National Corpus (BNC, Burnard (2000)) un-
earthed over 400,000 NN compound types, with a
combined token frequency of 1.3m;1 that is, over 1%
of words in the BNC are NN compounds. More-
over, if we plot the relative token coverage of the
most frequently-occurring NN compound types, we
find that the low-frequency types account for a sig-
nificant proportion of the type count (see Figure 12).
To achieve 50% token coverage, e.g., we require cov-
erage of the top 5% most-frequent NN compounds,
amounting to roughly 70,000 types with a minimum
token frequency of 10. NN compounds are especially
prevalent in technical domains, often with idiosyn-
cratic semantics: Tanaka and Matsuo (1999) found
that NN compounds accounted for almost 20% of en-
tries in a Japanese-English financial terminological
dictionary.
Various claims have been made about the level of
processing complexity required to translate NN com-
pounds, and proposed translation methods range over
a broad spectrum of processing complexity. There is
a clear division between the proposed methods based
on whether they attempt to interpret the semantics of
the NN compound (i.e. use deep processing), or sim-
ply use the source language word forms to carry out
the translation task (i.e. use shallow processing). It is
not hard to find examples of semantic mismatch in NN
compounds to motivate deep translation methods: the
Japanese idobatakaigi “(lit.) well-side
meeting”,3 e.g., translates most naturally into English
as “idle gossip”, which a shallow method would be
hard put to predict. Our interest is in the relative oc-
currence of such NN compounds and their impact on
the performance of shallow translation methods. In
particular, we seek to determine what proportion of
NN compounds shallow translation translation meth-
ods can reasonably translate and answer the question:
do shallow methods perform well enough to preclude
the need for deep processing? The answer to this
question takes the form of an estimation of the upper
bound on translation performance for shallow transla-
tion methods.
In order to answer this question, we have selected
the language pair of English and Japanese, due to
the high linguistic disparity between the two lan-
guages. We consider the tasks of both English-to-
Japanese (EJ) and Japanese-to-English (JE) NN com-
pound translation over fixed datasets of NN com-
pounds, and apply representative shallow MT meth-
ods to the data.
</bodyText>
<footnote confidence="0.996071833333333">
2The graph for Japanese NN compounds based on the
Mainichi Corpus is almost identical.
3With all Japanese NN compound examples, we explicitly
segment the compound into its component nouns through the use
of the “” symbol.
1Results based on the method described in 3.1.
</footnote>
<figure confidence="0.964772">
1
0.8
0.6
0.4
0.2
0 0.2 0.4 0.6 0.8 1
Type coverage
</figure>
<figureCaption confidence="0.999989">
Figure 1: Type vs. token coverage (English)
</figureCaption>
<bodyText confidence="0.999982055555556">
While stating that English and Japanese are highly
linguistically differentiated, we recognise that there
are strong syntactic parallels between the two lan-
guages with respect to the compound noun construc-
tion. At the same time, there are large volumes of sub-
tle lexical and expressional divergences between the
two languages, as evidenced between
jiteNshaseNshu “(lit.) bicycle athelete” and its trans-
lation competitive cyclist. In this sense, we claim that
English and Japanese are representative of the inher-
ent difficulty of NN compound translation.
The remainder of this paper is structured as follows.
In 2, we outline the basic MT strategies that exist
for translating NN compounds, and in 3 we describe
the method by which we evaluate each method. We
then present the results in 4, and analyse the results
and suggest an extension to the basic method in 5.
Finally, we conclude in 6
</bodyText>
<sectionHeader confidence="0.976313" genericHeader="method">
2 Methods for translating NN compounds
</sectionHeader>
<bodyText confidence="0.9986288">
Two basic paradigms exist for translating NN com-
pounds: memory-based machine translation and dy-
namic machine translation. Below, we discuss these
two paradigms in turn and representative instantia-
tions of each.
</bodyText>
<subsectionHeader confidence="0.997235">
2.1 Memory-based machine translation
</subsectionHeader>
<bodyText confidence="0.999077297297297">
Memory-based machine translation (MBMT) is a
simple and commonly-used method for translating
NN compounds, whereby translation pairs are stored
in a static translation database indexed by their
source language strings. MBMT has the ability to
produce consistent, high-quality translations (condi-
tioned on the quality of the original bilingual dictio-
nary) and is therefore suited to translating compounds
in closed domains. Its most obvious drawback is that
the method can translate only those source language
strings contained in the translation database.
There are a number of ways to populate the transla-
tion database used in MBMT, the easiest of which is
to take translation pairs directly from a bilingual dic-
tionary (dictionary-driven MBMT or MBMTDICT).
MBMTDICT offers an extremist solution to the id-
iomaticity problem, in treating all NN compounds as
being fully lexicalised. Overgeneration is not an issue,
as all translations are manually determined.
As an alternative to a precompiled bilingual dic-
tionary, translation pairs can be extracted from a
parallel corpus (Fung, 1995; Smadja et al., 1996;
Ohmori and Higashida, 1999), that is a bilingual doc-
ument set that is translation-equivalent at the sentence
or paragraph level; we term this MT configuration
alignment-driven MBMT (or MBMTALIGN). While
this method alleviates the problem of limited scalabil-
ity, it relies on the existence of a parallel corpus in
the desired domain, which is often an unreasonable
requirement.
Whereas a parallel corpus assumes translation
equivalence, a comparable corpus is simply a
crosslingual pairing of corpora from the same domain
(Fung and McKeown, 1997; Rapp, 1999; Tanaka and
Matsuo, 1999; Tanaka, 2002). It is possible to extract
translation pairs from a comparable corpus by way of
the following process (Cao and Li, 2002):
</bodyText>
<listItem confidence="0.990922333333333">
1. extract NN compounds from the source language
corpus by searching for NN bigrams (e.g.
kikai hoNyaku “machine translation”)
2. compositionally generate translation candidates
for each NN compound by accessing transla-
tions for each component word and slotting these
into translation templates; example JE transla-
tion templates for source Japanese string [N
N ]J are [N N ]E and [N ofN ]E, where the nu-
meric subscripts indicate word coindexation be-
tween Japanese and English (resulting in, e.g.,
machine translation and translation of machine)
3. use empirical evidence from the target language
corpus to select the most plausible translation
candidate
</listItem>
<bodyText confidence="0.993511846153846">
We term this process word-to-word compositional
MBMT (or MBMTCOMP). While the coverage of
MBMTCOMP is potentially higher than MBMTALIGN
due to the greater accessibility of corpus data, it is
limited to some degree by the coverage of the simplex
translation dictionary used in Step 2 of the translation
process. That is, only those NN compounds whose
component nouns occur in the bilingual dictionary can
be translated.
Note that both MBMTALIGN and MBMTCOMP lead
to a static translation database. MBMTCOMP is also
subject to overgeneration as a result of dynamically
generating translation candidates.
</bodyText>
<subsectionHeader confidence="0.992423">
2.2 Dynamic machine translation
</subsectionHeader>
<bodyText confidence="0.9877646">
Dynamic machine translation (DMT) is geared to-
wards translating arbitrary NN compounds. In this pa-
per, we consider two methods of dynamic translation:
word-to-word compositional DMT and interpretation-
driven DMT.
</bodyText>
<figure confidence="0.636738">
Token coverage
0
</figure>
<bodyText confidence="0.974638545454545">
Word-to-word compositional DMT (or
DMTCOMP) differs from MBMTCOMP only in
that the source NN compounds are fed directly into
the system rather than extracted out of a source
language corpus. That is, it applies Steps 2 and 3 of
the method for MBMTCOMP to an arbitrary source
language string.
Interpretation-driven DMT (or DMTINTERP) of-
fers the means to deal with NN compounds where
strict word-to-word alignment does not hold. It gen-
erally does this in two stages:
</bodyText>
<listItem confidence="0.99611625">
1. use semantics and/or pragmatics to carry out
deep analysis of the source NN compound,
and map it into some intermediate (i.e. inter-
lingual) semantic representation (Copestake and
Lascarides, 1997; Barker and Szpakowicz, 1998;
Rosario and Hearst, 2001)
2. generate the translation directly from the seman-
tic representation
</listItem>
<bodyText confidence="0.979806269230769">
DMTINTERP removes any direct source/target lan-
guage interdependence, and hence solves the prob-
lem of overgeneration due to crosslingual bias. At the
same time, it is forced into tackling idiomaticity head-
on, by way of interpreting each individual NN com-
pound. As for DMTCOMP, DMTINTERP suffers from
undergeneration.
With DMTINTERP, context must often be called
upon in interpreting NN compounds (e.g. apple
juice seat (Levi, 1978; Bauer, 1979)), and minimal
pairs with sharply-differentiated semantics such as
colour/group photograph illustrate the fine-grained
distinctions that must be made. It is interesting to note
that, while these examples are difficult to interpret, in
an MT context, they can all be translated word-to-
word compositionally into Japanese. That is, apple
juice seat translates most naturally as
appurujuusunoseki “apple-juice seat”,4
which retains the same scope for interpretation as
its English counterpart; similarly, colour photograph
translates trivially as karaashashiN
“colour photograph” and group photograph as
daNtaishashiN “group photograph”. In these
cases, therefore, DMTINTERP offers no advantage over
DMTCOMP, while incurring a sizeable cost in produc-
ing a full semantic interpretation.
</bodyText>
<sectionHeader confidence="0.998559" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.981030888888889">
We selected the tasks of Japanese-to-English and
English-to-Japanese NN compound MT for evalua-
tion, and tested MBMTDICT and DMTCOMP on each
task. Note that we do not evaluate MBMTALIGN as
results would have been too heavily conditioned on
the makeup of the parallel corpus and the particular
alignment method adopted. Below, we describe the
data and method used in evaluation.
4Here, no is the genitive marker.
</bodyText>
<subsectionHeader confidence="0.998275">
3.1 Testdata
</subsectionHeader>
<bodyText confidence="0.999975980392157">
In order to generate English and Japanese NN com-
pound testdata, we first extracted out all NN bigrams
from the BNC (90m word tokens, Burnard (2000))
and 1996 Mainichi Shimbun Corpus (32m word to-
kens, Mainichi Newspaper Co. (1996)), respectively.
The BNC had been tagged and chunked using fnTBL
(Ngai and Florian, 2001), and lemmatised using
morph (Minnen et al., 2001), while the Mainichi
Shimbun had been segmented and tagged using ALT-
JAWS.5 For both English and Japanese, we took only
those NN bigrams adjoined by non-nouns to ensure
that they were not part of a larger compound nomi-
nal. In the case of English, we additionally measured
the entropy of the left and right contexts for each NN
type, and filtered out all compounds where either en-
tropy value was .6 This was done in an attempt
to, once again, exclude NNs which were embedded
in larger MWEs, such as service department in social
service department.
We next extracted out the 250 most common NN
compounds from the English and Japanese data, and
from the remaining data, randomly selected a further
250 NN compounds of frequency 10 or greater (out
of 20,748 English and 169,899 Japanese NN com-
pounds). In this way, we generated a total of 500
NN compounds for each of English and Japanese. For
the Japanese NN compounds, any errors in segmenta-
tion were post-corrected. Note that the top-250 NN
compounds accounted for about 7.0% and 3.3% of
the total token occurrences of English and Japanese
NN compounds, respectively; for the random sample
of 250 NN compounds, the relative occurrence of the
English and Japanese compounds out of the total to-
ken sample was 0.5% and 0.1%, respectively.
We next generated a unique gold-standard transla-
tion for each of the English and Japanese NN com-
pounds. In order to reduce the manual translation
overhead and maintain consistency with the output of
MBMTDICT in evaluation, we first tried to translate
each English and Japanese NN compound automati-
cally by MBMTDICT. In this, we used the union of two
Japanese-English dictionaries: the ALTDIC dictio-
nary and the on-line EDICT dictionary (Breen, 1995).
The ALTDIC dictionary was compiled from the ALT-
J/E MT system (Ikehara et al., 1991), and has approx-
imately 400,000 entries including more than 200,000
proper nouns; EDICT has approximately 150,000 en-
tries. In the case that multiple translation candidates
were found for a given NN compound, the most ap-
propriate of these was selected manually, or in the
case that the dictionary translations were considered
</bodyText>
<footnote confidence="0.989906857142857">
5http://www.kecl.ntt.co.jp/icl/mtg/resources/altjaws.
html
6For the left token entropy, if the most-probable left context
was the, a or a sentence boundary, the threshold was switched
off. Similarly for the right token entropy, if the most-probable
right context was a punctuation mark or sentence boundary, the
threshold was switched off.
</footnote>
<equation confidence="0.895205818181818">
Templates (JE) Examples #
[N N ]J [N N ]E shijoukeizai “market economy” 83
[N N ]J [Adj N ]E iryoukikaN “medical institution” 71
[N N ]J [N Np ]E chousakekka “survey results” 14
[N N ]J [N of (the) N ]E seikeNkoutai “change of government” 11
[N N ]J [N of (the) Np ]E ikeNkoukaN “exchange of ideas” 8
[N N ]J [Adj Np ]E keizaiseisai “economic sanctions” 8
Templates (EJ) Examples #
[N N ]E [N N ]J exchange rate “kawasereeto” 192
[N N ]E [N no N ]J hotel room “hoterunoheya” 20
[N N ]E [N N ]J carbon dioxide “nisaNkataNso” 1
</equation>
<tableCaption confidence="0.99629">
Table 1: Example translation templates (N = noun (base), Np = noun (plural), and Adj = adjective)
</tableCaption>
<bodyText confidence="0.99984">
to be sub-optimal or inappropriate, the NN compound
was put aside for manual translation. Finally, all
dictionary-based translations were manually checked
for accuracy.
The residue of NN compounds for which a trans-
lation was not found were translated manually. Note
that as we manually check all translations, the accu-
racy of MBMTDICT is less than 100%. At the same
time, we give MBMTDICT full credit in evaluation for
containing an optimal translation, by virtue of using
the dictionaries as our primary source of translations.
</bodyText>
<subsectionHeader confidence="0.997079">
3.2 Upper bound accuracy-based evaluation
</subsectionHeader>
<bodyText confidence="0.997848238095238">
We use the testdata to evaluate MBMTDICT and
DMTCOMP. Both methods potentially produce mul-
tiple translations candidates for a given input, from
which a unique translation output must be selected in
some way. So as to establish an upper bound on the
feasibility of each method, we focus on the transla-
tion candidate generation step in this paper and leave
the second step of translation selection as an item for
further research.
With MBMTDICT, we calculate the upper bound
by simply checking for the gold-standard translation
within the translation candidates. In the case of
DMTCOMP, rather than generating all translation can-
didates and checking among them, we take a pre-
determined set of translation templates and a sim-
plex translation dictionary to test for word align-
ment. Word alignment is considered to have been
achieved if there exists a translation template and
set of word translations which lead to an isomor-
phic mapping onto the gold-standard translation. For
ryoudomoNdai “territorial dispute”, for
example, alignment is achieved through the word-
level translations ryoudo “territory” and
moNdai “dispute”, and the mapping conforms to the
[N N ]J [Adj N ]E translation template. It is thus
possible to translate by way of DMTCOMP.
Note here that derivational morphology is used to con-
vert the nominal translation of territory into the adjec-
tive territorial.
On the first word-alignment pass for DMTCOMP,
the translation pairs in each dataset were automati-
cally aligned using only ALTDIC. We then manual
inspected the unaligned translation pairs for transla-
tion pairs which were not aligned simply because of
patchy coverage in ALTDIC. In such cases, we manu-
ally supplemented ALTDIC with simplex translation
pairs taken from the Genius Japanese-English dic-
tionary (Konishi, 1997),7 resulting in an additional
178 simplex entries. We then performed a second
pass of alignment using the supplemented ALTDIC
(ALTDIC ). Below, we present the results for both
the original ALTDIC and ALTDIC .
</bodyText>
<subsectionHeader confidence="0.999329">
3.3 Learning translation templates
</subsectionHeader>
<bodyText confidence="0.999427357142857">
DMTCOMP relies on translation templates to map the
source language NN compound onto different con-
structions in the target language and generate trans-
lation candidates. For the JE task, the question of
what templates are used becomes particularly salient
due to the syntactic diversity of the gold standard En-
glish translations (see below). Rather than assuming
a manually-specified template set for the EJ and JE
NN compound translation tasks, we learn the tem-
plates from NN compound translation data. Given that
the EJ and JE testdata is partitioned equally into the
top-250 and random-250 NN compounds, we cross-
validate the translation templates. That is, we perform
two iterations over each of the JE and EJ datasets, tak-
ing one dataset of 250 NN compounds as the test set
and the remaining dataset as the training set in each
case. We first perform word-alignment on the train-
ing dataset, and in the case that both source language
nouns align leaving only closed-class function words
in the target language, extract out the mapping schema
as a translation template (with word coindices). We
then use this extracted set of translation templates as
a filter in analysing word alignment in the test set.
A total of 23 JE and 3 EJ translation templates were
learned from the training data in each case, a sample
of which are shown in Table 1.8 Here, the count for
each template is the combined number of activations
over each combined dataset of 500 compounds.
</bodyText>
<footnote confidence="0.9846375">
7The reason that we used Genius here is that, as an edited
dictionary, Genius has a more complete coverage of translations
for simplex words.
8For the 3 EJ templates learned on each iteration, there was an
intersection of 2, and for the 23 JE templates, the intersection was
only 10.
</footnote>
<table confidence="0.99930725">
TOP 250 RAND 250 TOTAL F
Cov Acc F Cov Acc F Cov Acc
JE 83.6 93.8 88.4 27.2 82.4 40.9 55.4 91.0 68.9
EJ 94.4 94.5 94.5 60.0 91.3 72.4 77.2 93.3 84.5
</table>
<tableCaption confidence="0.971227">
Table 2: Results for MBMTDICT (F = F-score)
</tableCaption>
<subsectionHeader confidence="0.918453">
3.4 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.999987285714286">
The principal evaluatory axes we consider in compar-
ing the different methods are coverage and accuracy:
coverage is the relative proportion of a given set of
NN compounds that the method can generate some
translation for, and accuracy describes the propor-
tion of translated NN compounds for which the gold-
standard translation is reproduced (irrespective of how
many other translations are generated). These two
tend to be in direct competition, in that more accurate
methods tend to have lower coverage, and conversely
higher coverage methods tend to have lower accuracy.
So as to make cross-system comparison simple, we
additionally combine these two measures into an F-
score, that is their harmonic mean.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999960333333333">
We first present the individual results for MBMTDICT
and DMTCOMP, and then discuss a cascaded system
combining the two.
</bodyText>
<subsectionHeader confidence="0.80507">
4.1 Dictionary-driven MBMT
</subsectionHeader>
<bodyText confidence="0.99997446">
The source of NN compound translations for
MBMTDICT was the combined ALTDIC and EDICT
dictionaries. Recall that this is the same dictionary
as was used in the first pass of generation of gold
standard translations (see 3.1), but that the gold-
standard translations were manually selected in the
case of multiple dictionary entries, and an alternate
translation manually generated in the case that a more
appropriate translation was considered to exist.
The results for MBMTDICT are given in Table 2,
for both translation directions. In each case, we carry
out evaluation over the 250 most-commonly occurring
NN compounds (TOP 250), the random sample of 250
NN compounds (RAND 250) and the combined 500-
element dataset (ALL).
The accuracies (Acc) are predictably high, although
slightly lower for the random-250 than the top-250.
The fact that they are below 100% indicates that the
translation dictionary is not infallible and contains
a number of sub-optimal or misleading translations.
One such example is kyuusaikikiN “relief
fund” for which the dictionary provides the unique,
highly-specialised translation lifeboat.
Coverage (Cov) is significantly lower than accu-
racy, but still respectable, particularly for the random-
250 datasets. This is a reflection of the inevitable
emphasis by lexicographers on more frequent expres-
sions, and underlines the brittleness of MBMTDICT.
An additional reason for coverage being generally
lower than accuracy is that dictionaries tend not to
contain transparently compositional compounds, an
observation which applies particularly to ALTDIC as
it was developed for use with a full MT system. Cov-
erage is markedly lower for the JE task, largely be-
cause ALTJAWS—which uses ALTDIC as its sys-
tem dictionary—tends to treat the compound nouns
in ALTDIC as single words. As we used ALTJAWS
to pre-process the corpus we extracted the Japanese
NN compounds from, a large component of the com-
pounds in the translation dictionary was excluded
from the JE data. One cause of a higher coverage for
the EJ task is that many English compounds are trans-
lated into single Japanese words (e.g. interest rate vs.
riritsu) and thus reliably recorded in bilingual
dictionaries. There are 127 single word translations in
the EJ dataset, but only 31 in the JE dataset.
In summary, MBMTDICT offers high accuracy but
mid-range coverage in translating NN compounds,
with coverage dropping off appreciably for less-
frequent compounds.
</bodyText>
<subsectionHeader confidence="0.996357">
4.2 Word-to-word composional DMT
</subsectionHeader>
<bodyText confidence="0.999222">
In order to establish an upper bound on the perfor-
mance of DMTCOMP, we word-aligned the source
language NN compounds with their translations, us-
ing the extracted translation templates as described in
3.3. The results of alignment are classified into four
mutually-exclusive classes, as detailed below:
</bodyText>
<listItem confidence="0.666116545454545">
(A) Completely aligned All component words
align according to one of the extracted translation
templates.
(B) No template The translation does not corre-
spond to a known translation template (irrespective of
whether component words align in the source com-
pound).
(C) Partially aligned Some but not all component
words align. We subclassify instances of this class
into: C1 compounds, where there are unaligned words
in both the source and target languages; C2 com-
</listItem>
<bodyText confidence="0.9287938125">
pounds, where there is an unaligned word in the
source language only; and C3 compounds where there
are unaligned words in the target language only.
(D) No alignment No component words align be-
tween the source NN compound and translation. We
subclassify D instances into: D1 compounds, where
the translation is a single word; and D2 compounds,
where no word pair aligns.
The results of alignment are shown in Table 3, for
each of the top-250, random-250 and combined 500-
element datasets. The alignment was carried out us-
ing both the basic ALTDIC and ALTDIC (ALTDIC
with 178 manually-added simplex entries). Around
40% of the data align completely using ALTDIC in
both translation directions. Importantly, DMTCOMP
is slightly more robust over the random-250 dataset
</bodyText>
<table confidence="0.991908730769231">
JAPANESE-TO-ENGLISH
ALTDIC ALTDIC All
Top Rand All Top Rand
Completely aligned (A) Total 26.4 26.0 26.2 39.6 43.6 41.6
No template (B) Total 5.2 5.2 5.2 5.2 6.0 5.6
Partially aligned (C) Total 44.0 48.8 46.4 38.4 36.4 37.4
C1 40.8 46.4 43.6 35.6 33.6 34.6
C2 3.2 2.4 2.8 2.8 2.4 2.6
C3 0.0 0.0 0.0 0.0 0.4 0.2
No alignment (D) Total 24.4 20.0 22.2 16.8 14.0 15.4
D1 5.2 2.4 3.8 5.2 2.4 3.8
D2 19.2 17.6 18.4 11.6 11.6 11.6
ENGLISH-TO-JAPANESE
ALTDIC
Top Rand All
29.6 34.4 32.0 39.2 45.6 42.4
0.4 0.4 0.4 0.4 0.8 0.6
29.2 39.2 34.2 24.8 30.8 27.8
25.2 36.8 31.0 20.8 28.4 24.6
4.0 2.4 3.2 4.0 2.4 3.2
0.0 0.0 0.0 0.0 0.0 0.0
40.8 26.0 33.4 35.6 22.8 29.2
31.2 13.2 22.2 31.2 13.2 22.2
9.6 12.8 11.2 4.4 9.6 7.0
ALTDIC
Top Rand All
</table>
<tableCaption confidence="0.95193">
Table 3: Alignment-based results for DMTCOMP
</tableCaption>
<table confidence="0.9998348">
JE EJ
Cov Acc F-score Cov Acc F-score
MBMTDICT 55.4 91.0 68.9 77.2 93.3 84.5
DMTCOMP 96.4 43.1 59.6 87.0 48.7 62.5
Cascaded 96.4 71.6 82.2 95.6 87.0 91.1
</table>
<tableCaption confidence="0.999458">
Table 4: Cascaded translation results
</tableCaption>
<bodyText confidence="0.9943565">
than top-250, in terms of both completely aligned
and partially aligned instances. This contrasts with
MBMTDICT which was found to be brittle over the
less-frequent random-250 dataset.
</bodyText>
<subsectionHeader confidence="0.999859">
4.3 Combination of MBMTDICT and DMTCOMP
</subsectionHeader>
<bodyText confidence="0.999984875">
We have demonstrated MBMTDICT to have high ac-
curacy but relatively low coverage (particularly over
lower-frequency NN compounds), and DMTCOMP to
have medium accuracy but high coverage. To com-
bine the relative strengths of the two methods, we test
a cascaded architecture, whereby we first attempt to
translate each NN compound using MBMTDICT, and
failing this, resort to DMTCOMP.
Table 4 shows the results for MBMTDICT and
DMTCOMP in isolation, and when cascaded (Cas-
cade). For both translation directions, cascading re-
sults in a sharp increase in F-score, with coverage
constantly above 95% and accuracy dropping only
marginally to just under 90% for the EJ task. The
cascaded method represents the best-achieved shallow
translation upper bound achieved in this research.
</bodyText>
<sectionHeader confidence="0.839922" genericHeader="evaluation">
5 Analysis and extensions
</sectionHeader>
<bodyText confidence="0.9953595">
In this section, we offer qualitative analysis of the un-
aligned translation pairs (i.e. members of classes B,
C and D in Table 3) with an eye to improving the
coverage of DMTCOMP. We make a tentative step in
this direction by suggesting one extension to the basic
DMTCOMP paradigm based on synonym substition.
</bodyText>
<subsectionHeader confidence="0.998711">
5.1 Analysis of unaligned translation pairs
</subsectionHeader>
<bodyText confidence="0.935996882352941">
We consider there to be 6 basic types of misalignment
in the translation pairs, each of which we illustrate
with examples (in which underlined words are aligned
and boldface words are the focus of discussion). In
listing each misalignment type, we indicate the corre-
sponding alignment classes in 4.2.
(a) Missing template (B) An example of misalig-
ment due to a missing template (but where all compo-
nent words align) is:
(a1) kesshoushiNshutsu “advancement to
finals”
Simply extending the coverage of translation tem-
plates would allow DMTCOMP to capture examples
such as this.
(b) Single-word translation (C2,D1) DMTCOMP
fails when the gold-standard translation is a single
word:
</bodyText>
<listItem confidence="0.895951">
(b1) jouhoukaiji “(lit.) information disclo-
sure” disclosure
(b2) shunoukaidaN “(lit.) leader meet-
ing” summit
(b3) interest rate riritsu
</listItem>
<bodyText confidence="0.999783933333333">
In (b1), the misalignment is caused by the English dis-
closure default-encoding information; a similar case
can be made for (b2), although here summit does not
align with kaidaN. DMTCOMP could potentially
cope with these given a lexical inference module inter-
facing with a semantically-rich lexicon (particularly
in the case of (b1) where translation selection at least
partially succeeds), but DMTINTERP seems the more
natural model for coping with this type of translation.
(b3) is slightly different again, in that riritsu can
be analysed as a two-character abbreviation derived
from risoku “interest” and ritsu “rate”, which
aligns fully with interest rate. Explicit abbreviation
expansion could unearth the full wordform and facili-
tate alignment.
</bodyText>
<listItem confidence="0.73658075">
(c) Synonym and association pairs (C1) This class
contains translation pairs where one or more pairs of
component nouns does not align under exact transla-
tion, but are conceptually similar:
(c1) budget deficit zaiseiakaji “finance
deficit”
(c2) kameikoku “affiliation state” mem-
ber state
</listItem>
<bodyText confidence="0.958375170731707">
In (c1), although zaisei “finance” is not an ex-
act translation of budget, they are both general finan-
cial terms. It may be possible to align such words us-
ing word similarity, which would enable DMTCOMP to
translate some component of the C1 data. In (c2), on
the other hand, kamei “affiliation” is lexically-
associated with the English membership, although
here the link becomes more tenuous.
(d) Mismatch in semantic explicitness (C1) This
translation class is essentially the same as class (b)
above, in that semantic content explicitly described
in the source NN compound is made implicit in the
translation. The only difference is that the translation
is not a single word so there is at least the potential for
word-to-word compositionality to hold:
(d1) shuuchijiseNkyo “(lit.) state-
governor election” state election
(e) Concept focus mismatch (C1-2,D2) The source
NN compound and translation express the same con-
cept differently due to a shift in semantic focus:
(e1) shuushokukatsudou “(lit.) activity
for getting new employment” job hunting.
Here, the mismatch is between the level of directed
participation in the process of finding a job. In
Japanese, katsudou “activity” describes simple
involvement, whereas hunting signifies a more goal-
oriented process.
(f) Lexical gaps (C3,D2) Members of this class
cannot be translated compositionally as they are either
non-compositional expressions or, more commonly,
there is no conventionalised way of expressing the de-
noted concept in the target language:
(f1) zokugiiN “legistors championing the
causes of selected industries”
These translation pairs pose an insurmountable obsta-
cle for DMTCOMP.
Of these types, (a), (b) and (c) are the most real-
istically achievable for DMTCOMP, which combined
account for about 20% of coverage, suggesting that
it would be worthwhile investing effort into resolving
them.
</bodyText>
<subsectionHeader confidence="0.993951">
5.2 Performance vs. translation fan-out
</subsectionHeader>
<bodyText confidence="0.999515571428572">
As mentioned in 5.1, there area number of avenues
for enhancing the performance of DMTCOMP. Here,
we propose synonym-based substitution as a means
of dealing with synonym pairs from class (c).
The basic model of word substitution can be ex-
tending simply by inserting synonym translations as
well as direct word translations into the translation
</bodyText>
<table confidence="0.999613428571429">
Configuration Cov Acc F-score Fan-out
MBMTDICT (orig) 55.4 91.0 68.9 2
DMTCOMP (orig) 96.4 43.1 59.6 74
DMTCOMP (6 TTs sim) 95.6 41.4 57.8 20
DMTCOMP (6 TTs sim) 95.6 47.1 63.1 6,577
DMTCOMP (13 TTs sim) 96.6 43.2 59.7 43
DMTCOMP (13 TTs sim) 96.6 48.1 64.1 13,911
</table>
<tableCaption confidence="0.999882">
Table 5: Performance vs. translation fan-out (JE)
</tableCaption>
<bodyText confidence="0.999054823529412">
templates. We test-run this extended method for the
JE translation task, using the Nihongo Goi-taikei the-
saurus (Ikehara et al., 1997) as the source of source
language synonyms, and ALTDIC as our translation
dictionary. The Nihongo Goi-taikei thesaurus clas-
sifies the contents of ALTDIC into 2,700 semantic
classes. We consider words occurring in the same
class to be synonyms, and add in the translations for
each. Note that we test this configuration over only
C1-type compounds due to the huge fan-out in transla-
tion candidates generated by the extended method (al-
though performance is evaluated over the full dataset,
with results for non-C1 compounds remaining con-
stant throughout).
One significant disadvantage of synonym-based
substitution is that it leads to an exponential increase
in the number of translation candidates. If we anal-
yse the complexity of simple word-based substitution
to be where is the average number of trans-
lations per word, the complexity of synonym based
substitution becomes where is the
average number of synonyms per class.
Table 5 shows the translation performance and
also translation fan-out (average number of translation
candidates) for DMTCOMP with and without synonym-
based substitution ( sim) over the top 6 and 13 trans-
lation templates (TTs). As baselines, we also present
the results for MBMTDICT (MBMTDICT (orig)) and
DMTCOMP (DMTCOMP (orig)) in their original con-
figurations (over the full 23 templates and without
synonym-substitution for DMTCOMP). From this,
the exponential translation fan-out for synonym-based
substitution is immediately evident, but accuracy can
also be seen to increase by over 4 percentage points
through the advent of synonym substitution. Indeed,
the accuracy when using synonym-substitution over
only the top 6 translation templates is greater than that
for the basic DMTCOMP method, although the number
of translation candidates is clearly greater. Note the
marked difference in fan-out for MBMTDICT vs. the
various incarnations of DMTCOMP, and that consider-
able faith is placed in the ability of translation selec-
tion with DMTCOMP.
While the large number of translation candidates
produced by synonym-substitution make translation
selection appear intractable, most candidates are
meaningless word sequences, which can easily be
filtered out based on target language corpus evi-
dence. Indeed, Tanaka (2002) successfully combines
synonym-substitution with translation selection and
achieves appreciable gains in accuracy.
</bodyText>
<sectionHeader confidence="0.864039" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999982571428571">
This paper has used the NN compound translation
task to establish performance upper bounds on shal-
low translation methods and in the process empirically
determine the relative need for deep translation meth-
ods. We focused particularly on dictionary-driven
MBMT and word-to-word compositional DMT, and
demonstrated the relative strengths of each. When
cascaded these two methods were shown to achieve
95% coverage and potentially high translation accu-
racy. As such, shallow translation methods are able
to translate the bulk of NN compound inputs success-
fully.
One question which we have tactfully avoided an-
swering is how deep translation methods perform over
the same data, and how successfully they can han-
dle the data that shallow translation fails to produce
a translation for. We leave these as items for future re-
search. Also, we have deferred the issue of translation
selection for the methods described here, and in future
work hope to compare a range of translation selection
methods using the data developed in this research.
</bodyText>
<sectionHeader confidence="0.999497" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.978430125">
This material is based upon work supported by the National Sci-
ence Foundation under Grant No. BCS-0094638 and also the
Research Collaboration between NTT Communication Science
Laboratories, Nippon Telegraph and Telephone Corporation and
CSLI, Stanford University. We would like to thank Emily Ben-
der, Francis Bond, Dan Flickinger, Stephan Oepen, Ivan Sag and
the three anonymous reviewers for their valuable input on this re-
search.
</reference>
<sectionHeader confidence="0.918859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999164152941176">
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recog-
nition of noun modifier relationships. In Proc. of the 36th An-
nual Meeting of the ACL and 17th International Conference on
Computational Linguistics (COLING/ACL-98), pages 96–102,
Montreal, Canada.
Laurie Bauer. 1979. On the need for pragmatics in the study of
nominal compounding. Journal of Pragmatics, 3:45–50.
Jim Breen. 1995. Building an electronic Japanese-English dic-
tionary. Japanese Studies Association of Australia Conference
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Yunbo Cao and Hang Li. 2002. Base noun phrase translation us-
ing Web data and the EM algorithm. In Proc. of the 19th Inter-
national Conference on Computational Linguistics (COLING
2002), Taipei, Taiwan.
Ann Copestake and Alex Lascarides. 1997. Integrating symbolic
and statistical representations: The lexicon pragmatics inter-
face. In Proc. of the 35th Annual Meeting of the ACL and
8th Conference of the EACL (ACL-EACL’97), pages 136–43,
Madrid, Spain.
Pascale Fung and Kathleen McKeown. 1997. Finding terminol-
ogy translations from non-parallel corpora. In Proc. of the
5th Annual Workshop on Very Large Corpora, pages 192–202,
Hong Kong.
Pascale Fung. 1995. A pattern matching method for finding noun
and proper noun translations from noisy parallel corpora. In
Proc. of the 33rd Annual Meeting of the ACL, pages 236–43,
Cambridge, USA.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi Nakaiwa.
1991. Toward an MT system without pre-editing – effects of
new methods in ALT-J/E–. In Proc. of the Third Machine
Translation Summit (MT Summit III), pages 101–106, Wash-
ington DC, USA.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo,
Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and
Yoshihiko Hayashi. 1997. Nihongo Goi-Taikei – A Japanese
Lexicon. Iwanami Shoten.
Tomoshichi Konishi, editor. 1997. Genius English-Japanese and
Japanese-English Dictionary CD-ROM edition. Taishukan
Publishing Co., Ltd.
Judith N. Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York, USA.
Mainichi Newspaper Co. 1996. Mainichi Shimbun CD-ROM
1996.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied
morphological processing of English. Natural Language En-
gineering, 7(3):207–23.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting of
the North American Chapter ofAssociation for Computational
Linguistics (NAACL2001), pages 40–7, Pittsburgh, USA.
Kumiko Ohmori and Masanobu Higashida. 1999. Extracting
bilingual collocations from non-aligned parallel corpora. In
Proc. of the 8th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI99), pages
88–97, Chester, UK.
Reinhard Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora. In Proc.
of the 37th Annual Meeting of the ACL, pages 1–17, College
Park, USA.
Barbara Rosario and Marti Hearst. 2001. Classifying the seman-
tic relations in noun compounds via a domain-specific lexical
hierarchy. In Proc. of the 6th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2001), Pitts-
burgh, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain in
the neck for NLP. In Proc. of the 3rd International Conference
on Intelligent Text Processing and Computational Linguistics
(CICLing-2002), pages 1–15, Mexico City, Mexico.
Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivas-
siloglou. 1996. Translating collocations for bilingual lex-
icons: A statistical approach. Computational Linguistics,
22(1):1–38.
Takaaki Tanaka and Yoshihiro Matsuo. 1999. Extraction of trans-
lation equivalents from non-parallel corpora. In Proc. of the
8th International Conference on Theoretical and Methodolog-
ical Issues in Machine Translation (TMI-99), pages 109–19,
Chester, UK.
Takaaki Tanaka. 2002. Measuring the similarity between com-
pound nouns in different languages using non-parallel corpora.
In Proc. of the 19th International Conference on Computa-
tional Linguistics (COLING 2002), pages 981–7, Taipei, Tai-
wan.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.145771">
<title confidence="0.95609525">Noun-Noun Compound Machine A Feasibility Study on Shallow Processing Communication Science Nippon Telephone and Telegraph</title>
<author confidence="0.232238">Kyoto</author>
<email confidence="0.80626">takaaki@cslab.kecl.ntt.co.jp</email>
<author confidence="0.841428">Timothy</author>
<affiliation confidence="0.866143">Stanford</affiliation>
<address confidence="0.997295">Stanford, CA 94305</address>
<email confidence="0.999703">tbaldwin@csli.stanford.edu</email>
<abstract confidence="0.990781923076923">The translation of compound nouns is a major issue in machine translation due to their frequency of occurrence and high productivity. Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation. This paper describes the results of a feasibility study on the ability of these methods to translate Japanese and English noun-noun compounds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Stanford University CSLI</author>
</authors>
<title>This material is based upon work supported by the National Science Foundation under Grant No.</title>
<booktitle>BCS-0094638 and also the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation</booktitle>
<marker>CSLI, </marker>
<rawString>This material is based upon work supported by the National Science Foundation under Grant No. BCS-0094638 and also the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University. We would like to thank Emily Bender, Francis Bond, Dan Flickinger, Stephan Oepen, Ivan Sag and the three anonymous reviewers for their valuable input on this research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semi-automatic recognition of noun modifier relationships.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics (COLING/ACL-98),</booktitle>
<pages>96--102</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="9826" citStr="Barker and Szpakowicz, 1998" startWordPosition="1515" endWordPosition="1518"> only in that the source NN compounds are fed directly into the system rather than extracted out of a source language corpus. That is, it applies Steps 2 and 3 of the method for MBMTCOMP to an arbitrary source language string. Interpretation-driven DMT (or DMTINTERP) offers the means to deal with NN compounds where strict word-to-word alignment does not hold. It generally does this in two stages: 1. use semantics and/or pragmatics to carry out deep analysis of the source NN compound, and map it into some intermediate (i.e. interlingual) semantic representation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP, DMTINTERP suffers from undergeneration. With DMTINTERP, context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with sharply-differentiated semantics </context>
</contexts>
<marker>Barker, Szpakowicz, 1998</marker>
<rawString>Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recognition of noun modifier relationships. In Proc. of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics (COLING/ACL-98), pages 96–102, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurie Bauer</author>
</authors>
<title>On the need for pragmatics in the study of nominal compounding.</title>
<date>1979</date>
<journal>Journal of Pragmatics,</journal>
<pages>3--45</pages>
<contexts>
<context position="10367" citStr="Bauer, 1979" startWordPosition="1599" endWordPosition="1600">esentation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP, DMTINTERP suffers from undergeneration. With DMTINTERP, context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with sharply-differentiated semantics such as colour/group photograph illustrate the fine-grained distinctions that must be made. It is interesting to note that, while these examples are difficult to interpret, in an MT context, they can all be translated word-toword compositionally into Japanese. That is, apple juice seat translates most naturally as appurujuusunoseki “apple-juice seat”,4 which retains the same scope for interpretation as its English counterpart; similarly, colour photograph translates trivially as karaashashiN “colour photograph” and group photograph as </context>
</contexts>
<marker>Bauer, 1979</marker>
<rawString>Laurie Bauer. 1979. On the need for pragmatics in the study of nominal compounding. Journal of Pragmatics, 3:45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Breen</author>
</authors>
<title>Building an electronic Japanese-English dictionary.</title>
<date>1995</date>
<booktitle>Japanese Studies Association of Australia Conference</booktitle>
<contexts>
<context position="13688" citStr="Breen, 1995" startWordPosition="2129" endWordPosition="2130">he random sample of 250 NN compounds, the relative occurrence of the English and Japanese compounds out of the total token sample was 0.5% and 0.1%, respectively. We next generated a unique gold-standard translation for each of the English and Japanese NN compounds. In order to reduce the manual translation overhead and maintain consistency with the output of MBMTDICT in evaluation, we first tried to translate each English and Japanese NN compound automatically by MBMTDICT. In this, we used the union of two Japanese-English dictionaries: the ALTDIC dictionary and the on-line EDICT dictionary (Breen, 1995). The ALTDIC dictionary was compiled from the ALTJ/E MT system (Ikehara et al., 1991), and has approximately 400,000 entries including more than 200,000 proper nouns; EDICT has approximately 150,000 entries. In the case that multiple translation candidates were found for a given NN compound, the most appropriate of these was selected manually, or in the case that the dictionary translations were considered 5http://www.kecl.ntt.co.jp/icl/mtg/resources/altjaws. html 6For the left token entropy, if the most-probable left context was the, a or a sentence boundary, the threshold was switched off. S</context>
</contexts>
<marker>Breen, 1995</marker>
<rawString>Jim Breen. 1995. Building an electronic Japanese-English dictionary. Japanese Studies Association of Australia Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User Reference Guide for the British National Corpus.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="1875" citStr="Burnard (2000)" startWordPosition="265" endWordPosition="266">y equivalent word combinations (e.g. many thanks vs. *several thanks). In this paper, we target the particular task of the Japanese English machine translation of noun-noun compounds to outline the various techniques that have been proposed to tackle idiomaticity and overgeneration, and carry out detailed analysis of their viability over naturally-occurring data. Noun-noun (NN) compounds (e.g. web server, car park) characteristically occur with high frequency and high lexical and semantic variability. A summary examination of the 90m-word written component of the British National Corpus (BNC, Burnard (2000)) unearthed over 400,000 NN compound types, with a combined token frequency of 1.3m;1 that is, over 1% of words in the BNC are NN compounds. Moreover, if we plot the relative token coverage of the most frequently-occurring NN compound types, we find that the low-frequency types account for a significant proportion of the type count (see Figure 12). To achieve 50% token coverage, e.g., we require coverage of the top 5% most-frequent NN compounds, amounting to roughly 70,000 types with a minimum token frequency of 10. NN compounds are especially prevalent in technical domains, often with idiosyn</context>
<context position="11734" citStr="Burnard (2000)" startWordPosition="1801" endWordPosition="1802">ull semantic interpretation. 3 Methodology We selected the tasks of Japanese-to-English and English-to-Japanese NN compound MT for evaluation, and tested MBMTDICT and DMTCOMP on each task. Note that we do not evaluate MBMTALIGN as results would have been too heavily conditioned on the makeup of the parallel corpus and the particular alignment method adopted. Below, we describe the data and method used in evaluation. 4Here, no is the genitive marker. 3.1 Testdata In order to generate English and Japanese NN compound testdata, we first extracted out all NN bigrams from the BNC (90m word tokens, Burnard (2000)) and 1996 Mainichi Shimbun Corpus (32m word tokens, Mainichi Newspaper Co. (1996)), respectively. The BNC had been tagged and chunked using fnTBL (Ngai and Florian, 2001), and lemmatised using morph (Minnen et al., 2001), while the Mainichi Shimbun had been segmented and tagged using ALTJAWS.5 For both English and Japanese, we took only those NN bigrams adjoined by non-nouns to ensure that they were not part of a larger compound nominal. In the case of English, we additionally measured the entropy of the left and right contexts for each NN type, and filtered out all compounds where either ent</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunbo Cao</author>
<author>Hang Li</author>
</authors>
<title>Base noun phrase translation using Web data and the EM algorithm.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="7616" citStr="Cao and Li, 2002" startWordPosition="1175" endWordPosition="1178">sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002): 1. extract NN compounds from the source language corpus by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”) 2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N N ]J are [N N ]E and [N ofN ]E, where the numeric subscripts indicate word coindexation between Japanese and English (resulting in, e.g., machine translation and translation of machine) 3. use empirical evidence from the target language co</context>
</contexts>
<marker>Cao, Li, 2002</marker>
<rawString>Yunbo Cao and Hang Li. 2002. Base noun phrase translation using Web data and the EM algorithm. In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Alex Lascarides</author>
</authors>
<title>Integrating symbolic and statistical representations: The lexicon pragmatics interface.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the ACL and 8th Conference of the EACL (ACL-EACL’97),</booktitle>
<pages>136--43</pages>
<location>Madrid,</location>
<contexts>
<context position="9797" citStr="Copestake and Lascarides, 1997" startWordPosition="1511" endWordPosition="1514">r DMTCOMP) differs from MBMTCOMP only in that the source NN compounds are fed directly into the system rather than extracted out of a source language corpus. That is, it applies Steps 2 and 3 of the method for MBMTCOMP to an arbitrary source language string. Interpretation-driven DMT (or DMTINTERP) offers the means to deal with NN compounds where strict word-to-word alignment does not hold. It generally does this in two stages: 1. use semantics and/or pragmatics to carry out deep analysis of the source NN compound, and map it into some intermediate (i.e. interlingual) semantic representation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP, DMTINTERP suffers from undergeneration. With DMTINTERP, context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with shar</context>
</contexts>
<marker>Copestake, Lascarides, 1997</marker>
<rawString>Ann Copestake and Alex Lascarides. 1997. Integrating symbolic and statistical representations: The lexicon pragmatics interface. In Proc. of the 35th Annual Meeting of the ACL and 8th Conference of the EACL (ACL-EACL’97), pages 136–43, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen McKeown</author>
</authors>
<title>Finding terminology translations from non-parallel corpora.</title>
<date>1997</date>
<booktitle>In Proc. of the 5th Annual Workshop on Very Large Corpora,</booktitle>
<pages>192--202</pages>
<location>Hong Kong.</location>
<contexts>
<context position="7443" citStr="Fung and McKeown, 1997" startWordPosition="1146" endWordPosition="1149">irs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002): 1. extract NN compounds from the source language corpus by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”) 2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N N ]J are [N N ]E and [N ofN ]E, where the numeric subscripts indic</context>
</contexts>
<marker>Fung, McKeown, 1997</marker>
<rawString>Pascale Fung and Kathleen McKeown. 1997. Finding terminology translations from non-parallel corpora. In Proc. of the 5th Annual Workshop on Very Large Corpora, pages 192–202, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A pattern matching method for finding noun and proper noun translations from noisy parallel corpora.</title>
<date>1995</date>
<booktitle>In Proc. of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>236--43</pages>
<location>Cambridge, USA.</location>
<contexts>
<context position="6876" citStr="Fung, 1995" startWordPosition="1061" endWordPosition="1062">slate only those source language strings contained in the translation database. There are a number of ways to populate the translation database used in MBMT, the easiest of which is to take translation pairs directly from a bilingual dictionary (dictionary-driven MBMT or MBMTDICT). MBMTDICT offers an extremist solution to the idiomaticity problem, in treating all NN compounds as being fully lexicalised. Overgeneration is not an issue, as all translations are manually determined. As an alternative to a precompiled bilingual dictionary, translation pairs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, </context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Pascale Fung. 1995. A pattern matching method for finding noun and proper noun translations from noisy parallel corpora. In Proc. of the 33rd Annual Meeting of the ACL, pages 236–43, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Satoshi Shirai</author>
<author>Akio Yokoo</author>
<author>Hiromi Nakaiwa</author>
</authors>
<title>Toward an MT system without pre-editing – effects of new methods in ALT-J/E–.</title>
<date>1991</date>
<booktitle>In Proc. of the Third Machine Translation Summit (MT Summit III),</booktitle>
<pages>101--106</pages>
<location>Washington DC, USA.</location>
<contexts>
<context position="13773" citStr="Ikehara et al., 1991" startWordPosition="2142" endWordPosition="2145"> and Japanese compounds out of the total token sample was 0.5% and 0.1%, respectively. We next generated a unique gold-standard translation for each of the English and Japanese NN compounds. In order to reduce the manual translation overhead and maintain consistency with the output of MBMTDICT in evaluation, we first tried to translate each English and Japanese NN compound automatically by MBMTDICT. In this, we used the union of two Japanese-English dictionaries: the ALTDIC dictionary and the on-line EDICT dictionary (Breen, 1995). The ALTDIC dictionary was compiled from the ALTJ/E MT system (Ikehara et al., 1991), and has approximately 400,000 entries including more than 200,000 proper nouns; EDICT has approximately 150,000 entries. In the case that multiple translation candidates were found for a given NN compound, the most appropriate of these was selected manually, or in the case that the dictionary translations were considered 5http://www.kecl.ntt.co.jp/icl/mtg/resources/altjaws. html 6For the left token entropy, if the most-probable left context was the, a or a sentence boundary, the threshold was switched off. Similarly for the right token entropy, if the most-probable right context was a punctu</context>
</contexts>
<marker>Ikehara, Shirai, Yokoo, Nakaiwa, 1991</marker>
<rawString>Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi Nakaiwa. 1991. Toward an MT system without pre-editing – effects of new methods in ALT-J/E–. In Proc. of the Third Machine Translation Summit (MT Summit III), pages 101–106, Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Nihongo Goi-Taikei – A Japanese Lexicon. Iwanami Shoten.</booktitle>
<contexts>
<context position="31398" citStr="Ikehara et al., 1997" startWordPosition="4989" endWordPosition="4992">irs from class (c). The basic model of word substitution can be extending simply by inserting synonym translations as well as direct word translations into the translation Configuration Cov Acc F-score Fan-out MBMTDICT (orig) 55.4 91.0 68.9 2 DMTCOMP (orig) 96.4 43.1 59.6 74 DMTCOMP (6 TTs sim) 95.6 41.4 57.8 20 DMTCOMP (6 TTs sim) 95.6 47.1 63.1 6,577 DMTCOMP (13 TTs sim) 96.6 43.2 59.7 43 DMTCOMP (13 TTs sim) 96.6 48.1 64.1 13,911 Table 5: Performance vs. translation fan-out (JE) templates. We test-run this extended method for the JE translation task, using the Nihongo Goi-taikei thesaurus (Ikehara et al., 1997) as the source of source language synonyms, and ALTDIC as our translation dictionary. The Nihongo Goi-taikei thesaurus classifies the contents of ALTDIC into 2,700 semantic classes. We consider words occurring in the same class to be synonyms, and add in the translations for each. Note that we test this configuration over only C1-type compounds due to the huge fan-out in translation candidates generated by the extended method (although performance is evaluated over the full dataset, with results for non-C1 compounds remaining constant throughout). One significant disadvantage of synonym-based </context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Nihongo Goi-Taikei – A Japanese Lexicon. Iwanami Shoten.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>Genius English-Japanese and Japanese-English Dictionary CD-ROM edition. Taishukan</booktitle>
<editor>Tomoshichi Konishi, editor.</editor>
<publisher>Publishing Co., Ltd.</publisher>
<marker>1997</marker>
<rawString>Tomoshichi Konishi, editor. 1997. Genius English-Japanese and Japanese-English Dictionary CD-ROM edition. Taishukan Publishing Co., Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith N Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="10353" citStr="Levi, 1978" startWordPosition="1597" endWordPosition="1598">emantic representation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP, DMTINTERP suffers from undergeneration. With DMTINTERP, context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with sharply-differentiated semantics such as colour/group photograph illustrate the fine-grained distinctions that must be made. It is interesting to note that, while these examples are difficult to interpret, in an MT context, they can all be translated word-toword compositionally into Japanese. That is, apple juice seat translates most naturally as appurujuusunoseki “apple-juice seat”,4 which retains the same scope for interpretation as its English counterpart; similarly, colour photograph translates trivially as karaashashiN “colour photograph” and group </context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Judith N. Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mainichi Newspaper Co</author>
</authors>
<date>1996</date>
<booktitle>Mainichi Shimbun CD-ROM</booktitle>
<marker>Co, 1996</marker>
<rawString>Mainichi Newspaper Co. 1996. Mainichi Shimbun CD-ROM 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="11955" citStr="Minnen et al., 2001" startWordPosition="1834" endWordPosition="1837">te MBMTALIGN as results would have been too heavily conditioned on the makeup of the parallel corpus and the particular alignment method adopted. Below, we describe the data and method used in evaluation. 4Here, no is the genitive marker. 3.1 Testdata In order to generate English and Japanese NN compound testdata, we first extracted out all NN bigrams from the BNC (90m word tokens, Burnard (2000)) and 1996 Mainichi Shimbun Corpus (32m word tokens, Mainichi Newspaper Co. (1996)), respectively. The BNC had been tagged and chunked using fnTBL (Ngai and Florian, 2001), and lemmatised using morph (Minnen et al., 2001), while the Mainichi Shimbun had been segmented and tagged using ALTJAWS.5 For both English and Japanese, we took only those NN bigrams adjoined by non-nouns to ensure that they were not part of a larger compound nominal. In the case of English, we additionally measured the entropy of the left and right contexts for each NN type, and filtered out all compounds where either entropy value was .6 This was done in an attempt to, once again, exclude NNs which were embedded in larger MWEs, such as service department in social service department. We next extracted out the 250 most common NN compounds</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proc. of the 2nd Annual Meeting of the North American Chapter ofAssociation for Computational Linguistics (NAACL2001),</booktitle>
<pages>40--7</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="11905" citStr="Ngai and Florian, 2001" startWordPosition="1826" endWordPosition="1829"> and DMTCOMP on each task. Note that we do not evaluate MBMTALIGN as results would have been too heavily conditioned on the makeup of the parallel corpus and the particular alignment method adopted. Below, we describe the data and method used in evaluation. 4Here, no is the genitive marker. 3.1 Testdata In order to generate English and Japanese NN compound testdata, we first extracted out all NN bigrams from the BNC (90m word tokens, Burnard (2000)) and 1996 Mainichi Shimbun Corpus (32m word tokens, Mainichi Newspaper Co. (1996)), respectively. The BNC had been tagged and chunked using fnTBL (Ngai and Florian, 2001), and lemmatised using morph (Minnen et al., 2001), while the Mainichi Shimbun had been segmented and tagged using ALTJAWS.5 For both English and Japanese, we took only those NN bigrams adjoined by non-nouns to ensure that they were not part of a larger compound nominal. In the case of English, we additionally measured the entropy of the left and right contexts for each NN type, and filtered out all compounds where either entropy value was .6 This was done in an attempt to, once again, exclude NNs which were embedded in larger MWEs, such as service department in social service department. We n</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast lane. In Proc. of the 2nd Annual Meeting of the North American Chapter ofAssociation for Computational Linguistics (NAACL2001), pages 40–7, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Ohmori</author>
<author>Masanobu Higashida</author>
</authors>
<title>Extracting bilingual collocations from non-aligned parallel corpora.</title>
<date>1999</date>
<booktitle>In Proc. of the 8th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI99),</booktitle>
<pages>88--97</pages>
<location>Chester, UK.</location>
<contexts>
<context position="6926" citStr="Ohmori and Higashida, 1999" startWordPosition="1067" endWordPosition="1070">strings contained in the translation database. There are a number of ways to populate the translation database used in MBMT, the easiest of which is to take translation pairs directly from a bilingual dictionary (dictionary-driven MBMT or MBMTDICT). MBMTDICT offers an extremist solution to the idiomaticity problem, in treating all NN compounds as being fully lexicalised. Overgeneration is not an issue, as all translations are manually determined. As an alternative to a precompiled bilingual dictionary, translation pairs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract tra</context>
</contexts>
<marker>Ohmori, Higashida, 1999</marker>
<rawString>Kumiko Ohmori and Masanobu Higashida. 1999. Extracting bilingual collocations from non-aligned parallel corpora. In Proc. of the 8th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI99), pages 88–97, Chester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Annual Meeting of the ACL,</booktitle>
<pages>1--17</pages>
<location>College Park, USA.</location>
<contexts>
<context position="7455" citStr="Rapp, 1999" startWordPosition="1150" endWordPosition="1151">m a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002): 1. extract NN compounds from the source language corpus by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”) 2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N N ]J are [N N ]E and [N ofN ]E, where the numeric subscripts indicate word coi</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proc. of the 37th Annual Meeting of the ACL, pages 1–17, College Park, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy.</title>
<date>2001</date>
<booktitle>In Proc. of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="9853" citStr="Rosario and Hearst, 2001" startWordPosition="1519" endWordPosition="1522">ompounds are fed directly into the system rather than extracted out of a source language corpus. That is, it applies Steps 2 and 3 of the method for MBMTCOMP to an arbitrary source language string. Interpretation-driven DMT (or DMTINTERP) offers the means to deal with NN compounds where strict word-to-word alignment does not hold. It generally does this in two stages: 1. use semantics and/or pragmatics to carry out deep analysis of the source NN compound, and map it into some intermediate (i.e. interlingual) semantic representation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP, DMTINTERP suffers from undergeneration. With DMTINTERP, context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with sharply-differentiated semantics such as colour/group photog</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Barbara Rosario and Marti Hearst. 2001. Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy. In Proc. of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002),</booktitle>
<pages>1--15</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="927" citStr="Sag et al., 2002" startWordPosition="122" endWordPosition="125">tract The translation of compound nouns is a major issue in machine translation due to their frequency of occurrence and high productivity. Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation. This paper describes the results of a feasibility study on the ability of these methods to translate Japanese and English noun-noun compounds. 1 Introduction Multiword expressions are problematic in machine translation (MT) due to the idiomaticity and overgeneration problems (Sag et al., 2002). Idiomaticity is the problem of compositional semantic unpredictability and/or syntactic markedness, as seen in expressions such as kick the bucket (= die) and by and large, respectively. Overgeneration occurs as a result of a system failing to capture idiosyncratic lexical affinities between words, such as the blocking of seemingly equivalent word combinations (e.g. many thanks vs. *several thanks). In this paper, we target the particular task of the Japanese English machine translation of noun-noun compounds to outline the various techniques that have been proposed to tackle idiomaticity an</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proc. of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002), pages 1–15, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6897" citStr="Smadja et al., 1996" startWordPosition="1063" endWordPosition="1066">hose source language strings contained in the translation database. There are a number of ways to populate the translation database used in MBMT, the easiest of which is to take translation pairs directly from a bilingual dictionary (dictionary-driven MBMT or MBMTDICT). MBMTDICT offers an extremist solution to the idiomaticity problem, in treating all NN compounds as being fully lexicalised. Overgeneration is not an issue, as all translations are manually determined. As an alternative to a precompiled bilingual dictionary, translation pairs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). </context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Yoshihiro Matsuo</author>
</authors>
<title>Extraction of translation equivalents from non-parallel corpora.</title>
<date>1999</date>
<booktitle>In Proc. of the 8th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-99),</booktitle>
<pages>109--19</pages>
<location>Chester, UK.</location>
<contexts>
<context position="2517" citStr="Tanaka and Matsuo (1999)" startWordPosition="370" endWordPosition="373">00,000 NN compound types, with a combined token frequency of 1.3m;1 that is, over 1% of words in the BNC are NN compounds. Moreover, if we plot the relative token coverage of the most frequently-occurring NN compound types, we find that the low-frequency types account for a significant proportion of the type count (see Figure 12). To achieve 50% token coverage, e.g., we require coverage of the top 5% most-frequent NN compounds, amounting to roughly 70,000 types with a minimum token frequency of 10. NN compounds are especially prevalent in technical domains, often with idiosyncratic semantics: Tanaka and Matsuo (1999) found that NN compounds accounted for almost 20% of entries in a Japanese-English financial terminological dictionary. Various claims have been made about the level of processing complexity required to translate NN compounds, and proposed translation methods range over a broad spectrum of processing complexity. There is a clear division between the proposed methods based on whether they attempt to interpret the semantics of the NN compound (i.e. use deep processing), or simply use the source language word forms to carry out the translation task (i.e. use shallow processing). It is not hard to</context>
<context position="7480" citStr="Tanaka and Matsuo, 1999" startWordPosition="1152" endWordPosition="1155"> corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002): 1. extract NN compounds from the source language corpus by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”) 2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N N ]J are [N N ]E and [N ofN ]E, where the numeric subscripts indicate word coindexation between Japanes</context>
</contexts>
<marker>Tanaka, Matsuo, 1999</marker>
<rawString>Takaaki Tanaka and Yoshihiro Matsuo. 1999. Extraction of translation equivalents from non-parallel corpora. In Proc. of the 8th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-99), pages 109–19, Chester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
</authors>
<title>Measuring the similarity between compound nouns in different languages using non-parallel corpora.</title>
<date>2002</date>
<contexts>
<context position="7495" citStr="Tanaka, 2002" startWordPosition="1156" endWordPosition="1157">ja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002): 1. extract NN compounds from the source language corpus by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”) 2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N N ]J are [N N ]E and [N ofN ]E, where the numeric subscripts indicate word coindexation between Japanese and English (</context>
<context position="33645" citStr="Tanaka (2002)" startWordPosition="5330" endWordPosition="5331">r only the top 6 translation templates is greater than that for the basic DMTCOMP method, although the number of translation candidates is clearly greater. Note the marked difference in fan-out for MBMTDICT vs. the various incarnations of DMTCOMP, and that considerable faith is placed in the ability of translation selection with DMTCOMP. While the large number of translation candidates produced by synonym-substitution make translation selection appear intractable, most candidates are meaningless word sequences, which can easily be filtered out based on target language corpus evidence. Indeed, Tanaka (2002) successfully combines synonym-substitution with translation selection and achieves appreciable gains in accuracy. 6 Conclusion and future work This paper has used the NN compound translation task to establish performance upper bounds on shallow translation methods and in the process empirically determine the relative need for deep translation methods. We focused particularly on dictionary-driven MBMT and word-to-word compositional DMT, and demonstrated the relative strengths of each. When cascaded these two methods were shown to achieve 95% coverage and potentially high translation accuracy. </context>
</contexts>
<marker>Tanaka, 2002</marker>
<rawString>Takaaki Tanaka. 2002. Measuring the similarity between compound nouns in different languages using non-parallel corpora.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<pages>981--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="33645" citStr="(2002)" startWordPosition="5331" endWordPosition="5331">the top 6 translation templates is greater than that for the basic DMTCOMP method, although the number of translation candidates is clearly greater. Note the marked difference in fan-out for MBMTDICT vs. the various incarnations of DMTCOMP, and that considerable faith is placed in the ability of translation selection with DMTCOMP. While the large number of translation candidates produced by synonym-substitution make translation selection appear intractable, most candidates are meaningless word sequences, which can easily be filtered out based on target language corpus evidence. Indeed, Tanaka (2002) successfully combines synonym-substitution with translation selection and achieves appreciable gains in accuracy. 6 Conclusion and future work This paper has used the NN compound translation task to establish performance upper bounds on shallow translation methods and in the process empirically determine the relative need for deep translation methods. We focused particularly on dictionary-driven MBMT and word-to-word compositional DMT, and demonstrated the relative strengths of each. When cascaded these two methods were shown to achieve 95% coverage and potentially high translation accuracy. </context>
</contexts>
<marker>2002</marker>
<rawString>In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), pages 981–7, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>