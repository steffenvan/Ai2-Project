<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000066">
<note confidence="0.869275">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<title confidence="0.7452685">
Association for Computational Linguistics
KUNLP System in SENSEVAL-3
</title>
<author confidence="0.997081">
Hee-Cheol Seo, Hae-Chang Rim
</author>
<affiliation confidence="0.960250333333333">
Dept. of Computer Science
and Engineering,
Korea University
</affiliation>
<address confidence="0.9594095">
1, 5-ka, Anam-dong, Seongbuk-Gu,
Seoul, 136-701, Korea
</address>
<email confidence="0.995815">
{hcseo, rim{@nlp.korea.ac.kr
</email>
<author confidence="0.99147">
Soo-Hong Kim
</author>
<affiliation confidence="0.885996333333333">
Dept. of Computer Software Engineering,
College of Engineering,
Sangmyung University,
</affiliation>
<address confidence="0.9669925">
San 98-20, Anso-Dong,
Chonan, Chungnam, Korea
</address>
<email confidence="0.99784">
soohkim@smuc.ac.kr
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99846275">
We have participated in both English all words
task and English lexical sample task of SENSEVAL-
3. Our system disambiguates senses of a target
word in a context by selecting a substituent among
WordNet relatives of the target word, such as syn-
onyms, hypernyms, meronyms and so on. The deci-
sion is made based on co-occurrence frequency be-
tween candidate relatives and each of the context
words. Since the co-occurrence frequency is obtain-
able from raw corpus, our method is considered to
be an unsupervised learning algorithm that does not
require a sense-tagged corpus.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998866361702128">
At SENSEVAL-3, we adopted an unsupervised ap-
proach based on WordNet and raw corpus, which
does not require any sense tagged corpus. Word-
Net specifies relationships among the meanings of
words.
Relatives of a word in WordNet are defined as
words that have a relationship with it, e.g. they
are synonyms, antonyms, superordinates (hyper-
nyms), or subordinates (hyponyms). Relatives, es-
pecially those in a synonym class, usually have
related meanings and tend to share similar con-
texts. Hence, some WordNet-based approaches ex-
tract relatives of each sense of a polysemous word
from WordNet, collect example sentences of the rel-
atives from a raw corpus, and learn the senses from
the example sentences for WSD. Yarowsky (1992)
first proposed this approach, but used International
Roget’s Thesaurus as a hierarchical lexical database
instead of WordNet. However, the approach seems
to suffer from examples irrelevant to the senses of
a polysemous word since many of the relatives are
polysemous. Leacock et al. (1998) attempted to ex-
clude irrelevant or spurious examples by using only
monosemous relatives in WordNet. However, some
senses do not have short distance monosemous rel-
atives through a relation such as synonym, child,
and parent. A possible alternative of using only
monosemous relatives in the long distance, how-
ever, is problematic because the longer the distance
of two synsets in WordNet, the weaker the relation-
ship between them. In other words, the monose-
mous relatives in the long distance may provide ir-
relevant examples for WSD.
Our approach is somewhat similar to the Word-
Net based approach of Leacock et al. (1998) in that
it acquires relatives of a target word from WordNet
and extracts co-occurrence frequencies of the rela-
tives from a raw corpus, but our system uses poly-
semous as well as monosemous relatives. To avoid
a negative effect of polysemous relatives on the co-
occurrence frequency calculation, our system han-
dles the example sentences of each relative sepa-
rately instead of putting together the example sen-
tences of all relatives into a pool. Also we devised
our system to efficiently disambiguate senses of all
words using only co-occurrence frequency between
words.
</bodyText>
<sectionHeader confidence="0.99746" genericHeader="method">
2 KUNLP system
</sectionHeader>
<subsectionHeader confidence="0.999599">
2.1 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.9661154375">
We disambiguate senses of a word in a context1
by selecting a substituent word from the WordNet2
relatives of the target word. Figure 1 represents a
flowchart of the proposed approach. Given a target
word and its context, a set of relatives of the target
word is created by searches in WordNet. Next, the
most appropriate relative that can be substituted for
the word in the context is chosen. In this step, co-
occurrence frequency is used. Finally, the sense of
the target word that is related to the selected relative
is determined.
The example in Figure 2 illustrates how the pro-
posed approach disambiguates senses of the tar-
get word chair given the context. The set of rel-
atives {president, professorship, ...} of chair is
built by WordNet searches, and the probability,
</bodyText>
<footnote confidence="0.999818666666667">
1In this paper, a context indicates a target word and six
words surrounding the target word in an instance.
2The WordNet version is 1.7.1.
</footnote>
<figureCaption confidence="0.999959">
Figure 1: Flowchart of KUNLP System
</figureCaption>
<figure confidence="0.996788235294118">
Determine
a Sense
Sense
Context
Acquire
Set of Relatives
Select
a Relative
Target Word
Context
Words
Surrounding
Target Word
WordNet
Co-occurrence
Information
Matrix
</figure>
<bodyText confidence="0.97142416">
“Pr(professorshipIContext),” that a relative can
be substituted for the target word in the given con-
text is estimated by the co-occurrence frequency be-
tween the relative and each of the context words. In
this example, the relative, seat, is selected with the
highest probability and the proper sense, “a seat for
one person, with a support for the back,” is chosen.
Thus, the second step of our system (i.e. selecting
a relative) has to be carefully implemented to select
the proper relative that can substitute for the target
word in the context, while the first step (i.e. acquir-
ing the set of relatives) and the third step (i.e. deter-
mining a sense) are done simply through searches in
WordNet.
The substituent word of the i-th target word twi
in a context C is defined to be the relative of twi
which has the largest co-occurrence probability with
the words in the context:
SW (twi, C) def = arg max P(r&apos; IC) (1)
rid
where SW is the substituent word, rig is the j-th
relative of twi, and r��� is the a-th sense related to
twi3. If a is 2, the 2-nd sense of rig is related to
twi. The right hand side of Equation 1 is calculated
with logarithm as follows:
</bodyText>
<equation confidence="0.883781444444444">
arg max P (r���IC)
rid.
P(CIr� ��)P(r� ��)
P(C)
= arg max P(CIr���)P(r&apos;
rid
ij
= arg max{logP(CIr~~) + logP(r���)1 (2)
rid
</equation>
<footnote confidence="0.948919">
3a is a function with two parameters twi and rzj, but it can
be written in brief without parameters.
</footnote>
<table confidence="0.674582952380952">
Instance :
He should sit in the chair beside the desk.
Target Word :
&apos;chair&apos;
Context :
sit in the chair beside the desk
Set of Relatives :
{professorship, president, chairman,
electronic chair, death chair, seat,
office, presiding officer, ...}
Probability of Relative given the Context :
P( professorship  |Context )
P( president  |Context )
...
P( seat  |Context )
...
Selected Relative :
&apos;seat&apos; - it is the most likely word occurred
from the above context among
the relatives of &apos;chair&apos;
Determined Sense :
</table>
<footnote confidence="0.688877666666667">
chair%1:06:00 - &amp;quot;a seat for one person,
with a support for the back.&amp;quot;
&apos;seat&apos; - the hypernym of chair%1:06:00.
</footnote>
<figureCaption confidence="0.990007">
Figure 2: Example of sense disambiguation proce-
dure for chair
</figureCaption>
<bodyText confidence="0.991972625">
= arg max
rid
Then Equation 2 may be calculated under the as-
sumption that words in C occur independently:
probabilities of finding relatives, given the con-
text. These probabilities can be estimated based on
the co-occurrence frequency between a relative and
context words as follows:
</bodyText>
<equation confidence="0.9545434">
arg max{logP(Clr&apos;) + logP(r&apos;)I
rid
� arg max II n logP(WI&apos;) +logP(r&apos;)1 (3) P (rig) = ��������� (6)
rid* l� ��
k=1
</equation>
<bodyText confidence="0.9999144">
where wk is the h-th word in C and n is the number
of words in C. In Equation 3, we assume indepen-
dence among words in C.
The first probability in Equation 3 is calculated as
follows:
</bodyText>
<equation confidence="0.995357">
P(Wkl&apos;)
ti P (WkIrig)
��������������
� (4)
P (rig)
</equation>
<bodyText confidence="0.941402">
The second probability in Equation 3 is computed
as follows:
</bodyText>
<equation confidence="0.997533">
P(&apos;) _ �(&apos;)P(rij) (5)
</equation>
<bodyText confidence="0.988888">
where Q(�� &apos;I) is the ratio of the frequency of T&apos;j&apos;j to
that of rid:
</bodyText>
<equation confidence="0.9798285">
R(&apos;) �
n * 0.5 + WNf(rij)
</equation>
<bodyText confidence="0.947119125">
where W Nf(Z9�) is the frequency of ri-j in Word-
Net, W Nf(rid) is the frequency of rid in WordNet,
0.5 is a smoothing factor, and n is the number of
senses of rid.
Applying Equations 4 and 5 to Equation 3, we
have the following equation for acquiring the rela-
tive with the largest co-occurrence probability:
arg max P(&apos; C)
</bodyText>
<equation confidence="0.9745515">
rid
�� ���������
��� � ����� + logNii)P(rij)
k=1
</equation>
<bodyText confidence="0.980376666666667">
In the case that several relatives have the largest
co-occurrence probability, all senses related to the
relatives are determined as proper senses.
</bodyText>
<subsectionHeader confidence="0.999331">
2.2 Co-occurrence Frequency Matrix
</subsectionHeader>
<bodyText confidence="0.991288">
In order to select a substituent word for a target
word in a given context, we must calculate the
</bodyText>
<equation confidence="0.9932934">
������ ���
��������� �
P(Wk)
freq(rij, WO �(7)
freq(Wk)
</equation>
<bodyText confidence="0.999602956521739">
where freq(rij) is the frequency of rid, CS is the
corpus size, P (rid, wk) is the probability that rid
and Wk co-occur, and freq(rij, wk) is the frequency
that rid and Wk co-occur.
In order to calculate these probabilities, frequen-
cies of words and word pairs are required. For this,
we build a co-occurrence frequency matrix that con-
tains co-occurrence frequencies of words pairs. In
this matrix, an element n4j represents the frequency
that the i-th word and j-th word in the vocabulary co-
occur in a corpus4. The frequency of a word can be
calculated by counting all frequencies in the same
row or column. The vocabulary is composed of all
content words in the corpus. Now, the equations 6
and 7 can be calculated with the matrix.
The matrix is easily built by counting each word
pair in a given corpus. It is not necessary to make an
individual matrix for each polysemous word, since
the matrix contains co-occurrence frequencies of all
word pairs. Hence, it is possible to disambiguate all
words with only one matrix. In other words, the pro-
posed method disambiguates the senses of all words
efficiently with only one matrix.
</bodyText>
<subsectionHeader confidence="0.999469">
2.3 WordNet Relatives
</subsectionHeader>
<bodyText confidence="0.998644272727273">
Our system used most of relationship types in Word-
Net, except sister and attribute types, to acquire
the relatives of target words. For a nominal word,
we included all hypernyms and hyponyms in dis-
tance 3 from a sense, which indicate parents, grand-
parents and great-grand parents for hypernymy and
children, grandchildren and great-children for hy-
ponymy5.
In order to identify part-of-speech (POS) of
words including target words in instances, our sys-
tem uses TreeTagger (Schmid, 1994). After POS
</bodyText>
<footnote confidence="0.9994622">
4The co-occurrence frequency matrix is a symmetric ma-
trix, thus mzj is the same as mjz.
5We implemented WordNet APIs with index files and
data files in WordNet package, which is downloadable from
http://www.cogsci.princeton.edu/ wn/.
</footnote>
<table confidence="0.61567875">
�� ��������������
���
P (rig)
k=1
+ log,Q(r,ij)P(rij)
� arg max
rid
W Nf(r�z�) + 0.5
= arg max
rid
fine grained coarse grained
recall prec. recall prec.
noun 0.451 0.451 0.556 0.556
verb(R) 0.354 0.354 0.496 0.496
adjective 0.497 0.497 0.610 0.610
overall 0.404 0.404 0.528 0.528
</table>
<tableCaption confidence="0.997168">
Table 1: Official Results: English Lexical Sample
</tableCaption>
<table confidence="0.942025333333333">
with U without U
recall prec. recall prec.
overall 0.500 0.500 0.496 0.510
</table>
<tableCaption confidence="0.8895635">
Table 2: Official Results (fine grained) : English All
Words
</tableCaption>
<bodyText confidence="0.999475">
of the target word is determined, relationship types
related to the POS are considered to acquire the can-
didate relatives of the target word. For instance, if a
target word is adverb, the following relationships of
the word are considered: synonymy, antonymy, and
derived.
</bodyText>
<subsectionHeader confidence="0.985388">
2.4 WordNet Multiword Expression
</subsectionHeader>
<bodyText confidence="0.999943">
Our system recognizes multiword expressions of
WordNet in an instance by a simple string match
before disambiguating senses of a target word. If
the instance has a multiword expression including
the target word, our system does not disambiguate
the senses of the multiword expression but just as-
signs all senses of the multiword expression to the
instance.
</bodyText>
<sectionHeader confidence="0.998585" genericHeader="method">
3 Official Results
</sectionHeader>
<bodyText confidence="0.999175277777778">
We have participated in both English lexical sample
task and English all words task. Table 1 and 2 show
the official results of our system for two tasks. Our
system disambiguates all instances, thus the cover-
age of our system is 100% and precision of our sys-
tem is the same as the recall.
Our system assigns WordNet sense key to each
instance, but verbs in English lexical sample task
are annotated based on Wordsmyth definitions. In
official submission, we did not map the WordNet
sense keys of verbs to Wordsmyth senses, thus
the recall of our system for verbs is 0%. Ta-
ble 1 shows the results after a mapping between
Wordsmyth and WordNet verb senses using the file
EnglishLS.dictionary.mapping.xml.
In English all word task, there are two additional
scoring measures in addition to fine- and coarse-
grained scoring: with U and without U6. In with U,
</bodyText>
<footnote confidence="0.549842">
6These measures are described in Benjamin Synder’s mail
</footnote>
<bodyText confidence="0.999323">
any instance without a WN sensekey is assumed to
be tagged with a ‘U’ and thus is tagged as correct
if the answer file (i.e. answer.key) has a ‘U’, incor-
rect otherwise. In without U, any instance without
a WN sensekey is assumed to have been skipped,
thus precision will not be affected, but recall will be
lowered.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999989535714286">
In SENSEVAL-3, we participated in both English all
words task and English lexical sample task with an
unsupervised system based on WordNet and a raw
corpus, which did not use any sense tagged cor-
pus. Our system disambiguated the senses of a tar-
get word by selecting a substituent among WordNet
relatives of the target word, which frequently co-
occurs with each word surrounding the target word
in a context. Since each relative is usually related
to only one sense of the target word, our system
identifies the proper sense with the selected rela-
tive. The substituent word is selected based on the
co-occurrence frequency between the relative and
the words surrounding the target word in a given
context. We collected the co-occurrence frequency
from a raw corpus, not a sense-tagged one that is
often required by other approaches. In short, our
system disambiguates senses of words only through
the set of WordNet relatives of the target words and
a raw corpus. The system was simple but seemed
to achieve a good performance when considered the
performance of systems in last SENSEVAL-2 En-
glish tasks.
For future research, we will investigate the depen-
dency between the types of relatives and the char-
acteristics of words or senses in order to devise an
improved method that better utilizes various types
of relatives for WSD.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967582357142857">
Claudia Leacock, Martin Chodorow, and George A.
Miller. 1998. Using corpus statistics and Word-
Net relations for sense identification. Computa-
tional Linguistics, 24(1):147–165.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in
Language Processing, Manchester,U.K.
David Yarowsky. 1992. Word-sense disambigua-
tion using statistical models of Roget’s cate-
gories trained on large corpora. In Proceedings
of COLING-92, pages 454–460, Nantes, France,
July.
about English all words task results
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.084644">
<note confidence="0.82137625">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics KUNLP System in SENSEVAL-3</note>
<author confidence="0.704856">Hee-Cheol Seo</author>
<author confidence="0.704856">Hae-Chang</author>
<affiliation confidence="0.842844">Dept. of Computer and Korea</affiliation>
<address confidence="0.7640005">1, 5-ka, Anam-dong, Seoul, 136-701,</address>
<email confidence="0.67637">Soo-Hong</email>
<affiliation confidence="0.866215">Dept. of Computer Software College of Sangmyung</affiliation>
<address confidence="0.858931">San 98-20, Chonan, Chungnam,</address>
<email confidence="0.985287">soohkim@smuc.ac.kr</email>
<abstract confidence="0.999691076923077">We have participated in both English all words and English lexical sample task of 3. Our system disambiguates senses of a target word in a context by selecting a substituent among WordNet relatives of the target word, such as synonyms, hypernyms, meronyms and so on. The decision is made based on co-occurrence frequency between candidate relatives and each of the context words. Since the co-occurrence frequency is obtainable from raw corpus, our method is considered to be an unsupervised learning algorithm that does not require a sense-tagged corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>George A Miller</author>
</authors>
<title>Using corpus statistics and WordNet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="2143" citStr="Leacock et al. (1998)" startWordPosition="320" endWordPosition="323"> those in a synonym class, usually have related meanings and tend to share similar contexts. Hence, some WordNet-based approaches extract relatives of each sense of a polysemous word from WordNet, collect example sentences of the relatives from a raw corpus, and learn the senses from the example sentences for WSD. Yarowsky (1992) first proposed this approach, but used International Roget’s Thesaurus as a hierarchical lexical database instead of WordNet. However, the approach seems to suffer from examples irrelevant to the senses of a polysemous word since many of the relatives are polysemous. Leacock et al. (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. However, some senses do not have short distance monosemous relatives through a relation such as synonym, child, and parent. A possible alternative of using only monosemous relatives in the long distance, however, is problematic because the longer the distance of two synsets in WordNet, the weaker the relationship between them. In other words, the monosemous relatives in the long distance may provide irrelevant examples for WSD. Our approach is somewhat similar to the WordNet based approach of L</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>Claudia Leacock, Martin Chodorow, and George A. Miller. 1998. Using corpus statistics and WordNet relations for sense identification. Computational Linguistics, 24(1):147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester,U.K.</location>
<contexts>
<context position="9711" citStr="Schmid, 1994" startWordPosition="1627" endWordPosition="1628"> other words, the proposed method disambiguates the senses of all words efficiently with only one matrix. 2.3 WordNet Relatives Our system used most of relationship types in WordNet, except sister and attribute types, to acquire the relatives of target words. For a nominal word, we included all hypernyms and hyponyms in distance 3 from a sense, which indicate parents, grandparents and great-grand parents for hypernymy and children, grandchildren and great-children for hyponymy5. In order to identify part-of-speech (POS) of words including target words in instances, our system uses TreeTagger (Schmid, 1994). After POS 4The co-occurrence frequency matrix is a symmetric matrix, thus mzj is the same as mjz. 5We implemented WordNet APIs with index files and data files in WordNet package, which is downloadable from http://www.cogsci.princeton.edu/ wn/. �� �������������� ��� P (rig) k=1 + log,Q(r,ij)P(rij) � arg max rid W Nf(r�z�) + 0.5 = arg max rid fine grained coarse grained recall prec. recall prec. noun 0.451 0.451 0.556 0.556 verb(R) 0.354 0.354 0.496 0.496 adjective 0.497 0.497 0.610 0.610 overall 0.404 0.404 0.528 0.528 Table 1: Official Results: English Lexical Sample with U without U recall </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, Manchester,U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>454--460</pages>
<location>Nantes, France,</location>
<contexts>
<context position="1853" citStr="Yarowsky (1992)" startWordPosition="278" endWordPosition="279"> any sense tagged corpus. WordNet specifies relationships among the meanings of words. Relatives of a word in WordNet are defined as words that have a relationship with it, e.g. they are synonyms, antonyms, superordinates (hypernyms), or subordinates (hyponyms). Relatives, especially those in a synonym class, usually have related meanings and tend to share similar contexts. Hence, some WordNet-based approaches extract relatives of each sense of a polysemous word from WordNet, collect example sentences of the relatives from a raw corpus, and learn the senses from the example sentences for WSD. Yarowsky (1992) first proposed this approach, but used International Roget’s Thesaurus as a hierarchical lexical database instead of WordNet. However, the approach seems to suffer from examples irrelevant to the senses of a polysemous word since many of the relatives are polysemous. Leacock et al. (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. However, some senses do not have short distance monosemous relatives through a relation such as synonym, child, and parent. A possible alternative of using only monosemous relatives in the long distance, howev</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora. In Proceedings of COLING-92, pages 454–460, Nantes, France, July.</rawString>
</citation>
<citation valid="false">
<title>about English all words task results</title>
<marker></marker>
<rawString>about English all words task results</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>