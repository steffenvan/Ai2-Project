<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.968691">
Features and Values
</title>
<author confidence="0.98192">
Lauri Karttunen
</author>
<affiliation confidence="0.902551333333333">
University of Texas at Austin
Artificial Intelligence Center
SRI International
and
Center for the Study of Language and Information
Stanford University
</affiliation>
<sectionHeader confidence="0.97711" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998463416666667">
The paper discusses the linguistic aspects of a new gen-
eral purpose facility for computing with features. The pro-
gram was developed in connection with the course I taught
at the University of Texas in the fall of 1983. It is a general-
ized and expanded version of a system that Stuart Shieber
originally designed for the PATR-II project at SRI in the
spring of 1983 with later modifications by Fernando Pereira
and me. Like its predecessors, the new Texas version of the
&amp;quot;DG (directed graph)&amp;quot; package is primarily intended for
representing morphological and syntactic information but
it may turn out to be very useful for semantic representa-
tions too.
</bodyText>
<sectionHeader confidence="0.997561" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.993504642857143">
Most schools of linguistics use some type of feature no-
tation in their phonological, morphological, syntactic, and
semantic descriptions. Although the objects that appear
in rules and conditions may have atomic names, such as
&amp;quot;k,&amp;quot; &amp;quot;NP,&amp;quot; &amp;quot;Subject,&amp;quot; and the like, such high-level terms
typically stand for collections of features. Features, in this
sense of the word, are usually thought of as attribute-value
pairs: [person: 1st], [number: sg], although singleton fea-
tures are also admitted in some theories. The values of
phonological and morphological features are traditionally
atomic; e.g. 1st, 2nd, 3rd; they are often binary: +,
Most current theories also allow features that have com-
plex values. A complex value is a collection of features, for
example:
</bodyText>
<sectionHeader confidence="0.856998" genericHeader="introduction">
1 . agreement
</sectionHeader>
<bodyText confidence="0.970860407407407">
number person
3rd
In graphs of this sort, values are reached by traversing
paths of attribute names. We use angle brackets to mark
expressions that designate paths. With that convention,
the above graph can also be represented as a set of equa-
tions:
&lt;agreement number&gt; = sg
&lt;agreement person&gt; = 3rd
Such equations also provide a convenient way to ex-
press conditions on features. This idea lies at the heart of
UG, LFG, and the PATR-II grammar for English [Shieber,
et al., 831 constructed at SRI. For example, the equation
&lt;subject agreement&gt; = &lt;predicate agreement&gt;
states that subject and predicate have the same value for
agreement. In graph terms, this corresponds to a lattice
where two vectors point to the same node:
[agreement: [ subj ect 1 predicate
person: 3rdn
number: sg
Lexical Functional Grammar (LFG) [Kaplan and Bres- agreement agreement
nan, 831, Unification Grammar (UG) [Kay, 79], General- number person
ized Phrase Structure Grammar (GPSG) [Gazdar and Pul- sg 3rd
lum, 821, among others, use complex features.
Another way to represent feature matrices is to think of
them as directed graphs where values correspond to nodes
and attributes to vectors:
</bodyText>
<page confidence="0.9146295">
1
28
</page>
<bodyText confidence="0.987794666666666">
In a case like this, the values of the two paths have been
&amp;quot;unified.&amp;quot; To represent unification in terms of feature ma-
trices we need to introduce some new convention to distin-
guish between identity and mere likeness. Even that would
not quite suffice because the graph formalism also allows
unification of values that have not -yet been assigned.
A third way to .view these structures is to think of
them as partial functions that assign values to attributes
[Sag et.al., Si].
</bodyText>
<sectionHeader confidence="0.842629" genericHeader="method">
2. Unification and Generalization
</sectionHeader>
<bodyText confidence="0.991683217391304">
Several related grammar formalisms (UG, LFG, PATH-
II, and GPS() now exist that are based on a very similar
conception of features and use unification as their basic op-
eration. Because feature matrices (lattice nodes) are sets
of attribute-value pairs, unification is closely related to the
operation of forming a union of two sets. However, while
the latter always yields something—at least the null set,
unification is an operation that may fail or succeed. When
it fails, no result. is produced and the operands remain un-
changed; when it succeeds, the operands are permanently
altered in the process. They become the same object. This
is an important characteristic. The result of unifying three
or more graphs in pairs with one another does not depend
on the order in which the operations are performed. They
all become the same graph at the end.
If graphs A and B contain the same attribute but have
incompatible values for it, they cannot be unified. If A
and B are compatible, then (Unify A B) contains every
attribute that appears only in A or only in B with the
value it has there. If some attribute appears both in A
and B, then the value of that attribute in (Unify A B) is
the unification of the two values. For example,
= [agreement: [number: pa]
</bodyText>
<figure confidence="0.8279058">
B = [agreement: [person: 3rd]]
case: nominative
[number: p I
[case: nominative
person: 3rd]
</figure>
<subsectionHeader confidence="0.823932">
(Unify A B) = agreement:
</subsectionHeader>
<bodyText confidence="0.959896272727273">
Simple cases of grammatical concord, such as number,
case and gender agreement between determiners and nouns
in many languages, can be expressed straight-forwardly by
stating that the values of these features must unify.
Another useful operation on feature matrices is gen-
eralization. It is closely related to set intersection. The
generalization of two simple matrices A and B consists of
the attribute-value pairs that A and B have in common.
If the values themselves are complex, we take the general-
ization of those values.
For example,
</bodyText>
<page confidence="0.98249">
2
</page>
<figure confidence="0.5970174">
A . agreement:
1
Lease: nominative
[number: sg
person: 2nd]
[[number: sg
agreement: person: 3rd
gender: mase
ease: genitive
(Generalize A B) = [agreement: [number: s£1]]
</figure>
<bodyText confidence="0.9994392">
Generalization seems to be a very useful notion for ex-
pressing how number and gender agreement works in coor-
dinate noun phrases. One curious fact about coordination
is that conjunction of &amp;quot;I&amp;quot; with &amp;quot;you&amp;quot; or &amp;quot;he&amp;quot; in the subject
position typically produces first person verb agreement. In
sentences like &amp;quot;he and I agree&amp;quot; the verb has the same form
as in &amp;quot;we agree. &amp;quot; The morphological equivalence of &amp;quot;he
and I,&amp;quot; &amp;quot;you and I,&amp;quot; and &amp;quot;we&amp;quot; is partially obscured in En-
glish but very clear in many other languages. The problem
is discussed in Section V below.
</bodyText>
<sectionHeader confidence="0.781844" genericHeader="method">
3. Limitations of Some Current For-
malisms
</sectionHeader>
<bodyText confidence="0.9612665">
Most current grammar formalisms for features have
certain built-in limitations. Three are relevant here:
</bodyText>
<listItem confidence="0.999887666666667">
• no cyclic structures
• no negation
• no disjunction.
</listItem>
<bodyText confidence="0.999745416666666">
The prohibition against cyclicity rules out structures
that contain circular paths, as in the following example.
Here the path &lt;a b c&gt; folds back onto itself, that is,
&lt;a&gt; = &lt;a b c&gt;. It is not clear whether such descriptions
should be ruled out on theoretical grounds. Whatever the
case might be, current implementations of LFG, UG, or
GPSG with which I am familiar do not support them.
The prohibition against negation makes it impossible
to characterize a feature by saying that it does NOT have
such and such a value. None of the above theories allows
specifications such as the following. We use the symbol &amp;quot;-&amp;quot;
to mean &apos;not.&apos;
</bodyText>
<figure confidence="0.332395">
[case: Pdat]]
B =
</figure>
<page confidence="0.989814">
29
</page>
<bodyText confidence="0.992066">
riperson: 3rol
number: sg
The first statement says that case is &amp;quot;not dative,&amp;quot; the
second says that the value of agreement is &amp;quot;anything but
3rd person singular.&amp;quot;
Not allowing disjunctive specifications rules out ma-
trices of the following sort. We indicate disjunction by
enclosing the alternative values in {}.
The first line describes the value of case as being &amp;quot;ei-
ther nominative or accusative.&amp;quot; The value for agreement
is given as &amp;quot;either feminine singular or plural.&amp;quot; Among
the theories mentioned above, only Kay&apos;s UG allows dis-
junctive feature specifications in its formalism. (In LFG,
disjunctions are allowed in control equations but not in the
specification of values.)
Of the three limitations, the first one may be theo-
retically justified since it has not been shown that there
are phenomena in natural languages that involve circular
structures (cf. [Kaplan and Bresnan, 831, p. 281). PATR-II
at SRI and its expanded version at the University of Texas
allow such structures for practical reasons because they
tend to arise, mostly inadvertently, in the course of gram-
mar construction and testing. An implementation that
does not handle unification correctly in such cases is too
fragile to use.
The other two restrictions are linguistically unmoti-
vated. There are many cases, especially in morphology,
in which the most natural feature specifications are nega-
tive or disjunctive. In fact, the examples given above all
represent such cases.
The first example, [case: -dat], arises in the plu-
ral paradigm of words like &amp;quot;Kind&amp;quot; child in German.
Such words have two forms in the plural: &amp;quot;Kinder&amp;quot; and
&amp;quot;Kindern.&amp;quot; The latter is used only in the plural dative,
the former in the other three cases (nominative, genitive,
accusative). If we accept the view that there should be just
one rather than three entries for the plural suffix &amp;quot;-er&amp;quot;, we
have the choice between
[
</bodyText>
<listItem confidence="0.89829">
• number: pi
case: {nom gen ace}]
</listItem>
<bodyText confidence="0.879928571428572">
[
number; pi
case: [-oat]
The second alternative seems preferrable given the fact
that there is, in this particular declension, a clear two-
way contrast. The marked dative is in opposition with an
unmarked form representing all the other cases.
</bodyText>
<page confidence="0.705112">
3
</page>
<bodyText confidence="0.9989555">
The Aecond example is from English. Although the fea-
tures &amp;quot;number&amp;quot; and &amp;quot;person&amp;quot; are both clearly needed in
English verb morphology, most verbs are very incompletely
specified for them. In fact, the present tense paradigm of
all regular verbs just has two forms of which one represents
the 3rd person singular (&amp;quot;walks&amp;quot;) and the other (&amp;quot;walk&amp;quot;)
is used for all other persons. Thus the most natural char-
acterization for &amp;quot;walk&amp;quot; is that it is not 3rd person singu-
lar. The alternative is to say, in effect, that &amp;quot;walk&amp;quot; in the
present tense has five different interpretations.
The system of articles in German provides many ex-
amples that call for disjunctive feature specifications. The
article &amp;quot;die,&amp;quot; for example, is used in the nominative and
accusative cases of singular feminine nouns and all plural
nouns. The entry given above succinctly encodes exactly
this fact.
There are many cases where disjunctive specifications
seem necessary for reasons other than just descriptive el-
egance. Agreement conditions on conjunctions, for exam-
ple, typically fail to exclude pairs where differences in case
and number are not overtly marked. For example, in Ger-
man [Eisenberg, 73] noun phrases like:
des Dozenten (gen sg) the docent&apos;s
der Dozenten (gen pl) the docents&apos;.
can blend as in
der Antrag des oder der Dozenten
the petition of the docent or docents.
This is not possible when the noun is overtly marked for
number, as in the case of &amp;quot;des Professors&amp;quot; (gen sg) and
&amp;quot;der Professoren&amp;quot; (gen pl):
*der Antrag des oder der Professors
*der Antrag des oder der Professoren
the petition of the professor or professors
In the light of such cases, it seems reasonable to as-
sume that there is a single form, &amp;quot;Dozenten,&amp;quot; which has
a disjunctive feature specification, instead of postulating
several fully specified, homonymous lexical entries. It is
obvious that the grammaticality of the example crucially
depends on the fact that &amp;quot;Dozenten&amp;quot; is not definitely sin-
gular or definitely plural but can be either.
</bodyText>
<sectionHeader confidence="0.826596" genericHeader="method">
4. Unification with Disjunctive and
Negative Feature Specifications
</sectionHeader>
<bodyText confidence="0.99995">
I sketch here briefly how the basic unification proce-
dure can be modified to admit negative and disjunctive
values. These ideas have been implemented in the new
Texas version of the PATR-II system for features. (I am
much indebted to Fernando Pereira for his advice on this
topic.)
Negative values are created by the following operation.
If A and B are distinct, i.e. contain a different value for
some feature, then (Negate A B) does nothing to them.
Otherwise both nodes acquire a &amp;quot;negative constraint.&amp;quot; In
effect, A is marked with -B and B with -A. These con-
straints prevent the two nodes from ever becoming alike.
</bodyText>
<figure confidence="0.980961777777778">
[agreement:
1 agreement:
case: {nom acc}
{
[numbegender:
[number:
r:
-er
-er
</figure>
<page confidence="0.977682">
30
</page>
<bodyText confidence="0.999507636363636">
When A is unified with C, unification succeeds only if the
result is distinct from B. The result of (Unify A C) has to
satisfy all the negative constraints of both A and C and it
inherits all that could fail in some later unification.
Disjunction is more complicated. Suppose A, B and
C are all simple atomic values. In this situation C unifies
with {A B) just in case it is identical to one or the other
of the disjuncts. The result is C. Now suppose that A, B,
and C are all complex. Furthermore, let us suppose that A
and B are distinct but C is compatible with both of them
as in the following:
</bodyText>
<equation confidence="0.86043325">
A = [gender: tem]
number: sg
B = [number: pa
C = [case: ace]
</equation>
<bodyText confidence="0.983619066666667">
What should be the result of (Unify {A B) C)? Because
A and B are incompatible, we cannot actually unify C with
both of them. That operation would fail. Because there is
no basis for choosing one, both alternatives have to be left
open. Nevertheless. we need to take note of the fact that
either A or B is to be unified with C. We can do this by
making the result a complex disjunction.
= {(AC) c) .
The new value of C, C&apos;, is a disjunction of tuples which
can be, but. have not yet been unified. Thus (A C) and (B
C) are sets that consist of compatible structures. Further-
more, at least one of the tuples in the complex disjunction
must remain consistent regardless of what happens to A
and B. After the first unification we can still unify A with
any structure that it. is compatible with, such as:
</bodyText>
<subsectionHeader confidence="0.358405">
D [case: nom]
</subsectionHeader>
<bodyText confidence="0.886681076923077">
If this happens, then the tuple (A C) is no longer con-
sistent. A side effect of A becoming
A&apos; = render: tem]
number: sg
case: nom
is that C&apos; simultaniously reduces to {(B C)). Since there
is now only one viable alternative left, B and C can at this
point be unified. The original result from (Unify {A Ill
C) now reduces to the same as (Unify B C).
C&amp;quot; = {(B C)} = [number:
Lease: aco
As the example shows, once C is unified with (A B), A
and B acquire a &amp;quot;positive constraint.&amp;quot; All later unifications
</bodyText>
<page confidence="0.939012">
4
</page>
<bodyText confidence="0.990067958333333">
involving them must keep at least one of the two pairs (A
C), (B C) unifieable. If at some later point one of the
two tuples becomes inconsistent, the members of the sole
remaining tuple finally can and should be unified. When
that has happened, the positive constraint on A and B can
also be discarded. A more elaborate example of this sort
is given in the Appendix.
Essentially the same procedure also works for more
complicated cases. For example, unification of {A B} with
{C DI yields {(A C) (A D) (B C) (B D)) assuming that
the two values in each tuple are compatible. Any pairs that
could not be unified are left out. The complex disjunction
is added as a positive constraint to all of the values that
appear in it. The result of unifying {(A C) (B C)) with
{(D F) (E F)} is {(A CD F) (A C E F) (B CD F) (B C
E F)), again assuming that no alternative can initially be
ruled out.
As for generalization, things are considerably simpler.
The result of (Generalize A B) inherits both negative and
positive constraints of A and B. This follows from the fact
that the generalization of A and B is the maximal sub-
graph of A and B that will unify with either one them.
Consequently, it is subject to any constraint that affects A
or B. This is analogous to the fact that, in set theory,
</bodyText>
<equation confidence="0.996324">
(A — C) n (B — D) = (A n B) — (C u D) .
</equation>
<bodyText confidence="0.99997108">
In our current implementation, negative constraints
are dropped as soon as they become redundant as far as
unification is concerned. For example, when [case: ace]
is unified with with [case: -dal], the resulting matrix is
simply [case: accl. The negative constraint is eliminated
since there is no possibility that it could ever be violated
later. This may be a wrong policy. It has to be modified
to make generalization work as proposed in Section V for
structures with negative constraints. If generalization is
defined as we have suggested above, negative constraints
must always be kept because they never become redundant
for generalization.
When negative or positive constraints are involved,
unification obviously takes more time. Nevertheless, the
basic algorithm remains pretty much the same. Allowing
for constraints does not significantly reduce the speed at
which values that do not have any get unified in the Texas
implementation.
In the course of working on the project, I gained one
insight that perhaps should have been obvious from the
very beginning: the problems that arise in this connection
are very similar to those that come up in logic program-
ming. One can actually use the feature system for certain
kind of inferencing. For example, let Mary, Jane, and John
have the following values:
</bodyText>
<sectionHeader confidence="0.176045" genericHeader="method">
Mary = [hair: blond]
</sectionHeader>
<reference confidence="0.4281275">
Jane [hair: dark]
John s&apos; [sister: {Jane Mary}]
</reference>
<page confidence="0.999712">
31
</page>
<bodyText confidence="0.952307285714286">
straints rather than additional features for establishing a
markedness hierarchy. For example, the following feature
specifications have the effect that we seek.
If we now unify John with
[sister: [eyes: blue]]
both Jane and Mary get marked with the positive con-
straint that at least one of them has blue eyes. Suppose
that we now learn that Mary has green eyes. This imme-
diately gives us more information about John and Jane as
well. Now we know that Jane&apos;s eyes are blue and that she
definitely is John&apos;s sister. The role of positive constraints
is to keep track of partial information in such a way that
no inconsistencies are allowed and proper updating is done
when more things become known.
</bodyText>
<sectionHeader confidence="0.844316" genericHeader="method">
5. Future prospects: Agreement in
Coordinate Structures
</sectionHeader>
<bodyText confidence="0.996506444444444">
One problem of long standing for which the present sys-
tem may provide a simple solution is person agreement in
coordinate noun phrases. The conjunction of a 1st person
pronoun with either 2nd or 3rd person pronoun invariably
yields 1st person agreement. &amp;quot;I and you&amp;quot; is equivalent to
&amp;quot;we,&amp;quot; as far as agreement is concerned. When a second
person pronoun is conjoined with a third person NP, the
resulting conjunction has the agreement properties of a
second person pronoun. Schematically:
</bodyText>
<figure confidence="0.763996333333333">
[conversant:
speaker +
[-conversant:
speaker:
1-
3rd = conversant:
speaker: ..
The corresponding negative constraints are:
[1-conversant: -1
speaker
[[conversant: -]]
(no constraints)
</figure>
<bodyText confidence="0.9986435">
Assuming that generalization with negative constraints
works as indicated above, i.e. negative constraints are al-
ways inherited, it immediately follows that the generaliza-
tion of 1st person with any other person is compatible with
only 1st person and that 2nd person wins over 3rd when
they are combined. The results are as follows.
</bodyText>
<figure confidence="0.928337285714286">
let •
2nd =
+1
let
2nd =
•
3rd =
let + 2nd = let
let + 3rd = let
2nd + 3rd i• 2nd.
let + 2nd
conversant: +
Iconversant:
speaker -
</figure>
<bodyText confidence="0.999291541666667">
Sag, Gazdar, Wasow, and Weisler [84] propose a so-
lution which is based on the idea of deriving the person
feature for a coordinate noun phrase by generalization (in-
tersection) from the person features of its heads. It is ob-
vious that the desired effect can be obtained in any feature
system that uses the fewest features to mark 1st person,
some additional feature for 2nd person, and yet another for
3rd person. Because generalization of 1st and 2nd, for ex-
ample, yields only the features that two have in common,
the one with fewest features wins.
Any such solution can probably be implemented easily
in the framework outlined above. However, this proposal
has one very counterintuitive aspect: markedness hierar-
chy is the reverse of what traditionally has been assumed.
Designating something as 3rd person requires the greatest
number of feature specifications. In the Sag et al. system,
3rd person is the most highly marked member and 1st per-
son the least marked member of the trio. Traditionally, 3rd
person has been regarded as the unmarked case.
In our system, there is a rather simple solution under
which the value of person feature in coordinate NPs is de-
rived by generalization, just as Sag it et al. propose, which
nevertheless preserves the traditional view of markedness.
The desired result can be obtained by using negative con-
</bodyText>
<page confidence="0.464385">
5
</page>
<sectionHeader confidence="0.37217" genericHeader="method">
[
</sectionHeader>
<bodyText confidence="0.627321">
let + 3rd - - conversant:
</bodyText>
<sectionHeader confidence="0.28688" genericHeader="method">
speaker: -
</sectionHeader>
<subsectionHeader confidence="0.585479">
2nd + 3rd
</subsectionHeader>
<bodyText confidence="0.949896210526316">
Note that the proper part of lst+2nd excludes 3rd person.
It is compatible with both 1st and 2nd person but the
negative constraint rules out the latter one. In tile case
of lst+3rd, the negative constraint is compatible with 1st
person but incompatible with 2nd and 3rd. In the last case,
the specification [speaker: -] rules out 1st person and the
negative constraint -[conversant: -1 eliminates 3rd person.
When negative constraints are counted in, 1st person
is the most and 3rd person the least marked member of
the three. In that respect, the proposed analysis is in line
with traditional views on markedness. Another relevant
observation is that the negative constraints on which the
result crucially depends are themselves not too unnatural.
In effect, they say of 1st person that it is &amp;quot;neither 2nd nor
3rd&amp;quot; and that 2nd person is &amp;quot;not 3rd.&amp;quot;
It will be interesting to see whether other cases of
markedness can be analyzed in the same way.
speaker: -
-[-conversant:
</bodyText>
<page confidence="0.643527">
32
</page>
<figure confidence="0.99910925">
[case: acc
[gender: mese]]
agn
number: sg
[case: dat
agr: [number: pa
den Kinder = *FAILS*
case: dat
In tl:
agn [gender: neut]]]
number: pl
[
[case: nom
[number: sg 1
agn
person: 1st]
[
case: nom
[gender. mascl
agn number: sg
person: 3rd
den 22
inf I:
den Kindern =
1=
[ml I:
he =
inf I:
do = [inf I:
I do = [infl:
he do = *FAILS*
tense: present
rinumber: sg 1
agn
person: 3rd]
Cense: present
case:
agn nom
[number: sg
&apos;person: 1st]
</figure>
<sectionHeader confidence="0.937203" genericHeader="conclusions">
6. Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997225">
I am indebted to Martin Kay for introducing me to uni-
fication and to Fernando Pereira, Stuart Shieber, Remo
Pareschi, and Annie Zaenen for many insightful sugges-
tions on the project.
</bodyText>
<sectionHeader confidence="0.999176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99449975">
Eisenberg, Peter, &apos;A Note on Identity of Constituents,&amp;quot; Linguis-
tic Inquiry 4:3. 417-20 (1973).
Gazdar, Gerald and G. Pullum. &apos;Generalized Phrase Structure
Grammar: A Theoretical Synopsis.&amp;quot; Indiana University
Linguistics Club, Bloomington, Indiana (1982).
Kaplan, Ronald M. and Joan Bresnan, 1983: &amp;quot;Lexical-
Functional Grammar: A Formal System for Grammatical
Representation,&apos; Ch.4 in J. Bresnan, The Mental Repre-
sentation of Grammatical Relations (ed.), Cambridge, MIT
Press.
Kay, Martin, 1979; &amp;quot;Functional Grammar.&amp;quot; Proceedings of the
Fifth Annual Meeting of the Berkeley Linguistic Society,
Berkeley Linguistic Society, Berkeley, California (February
17-19, 1979), pp. 142-158.
Pereira, Fernando and Stuart Shieber, 1984: &amp;quot;The semantics of
Grammar Formalism Seen as Computer Languages.&amp;quot; Pro-
ceedings of the Tenth International Conference on Compu-
tational Linguistics, Stanford University, Stanford Califor-
nia (4-7 July, 1984).
Sag, Ivan, Gerald Gazclar, Thomas Wasow, and Steven Weisler,
1984: &amp;quot;Coordination and How to Distinguish Categories.&amp;quot;
CLSI Report No. 3. Center for the Study of Language and X 2b-is:b: y = {[a: Z =
Information, Stanford, Ca., (March 1984). [cb:: [1:): :1 iCa::
Shieber, S., II. Uszkoreit, F. Pereira, J. Robinson, and M. Tyson, -+] :]
1983: &amp;quot;The Formalism and Implementation of PATR-II,&amp;quot; 114
in B. Grosz and M. Stickel, Research on Interactive Acqui- { c:
sition and Use of Knowledge, SRI Final Report 1894, SRI
International, Menlo Park, California (November 1983).
</reference>
<figure confidence="0.905518111111111">
A. Appendix: Some Examples of (Unify x y) (Unify (Unify x y) z)
Unification
{inf I:
Kinder
case: {nom ace}
{
inumber: sg genden fel}
[number: pl]
case: 1-dat3
n
[gender: neutd
in
number: pl
(These examples were produced using the Texas version of
the DG package.) _
agn
die =
infl:
_+]
[a::
b
c:
-]
die Kinder [infl: { agn n
case: {nom acc)-
[gender; eut]
number: pl
</figure>
<page confidence="0.984306">
6
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929181">
<title confidence="0.999838">Features and Values</title>
<author confidence="0.999995">Lauri Karttunen</author>
<affiliation confidence="0.998901833333333">University of Texas at Austin Artificial Intelligence Center SRI International and Center for the Study of Language and Information Stanford University</affiliation>
<abstract confidence="0.994565333333333">The paper discusses the linguistic aspects of a new general purpose facility for computing with features. The program was developed in connection with the course I taught at the University of Texas in the fall of 1983. It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me. Like its predecessors, the new Texas version of the &amp;quot;DG (directed graph)&amp;quot; package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representa-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Jane</author>
</authors>
<title>dark] John s&apos; [sister:</title>
<institution>Jane Mary</institution>
<marker>Jane, </marker>
<rawString>Jane [hair: dark] John s&apos; [sister: {Jane Mary}]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Eisenberg</author>
</authors>
<title>A Note on Identity of Constituents,&amp;quot;</title>
<date>1973</date>
<journal>Linguistic Inquiry</journal>
<volume>4</volume>
<pages>417--20</pages>
<marker>Eisenberg, 1973</marker>
<rawString>Eisenberg, Peter, &apos;A Note on Identity of Constituents,&amp;quot; Linguistic Inquiry 4:3. 417-20 (1973).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>G Pullum</author>
</authors>
<title>Generalized Phrase Structure Grammar: A Theoretical Synopsis.&amp;quot;</title>
<date>1982</date>
<institution>Indiana University Linguistics Club,</institution>
<location>Bloomington, Indiana</location>
<marker>Gazdar, Pullum, 1982</marker>
<rawString>Gazdar, Gerald and G. Pullum. &apos;Generalized Phrase Structure Grammar: A Theoretical Synopsis.&amp;quot; Indiana University Linguistics Club, Bloomington, Indiana (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>LexicalFunctional Grammar: A Formal System for Grammatical Representation,&apos; Ch.4</title>
<date>1983</date>
<booktitle>The Mental Representation of Grammatical Relations</booktitle>
<editor>in J. Bresnan,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge,</location>
<marker>Kaplan, Bresnan, 1983</marker>
<rawString>Kaplan, Ronald M. and Joan Bresnan, 1983: &amp;quot;LexicalFunctional Grammar: A Formal System for Grammatical Representation,&apos; Ch.4 in J. Bresnan, The Mental Representation of Grammatical Relations (ed.), Cambridge, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Functional Grammar.&amp;quot;</title>
<date>1979</date>
<booktitle>Proceedings of the Fifth Annual Meeting of the</booktitle>
<pages>142--158</pages>
<institution>Berkeley Linguistic Society, Berkeley Linguistic Society,</institution>
<location>Berkeley, California</location>
<marker>Kay, 1979</marker>
<rawString>Kay, Martin, 1979; &amp;quot;Functional Grammar.&amp;quot; Proceedings of the Fifth Annual Meeting of the Berkeley Linguistic Society, Berkeley Linguistic Society, Berkeley, California (February 17-19, 1979), pp. 142-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Stuart Shieber</author>
</authors>
<title>The semantics of Grammar Formalism Seen as Computer Languages.&amp;quot;</title>
<date>1984</date>
<booktitle>Proceedings of the Tenth International Conference on Computational Linguistics,</booktitle>
<location>Stanford University, Stanford California</location>
<marker>Pereira, Shieber, 1984</marker>
<rawString>Pereira, Fernando and Stuart Shieber, 1984: &amp;quot;The semantics of Grammar Formalism Seen as Computer Languages.&amp;quot; Proceedings of the Tenth International Conference on Computational Linguistics, Stanford University, Stanford California (4-7 July, 1984).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ivan Sag</author>
<author>Gerald Gazclar</author>
<author>Thomas Wasow</author>
<author>Steven Weisler</author>
</authors>
<title>Coordination and How to Distinguish Categories.&amp;quot;</title>
<date>1984</date>
<booktitle>Research on Interactive Acqui- sition and Use of Knowledge, SRI Final Report 1894, SRI International,</booktitle>
<tech>CLSI Report No. 3.</tech>
<institution>Center</institution>
<location>Stanford, Ca.,</location>
<note>in</note>
<marker>Sag, Gazclar, Wasow, Weisler, 1984</marker>
<rawString>Sag, Ivan, Gerald Gazclar, Thomas Wasow, and Steven Weisler, 1984: &amp;quot;Coordination and How to Distinguish Categories.&amp;quot; CLSI Report No. 3. Center for the Study of Language and Information, Stanford, Ca., (March 1984). Shieber, S., II. Uszkoreit, F. Pereira, J. Robinson, and M. Tyson, 1983: &amp;quot;The Formalism and Implementation of PATR-II,&amp;quot; in B. Grosz and M. Stickel, Research on Interactive Acqui- sition and Use of Knowledge, SRI Final Report 1894, SRI International, Menlo Park, California (November 1983). X 2b-is:b: [cb:: -+] 114 { c: y = {[a: [1:): :1 Z = iCa:: :]</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>