<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040340">
<title confidence="0.9973055">
NTNU: Measuring Semantic Similarity with
Sublexical Feature Representations and Soft Cardinality
</title>
<author confidence="0.927779">
Andr´e Lynum, Partha Pakray, Bj¨orn Gamb¨ack Sergio Jimenez
</author>
<email confidence="0.602442">
{andrely,parthap,gamback}@idi.ntnu.no sgjimenezv@unal.edu.co
</email>
<affiliation confidence="0.984797">
Norwegian University of Science and Technology Universidad Nacional de Colombia
Trondheim, Norway Bogot´a, Colombia
</affiliation>
<sectionHeader confidence="0.977808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964428571429">
The paper describes the approaches taken
by the NTNU team to the SemEval 2014
Semantic Textual Similarity shared task.
The solutions combine measures based
on lexical soft cardinality and character
n-gram feature representations with lexi-
cal distance metrics from TakeLab’s base-
line system. The final NTNU system is
based on bagged support vector machine
regression over the datasets from previous
shared tasks and shows highly competi-
tive performance, being the best system on
three of the datasets and third best overall
(on weighted mean over all six datasets).
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.932969064516129">
The Semantic Textual Similarity (STS) shared task
aims at providing a unified framework for evaluat-
ing textual semantic similarity, ranging from ex-
act semantic equivalence to completely unrelated
texts. This is represented by the prediction of
a similarity score between two sentences, drawn
from a particular category of text, which ranges
from 0 (different topics) to 5 (exactly equivalent)
through six grades of semantic similarity (Agirre
et al., 2013). This paper describes the NTNU
submission to the SemEval 2014 STS shared task
(Task 10). The approach is based on the lexical
and distributional features of the baseline Take-
Lab system from the 2012 shared task (ˇSari´c et al.,
2012), but improves on it in three ways: by adding
two new categories of features and by using a bag-
ging regression model to predict similarity scores.
The new feature categories added are based on
soft cardinality and character n-grams, described
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
in Section 2. The parameters of the two cate-
gories are optimised over several corpora and the
features are combined through support vector re-
gression (Section 3) to create the actual systems
(Section 4). As Section 5 shows, the new mea-
sures give the baseline system a substantial boost,
leading to very competitive results in the shared
task evaluation.
</bodyText>
<sectionHeader confidence="0.99103" genericHeader="method">
2 Feature Generation Methods
</sectionHeader>
<bodyText confidence="0.999964">
The methods used for creating new features utilise
soft cardinality and character n-grams. Soft cardi-
nality (Jimenez et al., 2010) was used successfully
for the STS task in previous SemEval editions
(Jimenez et al., 2012a; Jimenez et al., 2013a).
The NTNU systems utilise an ensemble of such 18
measures, based only on surface text information,
which were extracted using soft cardinality with
different similarity functions, as further described
in Section 2.1.
Section 2.2 then introduces the similarity mea-
sures based on character n-gram feature represen-
tations, which proved themselves as the strongest
features in the STS 2013 task (Marsi et al., 2013).
The measures used here replace character n-gram
features with cluster frequencies or vector val-
ues based on the n-gram collocational structure
learned in an unsupervised manner from text data.
A variety of n-gram feature representations were
trained on subsets of Wikipedia and the best per-
forming ones were used for the new measures,
which are based on cosine similarity between the
document vectors derived from each sentence in a
given pair.
</bodyText>
<subsectionHeader confidence="0.997412">
2.1 Soft Cardinality Measures
</subsectionHeader>
<bodyText confidence="0.9995242">
Soft cardinality resembles classical set cardinality
as it is a method for counting the number of ele-
ments in a set, but differs from it in that similarities
among elements are being considered for the “soft
counting”. The soft cardinality of a set of words
</bodyText>
<page confidence="0.968036">
448
</page>
<note confidence="0.729529">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448–453,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9865175">
A = {a1, a2,.., a|A|} (a sentence) is defined by:
Where p is a parameter that controls the cardinal-
ity’s softness (p’s default value is 1) and wai are
weights for each word, obtained through inverse
document frequency (idf) weighting. sim(ai, aj)
is a similarity function that compares two words
ai and aj using the symmetrized Tversky’s index
(Tversky, 1977; Jimenez et al., 2013a) represent-
ing them as sets of 3-grams of characters. That
is, ai = {ai,1, ai,2, ..., ai,|ai|} where ai,n is the nth
character trigram in the word ai in A. Thus, the
proposed word-to-word similarity is given by:
</bodyText>
<equation confidence="0.994091">
|c|
sim(ai, aj)= β(α|amin|+(1−α)|amax|)+|c |(2)
{ |c |= |ai ∩ aj |+ biassim
|amin |= min {|ai \ aj|, |aj \ ai|}
|amax |= max {|ai \aj|, |aj \ ai|}
</equation>
<bodyText confidence="0.999969681818182">
The sim function is equivalent to the Dice’s co-
efficient if the three parameters are given their de-
fault values, namely α = 0.5, β = 1 and bias = 0.
The soft cardinalities of any pair of sentences A,
B and A∪B can be obtained using Eq. 1. The soft
cardinality of the intersection is approximated by
|A∩B|sim = |A|sim+|B|sim−|A∪B|sim. These
four basic soft cardinalities are algebraically re-
combined to produce an extended set of 18 fea-
tures as shown in Table 1. The feature STSsim is a
parameterized similarity function built by reusing
at word level the symmetrized Tversky’s index
(Eq. 2), whose parameters are tuned from training
data (as further described in Subsection 3.2).
Although this method is based purely on string
matching, the soft cardinality has been shown to
be a very strong baseline for semantic textual com-
parison. The word-to-word similarity sim in Eq. 1
could be replaced by other similarity functions
based on semantic networks or any distributional
representation making this method able to capture
more complex semantic relations among words.
</bodyText>
<subsectionHeader confidence="0.999449">
2.2 Sublexical Feature Representations
</subsectionHeader>
<bodyText confidence="0.999838666666667">
We have created a set of similarity measures based
on induced representations of character n-grams.
The measures are based on similarity between
</bodyText>
<figure confidence="0.8485395">
STSsim (|A|−|A∩B|)/|A|
|A |(|A|−|A∩B|)/|A∪B|
|B ||B|/|A∪B|
|A n B |(|B|−|A∩B|)/|B|
|A U B |(|B|−|A∩B|)/|A∪B|
|A |− |A n B ||A∩B|/|A|
|B |− |A n B ||A∩B|/|B|
|A U B |− |A n B ||A∩B|/|A∪B|
|A|/|A∪B |(|A∪B|−|A∩B|)/|A∩B|
NB: in this table only,  |*  |is short for  |* |sim
</figure>
<tableCaption confidence="0.988514">
Table 1: Soft cardinality features.
</tableCaption>
<bodyText confidence="0.999972034482759">
document vectors, here the centroid of the individ-
ual term vector representations, which are trained
on character n-grams rather than full words. The
vector representations are induced in an unsuper-
vised manner from large unannotated corpora us-
ing word clustering, topic learning and word rep-
resentation learning methods.
In this paper, three different methods have
been used for creating the character n-gram fea-
ture representations: Brown Clusters (Brown et
al., 1992), Latent Semantic Indexing (LSI) topics
(Deerwester et al., 1990), and log linear skip-gram
models (Mikolov et al., 2013). The Brown clusters
were trained using the implementation by Liang
(2005), while the LSI topic vectors and log linear
skip-gram representations were trained using the
Gensim topic modelling framework (ˇReh˚uˇrek and
Sojka, 2010). In addition, tf-idf (Term-Frequency
Inverse Document Frequency) weighting was used
when training LSI topic models. We used a cosine
distance measure between document vectors con-
sisting of the centroid of the term representation
vectors. For Brown clusters, the normalized term
frequency vectors were used with the cluster IDs
instead of the terms themselves. For LSI topic rep-
resentations, the tf-idf weighted topic mixture for
each term was used as the term representation. For
the log linear skip-grams, the word representations
were extracted from the model weight matrix.
</bodyText>
<sectionHeader confidence="0.981983" genericHeader="method">
3 Feature and Parameter Optimisation
</sectionHeader>
<bodyText confidence="0.999973857142857">
The extracted features and the parameters for the
two methods described in the previous section
were optimised over several sets of training data.
As no training data was explicitly provided for the
STS evaluation campaign this year, we used dif-
ferent training sets from past campaigns and from
Wikipedia for the new test sets.
</bodyText>
<equation confidence="0.998284166666667">
|A|sim =
(1)
E�AI1 sim(ai, aj)p
� |A|
i=1
wai
</equation>
<page confidence="0.997287">
449
</page>
<table confidence="0.999573444444445">
Test set Training set
deft-forum MSRvid 2012 train and test +
OnWN 2012 and 2013 test
deft-news MSRvid 2012 train + test
headlines headlines 2013 test
images MSRvid 2012 train + test
OnWN OnWN 2012 and 2013 test
tweet-news SMTeuroparl 2012 test +
SMTnews 2012 test
</table>
<tableCaption confidence="0.998136">
Table 2: Training-test set pairs.
</tableCaption>
<subsectionHeader confidence="0.999911">
3.1 Training Data and Pre-processing
</subsectionHeader>
<bodyText confidence="0.999878571428572">
The training-test sets pairs used for optimising the
parameters of the soft cardinality methods were
selected from the STS 2012 and STS 2013 task,
as shown in Table 2. The character n-gram repre-
sentation vectors were trained in an unsupervised
manner on two subsets of Wikipedia consisting,
respectively, of the first 12 million words (108
characters, hence referred to as Wiki8) and of 125
million words (109 characters; Wiki9).
First, however, the training data had to be pre-
processed. Thus, before extracting the idf weights
and the soft cardinality features, all the texts
shown in Table 2 were passed through the follow-
ing four pre-processing steps:
</bodyText>
<listItem confidence="0.996194666666667">
(i) tokenization and stop-word removal (pro-
vided by NLTK, Bird et al. (2009)),1
(ii) conversion to lowercase characters,
(iii) punctuation and special character removal
(e.g., “.”, “;”, “$”, “&amp;”), and
(iv) Porter stemming.
</listItem>
<bodyText confidence="0.986414666666667">
Character n-grams including whitespace were
generated from the Wikipedia texts, which in con-
trast only were pre-processed in a 3-step chain:
</bodyText>
<listItem confidence="0.993098">
(i) removal of punctuation and extra whites-
pace,
(ii) replacing numbers with their single digit
word (‘one’, ‘two’, etc.), and
(iii) lowercasing all text.
</listItem>
<footnote confidence="0.969383">
1http://www.nltk.org/
</footnote>
<table confidence="0.999915428571429">
Data α Q bias p α&apos; Q&apos; bias&apos;
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
</table>
<tableCaption confidence="0.99944">
Table 3: Optimal parameters used for each dataset.
</tableCaption>
<subsectionHeader confidence="0.998219">
3.2 Soft Cardinality Parameter Optimisation
</subsectionHeader>
<bodyText confidence="0.999981157894737">
The first feature in Table 1, STSsim, was used to
optimise the four parameters α, Q, bias, and p in
the following way. First, we built a text similarity
function reusing Eq. 2 for comparing two sets of
words (instead of two sets of character 3-grams)
and replacing the classic cardinality |* |by the soft
cardinality  |* |sim from Eq. 1. This text similarity
function adds three parameters (α&apos;, Q&apos;, and bias&apos;)
to the initial four parameter set (α, Q, bias, and p).
Second, these seven parameters were set to their
default values and the scores obtained from this
function for each pair of sentences were compared
to the gold standards in the training data using
Pearson’s correlation. The parameter search space
was then explored iteratively using hill-climbing
until reaching optimal Pearson’s correlation. The
criterion for assignment of training-test set pairs
was by closeness of average character length. The
optimal training parameters are shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.972462">
3.3 Parameters for N-gram Feature Training
</subsectionHeader>
<bodyText confidence="0.992825571428571">
The character n-gram feature representation vec-
tors were trained while varying the parameters of
n-gram size, cluster size, and term frequency cut-
offs for all models. For the log linear skip-gram
models, our intuition is that a larger skip-gram
context is needed than the 5 or 10 wide skip-grams
used to train word-based representations due to the
smaller term vocabulary and dependency between
adjacent n-grams, so instead we trained models us-
ing skip-gram widths of 25 or 50 terms. Term fre-
quency cut-offs were set to limit the model size,
but also potentially serve as a regularization on
the resulting measure. In detail, the following sub-
lexical representation measures are used:
</bodyText>
<listItem confidence="0.9876236">
• Log linear skip-gram representations of char-
acter 3- and 4-grams of size 1000 and 2000,
respectively. Trained on the Wiki8 corpus us-
ing a skip gram window of size 25 and 50,
and frequency cut-off of 5.
</listItem>
<page confidence="0.799442">
450
</page>
<listItem confidence="0.999447">
• Brown clusters with size 1024 of character 4-
grams using a frequency cut-off of 20.
• Brown clusters of character 3-, 4- and 5-
grams with cluster sizes of resp. 1024, 2048
and 1024. The representations are trained on
the Wiki9 corpus with successively increas-
ing frequency cut-offs of 20, 320 and 1200.
• LSI topic vectors based on character 4-grams
of size 2000. Trained on the Wiki8 corpus
using a frequency cut-off of 5.
• LSI topic vectors based on character 4-grams
of size 1000. Trained on the Wiki9 corpus
using a frequency cut-off of 80.
</listItem>
<subsectionHeader confidence="0.983976">
3.4 Similarity Score Regression
</subsectionHeader>
<bodyText confidence="0.999068897435897">
The final sentence pair similarity score is predicted
by a Support Vector Regression (SVR) model with
a Radial Basis (RBF) kernel (Vapnik et al., 1997).
The model is trained on all the test data for the
2013 STS shared task combined with all the trial
and test data of the 2012 STS shared task.
The combined dataset hence consists of about
7,500 sentence pairs from nine different text cat-
egories: five sets from the annotated data sup-
plied to STS 2012, based on Microsoft Research
Paraphrase and Video description corpora (MSR-
par and MSvid), statistical machine translation
system output (SMTeuroparl and SMTnews), and
sense mappings between OntoNotes and WordNet
(OnWN); and four sets from the STS 2013 test
data: headlines (news headlines), SMT, OnWN,
and FNWM (mappings of sense definitions from
FrameNet and WordNet).
The SVR model was trained as a bagged classi-
fier, that is, for each run, 100 regression models
were trained with 80% of the samples and fea-
tures of the original training set drawn with re-
placement. The outputs of all models were then
averaged into a final prediction. This bagged train-
ing procedure adds extra regularization, which can
reduce the instability of prediction accuracy be-
tween different test data categories.
The prediction pipeline was implemented with
the Scikit-learn software framework (Pedregosa et
al., 2011), and the SVR models were trained with
the implementation’s default parameters: cost
penalty (C) 1.0, margin (E) 0.1, and RBF precision
(γ) 1/|featurecount|.
We were unable to improve the performance
over these defaults by cross validation parameter
search unless the models were trained for specific
text categories. Consequently no parameter opti-
mization was performed during training of the fi-
nal systems.
</bodyText>
<sectionHeader confidence="0.98875" genericHeader="method">
4 Submitted Systems
</sectionHeader>
<bodyText confidence="0.9997734">
The three submitted systems consist of one us-
ing only the soft cardinality features described in
Section 3.2 (NTNU-run1), one system using a
baseline set of lexical measures and WordNet aug-
mented similarity in addition to the new sublexical
representation measures (NTNU-run2), and one
(NTNU-run3) which combines the output from
the other two systems by taking the mean of the
two sets of predictions. NTNU-run3 thus repre-
sents a combination of the measures and methods
introduced by NTNU-run1 and NTNU-run2.
In addition to the sublexical feature measures
described in Section 3.3, NTNU-run2 uses the fol-
lowing baseline features adapted from the Take-
Lab 2012 system submission (ˇSari´c et al., 2012).
</bodyText>
<listItem confidence="0.997121181818182">
• Simple lexical features: Relative document
length differences, number overlap, case
overlap, and stock symbol named entity
recognition.
• Lemma and word n-gram overlap of orders 1-
3, frequency weighted lemma and word over-
lap, and WordNet augmented overlap.
• Cosine similarity between the summed word
representation vectors from each sentence us-
ing LSI models based on large corpora with
or without frequency weighting.
</listItem>
<bodyText confidence="0.999911857142857">
The specific measures used in the submitted
systems were found by training the regression
model on the STS 2012 shared task data and eval-
uating on the STS 2013 test data. We used a step-
wise forward feature selection method by compar-
ing mean (but unweighted) correlation on the four
test categories in order to identify the subset of
measures to include in the final system.
The system composes a feature set of similar-
ity scores from these 20 baseline measures and the
nine sublexical representation measures, and uses
these to train a bagged SVM regressor as described
in Section 3.4 in order to predict the final semantic
similarity score for new sentence pairs.
</bodyText>
<page confidence="0.996104">
451
</page>
<table confidence="0.9998852">
Dataset NTNU-run1 NTNU-run2 NTNU-run3 Best
r rank r rank r rank r
deft-forum 0.4369 16 0.5084 2 0.5305 1 0.5305
deft-news 0.7138 14 0.7656 6 0.7813 2 0.7850
headlines 0.7219 17 0.7525 13 0.7837 1 0.7837
images 0.8000 9 0.8129 4 0.8343 1 0.8343
OnWN 0.8348 7 0.7767 20 0.8502 4 0.8745
tweet-news 0.4109 33 0.7921 1 0.6755 13 0.7921
mean 0.6531 20 0.7347 4 0.7426 2 0.7429
weighted mean 0.6631 21 0.7491 4 0.7549 3 0.7610
</table>
<tableCaption confidence="0.998658">
Table 4: Final evaluation results for the submitted systems.
</tableCaption>
<sectionHeader confidence="0.997282" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999962255813954">
The final evaluation results for the three submit-
ted systems are shown in Table 4, where the right-
most column (‘Best’) for comparison displays the
performance figures obtained by any of the 38 sys-
tems on each dataset.
The systems using sublexical representation
based measures show competitive performance,
ranking third and fourth among the submitted sys-
tems with a weighted mean correlation of -0.75.
They also produced the best result in four out of
the six text categories in the evaluation dataset,
with NTNU-run3 being the #1 system on deft-
forum, headlines and images, #2 on deft-news, and
#4 on OnWN. It would thus have been the clear
winner if it had not been for its sub-par perfor-
mance on the tweet-news dataset, which on the
other hand is the category NTNU-run2 was the
best of all systems on.
The system based solely on soft cardinality fea-
tures, NTNU-run1, displays more modest perfor-
mance ranking at 21st place (of the in total 38 sub-
mitted systems) with -0.66 correlation. This is a
bit surprising, since this method for obtaining fea-
tures from pairs of texts was used successfully in
other SemEval tasks such as cross-lingual textual
entailment (Jimenez et al., 2012b) and student re-
sponse analysis (Jimenez et al., 2013b). Similarly,
Croce et al. (2012) used soft cardinality represent-
ing text as a bag of dependencies (syntactic soft
cardinality) obtaining the best results in the typed-
similarity task (Croce et al., 2013).
From our results it can be noted that for most
categories the sublexical representation measures
show strong performance in NTNU-run2, with a
significantly better result for the combined sys-
tem NTNU-run3. This indicates that while the soft
cardinality features are weaker predictors overall,
they are complimentary to the sublexical and lex-
ical features of NTNU-run2. It is also indicative
that this is not the case for the tweet-news cate-
gory, where the text is more “free form” and less
normative, so it would be expected that sublexical
approaches should have stronger performance.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999282166666667">
This work was made possible with the support
from Department of Computer and Information
Science, Norwegian University of Science and
Technology.
Partha Pakray was 2013–2014 supported by an
ERCIM Alain Bensoussan Fellowship.
The NTNU systems are partly based on code
made available by the Text Analysis and Knowl-
edge Engineering Laboratory, Department of
Electronics, Microelectronics, Computer and In-
telligent Systems, Faculty of Electrical Engineer-
ing and Computing, University of Zagreb.
</bodyText>
<page confidence="0.997591">
452
</page>
<sectionHeader confidence="0.980098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998502897196262">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32–43, Atlanta, Georgia, USA,
June.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242–249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 59–65, At-
lanta, Georgia, USA, June.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297–302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation (SemEval 2012), Montr´eal, Canada,
7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
Montr´eal, Canada, 7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Sim-
ilarity, Atlanta, Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, Atlanta, Georgia, USA, June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,
Bj¨orn Gamb¨ack, and Andr´e Lynum. 2013. NTNU-
CORE: Combining strong features for semantic sim-
ilarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 66–73, At-
lanta, Georgia, USA, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Radim &amp;quot;Reh˚u&amp;quot;rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50, Valletta, Malta, May. ELRA.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327–352, July.
Vladimir Vapnik, Steven E. Golowich, and Alex
Smola. 1997. Support vector method for function
approximation, regression estimation, and signal
processing. In Michael C. Mozer, Michael I. Jordan,
and Thomas Petsche, editors, Advances in Neural
Information Processing Systems, volume 9, pages
281–287. MIT Press, Cambridge, Massachusetts.
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441–448,
Montr´eal, Canada, 7-8 June.
</reference>
<page confidence="0.999375">
453
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945015">
<title confidence="0.9887015">NTNU: Measuring Semantic Similarity Sublexical Feature Representations and Soft Cardinality</title>
<author confidence="0.999781">Andr´e Lynum</author>
<author confidence="0.999781">Partha Pakray</author>
<author confidence="0.999781">Bj¨orn Gamb¨ack Sergio Jimenez</author>
<affiliation confidence="0.999865">Norwegian University of Science and Technology Universidad Nacional de Colombia</affiliation>
<address confidence="0.987187">Trondheim, Norway Bogot´a, Colombia</address>
<abstract confidence="0.998618466666667">The paper describes the approaches taken by the NTNU team to the SemEval 2014 Semantic Textual Similarity shared task. The solutions combine measures based on lexical soft cardinality and character n-gram feature representations with lexical distance metrics from TakeLab’s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>SEM 2013 shared task: Semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>32--43</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1379" citStr="Agirre et al., 2013" startWordPosition="191" endWordPosition="194">hows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets). 1 Introduction The Semantic Textual Similarity (STS) shared task aims at providing a unified framework for evaluating textual semantic similarity, ranging from exact semantic equivalence to completely unrelated texts. This is represented by the prediction of a similarity score between two sentences, drawn from a particular category of text, which ranges from 0 (different topics) to 5 (exactly equivalent) through six grades of semantic similarity (Agirre et al., 2013). This paper describes the NTNU submission to the SemEval 2014 STS shared task (Task 10). The approach is based on the lexical and distributional features of the baseline TakeLab system from the 2012 shared task (ˇSari´c et al., 2012), but improves on it in three ways: by adding two new categories of features and by using a bagging regression model to predict similarity scores. The new feature categories added are based on soft cardinality and character n-grams, described This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer a</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32–43, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python.</booktitle>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="9157" citStr="Bird et al. (2009)" startWordPosition="1443" endWordPosition="1446">m the STS 2012 and STS 2013 task, as shown in Table 2. The character n-gram representation vectors were trained in an unsupervised manner on two subsets of Wikipedia consisting, respectively, of the first 12 million words (108 characters, hence referred to as Wiki8) and of 125 million words (109 characters; Wiki9). First, however, the training data had to be preprocessed. Thus, before extracting the idf weights and the soft cardinality features, all the texts shown in Table 2 were passed through the following four pre-processing steps: (i) tokenization and stop-word removal (provided by NLTK, Bird et al. (2009)),1 (ii) conversion to lowercase characters, (iii) punctuation and special character removal (e.g., “.”, “;”, “$”, “&amp;”), and (iv) Porter stemming. Character n-grams including whitespace were generated from the Wikipedia texts, which in contrast only were pre-processed in a 3-step chain: (i) removal of punctuation and extra whitespace, (ii) replacing numbers with their single digit word (‘one’, ‘two’, etc.), and (iii) lowercasing all text. 1http://www.nltk.org/ Data α Q bias p α&apos; Q&apos; bias&apos; deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63 deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02 headlines 0.3</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6747" citStr="Brown et al., 1992" startWordPosition="1060" endWordPosition="1063">|B| |A U B |− |A n B ||A∩B|/|A∪B| |A|/|A∪B |(|A∪B|−|A∩B|)/|A∩B| NB: in this table only, |* |is short for |* |sim Table 1: Soft cardinality features. document vectors, here the centroid of the individual term vector representations, which are trained on character n-grams rather than full words. The vector representations are induced in an unsupervised manner from large unannotated corpora using word clustering, topic learning and word representation learning methods. In this paper, three different methods have been used for creating the character n-gram feature representations: Brown Clusters (Brown et al., 1992), Latent Semantic Indexing (LSI) topics (Deerwester et al., 1990), and log linear skip-gram models (Mikolov et al., 2013). The Brown clusters were trained using the implementation by Liang (2005), while the LSI topic vectors and log linear skip-gram representations were trained using the Gensim topic modelling framework (ˇReh˚uˇrek and Sojka, 2010). In addition, tf-idf (Term-Frequency Inverse Document Frequency) weighting was used when training LSI topic models. We used a cosine distance measure between document vectors consisting of the centroid of the term representation vectors. For Brown c</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Valerio Storch</author>
<author>P Annesi</author>
<author>Roberto Basili</author>
</authors>
<title>Distributional compositional semantics and text similarity.</title>
<date>2012</date>
<booktitle>In 2012 IEEE Sixth International Conference on Semantic Computing (ICSC),</booktitle>
<pages>242--249</pages>
<contexts>
<context position="17847" citStr="Croce et al. (2012)" startWordPosition="2870" endWordPosition="2873"> it had not been for its sub-par performance on the tweet-news dataset, which on the other hand is the category NTNU-run2 was the best of all systems on. The system based solely on soft cardinality features, NTNU-run1, displays more modest performance ranking at 21st place (of the in total 38 submitted systems) with -0.66 correlation. This is a bit surprising, since this method for obtaining features from pairs of texts was used successfully in other SemEval tasks such as cross-lingual textual entailment (Jimenez et al., 2012b) and student response analysis (Jimenez et al., 2013b). Similarly, Croce et al. (2012) used soft cardinality representing text as a bag of dependencies (syntactic soft cardinality) obtaining the best results in the typedsimilarity task (Croce et al., 2013). From our results it can be noted that for most categories the sublexical representation measures show strong performance in NTNU-run2, with a significantly better result for the combined system NTNU-run3. This indicates that while the soft cardinality features are weaker predictors overall, they are complimentary to the sublexical and lexical features of NTNU-run2. It is also indicative that this is not the case for the twee</context>
</contexts>
<marker>Croce, Storch, Annesi, Basili, 2012</marker>
<rawString>Danilo Croce, Valerio Storch, P. Annesi, and Roberto Basili. 2012. Distributional compositional semantics and text similarity. In 2012 IEEE Sixth International Conference on Semantic Computing (ICSC), pages 242–249, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Valerio Storch</author>
<author>Roberto Basili</author>
</authors>
<title>UNITOR-CORE TYPED: Combining text similarity and semantic filters through SV regression.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>59--65</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="18017" citStr="Croce et al., 2013" startWordPosition="2897" endWordPosition="2900"> solely on soft cardinality features, NTNU-run1, displays more modest performance ranking at 21st place (of the in total 38 submitted systems) with -0.66 correlation. This is a bit surprising, since this method for obtaining features from pairs of texts was used successfully in other SemEval tasks such as cross-lingual textual entailment (Jimenez et al., 2012b) and student response analysis (Jimenez et al., 2013b). Similarly, Croce et al. (2012) used soft cardinality representing text as a bag of dependencies (syntactic soft cardinality) obtaining the best results in the typedsimilarity task (Croce et al., 2013). From our results it can be noted that for most categories the sublexical representation measures show strong performance in NTNU-run2, with a significantly better result for the combined system NTNU-run3. This indicates that while the soft cardinality features are weaker predictors overall, they are complimentary to the sublexical and lexical features of NTNU-run2. It is also indicative that this is not the case for the tweet-news category, where the text is more “free form” and less normative, so it would be expected that sublexical approaches should have stronger performance. Acknowledgeme</context>
</contexts>
<marker>Croce, Storch, Basili, 2013</marker>
<rawString>Danilo Croce, Valerio Storch, and Roberto Basili. 2013. UNITOR-CORE TYPED: Combining text similarity and semantic filters through SV regression. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 59–65, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="6812" citStr="Deerwester et al., 1990" startWordPosition="1069" endWordPosition="1072">∩B| NB: in this table only, |* |is short for |* |sim Table 1: Soft cardinality features. document vectors, here the centroid of the individual term vector representations, which are trained on character n-grams rather than full words. The vector representations are induced in an unsupervised manner from large unannotated corpora using word clustering, topic learning and word representation learning methods. In this paper, three different methods have been used for creating the character n-gram feature representations: Brown Clusters (Brown et al., 1992), Latent Semantic Indexing (LSI) topics (Deerwester et al., 1990), and log linear skip-gram models (Mikolov et al., 2013). The Brown clusters were trained using the implementation by Liang (2005), while the LSI topic vectors and log linear skip-gram representations were trained using the Gensim topic modelling framework (ˇReh˚uˇrek and Sojka, 2010). In addition, tf-idf (Term-Frequency Inverse Document Frequency) weighting was used when training LSI topic models. We used a cosine distance measure between document vectors consisting of the centroid of the term representation vectors. For Brown clusters, the normalized term frequency vectors were used with the</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Fabio Gonzalez</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Text comparison using soft cardinality.</title>
<date>2010</date>
<booktitle>String Processing and Information Retrieval,</booktitle>
<volume>6393</volume>
<pages>297--302</pages>
<editor>In Edgar Chavez and Stefano Lonardi, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2580" citStr="Jimenez et al., 2010" startWordPosition="381" endWordPosition="384">proceedings footer are added by the organisers. Licence details:http://creativecommons.org/licenses/by/4.0/ in Section 2. The parameters of the two categories are optimised over several corpora and the features are combined through support vector regression (Section 3) to create the actual systems (Section 4). As Section 5 shows, the new measures give the baseline system a substantial boost, leading to very competitive results in the shared task evaluation. 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster f</context>
</contexts>
<marker>Jimenez, Gonzalez, Gelbukh, 2010</marker>
<rawString>Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh. 2010. Text comparison using soft cardinality. In Edgar Chavez and Stefano Lonardi, editors, String Processing and Information Retrieval, volume 6393 of LNCS, pages 297–302. Springer, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft cardinality: A parameterized similarity function for text comparison.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012),</booktitle>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2670" citStr="Jimenez et al., 2012" startWordPosition="396" endWordPosition="399">licenses/by/4.0/ in Section 2. The parameters of the two categories are optimised over several corpora and the features are combined through support vector regression (Section 3) to create the actual systems (Section 4). As Section 5 shows, the new measures give the baseline system a substantial boost, leading to very competitive results in the shared task evaluation. 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsu</context>
<context position="17759" citStr="Jimenez et al., 2012" startWordPosition="2856" endWordPosition="2859"> and images, #2 on deft-news, and #4 on OnWN. It would thus have been the clear winner if it had not been for its sub-par performance on the tweet-news dataset, which on the other hand is the category NTNU-run2 was the best of all systems on. The system based solely on soft cardinality features, NTNU-run1, displays more modest performance ranking at 21st place (of the in total 38 submitted systems) with -0.66 correlation. This is a bit surprising, since this method for obtaining features from pairs of texts was used successfully in other SemEval tasks such as cross-lingual textual entailment (Jimenez et al., 2012b) and student response analysis (Jimenez et al., 2013b). Similarly, Croce et al. (2012) used soft cardinality representing text as a bag of dependencies (syntactic soft cardinality) obtaining the best results in the typedsimilarity task (Croce et al., 2013). From our results it can be noted that for most categories the sublexical representation measures show strong performance in NTNU-run2, with a significantly better result for the combined system NTNU-run3. This indicates that while the soft cardinality features are weaker predictors overall, they are complimentary to the sublexical and lex</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012a. Soft cardinality: A parameterized similarity function for text comparison. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft cardinality+ ML: Learning adaptive similarity functions for cross-lingual textual entailment.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012),</booktitle>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2670" citStr="Jimenez et al., 2012" startWordPosition="396" endWordPosition="399">licenses/by/4.0/ in Section 2. The parameters of the two categories are optimised over several corpora and the features are combined through support vector regression (Section 3) to create the actual systems (Section 4). As Section 5 shows, the new measures give the baseline system a substantial boost, leading to very competitive results in the shared task evaluation. 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsu</context>
<context position="17759" citStr="Jimenez et al., 2012" startWordPosition="2856" endWordPosition="2859"> and images, #2 on deft-news, and #4 on OnWN. It would thus have been the clear winner if it had not been for its sub-par performance on the tweet-news dataset, which on the other hand is the category NTNU-run2 was the best of all systems on. The system based solely on soft cardinality features, NTNU-run1, displays more modest performance ranking at 21st place (of the in total 38 submitted systems) with -0.66 correlation. This is a bit surprising, since this method for obtaining features from pairs of texts was used successfully in other SemEval tasks such as cross-lingual textual entailment (Jimenez et al., 2012b) and student response analysis (Jimenez et al., 2013b). Similarly, Croce et al. (2012) used soft cardinality representing text as a bag of dependencies (syntactic soft cardinality) obtaining the best results in the typedsimilarity task (Croce et al., 2013). From our results it can be noted that for most categories the sublexical representation measures show strong performance in NTNU-run2, with a significantly better result for the combined system NTNU-run3. This indicates that while the soft cardinality features are weaker predictors overall, they are complimentary to the sublexical and lex</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012b. Soft cardinality+ ML: Learning adaptive similarity functions for cross-lingual textual entailment. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>SOFTCARDINALITY-CORE: Improving text overlap with distributional measures for semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2693" citStr="Jimenez et al., 2013" startWordPosition="400" endWordPosition="403">tion 2. The parameters of the two categories are optimised over several corpora and the features are combined through support vector regression (Section 3) to create the actual systems (Section 4). As Section 5 shows, the new measures give the baseline system a substantial boost, leading to very competitive results in the shared task evaluation. 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsupervised manner from te</context>
<context position="4363" citStr="Jimenez et al., 2013" startWordPosition="666" endWordPosition="669">mong elements are being considered for the “soft counting”. The soft cardinality of a set of words 448 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448–453, Dublin, Ireland, August 23-24, 2014. A = {a1, a2,.., a|A|} (a sentence) is defined by: Where p is a parameter that controls the cardinality’s softness (p’s default value is 1) and wai are weights for each word, obtained through inverse document frequency (idf) weighting. sim(ai, aj) is a similarity function that compares two words ai and aj using the symmetrized Tversky’s index (Tversky, 1977; Jimenez et al., 2013a) representing them as sets of 3-grams of characters. That is, ai = {ai,1, ai,2, ..., ai,|ai|} where ai,n is the nth character trigram in the word ai in A. Thus, the proposed word-to-word similarity is given by: |c| sim(ai, aj)= β(α|amin|+(1−α)|amax|)+|c |(2) { |c |= |ai ∩ aj |+ biassim |amin |= min {|ai \ aj|, |aj \ ai|} |amax |= max {|ai \aj|, |aj \ ai|} The sim function is equivalent to the Dice’s coefficient if the three parameters are given their default values, namely α = 0.5, β = 1 and bias = 0. The soft cardinalities of any pair of sentences A, B and A∪B can be obtained using Eq. 1. T</context>
<context position="17813" citStr="Jimenez et al., 2013" startWordPosition="2865" endWordPosition="2868"> thus have been the clear winner if it had not been for its sub-par performance on the tweet-news dataset, which on the other hand is the category NTNU-run2 was the best of all systems on. The system based solely on soft cardinality features, NTNU-run1, displays more modest performance ranking at 21st place (of the in total 38 submitted systems) with -0.66 correlation. This is a bit surprising, since this method for obtaining features from pairs of texts was used successfully in other SemEval tasks such as cross-lingual textual entailment (Jimenez et al., 2012b) and student response analysis (Jimenez et al., 2013b). Similarly, Croce et al. (2012) used soft cardinality representing text as a bag of dependencies (syntactic soft cardinality) obtaining the best results in the typedsimilarity task (Croce et al., 2013). From our results it can be noted that for most categories the sublexical representation measures show strong performance in NTNU-run2, with a significantly better result for the combined system NTNU-run3. This indicates that while the soft cardinality features are weaker predictors overall, they are complimentary to the sublexical and lexical features of NTNU-run2. It is also indicative that</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2013</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2013a. SOFTCARDINALITY-CORE: Improving text overlap with distributional measures for semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>SOFTCARDINALITY: Hierarchical text overlap for student response analysis.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2693" citStr="Jimenez et al., 2013" startWordPosition="400" endWordPosition="403">tion 2. The parameters of the two categories are optimised over several corpora and the features are combined through support vector regression (Section 3) to create the actual systems (Section 4). As Section 5 shows, the new measures give the baseline system a substantial boost, leading to very competitive results in the shared task evaluation. 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsupervised manner from te</context>
<context position="4363" citStr="Jimenez et al., 2013" startWordPosition="666" endWordPosition="669">mong elements are being considered for the “soft counting”. The soft cardinality of a set of words 448 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448–453, Dublin, Ireland, August 23-24, 2014. A = {a1, a2,.., a|A|} (a sentence) is defined by: Where p is a parameter that controls the cardinality’s softness (p’s default value is 1) and wai are weights for each word, obtained through inverse document frequency (idf) weighting. sim(ai, aj) is a similarity function that compares two words ai and aj using the symmetrized Tversky’s index (Tversky, 1977; Jimenez et al., 2013a) representing them as sets of 3-grams of characters. That is, ai = {ai,1, ai,2, ..., ai,|ai|} where ai,n is the nth character trigram in the word ai in A. Thus, the proposed word-to-word similarity is given by: |c| sim(ai, aj)= β(α|amin|+(1−α)|amax|)+|c |(2) { |c |= |ai ∩ aj |+ biassim |amin |= min {|ai \ aj|, |aj \ ai|} |amax |= max {|ai \aj|, |aj \ ai|} The sim function is equivalent to the Dice’s coefficient if the three parameters are given their default values, namely α = 0.5, β = 1 and bias = 0. The soft cardinalities of any pair of sentences A, B and A∪B can be obtained using Eq. 1. T</context>
<context position="17813" citStr="Jimenez et al., 2013" startWordPosition="2865" endWordPosition="2868"> thus have been the clear winner if it had not been for its sub-par performance on the tweet-news dataset, which on the other hand is the category NTNU-run2 was the best of all systems on. The system based solely on soft cardinality features, NTNU-run1, displays more modest performance ranking at 21st place (of the in total 38 submitted systems) with -0.66 correlation. This is a bit surprising, since this method for obtaining features from pairs of texts was used successfully in other SemEval tasks such as cross-lingual textual entailment (Jimenez et al., 2012b) and student response analysis (Jimenez et al., 2013b). Similarly, Croce et al. (2012) used soft cardinality representing text as a bag of dependencies (syntactic soft cardinality) obtaining the best results in the typedsimilarity task (Croce et al., 2013). From our results it can be noted that for most categories the sublexical representation measures show strong performance in NTNU-run2, with a significantly better result for the combined system NTNU-run3. This indicates that while the soft cardinality features are weaker predictors overall, they are complimentary to the sublexical and lexical features of NTNU-run2. It is also indicative that</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2013</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2013b. SOFTCARDINALITY: Hierarchical text overlap for student response analysis. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="6942" citStr="Liang (2005)" startWordPosition="1091" endWordPosition="1092"> term vector representations, which are trained on character n-grams rather than full words. The vector representations are induced in an unsupervised manner from large unannotated corpora using word clustering, topic learning and word representation learning methods. In this paper, three different methods have been used for creating the character n-gram feature representations: Brown Clusters (Brown et al., 1992), Latent Semantic Indexing (LSI) topics (Deerwester et al., 1990), and log linear skip-gram models (Mikolov et al., 2013). The Brown clusters were trained using the implementation by Liang (2005), while the LSI topic vectors and log linear skip-gram representations were trained using the Gensim topic modelling framework (ˇReh˚uˇrek and Sojka, 2010). In addition, tf-idf (Term-Frequency Inverse Document Frequency) weighting was used when training LSI topic models. We used a cosine distance measure between document vectors consisting of the centroid of the term representation vectors. For Brown clusters, the normalized term frequency vectors were used with the cluster IDs instead of the terms themselves. For LSI topic representations, the tf-idf weighted topic mixture for each term was u</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Hans Moen</author>
<author>Lars Bungum</author>
<author>Gleb Sizov</author>
<author>Bj¨orn Gamb¨ack</author>
<author>Andr´e Lynum</author>
</authors>
<title>NTNUCORE: Combining strong features for semantic similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>66--73</pages>
<location>Atlanta, Georgia, USA,</location>
<marker>Marsi, Moen, Bungum, Sizov, Gamb¨ack, Lynum, 2013</marker>
<rawString>Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bj¨orn Gamb¨ack, and Andr´e Lynum. 2013. NTNUCORE: Combining strong features for semantic similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 66–73, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="6868" citStr="Mikolov et al., 2013" startWordPosition="1078" endWordPosition="1081">1: Soft cardinality features. document vectors, here the centroid of the individual term vector representations, which are trained on character n-grams rather than full words. The vector representations are induced in an unsupervised manner from large unannotated corpora using word clustering, topic learning and word representation learning methods. In this paper, three different methods have been used for creating the character n-gram feature representations: Brown Clusters (Brown et al., 1992), Latent Semantic Indexing (LSI) topics (Deerwester et al., 1990), and log linear skip-gram models (Mikolov et al., 2013). The Brown clusters were trained using the implementation by Liang (2005), while the LSI topic vectors and log linear skip-gram representations were trained using the Gensim topic modelling framework (ˇReh˚uˇrek and Sojka, 2010). In addition, tf-idf (Term-Frequency Inverse Document Frequency) weighting was used when training LSI topic models. We used a cosine distance measure between document vectors consisting of the centroid of the term representation vectors. For Brown clusters, the normalized term frequency vectors were used with the cluster IDs instead of the terms themselves. For LSI to</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="13849" citStr="Pedregosa et al., 2011" startWordPosition="2212" endWordPosition="2215">news headlines), SMT, OnWN, and FNWM (mappings of sense definitions from FrameNet and WordNet). The SVR model was trained as a bagged classifier, that is, for each run, 100 regression models were trained with 80% of the samples and features of the original training set drawn with replacement. The outputs of all models were then averaged into a final prediction. This bagged training procedure adds extra regularization, which can reduce the instability of prediction accuracy between different test data categories. The prediction pipeline was implemented with the Scikit-learn software framework (Pedregosa et al., 2011), and the SVR models were trained with the implementation’s default parameters: cost penalty (C) 1.0, margin (E) 0.1, and RBF precision (γ) 1/|featurecount|. We were unable to improve the performance over these defaults by cross validation parameter search unless the models were trained for specific text categories. Consequently no parameter optimization was performed during training of the final systems. 4 Submitted Systems The three submitted systems consist of one using only the soft cardinality features described in Section 3.2 (NTNU-run1), one system using a baseline set of lexical measur</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim Reh˚urek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<publisher>ELRA.</publisher>
<location>Valletta, Malta,</location>
<marker>Reh˚urek, Sojka, 2010</marker>
<rawString>Radim &amp;quot;Reh˚u&amp;quot;rek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45– 50, Valletta, Malta, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review,</journal>
<volume>84</volume>
<issue>4</issue>
<contexts>
<context position="4341" citStr="Tversky, 1977" startWordPosition="664" endWordPosition="665"> similarities among elements are being considered for the “soft counting”. The soft cardinality of a set of words 448 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448–453, Dublin, Ireland, August 23-24, 2014. A = {a1, a2,.., a|A|} (a sentence) is defined by: Where p is a parameter that controls the cardinality’s softness (p’s default value is 1) and wai are weights for each word, obtained through inverse document frequency (idf) weighting. sim(ai, aj) is a similarity function that compares two words ai and aj using the symmetrized Tversky’s index (Tversky, 1977; Jimenez et al., 2013a) representing them as sets of 3-grams of characters. That is, ai = {ai,1, ai,2, ..., ai,|ai|} where ai,n is the nth character trigram in the word ai in A. Thus, the proposed word-to-word similarity is given by: |c| sim(ai, aj)= β(α|amin|+(1−α)|amax|)+|c |(2) { |c |= |ai ∩ aj |+ biassim |amin |= min {|ai \ aj|, |aj \ ai|} |amax |= max {|ai \aj|, |aj \ ai|} The sim function is equivalent to the Dice’s coefficient if the three parameters are given their default values, namely α = 0.5, β = 1 and bias = 0. The soft cardinalities of any pair of sentences A, B and A∪B can be o</context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Amos Tversky. 1977. Features of similarity. Psychological Review, 84(4):327–352, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
<author>Steven E Golowich</author>
<author>Alex Smola</author>
</authors>
<title>Support vector method for function approximation, regression estimation, and signal processing.</title>
<date>1997</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>9</volume>
<pages>281--287</pages>
<editor>In Michael C. Mozer, Michael I. Jordan, and Thomas Petsche, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="12650" citStr="Vapnik et al., 1997" startWordPosition="2018" endWordPosition="2021">ter 3-, 4- and 5- grams with cluster sizes of resp. 1024, 2048 and 1024. The representations are trained on the Wiki9 corpus with successively increasing frequency cut-offs of 20, 320 and 1200. • LSI topic vectors based on character 4-grams of size 2000. Trained on the Wiki8 corpus using a frequency cut-off of 5. • LSI topic vectors based on character 4-grams of size 1000. Trained on the Wiki9 corpus using a frequency cut-off of 80. 3.4 Similarity Score Regression The final sentence pair similarity score is predicted by a Support Vector Regression (SVR) model with a Radial Basis (RBF) kernel (Vapnik et al., 1997). The model is trained on all the test data for the 2013 STS shared task combined with all the trial and test data of the 2012 STS shared task. The combined dataset hence consists of about 7,500 sentence pairs from nine different text categories: five sets from the annotated data supplied to STS 2012, based on Microsoft Research Paraphrase and Video description corpora (MSRpar and MSvid), statistical machine translation system output (SMTeuroparl and SMTnews), and sense mappings between OntoNotes and WordNet (OnWN); and four sets from the STS 2013 test data: headlines (news headlines), SMT, On</context>
</contexts>
<marker>Vapnik, Golowich, Smola, 1997</marker>
<rawString>Vladimir Vapnik, Steven E. Golowich, and Alex Smola. 1997. Support vector method for function approximation, regression estimation, and signal processing. In Michael C. Mozer, Michael I. Jordan, and Thomas Petsche, editors, Advances in Neural Information Processing Systems, volume 9, pages 281–287. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>441--448</pages>
<location>Montr´eal,</location>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: Systems for measuring semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 441–448, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>