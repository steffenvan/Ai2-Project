<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000694">
<title confidence="0.989571">
Towards the Interpretation of Utterance Sequences in a Dialogue System
</title>
<author confidence="0.99441">
Ingrid Zukerman and Patrick Ye and Kapil Kumar Gupta and Enes Makalic
</author>
<affiliation confidence="0.9941935">
Faculty of Information Technology
Monash University
</affiliation>
<address confidence="0.830884">
Clayton, VICTORIA 3800, Australia
</address>
<email confidence="0.998043">
ingrid@infotech.monash.edu.au, {ye.patrick,kapil.k.gupta,emakalic}@gmail.com
</email>
<sectionHeader confidence="0.996665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820384615385">
This paper describes a probabilistic mech-
anism for the interpretation of sentence se-
quences developed for a spoken dialogue
system mounted on a robotic agent. The
mechanism receives as input a sequence of
sentences, and produces an interpretation
which integrates the interpretations of in-
dividual sentences. For our evaluation, we
collected a corpus of hypothetical requests
to a robot. Our mechanism exhibits good
performance for sentence pairs, but re-
quires further improvements for sentence
sequences.
</bodyText>
<sectionHeader confidence="0.998611" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99856072972973">
DORIS (Dialogue Oriented Roaming Interactive
System) is a spoken dialogue system under devel-
opment, which will eventually be mounted on a
household robot. The focus of our current work is
on DORIS’s language interpretation module called
Scusi?. In this paper, we consider the interpreta-
tion of a sequence of sentences.
People often utter several separate sentences to
convey their wishes, rather than producing a sin-
gle sentence that contains all the relevant informa-
tion (Zweig et al., 2008). For instance, people are
likely to say “Go to my office. Get my mug. It is
on the table.”, instead of “Get my mug on the table
in my office”. This observation, which was val-
idated in our corpus study (Section 4), motivates
the mechanism for the interpretation of a sequence
of sentences presented in this paper. Our mecha-
nism extends our probabilistic process for inter-
preting single spoken utterances (Zukerman et al.,
2008) in that (1) it determines which sentences in
a sequence are related, and if so, combines them
into an integrated interpretation; and (2) it pro-
vides a formulation for estimating the probability
of an interpretation of a sentence sequence, which
supports the selection of the most probable inter-
pretation. Our evaluation demonstrates that our
mechanism performs well in understanding textual
sentence pairs of different length and level of com-
plexity, and highlights particular aspects of our al-
gorithms that require further improvements (Sec-
tion 4).
In the next section, we describe our mechanism
for interpreting a sentence sequence. In Section 3,
we present our formalism for assessing the prob-
ability of an interpretation. The performance of
our system is evaluated in Section 4, followed by
related research and concluding remarks.
</bodyText>
<sectionHeader confidence="0.832906" genericHeader="method">
2 Interpreting a Sequence of Utterances
</sectionHeader>
<bodyText confidence="0.991912875">
Scusi? employs an anytime algorithm to interpret
a sequence of sentences (Algorithm 1). The algo-
rithm generates interpretations until time runs out
(in our case, until a certain number of iterations
has been executed). In Steps 1–5, Algorithm 1
processes each sentence separately according to
the interpretation process for single sentences de-
scribed in (Zukerman et al., 2008).1 Charniak’s
probabilistic parser2 is applied to generate parse
trees for each sentence in the sequence. The parser
produces up to N (= 50) parse trees for each sen-
tence, associating each parse tree with a probabil-
ity. The parse trees for each sentence are then it-
eratively considered in descending order of proba-
bility, and algorithmically mapped into Uninstan-
tiated Concept Graphs (UCGs) — a representa-
</bodyText>
<footnote confidence="0.953945">
1Although DORIS is a spoken dialogue system, our cur-
rent results pertain to textual input only. Hence, we omit the
aspects of our work pertaining to spoken input.
2ftp://ftp.cs.brown.edu/pub/nlparser/
</footnote>
<note confidence="0.882055">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 46–53,
</note>
<affiliation confidence="0.583965">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998845">
46
</page>
<note confidence="0.700594">
Algorithm 1 Interpret a sentence sequence
DECLARATIVE IMPERATIVE
The mug is on the table. Clean it.
Require: Sentences T1, ... , Tn
{ Interpret Sentences }
</note>
<listItem confidence="0.987123166666667">
1: for all sentences Ti do
2: Generate parse trees {Pi}, and UCGs {Ui}
3: Generate candidate modes {Mi}
4: For each identifier j in Ti, generate candi-
date referents {Rij}
5: end for
{ Combine UCGs }
6: while there is time do
7: Get {(U1, M1, R1), ... , (Un, Mn, Rn)} —
a sequence of tuples (one tuple per sen-
tence)
8: Generate {UD}, a sequence of declara-
tive UCGs, by merging the declarative
UCGs in {(Ui, Mi, Ri)} as specified by
their identifier-referent pairs and modes
9: Generate {UI}, a sequence of imperative
UCGs, by merging each imperative UCG
in {(Ui, Mi, Ri)} with declarative UCGs
as specified by their identifier-referent pairs
and modes
10: Generate candidate ICG sequences {IIj } for
the sequence {UI}
11: Select the best sequence of ICGs {II*}
12: end while
</listItem>
<bodyText confidence="0.994585285714286">
tion based on Concept Graphs (Sowa, 1984) —
one parse tree yielding one UCG (but several parse
trees may produce the same UCG). UCGs rep-
resent syntactic information, where the concepts
correspond to the words in the parent parse tree,
and the relations are derived from syntactic in-
formation in the parse tree and prepositions (Fig-
ure 1(a) illustrates UCGs UD and UI generated
from the sentences “The mug is on the table. Clean
it.”).
Our algorithm requires sentence mode (declar-
ative, imperative or interrogative3), and resolved
references to determine how to combine the sen-
tences in a sequence. Sentence mode is obtained
using a classifier trained on part of our corpus
(Section 2.2). The probability distribution for the
referents of each identifier is obtained from the
corpus and from rules derived from (Lappin and
Leass, 1994; Ng et al., 2005) (Section 2.3).
At this point, for each sentence Ti in a sequence,
we have a list of UCGs, a list of modes, and lists
</bodyText>
<footnote confidence="0.8849505">
3Interrogatives are treated as imperatives at present, so in
the remainder of the paper we do not mention interrogatives.
</footnote>
<figure confidence="0.9821325">
(a) Declarative and (b) Merged UCGs (c) Candidate ICGs
imperative UCGs
</figure>
<figureCaption confidence="0.999969">
Figure 1: Combining two sentences
</figureCaption>
<bodyText confidence="0.999672821428571">
of referents (one list for each identifier in the sen-
tence). In Step 7, Algorithm 1 generates a tu-
ple (Ui, Mi, Ri) for each sentence Ti by selecting
from these lists a UCG, a mode and a referent for
each identifier (yielding a list of identifier-referent
pairs). Each element in each (U, M, R) tuple is it-
eratively selected by traversing the appropriate list
in descending order of probability. For instance,
given sentences T1, T2, T3, the top UCG for T1 is
picked first, together with the top mode and the
top identifier-referent pairs for that sentence (like-
wise for T2 and T3); next the second-top UCG is
chosen for T1, but the other elements remain the
same; and so on.
Once the (U, M, R) tuples have been deter-
mined, the UCGs for the declarative sentences
are merged in the order they were given (Step 8).
This is done by first merging a pair of declara-
tive UCGs, then merging the resultant UCG with
the next declarative UCG, and so on. The idea is
that if the declarative sentences have co-referents,
then the information about these co-referents can
be combined into one representation. For exam-
ple, consider the sequence “The mug is on the ta-
ble. It is blue. Find it. The mug is near the phone.
Bring it to me.” Some of the UCG sequences ob-
tained from the declarative sentences (first, second
and fourth) are:
</bodyText>
<equation confidence="0.919777833333333">
{UD1 }1={mug(CLR blue)-
(on-table &amp; near-phone)}
{UD1 }2={mug-(on-table(CLR blue) &amp;
near-phone)}
{UD1 , UD2 }3={mug(CLR blue)-on-table,
mug-near-phone}.4
</equation>
<footnote confidence="0.6941835">
4The different notations are because colour (and size) are
properties of objects, while prepositions indicate relations.
</footnote>
<figure confidence="0.999079148148148">
U D UI
clean
clean clean0
{U }1 , R=&amp;quot;the table&amp;quot; {I1}1
I I {I1}2
1 I
table DEF
mug DEF
on
object
it
table DEF
clean
object
{UI
1 }2 , R=&amp;quot;the mug&amp;quot;
table DEF
mug DEF
object
on
Patient
table02
clean0
Patient
mug03
On
table01
</figure>
<page confidence="0.997307">
47
</page>
<bodyText confidence="0.998804770833334">
The first two sequences contain one declarative
merged UCG, and the third contains two UCGs.
In Step 9, Algorithm 1 considers a UCG for
each imperative sentence in turn, and merges it
with declarative UCGs (which may have resulted
from a merger), as specified by the modes and
identifier-referent pairs of the sentences in ques-
tion. For example, consider the sentence sequence
“Find my mug. It is in my office. Bring it.” One of
the (U, M, R)-tuple sequences for this instruction
set is
{(find-obj-mug-owner-me, imperative, NIL),
(it1-in-office-owner-me, declarative, it1-mug),
(bring-obj-it2, imperative, it2-mug)}.
After merging the first two UCGs (imperative-
declarative), and then the second and third UCGs
(declarative-imperative), we obtain the imperative
UCG sequence{UDU2}:
U1=find-obj-mug-(owner-me &amp;
in-office-owner-me)
U2=bring-obj-mug-(in-office-owner-me).
This process enables Scusi? to iteratively merge
ever-expanding UCGs with subsequent UCGs,
eventually yielding UCG sequences which contain
detailed UCGs that specify an action or object. A
limitation of this merging process is that the infor-
mation about the objects specified in an impera-
tive UCG is not aggregated with the information
about these objects in other imperative UCGs, and
this sometimes can cause the merged imperative
UCGs to be under-specified. This limitation will
be addressed in the immediate future.
After a sequence of imperative UCGs has been
generated, candidate Instantiated Concept Graphs
(ICGs) are proposed for each imperative UCG,
and the most probable ICG sequence is selected
(Steps 10–11 of Algorithm 1). We focus on im-
perative UCGs because they contain the actions
that the robot is required to perform; these actions
incorporate relevant information from declarative
UCGs. ICGs are generated by nominating dif-
ferent instantiated concepts and relations from the
system’s knowledge base as potential realizations
for each concept and relation in a UCG (Zukerman
et al., 2008); each UCG can generate many ICGs.
Since this paper focuses on the generation of UCG
sequences, the generation of ICGs will not be dis-
cussed further.
</bodyText>
<subsectionHeader confidence="0.995543">
2.1 Merging UCGs
</subsectionHeader>
<bodyText confidence="0.999930517241379">
Given tuples (UZ, MZ, RZ) and (Uj, Mj, Rj) where
j &gt; i, pronouns and one-anaphora in Uj are re-
placed with their referent in UZ on the basis of the
set of identifier-referent pairs in Rj (if there is no
referent in UZ for an identifier in Uj, the identifier
is left untouched). UZ and Uj are then merged into
a UCG Um by first finding a node n that is com-
mon to UZ and Uj, and then copying the sub-tree of
Uj whose root is n into a copy of UZ. If more than
one node can be merged, the node (head noun) that
is highest in the Uj structure is used. If one UCG
is declarative and the other imperative, we swap
them if necessary, so that UZ is imperative and Uj
declarative.
For instance, given the sentences “The mug is
on the table. Clean it.” in Figure 1, Step 4 of
Algorithm 1 produces the identifier-referent pairs
{(it, mug), (it, table)}, yielding two intermedi-
ate UCGs for the imperative sentence: (1) clean-
object-mug, and (2) clean-object-table. The first
UCG is merged with a UCG for the declarative
sentence using mug as root node, and the second
UCG is merged using table as root node. This
results in merged UCG sequences (of length 1)
corresponding to “Clean the table” and “Clean the
mug on the table” ({U1 }1 and {U1 }2 respectively
in Figure 1(b), which in turn produce ICG se-
quences {I�1}1 and {Ii}2 in Figure 1(c), among
others).
</bodyText>
<subsectionHeader confidence="0.999214">
2.2 Determining modes
</subsectionHeader>
<bodyText confidence="0.99995225">
We use the MaxEnt classifier5 to determine the
mode of a sentence. The input features to the clas-
sifier (obtained from the highest probability parse
tree for this sentence) are: (1) top parse-tree node;
(2) position and type of the top level phrases under
the top parse-tree node, e.g., (0, NP), (1, VP), (2,
PP); (3) top phrases under the top parse-tree node
reduced to a regular expression, e.g., VP-NP+ to
represent, say, VP NP NP; (4) top VP head – the
head word of the first top level VP; (5) top NP head
– the head word of the first top level NP; (6) first
three tokens in the sentence; and (7) last token in
the sentence. Using leave-one-out cross valida-
tion, this classifier has an accuracy of 97.8% on
the test data — a 30% improvement over the ma-
jority class (imperative) baseline.
</bodyText>
<subsectionHeader confidence="0.997771">
2.3 Resolving references
</subsectionHeader>
<bodyText confidence="0.999552666666667">
Scusi? handles pronouns, one-anaphora and NP
identifiers (e.g., “the book”). At present, we con-
sider only precise matches between NP identifiers
</bodyText>
<footnote confidence="0.997605">
5http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html
</footnote>
<page confidence="0.998647">
48
</page>
<bodyText confidence="0.999572813953489">
and referents, e.g., “the cup” does not match “the
dish”. In the future, we will incorporate similar-
ity scores based on WordNet, e.g., Leacock and
Chodorow’s (1998) scores for approximate lexical
matches; such matches occurred in 4% of our cor-
pus (Section 4).
To reduce the complexity of reference reso-
lution across a sequence of sentences, and the
amount of data required to reliably estimate prob-
abilities (Section 3), we separate our problem into
two parts: (1) identifying the sentence being re-
ferred to, and (2) determining the referent within
that sentence.
Identifying a sentence. Most referents in our
corpus appear in the current, previous or first sen-
tence in a sequence, with a few referents appear-
ing in other sentences (Section 4). Hence, we
have chosen the sentence classes {current, previ-
ous, first, other}. The probability of referring to
a sentence of a particular class from a sentence
in position i is estimated from our corpus, where
i = 1, ... , 5, &gt; 5 (there are only 13 sequences
with more than 5 sentences). We estimate this dis-
tribution for each leave-one-out cross-validation
fold in our evaluation (Section 4).
Determining a referent. We use heuristics
based on those described in (Lappin and Leass,
1994) to classify pronouns (an example of a non-
pronoun usage is “It is ModalAdjective that S”),
and heuristics based on the results obtained in (Ng
et al., 2005) to classify one-anaphora (an exam-
ple of a high-performing feature pattern is “one as
head-noun with NN or CD as Part-of-speech and
no attached of PP”). If a term is classified as a pro-
noun or one-anaphor, then a list of potential ref-
erents is constructed using the head nouns in the
target sentence. We use the values in (Lappin and
Leass, 1994) to assign a score to each anaphor-
referent pair according to the grammatical role of
the referent in the target UCG (obtained from the
highest probability parse tree that is a parent of this
UCG). These scores are then converted to proba-
bilities using a linear mapping function.
</bodyText>
<sectionHeader confidence="0.9973865" genericHeader="method">
3 Estimating the Probability of a Merged
Interpretation
</sectionHeader>
<bodyText confidence="0.982269166666667">
We now present our formulation for estimating the
probability of a sequence of UCGs, which sup-
ports the selection of the most probable sequence.
One sentence. The probability of a UCG gener-
ated from a sentence T is estimated as described
in (Zukerman et al., 2008), resulting in
</bodyText>
<equation confidence="0.967952">
Pr(U|T) a EP Pr(P|T)·Pr(U|P) (1)
</equation>
<bodyText confidence="0.985629363636364">
where T, P and U denote text, parse tree and UCG
respectively. The summation is taken over all pos-
sible parse trees from the text to the UCG, be-
cause a UCG can have more than one ancestor. As
mentioned above, the parser returns an estimate of
Pr(P|T); and Pr(U|P) = 1, since the process of
generating a UCG from a parse tree is determinis-
tic.
A sentence sequence. The probability of an in-
terpretation of a sequence of sentences T1, ... , Tn
is
</bodyText>
<equation confidence="0.9925585">
Pr(U1, ... , Um|T1, ... , Tn) =
Pr(U1, ...,Un,M1, ...,Mn,R1, ...,Rn|T1, ...,Tn)
</equation>
<bodyText confidence="0.9995134">
where m is the number of UCGs in a merged se-
quence.
By making judicious conditional independence
assumptions, and incorporating parse trees into the
formulation, we obtain
</bodyText>
<equation confidence="0.9988705">
Pr(U1, ... , Um|T1, ... , Tn) =
n
Pr(Ui|Ti)·Pr(Mi|Pi, Ti)·Pr(Ri|P1, ... , Pi)
i=1
</equation>
<bodyText confidence="0.998357333333333">
This formulation is independent of the num-
ber of UCGs in a merged sequence generated
by Algorithm 1, thereby supporting the compari-
son of UCG sequences of different lengths (pro-
duced when different numbers of mergers are per-
formed).
Pr(Ui|Ti) is calculated using Equation 1, and
Pr(Mi|Pi, Ti) is obtained as described in Sec-
tion 2.2 (recall that the input features to the clas-
sifier depend on the parse tree and the sentence).
In principle, Pr(Mi|Pi, Ti) and Pr(Ri|P1, ... , Pi)
could be obtained by summing over all parse trees,
as done in Equation 1. However, at present we use
the highest-probability parse tree to simplify our
calculations.
To estimate Pr(Ri|P1, ... , Pi) we assume con-
ditional independence between the identifiers in a
sentence, yielding
</bodyText>
<equation confidence="0.962435">
ki
Pr(Ri|P1, ... , Pi) = H Pr(Rij|P1,... ,Pi)
j=1
</equation>
<bodyText confidence="0.999287">
where ki is the number of identifiers in sentence
i, and Rij is the referent for identifier j in sen-
tence i. As mentioned in Section 2.3, this factor is
</bodyText>
<page confidence="0.998754">
49
</page>
<bodyText confidence="0.999715">
separated into determining a sentence, and deter-
mining a referent in that sentence. We also include
in our formulation the Type of the identifier (pro-
noun, one-anaphor or NP) and sentence position i,
yielding
</bodyText>
<equation confidence="0.588119">
Pr(Rij|P1,... ,Pi) =
Pr(Rij ref NPa in sent b, Type(Rij)|i, P1, ... , Pi)
After additional conditionalization we obtain
Pr(Rij|P1, ... , Pi) =
</equation>
<bodyText confidence="0.98436725">
Pr(Rij ref NPa|Rij ref sent b,Type(Rij),Pi,Pb)x
Pr(Rij ref sent b|Type(Rij), i)xPr(Type(Rij)|Pi)
As seen in Section 2.3, Pr(Type(Rij)|Pi) and
Pr(Rij ref NPa|Rij ref sent b,Type(Rij),Pi,Pb)
are estimated in a rule-based manner, and
Pr(Rij ref sent b|Type(Rij), i) is estimated from
the corpus (recall that we distinguish between
sentence classes, rather than specific sentences).
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999455">
We first describe our experimental set-up, fol-
lowed by our results.
</bodyText>
<subsectionHeader confidence="0.968409">
4.1 Experimental set-up
</subsectionHeader>
<bodyText confidence="0.925716153846154">
We conducted a web-based survey to collect a cor-
pus comprising multi-sentence requests. To this
effect, we presented participants with a scenario
where they are in a meeting room, and they ask
a robot to fetch something from their office. The
idea is that if people cannot see a scene, their in-
structions will be more segmented than if they can
view the scene. The participants were free to de-
cide which object to fetch, and what was in the
office. There were no restrictions on vocabulary
or grammatical form for the requests.
We collected 115 sets of instructions mostly
from different participants (a few people did the
survey more than once).6 The sentence sequences
in our corpus contain between 1 and 9 sentences,
with 74% of the sequences comprising 1 to 3 sen-
tences. Many of the sentences had grammatical
requirements which exceeded the capabilities of
our system. To be able to use these instruction
sets in our evaluation, we made systematic manual
changes to produce sentences that meet our sys-
tem’s grammatical restrictions (in the future, we
6We acknowledge the modest size of our corpus compared
to that of some publicly available corpora, e.g., ATIS. How-
ever, we must generate our own corpus since our task differs
in nature from the tasks where these large corpora are used.
</bodyText>
<figureCaption confidence="0.990137">
Figure 2: Our virtual environment (top view)
</figureCaption>
<bodyText confidence="0.996967666666667">
will relax these restrictions, as required by a de-
ployable system). Below are the main types of
changes we made.
</bodyText>
<listItem confidence="0.9994795">
• Indirect Speech Acts in the form of questions
were changed to imperatives. For instance,
“Can you get my tea?” was changed to “Get
my tea”.
• Conjoined verb phrases or sentences were sep-
arated into individual sentences.
• Composite verbs were simplified, e.g., “I think
I left it on” was changed to “it is on”, and out-
of-vocabulary composite nouns were replaced
by simple nouns or adjectives, e.g., “the diary
is A4 size” to “the diary is big”.
• Conditional sentences were removed.
</listItem>
<bodyText confidence="0.997474913043478">
Table 1 shows two original texts compared with
the corresponding modified texts (the changed
portions in the originals have been italicized).
Our evaluation consists of two experiments:
(1) ICGs for sentence pairs, and (2) UCGs for sen-
tence sequences.
Experiment 1. We extracted 106 sentence pairs
from our corpus — each pair containing one
declarative and one imperative sentence. To eval-
uate the ICGs, we constructed a virtual environ-
ment comprising a main office and a small office
(Figure 2). Furniture and objects were placed in
a manner compatible with what was mentioned in
the requests in our corpus; distractors were also
placed in the virtual space. In total, our environ-
ment contains 183 instantiated concepts (109 of-
fice and household objects, 43 actions and 31 re-
lations). The (x, y, z) coordinates, colour and di-
mensions of these objects were stored in a knowl-
edge base. Since we have two sentences and their
mode is known, no corpus-based information is
used for this experiment, and hence no training is
required.
</bodyText>
<figure confidence="0.994838153846154">
MAIN OFFICE SMALL OFFICE
WINDOW
BOOKCASE
GLASS
CABINET
CHAIR
SIDE
DESK
MAIN DESK
PRINTER TABLE
CABINET
FILING
BOOKCASE
</figure>
<page confidence="0.993193">
50
</page>
<bodyText confidence="0.923242555555556">
Original Get my book “The Wizard of Oz” from my office. It’s green and yellow. It has a picture
of a dog and a girl on it. It’s in my desk drawer on the right side of my desk, the second
drawer down. If it’s not there, it’s somewhere on my shelves that are on the left side of my
office as you face the window.
Modified Get my book from my office. It’s green. It’s in my drawer on the right of my desk.
Original DORIS, I left my mug in my office and I want a coffee. Can you go into my office and get
my mug. It is on top of the cabinet that is on the left side of my desk.
Modified My mug is in my office. Go into my office. Get my mug. It is on top of the cabinet on the
left of my desk.
</bodyText>
<tableCaption confidence="0.99032">
Table 1: Original and modified text
</tableCaption>
<bodyText confidence="0.994550521739131">
Experiment 2. Since UCGs contain only syn-
tactic information, no additional setup was re-
quired. However, for this experiment we need to
train our mode classifier (Section 2.2), and esti-
mate the probability distribution of referring to a
particular sentence in a sequence (Section 2.3).
Owing to the small size of our corpus, we use
leave-one-out cross validation.
For both experiments, Scusi? was set to gener-
ate up to 300 sub-interpretations (including parse
trees, UCGs and ICGs) for each sentence in the
test-set; on average, it took less than 1 second
to go from a text to a UCG. An interpretation
was deemed successful if it correctly represented
the speaker’s intention, which was represented by
an imperative Gold ICG for the first experiment,
and a sequence of imperative Gold UCGs for the
second experiment. These Gold interpretations
were manually constructed by the authors through
consensus-based annotation (Ang et al., 2002). As
mentioned in Section 2, we evaluated only imper-
ative ICGs and UCGs, as they contain the actions
the robot is expected to perform.
</bodyText>
<subsectionHeader confidence="0.689674">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999512925925926">
Table 2 summarizes our results. Column 1 shows
the type of outcome being evaluated (ICGs in Ex-
periment 1, and UCG sequences and individual
UCGs in Experiment 2). The next two columns
display how many sentences had Gold interpreta-
tions whose probability was among the top-1 and
top-3 probabilities. The average rank of the Gold
interpretation appears in Column 4 (“not found”
Gold interpretations are excluded from this rank).
The rank of an interpretation is its position in a
list sorted in descending order of probability (start-
ing from position 0), such that all equiprobable in-
terpretations have the same position. Columns 5
and 6 respectively show the median and 75%-ile
rank of the Gold interpretation. The number of
Gold interpretations that were not found appears in
Column 7, and the total number of requests/UCGs
is shown in the last column.
Experiment 1. As seen in the first row of Ta-
ble 2, the Gold ICG was top ranked in 75.5% of
the cases, and top-3 ranked in 85.8%. The aver-
age rank of 2.17 is mainly due to 7 outliers, which
together with the “not-found” Gold ICG, are due
to PP-attachment issues, e.g., for the sentence pair
“Fetch my phone from my desk. It is near the key-
board.”, the top parses and resultant UCGs have
“near the keyboard” attached to “the desk” (in-
stead of “the phone”). Nonetheless, the top-ranked
interpretation correctly identified the intended ob-
ject and action in 5 of these 7 cases. Median
and 75%-ile results confirm that most of the Gold
ICGs are top ranked.
Experiment 2. As seen in the second row of Ta-
ble 2, the Gold UCG sequence was top ranked for
51.3% of the requests, and top-3 ranked for 53.0%
of the requests. The third row shows that 62.4%
of the individual Gold UCGs were top-ranked,
and 65.4% were top-3 ranked. This indicates that
when Scusi? cannot fully interpret a request, it
can often generate a partially correct interpreta-
tion. As for Experiment 1, the average rank of
3.14 for the Gold UCG sequences is due to out-
liers, several of which were ranked above 30. The
median and 75%-ile results show that when Scusi?
generates the correct interpretation, it tends to be
highly ranked.
Unlike Experiment 1, in Experiment 2 there is
little difference between the top-1 and top-3 re-
sults. A possible explanation is that in Experi-
ment 1, the top-ranked UCG may yield several
probable ICGs, such that the Gold ICG is not top
ranked — a phenomenon that is not observable at
the UCG stage.
Even though Experiment 2 reaches only the
</bodyText>
<page confidence="0.999146">
51
</page>
<tableCaption confidence="0.992803">
Table 2: Scusi?’s interpretation performance
</tableCaption>
<table confidence="0.874959">
# Gold interps. with prob. in Average Median 75%-ile Not Total
top 1 top 3 rank rank rank found #
ICGs 80 (75.5%) 91 (85.8%) 2.17 0 0 1 (0.9%) 106 reqs.
UCG seqs. 59 (51.3%) 61 (53.0%) 3.14 0 1 36 (31.3%) 115 reqs.
UCGs 146 (62.4%) 153 (65.4%) NA NA NA 55 (23.5%) 234 UCGs
</table>
<bodyText confidence="0.996533909090909">
UCG stage, Scusi?’s performance for this exper-
iment is worse than for Experiment 1, as there
are more grounds for uncertainty. Table 2 shows
that 31.3% of Gold UCG sequences and 23.5% of
Gold UCGs were not found. Most of these cases
(as well as the poorly ranked UCG sequences
and UCGs) were due to (1) imperatives with
object specifications (19 sequences), (2) wrong
anaphora resolution (6 sequences), and (3) wrong
PP-attachment (6 sequences). In the near future,
we will refine the merging process to address the
first problem. The second problem occurs mainly
when there are multiple anaphoric references in a
sequence. We propose to include this factor in our
estimation of the probability of referring to a sen-
tence. We intend to alleviate the PP-attachment
problem, which also occurred in Experiment 1,
by interleaving semantic and pragmatic interpreta-
tion of prepositional phrases as done in (Brick and
Scheutz, 2007). The expectation is that this will
improve the rank of candidates which are pragmat-
ically more plausible.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="method">
5 Related Research
</sectionHeader>
<bodyText confidence="0.999988208333334">
This research extends our mechanism for inter-
preting stand-alone utterances (Zukerman et al.,
2008) to the interpretation of sentence sequences.
Our approach may be viewed as an information
state approach (Larsson and Traum, 2000; Becker
et al., 2006), in the sense that sentences may up-
date different informational aspects of other sen-
tences, without requiring a particular “legal” set of
dialogue acts. However, unlike these information
state approaches, ours is probabilistic.
Several researchers have investigated proba-
bilistic approaches to the interpretation of spo-
ken utterances in dialogue systems, e.g., (Pfleger
et al., 2003; Higashinaka et al., 2003; He and
Young, 2003; Gorniak and Roy, 2005; H¨uwel and
Wrede, 2006). Pfleger et al. (2003) and H¨uwel
and Wrede (2006) employ modality fusion to com-
bine hypotheses from different analyzers (linguis-
tic, visual and gesture), and apply a scoring mech-
anism to rank the resultant hypotheses. They dis-
ambiguate referring expressions by choosing the
first object that satisfies a ‘differentiation crite-
rion’, hence their system does not handle situa-
tions where more than one object satisfies this cri-
terion. He and Young (2003) and Gorniak and
Roy (2005) use Hidden Markov Models for the
ASR stage. However, these systems do not han-
dle utterance sequences. Like Scusi?, the system
developed by Higashinaka et al. (2003) maintains
multiple interpretations, but with respect to dia-
logue acts, rather than the propositional content of
sentences. All the above systems employ seman-
tic grammars, while Scusi? uses generic, statisti-
cal tools, and incorporates semantic- and domain-
related information only in the final stage of the
interpretation process. This approach is supported
by the findings reported in (Knight et al., 2001) for
relatively unconstrained utterances by users unfa-
miliar with the system, such as those expected by
DORIS.
Our mechanism is also well suited for process-
ing replies to clarification questions (Horvitz and
Paek, 2000; Bohus and Rudnicky, 2005), since a
reply can be considered an additional sentence to
be incorporated into top-ranked UCG sequences.
Further, our probabilistic output can be used by a
utility-based dialogue manager (Horvitz and Paek,
2000).
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999833083333333">
We have extended Scusi?, our spoken language
interpretation system, to interpret sentence se-
quences. Specifically, we have offered a procedure
that combines the interpretations of the sentences
in a sequence, and presented a formalism for es-
timating the probability of the merged interpre-
tation. This formalism supports the comparison
of interpretations comprising different numbers of
UCGs obtained from different mergers.
Our empirical evaluation shows that Scusi? per-
forms well for textual input corresponding to
(modified) sentence pairs. However, we still need
</bodyText>
<page confidence="0.994537">
52
</page>
<bodyText confidence="0.999971333333333">
to address some issues pertaining to the integra-
tion of UCGs for sentence sequences of arbitrary
length. Thereafter, we propose to investigate the
influence of speech recognition performance on
Scusi?’s performance. In the future, we intend to
expand Scusi?’s grammatical capabilities.
</bodyText>
<sectionHeader confidence="0.997136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998073">
This research was supported in part by grant
DP0878195 from the Australian Research Coun-
cil.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999959307692308">
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and
A. Stolcke. 2002. Prosody-based automatic de-
tection of annoyance and frustration in human-
computer dialog. In ICSLP’2002 – Proceedings of
the 7th International Conference on Spoken Lan-
guage Processing, pages 2037–2040, Denver, Col-
orado.
T. Becker, P. Poller, J. Schehl, N. Blaylock, C. Ger-
stenberger, and I. Kruijff-Korbayov´a. 2006. The
SAMMIE system: Multimodal in-car dialogue. In
Proceedings of the COLING/ACL 2006 Interactive
Presentation Sessions, pages 57–60, Sydney, Aus-
tralia.
D. Bohus and A. Rudnicky. 2005. Constructing accu-
rate beliefs in spoken dialog systems. In ASRU’05
– Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 272–
277, San Juan, Puerto Rico.
T. Brick and M. Scheutz. 2007. Incremental natural
language processing for HRI. In HRI 2007 – Pro-
ceedings of the 2nd ACM/IEEE International Con-
ference on Human-Robot Interaction, pages 263–
270, Washington, D.C.
P. Gorniak and D. Roy. 2005. Probabilistic grounding
of situated speech using plan recognition and refer-
ence resolution. In ICMI’05 – Proceedings of the
7th International Conference on Multimodal Inter-
faces, pages 138–143, Trento, Italy.
Y. He and S. Young. 2003. A data-driven spo-
ken language understanding system. In ASRU’03
– Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 583–
588, St. Thomas, US Virgin Islands.
R. Higashinaka, M. Nakano, and K. Aikawa. 2003.
Corpus-Based discourse understanding in spoken di-
alogue systems. In ACL-2003 – Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 240–247, Sapporo,
Japan.
E. Horvitz and T. Paek. 2000. DeepListener: Har-
nessing expected utility to guide clarification dialog
in spoken language systems. In ICSLP’2000 – Pro-
ceedings of the 6th International Conference on Spo-
ken Language Processing, pages 229–229, Beijing,
China.
S. H¨uwel and B. Wrede. 2006. Spontaneous speech
understanding for robust multi-modal human-robot
communication. In Proceedings of the COL-
ING/ACL Main Conference Poster Sessions, pages
391–398, Sydney, Australia.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: A
case study. In Proceedings of Eurospeech 2001,
pages 1779–1782, Aalborg, Denmark.
S. Lappin and H.J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20:535–561.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dialogue
move engine toolkit. Natural Language Engineer-
ing, 6:323–340.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet similarity for word sense
identification. In C. Fellbaum, editor, WordNet: An
Electronic Lexical Database, pages 265–285. MIT
Press.
H.T. Ng, Y. Zhou, R. Dale, and M. Gardiner. 2005.
A machine learning approach to identification and
resolution of one-anaphora. In IJCAI-05 – Proceed-
ings of the 19th International Joint Conference on
Artificial Intelligence, pages 1105–1110, Edinburgh,
Scotland.
N. Pfleger, R. Engel, and J. Alexandersson. 2003. Ro-
bust multimodal discourse processing. In Proceed-
ings of the 7th Workshop on the Semantics and Prag-
matics of Dialogue, pages 107–114, Saarbr¨ucken,
Germany.
J.F. Sowa. 1984. Conceptual Structures: Information
Processing in Mind and Machine. Addison-Wesley,
Reading, MA.
I. Zukerman, E. Makalic, M. Niemann, and S. George.
2008. A probabilistic approach to the interpreta-
tion of spoken utterances. In PRICAI 2008 – Pro-
ceedings of the 10th Pacific Rim International Con-
ference on Artificial Intelligence, pages 581–592,
Hanoi, Vietnam.
G. Zweig, D. Bohus, X. Li, and P. Nguyen. 2008.
Structured models for joint decoding of repeated ut-
terances. In Proceedings of Interspeech 2008, pages
1157–1160, Brisbane, Australia.
</reference>
<page confidence="0.999351">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358898">
<title confidence="0.615868">Towards the Interpretation of Utterance Sequences in a Dialogue System Ingrid Zukerman and Patrick Ye and Kapil Kumar Gupta and Enes Faculty of Information Monash</title>
<address confidence="0.995869">Clayton, VICTORIA 3800, Australia</address>
<abstract confidence="0.996609214285714">This paper describes a probabilistic mechanism for the interpretation of sentence sequences developed for a spoken dialogue system mounted on a robotic agent. The mechanism receives as input a sequence of sentences, and produces an interpretation which integrates the interpretations of individual sentences. For our evaluation, we collected a corpus of hypothetical requests to a robot. Our mechanism exhibits good performance for sentence pairs, but requires further improvements for sentence sequences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in humancomputer dialog.</title>
<date>2002</date>
<booktitle>In ICSLP’2002 – Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<pages>2037--2040</pages>
<location>Denver, Colorado.</location>
<contexts>
<context position="22024" citStr="Ang et al., 2002" startWordPosition="3683" endWordPosition="3686">us, we use leave-one-out cross validation. For both experiments, Scusi? was set to generate up to 300 sub-interpretations (including parse trees, UCGs and ICGs) for each sentence in the test-set; on average, it took less than 1 second to go from a text to a UCG. An interpretation was deemed successful if it correctly represented the speaker’s intention, which was represented by an imperative Gold ICG for the first experiment, and a sequence of imperative Gold UCGs for the second experiment. These Gold interpretations were manually constructed by the authors through consensus-based annotation (Ang et al., 2002). As mentioned in Section 2, we evaluated only imperative ICGs and UCGs, as they contain the actions the robot is expected to perform. 4.2 Results Table 2 summarizes our results. Column 1 shows the type of outcome being evaluated (ICGs in Experiment 1, and UCG sequences and individual UCGs in Experiment 2). The next two columns display how many sentences had Gold interpretations whose probability was among the top-1 and top-3 probabilities. The average rank of the Gold interpretation appears in Column 4 (“not found” Gold interpretations are excluded from this rank). The rank of an interpretati</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in humancomputer dialog. In ICSLP’2002 – Proceedings of the 7th International Conference on Spoken Language Processing, pages 2037–2040, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Becker</author>
<author>P Poller</author>
<author>J Schehl</author>
<author>N Blaylock</author>
<author>C Gerstenberger</author>
<author>I Kruijff-Korbayov´a</author>
</authors>
<title>The SAMMIE system: Multimodal in-car dialogue.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<pages>57--60</pages>
<location>Sydney, Australia.</location>
<marker>Becker, Poller, Schehl, Blaylock, Gerstenberger, Kruijff-Korbayov´a, 2006</marker>
<rawString>T. Becker, P. Poller, J. Schehl, N. Blaylock, C. Gerstenberger, and I. Kruijff-Korbayov´a. 2006. The SAMMIE system: Multimodal in-car dialogue. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57–60, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>Constructing accurate beliefs in spoken dialog systems.</title>
<date>2005</date>
<booktitle>In ASRU’05 – Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>272--277</pages>
<location>San Juan, Puerto Rico.</location>
<contexts>
<context position="28052" citStr="Bohus and Rudnicky, 2005" startWordPosition="4691" endWordPosition="4694">retations, but with respect to dialogue acts, rather than the propositional content of sentences. All the above systems employ semantic grammars, while Scusi? uses generic, statistical tools, and incorporates semantic- and domainrelated information only in the final stage of the interpretation process. This approach is supported by the findings reported in (Knight et al., 2001) for relatively unconstrained utterances by users unfamiliar with the system, such as those expected by DORIS. Our mechanism is also well suited for processing replies to clarification questions (Horvitz and Paek, 2000; Bohus and Rudnicky, 2005), since a reply can be considered an additional sentence to be incorporated into top-ranked UCG sequences. Further, our probabilistic output can be used by a utility-based dialogue manager (Horvitz and Paek, 2000). 6 Conclusion We have extended Scusi?, our spoken language interpretation system, to interpret sentence sequences. Specifically, we have offered a procedure that combines the interpretations of the sentences in a sequence, and presented a formalism for estimating the probability of the merged interpretation. This formalism supports the comparison of interpretations comprising differe</context>
</contexts>
<marker>Bohus, Rudnicky, 2005</marker>
<rawString>D. Bohus and A. Rudnicky. 2005. Constructing accurate beliefs in spoken dialog systems. In ASRU’05 – Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 272– 277, San Juan, Puerto Rico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brick</author>
<author>M Scheutz</author>
</authors>
<title>Incremental natural language processing for HRI.</title>
<date>2007</date>
<booktitle>In HRI 2007 – Proceedings of the 2nd ACM/IEEE International Conference on Human-Robot Interaction,</booktitle>
<pages>263--270</pages>
<location>Washington, D.C.</location>
<contexts>
<context position="25902" citStr="Brick and Scheutz, 2007" startWordPosition="4359" endWordPosition="4362">) imperatives with object specifications (19 sequences), (2) wrong anaphora resolution (6 sequences), and (3) wrong PP-attachment (6 sequences). In the near future, we will refine the merging process to address the first problem. The second problem occurs mainly when there are multiple anaphoric references in a sequence. We propose to include this factor in our estimation of the probability of referring to a sentence. We intend to alleviate the PP-attachment problem, which also occurred in Experiment 1, by interleaving semantic and pragmatic interpretation of prepositional phrases as done in (Brick and Scheutz, 2007). The expectation is that this will improve the rank of candidates which are pragmatically more plausible. 5 Related Research This research extends our mechanism for interpreting stand-alone utterances (Zukerman et al., 2008) to the interpretation of sentence sequences. Our approach may be viewed as an information state approach (Larsson and Traum, 2000; Becker et al., 2006), in the sense that sentences may update different informational aspects of other sentences, without requiring a particular “legal” set of dialogue acts. However, unlike these information state approaches, ours is probabili</context>
</contexts>
<marker>Brick, Scheutz, 2007</marker>
<rawString>T. Brick and M. Scheutz. 2007. Incremental natural language processing for HRI. In HRI 2007 – Proceedings of the 2nd ACM/IEEE International Conference on Human-Robot Interaction, pages 263– 270, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gorniak</author>
<author>D Roy</author>
</authors>
<title>Probabilistic grounding of situated speech using plan recognition and reference resolution.</title>
<date>2005</date>
<booktitle>In ICMI’05 – Proceedings of the 7th International Conference on Multimodal Interfaces,</booktitle>
<pages>138--143</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="26731" citStr="Gorniak and Roy, 2005" startWordPosition="4484" endWordPosition="4487">man et al., 2008) to the interpretation of sentence sequences. Our approach may be viewed as an information state approach (Larsson and Traum, 2000; Becker et al., 2006), in the sense that sentences may update different informational aspects of other sentences, without requiring a particular “legal” set of dialogue acts. However, unlike these information state approaches, ours is probabilistic. Several researchers have investigated probabilistic approaches to the interpretation of spoken utterances in dialogue systems, e.g., (Pfleger et al., 2003; Higashinaka et al., 2003; He and Young, 2003; Gorniak and Roy, 2005; H¨uwel and Wrede, 2006). Pfleger et al. (2003) and H¨uwel and Wrede (2006) employ modality fusion to combine hypotheses from different analyzers (linguistic, visual and gesture), and apply a scoring mechanism to rank the resultant hypotheses. They disambiguate referring expressions by choosing the first object that satisfies a ‘differentiation criterion’, hence their system does not handle situations where more than one object satisfies this criterion. He and Young (2003) and Gorniak and Roy (2005) use Hidden Markov Models for the ASR stage. However, these systems do not handle utterance seq</context>
</contexts>
<marker>Gorniak, Roy, 2005</marker>
<rawString>P. Gorniak and D. Roy. 2005. Probabilistic grounding of situated speech using plan recognition and reference resolution. In ICMI’05 – Proceedings of the 7th International Conference on Multimodal Interfaces, pages 138–143, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>A data-driven spoken language understanding system.</title>
<date>2003</date>
<booktitle>In ASRU’03 – Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>583--588</pages>
<publisher>St. Thomas, US Virgin Islands.</publisher>
<contexts>
<context position="26708" citStr="He and Young, 2003" startWordPosition="4480" endWordPosition="4483">ne utterances (Zukerman et al., 2008) to the interpretation of sentence sequences. Our approach may be viewed as an information state approach (Larsson and Traum, 2000; Becker et al., 2006), in the sense that sentences may update different informational aspects of other sentences, without requiring a particular “legal” set of dialogue acts. However, unlike these information state approaches, ours is probabilistic. Several researchers have investigated probabilistic approaches to the interpretation of spoken utterances in dialogue systems, e.g., (Pfleger et al., 2003; Higashinaka et al., 2003; He and Young, 2003; Gorniak and Roy, 2005; H¨uwel and Wrede, 2006). Pfleger et al. (2003) and H¨uwel and Wrede (2006) employ modality fusion to combine hypotheses from different analyzers (linguistic, visual and gesture), and apply a scoring mechanism to rank the resultant hypotheses. They disambiguate referring expressions by choosing the first object that satisfies a ‘differentiation criterion’, hence their system does not handle situations where more than one object satisfies this criterion. He and Young (2003) and Gorniak and Roy (2005) use Hidden Markov Models for the ASR stage. However, these systems do n</context>
</contexts>
<marker>He, Young, 2003</marker>
<rawString>Y. He and S. Young. 2003. A data-driven spoken language understanding system. In ASRU’03 – Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 583– 588, St. Thomas, US Virgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Higashinaka</author>
<author>M Nakano</author>
<author>K Aikawa</author>
</authors>
<title>Corpus-Based discourse understanding in spoken dialogue systems.</title>
<date>2003</date>
<booktitle>In ACL-2003 – Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>240--247</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="26688" citStr="Higashinaka et al., 2003" startWordPosition="4476" endWordPosition="4479">for interpreting stand-alone utterances (Zukerman et al., 2008) to the interpretation of sentence sequences. Our approach may be viewed as an information state approach (Larsson and Traum, 2000; Becker et al., 2006), in the sense that sentences may update different informational aspects of other sentences, without requiring a particular “legal” set of dialogue acts. However, unlike these information state approaches, ours is probabilistic. Several researchers have investigated probabilistic approaches to the interpretation of spoken utterances in dialogue systems, e.g., (Pfleger et al., 2003; Higashinaka et al., 2003; He and Young, 2003; Gorniak and Roy, 2005; H¨uwel and Wrede, 2006). Pfleger et al. (2003) and H¨uwel and Wrede (2006) employ modality fusion to combine hypotheses from different analyzers (linguistic, visual and gesture), and apply a scoring mechanism to rank the resultant hypotheses. They disambiguate referring expressions by choosing the first object that satisfies a ‘differentiation criterion’, hence their system does not handle situations where more than one object satisfies this criterion. He and Young (2003) and Gorniak and Roy (2005) use Hidden Markov Models for the ASR stage. However</context>
</contexts>
<marker>Higashinaka, Nakano, Aikawa, 2003</marker>
<rawString>R. Higashinaka, M. Nakano, and K. Aikawa. 2003. Corpus-Based discourse understanding in spoken dialogue systems. In ACL-2003 – Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 240–247, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Horvitz</author>
<author>T Paek</author>
</authors>
<title>DeepListener: Harnessing expected utility to guide clarification dialog in spoken language systems.</title>
<date>2000</date>
<booktitle>In ICSLP’2000 – Proceedings of the 6th International Conference on Spoken Language Processing,</booktitle>
<pages>229--229</pages>
<location>Beijing, China.</location>
<contexts>
<context position="28025" citStr="Horvitz and Paek, 2000" startWordPosition="4687" endWordPosition="4690">aintains multiple interpretations, but with respect to dialogue acts, rather than the propositional content of sentences. All the above systems employ semantic grammars, while Scusi? uses generic, statistical tools, and incorporates semantic- and domainrelated information only in the final stage of the interpretation process. This approach is supported by the findings reported in (Knight et al., 2001) for relatively unconstrained utterances by users unfamiliar with the system, such as those expected by DORIS. Our mechanism is also well suited for processing replies to clarification questions (Horvitz and Paek, 2000; Bohus and Rudnicky, 2005), since a reply can be considered an additional sentence to be incorporated into top-ranked UCG sequences. Further, our probabilistic output can be used by a utility-based dialogue manager (Horvitz and Paek, 2000). 6 Conclusion We have extended Scusi?, our spoken language interpretation system, to interpret sentence sequences. Specifically, we have offered a procedure that combines the interpretations of the sentences in a sequence, and presented a formalism for estimating the probability of the merged interpretation. This formalism supports the comparison of interpr</context>
</contexts>
<marker>Horvitz, Paek, 2000</marker>
<rawString>E. Horvitz and T. Paek. 2000. DeepListener: Harnessing expected utility to guide clarification dialog in spoken language systems. In ICSLP’2000 – Proceedings of the 6th International Conference on Spoken Language Processing, pages 229–229, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H¨uwel</author>
<author>B Wrede</author>
</authors>
<title>Spontaneous speech understanding for robust multi-modal human-robot communication.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL Main Conference Poster Sessions,</booktitle>
<pages>391--398</pages>
<location>Sydney, Australia.</location>
<marker>H¨uwel, Wrede, 2006</marker>
<rawString>S. H¨uwel and B. Wrede. 2006. Spontaneous speech understanding for robust multi-modal human-robot communication. In Proceedings of the COLING/ACL Main Conference Poster Sessions, pages 391–398, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Knight</author>
<author>G Gorrell</author>
<author>M Rayner</author>
<author>D Milward</author>
<author>R Koeling</author>
<author>I Lewin</author>
</authors>
<title>Comparing grammar-based and robust approaches to speech understanding: A case study.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<pages>1779--1782</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="27807" citStr="Knight et al., 2001" startWordPosition="4653" endWordPosition="4656">n. He and Young (2003) and Gorniak and Roy (2005) use Hidden Markov Models for the ASR stage. However, these systems do not handle utterance sequences. Like Scusi?, the system developed by Higashinaka et al. (2003) maintains multiple interpretations, but with respect to dialogue acts, rather than the propositional content of sentences. All the above systems employ semantic grammars, while Scusi? uses generic, statistical tools, and incorporates semantic- and domainrelated information only in the final stage of the interpretation process. This approach is supported by the findings reported in (Knight et al., 2001) for relatively unconstrained utterances by users unfamiliar with the system, such as those expected by DORIS. Our mechanism is also well suited for processing replies to clarification questions (Horvitz and Paek, 2000; Bohus and Rudnicky, 2005), since a reply can be considered an additional sentence to be incorporated into top-ranked UCG sequences. Further, our probabilistic output can be used by a utility-based dialogue manager (Horvitz and Paek, 2000). 6 Conclusion We have extended Scusi?, our spoken language interpretation system, to interpret sentence sequences. Specifically, we have offe</context>
</contexts>
<marker>Knight, Gorrell, Rayner, Milward, Koeling, Lewin, 2001</marker>
<rawString>S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koeling, and I. Lewin. 2001. Comparing grammar-based and robust approaches to speech understanding: A case study. In Proceedings of Eurospeech 2001, pages 1779–1782, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--535</pages>
<contexts>
<context position="5600" citStr="Lappin and Leass, 1994" startWordPosition="884" endWordPosition="887"> the parent parse tree, and the relations are derived from syntactic information in the parse tree and prepositions (Figure 1(a) illustrates UCGs UD and UI generated from the sentences “The mug is on the table. Clean it.”). Our algorithm requires sentence mode (declarative, imperative or interrogative3), and resolved references to determine how to combine the sentences in a sequence. Sentence mode is obtained using a classifier trained on part of our corpus (Section 2.2). The probability distribution for the referents of each identifier is obtained from the corpus and from rules derived from (Lappin and Leass, 1994; Ng et al., 2005) (Section 2.3). At this point, for each sentence Ti in a sequence, we have a list of UCGs, a list of modes, and lists 3Interrogatives are treated as imperatives at present, so in the remainder of the paper we do not mention interrogatives. (a) Declarative and (b) Merged UCGs (c) Candidate ICGs imperative UCGs Figure 1: Combining two sentences of referents (one list for each identifier in the sentence). In Step 7, Algorithm 1 generates a tuple (Ui, Mi, Ri) for each sentence Ti by selecting from these lists a UCG, a mode and a referent for each identifier (yielding a list of id</context>
<context position="13546" citStr="Lappin and Leass, 1994" startWordPosition="2220" endWordPosition="2223"> corpus appear in the current, previous or first sentence in a sequence, with a few referents appearing in other sentences (Section 4). Hence, we have chosen the sentence classes {current, previous, first, other}. The probability of referring to a sentence of a particular class from a sentence in position i is estimated from our corpus, where i = 1, ... , 5, &gt; 5 (there are only 13 sequences with more than 5 sentences). We estimate this distribution for each leave-one-out cross-validation fold in our evaluation (Section 4). Determining a referent. We use heuristics based on those described in (Lappin and Leass, 1994) to classify pronouns (an example of a nonpronoun usage is “It is ModalAdjective that S”), and heuristics based on the results obtained in (Ng et al., 2005) to classify one-anaphora (an example of a high-performing feature pattern is “one as head-noun with NN or CD as Part-of-speech and no attached of PP”). If a term is classified as a pronoun or one-anaphor, then a list of potential referents is constructed using the head nouns in the target sentence. We use the values in (Lappin and Leass, 1994) to assign a score to each anaphorreferent pair according to the grammatical role of the referent </context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>S. Lappin and H.J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20:535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
<author>D Traum</author>
</authors>
<title>Information state and dialogue management in the TRINDI dialogue move engine toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--323</pages>
<contexts>
<context position="26257" citStr="Larsson and Traum, 2000" startWordPosition="4413" endWordPosition="4416"> in our estimation of the probability of referring to a sentence. We intend to alleviate the PP-attachment problem, which also occurred in Experiment 1, by interleaving semantic and pragmatic interpretation of prepositional phrases as done in (Brick and Scheutz, 2007). The expectation is that this will improve the rank of candidates which are pragmatically more plausible. 5 Related Research This research extends our mechanism for interpreting stand-alone utterances (Zukerman et al., 2008) to the interpretation of sentence sequences. Our approach may be viewed as an information state approach (Larsson and Traum, 2000; Becker et al., 2006), in the sense that sentences may update different informational aspects of other sentences, without requiring a particular “legal” set of dialogue acts. However, unlike these information state approaches, ours is probabilistic. Several researchers have investigated probabilistic approaches to the interpretation of spoken utterances in dialogue systems, e.g., (Pfleger et al., 2003; Higashinaka et al., 2003; He and Young, 2003; Gorniak and Roy, 2005; H¨uwel and Wrede, 2006). Pfleger et al. (2003) and H¨uwel and Wrede (2006) employ modality fusion to combine hypotheses from</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>S. Larsson and D. Traum. 2000. Information state and dialogue management in the TRINDI dialogue move engine toolkit. Natural Language Engineering, 6:323–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database,</booktitle>
<pages>265--285</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 265–285. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>Y Zhou</author>
<author>R Dale</author>
<author>M Gardiner</author>
</authors>
<title>A machine learning approach to identification and resolution of one-anaphora.</title>
<date>2005</date>
<booktitle>In IJCAI-05 – Proceedings of the 19th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1105--1110</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="5618" citStr="Ng et al., 2005" startWordPosition="888" endWordPosition="891">and the relations are derived from syntactic information in the parse tree and prepositions (Figure 1(a) illustrates UCGs UD and UI generated from the sentences “The mug is on the table. Clean it.”). Our algorithm requires sentence mode (declarative, imperative or interrogative3), and resolved references to determine how to combine the sentences in a sequence. Sentence mode is obtained using a classifier trained on part of our corpus (Section 2.2). The probability distribution for the referents of each identifier is obtained from the corpus and from rules derived from (Lappin and Leass, 1994; Ng et al., 2005) (Section 2.3). At this point, for each sentence Ti in a sequence, we have a list of UCGs, a list of modes, and lists 3Interrogatives are treated as imperatives at present, so in the remainder of the paper we do not mention interrogatives. (a) Declarative and (b) Merged UCGs (c) Candidate ICGs imperative UCGs Figure 1: Combining two sentences of referents (one list for each identifier in the sentence). In Step 7, Algorithm 1 generates a tuple (Ui, Mi, Ri) for each sentence Ti by selecting from these lists a UCG, a mode and a referent for each identifier (yielding a list of identifier-referent </context>
<context position="13702" citStr="Ng et al., 2005" startWordPosition="2248" endWordPosition="2251">entence classes {current, previous, first, other}. The probability of referring to a sentence of a particular class from a sentence in position i is estimated from our corpus, where i = 1, ... , 5, &gt; 5 (there are only 13 sequences with more than 5 sentences). We estimate this distribution for each leave-one-out cross-validation fold in our evaluation (Section 4). Determining a referent. We use heuristics based on those described in (Lappin and Leass, 1994) to classify pronouns (an example of a nonpronoun usage is “It is ModalAdjective that S”), and heuristics based on the results obtained in (Ng et al., 2005) to classify one-anaphora (an example of a high-performing feature pattern is “one as head-noun with NN or CD as Part-of-speech and no attached of PP”). If a term is classified as a pronoun or one-anaphor, then a list of potential referents is constructed using the head nouns in the target sentence. We use the values in (Lappin and Leass, 1994) to assign a score to each anaphorreferent pair according to the grammatical role of the referent in the target UCG (obtained from the highest probability parse tree that is a parent of this UCG). These scores are then converted to probabilities using a </context>
</contexts>
<marker>Ng, Zhou, Dale, Gardiner, 2005</marker>
<rawString>H.T. Ng, Y. Zhou, R. Dale, and M. Gardiner. 2005. A machine learning approach to identification and resolution of one-anaphora. In IJCAI-05 – Proceedings of the 19th International Joint Conference on Artificial Intelligence, pages 1105–1110, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pfleger</author>
<author>R Engel</author>
<author>J Alexandersson</author>
</authors>
<title>Robust multimodal discourse processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>107--114</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="26662" citStr="Pfleger et al., 2003" startWordPosition="4472" endWordPosition="4475">extends our mechanism for interpreting stand-alone utterances (Zukerman et al., 2008) to the interpretation of sentence sequences. Our approach may be viewed as an information state approach (Larsson and Traum, 2000; Becker et al., 2006), in the sense that sentences may update different informational aspects of other sentences, without requiring a particular “legal” set of dialogue acts. However, unlike these information state approaches, ours is probabilistic. Several researchers have investigated probabilistic approaches to the interpretation of spoken utterances in dialogue systems, e.g., (Pfleger et al., 2003; Higashinaka et al., 2003; He and Young, 2003; Gorniak and Roy, 2005; H¨uwel and Wrede, 2006). Pfleger et al. (2003) and H¨uwel and Wrede (2006) employ modality fusion to combine hypotheses from different analyzers (linguistic, visual and gesture), and apply a scoring mechanism to rank the resultant hypotheses. They disambiguate referring expressions by choosing the first object that satisfies a ‘differentiation criterion’, hence their system does not handle situations where more than one object satisfies this criterion. He and Young (2003) and Gorniak and Roy (2005) use Hidden Markov Models </context>
</contexts>
<marker>Pfleger, Engel, Alexandersson, 2003</marker>
<rawString>N. Pfleger, R. Engel, and J. Alexandersson. 2003. Robust multimodal discourse processing. In Proceedings of the 7th Workshop on the Semantics and Pragmatics of Dialogue, pages 107–114, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Sowa</author>
</authors>
<date>1984</date>
<booktitle>Conceptual Structures: Information Processing in Mind and Machine.</booktitle>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="4808" citStr="Sowa, 1984" startWordPosition="756" endWordPosition="757">ile there is time do 7: Get {(U1, M1, R1), ... , (Un, Mn, Rn)} — a sequence of tuples (one tuple per sentence) 8: Generate {UD}, a sequence of declarative UCGs, by merging the declarative UCGs in {(Ui, Mi, Ri)} as specified by their identifier-referent pairs and modes 9: Generate {UI}, a sequence of imperative UCGs, by merging each imperative UCG in {(Ui, Mi, Ri)} with declarative UCGs as specified by their identifier-referent pairs and modes 10: Generate candidate ICG sequences {IIj } for the sequence {UI} 11: Select the best sequence of ICGs {II*} 12: end while tion based on Concept Graphs (Sowa, 1984) — one parse tree yielding one UCG (but several parse trees may produce the same UCG). UCGs represent syntactic information, where the concepts correspond to the words in the parent parse tree, and the relations are derived from syntactic information in the parse tree and prepositions (Figure 1(a) illustrates UCGs UD and UI generated from the sentences “The mug is on the table. Clean it.”). Our algorithm requires sentence mode (declarative, imperative or interrogative3), and resolved references to determine how to combine the sentences in a sequence. Sentence mode is obtained using a classifie</context>
</contexts>
<marker>Sowa, 1984</marker>
<rawString>J.F. Sowa. 1984. Conceptual Structures: Information Processing in Mind and Machine. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zukerman</author>
<author>E Makalic</author>
<author>M Niemann</author>
<author>S George</author>
</authors>
<title>A probabilistic approach to the interpretation of spoken utterances.</title>
<date>2008</date>
<booktitle>In PRICAI 2008 – Proceedings of the 10th Pacific Rim International Conference on Artificial Intelligence,</booktitle>
<pages>581--592</pages>
<location>Hanoi, Vietnam.</location>
<contexts>
<context position="1753" citStr="Zukerman et al., 2008" startWordPosition="260" endWordPosition="263">equence of sentences. People often utter several separate sentences to convey their wishes, rather than producing a single sentence that contains all the relevant information (Zweig et al., 2008). For instance, people are likely to say “Go to my office. Get my mug. It is on the table.”, instead of “Get my mug on the table in my office”. This observation, which was validated in our corpus study (Section 4), motivates the mechanism for the interpretation of a sequence of sentences presented in this paper. Our mechanism extends our probabilistic process for interpreting single spoken utterances (Zukerman et al., 2008) in that (1) it determines which sentences in a sequence are related, and if so, combines them into an integrated interpretation; and (2) it provides a formulation for estimating the probability of an interpretation of a sentence sequence, which supports the selection of the most probable interpretation. Our evaluation demonstrates that our mechanism performs well in understanding textual sentence pairs of different length and level of complexity, and highlights particular aspects of our algorithms that require further improvements (Section 4). In the next section, we describe our mechanism fo</context>
<context position="3007" citStr="Zukerman et al., 2008" startWordPosition="454" endWordPosition="457">ce. In Section 3, we present our formalism for assessing the probability of an interpretation. The performance of our system is evaluated in Section 4, followed by related research and concluding remarks. 2 Interpreting a Sequence of Utterances Scusi? employs an anytime algorithm to interpret a sequence of sentences (Algorithm 1). The algorithm generates interpretations until time runs out (in our case, until a certain number of iterations has been executed). In Steps 1–5, Algorithm 1 processes each sentence separately according to the interpretation process for single sentences described in (Zukerman et al., 2008).1 Charniak’s probabilistic parser2 is applied to generate parse trees for each sentence in the sequence. The parser produces up to N (= 50) parse trees for each sentence, associating each parse tree with a probability. The parse trees for each sentence are then iteratively considered in descending order of probability, and algorithmically mapped into Uninstantiated Concept Graphs (UCGs) — a representa1Although DORIS is a spoken dialogue system, our current results pertain to textual input only. Hence, we omit the aspects of our work pertaining to spoken input. 2ftp://ftp.cs.brown.edu/pub/nlpa</context>
<context position="9755" citStr="Zukerman et al., 2008" startWordPosition="1556" endWordPosition="1559"> addressed in the immediate future. After a sequence of imperative UCGs has been generated, candidate Instantiated Concept Graphs (ICGs) are proposed for each imperative UCG, and the most probable ICG sequence is selected (Steps 10–11 of Algorithm 1). We focus on imperative UCGs because they contain the actions that the robot is required to perform; these actions incorporate relevant information from declarative UCGs. ICGs are generated by nominating different instantiated concepts and relations from the system’s knowledge base as potential realizations for each concept and relation in a UCG (Zukerman et al., 2008); each UCG can generate many ICGs. Since this paper focuses on the generation of UCG sequences, the generation of ICGs will not be discussed further. 2.1 Merging UCGs Given tuples (UZ, MZ, RZ) and (Uj, Mj, Rj) where j &gt; i, pronouns and one-anaphora in Uj are replaced with their referent in UZ on the basis of the set of identifier-referent pairs in Rj (if there is no referent in UZ for an identifier in Uj, the identifier is left untouched). UZ and Uj are then merged into a UCG Um by first finding a node n that is common to UZ and Uj, and then copying the sub-tree of Uj whose root is n into a co</context>
<context position="14647" citStr="Zukerman et al., 2008" startWordPosition="2413" endWordPosition="2416">Lappin and Leass, 1994) to assign a score to each anaphorreferent pair according to the grammatical role of the referent in the target UCG (obtained from the highest probability parse tree that is a parent of this UCG). These scores are then converted to probabilities using a linear mapping function. 3 Estimating the Probability of a Merged Interpretation We now present our formulation for estimating the probability of a sequence of UCGs, which supports the selection of the most probable sequence. One sentence. The probability of a UCG generated from a sentence T is estimated as described in (Zukerman et al., 2008), resulting in Pr(U|T) a EP Pr(P|T)·Pr(U|P) (1) where T, P and U denote text, parse tree and UCG respectively. The summation is taken over all possible parse trees from the text to the UCG, because a UCG can have more than one ancestor. As mentioned above, the parser returns an estimate of Pr(P|T); and Pr(U|P) = 1, since the process of generating a UCG from a parse tree is deterministic. A sentence sequence. The probability of an interpretation of a sequence of sentences T1, ... , Tn is Pr(U1, ... , Um|T1, ... , Tn) = Pr(U1, ...,Un,M1, ...,Mn,R1, ...,Rn|T1, ...,Tn) where m is the number of UCG</context>
<context position="26127" citStr="Zukerman et al., 2008" startWordPosition="4393" endWordPosition="4396">m. The second problem occurs mainly when there are multiple anaphoric references in a sequence. We propose to include this factor in our estimation of the probability of referring to a sentence. We intend to alleviate the PP-attachment problem, which also occurred in Experiment 1, by interleaving semantic and pragmatic interpretation of prepositional phrases as done in (Brick and Scheutz, 2007). The expectation is that this will improve the rank of candidates which are pragmatically more plausible. 5 Related Research This research extends our mechanism for interpreting stand-alone utterances (Zukerman et al., 2008) to the interpretation of sentence sequences. Our approach may be viewed as an information state approach (Larsson and Traum, 2000; Becker et al., 2006), in the sense that sentences may update different informational aspects of other sentences, without requiring a particular “legal” set of dialogue acts. However, unlike these information state approaches, ours is probabilistic. Several researchers have investigated probabilistic approaches to the interpretation of spoken utterances in dialogue systems, e.g., (Pfleger et al., 2003; Higashinaka et al., 2003; He and Young, 2003; Gorniak and Roy, </context>
</contexts>
<marker>Zukerman, Makalic, Niemann, George, 2008</marker>
<rawString>I. Zukerman, E. Makalic, M. Niemann, and S. George. 2008. A probabilistic approach to the interpretation of spoken utterances. In PRICAI 2008 – Proceedings of the 10th Pacific Rim International Conference on Artificial Intelligence, pages 581–592, Hanoi, Vietnam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zweig</author>
<author>D Bohus</author>
<author>X Li</author>
<author>P Nguyen</author>
</authors>
<title>Structured models for joint decoding of repeated utterances.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>1157--1160</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="1326" citStr="Zweig et al., 2008" startWordPosition="186" endWordPosition="189">chanism exhibits good performance for sentence pairs, but requires further improvements for sentence sequences. 1 Introduction DORIS (Dialogue Oriented Roaming Interactive System) is a spoken dialogue system under development, which will eventually be mounted on a household robot. The focus of our current work is on DORIS’s language interpretation module called Scusi?. In this paper, we consider the interpretation of a sequence of sentences. People often utter several separate sentences to convey their wishes, rather than producing a single sentence that contains all the relevant information (Zweig et al., 2008). For instance, people are likely to say “Go to my office. Get my mug. It is on the table.”, instead of “Get my mug on the table in my office”. This observation, which was validated in our corpus study (Section 4), motivates the mechanism for the interpretation of a sequence of sentences presented in this paper. Our mechanism extends our probabilistic process for interpreting single spoken utterances (Zukerman et al., 2008) in that (1) it determines which sentences in a sequence are related, and if so, combines them into an integrated interpretation; and (2) it provides a formulation for estim</context>
</contexts>
<marker>Zweig, Bohus, Li, Nguyen, 2008</marker>
<rawString>G. Zweig, D. Bohus, X. Li, and P. Nguyen. 2008. Structured models for joint decoding of repeated utterances. In Proceedings of Interspeech 2008, pages 1157–1160, Brisbane, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>