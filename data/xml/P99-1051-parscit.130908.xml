<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001698">
<title confidence="0.981394">
Acquiring Lexical Generalizations from Corpora:
A Case Study for Diathesis Alternations
</title>
<author confidence="0.994932">
Maria Lapata
</author>
<affiliation confidence="0.941716666666667">
School of Cognitive Science
Division of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
</affiliation>
<email confidence="0.995057">
mlap@cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.997357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953555555556">
This paper examines the extent to which verb
diathesis alternations are empirically attested in
corpus data. We automatically acquire alternating
verbs from large balanced corpora by using partial-
parsing methods and taxonomic information, and
discuss how corpus data can be used to quantify lin-
guistic generalizations. We estimate the productiv-
ity of an alternation and the typicality of its mem-
bers using type and token frequencies.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997177804347826">
Diathesis alternations are changes in the realization
of the argument structure of a verb that are some-
times accompanied by changes in meaning (Levin,
1993). The phenomenon in English is illustrated in
(1)—(2) below.
John offers shares to his employees.
John offers his employees shares.
Leave a note for her.
Leave her a note.
Example (1) illustrates the dative alternation, which
is characterized by an alternation between the
prepositional frame &apos;V NP1 to NP2&apos; and the double
object frame &apos;V NPI NP2&apos;. The benefactive alterna-
tion (cf. (2)) is structurally similar to the dative, the
difference being that it involves the preposition for
rather than to.
Levin (1993) assumes that the syntactic realiza-
tion of a verb&apos;s arguments is directly correlated with
its meaning (cf. also Pinker (1989) for a similar pro-
posal). Thus one would expect verbs that undergo
the same alternations to form a semantically co-
herent class. Levin&apos;s study on diathesis alternations
has influenced recent work on word sense disam-
biguation (Dorr and Jones, 1996), machine transla-
tion (Dang et al., 1998), and automatic lexical ac-
quisition (McCarthy and Korhonen, 1998; Schulte
im Walde, 1998).
The objective of this paper is to investigate the ex-
tent to which diathesis alternations are empirically
attested in corpus data. Using the dative and bene-
factive alternations as a test case we attempt to de-
termine: (a) if some alternations are more frequent
than others, (b) if alternating verbs have frame pref-
erences and (c) what the representative members of
an alternation are.
In section 2 we describe and evaluate the set of
automatic methods we used to acquire verbs under-
going the dative and benefactive alternations. We
assess the acquired frames using a filtering method
presented in section 3. The results are detailed in
section 4. Sections 5 and 6 discuss how the derived
type and token frequencies can be used to estimate
how productive an alternation is for a given verb se-
mantic class and how typical its members are. Fi-
nally, section 7 offers some discussion on future
work and section 8 conclusive remarks.
</bodyText>
<sectionHeader confidence="0.987752" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.993216">
2.1 The parser
</subsectionHeader>
<bodyText confidence="0.999976833333333">
The part-of-speech tagged version of the British Na-
tional Corpus (BNC), a 100 million word collec-
tion of written and spoken British English (Burnard,
1995), was used to acquire the frames characteris-
tic of the dative and benefactive alternations. Sur-
face syntactic structure was identified using Gsearch
(Keller et al., 1999), a tool which allows the search
of arbitrary POS-tagged corpora for shallow syntac-
tic patterns based on a user-specified context-free
grammar and a syntactic query. It achieves this by
combining a left-corner parser with a regular ex-
pression matcher.
Depending on the grammar specification (i.e., re-
cursive or not) Gsearch can be used as a full context-
free parser or a chunk parser. Depending on the syn-
tactic query, Gsearch can parse full sentences, iden-
tify syntactic relations (e.g., verb-object, adjective-
noun) or even single words (e.g., all indefinite pro-
</bodyText>
<page confidence="0.996036">
397
</page>
<bodyText confidence="0.9990866">
nouns in the corpus).
Gsearch outputs all corpus sentences containing
substrings that match a given syntactic query. Given
two possible parses that begin at the same point in
the sentence, the parser chooses the longest match.
If there are two possible parses that can be produced
for the same substring, only one parse is returned.
This means that if the number of ambiguous rules in
the grammar is large, the correctness of the parsed
output is not guaranteed.
</bodyText>
<subsectionHeader confidence="0.982915">
2.2 Acquisition
</subsectionHeader>
<bodyText confidence="0.999783395348837">
We used Gsearch to extract tokens matching the
patterns &apos;V NPI NP2&apos;, &apos;VP NP1 to NP2&apos;, and &apos;V
NPI for NP2&apos; by specifying a chunk grammar for
recognizing the verbal complex and NPs. POS-tags
were retained in the parser&apos;s output which was post-
processed to remove adverbials and interjections.
Examples of the parser&apos;s output are given in (3).
Although there are cases where Gsearch produces
the right parse (cf. (3a)), the parser wrongly iden-
tifies as instances of the double object frame to-
kens containing compounds (cf. (3b)), bare relative
clauses (cf. (3c)) and NPs in apposition (cf. (3d)).
Sometimes the parser attaches prepositional phrases
to the wrong site (cf. (3e)) and cannot distinguish
between arguments and adjuncts (cf. (31)) or be-
tween different types of adjuncts (e.g., temporal
(cf. (30) versus benefactive (cf. (3g))). Erroneous
output also arises from tagging mistakes.
The police driver [v shot] [NP Jamie] [NI) a
look of enquiry] which he missed.
Some also [v offer] [Npa free bus] [NP ser-
vice], to encourage customers who do not
have their own transport.
A Jaffna schoolboy [v shows] [NP a draw-
ing] [NP he] made of helicopters strafing
his home town.
For the latter catalogue Barr [v chose]
[NI, the Surrealist writer] [NP Georges
Hugnet] to write a historical essay.
It [v controlled] [NF. access] [pp to [NP the
vault]].
Yesterday he [v rang] [NP the bell] [pp for
[NI, a long time]].
Don&apos;t Iv save] [NP the bread] [pp for
[NI) the birds]].
We identified erroneous subcategorization frames
(cf. (3b)—(3d)) by using linguistic heuristics and
a process for compound noun detection (cf. sec-
tion 2.3). We disambiguated the attachment site of
PPs (cf. (3e)) using Hindle and Rooth&apos;s (1993) lex-
ical association score (cf. section 2.4). Finally, we
recognized benefactive PPs (cf. (3g)) by exploiting
the WordNet taxonomy (cf. section 2.5).
</bodyText>
<subsectionHeader confidence="0.999679">
2.3 Guessing the double object frame
</subsectionHeader>
<bodyText confidence="0.9992996">
We developed a process which assesses whether the
syntactic patterns (called cues below) derived from
the corpus are instances of the double object frame.
Linguistic Heuristics. We applied several heuris-
tics to the parser&apos;s output which determined whether
corpus tokens were instances of the double object
frame. The &apos;Reject&apos; heuristics below identified er-
roneous matches (cf. (3b-d)), whereas the &apos;Accept&apos;
heuristics identified true instances of the double ob-
ject frame (cf. (3a)).
</bodyText>
<listItem confidence="0.961393117647059">
I. Reject if cue contains at least two proper
names adjacent to each other (e.g., killed
Henry Phipps).
2. Reject if cue contains possessive noun phrases
(e.g., give a showman&apos;s award).
3. Reject if cue&apos;s last word is a pronoun or an
anaphor (e.g., ask the subjects themselves).
4. Accept if verb is followed by a personal or in-
definite pronoun (e.g., found him a home).
5. Accept if verb is followed by an anaphor
(e.g., made herself a snack).
6. Accept if cue&apos;s surface structure is either &apos;V
MOD&apos; NP MOD NP&apos; or &apos;V NP MOD NP&apos;
(e.g., send Bailey a postcard).
7. Cannot decide if cue&apos;s surface structure is
&apos;V MOD* N N+&apos; (e.g., offer a free bus ser-
vice).
</listItem>
<bodyText confidence="0.9710627">
Compound Noun Detection. Tokens identified
by heuristic (7) were dealt with separately by a pro-
cedure which guesses whether the nouns following
the verb are two distinct arguments or parts of a
compound. This procedure was applied only to noun
sequences of length 2 and 3 which were extracted
from the parser&apos;s output2 and compared against a
compound noun dictionary (48,661 entries) com-
piled from WordNet. 13.9% of the noun sequences
were identified as compounds in the dictionary.
</bodyText>
<footnote confidence="0.9911336">
I Here MOD represents any prenominal modifier (e.g., arti-
cles, pronouns, adjectives, quantifiers, ordinals).
2Tokens containing noun sequences with length larger
than 3 (450 in total) were considered negative instances of the
double object frame.
</footnote>
<page confidence="0.998648">
398
</page>
<tableCaption confidence="0.999887">
Table 1: Random sample of two word compounds
</tableCaption>
<table confidence="0.99973925">
G-score 3-word compound
574.48 [[energy efficiency] office]
382.92 [[council tax] bills]
77.78 [alcohol [education course]]
48.84 [hospital [out-patient department]
36.44 [[tumour suppressor] function]
32.35 [[nature conservation] resources]
23.98 [[quality amplifier] circuits]
</table>
<tableCaption confidence="0.99925">
Table 2: Random sample of three word compounds
</tableCaption>
<figure confidence="0.9934863125">
G-score
2-word compound
1967.68
775.21
87.02
45.40
30.58
29.94
24.04
bank manager
tax liability
income tax
book reviewer
designer gear
safety plan
drama school
</figure>
<bodyText confidence="0.999656032258064">
For sequences of length 2 not found in WordNet,
we used the log-likelihood ratio (G-score) to esti-
mate the lexical association between the nouns, in
order to determine if they formed a compound noun.
We preferred the log-likelihood ratio to other statis-
tical scores, such as the association ratio (Church
and Hanks, 1990) or x2, since it adequately takes
into account the frequency of the co-occurring
words and is less sensitive to rare events and corpus-
size (Dunning, 1993; Daille, 1996). We assumed
that two nouns cannot be disjoint arguments of the
verb if they are lexically associated. On this basis,
tokens were rejected as instances of the double ob-
ject frame if they contained two nouns whose G-
score had a p-value less than 0.05.
A two-step process was applied to noun se-
quences of length 3: first their bracketing was de-
termined and second the G-score was computed be-
tween the single noun and the 2-noun sequence.
We inferred the bracketing by modifying an al-
gorithm initially proposed by Pustejovsky et al.
(1993). Given three nouns n , n2, n3, if either [n n2]
or [ro n3] are in the compound noun dictionary, we
built structures [[n1 n2] n3] or [n [n2 n3]] accord-
ingly; if both [n i n2] and [n2 n3] appear in the dic-
tionary, we chose the most frequent pair; if neither
[n1 n2] nor [n2 n3] appear in WordNet, we computed
the G-score for [n 1 n2] and [n2 n3] and chose the
pair with highest value (p &lt; 0.05). Tables 1 and
2 display a random sample of the compounds the
method found (p &lt;0.05).
</bodyText>
<subsectionHeader confidence="0.74732">
2.3.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.99984625">
The performance of the linguistic heuristics and the
compound detection procedure were evaluated by
randomly selecting approximately 3,000 corpus to-
kens which were previously accepted or rejected as
instances of the double object frame. Two judges de-
cided whether the tokens were classified correctly.
The judges&apos; agreement on the classification task was
calculated using the Kappa coefficient (Siegel and
</bodyText>
<subsectionHeader confidence="0.546541">
Method Prec Kappa
</subsectionHeader>
<table confidence="0.999190125">
Reject heuristics 96.9% K = 0.76, N = 1000
Accept heuristics 73.6% K = 0.82, N = 1000
2-word compounds 98.9% K = 0.83, N = 553
3-word compounds 99.1% K = 0.70, N = 447
Verb attach-to 74.4% K = 0.78, N = 494
Noun attach-to 80.0% K = 0.80, N = 500
Verb attach-for 73.6% K = 0.85, N = 630
Noun attach-for 36.0% K = 0.88, N = 500
</table>
<tableCaption confidence="0.997863">
Table 3: Precision of heuristics, compound noun de-
</tableCaption>
<bodyText confidence="0.9152152">
tection and lexical association
Castellan, 1988) which measures inter-rater agree-
ment among a set of coders making category judg-
ments.
The Kappa coefficient of agreement (K) is the ra-
tio of the proportion of times, P(A), that k raters
agree to the proportion of times, P(E), that we
would expect the raters to agree by chance (cf. (4)).
If there is a complete agreement among the raters,
then K = 1.
</bodyText>
<equation confidence="0.9911165">
(4) KP(A)P — (E)
1 — P(E)
</equation>
<bodyText confidence="0.997428833333333">
Precision figures3 (Prec) and inter-judge agreement
(Kappa) are summarized in table 3. In sum, the
heuristics achieved a high accuracy in classifying
cues for the double object frame. Agreement on the
classification was good given that the judges were
given minimal instructions and no prior training.
</bodyText>
<subsectionHeader confidence="0.999197">
2.4 Guessing the prepositional frames
</subsectionHeader>
<bodyText confidence="0.999784333333333">
In order to consider verbs with prepositional frames
as candidates for the dative and benefactive alterna-
tions the following requirements needed to be met:
</bodyText>
<footnote confidence="0.818226">
1. the PP must be attached to the verb;
3Throught the paper the reported percentages are the aver-
age of the judges&apos; individual classifications.
</footnote>
<page confidence="0.985613">
399
</page>
<listItem confidence="0.980316">
2. in the case of the &apos;V NPI to NP2&apos; structure, the
to-PP must be an argument of the verb;
3. in the case of the &apos;V NPI for NP2&apos; structure,
the for-PP must be benefactive.4
</listItem>
<bodyText confidence="0.9945111">
In order to meet requirements (1)—(3), we first de-
termined the attachment site (e.g., verb or noun) of
the PP and secondly developed a procedure for dis-
tinguishing benefactive from non-benefactive PPs.
Several approaches have statistically addressed
the problem of prepositional phrase ambiguity,
with comparable results (Hindle and Rooth, 1993;
Collins and Brooks, 1995; Ratnaparkhi, 1998). Hin-
dle and Rooth (1993) used a partial parser to extract
(v, n, p) tuples from a corpus, where p is the prepo-
sition whose attachment is ambiguous between the
verb v and the noun n. We used a variant of the
method described in Hindle and Rooth (1993), the
main difference being that we applied their lexical
association score (a log-likelihood ratio which com-
pares the probability of noun versus verb attach-
ment) in an unsupervised non-iterative manner. Fur-
thermore, the procedure was applied to the special
case of tuples containing the prepositions to and for
only.
</bodyText>
<subsectionHeader confidence="0.584964">
2.4.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.998608909090909">
We evaluated the procedure by randomly select-
ing 2,124 tokens containing to-PPs and for-PPs
for which the procedure guessed verb or noun at-
tachment. The tokens were disambiguated by two
judges. Precision figures are reported in table 3.
The lexical association score was highly accu-
rate on guessing both verb and noun attachment for
to-PPs. Further evaluation revealed that for 98.6%
(K = 0.9, N = 494, k = 2) of the tokens clas-
sified as instances of verb attachment, the to-PP
was an argument of the verb, which meant that the
log-likelihood ratio satisfied both requirements (1)
and (2) for to-PPs.
A low precision of 36% was achieved in detecting
instances of noun attachment for for-PPs. One rea-
son for this is the polysemy of the preposition for:
for-PPs can be temporal, purposive, benefactive or
causal adjuncts and consequently can attach to var-
ious sites. Another difficulty is that benefactive for-
PPs semantically license both attachment sites.
To further analyze the poor performance of the
log-likelihood ratio on this task, 500 tokens con-
</bodyText>
<footnote confidence="0.857008333333333">
4Syntactically speaking, benefactive for-PPs are not argu-
ments but adjuncts (Jackendoff, 1990) and can appear on any
verb with which they are semantically compatible.
</footnote>
<bodyText confidence="0.999959333333333">
taming for-PPs were randomly selected from the
parser&apos;s output and disambiguated. Of these 73.9%
(K = 0.9, N = 500, k = 2) were instances of verb
attachment, which indicates that verb attachments
outnumber noun attachments for for-PPs, and there-
fore a higher precision for verb attachment (cf. re-
quirement (1)) can be achieved without applying the
log-likelihood ratio, but instead classifying all in-
stances as verb attachment.
</bodyText>
<subsectionHeader confidence="0.994897">
2.5 Benefactive PPs
</subsectionHeader>
<bodyText confidence="0.999589146341463">
Although surface syntactic cues can be important
for determining the attachment site of prepositional
phrases, they provide no indication of the semantic
role of the preposition in question. This is particu-
larly the case for the preposition for which can have
several roles, besides the benefactive.
Two judges discriminated benefactive from non-
benefactive PPs for 500 tokens, randomly selected
from the parser&apos;s output. Only 18.5% (K = 0.73,
N = 500, k = 2) of the sample contained bene-
factive PPs. An analysis of the nouns headed by the
preposition for revealed that 59.6% were animate,
17% were collective, 4.9% denoted locations, and
the remaining 18.5% denoted events, artifacts, body
parts, or actions. Animate, collective and location
nouns account for 81.5% of the benefactive data.
We used the WordNet taxonomy (Miller et al.,
1990) to recognize benefactive PPs (cf. require-
ment (3)). Nouns in WordNet are organized into
an inheritance system defined by hypernymic rela-
tions. Instead of being contained in a single hier-
archy, nouns are partitioned into a set of seman-
tic primitives (e.g., act, animal, time) which are
treated as the unique beginners of separate hier-
archies. We compiled a &amp;quot;concept dictionary&amp;quot; from
WordNet (87,642 entries), where each entry con-
sisted of the noun and the semantic primitive dis-
tinguishing each noun sense (cf. table 4).
We considered a for-PP to be benefactive if the
noun headed by for was listed in the concept dic-
tionary and the semantic primitive of its prime
sense (Sense 1) was person, animal, group or lo-
cation. PPs with head nouns not listed in the dictio-
nary were considered benefactive only if their head
nouns were proper names. Tokens containing per-
sonal, indefinite and anaphoric pronouns were also
considered benefactive (e.g., build a home for him).
Two judges evaluated the procedure by judging
1,000 randomly selected tokens, which were ac-
cepted or rejected as benefactive. The procedure
achieved a precision of 48.8% (K = 0.89, N =
</bodyText>
<page confidence="0.991314">
400
</page>
<table confidence="0.999751333333333">
L_ Sense 1 Sense 2 Sense 3
gift possession cognition act
cooking food act
teacher person cognition
university group artifact
city location location
pencil artifact
group
group
</table>
<tableCaption confidence="0.87417">
Table 4: Sample entries from WordNet concept dic-
tionary
</tableCaption>
<bodyText confidence="0.860150666666667">
500, k = 2) in detecting benefactive tokens and
90.9% (K = .94, N = 499, k = 2) in detecting
non-benefactive ones.
</bodyText>
<sectionHeader confidence="0.997435" genericHeader="method">
3 Filtering
</sectionHeader>
<bodyText confidence="0.999972037037037">
Filtering assesses how probable it is for a verb to be
associated with a wrong frame. Erroneous frames
can be the result of tagging errors, parsing mistakes,
or errors introduced by the heuristics and proce-
dures we used to guess syntactic structure.
We discarded verbs for which we had very little
evidence (frame frequency = 1) and applied a rela-
tive frequency cutoff: the verb&apos;s acquired frame fre-
quency was compared against its overall frequency
in the BNC. Verbs whose relative frame frequency
was lower than an empirically established thresh-
old were discarded. The threshold values varied
from frame to frame but not from verb to verb and
were determined by taking into account for each
frame its overall frame frequency which was es-
timated from the COMLEX subcategorization dic-
tionary (6,000 verbs) (Grishman et al., 1994). This
meant that the threshold was higher for less frequent
frames (e.g., the double object frame for which only
79 verbs are listed in COMLEX).
We also experimented with a method suggested
by Brent (1993) which applies the binomial test
on frame frequency data. Both methods yielded
comparable results. However, the relative frequency
threshold worked slightly better and the results re-
ported in the following section are based on this
method.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.998166068965517">
We acquired 162 verbs for the double object frame,
426 verbs for the &apos;V NP1 to NP2&apos; frame and 962
for the &apos;V NP1 for NP2&apos; frame. Membership in al-
ternations was judged as follows: (a) a verb partic-
ipates in the dative alternation if it has the double
object and &apos;V NPI to NP2&apos; frames and (b) a verb
Dative Alternation
Alternating allot, assign, bring, fax, feed, flick,
give, grant, guarantee, leave, lend
offer, owe, take pass, pay, render,
repay, sell, show, teach, tell, throw,
toss, write, serve, send, award
V NP1 NP2 allocate, bequeath, carry, catapult,
cede, concede, drag, drive, extend,
ferry, fly, haul, hoist, issue, lease,
peddle, pose, preach, push, relay,
ship, tug, yield
V NPI to NP2 ask, chuck, promise, quote, read,
shoot, slip
Benefactive Alternation
Alternating bake, build, buy, cast, cook, earn,
fetch, find, fix, forge, gain, get,
keep, knit, leave, make, pour, save
procure, secure, set, toss, win, write
V NPI NP2 arrange, assemble, carve, choose,
compile, design, develop, dig,
gather, grind, hire, play, prepare,
reserve, run, sew
V NP1 for NP2 boil, call, shoot
</bodyText>
<tableCaption confidence="0.968801">
Table 5: Verbs common in corpus and Levin
</tableCaption>
<bodyText confidence="0.999712416666667">
participates in the benefactive alternation if it has
the double object and &apos;V NP1 for NP2&apos; frames. Ta-
ble 5 shows a comparison of the verbs found in the
corpus against Levin&apos;s list of verbs;5 rows &apos;V NPI to
NP2&apos; and &apos;V NPI for NP2&apos; contain verbs listed as
alternating in Levin but for which we acquired only
one frame. In Levin 115 verbs license the dative and
103 license the benefactive alternation. Of these we
acquired 68 for the dative and 43 for the benefactive
alternation (in both cases including verbs for which
only one frame was acquired).
The dative and benefactive alternations were also
acquired for 52 verbs not listed in Levin. Of these,
10 correctly alternate (cause, deliver, hand, refuse,
report and set for the dative alternation and cause,
spoil, afford and prescribe for the benefactive), and
12 can appear in either frame but do not alter-
nate (e.g., appoint, fix, proclaim). For 18 verbs two
frames were acquired but only one was correct (e.g.,
swap and forgive which take only the double object
frame), and finally 12 verbs neither alternated nor
had the acquired frames. A random sample of the
acquired verb frames and their (log-transformed)
frequencies is shown in figure 1.
</bodyText>
<footnote confidence="0.9889915">
5The comparisons reported henceforth exclude verbs listed
in Levin with overall corpus frequency less than 1 per million.
</footnote>
<page confidence="0.995353">
401
</page>
<figureCaption confidence="0.996881">
Figure 1: Random sample of acquired frequencies
for the dative and benefactive alternations
</figureCaption>
<figure confidence="0.995923">
INLevin
Corpus dative
I Corpus benefactive
20
I.
10
</figure>
<figureCaption confidence="0.9336065">
Figure 2: Semantic classes for the dative and bene-
factive alternations
</figureCaption>
<bodyText confidence="0.9999461875">
Levin defines 10 semantic classes of verbs for
which the dative alternation applies (e.g., GIVE
verbs, verbs of FUTURE HAVING, SEND verbs), and
5 classes for which the benefactive alternation ap-
plies (e.g., BUILD, CREATE, PREPARE verbs), as-
suming that verbs participating in the same class
share certain meaning components.
We partitioned our data according to Levin&apos;s pre-
defined classes. Figure 2 shows for each semantic
class the number of verbs acquired from the cor-
pus against the number of verbs listed in Levin. As
can be seen in figure 2, Levin and the corpus ap-
proximate each other for verbs of FUTURE HAVING
(e.g., guarantee), verbs of MESSAGE TRANSFER
(e.g., tell) and BRING-TAKE verbs (e.g., bring).
The semantic classes of GIVE (e.g., sell), CARRY
(e.g., drag), SEND (e.g., ship), GET (e.g., buy) and
PREPARE (e.g., bake) verbs are also fairly well rep-
resented in the corpus, in contrast to SLIDE verbs
(e.g., bounce) for which no instances were found.
Note that the corpus and Levin did not agree
with respect to the most popular classes licensing
the dative and benefactive alternations: THROWING
(e.g., toss) and BUILD verbs (e.g., carve) are the
biggest classes in Levin allowing the dative and
benefactive alternations respectively, in contrast to
FUTURE HAVING and GET verbs in the corpus.
This can be explained by looking at the average cor-
pus frequency of the verbs belonging to the seman-
tic classes in question: FUTURE HAVING and GET
verbs outnumber THROWING and BUILD verbs by
a factor of two to one.
</bodyText>
<sectionHeader confidence="0.991593" genericHeader="method">
5 Productivity
</sectionHeader>
<bodyText confidence="0.999528">
The relative productivity of an alternation for a se-
mantic class can be estimated by calculating the ra-
tio of acquired to possible verbs undergoing the al-
ternation (Aronoff, 1976; Briscoe and Copestake,
1996):
</bodyText>
<equation confidence="0.909501666666667">
f (acquired, class)
(5) P(acquirediclass) =
f (class)
</equation>
<bodyText confidence="0.999967222222222">
We express the productivity of an alternation for
a given class as f (acquired, class), the number of
verbs which were found in the corpus and are mem-
bers of the class, over f (class), the total number
of verbs which are listed in Levin as members of
the class (Total). The productivity values (Prod) for
both the dative and the benefactive alternation (Alt)
are summarized in table 6.
Note that productivity is sensitive to class size.
The productivity of BRING-TAKE verbs is esti-
mated to be 1 since it contains only 2 members
which were also found in the corpus. This is intu-
itively correct, as we would expect the alternation
to be more productive for specialized classes.
The productivity estimates discussed here can be
potentially useful for treating lexical rules proba-
bilistically, and for quantifying the degree to which
language users are willing to apply&apos; a rule in order
</bodyText>
<figure confidence="0.986336125">
NP—PP_to frame
NP—PP_for frame
NP—NP frame
e =
3 51)
a
Log frame frequency
30
</figure>
<page confidence="0.993104">
402
</page>
<table confidence="0.999838315789474">
Dative alternation
Class Total Alt Prod Typ
BRING-TAKE 2 2 1 0.327
FUTURE HAVING 19 17 0.89 0.313
GIVE 15 9 0.6 0.55
M.TRANSFER 17 10 0.58 0.66
CARRY 15 6 0.4 0.056
DRIVE 11 3 0.27 0.03
THROWING 30 7 0.23 0.658
SEND 23 3 0.13 0.181
INSTR. COM. 18 1 0.05 0.648
SLIDE 5 0 0 0
Benefactive alternation
Class Total Alt Prod Typ
GET 33 17 0.51 0.54
PREPARE 26 9 0.346 0.55
BUILD 35 12 0.342 0.34
PERFORMANCE 19 1 0.05 0.56
CREATE 20 2 0.1 0.05
</table>
<tableCaption confidence="0.980547">
Table 6: Productivity estimates and typicality values
</tableCaption>
<bodyText confidence="0.945118333333333">
for the dative and benefactive alternation
to produce a novel form (Briscoe and Copestake,
1996).
</bodyText>
<sectionHeader confidence="0.999201" genericHeader="method">
6 Typicality
</sectionHeader>
<bodyText confidence="0.999718636363637">
Estimating the productivity of an alternation for a
given class does not incorporate information about
the frequency of the verbs undergoing the alterna-
tion. We propose to use frequency data to quantify
the typicality of a verb or verb class for a given alter-
nation. The underlying assumption is that a verb is
typical for an alternation if it is equally frequent for
both frames which are characteristic for the alter-
nation. Thus the typicality of a verb can be defined
as the conditional probability of the frame given the
verb:
</bodyText>
<equation confidence="0.905788">
(6) P(frame,Iverb) =
E f (frame,,, verb)
fl
</equation>
<bodyText confidence="0.999958965517241">
We calculate P(frameilverb) by dividing
f (frame „ verb), the number of times the verb
was attested in the corpus with frame i, by
E„ f (frame,,, verb), the overall number of times
the verb was attested. In our case a verb has two
frames, hence P(framedverb) is close to 0.5 for
typical verbs (i.e., verbs with balanced frequencies)
and close to either 0 or 1 for peripheral verbs,
depending on their preferred frame. Consider the
verb owe as an example (cf. figure 1). 648 instances
of owe were found, of which 309 were instances
of the double object frame. By dividing the latter
by the former we can see that owe is highly typical
of the dative alternation: its typicality score for the
double object frame is 0.48.
By taking the average of P (frame i , verb) for all
verbs which undergo the alternation and belong to
the same semantic class, we can estimate how typi-
cal this class is for the alternation. Table 6 illustrates
the typicality (Typ) of the semantic classes for the
two alternations. (The typicality values were com-
puted for the double object frame). For the dative
alternation, the most typical class is GIVE, and the
most peripheral is DRIVE (e.g., ferry). For the bene-
factive alternation, PERFORMANCE (e.g., sing),
PREPARE (e.g., bake) and GET (e.g., buy) verbs are
the most typical, whereas CREATE verbs (e.g., com-
pose) are peripheral, which seems intuitively cor-
rect.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999982809523809">
The work reported in this paper relies on frame
frequencies acquired from corpora using partial-
parsing methods. For instance, frame frequency data
was used to estimate whether alternating verbs ex-
hibit different preferences for a given frame (typi-
cality).
However, it has been shown that corpus id-
iosyncrasies can affect subcategorization frequen-
cies (cf. Roland and Jurafsky (1998) for an exten-
sive discussion). This suggests that different corpora
may give different results with respect to verb al-
ternations. For instance, the to-PP frame is poorly
represented in the syntactically annotated version of
the Penn Treebank (Marcus et al., 1993). There are
only 26 verbs taking the to-PP frame, of which 20
have frame frequency of 1. This indicates that a very
small number of verbs undergoing the dative alter-
nation can be potentially acquired from this corpus.
In future work we plan to investigate the degree to
which corpus differences affect the productivity and
typicality estimates for verb alternations.
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.979916875">
This paper explored the degree to which diathesis
alternations can be identified in corpus data via shal-
low syntactic processing. Alternating verbs were ac-
quired from the BNC by using Gsearch as a chunk
parser. Erroneous frames were discarded by apply-
ing linguistic heuristics, statistical scores (the log-
likelihood ratio) and large-scale lexical resources
f (frame,, verb)
</bodyText>
<page confidence="0.983685">
403
</page>
<bodyText confidence="0.998321222222222">
(e.g., WordNet).
We have shown that corpus frequencies can be
used to quantify linguistic intuitions and lexical
generalizations such as Levin&apos;s (1993) semantic
classification. Furthermore, corpus frequencies can
make explicit predictions about word use. This was
demonstrated by using the frequencies to estimate
the productivity of an alternation for a given seman-
tic class and the typicality of its members.
</bodyText>
<sectionHeader confidence="0.999614" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.8625574">
The author was supported by the Alexander
S. Onassis Foundation and the UK Economic and
Social Research Council. Thanks to Chris Brew,
Frank Keller, Alex Lascarides and Scott McDonald
for valuable comments.
</reference>
<sectionHeader confidence="0.962802" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999764736263736">
Mark Aronoff. 1976. Word Formation in Generative
Grammar. Linguistic Inquiry Monograph 1. MIT
Press, Cambridge, MA.
Michael Brent. 1993. From grammar to lexicon: Un-
supervised learning of lexical syntax. Computational
Linguistics, 19(3):243-262.
Ted Briscoe and Ann Copestake. 1996. Contolling the
application of lexical rules. In Proceedings of ACL
SIGLEX Workshop on Breadth and Depth of Semantic
Lexicons, pages 7-19, Santa Cruz, CA.
Lou Burnard, 1995. Users Guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Service.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22-29.
COLING/ACL 1998. Proceedings of the 17th Interna-
tional Conference on Computational Linguistics and
36th Annual Meeting of the Association for Computa-
tional Linguistics, Montréal.
Michael Collins and James Brooks. 1995. Prepositional
phrase attachment through a backed-off model. In
Proceedings of the 3 rdWorkshop on Very Large Cor-
pora, pages 27-38.
Beatrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. In Judith Klavans and Philip Resnik, ed-
itors, The Balancing Act: Combining Symbolic and
Statistical Approaches to Language, pages 49-66.
MIT Press, Cambridge, MA.
Hoa Trang Dang, Karin Kipper, Martha Palmer, and
Joseph Rosenzweig. 1998. Investigating regular
sense extensions based on intersective Levin classes.
In COLING/ACL 1998, pages 293-299.
Bonnie J. Dorr and Doug Jones. 1996. Role of word
sense disambiguation in lexical acquisition: Predict-
ing semantics from syntactic cues. In Proceedings of
the 16th International Conference on Computational
Linguistics, pages 322-327, Copenhagen.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61-74.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. Comlex syntax: Building a computational lexi-
con. In Proceedings of the 15th International Confer-
ence on Computational Linguistics, pages 268-272,
Kyoto.
Donald Hindle and Mats Rooth. 1993. Structural am-
biguity and lexical relations. Computational Linguis-
tics, 19(1):103-120.
Ray Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, MA.
Frank Keller, Martin Corley, Steffan Corley, Matthew W.
Crocker, and Shari Trewin. 1999. Gsearch: A tool for
syntactic investigation of unparsed corpora. In Pro-
ceedings of the EACL Workshop on Linguistically In-
terpreted Corpora, Bergen.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313-330.
Diana McCarthy and Anna Korhonen. 1998. Detecting
verbal participation in diathesis alternations. In COL-
ING/ACL 1998, pages 1493-1495. Student Session.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4):235-244.
Steven Pinker. 1989. Learnability and Cognition: The
Acquisition of Argument Structure. MIT Press, Cam-
bridge MA.
James Pustejovsky, Sabine Bergler, and Peter Anick.
1993. Lexical semantic techniques for corpus anal-
ysis. Computational Linguistics, 19(3):331-358.
Adwait Ratnaparkhi. 1998. Unsupervised statistical
models for prepositional phrase attachment. In Pro-
ceedings of the 7th International Conference on Com-
putational Linguistics, pages 1079-1085.
Douglas Roland and Daniel Jurafsky. 1998. How verb
subcategorization frequencies are affected by corpus
choice. In COLING/ACL 1998, pages 1122-1128.
Sabine Schulte im Walde. 1998. Automatic semantic
classification of verbs according to their alternation
behaviour. Master&apos;s thesis, Institut f&apos;ur Maschinelle
Sprachverarbeitung, University of Stuttgart.
Sidney Siegel and N Castellan. 1988. Non Parametric
Statistics for the Behavioral Sciences. McGraw-Hill,
New York.
</reference>
<page confidence="0.999001">
404
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911854">
<title confidence="0.9991485">Acquiring Lexical Generalizations from Corpora: A Case Study for Diathesis Alternations</title>
<author confidence="0.999947">Maria Lapata</author>
<affiliation confidence="0.999953">School of Cognitive Science Division of Informatics, University of Edinburgh</affiliation>
<address confidence="0.926936">2 Buccleuch Place, Edinburgh EH8 9LW, UK</address>
<email confidence="0.997668">mlap@cogsci.ed.ac.uk</email>
<abstract confidence="0.9987329">This paper examines the extent to which verb diathesis alternations are empirically attested in corpus data. We automatically acquire alternating verbs from large balanced corpora by using partialparsing methods and taxonomic information, and discuss how corpus data can be used to quantify linguistic generalizations. We estimate the productivity of an alternation and the typicality of its members using type and token frequencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>The author was supported by the Alexander S. Onassis Foundation and the UK Economic and Social Research Council. Thanks to Chris Brew, Frank Keller, Alex Lascarides and Scott McDonald for valuable comments.</title>
<marker></marker>
<rawString>The author was supported by the Alexander S. Onassis Foundation and the UK Economic and Social Research Council. Thanks to Chris Brew, Frank Keller, Alex Lascarides and Scott McDonald for valuable comments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Aronoff</author>
</authors>
<title>Word Formation in Generative Grammar. Linguistic Inquiry Monograph 1.</title>
<date>1976</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22720" citStr="Aronoff, 1976" startWordPosition="3749" endWordPosition="3750">HROWING (e.g., toss) and BUILD verbs (e.g., carve) are the biggest classes in Levin allowing the dative and benefactive alternations respectively, in contrast to FUTURE HAVING and GET verbs in the corpus. This can be explained by looking at the average corpus frequency of the verbs belonging to the semantic classes in question: FUTURE HAVING and GET verbs outnumber THROWING and BUILD verbs by a factor of two to one. 5 Productivity The relative productivity of an alternation for a semantic class can be estimated by calculating the ratio of acquired to possible verbs undergoing the alternation (Aronoff, 1976; Briscoe and Copestake, 1996): f (acquired, class) (5) P(acquirediclass) = f (class) We express the productivity of an alternation for a given class as f (acquired, class), the number of verbs which were found in the corpus and are members of the class, over f (class), the total number of verbs which are listed in Levin as members of the class (Total). The productivity values (Prod) for both the dative and the benefactive alternation (Alt) are summarized in table 6. Note that productivity is sensitive to class size. The productivity of BRING-TAKE verbs is estimated to be 1 since it contains o</context>
</contexts>
<marker>Aronoff, 1976</marker>
<rawString>Mark Aronoff. 1976. Word Formation in Generative Grammar. Linguistic Inquiry Monograph 1. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>From grammar to lexicon: Unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--3</pages>
<contexts>
<context position="18080" citStr="Brent (1993)" startWordPosition="2978" endWordPosition="2979">inst its overall frequency in the BNC. Verbs whose relative frame frequency was lower than an empirically established threshold were discarded. The threshold values varied from frame to frame but not from verb to verb and were determined by taking into account for each frame its overall frame frequency which was estimated from the COMLEX subcategorization dictionary (6,000 verbs) (Grishman et al., 1994). This meant that the threshold was higher for less frequent frames (e.g., the double object frame for which only 79 verbs are listed in COMLEX). We also experimented with a method suggested by Brent (1993) which applies the binomial test on frame frequency data. Both methods yielded comparable results. However, the relative frequency threshold worked slightly better and the results reported in the following section are based on this method. 4 Results We acquired 162 verbs for the double object frame, 426 verbs for the &apos;V NP1 to NP2&apos; frame and 962 for the &apos;V NP1 for NP2&apos; frame. Membership in alternations was judged as follows: (a) a verb participates in the dative alternation if it has the double object and &apos;V NPI to NP2&apos; frames and (b) a verb Dative Alternation Alternating allot, assign, bring,</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Michael Brent. 1993. From grammar to lexicon: Unsupervised learning of lexical syntax. Computational Linguistics, 19(3):243-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Ann Copestake</author>
</authors>
<title>Contolling the application of lexical rules.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL SIGLEX Workshop on Breadth and Depth of Semantic Lexicons,</booktitle>
<pages>7--19</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="22750" citStr="Briscoe and Copestake, 1996" startWordPosition="3751" endWordPosition="3754">toss) and BUILD verbs (e.g., carve) are the biggest classes in Levin allowing the dative and benefactive alternations respectively, in contrast to FUTURE HAVING and GET verbs in the corpus. This can be explained by looking at the average corpus frequency of the verbs belonging to the semantic classes in question: FUTURE HAVING and GET verbs outnumber THROWING and BUILD verbs by a factor of two to one. 5 Productivity The relative productivity of an alternation for a semantic class can be estimated by calculating the ratio of acquired to possible verbs undergoing the alternation (Aronoff, 1976; Briscoe and Copestake, 1996): f (acquired, class) (5) P(acquirediclass) = f (class) We express the productivity of an alternation for a given class as f (acquired, class), the number of verbs which were found in the corpus and are members of the class, over f (class), the total number of verbs which are listed in Levin as members of the class (Total). The productivity values (Prod) for both the dative and the benefactive alternation (Alt) are summarized in table 6. Note that productivity is sensitive to class size. The productivity of BRING-TAKE verbs is estimated to be 1 since it contains only 2 members which were also </context>
<context position="24359" citStr="Briscoe and Copestake, 1996" startWordPosition="4039" endWordPosition="4042"> e = 3 51) a Log frame frequency 30 402 Dative alternation Class Total Alt Prod Typ BRING-TAKE 2 2 1 0.327 FUTURE HAVING 19 17 0.89 0.313 GIVE 15 9 0.6 0.55 M.TRANSFER 17 10 0.58 0.66 CARRY 15 6 0.4 0.056 DRIVE 11 3 0.27 0.03 THROWING 30 7 0.23 0.658 SEND 23 3 0.13 0.181 INSTR. COM. 18 1 0.05 0.648 SLIDE 5 0 0 0 Benefactive alternation Class Total Alt Prod Typ GET 33 17 0.51 0.54 PREPARE 26 9 0.346 0.55 BUILD 35 12 0.342 0.34 PERFORMANCE 19 1 0.05 0.56 CREATE 20 2 0.1 0.05 Table 6: Productivity estimates and typicality values for the dative and benefactive alternation to produce a novel form (Briscoe and Copestake, 1996). 6 Typicality Estimating the productivity of an alternation for a given class does not incorporate information about the frequency of the verbs undergoing the alternation. We propose to use frequency data to quantify the typicality of a verb or verb class for a given alternation. The underlying assumption is that a verb is typical for an alternation if it is equally frequent for both frames which are characteristic for the alternation. Thus the typicality of a verb can be defined as the conditional probability of the frame given the verb: (6) P(frame,Iverb) = E f (frame,,, verb) fl We calcula</context>
</contexts>
<marker>Briscoe, Copestake, 1996</marker>
<rawString>Ted Briscoe and Ann Copestake. 1996. Contolling the application of lexical rules. In Proceedings of ACL SIGLEX Workshop on Breadth and Depth of Semantic Lexicons, pages 7-19, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>Users Guide for the British National Corpus.</title>
<date>1995</date>
<institution>British National Corpus Consortium, Oxford University Computing Service.</institution>
<contexts>
<context position="2974" citStr="Burnard, 1995" startWordPosition="474" endWordPosition="475">ative and benefactive alternations. We assess the acquired frames using a filtering method presented in section 3. The results are detailed in section 4. Sections 5 and 6 discuss how the derived type and token frequencies can be used to estimate how productive an alternation is for a given verb semantic class and how typical its members are. Finally, section 7 offers some discussion on future work and section 8 conclusive remarks. 2 Method 2.1 The parser The part-of-speech tagged version of the British National Corpus (BNC), a 100 million word collection of written and spoken British English (Burnard, 1995), was used to acquire the frames characteristic of the dative and benefactive alternations. Surface syntactic structure was identified using Gsearch (Keller et al., 1999), a tool which allows the search of arbitrary POS-tagged corpora for shallow syntactic patterns based on a user-specified context-free grammar and a syntactic query. It achieves this by combining a left-corner parser with a regular expression matcher. Depending on the grammar specification (i.e., recursive or not) Gsearch can be used as a full contextfree parser or a chunk parser. Depending on the syntactic query, Gsearch can </context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Lou Burnard, 1995. Users Guide for the British National Corpus. British National Corpus Consortium, Oxford University Computing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--1</pages>
<contexts>
<context position="8807" citStr="Church and Hanks, 1990" startWordPosition="1413" endWordPosition="1416">ressor] function] 32.35 [[nature conservation] resources] 23.98 [[quality amplifier] circuits] Table 2: Random sample of three word compounds G-score 2-word compound 1967.68 775.21 87.02 45.40 30.58 29.94 24.04 bank manager tax liability income tax book reviewer designer gear safety plan drama school For sequences of length 2 not found in WordNet, we used the log-likelihood ratio (G-score) to estimate the lexical association between the nouns, in order to determine if they formed a compound noun. We preferred the log-likelihood ratio to other statistical scores, such as the association ratio (Church and Hanks, 1990) or x2, since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize (Dunning, 1993; Daille, 1996). We assumed that two nouns cannot be disjoint arguments of the verb if they are lexically associated. On this basis, tokens were rejected as instances of the double object frame if they contained two nouns whose Gscore had a p-value less than 0.05. A two-step process was applied to noun sequences of length 3: first their bracketing was determined and second the G-score was computed between the single noun and the 2-noun sequenc</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>COLINGACL</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Montréal.</location>
<marker>COLINGACL, 1998</marker>
<rawString>COLING/ACL 1998. Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, Montréal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3 rdWorkshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<contexts>
<context position="12419" citStr="Collins and Brooks, 1995" startWordPosition="2045" endWordPosition="2048">orted percentages are the average of the judges&apos; individual classifications. 399 2. in the case of the &apos;V NPI to NP2&apos; structure, the to-PP must be an argument of the verb; 3. in the case of the &apos;V NPI for NP2&apos; structure, the for-PP must be benefactive.4 In order to meet requirements (1)—(3), we first determined the attachment site (e.g., verb or noun) of the PP and secondly developed a procedure for distinguishing benefactive from non-benefactive PPs. Several approaches have statistically addressed the problem of prepositional phrase ambiguity, with comparable results (Hindle and Rooth, 1993; Collins and Brooks, 1995; Ratnaparkhi, 1998). Hindle and Rooth (1993) used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n. We used a variant of the method described in Hindle and Rooth (1993), the main difference being that we applied their lexical association score (a log-likelihood ratio which compares the probability of noun versus verb attachment) in an unsupervised non-iterative manner. Furthermore, the procedure was applied to the special case of tuples containing the prepositions to and for only. 2.4.1 Evalu</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional phrase attachment through a backed-off model. In Proceedings of the 3 rdWorkshop on Very Large Corpora, pages 27-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology.</title>
<date>1996</date>
<booktitle>In Judith Klavans and</booktitle>
<pages>49--66</pages>
<editor>Philip Resnik, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8975" citStr="Daille, 1996" startWordPosition="1443" endWordPosition="1444">21 87.02 45.40 30.58 29.94 24.04 bank manager tax liability income tax book reviewer designer gear safety plan drama school For sequences of length 2 not found in WordNet, we used the log-likelihood ratio (G-score) to estimate the lexical association between the nouns, in order to determine if they formed a compound noun. We preferred the log-likelihood ratio to other statistical scores, such as the association ratio (Church and Hanks, 1990) or x2, since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize (Dunning, 1993; Daille, 1996). We assumed that two nouns cannot be disjoint arguments of the verb if they are lexically associated. On this basis, tokens were rejected as instances of the double object frame if they contained two nouns whose Gscore had a p-value less than 0.05. A two-step process was applied to noun sequences of length 3: first their bracketing was determined and second the G-score was computed between the single noun and the 2-noun sequence. We inferred the bracketing by modifying an algorithm initially proposed by Pustejovsky et al. (1993). Given three nouns n , n2, n3, if either [n n2] or [ro n3] are i</context>
</contexts>
<marker>Daille, 1996</marker>
<rawString>Beatrice Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. In Judith Klavans and Philip Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Approaches to Language, pages 49-66. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Karin Kipper</author>
<author>Martha Palmer</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Investigating regular sense extensions based on intersective Levin classes. In COLING/ACL</title>
<date>1998</date>
<pages>293--299</pages>
<contexts>
<context position="1778" citStr="Dang et al., 1998" startWordPosition="271" endWordPosition="274">&apos; and the double object frame &apos;V NPI NP2&apos;. The benefactive alternation (cf. (2)) is structurally similar to the dative, the difference being that it involves the preposition for rather than to. Levin (1993) assumes that the syntactic realization of a verb&apos;s arguments is directly correlated with its meaning (cf. also Pinker (1989) for a similar proposal). Thus one would expect verbs that undergo the same alternations to form a semantically coherent class. Levin&apos;s study on diathesis alternations has influenced recent work on word sense disambiguation (Dorr and Jones, 1996), machine translation (Dang et al., 1998), and automatic lexical acquisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998). The objective of this paper is to investigate the extent to which diathesis alternations are empirically attested in corpus data. Using the dative and benefactive alternations as a test case we attempt to determine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame preferences and (c) what the representative members of an alternation are. In section 2 we describe and evaluate the set of automatic methods we used to acquire verbs undergoing the dative and benefact</context>
</contexts>
<marker>Dang, Kipper, Palmer, Rosenzweig, 1998</marker>
<rawString>Hoa Trang Dang, Karin Kipper, Martha Palmer, and Joseph Rosenzweig. 1998. Investigating regular sense extensions based on intersective Levin classes. In COLING/ACL 1998, pages 293-299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Doug Jones</author>
</authors>
<title>Role of word sense disambiguation in lexical acquisition: Predicting semantics from syntactic cues.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>322--327</pages>
<location>Copenhagen.</location>
<contexts>
<context position="1737" citStr="Dorr and Jones, 1996" startWordPosition="264" endWordPosition="267">etween the prepositional frame &apos;V NP1 to NP2&apos; and the double object frame &apos;V NPI NP2&apos;. The benefactive alternation (cf. (2)) is structurally similar to the dative, the difference being that it involves the preposition for rather than to. Levin (1993) assumes that the syntactic realization of a verb&apos;s arguments is directly correlated with its meaning (cf. also Pinker (1989) for a similar proposal). Thus one would expect verbs that undergo the same alternations to form a semantically coherent class. Levin&apos;s study on diathesis alternations has influenced recent work on word sense disambiguation (Dorr and Jones, 1996), machine translation (Dang et al., 1998), and automatic lexical acquisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998). The objective of this paper is to investigate the extent to which diathesis alternations are empirically attested in corpus data. Using the dative and benefactive alternations as a test case we attempt to determine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame preferences and (c) what the representative members of an alternation are. In section 2 we describe and evaluate the set of automatic methods we used to acquire</context>
</contexts>
<marker>Dorr, Jones, 1996</marker>
<rawString>Bonnie J. Dorr and Doug Jones. 1996. Role of word sense disambiguation in lexical acquisition: Predicting semantics from syntactic cues. In Proceedings of the 16th International Conference on Computational Linguistics, pages 322-327, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="8960" citStr="Dunning, 1993" startWordPosition="1441" endWordPosition="1442">nd 1967.68 775.21 87.02 45.40 30.58 29.94 24.04 bank manager tax liability income tax book reviewer designer gear safety plan drama school For sequences of length 2 not found in WordNet, we used the log-likelihood ratio (G-score) to estimate the lexical association between the nouns, in order to determine if they formed a compound noun. We preferred the log-likelihood ratio to other statistical scores, such as the association ratio (Church and Hanks, 1990) or x2, since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize (Dunning, 1993; Daille, 1996). We assumed that two nouns cannot be disjoint arguments of the verb if they are lexically associated. On this basis, tokens were rejected as instances of the double object frame if they contained two nouns whose Gscore had a p-value less than 0.05. A two-step process was applied to noun sequences of length 3: first their bracketing was determined and second the G-score was computed between the single noun and the 2-noun sequence. We inferred the bracketing by modifying an algorithm initially proposed by Pustejovsky et al. (1993). Given three nouns n , n2, n3, if either [n n2] o</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Catherine Macleod</author>
<author>Adam Meyers</author>
</authors>
<title>Comlex syntax: Building a computational lexicon.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>268--272</pages>
<location>Kyoto.</location>
<contexts>
<context position="17874" citStr="Grishman et al., 1994" startWordPosition="2941" endWordPosition="2944">dures we used to guess syntactic structure. We discarded verbs for which we had very little evidence (frame frequency = 1) and applied a relative frequency cutoff: the verb&apos;s acquired frame frequency was compared against its overall frequency in the BNC. Verbs whose relative frame frequency was lower than an empirically established threshold were discarded. The threshold values varied from frame to frame but not from verb to verb and were determined by taking into account for each frame its overall frame frequency which was estimated from the COMLEX subcategorization dictionary (6,000 verbs) (Grishman et al., 1994). This meant that the threshold was higher for less frequent frames (e.g., the double object frame for which only 79 verbs are listed in COMLEX). We also experimented with a method suggested by Brent (1993) which applies the binomial test on frame frequency data. Both methods yielded comparable results. However, the relative frequency threshold worked slightly better and the results reported in the following section are based on this method. 4 Results We acquired 162 verbs for the double object frame, 426 verbs for the &apos;V NP1 to NP2&apos; frame and 962 for the &apos;V NP1 for NP2&apos; frame. Membership in a</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>Ralph Grishman, Catherine Macleod, and Adam Meyers. 1994. Comlex syntax: Building a computational lexicon. In Proceedings of the 15th International Conference on Computational Linguistics, pages 268-272, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="12393" citStr="Hindle and Rooth, 1993" startWordPosition="2041" endWordPosition="2044">rought the paper the reported percentages are the average of the judges&apos; individual classifications. 399 2. in the case of the &apos;V NPI to NP2&apos; structure, the to-PP must be an argument of the verb; 3. in the case of the &apos;V NPI for NP2&apos; structure, the for-PP must be benefactive.4 In order to meet requirements (1)—(3), we first determined the attachment site (e.g., verb or noun) of the PP and secondly developed a procedure for distinguishing benefactive from non-benefactive PPs. Several approaches have statistically addressed the problem of prepositional phrase ambiguity, with comparable results (Hindle and Rooth, 1993; Collins and Brooks, 1995; Ratnaparkhi, 1998). Hindle and Rooth (1993) used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n. We used a variant of the method described in Hindle and Rooth (1993), the main difference being that we applied their lexical association score (a log-likelihood ratio which compares the probability of noun versus verb attachment) in an unsupervised non-iterative manner. Furthermore, the procedure was applied to the special case of tuples containing the prepositions to</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="14170" citStr="Jackendoff, 1990" startWordPosition="2335" endWordPosition="2336">log-likelihood ratio satisfied both requirements (1) and (2) for to-PPs. A low precision of 36% was achieved in detecting instances of noun attachment for for-PPs. One reason for this is the polysemy of the preposition for: for-PPs can be temporal, purposive, benefactive or causal adjuncts and consequently can attach to various sites. Another difficulty is that benefactive forPPs semantically license both attachment sites. To further analyze the poor performance of the log-likelihood ratio on this task, 500 tokens con4Syntactically speaking, benefactive for-PPs are not arguments but adjuncts (Jackendoff, 1990) and can appear on any verb with which they are semantically compatible. taming for-PPs were randomly selected from the parser&apos;s output and disambiguated. Of these 73.9% (K = 0.9, N = 500, k = 2) were instances of verb attachment, which indicates that verb attachments outnumber noun attachments for for-PPs, and therefore a higher precision for verb attachment (cf. requirement (1)) can be achieved without applying the log-likelihood ratio, but instead classifying all instances as verb attachment. 2.5 Benefactive PPs Although surface syntactic cues can be important for determining the attachment</context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>Ray Jackendoff. 1990. Semantic Structures. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Martin Corley</author>
<author>Steffan Corley</author>
<author>Matthew W Crocker</author>
<author>Shari Trewin</author>
</authors>
<title>Gsearch: A tool for syntactic investigation of unparsed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL Workshop on Linguistically Interpreted Corpora,</booktitle>
<location>Bergen.</location>
<contexts>
<context position="3144" citStr="Keller et al., 1999" startWordPosition="498" endWordPosition="501"> and 6 discuss how the derived type and token frequencies can be used to estimate how productive an alternation is for a given verb semantic class and how typical its members are. Finally, section 7 offers some discussion on future work and section 8 conclusive remarks. 2 Method 2.1 The parser The part-of-speech tagged version of the British National Corpus (BNC), a 100 million word collection of written and spoken British English (Burnard, 1995), was used to acquire the frames characteristic of the dative and benefactive alternations. Surface syntactic structure was identified using Gsearch (Keller et al., 1999), a tool which allows the search of arbitrary POS-tagged corpora for shallow syntactic patterns based on a user-specified context-free grammar and a syntactic query. It achieves this by combining a left-corner parser with a regular expression matcher. Depending on the grammar specification (i.e., recursive or not) Gsearch can be used as a full contextfree parser or a chunk parser. Depending on the syntactic query, Gsearch can parse full sentences, identify syntactic relations (e.g., verb-object, adjectivenoun) or even single words (e.g., all indefinite pro397 nouns in the corpus). Gsearch outp</context>
</contexts>
<marker>Keller, Corley, Corley, Crocker, Trewin, 1999</marker>
<rawString>Frank Keller, Martin Corley, Steffan Corley, Matthew W. Crocker, and Shari Trewin. 1999. Gsearch: A tool for syntactic investigation of unparsed corpora. In Proceedings of the EACL Workshop on Linguistically Interpreted Corpora, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="854" citStr="Levin, 1993" startWordPosition="123" endWordPosition="124">stract This paper examines the extent to which verb diathesis alternations are empirically attested in corpus data. We automatically acquire alternating verbs from large balanced corpora by using partialparsing methods and taxonomic information, and discuss how corpus data can be used to quantify linguistic generalizations. We estimate the productivity of an alternation and the typicality of its members using type and token frequencies. 1 Introduction Diathesis alternations are changes in the realization of the argument structure of a verb that are sometimes accompanied by changes in meaning (Levin, 1993). The phenomenon in English is illustrated in (1)—(2) below. John offers shares to his employees. John offers his employees shares. Leave a note for her. Leave her a note. Example (1) illustrates the dative alternation, which is characterized by an alternation between the prepositional frame &apos;V NP1 to NP2&apos; and the double object frame &apos;V NPI NP2&apos;. The benefactive alternation (cf. (2)) is structurally similar to the dative, the difference being that it involves the preposition for rather than to. Levin (1993) assumes that the syntactic realization of a verb&apos;s arguments is directly correlated wit</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="26997" citStr="Marcus et al., 1993" startWordPosition="4483" endWordPosition="4486"> paper relies on frame frequencies acquired from corpora using partialparsing methods. For instance, frame frequency data was used to estimate whether alternating verbs exhibit different preferences for a given frame (typicality). However, it has been shown that corpus idiosyncrasies can affect subcategorization frequencies (cf. Roland and Jurafsky (1998) for an extensive discussion). This suggests that different corpora may give different results with respect to verb alternations. For instance, the to-PP frame is poorly represented in the syntactically annotated version of the Penn Treebank (Marcus et al., 1993). There are only 26 verbs taking the to-PP frame, of which 20 have frame frequency of 1. This indicates that a very small number of verbs undergoing the dative alternation can be potentially acquired from this corpus. In future work we plan to investigate the degree to which corpus differences affect the productivity and typicality estimates for verb alternations. 8 Conclusions This paper explored the degree to which diathesis alternations can be identified in corpus data via shallow syntactic processing. Alternating verbs were acquired from the BNC by using Gsearch as a chunk parser. Erroneou</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Anna Korhonen</author>
</authors>
<title>Detecting verbal participation in diathesis alternations.</title>
<date>1998</date>
<booktitle>In COLING/ACL</booktitle>
<pages>1493--1495</pages>
<publisher>Student Session.</publisher>
<contexts>
<context position="1842" citStr="McCarthy and Korhonen, 1998" startWordPosition="280" endWordPosition="283">ive alternation (cf. (2)) is structurally similar to the dative, the difference being that it involves the preposition for rather than to. Levin (1993) assumes that the syntactic realization of a verb&apos;s arguments is directly correlated with its meaning (cf. also Pinker (1989) for a similar proposal). Thus one would expect verbs that undergo the same alternations to form a semantically coherent class. Levin&apos;s study on diathesis alternations has influenced recent work on word sense disambiguation (Dorr and Jones, 1996), machine translation (Dang et al., 1998), and automatic lexical acquisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998). The objective of this paper is to investigate the extent to which diathesis alternations are empirically attested in corpus data. Using the dative and benefactive alternations as a test case we attempt to determine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame preferences and (c) what the representative members of an alternation are. In section 2 we describe and evaluate the set of automatic methods we used to acquire verbs undergoing the dative and benefactive alternations. We assess the acquired frames using a filterin</context>
</contexts>
<marker>McCarthy, Korhonen, 1998</marker>
<rawString>Diana McCarthy and Anna Korhonen. 1998. Detecting verbal participation in diathesis alternations. In COLING/ACL 1998, pages 1493-1495. Student Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="15532" citStr="Miller et al., 1990" startWordPosition="2550" endWordPosition="2553">for the preposition for which can have several roles, besides the benefactive. Two judges discriminated benefactive from nonbenefactive PPs for 500 tokens, randomly selected from the parser&apos;s output. Only 18.5% (K = 0.73, N = 500, k = 2) of the sample contained benefactive PPs. An analysis of the nouns headed by the preposition for revealed that 59.6% were animate, 17% were collective, 4.9% denoted locations, and the remaining 18.5% denoted events, artifacts, body parts, or actions. Animate, collective and location nouns account for 81.5% of the benefactive data. We used the WordNet taxonomy (Miller et al., 1990) to recognize benefactive PPs (cf. requirement (3)). Nouns in WordNet are organized into an inheritance system defined by hypernymic relations. Instead of being contained in a single hierarchy, nouns are partitioned into a set of semantic primitives (e.g., act, animal, time) which are treated as the unique beginners of separate hierarchies. We compiled a &amp;quot;concept dictionary&amp;quot; from WordNet (87,642 entries), where each entry consisted of the noun and the semantic primitive distinguishing each noun sense (cf. table 4). We considered a for-PP to be benefactive if the noun headed by for was listed i</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>Learnability and Cognition: The Acquisition of Argument Structure.</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="1491" citStr="Pinker (1989)" startWordPosition="226" endWordPosition="227">ish is illustrated in (1)—(2) below. John offers shares to his employees. John offers his employees shares. Leave a note for her. Leave her a note. Example (1) illustrates the dative alternation, which is characterized by an alternation between the prepositional frame &apos;V NP1 to NP2&apos; and the double object frame &apos;V NPI NP2&apos;. The benefactive alternation (cf. (2)) is structurally similar to the dative, the difference being that it involves the preposition for rather than to. Levin (1993) assumes that the syntactic realization of a verb&apos;s arguments is directly correlated with its meaning (cf. also Pinker (1989) for a similar proposal). Thus one would expect verbs that undergo the same alternations to form a semantically coherent class. Levin&apos;s study on diathesis alternations has influenced recent work on word sense disambiguation (Dorr and Jones, 1996), machine translation (Dang et al., 1998), and automatic lexical acquisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998). The objective of this paper is to investigate the extent to which diathesis alternations are empirically attested in corpus data. Using the dative and benefactive alternations as a test case we attempt to determine: (a) if</context>
</contexts>
<marker>Pinker, 1989</marker>
<rawString>Steven Pinker. 1989. Learnability and Cognition: The Acquisition of Argument Structure. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Sabine Bergler</author>
<author>Peter Anick</author>
</authors>
<title>Lexical semantic techniques for corpus analysis.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--3</pages>
<contexts>
<context position="9510" citStr="Pustejovsky et al. (1993)" startWordPosition="1535" endWordPosition="1538">ng words and is less sensitive to rare events and corpussize (Dunning, 1993; Daille, 1996). We assumed that two nouns cannot be disjoint arguments of the verb if they are lexically associated. On this basis, tokens were rejected as instances of the double object frame if they contained two nouns whose Gscore had a p-value less than 0.05. A two-step process was applied to noun sequences of length 3: first their bracketing was determined and second the G-score was computed between the single noun and the 2-noun sequence. We inferred the bracketing by modifying an algorithm initially proposed by Pustejovsky et al. (1993). Given three nouns n , n2, n3, if either [n n2] or [ro n3] are in the compound noun dictionary, we built structures [[n1 n2] n3] or [n [n2 n3]] accordingly; if both [n i n2] and [n2 n3] appear in the dictionary, we chose the most frequent pair; if neither [n1 n2] nor [n2 n3] appear in WordNet, we computed the G-score for [n 1 n2] and [n2 n3] and chose the pair with highest value (p &lt; 0.05). Tables 1 and 2 display a random sample of the compounds the method found (p &lt;0.05). 2.3.1 Evaluation The performance of the linguistic heuristics and the compound detection procedure were evaluated by rand</context>
</contexts>
<marker>Pustejovsky, Bergler, Anick, 1993</marker>
<rawString>James Pustejovsky, Sabine Bergler, and Peter Anick. 1993. Lexical semantic techniques for corpus analysis. Computational Linguistics, 19(3):331-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Unsupervised statistical models for prepositional phrase attachment.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th International Conference on Computational Linguistics,</booktitle>
<pages>1079--1085</pages>
<contexts>
<context position="12439" citStr="Ratnaparkhi, 1998" startWordPosition="2049" endWordPosition="2050">average of the judges&apos; individual classifications. 399 2. in the case of the &apos;V NPI to NP2&apos; structure, the to-PP must be an argument of the verb; 3. in the case of the &apos;V NPI for NP2&apos; structure, the for-PP must be benefactive.4 In order to meet requirements (1)—(3), we first determined the attachment site (e.g., verb or noun) of the PP and secondly developed a procedure for distinguishing benefactive from non-benefactive PPs. Several approaches have statistically addressed the problem of prepositional phrase ambiguity, with comparable results (Hindle and Rooth, 1993; Collins and Brooks, 1995; Ratnaparkhi, 1998). Hindle and Rooth (1993) used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n. We used a variant of the method described in Hindle and Rooth (1993), the main difference being that we applied their lexical association score (a log-likelihood ratio which compares the probability of noun versus verb attachment) in an unsupervised non-iterative manner. Furthermore, the procedure was applied to the special case of tuples containing the prepositions to and for only. 2.4.1 Evaluation We evaluated t</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Unsupervised statistical models for prepositional phrase attachment. In Proceedings of the 7th International Conference on Computational Linguistics, pages 1079-1085.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Roland</author>
<author>Daniel Jurafsky</author>
</authors>
<title>How verb subcategorization frequencies are affected by corpus choice.</title>
<date>1998</date>
<booktitle>In COLING/ACL</booktitle>
<pages>1122--1128</pages>
<contexts>
<context position="26734" citStr="Roland and Jurafsky (1998)" startWordPosition="4442" endWordPosition="4445"> (e.g., ferry). For the benefactive alternation, PERFORMANCE (e.g., sing), PREPARE (e.g., bake) and GET (e.g., buy) verbs are the most typical, whereas CREATE verbs (e.g., compose) are peripheral, which seems intuitively correct. 7 Future Work The work reported in this paper relies on frame frequencies acquired from corpora using partialparsing methods. For instance, frame frequency data was used to estimate whether alternating verbs exhibit different preferences for a given frame (typicality). However, it has been shown that corpus idiosyncrasies can affect subcategorization frequencies (cf. Roland and Jurafsky (1998) for an extensive discussion). This suggests that different corpora may give different results with respect to verb alternations. For instance, the to-PP frame is poorly represented in the syntactically annotated version of the Penn Treebank (Marcus et al., 1993). There are only 26 verbs taking the to-PP frame, of which 20 have frame frequency of 1. This indicates that a very small number of verbs undergoing the dative alternation can be potentially acquired from this corpus. In future work we plan to investigate the degree to which corpus differences affect the productivity and typicality est</context>
</contexts>
<marker>Roland, Jurafsky, 1998</marker>
<rawString>Douglas Roland and Daniel Jurafsky. 1998. How verb subcategorization frequencies are affected by corpus choice. In COLING/ACL 1998, pages 1122-1128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Automatic semantic classification of verbs according to their alternation behaviour. Master&apos;s thesis,</title>
<date>1998</date>
<institution>Institut f&apos;ur Maschinelle Sprachverarbeitung, University of Stuttgart.</institution>
<contexts>
<context position="1867" citStr="Walde, 1998" startWordPosition="286" endWordPosition="287">y similar to the dative, the difference being that it involves the preposition for rather than to. Levin (1993) assumes that the syntactic realization of a verb&apos;s arguments is directly correlated with its meaning (cf. also Pinker (1989) for a similar proposal). Thus one would expect verbs that undergo the same alternations to form a semantically coherent class. Levin&apos;s study on diathesis alternations has influenced recent work on word sense disambiguation (Dorr and Jones, 1996), machine translation (Dang et al., 1998), and automatic lexical acquisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998). The objective of this paper is to investigate the extent to which diathesis alternations are empirically attested in corpus data. Using the dative and benefactive alternations as a test case we attempt to determine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame preferences and (c) what the representative members of an alternation are. In section 2 we describe and evaluate the set of automatic methods we used to acquire verbs undergoing the dative and benefactive alternations. We assess the acquired frames using a filtering method presented in sec</context>
</contexts>
<marker>Walde, 1998</marker>
<rawString>Sabine Schulte im Walde. 1998. Automatic semantic classification of verbs according to their alternation behaviour. Master&apos;s thesis, Institut f&apos;ur Maschinelle Sprachverarbeitung, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N Castellan</author>
</authors>
<title>Non Parametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Sidney Siegel and N Castellan. 1988. Non Parametric Statistics for the Behavioral Sciences. McGraw-Hill, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>