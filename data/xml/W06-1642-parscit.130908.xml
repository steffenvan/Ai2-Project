<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996209">
Fully Automatic Lexicon Expansion
for Domain-oriented Sentiment Analysis
</title>
<author confidence="0.997334">
Hiroshi Kanayama Tetsuya Nasukawa
</author>
<affiliation confidence="0.996161">
Tokyo Research Laboratory, IBM Japan, Ltd.
</affiliation>
<address confidence="0.969373">
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
</address>
<email confidence="0.999642">
{hkana,nasukawa}@jp.ibm.com
</email>
<sectionHeader confidence="0.995672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986391304348">
This paper proposes an unsupervised
lexicon building method for the detec-
tion of polar clauses, which convey pos-
itive or negative aspects in a specific
domain. The lexical entries to be ac-
quired are called polar atoms, the min-
imum human-understandable syntactic
structures that specify the polarity of
clauses. As a clue to obtain candidate
polar atoms, we use context coherency,
the tendency for same polarities to ap-
pear successively in contexts. Using
the overall density and precision of co-
herency in the corpus, the statistical
estimation picks up appropriate polar
atoms among candidates, without any
manual tuning of the threshold values.
The experimental results show that the
precision of polarity assignment with
the automatically acquired lexicon was
94% on average, and our method is ro-
bust for corpora in diverse domains and
for the size of the initial lexicon.
</bodyText>
<sectionHeader confidence="0.998734" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998604642857143">
Sentiment Analysis (SA) (Nasukawa and Yi,
2003; Yi et al., 2003) is a task to recognize
writers’ feelings as expressed in positive or
negative comments, by analyzing unreadably
large numbers of documents. Extensive syn-
tactic patterns enable us to detect sentiment
expressions and to convert them into seman-
tic structures with high precision, as reported
by Kanayama et al. (2004). From the exam-
ple Japanese sentence (1) in the digital cam-
era domain, the SA system extracts a senti-
ment representation as (2), which consists of
a predicate and an argument with positive (+)
polarity.
</bodyText>
<listItem confidence="0.977746666666667">
(1) Kono kamera-ha subarashii-to omou.
`I think this camera is splendid.’
(2) [+] splendid(camera)
</listItem>
<bodyText confidence="0.998638357142857">
SA in general tends to focus on subjec-
tive sentiment expressions, which explicitly de-
scribe an author’s preference as in the above
example (1). Objective (or factual) expres-
sions such as in the following examples (3) and
(4) may be out of scope even though they de-
scribe desirable aspects in a specific domain.
However, when customers or corporate users
use SA system for their commercial activities,
such domain-specific expressions have a more
important role, since they convey strong or
weak points of the product more directly, and
may influence their choice to purchase a spe-
cific product, as an example.
</bodyText>
<listItem confidence="0.88092925">
(3) Kontorasuto-ga kukkiri-suru.
‘The contrast is sharp.’
(4) Atarashii kishu-ha zuumu-mo tsuite-iru.
‘The new model has a zoom lens, too.&apos;
</listItem>
<bodyText confidence="0.999682">
This paper addresses the Japanese ver-
sion of Domain-oriented Sentiment Analysis,
which identifies polar clauses conveying good-
ness and badness in a specific domain, in-
cluding rather objective expressions. Building
domain-dependent lexicons for many domains
is much harder work than preparing domain-
independent lexicons and syntactic patterns,
because the possible lexical entries are too
numerous, and they may differ in each do-
main. To solve this problem, we have devised
an unsupervised method to acquire domain-
dependent lexical knowledge where a user has
only to collect unannotated domain corpora.
The knowledge to be acquired is a domain-
dependent set of polar atoms. A polar atom is
a minimum syntactic structure specifying po-
larity in a predicative expression. For exam-
ple, to detect polar clauses in the sentences (3)
</bodyText>
<page confidence="0.984296">
355
</page>
<note confidence="0.857351">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 355–363,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9749635">
and (4)i, the following polar atoms (5) and (6)
should appear in the lexicon:
</bodyText>
<figure confidence="0.7253515">
(5) [+] kukkiri-suru
`to be sharp’
(6) [+] tsuku +— zuumu-ga
`to have +— zoom lens-NOM’
</figure>
<bodyText confidence="0.984285527272727">
The polar atom (5) specified the positive po-
larity of the verb kukkiri-suru. This atom can
be generally used for this verb regardless of
its arguments. In the polar atom (6), on the
other hand, the nominative case of the verb
tsuku (‘have’) is limited to a specific noun zu-
umu (‘zoom lens’), since the verb tsuku does
not hold the polarity in itself. The automatic
decision for the scopes of the atoms is one of
the major issues.
For lexical learning from unannotated cor-
pora, our method uses context coherency in
terms of polarity, an assumption that polar
clauses with the same polarity appear suc-
cessively unless the context is changed with
adversative expressions. Exploiting this ten-
dency, we can collect candidate polar atoms
with their tentative polarities as those adja-
cent to the polar clauses which have been
identified by their domain-independent polar
atoms in the initial lexicon. We use both intra-
sentential and inter-sentential contexts to ob-
tain more candidate polar atoms.
Our assumption is intuitively reasonable,
but there are many non-polar (neutral) clauses
adjacent to polar clauses. Errors in sentence
delimitation or syntactic parsing also result in
false candidate atoms. Thus, to adopt a can-
didate polar atom for the new lexicon, some
threshold values for the frequencies or ratios
are required, but they depend on the type of
the corpus, the size of the initial lexicon, etc.
Our algorithm is fully automatic in the
sense that the criteria for the adoption of po-
lar atoms are set automatically by statistical
estimation based on the distributions of co-
herency: coherent precision and coherent den-
sity. No manual tuning process is required,
so the algorithm only needs unannotated do-
main corpora and the initial lexicon. Thus
our learning method can be used not only by
the developers of the system, but also by end-
users. This feature is very helpful for users to
&apos;The English translations are included only for con-
venience.
analyze documents in new domains.
In the next section, we review related work,
and Section 3 describes our runtime SA sys-
tem. In Section 4, our assumption for unsu-
pervised learning, context coherency and its
key metrics, coherent precision and coherent
density are discussed. Section 5 describes our
unsupervised learning method. Experimental
results are shown in Section 6, and we conclude
in Section 7.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999868384615385">
Sentiment analysis has been extensively stud-
ied in recent years. The target of SA in this
paper is wider than in previous work. For ex-
ample, Yu and Hatzivassiloglou (2003) sepa-
rated facts from opinions and assigned polari-
ties only to opinions. In contrast, our system
detects factual polar clauses as well as senti-
ments.
Unsupervised learning for sentiment analy-
sis is also being studied. For example, Hatzi-
vassiloglou and McKeown (1997) labeled ad-
jectives as positive or negative, relying on se-
mantic orientation. Turney (2002) used col-
location with “excellent” or “poor” to obtain
positive and negative clues for document clas-
sification. In this paper, we use contextual
information which is wider than for the con-
texts they used, and address the problem of
acquiring lexical entries from the noisy clues.
Inter-sentential contexts as in our approach
were used as a clue also for subjectivity anal-
ysis (Riloff and Wiebe, 2003; Pang and Lee,
2004), which is two-fold classification into sub-
jective and objective sentences. Compared to
it, this paper solves a more difficult problem:
three-fold classification into positive, negative
and non-polar expressions using imperfect co-
herency in terms of sentiment polarity.
Learning methods for phrase-level sentiment
analysis closely share an objective of our ap-
proach. Popescu and Etzioni (2005) achieved
high-precision opinion phrases extraction by
using relaxation labeling. Their method itera-
tively assigns a polarity to a phrase, relying on
semantic orientation of co-occurring words in
specific relations in a sentence, but the scope
of semantic orientation is limited to within a
sentence. Wilson et al. (2005) proposed su-
pervised learning, dividing the resources into
</bodyText>
<page confidence="0.995498">
356
</page>
<figure confidence="0.999183625">
Conjunctive
Patterns
Clauses
► Polarity Assignment
v
+ Polarities
�
Polar Clauses
</figure>
<figureCaption confidence="0.997837">
Figure 1: The flow of the clause-level SA.
</figureCaption>
<figure confidence="0.995451066666667">
� Proposition Detection
v
Propositions
Modality
Patterns
Polar
Atoms
�
Sentence
Delimitation
}
���
Document
to analyze
Sentences
</figure>
<bodyText confidence="0.991654">
prior polarity and context polarity, which are
similar to polar atoms and syntactic patterns
in this paper, respectively. Wilson et al. pre-
pared prior polarities from existing resources,
and learned the context polarities by using
prior polarities and annotated corpora. There-
fore the prerequisite data and learned data
are opposite from those in our approach. We
took the approach used in this paper because
we want to acquire more domain-dependent
knowledge, and context polarity is easier to
access in Japanese&apos;. Our approach and their
work can complement each other.
</bodyText>
<sectionHeader confidence="0.978827" genericHeader="method">
3 Methodology of Clause-level SA
</sectionHeader>
<bodyText confidence="0.999966357142857">
As Figure 1 illustrates, the flow of our sen-
timent analysis system involves three steps.
The first step is sentence delimitation: the in-
put document is divided into sentences. The
second step is proposition detection: proposi-
tions which can form polar clauses are identi-
fied in each sentence. The third step is polarity
assignment: the polarity of each proposition
is examined by considering the polar atoms.
This section describes the last two processes,
which are based on a deep sentiment analy-
sis method analogous to machine translation
(Kanayama et al., 2004) (hereafter “the MT
method”).
</bodyText>
<subsectionHeader confidence="0.997326">
3.1 Proposition Detection
</subsectionHeader>
<bodyText confidence="0.9999035">
Our basic tactic for clause-level SA is the high-
precision detection of polar clauses based on
deep syntactic analysis. ‘Clause-level’ means
that only predicative verbs and adjectives such
</bodyText>
<footnote confidence="0.919867666666667">
2For example, indirect negation such as caused by
a subject “nobody” or a modifier “seldom” is rare in
Japanese.
</footnote>
<bodyText confidence="0.952351">
as in (7) are detected, and adnominal (attribu-
tive) usages of verbs and adjectives as in (8)
are ignored, because utsukushii (‘beautiful’) in
(8) does not convey a positive polarity.
</bodyText>
<figure confidence="0.817049">
(7) E-ga utsukushii.
‘The picture is beautiful.’
(8) Utsukushii hito-ni aitai.
`I want to meet a beautiful person.’
</figure>
<bodyText confidence="0.999909875">
Here we use the notion of a proposition as a
clause without modality, led by a predicative
verb or a predicative adjective. The proposi-
tions detected from a sentence are subject to
the assignment of polarities.
Basically, we detect a proposition only at
the head of a syntactic tree3. However, this
limitation reduces the recall of sentiment anal-
ysis to a very low level. In the example (7)
above, utsukushii is the head of the tree, while
those initial clauses in (9) to (11) below are
not. In order to achieve higher recall while
maintaining high precision, we apply two types
of syntactic patterns, modality patterns and
conjunctive patterns4, to the tree structures
from the full-parsing.
</bodyText>
<equation confidence="0.494919">
(9) Sore-ha utsukushii-to omou.
`I think it is beautiful.’
(10) Sore-ha utsukushiku-nai.
`It is not beautiful.’
(11) Sore-ga utsukushii-to yoi.
`I hope it is beautiful.’
</equation>
<bodyText confidence="0.997029666666667">
Modality patterns match some auxiliary
verbs or corresponding sentence-final expres-
sions, to allow for specific kinds of modality
and negation. One of the typical patterns is
[ v to omou] (`I think v &apos;)5, which allows ut-
sukushii in (9) to be a proposition. Also nega-
tion is handled with a modality pattern, such
as [ v nai] (`not v &apos;). In this case a neg fea-
ture is attached to the proposition to identify
utsukushii in (10) as a negated proposition.
On the other hand, no proposition is identi-
fied in (11) due to the deliberate absence of
a pattern [ v to yoi] (`I hope v &apos;). We used
a total of 103 domain-independent modality
patterns, most of which are derived from the
</bodyText>
<footnote confidence="0.517465833333333">
3This is same as the rightmost part of the sentence
since all Japanese modification is directed left to right.
¢These two types of patterns correspond to auxil-
iary patterns in the MT method, and can be applied
independent of domains.
5 v denotes a verb or an adjective.
</footnote>
<page confidence="0.991391">
357
</page>
<bodyText confidence="0.8986755">
coordinative (roughly `and&apos;)
-te, -shi, -ueni, -dakedenaku, -nominarazu
causal (roughly ‘because’)
-tame, -kara, -node
adversative (roughly `but&apos;)
-ga, -kedo, -keredo, - monono, -nodaga
</bodyText>
<figure confidence="0.998876125">
small-LCD
Intra-sentential
Context
light
�
satisfied
AL �
AL
have-zoom
high-price
splendid
�
V.
�
Inter-sentential
Context
</figure>
<tableCaption confidence="0.72366475">
Table 1: Japanese conjunctions used for con-
junctive patterns.
MT method, and some patterns are manually
added for this work to achieve higher recall.
</tableCaption>
<bodyText confidence="0.9998978">
Another type of pattern is conjunctive pat-
terns, which allow multiple propositions in a
sentence. We used a total of 22 conjunctive
patterns also derived from the MT method, as
exemplified in Table 1. In such cases of coordi-
native clauses and causal clauses, both clauses
can be polar clauses. On the other hand, no
proposition is identified in a conditional clause
due to the absence of corresponding conjunc-
tive patterns.
</bodyText>
<subsectionHeader confidence="0.9775435">
3.2 Polarity Assignment Using Polar
Atoms
</subsectionHeader>
<bodyText confidence="0.999999">
To assign a polarity to each proposition, po-
lar atoms in the lexicon are compared to the
proposition. A polar atom consists of po-
larity, verb or adjective, and optionally, its
arguments. Example (12) is a simple polar
atom, where no argument is specified. This
atom matches any proposition whose head is
utsukushii. Example (13) is a complex polar
atom, which assigns a negative polarity to any
proposition whose head is the verb kaku and
where the accusative case is miryoku.
</bodyText>
<listItem confidence="0.660181333333333">
(12) [+] utsukushii
`to be beautiful’
(13) [−] kaku +— miryoku-wo
</listItem>
<bodyText confidence="0.976207">
`to lack t— attraction-ACC’
A polarity is assigned if there exists a polar
atom for which verb/adjective and the argu-
ments coincide with the proposition, and oth-
erwise no polarity is assigned. The opposite
polarity of the polar atom is assigned to a
proposition which has the neg feature.
We used a total of 3,275 polar atoms, most
of which are derived from an English sentiment
lexicon (Yi et al., 2003).
According to the evaluation of the MT
method (Kanayama et al., 2004), high-
precision sentiment analysis had been achieved
using the polar atoms and patterns, where the
</bodyText>
<figureCaption confidence="0.991719">
Figure 2: The concept of the intra- and inter-
sentential contexts, where the polarities are
perfectly coherent. The symbol `0&apos; denotes
the existence of an adversative conjunction.
</figureCaption>
<bodyText confidence="0.999170375">
system never took positive sentiment for neg-
ative and vice versa, and judged positive or
negative to neutral expressions in only about
10% cases. However, the recall is too low, and
most of the lexicon is for domain-independent
expressions, and thus we need more lexical en-
tries to grasp the positive and negative aspects
in a specific domain.
</bodyText>
<sectionHeader confidence="0.991112" genericHeader="method">
4 Context Coherency
</sectionHeader>
<bodyText confidence="0.99980775">
This section introduces the intra- and inter-
sentential contexts in which we assume context
coherency for polarity, and describes some pre-
liminary analysis of the assumption.
</bodyText>
<sectionHeader confidence="0.8666725" genericHeader="method">
4.1 Intra-sentential and
Inter-sentential Context
</sectionHeader>
<bodyText confidence="0.999886857142857">
The identification of propositions described
in Section 3.1 clarifies our viewpoint of the
contexts. Here we consider two types of
contexts: intra-sentential context and inter-
sentential context. Figure 2 illustrates the
context coherency in a sample discourse (14),
where the polarities are perfectly coherent.
</bodyText>
<listItem confidence="0.954729666666667">
(14) Kono kamera-ha subarashii-to omou.
`I think this camera is splendid.’
Karui-shi, zuumu-mo tsuite-iru.
</listItem>
<bodyText confidence="0.634902">
`It&apos;s light and has a zoom lens.’
</bodyText>
<subsubsectionHeader confidence="0.313728">
Ekishou-ga chiisai-kedo, manzoku-da.
</subsubsectionHeader>
<bodyText confidence="0.906983636363636">
‘Though the LCD is small, I&apos;m satisfied.&apos;
Tada, nedan-ga chotto takai.
`But, the price is a little high.’
The intra-sentential context is the link be-
tween propositions in a sentence, which are
detected as coordinative or causal clauses. If
there is an adversative conjunction such as
-kedo (`but&apos;) in the third sentence in (14), a
flag is attached to the relation, as denoted
with `0&apos; in Figure 2. Though there are dif-
ferences in syntactic phenomena, this is sim-
</bodyText>
<page confidence="0.99352">
358
</page>
<bodyText confidence="0.528092">
shikashi (‘however’), demo (`but&apos;), sorenanoni
(‘even though’), tadashi (`on condition that’),
dakedo (`but&apos;), gyakuni (`on the contrary’),
tohaie (‘although’), keredomo (’however’),
ippou (`on the other hand’)
</bodyText>
<tableCaption confidence="0.8901005">
Table 2: Inter-sentential adversative expres-
sions.
</tableCaption>
<table confidence="0.9999016">
Domain Post. Sent. Len.
digital cameras 263,934 1,757,917 28.3
movies 163,993 637,054 31.5
mobile phones 155,130 609,072 25.3
cars 159,135 959,831 30.9
</table>
<tableCaption confidence="0.999534">
Table 3: The corpora from four domains
</tableCaption>
<bodyText confidence="0.9700206875">
used in this paper. The “Post.” and “Sent.”
columns denote the numbers of postings and
sentences, respectively. “Len.&amp;quot; is the average
length of sentences (in Japanese characters).
ilar to the semantic orientation proposed by
Hatzivassiloglou and McKeown (1997).
The inter-sentential context is the link be-
tween propositions in the main clauses of pairs
of adjacent sentences in a discourse. The po-
larities are assumed to be the same in the
inter-sentential context, unless there is an ad-
versative expression as those listed in Table 2.
If no proposition is detected as in a nominal
sentence, the context is split. That is, there is
no link between the proposition of the previous
sentence and that of the next sentence.
</bodyText>
<subsectionHeader confidence="0.724967">
4.2 Preliminary Study on Context
Coherency
</subsectionHeader>
<bodyText confidence="0.999901833333333">
We claim these two types of context can be
used for unsupervised learning as clues to as-
sign a tentative polarity to unknown expres-
sions. To validate our assumption, we con-
ducted preliminary observations using various
corpora.
</bodyText>
<subsubsectionHeader confidence="0.788308">
4.2.1 Corpora
</subsubsectionHeader>
<bodyText confidence="0.9988085">
Throughout this paper we used Japanese
corpora from discussion boards in four differ-
ent domains, whose features are shown in Ta-
ble 3. All of the corpora have clues to the
boundaries of postings, so they were suitable
to identify the discourses.
</bodyText>
<subsubsectionHeader confidence="0.994438">
4.2.2 Coherent Precision
</subsubsectionHeader>
<bodyText confidence="0.9998708">
How strong is the coherency in the con-
text proposed in Section 4.1? Using the polar
clauses detected by the SA system with the
initial lexicon, we observed the coherent pre-
cision of domain d with lexicon L, defined as:
</bodyText>
<equation confidence="0.997701">
#(Coherent)
cp(d, L) = #(Coherent)+#(Conflict) (15)
</equation>
<bodyText confidence="0.999953604651163">
where #(Coherent) and #(Conflict) are oc-
currence counts of the same and opposite po-
larities observed between two polar clauses as
observed in the discourse. As the two polar
clauses, we consider the following types:
Window. A polar clause and the nearest po-
lar clause which is found in the preceding
n sentences in the discourse.
Context. Two polar clauses in the intra-
sentential and/or inter-sentential context
described in Section 4.1. This is the view-
point of context in our method.
Table 4 shows the frequencies of coherent
pairs, conflicting pairs, and the coherent pre-
cision for half of the digital camera domain
corpus. “Baseline” is the percentage of posi-
tive clauses among the polar clauses6.
For the “Window” method, we tested for
n=0, 1, 2, and oc. “0&amp;quot; means two propositions
within a sentence. Apparently, the larger the
window size, the smaller the cp value. When
the window size is “oo&amp;quot;, implying anywhere
within a discourse, the ratio is larger than the
baseline by only 2.7%, and thus these types
of coherency are not reliable even though the
number of clues is relatively large.
“Context” shows the coherency of the two
types of context that we considered. The
cp values are much higher than those in the
“Window” methods, because the relationships
between adjacent pairs of clauses are handled
more appropriately by considering syntactic
trees, adversative conjunctions, etc. The cp
values for inter-sentential and intra-sentential
contexts are almost the same, and thus both
contexts can be used to obtain 2.5 times more
clues for the intra-sentential context. In the
rest of this paper we will use both contexts.
We also observed the coherent precision for
each domain corpus. The results in the cen-
ter column of Table 5 indicate the number
is slightly different among corpora, but all of
them are far from perfect coherency.
</bodyText>
<footnote confidence="0.941270666666667">
sIf there is a polar clause whose polarity is unknown,
the polarity is correctly predicted with at least 57.0%
precision by assuming “positive”.
</footnote>
<page confidence="0.994532">
359
</page>
<table confidence="0.996315">
Model Coherent Conflict cp(d, L)
Baseline 57.0%
Window n = 0 3,428 1,916 64.1%
n = 1 11,448 6,865 62.5%
n = 2 16,231 10,126 61.6%
n = oo 26,365 17,831 59.7%
Context intra. 2,583 996 72.2%
inter. 3,987 1,533 72.2%
both 6,570 2,529 72.2%
</table>
<figureCaption confidence="0.997556">
Figure 3: The flow of the learning process.
</figureCaption>
<figure confidence="0.999121314285714">
Domain
Corpus d
Initial
Lexicon L
Polar
Clauses
�
SA
�
AL
context
cd(d, L)
v
� cp(d, L)
Candidate
Polar Atoms
f(a), p(a), n(a)
test
1
AL
� ∧ �
?
�
� �
? test
4
New
Lexicon
ID Candidate Polar Atom f(a) p(a) n(a)
1* chiisai &apos;to be small’ 3,014 226 227
2 shikkari-suru &apos;to be firm&apos; 246 54 10
3 chiisai t— bodii-ga 11 4 0
`to be small .— body-NOM’
4 * todoku t— mokuyou-ni 2 0 2
�to be delivered.—on Thursday’
</figure>
<tableCaption confidence="0.69904">
Table 4: Coherent precision with various view-
points of contexts.
</tableCaption>
<table confidence="0.999556">
Domain cp(d, L) cd(d, L)
digital cameras 72.2% 7.23%
movies 76.7% 18.71%
mobile phones 72.9% 7.31%
cars 73.4% 7.36%
</table>
<tableCaption confidence="0.987824">
Table 5: Coherent precision and coherent den-
sity for each domain.
</tableCaption>
<subsubsectionHeader confidence="0.967402">
4.2.3 Coherent Density
</subsubsectionHeader>
<bodyText confidence="0.9976642">
Besides the conflicting cases, there are many
more cases where a polar clause does not ap-
pear in the polar context. We also observed
the coherent density of the domain d with the
lexicon L defined as:
</bodyText>
<equation confidence="0.947649">
heren
cd(d, L) _ #(Polar)t) (16)
</equation>
<bodyText confidence="0.999965083333334">
This indicates the ratio of polar clauses that
appear in the coherent context, among all of
the polar clauses detected by the system.
The right column of Table 5 shows the co-
herent density in each domain. The movie
domain has notably higher coherent density
than the others. This indicates the sentiment
expressions are more frequently used in the
movie domain.
The next section describes the method of
our unsupervised learning using this imperfect
context coherency.
</bodyText>
<sectionHeader confidence="0.9328775" genericHeader="method">
5 Unsupervised Learning for
Acquisition of Polar Atoms
</sectionHeader>
<bodyText confidence="0.998944538461539">
Figure 3 shows the flow of our unsupervised
learning method. First, the runtime SA sys-
tem identifies the polar clauses, and the can-
didate polar atoms are collected. Then, each
candidate atom is validated using the two met-
rics in the previous section, cp and cd, which
are calculated from all of the polar clauses
found in the domain corpus.
Table 6: Examples of candidate polar atoms
and their frequencies. `*&apos; denotes that it
should not be added to the lexicon. f(a), p(a),
and n(a) denote the frequency of the atom and
in positive and negative contexts, respectively.
</bodyText>
<subsectionHeader confidence="0.997499">
5.1 Counts of Candidate Polar Atoms
</subsectionHeader>
<bodyText confidence="0.9999665">
From each proposition which does not have a
polarity, candidate polar atoms in the form of
simple atoms (just a verb or adjective) or com-
plex atoms (a verb or adjective and its right-
most argument consisting of a pair of a noun
and a postpositional) are extracted. For each
candidate polar atom a, the total appearances
f(a), and the occurrences in positive contexts
p(a) and negative contexts n(a) are counted,
based on the context of the adjacent clauses
(using the method described in Section 4.1).
If the proposition has the neg feature, the po-
larity is inverted. Table 6 shows examples of
candidate polar atoms with their frequencies.
</bodyText>
<subsectionHeader confidence="0.9486895">
5.2 Determination for Adding to
Lexicon
</subsectionHeader>
<bodyText confidence="0.999934">
Among the located candidate polar atoms,
how can we distinguish true polar atoms,
which should be added to the lexicon, from
fake polar atoms, which should be discarded?
As shown in Section 4, both the coherent
precision (72-77%) and the coherent density
(7-19%) are so small that we cannot rely on
each single appearance of the atom in the po-
lar context. One possible approach is to set
the threshold values for frequency in a polar
context, max(p(a), n(a)) and for the ratio of
appearances in polar contexts among the to-
</bodyText>
<page confidence="0.972987">
360
</page>
<bodyText confidence="0.930164333333333">
tal appearances, max(p(a),n(a)) f(a) . However, the
optimum threshold values should depend on
the corpus and the initial lexicon.
In order to set general criteria, here we as-
sume that a true positive polar atom a should
have higher p(a)
f(a) than its average i.e. coher-
ent density, cd(d, L+a), and also have higher
p(a) than its average i.e. coherent preci-
</bodyText>
<equation confidence="0.697112">
p(a)+n(a)
</equation>
<bodyText confidence="0.9998815">
sion, cp(d, L+a) and these criteria should be
met with 90% confidence, where L+a is the
initial lexicon with a added. Assuming the bi-
nomial distribution, a candidate polar atom is
adopted as a positive polar atom7 if both (17)
and (18) are satisfied$.
</bodyText>
<equation confidence="0.57075">
q &gt; cd(d, L),
</equation>
<bodyText confidence="0.815428">
where
</bodyText>
<equation confidence="0.9589035">
p(a) (17)
E f(a)Ckqk(1 − q)f(a)−k = 0.9
k=0
r &gt; cp(d, L) or n(a) = 0,
where
p(a) (18)
E p(a)+n(a)Ckrk(1 − r)p(a)+n(a)−k= 0.9
k=0
</equation>
<bodyText confidence="0.9948425">
We can assume cd(d, L+a) ^_ cd(d, L), and
cp(d, L+a) ^_ cp(d, L) when L is large. We
compute the confidence interval using approx-
imation with the F-distribution (Blyth, 1986).
These criteria solve the problems in mini-
mum frequency and scope of the polar atoms
simultaneously. In the example of Table 6, the
simple atom chiisai (ID=1) is discarded be-
cause it does not meet (18), while the complex
atom chiisai +— bodii-ga (ID=3) is adopted
as a positive atom. shikkari-suru (ID=2)
is adopted as a positive simple atom, even
though 10 cases out of 64 were observed in the
negative context. On the other hand, todoku
+— mokuyou-ni (ID=4) is discarded because it
does not meet (17), even though n(a)
</bodyText>
<equation confidence="0.918346">
f(a) = 1.0,
</equation>
<bodyText confidence="0.932451">
i.e. always observed in negative contexts.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="method">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999696">
6.1 Evaluation by Polar Atoms
</subsectionHeader>
<bodyText confidence="0.9795755">
First we propose a method of evaluation of the
lexical learning.
</bodyText>
<footnote confidence="0.996904">
7The criteria for the negative atoms are analogous.
8nC,. notation is used here for combination (n
choose k).
</footnote>
<table confidence="0.9991088">
Annotator B
Positive Neutral Negative
Anno- Positive 65 11 3
tator Neutral 3 72 0
A Negative 1 4 41
</table>
<tableCaption confidence="0.989876">
Table 7: Agreement of two annotators’ judg-
</tableCaption>
<bodyText confidence="0.98967655">
ments of 200 polar atoms. K=0.83.
It is costly to make consistent and large
‘gold standards’ in multiple domains, espe-
cially in identification tasks such as clause-
level SA (cf. classification tasks). Therefore
we evaluated the learning results by asking hu-
man annotators to classify the acquired polar
atoms as positive, negative, and neutral, in-
stead of the instances of polar clauses detected
with the new lexicon. This can be done be-
cause the polar atoms themselves are informa-
tive enough to imply to humans whether the
expressions hold positive or negative meanings
in the domain.
To justify the reliability of this evaluation
method, two annotators9 evaluated 200 ran-
domly selected candidate polar atoms in the
digital camera domain. The agreement results
are shown in Table 7. The manual classifi-
cation was agreed upon in 89% of the cases
and the Kappa value was 0.83, which is high
enough to be considered consistent.
Using manual judgment of the polar atoms,
we evaluated the performance with the follow-
ing three metrics.
Type Precision. The coincidence rate of the
polarity between the acquired polar atom
and the human evaluators’ judgments. It
is always false if the evaluators judged it
as ‘neutral.’
Token Precision. The coincidence rate of
the polarity, weighted by its frequency in
the corpus. This metric emulates the pre-
cision of the detection of polar clauses
with newly acquired poler atoms, in the
runtime SA system.
Relative Recall. The estimated ratio of the
number of detected polar clauses with the
expanded lexicon to the number of de-
tected polar clauses with the initial lex-
</bodyText>
<footnote confidence="0.904639666666667">
9For each domain, we asked different annotators
who are familiar with the domain. They are not the
authors of this paper.
</footnote>
<page confidence="0.986632">
361
</page>
<table confidence="0.999889166666667">
Domain # Type Token Relative
Prec. Prec. Recall
digital cameras 708 65% 96.5% 1.28
movies 462 75% 94.4% 1.19
mobile phones 228 54% 92.1% 1.13
cars 487 68% 91.5% 1.18
</table>
<tableCaption confidence="0.805642666666667">
Table 8: Evaluation results with our method.
The column `#&apos; denotes the number of polar
atoms acquired in each domain.
</tableCaption>
<bodyText confidence="0.997863">
icon. Relative recall will be 1 when no
new polar atom is acquired. Since the pre-
cision was high enough, this metric can
be used for approximation of the recall,
which is hard to evaluate in extraction
tasks such as clause-/phrase-level SA.
</bodyText>
<sectionHeader confidence="0.981831" genericHeader="method">
6.2 Robustness for Different
Conditions
</sectionHeader>
<subsectionHeader confidence="0.869475">
6.2.1 Diversity of Corpora
</subsectionHeader>
<bodyText confidence="0.986394633333333">
For each of the four domain corpora, the an-
notators evaluated 100 randomly selected po-
lar atoms which were newly acquired by our
method, to measure the precisions. Relative
recall is estimated by comparing the numbers
of detected polar clauses from randomly se-
lected 2,000 sentences, with and without the
acquired polar atoms. Table 8 shows the re-
sults. The token precision is higher than 90%
in all of the corpora, including the movie do-
main, which is considered to be difficult for SA
(Turney, 2002). This is extremely high preci-
sion for this task, because the correctness of
both the extraction and polarity assignment
was evaluated simultaneously. The relative re-
call 1.28 in the digital camera domain means
the recall is increased from 43%10 to 55%. The
difference was smaller in other domains, but
the domain-dependent polar clauses are much
informative than general ones, thus the high-
precision detection significantly enhances the
system.
To see the effects of our method, we con-
ducted a control experiment which used pre-
set criteria. To adopt the candidate atom a,
the frequency of polarity, max(p(a), n(a)) was
required to be 3 or more, and the ratio of po-
larity, max(�(a)�n(a)) was required to be higher
�(a)
than the threshold 0. Varying 0 from 0.05 to
</bodyText>
<footnote confidence="0.73819">
10The human evaluation result for digital camera do-
main (Kanayama et al., 2004).
</footnote>
<figure confidence="0.58318">
Relative recall
</figure>
<figureCaption confidence="0.935167">
Figure 4: Relative recall vs. token precision
</figureCaption>
<bodyText confidence="0.9464885">
with various preset threshold values 0 for the
digital camera and movie domains. The right-
most star and circle denote the performance of
our method.
0.8, we evaluated the token precision and the
relative recall in the domains of digital cam-
eras and movies. Figure 4 shows the results.
The results showed both relative recall and
token precision were lower than in our method
for every 0, in both corpora. The optimum 0
was 0.3 in the movie domain and 0.1 in the
digital camera domain. Therefore, in this pre-
set approach, a tuning process is necessary for
each domain. Our method does not require
this tuning, and thus fully automatic learning
was possible.
Unlike the normal precision-recall tradeoff,
the token precision in the movie domain got
lower when the 0 is strict. This is due to the
frequent polar atoms which can be acquired
at the low ratios of the polarity. Our method
does not discard these important polar atoms.
</bodyText>
<subsubsectionHeader confidence="0.518622">
6.2.2 Size of the Initial Lexicon
</subsubsectionHeader>
<bodyText confidence="0.999935133333333">
We also tested the performance while vary-
ing the size of the initial lexicon L. We pre-
pared three subsets of the initial lexicon, L0.8,
L0.5, and L0.2, removing polar atoms ran-
domly. These lexicons had 0.8, 0.5, 0.2 times
the polar atoms, respectively, compared to
L. Table 9 shows the precisions and recalls
using these lexicons for the learning process.
Though the cd values vary, the precision was
stable, which means that our method was ro-
bust even for different sizes of the lexicon. The
smaller the initial lexicon, the higher the rela-
tive recall, because the polar atoms which were
removed from L were recovered in the learning
process. This result suggests the possibility of
</bodyText>
<figure confidence="0.999836666666666">
• θ = 0.3
•
θ = 0.8
(our method)
?
•
? ??
•
?θ = 0.3
•
• ? •
?
1
AL
Token
precision
0.5
?
.1
? θ = 0.1
•
�
θ=0.1
• • θ = 0.05
• • movies
? ?digital cameras
∼∼
1.0 1.1 1.2
? θ = 0.05
�
</figure>
<page confidence="0.987978">
362
</page>
<table confidence="0.9986644">
lexicon cd Token Prec. Relative Rec.
L 7.2% 96.5% 1.28
L0.s 6.1% 97.5% 1.41
L0.s 3.9% 94.2% 2.10
L0.2 3.6% 84.8% 3.55
</table>
<tableCaption confidence="0.986911">
Table 9: Evaluation results for various sizes of
</tableCaption>
<bodyText confidence="0.877918666666667">
the initial lexicon (the digital camera domain).
the bootstrapping method from a small initial
lexicon.
</bodyText>
<subsectionHeader confidence="0.99074">
6.3 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.99998808">
As seen in the agreement study, the polar
atoms used in our study were intrinsically
meaningful to humans. This is because the
atoms are predicate-argument structures de-
rived from predicative clauses, and thus hu-
mans could imagine the meaning of a polar
atom by generating the corresponding sen-
tence in its predicative form.
In the evaluation process, some interesting
results were observed. For example, a nega-
tive atom nai +- kerare-ga (`to be free from
vignetting’) was acquired in the digital cam-
era domain. Even the evaluator who was fa-
miliar with digital cameras did not know the
term kerare (‘vignetting’), but after looking up
the dictionary she labeled it as negative. Our
learning method could pick up such technical
terms and labeled them appropriately.
Also, there were discoveries in the error
analysis. An evaluator assigned positive to aru
+- kamera-ga (`to have camera’) in the mobile
phone domain, but the acquired polar atom
had the negative polarity. This was actually
an insight from the recent opinions that many
users want phones without camera functions11.
</bodyText>
<sectionHeader confidence="0.997669" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.997325466666667">
We proposed an unsupervised method to ac-
quire polar atoms for domain-oriented SA, and
demonstrated its high performance. The lex-
icon can be expanded automatically by us-
ing unannotated corpora, and tuning of the
threshold values is not required. Therefore
even end-users can use this approach to im-
prove the sentiment analysis. These features
allow them to do on-demand analysis of more
narrow domains, such as the domain of digital
&amp;quot;Perhaps because cameras tend to consume battery
power and some users don’t need them.
cameras of a specific manufacturer, or the do-
main of mobile phones from the female users’
point of view.
</bodyText>
<sectionHeader confidence="0.985433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998564">
C. R. Blyth. 1986. Approximate binomial confi-
dence limits. Journal of the American Statistical
Asscoiation, 81(395):843–855.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Proceedings of the 35th ACL
and the 8th EACL, pages 174–181.
Hiroshi Kanayama, Tetsuya Nasukawa, and Hideo
Watanabe. 2004. Deeper sentiment analysis us-
ing machine translation technology. In Proceed-
ings of the 20th COLING, pages 494–500.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using nat-
ural language processing. In Proceedings of the
Second K-CAP, pages 70–77.
Bo Pang and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectiv-
ity summarization based on minimum cuts. In
Proceedings of the 42nd ACL, pages 271–278.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from
reviews. In Proceedings of HLT/EMNLP-05,
pages 339–346.
Ellen Riloff and Janyee Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In
Proceedings of EMNLP-03, pages 105–112.
Peter D. Turney. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proceedings of
the 40th ACL, pages 417–424.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings
of HLT/EMNLP-05, pages 347–354.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In Pro-
ceedings of the Third IEEE International Con-
ference on Data Mining, pages 427–434.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity
of opinion sentences. In Proceedings of EMNLP-
2003, pages 129–136.
</reference>
<page confidence="0.999369">
363
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909290">
<title confidence="0.9993715">Fully Automatic Lexicon for Domain-oriented Sentiment Analysis</title>
<author confidence="0.975719">Hiroshi Kanayama Tetsuya Nasukawa</author>
<affiliation confidence="0.984525">Tokyo Research Laboratory, IBM Japan, Ltd.</affiliation>
<address confidence="0.979118">1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502</address>
<abstract confidence="0.998042791666667">This paper proposes an unsupervised lexicon building method for the detecof which convey positive or negative aspects in a specific domain. The lexical entries to be acare called the minimum human-understandable syntactic structures that specify the polarity of clauses. As a clue to obtain candidate atoms, we use the tendency for same polarities to appear successively in contexts. Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values. The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C R Blyth</author>
</authors>
<title>Approximate binomial confidence limits.</title>
<date>1986</date>
<journal>Journal of the American Statistical Asscoiation,</journal>
<volume>81</volume>
<issue>395</issue>
<contexts>
<context position="23956" citStr="Blyth, 1986" startWordPosition="3911" endWordPosition="3912">average i.e. coherent precip(a)+n(a) sion, cp(d, L+a) and these criteria should be met with 90% confidence, where L+a is the initial lexicon with a added. Assuming the binomial distribution, a candidate polar atom is adopted as a positive polar atom7 if both (17) and (18) are satisfied$. q &gt; cd(d, L), where p(a) (17) E f(a)Ckqk(1 − q)f(a)−k = 0.9 k=0 r &gt; cp(d, L) or n(a) = 0, where p(a) (18) E p(a)+n(a)Ckrk(1 − r)p(a)+n(a)−k= 0.9 k=0 We can assume cd(d, L+a) ^_ cd(d, L), and cp(d, L+a) ^_ cp(d, L) when L is large. We compute the confidence interval using approximation with the F-distribution (Blyth, 1986). These criteria solve the problems in minimum frequency and scope of the polar atoms simultaneously. In the example of Table 6, the simple atom chiisai (ID=1) is discarded because it does not meet (18), while the complex atom chiisai +— bodii-ga (ID=3) is adopted as a positive atom. shikkari-suru (ID=2) is adopted as a positive simple atom, even though 10 cases out of 64 were observed in the negative context. On the other hand, todoku +— mokuyou-ni (ID=4) is discarded because it does not meet (17), even though n(a) f(a) = 1.0, i.e. always observed in negative contexts. 6 Evaluation 6.1 Evalua</context>
</contexts>
<marker>Blyth, 1986</marker>
<rawString>C. R. Blyth. 1986. Approximate binomial confidence limits. Journal of the American Statistical Asscoiation, 81(395):843–855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th ACL and the 8th EACL,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="6558" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="1040" endWordPosition="1044">oherent precision and coherent density are discussed. Section 5 describes our unsupervised learning method. Experimental results are shown in Section 6, and we conclude in Section 7. 2 Related Work Sentiment analysis has been extensively studied in recent years. The target of SA in this paper is wider than in previous work. For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. In contrast, our system detects factual polar clauses as well as sentiments. Unsupervised learning for sentiment analysis is also being studied. For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation. Turney (2002) used collocation with “excellent” or “poor” to obtain positive and negative clues for document classification. In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues. Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences. Compared to i</context>
<context position="16210" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="2574" endWordPosition="2577">ondition that’), dakedo (`but&apos;), gyakuni (`on the contrary’), tohaie (‘although’), keredomo (’however’), ippou (`on the other hand’) Table 2: Inter-sentential adversative expressions. Domain Post. Sent. Len. digital cameras 263,934 1,757,917 28.3 movies 163,993 637,054 31.5 mobile phones 155,130 609,072 25.3 cars 159,135 959,831 30.9 Table 3: The corpora from four domains used in this paper. The “Post.” and “Sent.” columns denote the numbers of postings and sentences, respectively. “Len.&amp;quot; is the average length of sentences (in Japanese characters). ilar to the semantic orientation proposed by Hatzivassiloglou and McKeown (1997). The inter-sentential context is the link between propositions in the main clauses of pairs of adjacent sentences in a discourse. The polarities are assumed to be the same in the inter-sentential context, unless there is an adversative expression as those listed in Table 2. If no proposition is detected as in a nominal sentence, the context is split. That is, there is no link between the proposition of the previous sentence and that of the next sentence. 4.2 Preliminary Study on Context Coherency We claim these two types of context can be used for unsupervised learning as clues to assign a te</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th ACL and the 8th EACL, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Tetsuya Nasukawa</author>
<author>Hideo Watanabe</author>
</authors>
<title>Deeper sentiment analysis using machine translation technology.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING,</booktitle>
<pages>494--500</pages>
<contexts>
<context position="1519" citStr="Kanayama et al. (2004)" startWordPosition="224" endWordPosition="227">he experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon. 1 Introduction Sentiment Analysis (SA) (Nasukawa and Yi, 2003; Yi et al., 2003) is a task to recognize writers’ feelings as expressed in positive or negative comments, by analyzing unreadably large numbers of documents. Extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by Kanayama et al. (2004). From the example Japanese sentence (1) in the digital camera domain, the SA system extracts a sentiment representation as (2), which consists of a predicate and an argument with positive (+) polarity. (1) Kono kamera-ha subarashii-to omou. `I think this camera is splendid.’ (2) [+] splendid(camera) SA in general tends to focus on subjective sentiment expressions, which explicitly describe an author’s preference as in the above example (1). Objective (or factual) expressions such as in the following examples (3) and (4) may be out of scope even though they describe desirable aspects in a spec</context>
<context position="9268" citStr="Kanayama et al., 2004" startWordPosition="1460" endWordPosition="1463">n complement each other. 3 Methodology of Clause-level SA As Figure 1 illustrates, the flow of our sentiment analysis system involves three steps. The first step is sentence delimitation: the input document is divided into sentences. The second step is proposition detection: propositions which can form polar clauses are identified in each sentence. The third step is polarity assignment: the polarity of each proposition is examined by considering the polar atoms. This section describes the last two processes, which are based on a deep sentiment analysis method analogous to machine translation (Kanayama et al., 2004) (hereafter “the MT method”). 3.1 Proposition Detection Our basic tactic for clause-level SA is the highprecision detection of polar clauses based on deep syntactic analysis. ‘Clause-level’ means that only predicative verbs and adjectives such 2For example, indirect negation such as caused by a subject “nobody” or a modifier “seldom” is rare in Japanese. as in (7) are detected, and adnominal (attributive) usages of verbs and adjectives as in (8) are ignored, because utsukushii (‘beautiful’) in (8) does not convey a positive polarity. (7) E-ga utsukushii. ‘The picture is beautiful.’ (8) Utsukus</context>
<context position="13676" citStr="Kanayama et al., 2004" startWordPosition="2193" endWordPosition="2196">proposition whose head is the verb kaku and where the accusative case is miryoku. (12) [+] utsukushii `to be beautiful’ (13) [−] kaku +— miryoku-wo `to lack t— attraction-ACC’ A polarity is assigned if there exists a polar atom for which verb/adjective and the arguments coincide with the proposition, and otherwise no polarity is assigned. The opposite polarity of the polar atom is assigned to a proposition which has the neg feature. We used a total of 3,275 polar atoms, most of which are derived from an English sentiment lexicon (Yi et al., 2003). According to the evaluation of the MT method (Kanayama et al., 2004), highprecision sentiment analysis had been achieved using the polar atoms and patterns, where the Figure 2: The concept of the intra- and intersentential contexts, where the polarities are perfectly coherent. The symbol `0&apos; denotes the existence of an adversative conjunction. system never took positive sentiment for negative and vice versa, and judged positive or negative to neutral expressions in only about 10% cases. However, the recall is too low, and most of the lexicon is for domain-independent expressions, and thus we need more lexical entries to grasp the positive and negative aspects </context>
<context position="28556" citStr="Kanayama et al., 2004" startWordPosition="4685" endWordPosition="4688"> is increased from 43%10 to 55%. The difference was smaller in other domains, but the domain-dependent polar clauses are much informative than general ones, thus the highprecision detection significantly enhances the system. To see the effects of our method, we conducted a control experiment which used preset criteria. To adopt the candidate atom a, the frequency of polarity, max(p(a), n(a)) was required to be 3 or more, and the ratio of polarity, max(�(a)�n(a)) was required to be higher �(a) than the threshold 0. Varying 0 from 0.05 to 10The human evaluation result for digital camera domain (Kanayama et al., 2004). Relative recall Figure 4: Relative recall vs. token precision with various preset threshold values 0 for the digital camera and movie domains. The rightmost star and circle denote the performance of our method. 0.8, we evaluated the token precision and the relative recall in the domains of digital cameras and movies. Figure 4 shows the results. The results showed both relative recall and token precision were lower than in our method for every 0, in both corpora. The optimum 0 was 0.3 in the movie domain and 0.1 in the digital camera domain. Therefore, in this preset approach, a tuning proces</context>
</contexts>
<marker>Kanayama, Nasukawa, Watanabe, 2004</marker>
<rawString>Hiroshi Kanayama, Tetsuya Nasukawa, and Hideo Watanabe. 2004. Deeper sentiment analysis using machine translation technology. In Proceedings of the 20th COLING, pages 494–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: Capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second K-CAP,</booktitle>
<pages>70--77</pages>
<contexts>
<context position="1186" citStr="Nasukawa and Yi, 2003" startWordPosition="171" endWordPosition="174"> clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts. Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values. The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon. 1 Introduction Sentiment Analysis (SA) (Nasukawa and Yi, 2003; Yi et al., 2003) is a task to recognize writers’ feelings as expressed in positive or negative comments, by analyzing unreadably large numbers of documents. Extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by Kanayama et al. (2004). From the example Japanese sentence (1) in the digital camera domain, the SA system extracts a sentiment representation as (2), which consists of a predicate and an argument with positive (+) polarity. (1) Kono kamera-ha subarashii-to omou. `I think this camera is s</context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: Capturing favorability using natural language processing. In Proceedings of the Second K-CAP, pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="7069" citStr="Pang and Lee, 2004" startWordPosition="1125" endWordPosition="1128">ised learning for sentiment analysis is also being studied. For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation. Turney (2002) used collocation with “excellent” or “poor” to obtain positive and negative clues for document classification. In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues. Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences. Compared to it, this paper solves a more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect coherency in terms of sentiment polarity. Learning methods for phrase-level sentiment analysis closely share an objective of our approach. Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling. Their method iteratively assigns a polarity to a phrase, relying on semantic orientation of co-occurring words in specific re</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP-05,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="7462" citStr="Popescu and Etzioni (2005)" startWordPosition="1180" endWordPosition="1183">ntexts they used, and address the problem of acquiring lexical entries from the noisy clues. Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences. Compared to it, this paper solves a more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect coherency in terms of sentiment polarity. Learning methods for phrase-level sentiment analysis closely share an objective of our approach. Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling. Their method iteratively assigns a polarity to a phrase, relying on semantic orientation of co-occurring words in specific relations in a sentence, but the scope of semantic orientation is limited to within a sentence. Wilson et al. (2005) proposed supervised learning, dividing the resources into 356 Conjunctive Patterns Clauses ► Polarity Assignment v + Polarities � Polar Clauses Figure 1: The flow of the clause-level SA. � Proposition Detection v Propositions Modality Patterns Polar Atoms � Sentence Delimitatio</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT/EMNLP-05, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyee Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-03,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="7048" citStr="Riloff and Wiebe, 2003" startWordPosition="1121" endWordPosition="1124"> as sentiments. Unsupervised learning for sentiment analysis is also being studied. For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation. Turney (2002) used collocation with “excellent” or “poor” to obtain positive and negative clues for document classification. In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues. Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences. Compared to it, this paper solves a more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect coherency in terms of sentiment polarity. Learning methods for phrase-level sentiment analysis closely share an objective of our approach. Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling. Their method iteratively assigns a polarity to a phrase, relying on semantic orientation of co-occurring</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyee Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of EMNLP-03, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="6649" citStr="Turney (2002)" startWordPosition="1057" endWordPosition="1058">imental results are shown in Section 6, and we conclude in Section 7. 2 Related Work Sentiment analysis has been extensively studied in recent years. The target of SA in this paper is wider than in previous work. For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. In contrast, our system detects factual polar clauses as well as sentiments. Unsupervised learning for sentiment analysis is also being studied. For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation. Turney (2002) used collocation with “excellent” or “poor” to obtain positive and negative clues for document classification. In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues. Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences. Compared to it, this paper solves a more difficult problem: three-fold classification into positive, neg</context>
<context position="27713" citStr="Turney, 2002" startWordPosition="4547" endWordPosition="4548">ction tasks such as clause-/phrase-level SA. 6.2 Robustness for Different Conditions 6.2.1 Diversity of Corpora For each of the four domain corpora, the annotators evaluated 100 randomly selected polar atoms which were newly acquired by our method, to measure the precisions. Relative recall is estimated by comparing the numbers of detected polar clauses from randomly selected 2,000 sentences, with and without the acquired polar atoms. Table 8 shows the results. The token precision is higher than 90% in all of the corpora, including the movie domain, which is considered to be difficult for SA (Turney, 2002). This is extremely high precision for this task, because the correctness of both the extraction and polarity assignment was evaluated simultaneously. The relative recall 1.28 in the digital camera domain means the recall is increased from 43%10 to 55%. The difference was smaller in other domains, but the domain-dependent polar clauses are much informative than general ones, thus the highprecision detection significantly enhances the system. To see the effects of our method, we conducted a control experiment which used preset criteria. To adopt the candidate atom a, the frequency of polarity, </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP-05,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="7783" citStr="Wilson et al. (2005)" startWordPosition="1228" endWordPosition="1231">is paper solves a more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect coherency in terms of sentiment polarity. Learning methods for phrase-level sentiment analysis closely share an objective of our approach. Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling. Their method iteratively assigns a polarity to a phrase, relying on semantic orientation of co-occurring words in specific relations in a sentence, but the scope of semantic orientation is limited to within a sentence. Wilson et al. (2005) proposed supervised learning, dividing the resources into 356 Conjunctive Patterns Clauses ► Polarity Assignment v + Polarities � Polar Clauses Figure 1: The flow of the clause-level SA. � Proposition Detection v Propositions Modality Patterns Polar Atoms � Sentence Delimitation } ��� Document to analyze Sentences prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. Wilson et al. prepared prior polarities from existing resources, and learned the context polarities by using prior polarities and annotated corpora. Therefore th</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT/EMNLP-05, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In Proceedings of the Third IEEE International Conference on Data Mining,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="1204" citStr="Yi et al., 2003" startWordPosition="175" endWordPosition="178">te polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts. Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values. The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon. 1 Introduction Sentiment Analysis (SA) (Nasukawa and Yi, 2003; Yi et al., 2003) is a task to recognize writers’ feelings as expressed in positive or negative comments, by analyzing unreadably large numbers of documents. Extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by Kanayama et al. (2004). From the example Japanese sentence (1) in the digital camera domain, the SA system extracts a sentiment representation as (2), which consists of a predicate and an argument with positive (+) polarity. (1) Kono kamera-ha subarashii-to omou. `I think this camera is splendid.’ (2) [+] </context>
<context position="13606" citStr="Yi et al., 2003" startWordPosition="2181" endWordPosition="2184"> a complex polar atom, which assigns a negative polarity to any proposition whose head is the verb kaku and where the accusative case is miryoku. (12) [+] utsukushii `to be beautiful’ (13) [−] kaku +— miryoku-wo `to lack t— attraction-ACC’ A polarity is assigned if there exists a polar atom for which verb/adjective and the arguments coincide with the proposition, and otherwise no polarity is assigned. The opposite polarity of the polar atom is assigned to a proposition which has the neg feature. We used a total of 3,275 polar atoms, most of which are derived from an English sentiment lexicon (Yi et al., 2003). According to the evaluation of the MT method (Kanayama et al., 2004), highprecision sentiment analysis had been achieved using the polar atoms and patterns, where the Figure 2: The concept of the intra- and intersentential contexts, where the polarities are perfectly coherent. The symbol `0&apos; denotes the existence of an adversative conjunction. system never took positive sentiment for negative and vice versa, and judged positive or negative to neutral expressions in only about 10% cases. However, the recall is too low, and most of the lexicon is for domain-independent expressions, and thus we</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Proceedings of the Third IEEE International Conference on Data Mining, pages 427–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP2003,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="6292" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="999" endWordPosition="1002">ons are included only for convenience. analyze documents in new domains. In the next section, we review related work, and Section 3 describes our runtime SA system. In Section 4, our assumption for unsupervised learning, context coherency and its key metrics, coherent precision and coherent density are discussed. Section 5 describes our unsupervised learning method. Experimental results are shown in Section 6, and we conclude in Section 7. 2 Related Work Sentiment analysis has been extensively studied in recent years. The target of SA in this paper is wider than in previous work. For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. In contrast, our system detects factual polar clauses as well as sentiments. Unsupervised learning for sentiment analysis is also being studied. For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation. Turney (2002) used collocation with “excellent” or “poor” to obtain positive and negative clues for document classification. In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring l</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP2003, pages 129–136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>