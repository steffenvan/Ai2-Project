<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.232145">
<title confidence="0.971831">
Columbia NLP: Sentiment Detection of Subjective Phrases in Social Media
</title>
<author confidence="0.997311">
Sara Rosenthal
</author>
<affiliation confidence="0.9964975">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.981282">
New York, NY 10027, USA
</address>
<email confidence="0.999432">
sara@cs.columbia.edu
</email>
<author confidence="0.998024">
Kathleen McKeown
</author>
<affiliation confidence="0.996502">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.981288">
New York, NY 10027, USA
</address>
<email confidence="0.999543">
kathy@cs.columbia.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.988899166666667">
We present a supervised sentiment detection
system that classifies the polarity of subjec-
tive phrases as positive, negative, or neutral. It
is tailored towards online genres, specifically
Twitter, through the inclusion of dictionaries
developed to capture vocabulary used in on-
line conversations (e.g., slang and emoticons)
as well as stylistic features common to social
media. We show how to incorporate these
new features within a state of the art system
and evaluate it on subtask A in SemEval-2013
Task 2: Sentiment Analysis in Twitter.
</bodyText>
<sectionHeader confidence="0.99892" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848666666667">
People use social media to write openly about their
personal experiences, likes and dislikes. The follow-
ing sentence from Twitter is a typical example: “To-
morrow I’m coming back from Barcelona...I don’t
want! :(((”. The ability to detect the sentiment ex-
pressed in social media can be useful for understand-
ing what people think about the restaurants they
visit, the political viewpoints of the day, and the
products they buy. These sentiments can be used
to provided targeted advertising, automatically gen-
erate reviews, and make various predictions, such as
political outcomes.
In this paper we develop a sentiment detection al-
gorithm for social media that classifies the polarity
of sentence phrases as positive, negative, or neutral
and test its performance in Twitter through the par-
ticipation in the expression level task (subtask A)
of the SemEval-2013 Task 2: Sentiment Analysis
in Twitter (Wilson et al., 2013) which the authors
helped organize. To do so, we build on previous
work on sentiment detection algorithms for the more
formal news genre, notably the work of Agarwal et
al (2009), but adapt it for the language of social me-
dia, in particular Twitter. We show that exploiting
lexical-stylistic features and dictionaries geared to-
ward social media are useful in detecting sentiment.
In this rest of this paper, we discuss related work,
including the state of the art sentiment system (Agar-
wal et al., 2009) our method is based on, the lexicons
we used, our method, and experiments and results.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999849411764706">
Several recent papers have explored sentiment anal-
ysis in Twitter. Go et al (2009) and Pak and
Paroubek (2010) classify the sentiment of tweets
containing emoticons using n-grams and POS. Bar-
bosa and Feng (2010) detect sentiment using a po-
larity dictionary that includes web vocabulary and
tweet-specific social media features. Bermingham
and Smeaton (2010) compare polarity detection in
twitter to blogs and movie reviews using lexical fea-
tures. Agarwal et al (2011) perform polarity senti-
ment detection on the entire tweet using features that
are somewhat similar to ours: the DAL, lexical fea-
tures (e.g. POS and n-grams), social media features
(e.g. slang and hashtags) and tree kernel features. In
contrast to this related work, our approach is geared
towards predicting sentiment is at the phrase level as
opposed to the tweet level.
</bodyText>
<sectionHeader confidence="0.995638" genericHeader="method">
3 Lexicons
</sectionHeader>
<bodyText confidence="0.980151">
Several lexicons are used in our system. We use the
DAL and expand it with WordNet, as it was used in
</bodyText>
<page confidence="0.975551">
478
</page>
<table confidence="0.969377333333333">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 478–482, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
Corpus DAL NNP Word WordNet Wiktionary Emoticons Punctuation Not
(Post Length- &amp; Numbers Covered
DAL) ening
Twitter - Train 42.9% 19.2% 1.4% 10.2% 12.7% 0.3% 1.5% 11.7%
Twitter - Dev 57.3% 13.8% 1.1% 7.1% 12.2% 0.4% 2.7% 5.4%
Twitter - Test 49.9% 15.6% 1.4% 9.6% 12.1% 0.5% 1.6% 9.3%
SMS - Test 60.1% 3.6% 0.6% 7.9% 14.7% 0.6% 1.9% 10.3%
</table>
<tableCaption confidence="0.99954">
Table 1: Coverage for each of the lexicons in the training and test corpora’s.
</tableCaption>
<bodyText confidence="0.999816571428571">
the original work (Agarwal et al., 2009), and expand
it further to use Wiktionary and an emoticon lexicon.
We consider proper nouns that are not in the DAL to
be objective. We also shorten words that are length-
ened to see if we can find the shortened version in
the lexicons (e.g. sweeeet -* sweet). The coverage
of the lexicons for each corpus is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.978686">
3.1 DAL
</subsectionHeader>
<bodyText confidence="0.976415866666667">
The Dictionary of Affect and Language (DAL)
(Whissel, 1989) is an English language dictionary
of 8742 words built to measure the emotional mean-
ing of texts. In addition to using newswire, it was
also built from individual sources such as interviews
on abuse, students’ retelling of a story, and adoles-
cent’s descriptions of emotions. It therefore covers a
broad set of words. Each word is given three scores
(pleasantness - also called evaluation (ee), active-
ness (aa), and imagery (ii)) on a scale of 1 (low)
to 3 (high). We compute the polarity of a chunk in
the same manner as the original work (Agarwal et
al., 2009), using the sum of the AE Space Score’s
(|�
ee2 + aa2|) of each word within the chunk.
</bodyText>
<subsectionHeader confidence="0.995314">
3.2 WordNet
</subsectionHeader>
<bodyText confidence="0.9999614">
The DAL does cover a broad set of words, but we
will still often encounter words that are not included
in the dictionary. Any word that is not in the DAL
and is not a proper noun is accessed in WordNet
(Fellbaum, 1998) 1 and, if it exists, the DAL scores
of the synonyms of its first sense are used in its
place. In addition to the original approach, if there
are no synonyms we look at the hypernym. We then
compute the average scores (ee, aa, and ii) of all the
words and use that as the score for the word.
</bodyText>
<footnote confidence="0.9533335">
1We cannot use SentiWordNet because we are interested in
the DAL scores
</footnote>
<subsectionHeader confidence="0.994395">
3.3 Wiktionary
</subsectionHeader>
<bodyText confidence="0.999984733333333">
We use Wiktionary, an online dictionary, to supple-
ment the common words that are not found in Word-
Net and the DAL. We first examine all “form of” re-
lationships for the word such as “doesnt” is a “mis-
spelling of” “doesn’t”, and ‘tonite” is an “alternate
form of” “tonight”. If no “form of” relationships ex-
ist, we take all the words in the definitions that have
their own Wiktionary page and look up the scores
for each word in the DAL. (e.g., the verb definition
for LOL (laugh out loud) in Wiktionary is “To laugh
out loud” with “laugh” having its own Wiktionary
definition; it is therefore looked up in the DAL and
the score for “laugh” is used for “LOL”.) We then
compute the average scores (ee, aa, and ii) of all the
words and use that as the score for the word.
</bodyText>
<subsectionHeader confidence="0.961526">
3.4 Emoticon Dictionary
</subsectionHeader>
<bodyText confidence="0.7668645">
emoticon :) :D &lt;3 :( ;)
definition happy laughter love sad wink
</bodyText>
<tableCaption confidence="0.977631">
Table 2: Popular emoticons and their definitions
</tableCaption>
<bodyText confidence="0.999949181818182">
We created a simple lexicon to map common
emoticons to a definition in the DAL. We looked at
over 1000 emoticons gathered from several lists on
the internet2 and computed their frequencies within
a LiveJournal blog corpus. (In the future we would
like to use an external Twitter corpus). We kept
the 192 emoticons that appeared at least once and
mapped each emoticon to a single word definition.
The top 5 emoticons and their definitions are shown
in Table 2. When an emoticon is found in a tweet we
look up its definition in the DAL.
</bodyText>
<sectionHeader confidence="0.9995" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.99965">
We run our data through several pre-processing steps
to preserve emoticons and expand contractions. We
</bodyText>
<footnote confidence="0.990255">
2www.chatropolis.com, www.piology.org, en.wikipedia.org
</footnote>
<page confidence="0.984664">
479
</page>
<table confidence="0.9996309">
General Social Media
Feature Example Feature Example
Capital Words Hello Emoticons :)
Out of Vocabulary duh Acronyms LOL
Punctuation . Repeated Questions ???
Repeated Punctuation #@. Exclamation Points !
Punctuation Count 5 Repeated Exclamations !!!!
Question Marks ? Word Lengthening sweeeet
Ellipses ... All Caps HAHA
Avg Word Length 5 Links/Images www.url.com
</table>
<tableCaption confidence="0.999834">
Table 3: List of lexical-stylistic features and examples.
</tableCaption>
<bodyText confidence="0.999873785714286">
then pre-process the sentences to add Part-of-Speech
tags (POS) and chunk the sentences using the CRF
tagger and chunker (Phan, 2006a; Phan, 2006b).
The chunker uses three labels, ‘B’ (beginning), ‘I’
(in), and ‘O’ (out). The ‘O’ label tends to be ap-
plied to punctuation which one typically wants to
ignore. However, in this context, punctation can be
very important (e.g. exclamation points, and emoti-
cons). Therefore, we append words/phrases tagged
as O to the prior B-I chunk.
We apply the dictionaries to the preprocessed sen-
tences to generate lexical, syntactic, and stylistic
features. All sets of features were reduced using chi-
square in Weka (Hall et al., 2009).
</bodyText>
<subsectionHeader confidence="0.993202">
4.1 Lexical and Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999950555555556">
We include POS tags and the top 500 n-gram fea-
tures(Agarwal et al., 2009). We experimented with
different amounts of n-grams and found that more
than 500 n-grams reduced performance.
The DAL and other dictionaries are used along
with a negation state machine(Agarwal et al., 2009)
to determine the polarity for each word in the sen-
tence. We include all the features described in the
original system (Agarwal et al., 2009).
</bodyText>
<subsectionHeader confidence="0.963667">
4.2 Lexical-Stylistic Features
</subsectionHeader>
<bodyText confidence="0.919626">
We include several lexical-stylistic features (see Ta-
ble 3) that can occur in all datasets. We divide these
features into two groups, general: ones that are
common across online and traditional genres, and
social media: one that are far more common in on-
line genres. Examples of general style features are
exclamation points and ellipses. Examples of social
media style features are emoticons and word length-
ening. Word lengthening is a common phenomenon
</bodyText>
<figureCaption confidence="0.999543333333333">
Figure 1: Percentage of lexical-stylistic features that are
negative (top), neutral (middle), and positive (bottom) in
the Twitter training corpus.
</figureCaption>
<bodyText confidence="0.999017666666667">
in social media where letters are repeated to indi-
cate emphasis (e.g. sweeeet). It is particularly com-
mon in opinionated words (Brody and Diakopoulos,
2011). The count values of each feature was normal-
ized by the number of words in the phrase.
The percentage of lexical-stylistic features that
are positive/negative/neutral is shown in Figure 1.
For example, emoticons tend to indicate a positive
phrase in Twitter. Each stylistic feature accounts for
less than 2% of the sentence but at least one of the
stylistic features exists in 61% of the Tweets.
We also computed the most frequent emoticons
(&lt;3, :D), acronyms (lol), and punctuation symbols
(#) within a subset of the Twitter training set and
included those as additional features.
</bodyText>
<sectionHeader confidence="0.997459" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99994125">
This task was evaluated on the Twitter dataset pro-
vided by Semeval-2013 Task 2, subtask A, which the
authors helped organize. Therefore, a large portion
of time was spent on creating the dataset.
</bodyText>
<page confidence="0.99545">
480
</page>
<table confidence="0.999811727272727">
Experiment Twitter Test SMS
Dev
Majority 36.3 38.1 31.5
Just DAL 70.1 72.3 67.1
WordNet 72.2 73.6 67.7
Wiktionary 72.8 73.7 68.7
Style 71.5 73.7 69.7
n-grams 75.2 75.7 72.5
WordNet+Style 73.2 74.6 70.1
Dictionaries+Style 74.0 75.0 70.2
Dictionaries+Style+n-grams 75.8 77.6 73.3
</table>
<tableCaption confidence="0.988468166666667">
Table 4: Experiments using the Twitter corpus. Results
are shown using average F-measure of the positive and
negative class. All experiments include the DAL. The
dictionaries refer to WordNet, Wiktionary, and Emoticon.
Style refers to Lexical-Stylistic features. All results ex-
ceed the majority baseline significantly.
</tableCaption>
<bodyText confidence="0.999949772727273">
We ran all of our experiments in Weka (Hall et
al., 2009) using Logistic Regression. We also exper-
imented with other learning methods but found that
this worked best. All results are shown using the av-
erage F-measure of the positive and negative class.
We tuned our system for Semeval-2013 Task 2,
subtask A, using the provided development set and
ran it on the provided Twitter and SMS test data.
Our results are shown in Table 4 with all results
being statistically significant over a majority base-
line. We also use the DAL as a baseline to in-
dicate how useful lexical-stylistic features (specifi-
cally those geared towards social media) and the dic-
tionaries are in improving the performance of sen-
timent detection of phrases in online genres in con-
trast to using just the DAL. The results that are statis-
tically significant (computed using the Wilcoxon’s
test, p &lt; .02) shown in bold. Our best results for
each dataset include all features with an average F-
measure of 77.6% and 73.3% for the Twitter and
SMS test sets respectively resulting in a significant
improvement of more than 5% for each test set over
the DAL baseline.
At the time of submission, we had not experi-
mented with n-grams, and therefore chose the Dic-
tionaries+Style system as our final version for the
official run resulting in a rank of 12/22 (75% F-
measure) for Twitter and 13/19 (70.2% F-measure)
for SMS. Our rank with the best system, which in-
cludes n-grams, would remain the same for Twitter,
but bring our rank up to 10/19 for SMS.
We looked more closely at the impact of our new
features and as one would expect, feature selection
found the general and social media style features
(e.g. emoticons, :(, lol, word lengthening) to be use-
ful in Twitter and SMS data. Using additional online
dictionaries is useful in Twitter and SMS, which is
understandable because they both have poor cover-
age in the DAL and WordNet. In all cases using
n-grams was the most useful which indicates that
context is most important. Using Dictionaries and
Style in addition to n-grams did provide a signifi-
cant improvement in the Twitter test set, but not in
the Twitter Dev and SMS test set.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989047619047">
We have explored whether social media features,
Wiktionary, and emoticon dictionaries positively im-
pact the accuracy of polarity detection in Twitter and
other online genres. We found that social media re-
lated features can be used to predict sentiment in
Twitter and SMS. In addition, Wiktionary helps im-
prove the word coverage and though it does not pro-
vide a significant improvement over WordNet, it can
be used in place of WordNet. On the other hand, we
found that using the DAL and n-grams alone does al-
most as well as the best system. This is encouraging
as it indicates that content is important and domain
independent sentiment systems can do a good job of
predicting sentiment in social media.
The results of the SMS messages dataset indicate
that even though the online genres are different, the
training data in one online genre can indeed be used
to predict results with reasonable accuracy in the
other online genre. These results show promise for
further work on domain adaptation across different
kinds of social media.
</bodyText>
<sectionHeader confidence="0.998309" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999779625">
This research was partially funded by (a) the ODNI,
IARPA, through the U.S. Army Research Lab and
(b) the DARPA DEFT Program. All statements of
fact, opinion or conclusions contained herein are
those of the authors and should not be construed as
representing the official views, policies, or positions
of IARPA, the ODNI, the Department of Defense, or
the U.S. Government.
</bodyText>
<page confidence="0.998402">
481
</page>
<sectionHeader confidence="0.989986" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681442622951">
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-
own. 2009. Contextual phrase-level polarity analysis
using lexical affect scoring and syntactic n-grams. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL ’09, pages 24–32, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analy-
sis of twitter data. In Proceedings of the Workshop
on Language in Social Media (LSM 2011), pages 30–
38, Portland, Oregon, June. Association for Computa-
tional Linguistics.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In COLING (Posters), pages 36–44.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, CIKM, pages 1833–1836. ACM.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word
lengthening to detect sentiment in microblogs. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562–570,
Edinburgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA ; London, May.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1–6.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, November.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC’10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Xuan-Hieu Phan. 2006a. Crfchunker: Crf english phrase
chunker.
Xuan-Hieu Phan. 2006b. Crftagger: Crf english phrase
tagger.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors, Emo-
tion: theory research and experience, volume 4, Lon-
don. Acad. Press.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
</reference>
<page confidence="0.998452">
482
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.102830">
<title confidence="0.417386">Columbia NLP: Sentiment Detection of Subjective Phrases in Social Media</title>
<author confidence="0.606395">Sara</author>
<affiliation confidence="0.999778">Department of Computer</affiliation>
<address confidence="0.8969505">Columbia New York, NY 10027,</address>
<email confidence="0.998812">sara@cs.columbia.edu</email>
<author confidence="0.994033">Kathleen</author>
<affiliation confidence="0.999953">Department of Computer</affiliation>
<address confidence="0.8971905">Columbia New York, NY 10027,</address>
<email confidence="0.999746">kathy@cs.columbia.edu</email>
<abstract confidence="0.99279775">We present a supervised sentiment detection system that classifies the polarity of subjective phrases as positive, negative, or neutral. It is tailored towards online genres, specifically Twitter, through the inclusion of dictionaries developed to capture vocabulary used in online conversations (e.g., slang and emoticons) as well as stylistic features common to social media. We show how to incorporate these new features within a state of the art system and evaluate it on subtask A in SemEval-2013</abstract>
<intro confidence="0.398142">Task 2: Sentiment Analysis in Twitter.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Fadi Biadsy</author>
<author>Kathleen R Mckeown</author>
</authors>
<title>Contextual phrase-level polarity analysis using lexical affect scoring and syntactic n-grams.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>24--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1958" citStr="Agarwal et al (2009)" startWordPosition="298" endWordPosition="301">advertising, automatically generate reviews, and make various predictions, such as political outcomes. In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the SemEval-2013 Task 2: Sentiment Analysis in Twitter (Wilson et al., 2013) which the authors helped organize. To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009), but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results. 2 Related Work Several recent papers have explored sentiment analysis in Twitter. Go et al (2009) and Pak and Paroubek (2010) classify the sentiment of tweets containing emoticons using n</context>
<context position="4050" citStr="Agarwal et al., 2009" startWordPosition="640" endWordPosition="643">h International Workshop on Semantic Evaluation (SemEval 2013), pages 478–482, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Corpus DAL NNP Word WordNet Wiktionary Emoticons Punctuation Not (Post Length- &amp; Numbers Covered DAL) ening Twitter - Train 42.9% 19.2% 1.4% 10.2% 12.7% 0.3% 1.5% 11.7% Twitter - Dev 57.3% 13.8% 1.1% 7.1% 12.2% 0.4% 2.7% 5.4% Twitter - Test 49.9% 15.6% 1.4% 9.6% 12.1% 0.5% 1.6% 9.3% SMS - Test 60.1% 3.6% 0.6% 7.9% 14.7% 0.6% 1.9% 10.3% Table 1: Coverage for each of the lexicons in the training and test corpora’s. the original work (Agarwal et al., 2009), and expand it further to use Wiktionary and an emoticon lexicon. We consider proper nouns that are not in the DAL to be objective. We also shorten words that are lengthened to see if we can find the shortened version in the lexicons (e.g. sweeeet -* sweet). The coverage of the lexicons for each corpus is shown in Table 1. 3.1 DAL The Dictionary of Affect and Language (DAL) (Whissel, 1989) is an English language dictionary of 8742 words built to measure the emotional meaning of texts. In addition to using newswire, it was also built from individual sources such as interviews on abuse, student</context>
<context position="8513" citStr="Agarwal et al., 2009" startWordPosition="1418" endWordPosition="1422">er uses three labels, ‘B’ (beginning), ‘I’ (in), and ‘O’ (out). The ‘O’ label tends to be applied to punctuation which one typically wants to ignore. However, in this context, punctation can be very important (e.g. exclamation points, and emoticons). Therefore, we append words/phrases tagged as O to the prior B-I chunk. We apply the dictionaries to the preprocessed sentences to generate lexical, syntactic, and stylistic features. All sets of features were reduced using chisquare in Weka (Hall et al., 2009). 4.1 Lexical and Syntactic Features We include POS tags and the top 500 n-gram features(Agarwal et al., 2009). We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance. The DAL and other dictionaries are used along with a negation state machine(Agarwal et al., 2009) to determine the polarity for each word in the sentence. We include all the features described in the original system (Agarwal et al., 2009). 4.2 Lexical-Stylistic Features We include several lexical-stylistic features (see Table 3) that can occur in all datasets. We divide these features into two groups, general: ones that are common across online and traditional genres, and social media:</context>
</contexts>
<marker>Agarwal, Biadsy, Mckeown, 2009</marker>
<rawString>Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mckeown. 2009. Contextual phrase-level polarity analysis using lexical affect scoring and syntactic n-grams. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 24–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media (LSM 2011),</booktitle>
<pages>30--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="2853" citStr="Agarwal et al (2011)" startWordPosition="443" endWordPosition="446">of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results. 2 Related Work Several recent papers have explored sentiment analysis in Twitter. Go et al (2009) and Pak and Paroubek (2010) classify the sentiment of tweets containing emoticons using n-grams and POS. Barbosa and Feng (2010) detect sentiment using a polarity dictionary that includes web vocabulary and tweet-specific social media features. Bermingham and Smeaton (2010) compare polarity detection in twitter to blogs and movie reviews using lexical features. Agarwal et al (2011) perform polarity sentiment detection on the entire tweet using features that are somewhat similar to ours: the DAL, lexical features (e.g. POS and n-grams), social media features (e.g. slang and hashtags) and tree kernel features. In contrast to this related work, our approach is geared towards predicting sentiment is at the phrase level as opposed to the tweet level. 3 Lexicons Several lexicons are used in our system. We use the DAL and expand it with WordNet, as it was used in 478 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 30– 38, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="2597" citStr="Barbosa and Feng (2010)" startWordPosition="404" endWordPosition="408">or the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results. 2 Related Work Several recent papers have explored sentiment analysis in Twitter. Go et al (2009) and Pak and Paroubek (2010) classify the sentiment of tweets containing emoticons using n-grams and POS. Barbosa and Feng (2010) detect sentiment using a polarity dictionary that includes web vocabulary and tweet-specific social media features. Bermingham and Smeaton (2010) compare polarity detection in twitter to blogs and movie reviews using lexical features. Agarwal et al (2011) perform polarity sentiment detection on the entire tweet using features that are somewhat similar to ours: the DAL, lexical features (e.g. POS and n-grams), social media features (e.g. slang and hashtags) and tree kernel features. In contrast to this related work, our approach is geared towards predicting sentiment is at the phrase level as </context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In COLING (Posters), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan F Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: is brevity an advantage?</title>
<date>2010</date>
<pages>1833--1836</pages>
<editor>In Jimmy Huang, Nick Koudas, Gareth J. F. Jones, Xindong Wu, Kevyn Collins-Thompson, and Aijun An, editors, CIKM,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="2743" citStr="Bermingham and Smeaton (2010)" startWordPosition="425" endWordPosition="428">cial media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results. 2 Related Work Several recent papers have explored sentiment analysis in Twitter. Go et al (2009) and Pak and Paroubek (2010) classify the sentiment of tweets containing emoticons using n-grams and POS. Barbosa and Feng (2010) detect sentiment using a polarity dictionary that includes web vocabulary and tweet-specific social media features. Bermingham and Smeaton (2010) compare polarity detection in twitter to blogs and movie reviews using lexical features. Agarwal et al (2011) perform polarity sentiment detection on the entire tweet using features that are somewhat similar to ours: the DAL, lexical features (e.g. POS and n-grams), social media features (e.g. slang and hashtags) and tree kernel features. In contrast to this related work, our approach is geared towards predicting sentiment is at the phrase level as opposed to the tweet level. 3 Lexicons Several lexicons are used in our system. We use the DAL and expand it with WordNet, as it was used in 478 S</context>
</contexts>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In Jimmy Huang, Nick Koudas, Gareth J. F. Jones, Xindong Wu, Kevyn Collins-Thompson, and Aijun An, editors, CIKM, pages 1833–1836. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--570</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="9653" citStr="Brody and Diakopoulos, 2011" startWordPosition="1597" endWordPosition="1600">s, general: ones that are common across online and traditional genres, and social media: one that are far more common in online genres. Examples of general style features are exclamation points and ellipses. Examples of social media style features are emoticons and word lengthening. Word lengthening is a common phenomenon Figure 1: Percentage of lexical-stylistic features that are negative (top), neutral (middle), and positive (bottom) in the Twitter training corpus. in social media where letters are repeated to indicate emphasis (e.g. sweeeet). It is particularly common in opinionated words (Brody and Diakopoulos, 2011). The count values of each feature was normalized by the number of words in the phrase. The percentage of lexical-stylistic features that are positive/negative/neutral is shown in Figure 1. For example, emoticons tend to indicate a positive phrase in Twitter. Each stylistic feature accounts for less than 2% of the sentence but at least one of the stylistic features exists in 61% of the Tweets. We also computed the most frequent emoticons (&lt;3, :D), acronyms (lol), and punctuation symbols (#) within a subset of the Twitter training set and included those as additional features. 5 Experiments and</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 562–570, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA ; London,</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press, Cambridge, MA ; London, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<booktitle>Processing,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="2468" citStr="Go et al (2009)" startWordPosition="384" endWordPosition="387">n sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009), but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results. 2 Related Work Several recent papers have explored sentiment analysis in Twitter. Go et al (2009) and Pak and Paroubek (2010) classify the sentiment of tweets containing emoticons using n-grams and POS. Barbosa and Feng (2010) detect sentiment using a polarity dictionary that includes web vocabulary and tweet-specific social media features. Bermingham and Smeaton (2010) compare polarity detection in twitter to blogs and movie reviews using lexical features. Agarwal et al (2011) perform polarity sentiment detection on the entire tweet using features that are somewhat similar to ours: the DAL, lexical features (e.g. POS and n-grams), social media features (e.g. slang and hashtags) and tree </context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Processing, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="8403" citStr="Hall et al., 2009" startWordPosition="1400" endWordPosition="1403">h tags (POS) and chunk the sentences using the CRF tagger and chunker (Phan, 2006a; Phan, 2006b). The chunker uses three labels, ‘B’ (beginning), ‘I’ (in), and ‘O’ (out). The ‘O’ label tends to be applied to punctuation which one typically wants to ignore. However, in this context, punctation can be very important (e.g. exclamation points, and emoticons). Therefore, we append words/phrases tagged as O to the prior B-I chunk. We apply the dictionaries to the preprocessed sentences to generate lexical, syntactic, and stylistic features. All sets of features were reduced using chisquare in Weka (Hall et al., 2009). 4.1 Lexical and Syntactic Features We include POS tags and the top 500 n-gram features(Agarwal et al., 2009). We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance. The DAL and other dictionaries are used along with a negation state machine(Agarwal et al., 2009) to determine the polarity for each word in the sentence. We include all the features described in the original system (Agarwal et al., 2009). 4.2 Lexical-Stylistic Features We include several lexical-stylistic features (see Table 3) that can occur in all datasets. We divide these f</context>
<context position="11116" citStr="Hall et al., 2009" startWordPosition="1829" endWordPosition="1832">rity 36.3 38.1 31.5 Just DAL 70.1 72.3 67.1 WordNet 72.2 73.6 67.7 Wiktionary 72.8 73.7 68.7 Style 71.5 73.7 69.7 n-grams 75.2 75.7 72.5 WordNet+Style 73.2 74.6 70.1 Dictionaries+Style 74.0 75.0 70.2 Dictionaries+Style+n-grams 75.8 77.6 73.3 Table 4: Experiments using the Twitter corpus. Results are shown using average F-measure of the positive and negative class. All experiments include the DAL. The dictionaries refer to WordNet, Wiktionary, and Emoticon. Style refers to Lexical-Stylistic features. All results exceed the majority baseline significantly. We ran all of our experiments in Weka (Hall et al., 2009) using Logistic Regression. We also experimented with other learning methods but found that this worked best. All results are shown using the average F-measure of the positive and negative class. We tuned our system for Semeval-2013 Task 2, subtask A, using the provided development set and ran it on the provided Twitter and SMS test data. Our results are shown in Table 4 with all results being statistically significant over a majority baseline. We also use the DAL as a baseline to indicate how useful lexical-stylistic features (specifically those geared towards social media) and the dictionari</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="2496" citStr="Pak and Paroubek (2010)" startWordPosition="389" endWordPosition="392">n algorithms for the more formal news genre, notably the work of Agarwal et al (2009), but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results. 2 Related Work Several recent papers have explored sentiment analysis in Twitter. Go et al (2009) and Pak and Paroubek (2010) classify the sentiment of tweets containing emoticons using n-grams and POS. Barbosa and Feng (2010) detect sentiment using a polarity dictionary that includes web vocabulary and tweet-specific social media features. Bermingham and Smeaton (2010) compare polarity detection in twitter to blogs and movie reviews using lexical features. Agarwal et al (2011) perform polarity sentiment detection on the entire tweet using features that are somewhat similar to ours: the DAL, lexical features (e.g. POS and n-grams), social media features (e.g. slang and hashtags) and tree kernel features. In contrast</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
</authors>
<date>2006</date>
<note>Crfchunker: Crf english phrase chunker.</note>
<contexts>
<context position="7866" citStr="Phan, 2006" startWordPosition="1314" endWordPosition="1315">w.chatropolis.com, www.piology.org, en.wikipedia.org 479 General Social Media Feature Example Feature Example Capital Words Hello Emoticons :) Out of Vocabulary duh Acronyms LOL Punctuation . Repeated Questions ??? Repeated Punctuation #@. Exclamation Points ! Punctuation Count 5 Repeated Exclamations !!!! Question Marks ? Word Lengthening sweeeet Ellipses ... All Caps HAHA Avg Word Length 5 Links/Images www.url.com Table 3: List of lexical-stylistic features and examples. then pre-process the sentences to add Part-of-Speech tags (POS) and chunk the sentences using the CRF tagger and chunker (Phan, 2006a; Phan, 2006b). The chunker uses three labels, ‘B’ (beginning), ‘I’ (in), and ‘O’ (out). The ‘O’ label tends to be applied to punctuation which one typically wants to ignore. However, in this context, punctation can be very important (e.g. exclamation points, and emoticons). Therefore, we append words/phrases tagged as O to the prior B-I chunk. We apply the dictionaries to the preprocessed sentences to generate lexical, syntactic, and stylistic features. All sets of features were reduced using chisquare in Weka (Hall et al., 2009). 4.1 Lexical and Syntactic Features We include POS tags and th</context>
</contexts>
<marker>Phan, 2006</marker>
<rawString>Xuan-Hieu Phan. 2006a. Crfchunker: Crf english phrase chunker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
</authors>
<date>2006</date>
<note>Crftagger: Crf english phrase tagger.</note>
<contexts>
<context position="7866" citStr="Phan, 2006" startWordPosition="1314" endWordPosition="1315">w.chatropolis.com, www.piology.org, en.wikipedia.org 479 General Social Media Feature Example Feature Example Capital Words Hello Emoticons :) Out of Vocabulary duh Acronyms LOL Punctuation . Repeated Questions ??? Repeated Punctuation #@. Exclamation Points ! Punctuation Count 5 Repeated Exclamations !!!! Question Marks ? Word Lengthening sweeeet Ellipses ... All Caps HAHA Avg Word Length 5 Links/Images www.url.com Table 3: List of lexical-stylistic features and examples. then pre-process the sentences to add Part-of-Speech tags (POS) and chunk the sentences using the CRF tagger and chunker (Phan, 2006a; Phan, 2006b). The chunker uses three labels, ‘B’ (beginning), ‘I’ (in), and ‘O’ (out). The ‘O’ label tends to be applied to punctuation which one typically wants to ignore. However, in this context, punctation can be very important (e.g. exclamation points, and emoticons). Therefore, we append words/phrases tagged as O to the prior B-I chunk. We apply the dictionaries to the preprocessed sentences to generate lexical, syntactic, and stylistic features. All sets of features were reduced using chisquare in Weka (Hall et al., 2009). 4.1 Lexical and Syntactic Features We include POS tags and th</context>
</contexts>
<marker>Phan, 2006</marker>
<rawString>Xuan-Hieu Phan. 2006b. Crftagger: Crf english phrase tagger.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Whissel</author>
</authors>
<title>The dictionary of affect in language. In</title>
<date>1989</date>
<volume>4</volume>
<editor>R. Plutchik and H. Kellerman, editors, Emotion: theory research and experience,</editor>
<publisher>Acad. Press.</publisher>
<location>London.</location>
<contexts>
<context position="4443" citStr="Whissel, 1989" startWordPosition="714" endWordPosition="715">Test 49.9% 15.6% 1.4% 9.6% 12.1% 0.5% 1.6% 9.3% SMS - Test 60.1% 3.6% 0.6% 7.9% 14.7% 0.6% 1.9% 10.3% Table 1: Coverage for each of the lexicons in the training and test corpora’s. the original work (Agarwal et al., 2009), and expand it further to use Wiktionary and an emoticon lexicon. We consider proper nouns that are not in the DAL to be objective. We also shorten words that are lengthened to see if we can find the shortened version in the lexicons (e.g. sweeeet -* sweet). The coverage of the lexicons for each corpus is shown in Table 1. 3.1 DAL The Dictionary of Affect and Language (DAL) (Whissel, 1989) is an English language dictionary of 8742 words built to measure the emotional meaning of texts. In addition to using newswire, it was also built from individual sources such as interviews on abuse, students’ retelling of a story, and adolescent’s descriptions of emotions. It therefore covers a broad set of words. Each word is given three scores (pleasantness - also called evaluation (ee), activeness (aa), and imagery (ii)) on a scale of 1 (low) to 3 (high). We compute the polarity of a chunk in the same manner as the original work (Agarwal et al., 2009), using the sum of the AE Space Score’s</context>
</contexts>
<marker>Whissel, 1989</marker>
<rawString>C. M. Whissel. 1989. The dictionary of affect in language. In R. Plutchik and H. Kellerman, editors, Emotion: theory research and experience, volume 4, London. Acad. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1780" citStr="Wilson et al., 2013" startWordPosition="267" endWordPosition="270">derstanding what people think about the restaurants they visit, the political viewpoints of the day, and the products they buy. These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes. In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the SemEval-2013 Task 2: Sentiment Analysis in Twitter (Wilson et al., 2013) which the authors helped organize. To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009), but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results. 2 Related</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>