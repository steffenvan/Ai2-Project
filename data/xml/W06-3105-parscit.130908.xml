<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.846261">
Why Generative Phrase Models Underperform Surface Heuristics
</title>
<author confidence="0.99928">
John DeNero, Dan Gillick, James Zhang, Dan Klein
</author>
<affiliation confidence="0.9994305">
Department of Electrical Engineering and Computer Science
University of California, Berkeley
</affiliation>
<address confidence="0.88352">
Berkeley, CA 94705
</address>
<email confidence="0.992277">
{denero, dgillick, jyzhang, klein}@eecs.berkeley.edu
</email>
<sectionHeader confidence="0.997352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972052631579">
We investigate why weights from generative mod-
els underperform heuristic estimates in phrase-
based machine translation. We first propose a sim-
ple generative, phrase-based model and verify that
its estimates are inferior to those given by surface
statistics. The performance gap stems primarily
from the addition of a hidden segmentation vari-
able, which increases the capacity for overfitting
during maximum likelihood training with EM. In
particular, while word level models benefit greatly
from re-estimation, phrase-level models do not: the
crucial difference is that distinct word alignments
cannot all be correct, while distinct segmentations
can. Alternate segmentations rather than alternate
alignments compete, resulting in increased deter-
minization of the phrase table, decreased general-
ization, and decreased final BLEU score. We also
show that interpolation of the two methods can re-
sult in a modest increase in BLEU score.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973140350878">
At the core of a phrase-based statistical machine
translation system is a phrase table containing
pairs of source and target language phrases, each
weighted by a conditional translation probability.
Koehn et al. (2003a) showed that translation qual-
ity is very sensitive to how this table is extracted
from the training data. One particularly surprising
result is that a simple heuristic extraction algorithm
based on surface statistics of a word-aligned training
set outperformed the phrase-based generative model
proposed by Marcu and Wong (2002).
This result is surprising in light of the reverse sit-
uation for word-based statistical translation. Specif-
ically, in the task of word alignment, heuristic ap-
proaches such as the Dice coefficient consistently
underperform their re-estimated counterparts, such
as the IBM word alignment models (Brown et al.,
1993). This well-known result is unsurprising: re-
estimation introduces an element of competition into
the learning process. The key virtue of competition
in word alignment is that, to a first approximation,
only one source word should generate each target
word. If a good alignment for a word token is found,
other plausible alignments are explained away and
should be discounted as incorrect for that token.
As we show in this paper, this effect does not pre-
vail for phrase-level alignments. The central differ-
ence is that phrase-based models, such as the ones
presented in section 2 or Marcu and Wong (2002),
contain an element of segmentation. That is, they do
not merely learn correspondences between phrases,
but also segmentations of the source and target sen-
tences. However, while it is reasonable to sup-
pose that if one alignment is right, others must be
wrong, the situation is more complex for segmenta-
tions. For example, if one segmentation subsumes
another, they are not necessarily incompatible: both
may be equally valid. While in some cases, such
as idiomatic vs. literal translations, two segmenta-
tions may be in true competition, we show that the
most common result is for different segmentations
to be recruited for different examples, overfitting the
training data and overly determinizing the phrase
translation estimates.
In this work, we first define a novel (but not rad-
ical) generative phrase-based model analogous to
IBM Model 3. While its exact training is intractable,
we describe a training regime which uses word-
level alignments to constrain the space of feasible
segmentations down to a manageable number. We
demonstrate that the phrase analogue of the Dice co-
efficient is superior to our generative model (a re-
sult also echoing previous work). In the primary
contribution of the paper, we present a series of ex-
periments designed to elucidate what re-estimation
learns in this context. We show that estimates are
overly determinized because segmentations are used
</bodyText>
<page confidence="0.9986">
31
</page>
<note confidence="0.883241">
Proceedings of the Workshop on Statistical Machine Translation, pages 31–38,
New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999612571428572">
in unintuitive ways for the sake of data likelihood.
We comment on both the beneficial instances of seg-
ment competition (idioms) as well as the harmful
ones (most everything else). Finally, we demon-
strate that interpolation of the two estimates can
provide a modest increase in BLEU score over the
heuristic baseline.
</bodyText>
<sectionHeader confidence="0.97957" genericHeader="introduction">
2 Approach and Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.99995975">
The generative model defined below is evaluated
based on the BLEU score it produces in an end-
to-end machine translation system from English to
French. The top-performing diag-and extraction
heuristic (Zens et al., 2002) serves as the baseline for
evaluation.1 Each approach – the generative model
and heuristic baseline – produces an estimated con-
ditional distribution of English phrases given French
phrases. We will refer to the distribution derived
from the baseline heuristic as φH. The distribution
learned via the generative model, denoted φEM, is
described in detail below.
</bodyText>
<subsectionHeader confidence="0.997682">
2.1 A Generative Phrase Model
</subsectionHeader>
<bodyText confidence="0.999908357142857">
While our model for computing φEM is novel, it
is meant to exemplify a class of models that are
not only clear extensions to generative word align-
ment models, but also compatible with the statistical
framework assumed during phrase-based decoding.
The generative process we modeled produces a
phrase-aligned English sentence from a French sen-
tence where the former is a translation of the lat-
ter. Note that this generative process is opposite to
the translation direction of the larger system because
of the standard noisy-channel decomposition. The
learned parameters from this model will be used to
translate sentences from English to French. The gen-
erative process modeled has four steps:2
</bodyText>
<footnote confidence="0.785981583333333">
1. Begin with a French sentence f.
1This well-known heuristic extracts phrases from a sentence
pair by computing a word-level alignment for the sentence and
then enumerating all phrases compatible with that alignment.
The word alignment is computed by first intersecting the direc-
tional alignments produced by a generative IBM model (e.g.,
model 4 with minor enhancements) in each translation direc-
tion, then adding certain alignments from the union of the di-
rectional alignments based on local growth rules.
2Our notation matches the literature for phrase-based trans-
lation: a is an English word, a� is an English phrase, and eI1 is a
sequence of I English phrases, and e is an English sentence.
</footnote>
<listItem confidence="0.9882885">
2. Segment f into a sequence of I multi-word
phrases that span the sentence, ¯fi .
3. For each phrase fi E ¯fi , choose a correspond-
ing position j in the English sentence and es-
tablish the alignment aj = i, then generate ex-
actly one English phrase ¯ej from ¯fi.
4. The sequence ¯ej ordered by a describes an En-
glish sentence e.
</listItem>
<bodyText confidence="0.995242">
The corresponding probabilistic model for this gen-
erative process is:
</bodyText>
<equation confidence="0.9827896">
�
P(e|f) =
�fI1 ,EI1,a
E=
1I1 ,EI1,a
</equation>
<bodyText confidence="0.99915225">
where P(e, ¯fi , ¯ei, a|f) factors into a segmentation
model σ, a translation model φ and a distortion
model d. The parameters for each component of this
model are estimated differently:
</bodyText>
<listItem confidence="0.990369857142857">
• The segmentation model σ(¯fi |f) is assumed to
be uniform over all possible segmentations for
a sentence.3
• The phrase translation model φ(¯ej |fi) is pa-
rameterized by a large table of phrase transla-
tion probabilities.
• The distortion model d(aj = i|f) is a discount-
</listItem>
<bodyText confidence="0.9162365">
ing function based on absolute sentence posi-
tion akin to the one used in IBM model 3.
While similar to the joint model in Marcu and Wong
(2002), our model takes a conditional form com-
patible with the statistical assumptions used by the
Pharaoh decoder. Thus, after training, the param-
eters of the phrase translation model φEM can be
used directly for decoding.
</bodyText>
<subsectionHeader confidence="0.997078">
2.2 Training
</subsectionHeader>
<bodyText confidence="0.9991864">
Significant approximation and pruning is required
to train a generative phrase model and table – such
as φEM – with hidden segmentation and alignment
variables using the expectation maximization algo-
rithm (EM). Computing the likelihood of the data
</bodyText>
<footnote confidence="0.85834">
3This segmentation model is deficient given a maximum
phrase length: many segmentations are disallowed in practice.
</footnote>
<equation confidence="0.995196666666667">
P(e, ¯fi , ¯ei, a|f)
σ( ¯f� 1 |f) � φ(¯ej |fi)d(aj = i|f)
fiE �fI1
</equation>
<page confidence="0.987749">
32
</page>
<bodyText confidence="0.999865076923077">
for a set of parameters (the e-step) involves summing
over exponentially many possible segmentations for
each training sentence. Unlike previous attempts to
train a similar model (Marcu and Wong, 2002), we
allow information from a word-alignment model to
inform our approximation. This approach allowed
us to directly estimate translation probabilities even
for rare phrase pairs, which were estimated heuristi-
cally in previous work.
In each iteration of EM, we re-estimate each
phrase translation probability by summing fractional
phrase counts (soft counts) from the data given the
current model parameters.
</bodyText>
<equation confidence="0.986531333333333">
(f,e)
E EfI1 : jiE�fi EeI1:�ejEei Ea:aj=i P(e, �fI1 , eI1, aIf)
EfI1 : !zE�fl EeI1 Ea P(e, fI1 , ei, a|f)
</equation>
<bodyText confidence="0.999164230769231">
This training loop necessitates approximation be-
cause summing over all possible segmentations and
alignments for each sentence is intractable, requiring
time exponential in the length of the sentences. Ad-
ditionally, the set of possible phrase pairs grows too
large to fit in memory. Using word alignments, we
can address both problems.4 In particular, we can
determine for any aligned segmentation ( 1I1, eI1, a)
whether it is compatible with the word-level align-
ment for the sentence pair. We define a phrase pair
to be compatible with a word-alignment if no word
in either phrase is aligned with a word outside the
other phrase (Zens et al., 2002). Then, ( 1I1, eI1, a)
is compatible with the word-alignment if each of its
aligned phrases is a compatible phrase pair.
The training process is then constrained such that,
when evaluating the above sum, only compatible
aligned segmentations are considered. That is, we
allow P(e, �fI1 , eI1, aIf) &gt; 0 only for aligned seg-
mentations ( 1I1, eI1, a) such that a provides a one-
to-one mapping from �fI1 to eI1 where all phrase pairs
(�faj, ej) are compatible with the word alignment.
This constraint has two important effects. First,
we force P(ej |li) = 0 for all phrase pairs not com-
patible with the word-level alignment for some sen-
tence pair. This restriction successfully reduced the
</bodyText>
<footnote confidence="0.946749">
4The word alignments used in approximating the e-step
were the same as those used to create the heuristic diag-and
baseline.
</footnote>
<bodyText confidence="0.99997985">
total legal phrase pair types from approximately 250
million to 17 million for 100,000 training sentences.
However, some desirable phrases were eliminated
because of errors in the word alignments.
Second, the time to compute the e-step is reduced.
While in principle it is still intractable, in practice
we can compute most sentence pairs’ contributions
in under a second each. However, some spurious
word alignments can disallow all segmentations for
a sentence pair, rendering it unusable for training.
Several factors including errors in the word-level
alignments, sparse word alignments and non-literal
translations cause our constraint to rule out approx-
imately 54% of the training set. Thus, the reduced
size of the usable training set accounts for some of
the degraded performance of OEM relative to OH.
However, the results in figure 1 of the following sec-
tion show that OEM trained on twice as much data
as OH still underperforms the heuristic, indicating a
larger issue than decreased training set size.
</bodyText>
<subsectionHeader confidence="0.992418">
2.3 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999997038461538">
To test the relative performance of OEM and OH,
we evaluated each using an end-to-end translation
system from English to French. We chose this non-
standard translation direction so that the examples
in this paper would be more accessible to a primar-
ily English-speaking audience. All training and test
data were drawn from the French/English section of
the Europarl sentence-aligned corpus. We tested on
the first 1,000 unique sentences of length 5 to 15 in
the corpus and trained on sentences of length 1 to 60
starting after the first 10,000.
The system follows the structure proposed in
the documentation for the Pharaoh decoder and
uses many publicly available components (Koehn,
2003b). The language model was generated from
the Europarl corpus using the SRI Language Model-
ing Toolkit (Stolcke, 2002). Pharaoh performed de-
coding using a set of default parameters for weight-
ing the relative influence of the language, translation
and distortion models (Koehn, 2003b). A maximum
phrase length of three was used for all experiments.
To properly compare OEM to OH, all aspects of
the translation pipeline were held constant except for
the parameters of the phrase translation table. In par-
ticular, we did not tune the decoding hyperparame-
ters for the different phrase tables.
</bodyText>
<equation confidence="0.947635">
Onew(ej
fi) = c(
fi, �ej)
=
c(
fi)
</equation>
<page confidence="0.959051">
33
</page>
<figure confidence="0.9937725">
Heuristic
Iteration 1
iteration 3
0.40
0.39
0.38
0.37
0.36
25k 50k 100k
Training sentences
</figure>
<figureCaption confidence="0.969299">
Figure 1: Statistical re-estimation using a generative
phrase model degrades BLEU score relative to its
heuristic initialization.
</figureCaption>
<bodyText confidence="0.74974">
pe
</bodyText>
<sectionHeader confidence="0.996422" genericHeader="method">
3 Results 8
</sectionHeader>
<bodyText confidence="0.999816">
Having generated OH heuristically and OEM with
EM, we now0compare their performance. While the
model and training regimen for OEM differ from the
model fromMarcu and Wong (2002), we achieved
</bodyText>
<figure confidence="0.638624857142857">
20%
results similar to Koehn et al. (2003a): OEM slightly
0%
underperformed OH. Figure 1 compares the BLEU
0 0 20 30 40 50 60
scores using each estimate. Note that the expecta-
Sentence Length
</figure>
<bodyText confidence="0.996686">
tion maximization algorithm for training OEM was
initialized with the heuristic parameters OH, so the
heuristic curve can be equivalently labeled as itera-
tion 0.
Thus, the first iteration of EM increases the ob-
served likelihood of the training sentences while si-
multaneously degrading translation performance on
the test set. As training proceeds, performance on
the test set levels off after three iterations of EM. The
system never achieves the performance of its initial-
ization parameters. The pruning of our training regi-
men accounts for part of this degradation, but not all;
augmenting OEM by adding back in all phrase pairs
that were dropped during training does not close the
performance gap between OEM and OH.
</bodyText>
<subsectionHeader confidence="0.996644">
3.1 Analysis
</subsectionHeader>
<bodyText confidence="0.999984052631579">
Learning OEM degrades translation quality in large
part because EM learns overly determinized seg-
mentations and translation parameters, overfitting
the training data and failing to generalize. The pri-
mary increase in richness from generative word-
level models to generative phrase-level models is
due to the additional latent segmentation variable.
Although we impose a uniform distribution over
segmentations, it nonetheless plays a crucial role
during training. We will characterize this phe-
nomenon through aggregate statistics and transla-
tion examples shortly, but begin by demonstrating
the model’s capacity to overfit the training data.
Let us first return to the motivation behind in-
troducing and learning phrases in machine transla-
tion. For any language pair, there are contiguous
strings of words whose collocational translation is
non-compositional; that is, they translate together
differently than they would in isolation. For in-
stance, chat in French generally translates to cat in
English, but appeler un chat un chat is an idiom
which translates to call a spade a spade. Introduc-
ing phrases allows us to translate chat un chat atom-
ically to spade a spade and vice versa.
While introducing phrases and parameterizing
their translation probabilities with a surface heuris-
tic allows for this possibility, statistical re-estimation
would be required to learn that chat should never be
translated to spade in isolation. Hence, translating I
have a spade with OH could yield an error.
But enforcing competition among segmentations
introduces a new problem: true translation ambigu-
ity can also be spuriously explained by the segmen-
tation. Consider the french fragment carte sur la
table, which could translate to map on the table or
notice on the chart. Using these two sentence pairs
as training, one would hope to capture the ambiguity
in the parameter table as:
</bodyText>
<table confidence="0.915240666666667">
French English O(elf)
carte map 0.5
carte notice 0.5
carte sur map on 0.5
carte sur notice on 0.5
sur on 1.0
... ... ...
table table 0.5
table chart 0.5
</table>
<bodyText confidence="0.998763">
Assuming we only allow non-degenerate seg-
mentations and disallow non-monotonic alignments,
this parameter table yields a marginal likelihood
P(fle) = 0.25 for both sentence pairs – the intu-
itive result given two independent lexical ambigu-
</bodyText>
<figure confidence="0.8325869">
BLEU
34
French English φ(e|f)
carte map 1.0
carte sur notice on 1.0
carte sur la notice on the 1.0
sur on 1.0
sur la table on the table 1.0
la the 1.0
la table the table 1.0
table chart 1.0
ities. However, the following table yields a likeli-
hood of 0.28 for both sentences:5
Entropy
.01 - .5
0 - .01
1.5 - 2
1 - 1.5
.5 - 1
&gt; 2
</figure>
<bodyText confidence="0.999483789473684">
Hence, a higher likelihood can be achieved by al-
locating some phrases to certain translations while
reserving overlapping phrases for others, thereby
failing to model the real ambiguity that exists across
the language pair. Also, notice that the phrase sur
la can take on an arbitrary distribution over any en-
glish phrases without affecting the likelihood of ei-
ther sentence pair. Not only does this counterintu-
itive parameterization give a high data likelihood,
but it is also a fixed point of the EM algorithm.
The phenomenon demonstrated above poses a
problem for generative phrase models in general.
The ambiguous process of translation can be mod-
eled either by the latent segmentation variable or the
phrase translation probabilities. In some cases, opti-
mizing the likelihood of the training corpus adjusts
for the former when we would prefer the latter. We
next investigate how this problem manifests in φEM
and its effect on translation quality.
</bodyText>
<subsectionHeader confidence="0.999684">
3.2 Learned parameters
</subsectionHeader>
<bodyText confidence="0.999948">
The parameters of φEM differ from the heuristically
extracted parameters φH in that the conditional dis-
tributions over English translations for some French
words are sharply peaked for φEM compared to flat-
ter distributions generated by φH. This determinism
– predicted by the previous section’s example – is
not atypical of EM training for other tasks.
To quantify the notion of peaked distributions
over phrase translations, we compute the entropy of
the distribution for each French phrase according to
</bodyText>
<footnote confidence="0.788836333333333">
5For example, summing over the first translation ex-
pands to 1 �(φ(map  |carte)φ(on the table  |sur la table)
+φ(map  |carte)φ(on  |sur)φ(the table  |la table)).
</footnote>
<figureCaption confidence="0.9864105">
Figure 2: Many more French phrases have very low
entropy under the learned parameterization.
</figureCaption>
<bodyText confidence="0.970877">
the standard definition.
</bodyText>
<equation confidence="0.990112">
H(φ(�e |�f)) = � φ(6 |f) lo92 φ(�e |f)
e
</equation>
<bodyText confidence="0.999931631578947">
The average entropy, weighted by frequency, for the
most common 10,000 phrases in the learned table
was 1.55, comparable to 3.76 for the heuristic table.
The difference between the tables becomes much
more striking when we consider the histogram of
entropies for phrases in figure 2. In particular, the
learned table has many more phrases with entropy
near zero. The most pronounced entropy differences
often appear for common phrases. Ten of the most
common phrases in the French corpus are shown in
figure 3.
As more probability mass is reserved for fewer
translations, many of the alternative translations un-
der φH are assigned prohibitively small probabili-
ties. In translating 1,000 test sentences, for example,
no phrase translation with φ(e |f) less than 10−5 was
used by the decoder. Given this empirical threshold,
nearly 60% of entries in φEM are unusable, com-
pared with 1% in φH.
</bodyText>
<subsectionHeader confidence="0.997839">
3.3 Effects on Translation
</subsectionHeader>
<bodyText confidence="0.995104">
While this determinism of φEM may be desirable
in some circumstances, we found that the ambi-
guity in φH is often preferable at decoding time.
</bodyText>
<figure confidence="0.684426">
Learned 0 10 20 30 40
Heuristic % Phrase Translations
</figure>
<page confidence="0.837862">
35
</page>
<equation confidence="0.711371769230769">
1E-04 1E-02 1E+00 1E+02
&apos;
,
.
l
l &apos;
n &apos;
que
qui
plus
l &apos; union
ase
ios
</equation>
<figureCaption confidence="0.877501">
Figure 3: Entropy of 10 common French phrases.
40
</figureCaption>
<bodyText confidence="0.6979435">
Several learned distributions have very low entropy.
30
</bodyText>
<equation confidence="0.458467">
T
</equation>
<bodyText confidence="0.997327652173913">
In particular, the pattern of translation-ambiguous
0
phrases receiving spuriously peaked distributions (as
0 - 01 01 - .5 5 - 1 1
described in section 3.1) introduces new traslation
Entropy
errors relative to the baseline. We now investigate
both positive and negative effects of the learning
process.
The issue that motivated training a generative
model is sometimes resolved correctly: for a word
that translates differently alone than in the context
of an idiom, the translation probabilities can more
accurately reflect this. Returning to the previous ex-
ample, the phrase table for chat has been corrected
through the learning process. The heuristic process
gives the incorrect translation spade with 61% prob-
ability, while the statistical learning approach gives
cat with 95% probability.
While such examples of improvement are en-
couraging, the trend of spurious determinism over-
whelms this benefit by introducing errors in four re-
lated ways, each of which will be explored in turn.
</bodyText>
<listItem confidence="0.967686636363636">
1. Useful phrase pairs can be assigned very low
probabilities and therefore become unusable.
2. A proper translation for a phrase can be over-
ridden by another translation with spuriously
high probability.
3. Error-prone, common, ambiguous phrases be-
come active during decoding.
4. The language model cannot distinguish be-
tween different translation options as effec-
tively due to deterministic translation model
distributions.
</listItem>
<bodyText confidence="0.999872511627907">
The first effect follows from our observation in
section 3.2 that many phrase pairs are unusable due
to vanishingly small probabilities. Some of the en-
tries that are made unusable by re-estimation are
helpful at decoding time, evidenced by the fact
that pruning the set of OEM’s low-scoring learned
phrases from the original heuristic table reduces
BLEU score by 0.02 for 25k training sentences (be-
low the score for OEM).
The second effect is more subtle. Consider the
sentence in figure 4, which to a first approxima-
tion can be translated as a series of cognates, as
demonstrated by the decoding that follows from the
Heuristic
heuristic parameterization OH.6 Notice also that the
Learned
translation probabilities from heuristic extraction are
non-deterministic. On the other hand, the translation
system makes a significant lexical error on this sim-
&gt; 2
ple sentence when parameterized by OEM: the use
of caract´erise in this context is incorrect. This error
arises from a sharply peaked distribution over En-
glish phrases for caract´erise.
This example illustrates a recurring problem: er-
rors do not necessarily arise because a correct trans-
lation is not available. Notice that a preferable trans-
lation of degree as degr´e is available under both pa-
rameterizations. Degr´e is not used, however, be-
cause of the peaked distribution of a competing
translation candidate. In this way, very high prob-
ability translations can effectively block the use of
more appropriate translations at decoding time.
What is furthermore surprising and noteworthy in
this example is that the learned, near-deterministic
translation for caract´erise is not a common trans-
lation for the word. Not only does the statistical
learning process yield low-entropy translation dis-
tributions, but occasionally the translation with un-
desirably high conditional probability does not have
a strong surface correlation with the source phrase.
This example is not unique; during different initial-
izations of the EM algorithm, we noticed such pat-
</bodyText>
<footnote confidence="0.99110475">
6While there is some agreement error and awkwardness, the
heuristic translation is comprehensible to native speakers. The
learned translation incorrectly translates degree, degrading the
translation quality.
</footnote>
<note confidence="0.498068">
Learned Heuristic Entropy
</note>
<figure confidence="0.919230020408163">
Common French Phrases
a
i
36
caract´erise
English O(elf)
characterises 0.49
characterised 0.21
perm eate 0.05
features 0.05
typifies 0.05
degr´e
English O(e|f)
degree 0.49
level 0.38
extent 0.02
amount 0.02
how 0.01
Heuristically Extracted Phrase Table
la situation varie
d &apos; une
immense
degre
the situation
varies
to an
enormous
degree
degree
an enormous
caracterise
une immense
caract´erise
English O(elf)
degree 0.998
characterises 0.001
characterised 0.001
degr´e
English O(e|f)
degree 0.64
level 0.26
extent 0.10
la
varie d &apos;
situation
Learned Phrase Table
the
situation varies
to
</figure>
<figureCaption confidence="0.999968">
Figure 4: Spurious determinism in the learned phrase parameters degrades translation quality.
</figureCaption>
<bodyText confidence="0.999084642857143">
terns even for common French phrases such as de
and ne.
The third source of errors is closely related: com-
mon phrases that translate in many ways depending
on the context can introduce errors if they have a
spuriously peaked distribution. For instance, con-
sider the lone apostrophe, which is treated as a sin-
gle token in our data set (figure 5). The shape of
the heuristic translation distribution for the phrase is
intuitively appealing, showing a relatively flat dis-
tribution among many possible translations. Such
a distribution has very high entropy. On the other
hand, the learned table translates the apostrophe to
the with probability very near 1.
</bodyText>
<table confidence="0.636258">
Heuristic Learned
English OH(e|f) English OEM(e|f)
our 0.10 the 0.99
that 0.09 , 4.1 · 10−3
is 0.06 is 6.5 · 10−4
we 0.05 to 6.3 · 10−4
next 0.05 in 5.3 · 10−4
</table>
<figureCaption confidence="0.624962">
Figure 5: Translation probabilities for an apostro-
</figureCaption>
<bodyText confidence="0.9763188">
phe, the most common french phrase. The learned
table contains a highly peaked distribution.
Such common phrases whose translation depends
highly on the context are ripe for producing transla-
tion errors. The flatness of the distribution of OH en-
sures that the single apostrophe will rarely be used
during decoding because no one phrase table entry
has high enough probability to promote its use. On
the other hand, using the peaked entry OEM(the|&apos;)
incurs virtually no cost to the score of a translation.
The final kind of errors stems from interactions
between the language and translation models. The
selection among translation choices via a language
model – a key virtue of the noisy channel frame-
work – is hindered by the determinism of the transla-
tion model. This effect appears to be less significant
than the previous three. We should note, however,
that adjusting the language and translation model
weights during decoding does not close the perfor-
mance gap between OH and OEM.
</bodyText>
<subsectionHeader confidence="0.83271">
3.4 Improvements
</subsectionHeader>
<bodyText confidence="0.999985">
In light of the low entropy of OEM, we could hope to
improve translations by retaining entropy. There are
several strategies we have considered to achieve this.
Broadly, we have tried two approaches: combin-
ing OEM and OH via heuristic interpolation methods
and modifying the training loop to limit determin-
ism.
The simplest strategy to increase entropy is to
interpolate the heuristic and learned phrase tables.
Varying the weight of interpolation showed an im-
provement over the heuristic of up to 0.01 for 100k
sentences. A more modest improvement of 0.003 for
25k training sentences appears in table 1.
In another experiment, we interpolated the out-
put of each iteration of EM with its input, thereby
maintaining some entropy from the initialization pa-
rameters. BLEU score increased to a maximum of
0.394 using this technique with 100k training sen-
tences, outperforming the heuristic by a slim margin
of 0.005.
We might address the determinization in OEM
without resorting to interpolation by modifying the
</bodyText>
<page confidence="0.997523">
37
</page>
<bodyText confidence="0.999614428571428">
training procedure to retain entropy. By imposing a
non-uniform segmentation model that favors shorter
phrases over longer ones, we hope to prevent the
error-causing effects of EM training outlined above.
In principle, this change will encourage EM to ex-
plain training sentences with shorter sentences. In
practice, however, this approach has not led to an
improvement in BLEU.
Another approach to maintaining entropy during
the training process is to smooth the probabilities
generated by EM. In particular, we can use the fol-
lowing smoothed update equation during the train-
ing loop, which reserves a portion of probability
mass for unseen translations.
</bodyText>
<table confidence="0.89967575">
Estimate BLEU
OH 0.385
OH phrase pairs that also appear in OEM 0.365
OEM 0.374
OEM with a non-uniform segmentation model 0.374
OEM with smoothing 0.381
OEM with gaps filled in by OH 0.374
OEM interpolated with OH 0.388
</table>
<tableCaption confidence="0.998253">
Table 1: BLEU results for 25k training sentences.
</tableCaption>
<sectionHeader confidence="0.997892" genericHeader="method">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997725">
We would like to thank the anonymous reviewers for
their valuable feedback on this paper.
</bodyText>
<equation confidence="0.9900148">
Onew(�ej�
�fz) = c(
fz, ej)
c(
fz) + kl−1
</equation>
<bodyText confidence="0.999896">
In the equation above, l is the length of the French
phrase and k is a tuning parameter. This formula-
tion not only serves to reduce very spiked probabili-
ties in OEM, but also boosts the probability of short
phrases to encourage their use. With k = 2.5, this
smoothing approach improves BLEU by .007 using
25k training sentences, nearly equaling the heuristic
(table 1).
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99994447368421">
Re-estimating phrase translation probabilities using
a generative model holds the promise of improving
upon heuristic techniques. However, the combina-
torial properties of a phrase-based generative model
have unfortunate side effects. In cases of true ambi-
guity in the language pair to be translated, parameter
estimates that explain the ambiguity using segmen-
tation variables can in some cases yield higher data
likelihoods by determinizing phrase translation esti-
mates. However, this behavior in turn leads to errors
at decoding time.
We have also shown that some modest benefit can
be obtained from re-estimation through the blunt in-
strument of interpolation. A remaining challenge is
to design more appropriate statistical models which
tie segmentations together unless sufficient evidence
of true non-compositionality is present; perhaps
such models could properly combine the benefits of
both current approaches.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999841892857143">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2), 1993.
Philipp Koehn. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation. USC Information
Sciences Institute, 2002.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. Sta-
tistical phrase-based translation. HLT-NAACL, 2003.
Philipp Koehn. Pharaoh: A Beam Search Decoder for
Phrase-Based Statisical Machine Translation Models.
USC Information Sciences Institute, 2003.
Daniel Marcu and William Wong. A phrase-based, joint
probability model for statistical machine translation.
Conference on Empirical Methods in Natual Language
Processing, 2002.
Franz Josef Och and Hermann Ney. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51, 2003.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
Improved alignment models for statistical machine
translation. ACL Workshops, 1999.
Andreas Stolcke. Srilm – an extensible language model-
ing toolkit. Proceedings of the International Confer-
ence on Statistical Language Processing, 2002.
Richard Zens, Franz Josef Och and Hermann Ney.
Phrase-Based Statistical Machine Translation. Annual
German Conference on AI, 2002.
</reference>
<page confidence="0.999352">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912908">
<title confidence="0.997186">Why Generative Phrase Models Underperform Surface Heuristics</title>
<author confidence="0.999344">John DeNero</author>
<author confidence="0.999344">Dan Gillick</author>
<author confidence="0.999344">James Zhang</author>
<author confidence="0.999344">Dan</author>
<affiliation confidence="0.999978">Department of Electrical Engineering and Computer University of California,</affiliation>
<address confidence="0.998958">Berkeley, CA</address>
<email confidence="0.961796">dgillick,jyzhang,</email>
<abstract confidence="0.997645">We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2093" citStr="Brown et al., 1993" startWordPosition="298" endWordPosition="301">n quality is very sensitive to how this table is extracted from the training data. One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002). This result is surprising in light of the reverse situation for word-based statistical translation. Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993). This well-known result is unsurprising: reestimation introduces an element of competition into the learning process. The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word. If a good alignment for a word token is found, other plausible alignments are explained away and should be discounted as incorrect for that token. As we show in this paper, this effect does not prevail for phrase-level alignments. The central difference is that phrase-based models, such as the ones presented in section 2 or Marcu and Wong (2</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2), 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>USC Information Sciences Institute,</booktitle>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. Europarl: A Multilingual Corpus for Evaluation of Machine Translation. USC Information Sciences Institute, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation. HLT-NAACL,</title>
<date>2003</date>
<contexts>
<context position="1449" citStr="Koehn et al. (2003" startWordPosition="202" endWordPosition="205">al difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. 1 Introduction At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability. Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002). This result is surprising in light of the reverse situation for word-based statistical translation. Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IB</context>
<context position="13290" citStr="Koehn et al. (2003" startWordPosition="2120" endWordPosition="2123">translation table. In particular, we did not tune the decoding hyperparameters for the different phrase tables. Onew(ej fi) = c( fi, �ej) = c( fi) 33 Heuristic Iteration 1 iteration 3 0.40 0.39 0.38 0.37 0.36 25k 50k 100k Training sentences Figure 1: Statistical re-estimation using a generative phrase model degrades BLEU score relative to its heuristic initialization. pe 3 Results 8 Having generated OH heuristically and OEM with EM, we now0compare their performance. While the model and training regimen for OEM differ from the model fromMarcu and Wong (2002), we achieved 20% results similar to Koehn et al. (2003a): OEM slightly 0% underperformed OH. Figure 1 compares the BLEU 0 0 20 30 40 50 60 scores using each estimate. Note that the expectaSentence Length tion maximization algorithm for training OEM was initialized with the heuristic parameters OH, so the heuristic curve can be equivalently labeled as iteration 0. Thus, the first iteration of EM increases the observed likelihood of the training sentences while simultaneously degrading translation performance on the test set. As training proceeds, performance on the test set levels off after three iterations of EM. The system never achieves the per</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based translation. HLT-NAACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A Beam Search Decoder for Phrase-Based Statisical Machine Translation Models.</title>
<date>2003</date>
<booktitle>USC Information Sciences Institute,</booktitle>
<contexts>
<context position="12195" citStr="Koehn, 2003" startWordPosition="1944" endWordPosition="1945">nd-to-end translation system from English to French. We chose this nonstandard translation direction so that the examples in this paper would be more accessible to a primarily English-speaking audience. All training and test data were drawn from the French/English section of the Europarl sentence-aligned corpus. We tested on the first 1,000 unique sentences of length 5 to 15 in the corpus and trained on sentences of length 1 to 60 starting after the first 10,000. The system follows the structure proposed in the documentation for the Pharaoh decoder and uses many publicly available components (Koehn, 2003b). The language model was generated from the Europarl corpus using the SRI Language Modeling Toolkit (Stolcke, 2002). Pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models (Koehn, 2003b). A maximum phrase length of three was used for all experiments. To properly compare OEM to OH, all aspects of the translation pipeline were held constant except for the parameters of the phrase translation table. In particular, we did not tune the decoding hyperparameters for the different phrase tables. Onew(ej fi)</context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>Philipp Koehn. Pharaoh: A Beam Search Decoder for Phrase-Based Statisical Machine Translation Models. USC Information Sciences Institute, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>Conference on Empirical Methods in Natual Language Processing,</booktitle>
<contexts>
<context position="1779" citStr="Marcu and Wong (2002)" startWordPosition="251" endWordPosition="254">two methods can result in a modest increase in BLEU score. 1 Introduction At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability. Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002). This result is surprising in light of the reverse situation for word-based statistical translation. Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993). This well-known result is unsurprising: reestimation introduces an element of competition into the learning process. The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word. If a good alignment for a wor</context>
<context position="7630" citStr="Marcu and Wong (2002)" startWordPosition="1203" endWordPosition="1206">,EI1,a E= 1I1 ,EI1,a where P(e, ¯fi , ¯ei, a|f) factors into a segmentation model σ, a translation model φ and a distortion model d. The parameters for each component of this model are estimated differently: • The segmentation model σ(¯fi |f) is assumed to be uniform over all possible segmentations for a sentence.3 • The phrase translation model φ(¯ej |fi) is parameterized by a large table of phrase translation probabilities. • The distortion model d(aj = i|f) is a discounting function based on absolute sentence position akin to the one used in IBM model 3. While similar to the joint model in Marcu and Wong (2002), our model takes a conditional form compatible with the statistical assumptions used by the Pharaoh decoder. Thus, after training, the parameters of the phrase translation model φEM can be used directly for decoding. 2.2 Training Significant approximation and pruning is required to train a generative phrase model and table – such as φEM – with hidden segmentation and alignment variables using the expectation maximization algorithm (EM). Computing the likelihood of the data 3This segmentation model is deficient given a maximum phrase length: many segmentations are disallowed in practice. P(e, </context>
<context position="13235" citStr="Marcu and Wong (2002)" startWordPosition="2110" endWordPosition="2113">ere held constant except for the parameters of the phrase translation table. In particular, we did not tune the decoding hyperparameters for the different phrase tables. Onew(ej fi) = c( fi, �ej) = c( fi) 33 Heuristic Iteration 1 iteration 3 0.40 0.39 0.38 0.37 0.36 25k 50k 100k Training sentences Figure 1: Statistical re-estimation using a generative phrase model degrades BLEU score relative to its heuristic initialization. pe 3 Results 8 Having generated OH heuristically and OEM with EM, we now0compare their performance. While the model and training regimen for OEM differ from the model fromMarcu and Wong (2002), we achieved 20% results similar to Koehn et al. (2003a): OEM slightly 0% underperformed OH. Figure 1 compares the BLEU 0 0 20 30 40 50 60 scores using each estimate. Note that the expectaSentence Length tion maximization algorithm for training OEM was initialized with the heuristic parameters OH, so the heuristic curve can be equivalently labeled as iteration 0. Thus, the first iteration of EM increases the observed likelihood of the training sentences while simultaneously degrading translation performance on the test set. As training proceeds, performance on the test set levels off after th</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. A phrase-based, joint probability model for statistical machine translation. Conference on Empirical Methods in Natual Language Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation. ACL Workshops,</title>
<date>1999</date>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. Improved alignment models for statistical machine translation. ACL Workshops, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>Proceedings of the International Conference on Statistical Language Processing,</booktitle>
<contexts>
<context position="12312" citStr="Stolcke, 2002" startWordPosition="1962" endWordPosition="1963">xamples in this paper would be more accessible to a primarily English-speaking audience. All training and test data were drawn from the French/English section of the Europarl sentence-aligned corpus. We tested on the first 1,000 unique sentences of length 5 to 15 in the corpus and trained on sentences of length 1 to 60 starting after the first 10,000. The system follows the structure proposed in the documentation for the Pharaoh decoder and uses many publicly available components (Koehn, 2003b). The language model was generated from the Europarl corpus using the SRI Language Modeling Toolkit (Stolcke, 2002). Pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models (Koehn, 2003b). A maximum phrase length of three was used for all experiments. To properly compare OEM to OH, all aspects of the translation pipeline were held constant except for the parameters of the phrase translation table. In particular, we did not tune the decoding hyperparameters for the different phrase tables. Onew(ej fi) = c( fi, �ej) = c( fi) 33 Heuristic Iteration 1 iteration 3 0.40 0.39 0.38 0.37 0.36 25k 50k 100k Training sentences</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. Srilm – an extensible language modeling toolkit. Proceedings of the International Conference on Statistical Language Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<date>2002</date>
<booktitle>Phrase-Based Statistical Machine Translation. Annual German Conference on AI,</booktitle>
<contexts>
<context position="4813" citStr="Zens et al., 2002" startWordPosition="730" endWordPosition="733"> Association for Computational Linguistics in unintuitive ways for the sake of data likelihood. We comment on both the beneficial instances of segment competition (idioms) as well as the harmful ones (most everything else). Finally, we demonstrate that interpolation of the two estimates can provide a modest increase in BLEU score over the heuristic baseline. 2 Approach and Evaluation Methodology The generative model defined below is evaluated based on the BLEU score it produces in an endto-end machine translation system from English to French. The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach – the generative model and heuristic baseline – produces an estimated conditional distribution of English phrases given French phrases. We will refer to the distribution derived from the baseline heuristic as φH. The distribution learned via the generative model, denoted φEM, is described in detail below. 2.1 A Generative Phrase Model While our model for computing φEM is novel, it is meant to exemplify a class of models that are not only clear extensions to generative word alignment models, but also compatible with the statistical framewor</context>
<context position="9660" citStr="Zens et al., 2002" startWordPosition="1529" endWordPosition="1532"> approximation because summing over all possible segmentations and alignments for each sentence is intractable, requiring time exponential in the length of the sentences. Additionally, the set of possible phrase pairs grows too large to fit in memory. Using word alignments, we can address both problems.4 In particular, we can determine for any aligned segmentation ( 1I1, eI1, a) whether it is compatible with the word-level alignment for the sentence pair. We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002). Then, ( 1I1, eI1, a) is compatible with the word-alignment if each of its aligned phrases is a compatible phrase pair. The training process is then constrained such that, when evaluating the above sum, only compatible aligned segmentations are considered. That is, we allow P(e, �fI1 , eI1, aIf) &gt; 0 only for aligned segmentations ( 1I1, eI1, a) such that a provides a oneto-one mapping from �fI1 to eI1 where all phrase pairs (�faj, ej) are compatible with the word alignment. This constraint has two important effects. First, we force P(ej |li) = 0 for all phrase pairs not compatible with the wo</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Josef Och and Hermann Ney. Phrase-Based Statistical Machine Translation. Annual German Conference on AI, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>