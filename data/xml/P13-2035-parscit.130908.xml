<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.73195">
Is word-to-phone mapping better than phone-phone mapping for
handling English words?
Naresh Kumar Elluru
</title>
<note confidence="0.777160333333333">
Speech and Vision Lab
IIIT Hyderabad, India
nareshkumar.elluru@
research.iiit.ac.in
Anandaswarup Vadapalli
Speech and Vision Lab
IIIT Hyderabad, India
anandaswarup.vadapalli@
research.iiit.ac.in
</note>
<author confidence="0.634932">
Raghavendra Elluru
</author>
<affiliation confidence="0.587925">
Speech and Vision Lab
IIIT Hyderabad, India
</affiliation>
<email confidence="0.5644025">
raghavendra.veera@
gmail.com
</email>
<author confidence="0.994499">
Hema Murthy
</author>
<affiliation confidence="0.9297215">
Department of CSE
IIT Madras, India
</affiliation>
<email confidence="0.988288">
hema@iitm.ac.in
</email>
<author confidence="0.842534">
Kishore Prahallad
</author>
<affiliation confidence="0.8010885">
Speech and Vision Lab
IIIT Hyderabad, India
</affiliation>
<email confidence="0.995128">
kishore@iiit.ac.in
</email>
<sectionHeader confidence="0.995603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933846153846">
In this paper, we relook at the problem
of pronunciation of English words using
native phone set. Specifically, we in-
vestigate methods of pronouncing English
words using Telugu phoneset in the con-
text of Telugu Text-to-Speech. We com-
pare phone-phone substitution and word-
phone mapping for pronunciation of En-
glish words using Telugu phones. We are
not considering other than native language
phoneset in all our experiments. This dif-
ferentiates our approach from other works
in polyglot speech synthesis.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838705882353">
The objective of a Text-to-Speech (TTS) system is
to convert a given text input into a spoken wave-
form. Text processing and waveform generation
are the two main components of a TTS system.
The objective of the text processing component is
to convert the given input text into an appropriate
sequence of valid phonemic units. These phone-
mic units are then realized by the waveform gener-
ation component. For high quality speech synthe-
sis, it is necessary that the text processing unit pro-
duce the appropriate sequence of phonemic units,
for the given input text.
There has been a rise in the phenomenon of
“code mixing” (Romaine and Kachru, 1992). This
is a phenomenon where lexical items of two lan-
guages appear in a single sentence. In a multi-
lingual country such as India, we commonly find
Indian language text being freely interspersed with
English words and phrases. This is particularly no-
ticeable in the case of text from web sources like
blogs, tweets etc. An informal analysis of a Telugu
blog on the web showed that around 20-30% of the
text is in English (ASCII) while the remaining is
in Telugu (Unicode). Due to the growth of “code
mixing” it has become necessary to develop strate-
gies for dealing with such multilingual text in TTS
systems. These multilingual TTS systems should
be capable of synthesizing utterances which con-
tain foreign language words or word groups, with-
out sounding unnatural.
The different ways of achieving multilingual
TTS synthesis are as follows (Traber et al., 1999;
Latorre et al., 2006; Campbell, 1998; Campbell,
2001).
</bodyText>
<listItem confidence="0.84096">
1. Separate TTS systems for each language:
</listItem>
<bodyText confidence="0.945817285714286">
In this paradigm, a seperate TTS system is
built for each language under consideration.
When the language of the input text changes,
the TTS system also has to be changed.
This can only be done between two sen-
tences/utterances and not in the middle of a
sentence.
</bodyText>
<sectionHeader confidence="0.591164" genericHeader="introduction">
2. Polyglot speech synthesis:
</sectionHeader>
<bodyText confidence="0.999757692307692">
This is a type of multilingual speech syn-
thesis achieved using a single TTS system.
This method involves recording a multi lan-
guage speech corpus by someone who is flu-
ent in multiple languages. This speech cor-
pus is then used to build a multilingual TTS
system. The primary issue with polyglot
speech synthesis is that it requires develop-
ment of a combined phoneset, incorporating
phones from all the languages under consid-
eration. This is a time consuming process
requiring linguistic knowledge of both lan-
guages. Also, finding a speaker fluent in mul-
</bodyText>
<page confidence="0.985424">
196
</page>
<bodyText confidence="0.329953">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 196–200,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
tiple languages is not an easy task.
</bodyText>
<listItem confidence="0.821623">
3. Phone mapping:
</listItem>
<bodyText confidence="0.999966465116279">
This type of multilingual synthesis is based
upon phone mapping, whereby the phones
of the foreign language are substituted with
the closest sounding phones of the primary
language. This method results in a strong
foreign accent while synthesizing the foreign
words. This may not always be acceptable.
Also, if the sequence of the mapped phones
does not exist or is not frequently occurring
in the primary language, then the synthesized
output quality would be poor. Hence, an aver-
age polyglot synthesis technique using HMM
based synthesis and speaker adaptation has
been proposed (Latorre et al., 2006). Such
methods make use of speech data from dif-
ferent languages and different speakers.
In this paper, we relook at the problem of pro-
nunciation of English words using native phone
set. Specifically, we investigate methods of pro-
nouncing English words using Telugu phoneset in
the context of Telugu Text-to-Speech. Our moti-
vation for doing so, comes from our understand-
ing of how humans pronounce foreign words while
speaking. The speaker maps the foreign words to
a sequence of phones of his/her native language
while pronouncing that foreign word. For exam-
ple, a native speaker of Telugu, while pronounc-
ing an English word, mentally maps the English
word to a sequence of Telugu phones as opposed
to simply substituting English phones with the cor-
responding Telugu phones. Also, the receiver of
the synthesized speech would be a Telugu native
speaker, who may not have the knowledge of En-
glish phone set. Hence, approximating an English
word using Telugu phone sequence may be more
acceptable for a Telugu native speaker.
We compare phone-phone substitution and
word-phone mapping (also referred to LTS rules)
for the pronunciation of English words using Tel-
ugu phones. We are not considering other than
native language phoneset in all our experiments.
This differentiates our work from other works in
polyglot speech synthesis.
</bodyText>
<sectionHeader confidence="0.471894" genericHeader="method">
2 Comparison of word-phone and
phone-phone mapping
</sectionHeader>
<bodyText confidence="0.9977815">
Table 1 shows an example of the word computer
represented as a US English phone sequence, En-
</bodyText>
<figure confidence="0.236824888888889">
Computer
/k ax m p y uw t er/
US English Phones
[k @ m p j u t 3~]
/k e m p y uu t: r/
phone-phone mapping
[kempju: ú r]
/kampyuut: a r/
[k a m p j u: ú a r]
</figure>
<tableCaption confidence="0.621075">
Table 1: English word computer represented as
US English phone sequence, US English phone-
Telugu phone mapping and English word-Telugu
phone mapping
</tableCaption>
<bodyText confidence="0.999907041666667">
glish phone-Telugu phone mapping and English
word-Telugu phone mapping, along with the cor-
responding IPA transcription. The English word-
Telugu phone mapping is not a one to one map-
ping, as it is in the case of English phone-Telugu
phone mapping. Each letter has a correspondence
with one or more than one phones. As some let-
ters do not have a equivalent pronunciation sound
(the letter is not mapped to any phone) the term
epsilon is used whenever there is a letter which
does not have a mapping with a phone.
To compare word-phone (W-P) mapping and
phone-phone (P-P) mapping, we manually pre-
pared word-phone and phone-phone mappings for
10 bilingual utterances and synthesized them us-
ing our baseline Telugu TTS system. We then per-
formed perceptual listening evaluations on these
synthesized utterances, using five native speakers
of Telugu as the subjects of the evaluations. The
perceptual listening evaluations were setup both
as MOS (mean opinion score) evaluations and as
ABX evaluations. An explanation of MOS and
ABX evaluations is given in Section 4. Table 2
shows that results of these evaluations.
</bodyText>
<table confidence="0.994674">
MOS ABX
W-P P-P W-P P-P No. Pref
3.48 2.66 32/50 4/50 14/50
</table>
<tableCaption confidence="0.996898333333333">
Table 2: Perceptual evaluation scores for baseline
Telugu TTS system with different pronunciation
rules for English
</tableCaption>
<figure confidence="0.291003">
word-phone mapping
</figure>
<page confidence="0.980732">
197
</page>
<bodyText confidence="0.999975210526316">
An examination of the results in Table 2 shows
that manually prepared word-phone mapping is
preferred perceptually when compared to manual
phone-phone mapping. The MOS score of 3.48
indicates that native speakers accept W-P mapping
for pronouncing English words in Telugu TTS.
For the remainder of this paper, we focus ex-
clusively on word-phone mapping. We propose a
method of automatically generating these word-
phone mapping from data. We experiment our
approach by generating a word-phone mapping
which maps each English word to a Telugu phone
sequence (henceforth called EW-TP mapping).
We report the accuracy of learning the word-phone
mappings both on a held out test set and on a test
set from a different domain. Finally, we incorpo-
rate this word-phone mapping in our baseline Tel-
ugu TTS system and demonstrate its usefulness by
means of perceptual listening tests.
</bodyText>
<sectionHeader confidence="0.9405675" genericHeader="method">
3 Automatic generation of word-phone
mapping
</sectionHeader>
<bodyText confidence="0.999934333333333">
We have previously mentioned that letter to phone
mapping is not a one to one mapping. Each let-
ter may have a correspondence with one or more
than one phones, or it may not have correspon-
dence with any phone. As we require a fixed sized
learning vector to build a model for learning word-
phone mapping rules, we need to align the letter
(graphemic) and phone sequences. For this we use
the automatic epsilon scattering method.
</bodyText>
<subsectionHeader confidence="0.997278">
3.1 Automatic Epsilon Scattering Method
</subsectionHeader>
<bodyText confidence="0.999996333333333">
The idea in automatic epsilon scattering is to esti-
mate the probabilities for one letter (grapheme) G
to match with one phone P, and then use string
alignment to introduce epsilons maximizing the
probability of the word’s alignment path. Once
the all the words have been aligned, the associa-
tion probability is calculated again and so on until
convergence. The algorithm for automatic epsilon
scattering is given below (Pagel et al., 1998).
</bodyText>
<subsectionHeader confidence="0.997995">
3.2 Evaluation and Results
</subsectionHeader>
<bodyText confidence="0.999551857142857">
Once the alignment between the each word and the
corresponding phone sequence was complete, we
built two phone models using Classification and
Regression Trees (CART). For the first model, we
used data from the CMU pronunciation dictionary
where each English word had been aligned to a se-
quence of US English phones (EW-EP mapping).
</bodyText>
<figure confidence="0.504483153846154">
Algorithm for Epsilon Scattering :
/*Initialize prob(G, P) the probability of G
matching P*/
1. for each wordi in training set
count with string alignment all possible G/P
association for all possible epsilon positions in the
phonetic transcription
/* EM loop */
2. for each wordi in training set
�alignment path = argmax P(Gi, Pj)
i,j
compute probnew(G, P) on alignment path
3. if(prob =� probnew) go to 2
</figure>
<bodyText confidence="0.999385135135135">
The second model was the EW-TP mapping.
Once both the models had been built, they were
used to predict the mapped phone sequences for
each English word in the test data. For the pur-
poses of testing, we performed the prediction on
both held out test data as well as on test data from
a different domain. The held out test data was pre-
pared by removing every ninth word from the lex-
icon.
As we knew the correct phone sequence for
each word in the test data, a ground truth against
which to compute the accuracy of prediction was
available. We measured the accuracy of the pre-
diction both at the letter level and at the word level.
At the letter level, the accuracy was computed by
counting the number of times the predicted letter
to phone mapping matched with the ground truth.
For computing the accuracy at the word level, we
counted the number of times the predicted phone
sequence of each word in the test data matched
with the actual phone sequence for that word (de-
rived from the ground truth). We also varied the
size of the training data and then computed the
prediction accuracy for each model. We did so in
order to study the effect of training data size on the
prediction accuracy.
Tables 3, 4 show the accuracy of the models.
An examination of the results in the two tables
shows that incrementally increasing the size of the
training data results in an increase of the predic-
tion accuracy. The native speakers of Indian lan-
guages prefer to speak what is written. As a result
there are fewer variations in word-phone mapping
as compared to US English. This is reflected in
our results, which show that the word level pre-
diction accuracy is higher for EW-TP mapping as
compared to EW-EP mapping.
</bodyText>
<page confidence="0.993876">
198
</page>
<table confidence="0.9999435">
Training set Held-out(%) Testing(%)
size
Letters words Letters words
1000 92.04 39 81.43 16.6
2000 94.25 44.98 82.47 17.5
5000 94.55 47 84.40 25.1
10000 95.82 59.86 89.46 44.7
100000 94.09 56.37 93.27 55.10
</table>
<tableCaption confidence="0.883934">
Table 3: Accuracy of prediction for English word
- English phone mapping
</tableCaption>
<table confidence="0.999954857142857">
Training set Held-out(%) Testing(%)
size
Letters words Letters words
1000 92.37 28 82.22 18.8
2000 94.34 45.45 83.79 25.1
5000 95.89 68.2 88.40 42.7
10000 96.54 71.67 94.74 70.9
</table>
<tableCaption confidence="0.9912275">
Table 4: Accuracy of prediction for English word-
Telugu phone mapping
</tableCaption>
<sectionHeader confidence="0.957805" genericHeader="method">
4 Integrating word-phone mapping rules
in TTS
</sectionHeader>
<bodyText confidence="0.998465696969697">
For the purpose of perceptual evaluations we built
a baseline TTS systems for Telugu using the
HMM based speech synthesis technique (Zen et
al., 2007).
To conduct perceptual evaluations of the word-
phone mapping rules built from data in 3.2, we
incorporated these rules in our Telugu TTS sys-
tem. This system is henceforth refered to as T A.
A set of 25 bilingual sentences were synthesized
by the Telugu TTS, and ten native speakers of Tel-
ugu performed perceptual evaluations on the syn-
thesized utterances. As a baseline, we also synthe-
sized the same 25 sentences by incorporating man-
ually written word-phone mapping for the English
words, instead of using the automatically gener-
ated word-phone mapping rules. We refer to this
system as T M.
The perceptual evaluations were set up both
as MOS (mean opinion score) evaluations and as
ABX evaluations. In the MOS evaluations, the
listeners were asked to rate the synthesized utter-
ances from all systems on a scale of 1 to 5 (1 being
worst and 5 best), and the average scores for each
system was calculated. This average is the MOS
score for that system. In a typical ABX evalua-
tion, the listeners are presented with the the same
set of utterances synthesized using two systems A
and B, and are asked to mark their preference for
either A or B. The listeners also have an option of
marking no preference. In this case, the listeners
were asked to mark their preference between T A
and T M. The results of the perceptual evaluations
are shown in Table 5.
</bodyText>
<table confidence="0.991781666666667">
MOS ABX Test
T M T A T M T A No. Pref
3.48 3.43 51/250 38/250 161/250
</table>
<tableCaption confidence="0.9534385">
Table 5: Perceptual results comparing systems
T M and T A
</tableCaption>
<bodyText confidence="0.999772">
An examination of the results shows that per-
ceptually there is no significant preference for the
manual system over the automated system. The
MOS scores also show that there is not much sig-
nificant difference between the ratings of the man-
ual and the automated system.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999499666666667">
In this paper we present a method of automati-
cally learning word-phone mapping rules for syn-
thesizing foreign words occurring in text. We
show the effectiveness of the method by com-
puting the accuracy of prediction and also by
means of perceptual evaluations. The synthe-
sized multilingual wave files are available for
download at https://www.dropbox.com/
s/7hja51r5rpkz5mz/ACL-2013.zip.
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999664">
This work is partially supported by MCIT-TTS
consortium project funded by MCIT, Government
of India. The authors would also like to thank all
the native speakers who participated in the percep-
tual evaluations.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997627">
A.W. Black and K. Lenzo. 2004. Multilingual Text to
Speech synthesis. In Proceedings of ICASSP, Mon-
treal, Canada.
N. Campbell. 1998. Foreign language speech synthe-
sis. In Proceedings ESCA/COCOSDA workshop on
speech synthesis, Jenolan Caves, Australia.
N. Campbell. 2001. Talking foreign. Concatena-
tive speech synthesis and the language barrier. In
Proceedings Eurospeech, pages 337–340, Aalborg,
Denmark.
</reference>
<page confidence="0.985565">
199
</page>
<reference confidence="0.999257263157895">
J. Latorre, K. Iwano, and S. Furui. 2006. New ap-
proach to polygot speech generation by means of an
HMM based speaker adaptable synthesizer. Speech
Communication, 48:1227–1242.
V. Pagel, K. Lenzo, and A.W. Black. 1998. Letter to
sound rules for accented lexicon compression. In
Proceedings of ICSLP 98, volume 5, Sydney, Aus-
tralia.
Suzzane Romaine and Braj Kachru. 1992. The Ox-
ford Companion to the English Language. Oxford
University Press.
C. Traber, K. Huber, K. Nedir, B. Pfister, E. Keller,
and B. Zellner. 1999. From multilingual to polyglot
speech synthesis. In Proceedings of Eurospeech 99,
pages 835–838.
H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko,
A.W. Black, and K. Tokuda. 2007. The HMM-
based speech synthesis system version 2.0. In Pro-
ceedings of ISCA SSW6, Bonn, Germany.
</reference>
<page confidence="0.996613">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000495">
<title confidence="0.9322425">Is word-to-phone mapping better than phone-phone mapping handling English words?</title>
<author confidence="0.709723">Naresh Kumar</author>
<affiliation confidence="0.537351">Speech and Vision</affiliation>
<address confidence="0.862966">IIIT Hyderabad, India</address>
<email confidence="0.5779455">nareshkumar.elluru@research.iiit.ac.in</email>
<title confidence="0.4548865">Anandaswarup Speech and Vision</title>
<affiliation confidence="0.477504">IIIT Hyderabad,</affiliation>
<email confidence="0.912447">research.iiit.ac.in</email>
<title confidence="0.349203">Raghavendra Speech and Vision</title>
<address confidence="0.87012">IIIT Hyderabad, India</address>
<email confidence="0.979267">raghavendra.veera@gmail.com</email>
<author confidence="0.984617">Hema</author>
<affiliation confidence="0.995246">Department of IIT Madras,</affiliation>
<email confidence="0.923463">hema@iitm.ac.in</email>
<title confidence="0.476008">Kishore Speech and Vision</title>
<affiliation confidence="0.518908">IIIT Hyderabad,</affiliation>
<email confidence="0.986952">kishore@iiit.ac.in</email>
<abstract confidence="0.999767642857143">In this paper, we relook at the problem of pronunciation of English words using native phone set. Specifically, we investigate methods of pronouncing English words using Telugu phoneset in the context of Telugu Text-to-Speech. We compare phone-phone substitution and wordphone mapping for pronunciation of English words using Telugu phones. We are not considering other than native language phoneset in all our experiments. This differentiates our approach from other works in polyglot speech synthesis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A W Black</author>
<author>K Lenzo</author>
</authors>
<title>Multilingual Text to Speech synthesis.</title>
<date>2004</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Montreal, Canada.</location>
<marker>Black, Lenzo, 2004</marker>
<rawString>A.W. Black and K. Lenzo. 2004. Multilingual Text to Speech synthesis. In Proceedings of ICASSP, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Campbell</author>
</authors>
<title>Foreign language speech synthesis.</title>
<date>1998</date>
<booktitle>In Proceedings ESCA/COCOSDA workshop on speech synthesis,</booktitle>
<location>Jenolan Caves, Australia.</location>
<contexts>
<context position="2607" citStr="Campbell, 1998" startWordPosition="405" endWordPosition="406">eb sources like blogs, tweets etc. An informal analysis of a Telugu blog on the web showed that around 20-30% of the text is in English (ASCII) while the remaining is in Telugu (Unicode). Due to the growth of “code mixing” it has become necessary to develop strategies for dealing with such multilingual text in TTS systems. These multilingual TTS systems should be capable of synthesizing utterances which contain foreign language words or word groups, without sounding unnatural. The different ways of achieving multilingual TTS synthesis are as follows (Traber et al., 1999; Latorre et al., 2006; Campbell, 1998; Campbell, 2001). 1. Separate TTS systems for each language: In this paradigm, a seperate TTS system is built for each language under consideration. When the language of the input text changes, the TTS system also has to be changed. This can only be done between two sentences/utterances and not in the middle of a sentence. 2. Polyglot speech synthesis: This is a type of multilingual speech synthesis achieved using a single TTS system. This method involves recording a multi language speech corpus by someone who is fluent in multiple languages. This speech corpus is then used to build a multili</context>
</contexts>
<marker>Campbell, 1998</marker>
<rawString>N. Campbell. 1998. Foreign language speech synthesis. In Proceedings ESCA/COCOSDA workshop on speech synthesis, Jenolan Caves, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Campbell</author>
</authors>
<title>Talking foreign. Concatenative speech synthesis and the language barrier.</title>
<date>2001</date>
<booktitle>In Proceedings Eurospeech,</booktitle>
<pages>337--340</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="2624" citStr="Campbell, 2001" startWordPosition="407" endWordPosition="408">blogs, tweets etc. An informal analysis of a Telugu blog on the web showed that around 20-30% of the text is in English (ASCII) while the remaining is in Telugu (Unicode). Due to the growth of “code mixing” it has become necessary to develop strategies for dealing with such multilingual text in TTS systems. These multilingual TTS systems should be capable of synthesizing utterances which contain foreign language words or word groups, without sounding unnatural. The different ways of achieving multilingual TTS synthesis are as follows (Traber et al., 1999; Latorre et al., 2006; Campbell, 1998; Campbell, 2001). 1. Separate TTS systems for each language: In this paradigm, a seperate TTS system is built for each language under consideration. When the language of the input text changes, the TTS system also has to be changed. This can only be done between two sentences/utterances and not in the middle of a sentence. 2. Polyglot speech synthesis: This is a type of multilingual speech synthesis achieved using a single TTS system. This method involves recording a multi language speech corpus by someone who is fluent in multiple languages. This speech corpus is then used to build a multilingual TTS system.</context>
</contexts>
<marker>Campbell, 2001</marker>
<rawString>N. Campbell. 2001. Talking foreign. Concatenative speech synthesis and the language barrier. In Proceedings Eurospeech, pages 337–340, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Latorre</author>
<author>K Iwano</author>
<author>S Furui</author>
</authors>
<title>New approach to polygot speech generation by means of an HMM based speaker adaptable synthesizer.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<pages>48--1227</pages>
<contexts>
<context position="2591" citStr="Latorre et al., 2006" startWordPosition="401" endWordPosition="404">he case of text from web sources like blogs, tweets etc. An informal analysis of a Telugu blog on the web showed that around 20-30% of the text is in English (ASCII) while the remaining is in Telugu (Unicode). Due to the growth of “code mixing” it has become necessary to develop strategies for dealing with such multilingual text in TTS systems. These multilingual TTS systems should be capable of synthesizing utterances which contain foreign language words or word groups, without sounding unnatural. The different ways of achieving multilingual TTS synthesis are as follows (Traber et al., 1999; Latorre et al., 2006; Campbell, 1998; Campbell, 2001). 1. Separate TTS systems for each language: In this paradigm, a seperate TTS system is built for each language under consideration. When the language of the input text changes, the TTS system also has to be changed. This can only be done between two sentences/utterances and not in the middle of a sentence. 2. Polyglot speech synthesis: This is a type of multilingual speech synthesis achieved using a single TTS system. This method involves recording a multi language speech corpus by someone who is fluent in multiple languages. This speech corpus is then used to</context>
<context position="4364" citStr="Latorre et al., 2006" startWordPosition="687" endWordPosition="690">3. Phone mapping: This type of multilingual synthesis is based upon phone mapping, whereby the phones of the foreign language are substituted with the closest sounding phones of the primary language. This method results in a strong foreign accent while synthesizing the foreign words. This may not always be acceptable. Also, if the sequence of the mapped phones does not exist or is not frequently occurring in the primary language, then the synthesized output quality would be poor. Hence, an average polyglot synthesis technique using HMM based synthesis and speaker adaptation has been proposed (Latorre et al., 2006). Such methods make use of speech data from different languages and different speakers. In this paper, we relook at the problem of pronunciation of English words using native phone set. Specifically, we investigate methods of pronouncing English words using Telugu phoneset in the context of Telugu Text-to-Speech. Our motivation for doing so, comes from our understanding of how humans pronounce foreign words while speaking. The speaker maps the foreign words to a sequence of phones of his/her native language while pronouncing that foreign word. For example, a native speaker of Telugu, while pro</context>
</contexts>
<marker>Latorre, Iwano, Furui, 2006</marker>
<rawString>J. Latorre, K. Iwano, and S. Furui. 2006. New approach to polygot speech generation by means of an HMM based speaker adaptable synthesizer. Speech Communication, 48:1227–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pagel</author>
<author>K Lenzo</author>
<author>A W Black</author>
</authors>
<title>Letter to sound rules for accented lexicon compression.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP 98,</booktitle>
<volume>5</volume>
<location>Sydney, Australia.</location>
<contexts>
<context position="9275" citStr="Pagel et al., 1998" startWordPosition="1507" endWordPosition="1510">dphone mapping rules, we need to align the letter (graphemic) and phone sequences. For this we use the automatic epsilon scattering method. 3.1 Automatic Epsilon Scattering Method The idea in automatic epsilon scattering is to estimate the probabilities for one letter (grapheme) G to match with one phone P, and then use string alignment to introduce epsilons maximizing the probability of the word’s alignment path. Once the all the words have been aligned, the association probability is calculated again and so on until convergence. The algorithm for automatic epsilon scattering is given below (Pagel et al., 1998). 3.2 Evaluation and Results Once the alignment between the each word and the corresponding phone sequence was complete, we built two phone models using Classification and Regression Trees (CART). For the first model, we used data from the CMU pronunciation dictionary where each English word had been aligned to a sequence of US English phones (EW-EP mapping). Algorithm for Epsilon Scattering : /*Initialize prob(G, P) the probability of G matching P*/ 1. for each wordi in training set count with string alignment all possible G/P association for all possible epsilon positions in the phonetic tra</context>
</contexts>
<marker>Pagel, Lenzo, Black, 1998</marker>
<rawString>V. Pagel, K. Lenzo, and A.W. Black. 1998. Letter to sound rules for accented lexicon compression. In Proceedings of ICSLP 98, volume 5, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzzane Romaine</author>
<author>Braj Kachru</author>
</authors>
<title>The Oxford Companion to the English Language.</title>
<date>1992</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1709" citStr="Romaine and Kachru, 1992" startWordPosition="252" endWordPosition="255">Text-to-Speech (TTS) system is to convert a given text input into a spoken waveform. Text processing and waveform generation are the two main components of a TTS system. The objective of the text processing component is to convert the given input text into an appropriate sequence of valid phonemic units. These phonemic units are then realized by the waveform generation component. For high quality speech synthesis, it is necessary that the text processing unit produce the appropriate sequence of phonemic units, for the given input text. There has been a rise in the phenomenon of “code mixing” (Romaine and Kachru, 1992). This is a phenomenon where lexical items of two languages appear in a single sentence. In a multilingual country such as India, we commonly find Indian language text being freely interspersed with English words and phrases. This is particularly noticeable in the case of text from web sources like blogs, tweets etc. An informal analysis of a Telugu blog on the web showed that around 20-30% of the text is in English (ASCII) while the remaining is in Telugu (Unicode). Due to the growth of “code mixing” it has become necessary to develop strategies for dealing with such multilingual text in TTS </context>
</contexts>
<marker>Romaine, Kachru, 1992</marker>
<rawString>Suzzane Romaine and Braj Kachru. 1992. The Oxford Companion to the English Language. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Traber</author>
<author>K Huber</author>
<author>K Nedir</author>
<author>B Pfister</author>
<author>E Keller</author>
<author>B Zellner</author>
</authors>
<title>From multilingual to polyglot speech synthesis.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech 99,</booktitle>
<pages>835--838</pages>
<contexts>
<context position="2569" citStr="Traber et al., 1999" startWordPosition="397" endWordPosition="400">larly noticeable in the case of text from web sources like blogs, tweets etc. An informal analysis of a Telugu blog on the web showed that around 20-30% of the text is in English (ASCII) while the remaining is in Telugu (Unicode). Due to the growth of “code mixing” it has become necessary to develop strategies for dealing with such multilingual text in TTS systems. These multilingual TTS systems should be capable of synthesizing utterances which contain foreign language words or word groups, without sounding unnatural. The different ways of achieving multilingual TTS synthesis are as follows (Traber et al., 1999; Latorre et al., 2006; Campbell, 1998; Campbell, 2001). 1. Separate TTS systems for each language: In this paradigm, a seperate TTS system is built for each language under consideration. When the language of the input text changes, the TTS system also has to be changed. This can only be done between two sentences/utterances and not in the middle of a sentence. 2. Polyglot speech synthesis: This is a type of multilingual speech synthesis achieved using a single TTS system. This method involves recording a multi language speech corpus by someone who is fluent in multiple languages. This speech </context>
</contexts>
<marker>Traber, Huber, Nedir, Pfister, Keller, Zellner, 1999</marker>
<rawString>C. Traber, K. Huber, K. Nedir, B. Pfister, E. Keller, and B. Zellner. 1999. From multilingual to polyglot speech synthesis. In Proceedings of Eurospeech 99, pages 835–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zen</author>
<author>T Nose</author>
<author>J Yamagishi</author>
<author>S Sako</author>
<author>T Masuko</author>
<author>A W Black</author>
<author>K Tokuda</author>
</authors>
<title>The HMMbased speech synthesis system version 2.0.</title>
<date>2007</date>
<booktitle>In Proceedings of ISCA SSW6,</booktitle>
<location>Bonn, Germany.</location>
<contexts>
<context position="12476" citStr="Zen et al., 2007" startWordPosition="2062" endWordPosition="2065">4.98 82.47 17.5 5000 94.55 47 84.40 25.1 10000 95.82 59.86 89.46 44.7 100000 94.09 56.37 93.27 55.10 Table 3: Accuracy of prediction for English word - English phone mapping Training set Held-out(%) Testing(%) size Letters words Letters words 1000 92.37 28 82.22 18.8 2000 94.34 45.45 83.79 25.1 5000 95.89 68.2 88.40 42.7 10000 96.54 71.67 94.74 70.9 Table 4: Accuracy of prediction for English wordTelugu phone mapping 4 Integrating word-phone mapping rules in TTS For the purpose of perceptual evaluations we built a baseline TTS systems for Telugu using the HMM based speech synthesis technique (Zen et al., 2007). To conduct perceptual evaluations of the wordphone mapping rules built from data in 3.2, we incorporated these rules in our Telugu TTS system. This system is henceforth refered to as T A. A set of 25 bilingual sentences were synthesized by the Telugu TTS, and ten native speakers of Telugu performed perceptual evaluations on the synthesized utterances. As a baseline, we also synthesized the same 25 sentences by incorporating manually written word-phone mapping for the English words, instead of using the automatically generated word-phone mapping rules. We refer to this system as T M. The perc</context>
</contexts>
<marker>Zen, Nose, Yamagishi, Sako, Masuko, Black, Tokuda, 2007</marker>
<rawString>H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko, A.W. Black, and K. Tokuda. 2007. The HMMbased speech synthesis system version 2.0. In Proceedings of ISCA SSW6, Bonn, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>