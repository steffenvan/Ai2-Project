<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032028">
<title confidence="0.7548235">
Memory-based one-step named-entity recognition: Effects of seed list
features, classifier stacking, and unannotated data
</title>
<author confidence="0.610209">
Iris Hendrickx and Antal van den Bosch
</author>
<affiliation confidence="0.617811">
ILK / Computational Linguistics
Tilburg University, The Netherlands
</affiliation>
<email confidence="0.996531">
{I.H.E.Hendrickx,Antal.vdnBosch}@uvt.nl
</email>
<sectionHeader confidence="0.999762" genericHeader="abstract">
1 Outline
</sectionHeader>
<bodyText confidence="0.9998875">
We present a memory-based named-entity recognition
system that chunks and labels named entities in a one-
shot task. Training and testing on CoNLL-2003 shared
task data, we measure the effects of three extensions.
First, we incorporate features that signal the presence of
wordforms in external, language-specific seed (gazetteer)
lists. Second, we build a second-stage stacked classifier
that corrects first-stage output errors. Third, we add se-
lected instances from classified unannotated data to the
training material. The system that incorporates all attains
an overall F-rate on the final test set of 78.20 on English
and 63.02 on German.
</bodyText>
<sectionHeader confidence="0.876018" genericHeader="categories and subject descriptors">
2 Data and features
</sectionHeader>
<bodyText confidence="0.999762727272727">
The CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003) supplied datasets in two languages,
English and German, using four named entity cate-
gories: persons, organisations, locations, and “miscellany
names”. Manual annotation has been performed at the
University of Antwerp. Apart from tokenized wordforms,
the data provides predicted PoS-tags and chunks.
Additionally we computed the following features with
each wordform, largely following those used by the best-
performing submission of the 2002 shared task (Carreras
et al., 2002):
</bodyText>
<listItem confidence="0.940417571428571">
• Orthographic features represented as binary
features: Begin cap, All caps, Internal cap,
Contains digit, Contains digit en alpha, Initial,
Lower case, First word
• The wordform’s first letter and last three letters (as
three separate features)
• The direct output of the memory-based lemmatizer
(Van den Bosch and Daelemans, 1999), provid-
ing PoS tag, morphological features, and spelling
change information
• PoS tag from a slow but accurate version of the
memory-based tagger trained on a portion of the
British National Corpus, according to the CLAWS-5
tagset (for English data only)
</listItem>
<bodyText confidence="0.999889666666667">
For example, for the English word Indian the following
feature representation is made: Indian NNP I-NP
1 0 0 0 0 0 0 0 I i a n AJ0-NN1 N-s
I-MISC, where NNP is the provided PoS tag, I-NP the
chunk tag; the binary features represent the orthographic
features (where in this case only Begin cap is positive);
AJO-NN1 is the PoS tag of the BNC-trained-tagger;
N-s is the lemmatizer output for noun-singular; the last
element, I-MISC, is the annotated class label.
In our experiments we construct instances around
wordforms, where we take a windowed snapshot of the
word in its direct local context. By default, we select a
window of two words to the left and right. For all five
words in each input instance (feature vector), in principle
all of the above features are included.
</bodyText>
<sectionHeader confidence="0.997193" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.999366">
In two subsections we briefly detail how the memory-
based learner works, and how we optimized its param-
eters through an automatic process called iterative deep-
ening.
</bodyText>
<subsectionHeader confidence="0.996513">
3.1 Memory-based learning
</subsectionHeader>
<bodyText confidence="0.999842941176471">
Memory-based learning is a supervised inductive learn-
ing algorithm for learning classification tasks. Memory-
based learning treats a set of training instances as points
in a multi-dimensional feature space, and stores them as
such in an instance base in memory (rather than perform-
ing some abstraction over them).
New (test) instances are classified by matching them
to all instances in memory, and by calculating with each
match the distance, given by a distance function between
the new instance X and each of the n memory instances
Y1...n. Classification in memory-based learning is per-
formed by the k-NN algorithm that searches for the k
‘nearest neighbours’ among the memory instances ac-
cording to the distance function. The majority class of
the k nearest neighbours then determines the class of the
new instance X. Cf. (Daelemans et al., 2002) for algo-
rithmic details and background.
</bodyText>
<subsectionHeader confidence="0.999057">
3.2 Iterative deepening
</subsectionHeader>
<bodyText confidence="0.998836035714286">
Iterative deepening (ID) is a heuristic search algorithm
for the optimization of algorithmic parameter and fea-
ture selection, that combines classifier wrapping (using
the training material internally to test experimental vari-
ants) (Kohavi and John, 1997) with progressive sampling
of training material (Provost et al., 1999). We start with
a large pool of experiments, each with a unique combina-
tion of input features and algorithmic parameter settings.
In the first step, each attempted setting is applied to a
small amount of training material and tested on a fixed
amount of held-out data (a held-out part of the training
set). Only the best settings are kept; all others are re-
moved from the pool of competing settings. In subse-
quent iterations, this step is repeated, cutting the num-
ber of settings in the pool by a half and retaining the
best-performing half, while at the same time doubling the
amount of training material.
We selected 10% of the training set as held-out data.
Six iterations were performed with increasing training
set sizes, starting with 2000 instances, and doubling with
each iteration up to 128,000 training instances, resulting
in 16 best settings after the last iteration. Selection of
the best experiments was based on their overall F-rate as
computed by the conlleval script.
The initial pool of experiments was created by system-
atically varying parameters of the memory-based learner
and some limited feature selections, (for details, cf.
(Daelemans et al., 2002)):
</bodyText>
<listItem confidence="0.999264461538461">
• Basic distance function: Overlap or modified value
difference metric (MVDM)
• Feature weighting: gain ratio, information gain, χ2,
or shared variance
• k in the k-NN classifier: 5, 9, 13, 15, 17, 19, 21, 25,
and 29
• Distance weighting: none, linear-inverse, inverse,
exponential decay with α=1 and α=4
• Feature selection: apart from the wordform and its
provided CoNLL-2003 PoS tag, create a local win-
dow of either no, 1, or 2 wordforms to the left and
right of the focus word. For all words in a window,
all features are selected.
</listItem>
<bodyText confidence="0.901695333333333">
The first round of the ID process therefore tests 2 x
4 x 9 x 5 x 3 = 1080 systematic permutations of these
parameter settings and feature selection.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="method">
4 Extensions
</sectionHeader>
<subsectionHeader confidence="0.996375">
4.1 Seed list features
</subsectionHeader>
<bodyText confidence="0.999986666666667">
The first extension is to incorporate language-specific
seed-list (gazetteer) information. Rather than using these
lists external to the classifier, we encode them as internal
features associated to wordforms. For each of the four
named entity classes we gathered one list of names, con-
taining material garnered from name sites on the inter-
net, from the training set (for the MISC category), and
from the CELEX English lexical data base (Baayen et
al., 1993). These lists vary in size from 1269 names to
78,732 names. Each wordform in the training and test
data is then enriched with four binary features, each rep-
resenting whether the word occurs in the respective seed
list. One problem with seed lists is that a word can occur
in more than one seed list, so that more than one of these
four bits may be active.
</bodyText>
<subsectionHeader confidence="0.991579">
4.2 Second-stage stacking
</subsectionHeader>
<bodyText confidence="0.999988066666667">
The second extension is to use second-stage stacking.
Stacking in general (Wolpert, 1992) encompasses a class
of meta-learning systems that learn to correct errors made
by lower-level classifiers. We adopt the particular method
pioneered in (Veenstra, 1998) in which classifications of a
first memory-based classifier are added as windowed fea-
tures to the instances presented to the second classifier.
Since the second-stage classifier also computes the sim-
ilarities between instances using these extra features, it
is able, in principle, to recognise and correct reoccurring
patterns of errors within sub-sentential sequences. This
could correct errors made due to the “blindness” of the
first-stage classifier, which is unaware of its own classifi-
cations left or right of the wordform in the current focus
position. We used stacking on top of the first extension.
</bodyText>
<subsectionHeader confidence="0.994837">
4.3 Unannotated data
</subsectionHeader>
<bodyText confidence="0.998240565217391">
For both languages a large unannotated dataset was
made available for extracting data or information. Al-
ternative to using this data to expand or bootstrap seed
lists (Cucerzan and Yarowsky, 1999; Buchholz and
Van den Bosch, 2000), we use the unannotated corpus
to select useful instances to be added directly to the train-
ing set. Not unlike (Yarowsky, 1995) we use confidence
of our classifier on unannotated data to enrich itself; that
is, by adding confidently-classified instances to the mem-
ory. We make the simple assumption that entropy in the
class distribution in the nearest neighbour set computed
in the classification of a new instance is correlated with
the reliability of the classification, when k &gt; 1. When k
nearest neighbours all vote for the same class, the entropy
of that class vote is 0.0. Alternatively, when the votes tie,
the entropy is maximal.
A secondary heuristic assumption is that it is proba-
bly not useful to add (almost) exact matches to the mem-
ory, since adding those is likely to have little effect on
the performance of the k-NN classifier. More effect can
be expected from adding instances to memory that have a
low-entropy class distribution in their nearest neighbour
set and of which the nearest neighbours are at a relatively
</bodyText>
<table confidence="0.9999748">
Precision Recall F0=1
English devel. 84.54% 87.16% 85.83
English test 77.01% 80.74% 78.83
German devel. 64.01% 52.29% 57.56
German test 66.71% 56.47% 61.16
</table>
<tableCaption confidence="0.636608222222222">
Table 1: Overall results (precision, recall, F-rate) of the
initial system on the test sets of both languages.
settings mvdm feat. weight k dist. w
Eng, initial yes gain ratio 21 IL 1
Eng, seedlist yes gain ratio 5 ID 1
Ger, initial yes gain ratio 21 IL 2
Ger, seedlist yes gain ratio 9 ID 2
Table 2: Optimal parameter settings estimated by itera-
tive deepening. “w” stands for window.
</tableCaption>
<bodyText confidence="0.999234">
large distance. A large distance entails that the instances
contains previously unseen feature values (words), and
assuming that the predicted class label is correct, these
new values can be valuable in matching and therefore
classifying new test material better.
We applied our selection method to the first 2 million
words of the unannotated English dataset. For German
we were able to process 0.25 million words. First we
applied the classifier with two extensions, seed list infor-
mation and second stage stacking, to classify the unan-
notated data. We selected instances with an entropy in
the class distribution lower than 0.05 and a distance of
the nearest neighbour of at least 0.1. For English, in total
179,391 instances (9%) were selected from the unanno-
tated dataset and added to the training set. For German.
markedly less instances were selected: 467 (0.19%).
</bodyText>
<sectionHeader confidence="0.999991" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.993858">
5.1 Initial classifier: Iterative deepening
</subsectionHeader>
<bodyText confidence="0.9997335">
Iterative deepening produced estimations of optimal pa-
rameter settings for our initial systems for the two lan-
guages, displayed in the first and third row of Table 2.
With this setting we achieved an overall F-rate of 78.83
for English and 61.16 for German. Table 1 lists the full
evaluation results.
</bodyText>
<subsectionHeader confidence="0.959272">
5.2 First and second extension: seed list features
and stacking
</subsectionHeader>
<bodyText confidence="0.999966666666667">
We have also performed iterative deepening in the exper-
iment with the seed list information. This altered the best
setting found by the iterative deepening process (the sec-
ond and fourth rows of Table 2). The results on the En-
glish development set are slightly better than the initial
system, as can be seen in Table 3. The classifier with
</bodyText>
<table confidence="0.999958">
Precision Recall F0=1
English devel. 85.04% 87.26% 86.14
English test 75.03% 79.75% 77.32
German devel. 65.27% 50.76% 57.11
German test 69.31% 55.70% 61.77
</table>
<tableCaption confidence="0.971845">
Table 3: Overall results (precision, recall, F-rate) of the
system with seed-list features on the test sets of both lan-
guages.
</tableCaption>
<table confidence="0.9999166">
Precision Recall F0=1
English devel. 85.98% 87.63% 86.80
English test 76.26% 80.21% 78.18
German devel. 68.80% 52.29% 59.42
German test 71.19% 56.38% 62.93
</table>
<tableCaption confidence="0.947944">
Table 4: Overall results (precision, recall, F-rate) of the
</tableCaption>
<bodyText confidence="0.9469043">
system with seed-list features and second-stage stacking
on the test sets of both languages.
seed list information performs worse on the English test
set than the one without seed lists. The reverse effect is
seen on the German data. On the development set, using
the seed list information gave a slight lower performance,
but on the test set it has a slightly positive effect.
Our second extension, stacking, improves on all over-
all F-scores of both languages as compared to the seed-
list extended systems, as shown in Table 4.
</bodyText>
<subsectionHeader confidence="0.945993">
5.3 Third extension: Selecting instances from
unannotated data
</subsectionHeader>
<bodyText confidence="0.9999932">
The three extensions, using seed list information, per-
forming second stage stacking and adding information
from unannotated data, are combined in the final experi-
ment. This experiment achieves the highest result on the
English development set, and on both German test sets,
as listed in Table 5. The positive effect of adding selected
unannotated data on the German test sets is rather mini-
mal, but we added only a very small amount of unlabeled
material. The performance on the English test set is not
better than the initial classifier.
</bodyText>
<sectionHeader confidence="0.999167" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999345638888889">
In this paper we have presented a memory-based named-
entity recognition system that chunks and labels named
entities in one shot. We reported on three extensions; in-
corporating seed list information, second-stage stacking
and adding selected instances from classified unannotated
data to the training material.
First, we trained and tested a basic classifier without
any of the extensions. Subsequently, we found that (i) in-
corporating seed list information as binary features does
not always help; only in two of the four test sets the
seedlists had a positive effect. There can be several ex-
planations for this, such as the quality of the seed lists,
the chosen parameter setting from the iterative deepening
process or overestimated weights given to the features by
the classifier. Due to the tight time schedule we could not
further investigate this.
Second, second-stage stacking improves generalisa-
tion performance consistently on all test sets as compared
to the seed-list extended systems.
Third, only in the final experiment we added selected
classified instances from unannotated data. This gave an
additional reasonable boost in performance on the En-
glish development set, it attains an overall F-rate of 86.97
(an error reduction of 8%) over the initial classifier. The
same effect was seen on both German test sets, on which
the combination of the three extensions achieved a Fscore
of 59.58 ( 5% error reduction ) and 63.02 ( 5% error re-
duction). This effect is not seen on the English test set;
here the initial classifier performs best. This can partly
be explained by the fact that the last two extensions were
built upon the first extension, which had a markedly lower
score than the initial classifier to begin with.
In sum, our results suggest that two of the three ex-
tensions, the stacking method, and the unlabeled instance
selection method, have been consistently helpful. Seed
list features, however, have not.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998677333333334">
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX lexical data base on CD-ROM. Linguistic
Data Consortium, Philadelphia, PA.
S. Buchholz and A. Van den Bosch. 2000. Integrating
seed names and n-grams for a named entity list and
classifier. In LREC-2000 (Second International Con-
ference on Language Resources and Evaluation) Pro-
ceedings. Vol. II, pages 1215–1221.
X. Carreras, L. Marques, and L. Padro. 2002. Named
entity extraction using AdaBoost. In Proceedings of
CoNLL-2002, pages 167–170.
S. Cucerzan and D. Yarowsky. 1999. Language indepen-
dent named entity recognition combining morpholog-
ical and contextual evidence. In Proceedings of 1999
Joint SIGDAT Conference on EMNLP and VLC.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2002. TiMBL: Tilburg Memory
Based Learner, version 4.3, reference guide. Technical
Report ILK-0210, ILK, Tilburg University.
R. Kohavi and G. John. 1997. Wrappers for feature
subset selection. Artificial Intelligence Journal, 97(1–
2):273–324.
F. Provost, D. Jensen, and T. Oates. 1999. Efficient pro-
gressive sampling. In Proceedings of the Fifth Interna-
</reference>
<table confidence="0.995584666666667">
English devel. Precision Recall F0=1
LOC 89.42% 91.13% 90.27
MISC 91.36% 80.26% 85.45
ORG 74.32% 83.30% 78.55
PER 90.16% 91.53% 90.84
Overall 86.16% 87.80% 86.97
English test Precision Recall F0=1
LOC 80.81% 86.33% 83.48
MISC 66.96% 75.93% 71.16
ORG 69.24% 73.99% 71.54
PER 83.98% 82.00% 82.98
Overall 76.33% 80.17% 78.20
German devel. Precision Recall F0=1
LOC 61.90% 69.60% 65.52
MISC 83.25% 32.97% 47.23
ORG 67.55% 49.32% 57.01
PER 73.40% 54.96% 62.86
Overall 68.88% 52.49% 59.58
German test Precision Recall F0=1
LOC 63.36% 64.83% 64.09
MISC 75.60% 32.84% 45.79
ORG 62.90% 48.90% 55.02
PER 83.47% 67.62% 74.71
Overall 71.15% 56.55% 63.02
</table>
<tableCaption confidence="0.9554775">
Table 5: Results on the test sets of the variant combining
all three extensions to the initial classifier.
</tableCaption>
<reference confidence="0.99918965">
tional Conference on Knowledge Discovery and Data
Mining, pages 23–32.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003. Edmonton, Canada.
A. Van den Bosch and W. Daelemans. 1999. Memory-
based morphological analysis. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 285–292, New Brunswick,
NJ. ACL.
J. Veenstra. 1998. Fast np chunking using memory-
based learning techniques. In Proceedings of Bene-
learn 1998, pages 71–79.
D. H. Wolpert. 1992. On overfitting avoidance as bias.
Technical Report SFI TR 92-03-5001, The Santa Fe
Institute.
D. Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL33,
pages 189–196, Cambridge, MA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003390">
<title confidence="0.903715">Memory-based one-step named-entity recognition: Effects of seed features, classifier stacking, and unannotated data</title>
<author confidence="0.998282">Hendrickx van_den</author>
<affiliation confidence="0.798862">ILK / Computational Tilburg University, The 1 Outline</affiliation>
<abstract confidence="0.969923043771044">We present a memory-based named-entity recognition system that chunks and labels named entities in a oneshot task. Training and testing on CoNLL-2003 shared task data, we measure the effects of three extensions. First, we incorporate features that signal the presence of wordforms in external, language-specific seed (gazetteer) lists. Second, we build a second-stage stacked classifier that corrects first-stage output errors. Third, we add selected instances from classified unannotated data to the training material. The system that incorporates all attains an overall F-rate on the final test set of 78.20 on English and 63.02 on German. 2 Data and features The CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) supplied datasets in two languages, English and German, using four named entity categories: persons, organisations, locations, and “miscellany names”. Manual annotation has been performed at the University of Antwerp. Apart from tokenized wordforms, the data provides predicted PoS-tags and chunks. Additionally we computed the following features with each wordform, largely following those used by the bestperforming submission of the 2002 shared task (Carreras et al., 2002): • Orthographic features represented as binary cap, All caps, Internal Contains digit, Contains digit en alpha, Initial, Lower case, First word • The wordform’s first letter and last three letters (as three separate features) • The direct output of the memory-based lemmatizer (Van den Bosch and Daelemans, 1999), providing PoS tag, morphological features, and spelling change information • PoS tag from a slow but accurate version of the memory-based tagger trained on a portion of the British National Corpus, according to the CLAWS-5 tagset (for English data only) example, for the English word following representation is made: NNP I-NP 1 0 0 0 0 0 0 0 I i a n AJ0-NN1 N-s where the provided PoS tag, chunk tag; the binary features represent the orthographic (where in this case only cap positive); the PoS tag of the BNC-trained-tagger; the lemmatizer output for noun-singular; the last is the annotated class label. In our experiments we construct instances around wordforms, where we take a windowed snapshot of the word in its direct local context. By default, we select a window of two words to the left and right. For all five words in each input instance (feature vector), in principle all of the above features are included. 3 Experimental setup In two subsections we briefly detail how the memorybased learner works, and how we optimized its parameters through an automatic process called iterative deepening. 3.1 Memory-based learning Memory-based learning is a supervised inductive learning algorithm for learning classification tasks. Memorybased learning treats a set of training instances as points in a multi-dimensional feature space, and stores them as in an base memory (rather than performing some abstraction over them). New (test) instances are classified by matching them to all instances in memory, and by calculating with each the given by a distance function between new instance each of the instances Classification in memory-based learning is perby the algorithm that searches for the ‘nearest neighbours’ among the memory instances according to the distance function. The majority class of neighbours then determines the class of the instance Cf. (Daelemans et al., 2002) for algorithmic details and background. 3.2 Iterative deepening Iterative deepening (ID) is a heuristic search algorithm for the optimization of algorithmic parameter and feature selection, that combines classifier wrapping (using the training material internally to test experimental variants) (Kohavi and John, 1997) with progressive sampling of training material (Provost et al., 1999). We start with a large pool of experiments, each with a unique combination of input features and algorithmic parameter settings. In the first step, each attempted setting is applied to a small amount of training material and tested on a fixed amount of held-out data (a held-out part of the training set). Only the best settings are kept; all others are removed from the pool of competing settings. In subsequent iterations, this step is repeated, cutting the number of settings in the pool by a half and retaining the best-performing half, while at the same time doubling the amount of training material. We selected 10% of the training set as held-out data. Six iterations were performed with increasing training set sizes, starting with 2000 instances, and doubling with each iteration up to 128,000 training instances, resulting in 16 best settings after the last iteration. Selection of the best experiments was based on their overall F-rate as by the The initial pool of experiments was created by systematically varying parameters of the memory-based learner and some limited feature selections, (for details, cf. (Daelemans et al., 2002)): • Basic distance function: Overlap or modified value difference metric (MVDM) Feature weighting: gain ratio, information gain, or shared variance k the classifier: 5, 9, 13, 15, 17, 19, 21, 25, and 29 • Distance weighting: none, linear-inverse, inverse, decay with and • Feature selection: apart from the wordform and its provided CoNLL-2003 PoS tag, create a local window of either no, 1, or 2 wordforms to the left and right of the focus word. For all words in a window, all features are selected. first round of the ID process therefore tests = 1080 permutations of these parameter settings and feature selection. 4 Extensions 4.1 Seed list features The first extension is to incorporate language-specific seed-list (gazetteer) information. Rather than using these lists external to the classifier, we encode them as internal features associated to wordforms. For each of the four named entity classes we gathered one list of names, containing material garnered from name sites on the internet, from the training set (for the MISC category), and from the CELEX English lexical data base (Baayen et al., 1993). These lists vary in size from 1269 names to 78,732 names. Each wordform in the training and test data is then enriched with four binary features, each representing whether the word occurs in the respective seed list. One problem with seed lists is that a word can occur in more than one seed list, so that more than one of these four bits may be active. 4.2 Second-stage stacking The second extension is to use second-stage stacking. Stacking in general (Wolpert, 1992) encompasses a class of meta-learning systems that learn to correct errors made by lower-level classifiers. We adopt the particular method pioneered in (Veenstra, 1998) in which classifications of a first memory-based classifier are added as windowed features to the instances presented to the second classifier. Since the second-stage classifier also computes the similarities between instances using these extra features, it is able, in principle, to recognise and correct reoccurring patterns of errors within sub-sentential sequences. This could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first extension. 4.3 Unannotated data For both languages a large unannotated dataset was made available for extracting data or information. Alternative to using this data to expand or bootstrap seed lists (Cucerzan and Yarowsky, 1999; Buchholz and Van den Bosch, 2000), we use the unannotated corpus to select useful instances to be added directly to the training set. Not unlike (Yarowsky, 1995) we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory. We make the simple assumption that entropy in the class distribution in the nearest neighbour set computed in the classification of a new instance is correlated with reliability of the classification, when &gt; When nearest neighbours all vote for the same class, the entropy that class vote is Alternatively, when the votes tie, the entropy is maximal. A secondary heuristic assumption is that it is probably not useful to add (almost) exact matches to the memory, since adding those is likely to have little effect on performance of the classifier. More effect can be expected from adding instances to memory that have a low-entropy class distribution in their nearest neighbour which the nearest neighbours are at a relatively Precision Recall English devel. 84.54% 87.16% 85.83 English test 77.01% 80.74% 78.83 German devel. 64.01% 52.29% 57.56 German test 66.71% 56.47% 61.16 Table 1: Overall results (precision, recall, F-rate) of the initial system on the test sets of both languages. settings mvdm feat. weight k dist. w Eng, initial yes gain ratio 21 IL 1 Eng, seedlist yes gain ratio 5 ID 1 Ger, initial yes gain ratio 21 IL 2 Ger, seedlist yes gain ratio 9 ID 2 Table 2: Optimal parameter settings estimated by iterative deepening. “w” stands for window. large distance. A large distance entails that the instances contains previously unseen feature values (words), and assuming that the predicted class label is correct, these new values can be valuable in matching and therefore classifying new test material better. We applied our selection method to the first 2 million words of the unannotated English dataset. For German we were able to process 0.25 million words. First we applied the classifier with two extensions, seed list information and second stage stacking, to classify the unannotated data. We selected instances with an entropy in the class distribution lower than 0.05 and a distance of the nearest neighbour of at least 0.1. For English, in total 179,391 instances (9%) were selected from the unannotated dataset and added to the training set. For German. markedly less instances were selected: 467 (0.19%). 5 Results 5.1 Initial classifier: Iterative deepening Iterative deepening produced estimations of optimal parameter settings for our initial systems for the two languages, displayed in the first and third row of Table 2. With this setting we achieved an overall F-rate of 78.83 for English and 61.16 for German. Table 1 lists the full evaluation results. 5.2 First and second extension: seed list features and stacking We have also performed iterative deepening in the experiment with the seed list information. This altered the best setting found by the iterative deepening process (the second and fourth rows of Table 2). The results on the English development set are slightly better than the initial system, as can be seen in Table 3. The classifier with Precision Recall English devel. 85.04% 87.26% 86.14 English test 75.03% 79.75% 77.32 German devel. 65.27% 50.76% 57.11 German test 69.31% 55.70% 61.77 Table 3: Overall results (precision, recall, F-rate) of the system with seed-list features on the test sets of both languages. Precision Recall English devel. 85.98% 87.63% 86.80 English test 76.26% 80.21% 78.18 German devel. 68.80% 52.29% 59.42 German test 71.19% 56.38% 62.93 Table 4: Overall results (precision, recall, F-rate) of the system with seed-list features and second-stage stacking on the test sets of both languages. seed list information performs worse on the English test set than the one without seed lists. The reverse effect is seen on the German data. On the development set, using the seed list information gave a slight lower performance, but on the test set it has a slightly positive effect. Our second extension, stacking, improves on all overall F-scores of both languages as compared to the seedlist extended systems, as shown in Table 4. 5.3 Third extension: Selecting instances from unannotated data The three extensions, using seed list information, performing second stage stacking and adding information from unannotated data, are combined in the final experiment. This experiment achieves the highest result on the English development set, and on both German test sets, as listed in Table 5. The positive effect of adding selected unannotated data on the German test sets is rather minimal, but we added only a very small amount of unlabeled material. The performance on the English test set is not better than the initial classifier. 6 Discussion In this paper we have presented a memory-based namedentity recognition system that chunks and labels named entities in one shot. We reported on three extensions; incorporating seed list information, second-stage stacking and adding selected instances from classified unannotated data to the training material. First, we trained and tested a basic classifier without any of the extensions. Subsequently, we found that (i) incorporating seed list information as binary features does not always help; only in two of the four test sets the seedlists had a positive effect. There can be several explanations for this, such as the quality of the seed lists, the chosen parameter setting from the iterative deepening process or overestimated weights given to the features by the classifier. Due to the tight time schedule we could not further investigate this. Second, second-stage stacking improves generalisation performance consistently on all test sets as compared to the seed-list extended systems. Third, only in the final experiment we added selected classified instances from unannotated data. This gave an additional reasonable boost in performance on the English development set, it attains an overall F-rate of 86.97 (an error reduction of 8%) over the initial classifier. The same effect was seen on both German test sets, on which the combination of the three extensions achieved a Fscore of 59.58 ( 5% error reduction ) and 63.02 ( 5% error reduction). This effect is not seen on the English test set; here the initial classifier performs best. This can partly be explained by the fact that the last two extensions were built upon the first extension, which had a markedly lower score than the initial classifier to begin with. In sum, our results suggest that two of the three extensions, the stacking method, and the unlabeled instance selection method, have been consistently helpful. Seed list features, however, have not.</abstract>
<note confidence="0.90163187037037">References R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. CELEX lexical data base on Linguistic Data Consortium, Philadelphia, PA. S. Buchholz and A. Van den Bosch. 2000. Integrating seed names and n-grams for a named entity list and In (Second International Conference on Language Resources and Evaluation) Pro- Vol. pages 1215–1221. X. Carreras, L. Marques, and L. Padro. 2002. Named extraction using AdaBoost. In of pages 167–170. S. Cucerzan and D. Yarowsky. 1999. Language independent named entity recognition combining morphologand contextual evidence. In of 1999 SIGDAT Conference on EMNLP and W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2002. TiMBL: Tilburg Memory Based Learner, version 4.3, reference guide. Technical Report ILK-0210, ILK, Tilburg University. R. Kohavi and G. John. 1997. Wrappers for feature selection. Intelligence 97(1– 2):273–324. F. Provost, D. Jensen, and T. Oates. 1999. Efficient prosampling. In of the Fifth Interna- English devel. Precision Recall LOC 89.42% 91.13% 90.27 MISC 91.36% 80.26% 85.45 ORG 74.32% 83.30% 78.55 PER 90.16% 91.53% 90.84 Overall 86.16% 87.80% 86.97 English test Precision Recall LOC 80.81% 86.33% 83.48 MISC 66.96% 75.93% 71.16 ORG 69.24% 73.99% 71.54 PER 83.98% 82.00% 82.98 Overall 76.33% 80.17% 78.20 German devel. Precision Recall LOC 61.90% 69.60% 65.52 MISC 83.25% 32.97% 47.23 ORG 67.55% 49.32% 57.01 PER 73.40% 54.96% 62.86 Overall 68.88% 52.49% 59.58 German test Precision Recall LOC 63.36% 64.83% 64.09 MISC 75.60% 32.84% 45.79 ORG 62.90% 48.90% 55.02 PER 83.47% 67.62% 74.71 Overall 71.15% 56.55% 63.02 Table 5: Results on the test sets of the variant combining all three extensions to the initial classifier. tional Conference on Knowledge Discovery and Data pages 23–32. Erik F. Tjong Kim Sang and Fien De Meulder. 2003.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX lexical data base on CD-ROM. Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. The CELEX lexical data base on CD-ROM. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>A Van den Bosch</author>
</authors>
<title>Integrating seed names and n-grams for a named entity list and classifier.</title>
<date>2000</date>
<booktitle>In LREC-2000 (Second International Conference on Language Resources and Evaluation) Proceedings. Vol. II,</booktitle>
<pages>1215--1221</pages>
<marker>Buchholz, Van den Bosch, 2000</marker>
<rawString>S. Buchholz and A. Van den Bosch. 2000. Integrating seed names and n-grams for a named entity list and classifier. In LREC-2000 (Second International Conference on Language Resources and Evaluation) Proceedings. Vol. II, pages 1215–1221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marques</author>
<author>L Padro</author>
</authors>
<title>Named entity extraction using AdaBoost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>167--170</pages>
<contexts>
<context position="1481" citStr="Carreras et al., 2002" startWordPosition="208" endWordPosition="211"> the final test set of 78.20 on English and 63.02 on German. 2 Data and features The CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) supplied datasets in two languages, English and German, using four named entity categories: persons, organisations, locations, and “miscellany names”. Manual annotation has been performed at the University of Antwerp. Apart from tokenized wordforms, the data provides predicted PoS-tags and chunks. Additionally we computed the following features with each wordform, largely following those used by the bestperforming submission of the 2002 shared task (Carreras et al., 2002): • Orthographic features represented as binary features: Begin cap, All caps, Internal cap, Contains digit, Contains digit en alpha, Initial, Lower case, First word • The wordform’s first letter and last three letters (as three separate features) • The direct output of the memory-based lemmatizer (Van den Bosch and Daelemans, 1999), providing PoS tag, morphological features, and spelling change information • PoS tag from a slow but accurate version of the memory-based tagger trained on a portion of the British National Corpus, according to the CLAWS-5 tagset (for English data only) For exampl</context>
</contexts>
<marker>Carreras, Marques, Padro, 2002</marker>
<rawString>X. Carreras, L. Marques, and L. Padro. 2002. Named entity extraction using AdaBoost. In Proceedings of CoNLL-2002, pages 167–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>In Proceedings of 1999 Joint SIGDAT Conference on EMNLP and VLC.</booktitle>
<contexts>
<context position="8117" citStr="Cucerzan and Yarowsky, 1999" startWordPosition="1292" endWordPosition="1295">imilarities between instances using these extra features, it is able, in principle, to recognise and correct reoccurring patterns of errors within sub-sentential sequences. This could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first extension. 4.3 Unannotated data For both languages a large unannotated dataset was made available for extracting data or information. Alternative to using this data to expand or bootstrap seed lists (Cucerzan and Yarowsky, 1999; Buchholz and Van den Bosch, 2000), we use the unannotated corpus to select useful instances to be added directly to the training set. Not unlike (Yarowsky, 1995) we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory. We make the simple assumption that entropy in the class distribution in the nearest neighbour set computed in the classification of a new instance is correlated with the reliability of the classification, when k &gt; 1. When k nearest neighbours all vote for the same class, the entropy of that cla</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. In Proceedings of 1999 Joint SIGDAT Conference on EMNLP and VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 4.3, reference guide.</title>
<date>2002</date>
<tech>Technical Report ILK-0210,</tech>
<institution>ILK, Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2002</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2002. TiMBL: Tilburg Memory Based Learner, version 4.3, reference guide. Technical Report ILK-0210, ILK, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kohavi</author>
<author>G John</author>
</authors>
<title>Wrappers for feature subset selection.</title>
<date>1997</date>
<journal>Artificial Intelligence Journal,</journal>
<volume>97</volume>
<issue>1</issue>
<pages>2--273</pages>
<contexts>
<context position="4227" citStr="Kohavi and John, 1997" startWordPosition="652" endWordPosition="655">sification in memory-based learning is performed by the k-NN algorithm that searches for the k ‘nearest neighbours’ among the memory instances according to the distance function. The majority class of the k nearest neighbours then determines the class of the new instance X. Cf. (Daelemans et al., 2002) for algorithmic details and background. 3.2 Iterative deepening Iterative deepening (ID) is a heuristic search algorithm for the optimization of algorithmic parameter and feature selection, that combines classifier wrapping (using the training material internally to test experimental variants) (Kohavi and John, 1997) with progressive sampling of training material (Provost et al., 1999). We start with a large pool of experiments, each with a unique combination of input features and algorithmic parameter settings. In the first step, each attempted setting is applied to a small amount of training material and tested on a fixed amount of held-out data (a held-out part of the training set). Only the best settings are kept; all others are removed from the pool of competing settings. In subsequent iterations, this step is repeated, cutting the number of settings in the pool by a half and retaining the best-perfo</context>
</contexts>
<marker>Kohavi, John, 1997</marker>
<rawString>R. Kohavi and G. John. 1997. Wrappers for feature subset selection. Artificial Intelligence Journal, 97(1– 2):273–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Provost</author>
<author>D Jensen</author>
<author>T Oates</author>
</authors>
<title>Efficient progressive sampling.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>23--32</pages>
<contexts>
<context position="4297" citStr="Provost et al., 1999" startWordPosition="662" endWordPosition="665">that searches for the k ‘nearest neighbours’ among the memory instances according to the distance function. The majority class of the k nearest neighbours then determines the class of the new instance X. Cf. (Daelemans et al., 2002) for algorithmic details and background. 3.2 Iterative deepening Iterative deepening (ID) is a heuristic search algorithm for the optimization of algorithmic parameter and feature selection, that combines classifier wrapping (using the training material internally to test experimental variants) (Kohavi and John, 1997) with progressive sampling of training material (Provost et al., 1999). We start with a large pool of experiments, each with a unique combination of input features and algorithmic parameter settings. In the first step, each attempted setting is applied to a small amount of training material and tested on a fixed amount of held-out data (a held-out part of the training set). Only the best settings are kept; all others are removed from the pool of competing settings. In subsequent iterations, this step is repeated, cutting the number of settings in the pool by a half and retaining the best-performing half, while at the same time doubling the amount of training mat</context>
</contexts>
<marker>Provost, Jensen, Oates, 1999</marker>
<rawString>F. Provost, D. Jensen, and T. Oates. 1999. Efficient progressive sampling. In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining, pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of CoNLL-2003. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
<author>W Daelemans</author>
</authors>
<title>Memorybased morphological analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>285--292</pages>
<publisher>ACL.</publisher>
<location>New Brunswick, NJ.</location>
<marker>Van den Bosch, Daelemans, 1999</marker>
<rawString>A. Van den Bosch and W. Daelemans. 1999. Memorybased morphological analysis. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 285–292, New Brunswick, NJ. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
</authors>
<title>Fast np chunking using memorybased learning techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of Benelearn</booktitle>
<pages>71--79</pages>
<contexts>
<context position="7292" citStr="Veenstra, 1998" startWordPosition="1166" endWordPosition="1167">ze from 1269 names to 78,732 names. Each wordform in the training and test data is then enriched with four binary features, each representing whether the word occurs in the respective seed list. One problem with seed lists is that a word can occur in more than one seed list, so that more than one of these four bits may be active. 4.2 Second-stage stacking The second extension is to use second-stage stacking. Stacking in general (Wolpert, 1992) encompasses a class of meta-learning systems that learn to correct errors made by lower-level classifiers. We adopt the particular method pioneered in (Veenstra, 1998) in which classifications of a first memory-based classifier are added as windowed features to the instances presented to the second classifier. Since the second-stage classifier also computes the similarities between instances using these extra features, it is able, in principle, to recognise and correct reoccurring patterns of errors within sub-sentential sequences. This could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first ex</context>
</contexts>
<marker>Veenstra, 1998</marker>
<rawString>J. Veenstra. 1998. Fast np chunking using memorybased learning techniques. In Proceedings of Benelearn 1998, pages 71–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Wolpert</author>
</authors>
<title>On overfitting avoidance as bias.</title>
<date>1992</date>
<tech>Technical Report SFI TR 92-03-5001,</tech>
<institution>The Santa Fe Institute.</institution>
<contexts>
<context position="7124" citStr="Wolpert, 1992" startWordPosition="1142" endWordPosition="1143">m name sites on the internet, from the training set (for the MISC category), and from the CELEX English lexical data base (Baayen et al., 1993). These lists vary in size from 1269 names to 78,732 names. Each wordform in the training and test data is then enriched with four binary features, each representing whether the word occurs in the respective seed list. One problem with seed lists is that a word can occur in more than one seed list, so that more than one of these four bits may be active. 4.2 Second-stage stacking The second extension is to use second-stage stacking. Stacking in general (Wolpert, 1992) encompasses a class of meta-learning systems that learn to correct errors made by lower-level classifiers. We adopt the particular method pioneered in (Veenstra, 1998) in which classifications of a first memory-based classifier are added as windowed features to the instances presented to the second classifier. Since the second-stage classifier also computes the similarities between instances using these extra features, it is able, in principle, to recognise and correct reoccurring patterns of errors within sub-sentential sequences. This could correct errors made due to the “blindness” of the </context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>D. H. Wolpert. 1992. On overfitting avoidance as bias. Technical Report SFI TR 92-03-5001, The Santa Fe Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In ACL33,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="8280" citStr="Yarowsky, 1995" startWordPosition="1322" endWordPosition="1323">is could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first extension. 4.3 Unannotated data For both languages a large unannotated dataset was made available for extracting data or information. Alternative to using this data to expand or bootstrap seed lists (Cucerzan and Yarowsky, 1999; Buchholz and Van den Bosch, 2000), we use the unannotated corpus to select useful instances to be added directly to the training set. Not unlike (Yarowsky, 1995) we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory. We make the simple assumption that entropy in the class distribution in the nearest neighbour set computed in the classification of a new instance is correlated with the reliability of the classification, when k &gt; 1. When k nearest neighbours all vote for the same class, the entropy of that class vote is 0.0. Alternatively, when the votes tie, the entropy is maximal. A secondary heuristic assumption is that it is probably not useful to add (almost) exact</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL33, pages 189–196, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>