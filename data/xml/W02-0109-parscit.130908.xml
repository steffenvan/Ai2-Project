<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001935">
<title confidence="0.951309">
NLTK: The Natural Language Toolkit
</title>
<author confidence="0.997028">
Edward Loper and Steven Bird
</author>
<affiliation confidence="0.999802">
Department of Computer and Information Science
University of Pennsylvania, Philadelphia, PA 19104-6389, USA
</affiliation>
<sectionHeader confidence="0.988122" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963166666667">
NLTK, the Natural Language Toolkit,
is a suite of open source program
modules, tutorials and problem sets,
providing ready-to-use computational
linguistics courseware. NLTK covers
symbolic and statistical natural lan-
guage processing, and is interfaced to
annotated corpora. Students augment
and replace existing components, learn
structured programming by example,
and manipulate sophisticated models
from the outset.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878032258065">
Teachers of introductory courses on compu-
tational linguistics are often faced with the
challenge of setting up a practical programming
component for student assignments and
projects. This is a difficult task because
different computational linguistics domains
require a variety of different data structures
and functions, and because a diverse range of
topics may need to be included in the syllabus.
A widespread practice is to employ multiple
programming languages, where each language
provides native data structures and functions
that are a good fit for the task at hand. For
example, a course might use Prolog for pars-
ing, Perl for corpus processing, and a finite-state
toolkit for morphological analysis. By relying
on the built-in features of various languages, the
teacher avoids having to develop a lot of software
infrastructure.
An unfortunate consequence is that a
significant part of such courses must be devoted
to teaching programming languages. Further,
many interesting projects span a variety of
domains, and would require that multiple
languages be bridged. For example, a student
project that involved syntactic parsing of corpus
data from a morphologically rich language might
involve all three of the languages mentioned
above: Perl for string processing; a finite state
toolkit for morphological analysis; and Prolog
for parsing. It is clear that these considerable
overheads and shortcomings warrant a fresh
approach.
Apart from the practical component, compu-
tational linguistics courses may also depend on
software for in-class demonstrations. This con-
text calls for highly interactive graphical user
interfaces, making it possible to view program
state (e.g. the chart of a chart parser), observe
program execution step-by-step (e.g. execu-
tion of a finite-state machine), and even make
minor modifications to programs in response to
“what if” questions from the class. Because
of these difficulties it is common to avoid live
demonstrations, and keep classes for theoreti-
cal presentations only. Apart from being dull,
this approach leaves students to solve important
practical problems on their own, or to deal with
them less efficiently in office hours.
In this paper we introduce a new approach to
the above challenges, a streamlined and flexible
way of organizing the practical component
of an introductory computational linguistics
course. We describe NLTK, the Natural
Language Toolkit, which we have developed in
conjunction with a course we have taught at
the University of Pennsylvania.
The Natural Language Toolkit is avail-
able under an open source license from
http://nltk.sf.net/. NLTK runs on all
platforms supported by Python, including
Windows, OS X, Linux, and Unix.
</bodyText>
<sectionHeader confidence="0.711089" genericHeader="introduction">
2 Choice of Programming Language
</sectionHeader>
<bodyText confidence="0.9999494">
The most basic step in setting up a practical
component is choosing a suitable programming
language. A number of considerations
influenced our choice. First, the language must
have a shallow learning curve, so that novice
programmers get immediate rewards for their
efforts. Second, the language must support
rapid prototyping and a short develop/test
cycle; an obligatory compilation step is a
serious detraction. Third, the code should be
self-documenting, with a transparent syntax and
semantics. Fourth, it should be easy to write
structured programs, ideally object-oriented but
without the burden associated with languages
like C++. Finally, the language must have
an easy-to-use graphics library to support the
development of graphical user interfaces.
In surveying the available languages, we
believe that Python offers an especially good
fit to the above requirements. Python is an
object-oriented scripting language developed
by Guido van Rossum and available on all
platforms (www.python.org). Python offers
a shallow learning curve; it was designed to
be easily learnt by children (van Rossum,
1999). As an interpreted language, Python is
suitable for rapid prototyping. Python code is
exceptionally readable, and it has been praised
as “executable pseudocode.” Python is an
object-oriented language, but not punitively
so, and it is easy to encapsulate data and
methods inside Python classes. Finally, Python
has an interface to the Tk graphics toolkit
(Lundh, 1999), and writing graphical interfaces
is straightforward.
</bodyText>
<sectionHeader confidence="0.996528" genericHeader="method">
3 Design Criteria
</sectionHeader>
<bodyText confidence="0.999945">
Several criteria were considered in the design
and implementation of the toolkit. These design
criteria are listed in the order of their impor-
tance. It was also important to decide what
goals the toolkit would not attempt to accom-
plish; we therefore include an explicit set of non-
requirements, which the toolkit is not expected
to satisfy.
</bodyText>
<subsectionHeader confidence="0.997014">
3.1 Requirements
</subsectionHeader>
<bodyText confidence="0.999794916666667">
Ease of Use. The primary purpose of the
toolkit is to allow students to concentrate on
building natural language processing (NLP) sys-
tems. The more time students must spend learn-
ing to use the toolkit, the less useful it is.
Consistency. The toolkit should use consis-
tent data structures and interfaces.
Extensibility. The toolkit should easily
accommodate new components, whether those
components replicate or extend the toolkit’s
existing functionality. The toolkit should
be structured in such a way that it is obvious
where new extensions would fit into the toolkit’s
infrastructure.
Documentation. The toolkit, its data
structures, and its implementation all need to
be carefully and thoroughly documented. All
nomenclature must be carefully chosen and
consistently used.
Simplicity. The toolkit should structure the
complexities of building NLP systems, not hide
them. Therefore, each class defined by the
toolkit should be simple enough that a student
could implement it by the time they finish an
introductory course in computational linguis-
tics.
Modularity. The interaction between differ-
ent components of the toolkit should be kept
to a minimum, using simple, well-defined inter-
faces. In particular, it should be possible to
complete individual projects using small parts
of the toolkit, without worrying about how they
interact with the rest of the toolkit. This allows
students to learn how to use the toolkit incre-
mentally throughout a course. Modularity also
makes it easier to change and extend the toolkit.
</bodyText>
<subsectionHeader confidence="0.999754">
3.2 Non-Requirements
</subsectionHeader>
<bodyText confidence="0.999872833333333">
Comprehensiveness. The toolkit is not
intended to provide a comprehensive set of
tools. Indeed, there should be a wide variety of
ways in which students can extend the toolkit.
Efficiency. The toolkit does not need to
be highly optimized for runtime performance.
However, it should be efficient enough that
students can use their NLP systems to perform
real tasks.
Cleverness. Clear designs and implementa-
tions are far preferable to ingenious yet inde-
cipherable ones.
</bodyText>
<sectionHeader confidence="0.99498" genericHeader="method">
4 Modules
</sectionHeader>
<bodyText confidence="0.999984842105263">
The toolkit is implemented as a collection of
independent modules, each of which defines a
specific data structure or task.
A set of core modules defines basic data
types and processing systems that are used
throughout the toolkit. The token module
provides basic classes for processing individual
elements of text, such as words or sentences.
The tree module defines data structures for
representing tree structures over text, such
as syntax trees and morphological trees. The
probability module implements classes that
encode frequency distributions and probability
distributions, including a variety of statistical
smoothing techniques.
The remaining modules define data structures
and interfaces for performing specific NLP tasks.
This list of modules will grow over time, as we
add new tasks and algorithms to the toolkit.
</bodyText>
<subsectionHeader confidence="0.994935">
Parsing Modules
</subsectionHeader>
<bodyText confidence="0.999984411764706">
The parser module defines a high-level inter-
face for producing trees that represent the struc-
tures of texts. The chunkparser module defines
a sub-interface for parsers that identify non-
overlapping linguistic groups (such as base noun
phrases) in unrestricted text.
Four modules provide implementations
for these abstract interfaces. The srparser
module implements a simple shift-reduce
parser. The chartparser module defines a
flexible parser that uses a chart to record
hypotheses about syntactic constituents. The
pcfgparser module provides a variety of
different parsers for probabilistic grammars.
And the rechunkparser module defines a
transformational regular-expression based
implementation of the chunk parser interface.
</bodyText>
<subsectionHeader confidence="0.99159">
Tagging Modules
</subsectionHeader>
<bodyText confidence="0.9984084">
The tagger module defines a standard interface
for augmenting each token of a text with supple-
mentary information, such as its part of speech
or its WordNet synset tag; and provides several
different implementations for this interface.
</bodyText>
<subsectionHeader confidence="0.970886">
Finite State Automata
</subsectionHeader>
<bodyText confidence="0.999956666666667">
The fsa module defines a data type for encod-
ing finite state automata; and an interface for
creating automata from regular expressions.
</bodyText>
<subsectionHeader confidence="0.990114">
Type Checking
</subsectionHeader>
<bodyText confidence="0.999982615384615">
Debugging time is an important factor in the
toolkit’s ease of use. To reduce the amount of
time students must spend debugging their code,
we provide a type checking module, which can
be used to ensure that functions are given valid
arguments. The type checking module is used
by all of the basic data types and processing
classes.
Since type checking is done explicitly, it can
slow the toolkit down. However, when efficiency
is an issue, type checking can be easily turned
off; and with type checking is disabled, there is
no performance penalty.
</bodyText>
<subsectionHeader confidence="0.733951">
Visualization
</subsectionHeader>
<bodyText confidence="0.999950857142857">
Visualization modules define graphical
interfaces for viewing and manipulating
data structures, and graphical tools for
experimenting with NLP tasks. The draw.tree
module provides a simple graphical inter-
face for displaying tree structures. The
draw.tree edit module provides an interface
for building and modifying tree structures.
The draw.plot graph module can be used to
graph mathematical functions. The draw.fsa
module provides a graphical tool for displaying
and simulating finite state automata. The
draw.chart module provides an interactive
graphical tool for experimenting with chart
parsers.
The visualization modules provide interfaces
for interaction and experimentation; they do
not directly implement NLP data structures or
tasks. Simplicity of implementation is therefore
less of an issue for the visualization modules
than it is for the rest of the toolkit.
</bodyText>
<subsectionHeader confidence="0.900956">
Text Classification
</subsectionHeader>
<bodyText confidence="0.9999085">
The classifier module defines a standard
interface for classifying texts into categories.
This interface is currently implemented by two
modules. The classifier.naivebayes module
defines a text classifier based on the Naive Bayes
assumption. The classifier.maxent module
defines the maximum entropy model for text
classification, and implements two algorithms
for training the model: Generalized Iterative
Scaling and Improved Iterative Scaling.
The classifier.feature module provides
a standard encoding for the information that
is used to make decisions for a particular
classification task. This standard encoding
allows students to experiment with the
differences between different text classification
algorithms, using identical feature sets.
The classifier.featureselection module
defines a standard interface for choosing which
features are relevant for a particular classifica-
tion task. Good feature selection can signifi-
cantly improve classification performance.
</bodyText>
<sectionHeader confidence="0.998686" genericHeader="method">
5 Documentation
</sectionHeader>
<bodyText confidence="0.99713362962963">
The toolkit is accompanied by extensive
documentation that explains the toolkit, and
describes how to use and extend it. This
documentation is divided into three primary
categories:
Tutorials teach students how to use the
toolkit, in the context of performing specific
tasks. Each tutorial focuses on a single domain,
such as tagging, probabilistic systems, or text
classification. The tutorials include a high-level
discussion that explains and motivates the
domain, followed by a detailed walk-through
that uses examples to show how NLTK can be
used to perform specific tasks.
Reference Documentation provides precise
definitions for every module, interface, class,
method, function, and variable in the toolkit. It
is automatically extracted from docstring com-
ments in the Python source code, using Epydoc
(Loper, 2002).
Technical Reports explain and justify the
toolkit’s design and implementation. They are
used by the developers of the toolkit to guide
and document the toolkit’s construction. Stu-
dents can also consult these reports if they would
like further information about how the toolkit is
designed, and why it is designed that way.
</bodyText>
<sectionHeader confidence="0.94571" genericHeader="method">
6 Uses of NLTK
</sectionHeader>
<subsectionHeader confidence="0.9851">
6.1 Assignments
</subsectionHeader>
<bodyText confidence="0.999960615384615">
NLTK can be used to create student assign-
ments of varying difficulty and scope. In the
simplest assignments, students experiment with
an existing module. The wide variety of existing
modules provide many opportunities for creat-
ing these simple assignments. Once students
become more familiar with the toolkit, they can
be asked to make minor changes or extensions to
an existing module. A more challenging task is
to develop a new module. Here, NLTK provides
some useful starting points: predefined inter-
faces and data structures, and existing modules
that implement the same interface.
</bodyText>
<sectionHeader confidence="0.561533" genericHeader="method">
Example: Chunk Parsing
</sectionHeader>
<bodyText confidence="0.999953454545454">
As an example of a moderately difficult
assignment, we asked students to construct
a chunk parser that correctly identifies base
noun phrase chunks in a given text, by
defining a cascade of transformational chunking
rules. The NLTK rechunkparser module
provides a variety of regular-expression
based rule types, which the students can
instantiate to construct complete rules.
For example, ChunkRule(’&lt;NN.W) builds
chunks from sequences of consecutive nouns;
ChinkRule(’&lt;VB.&gt;’) excises verbs from
existing chunks; SplitRule(’&lt;NN&gt;’, ’&lt;DT&gt;’)
splits any existing chunk that contains a
singular noun followed by determiner into
two pieces; and MergeRule(’&lt;JJ&gt;’, ’&lt;JJ&gt;’)
combines two adjacent chunks where the first
chunk ends and the second chunk starts with
adjectives.
The chunking tutorial motivates chunk pars-
ing, describes each rule type, and provides all
the necessary code for the assignment. The pro-
vided code is responsible for loading the chun-
ked, part-of-speech tagged text using an existing
tokenizer, creating an unchunked version of the
text, applying the chunk rules to the unchunked
text, and scoring the result. Students focus on
the NLP task only – providing a rule set with
the best coverage.
In the remainder of this section we reproduce
some of the cascades created by the students.
The first example illustrates a combination of
several rule types:
</bodyText>
<equation confidence="0.664218">
cascade = [
</equation>
<bodyText confidence="0.853253466666667">
ChunkRule(’&lt;DT&gt;&lt;NN.*&gt;&lt;VB.&gt;&lt;NN.*&gt;’),
ChunkRule(’&lt;DT&gt;&lt;VB.&gt;&lt;NN.*&gt;’),
ChunkRule(’&lt;.*&gt;’),
UnChunkRule(’&lt;IN|VB.*|CC|MD|RB.*&gt;’),
UnChunkRule(&amp;quot;&lt;,|\\.|“|’’&gt;&amp;quot;),
MergeRule(’&lt;NN.*|DT|JJ.*|CD&gt;’,
’&lt;NN.*|DT|JJ.*|CD&gt;’),
SplitRule(’&lt;NN.*&gt;’, ’&lt;DT|JJ&gt;’)
]
The next example illustrates a brute-force sta-
tistical approach. The student calculated how
often each part-of-speech tag was included in
a noun phrase. They then constructed chunks
from any sequence of tags that occurred in a
noun phrase more than 50% of the time.
</bodyText>
<equation confidence="0.9041288">
cascade = [
ChunkRule(’&lt;\\$|CD|DT|EX|PDT
|PRP.*|WP.*|\\#|FW
|JJ.*|NN.*|POS|RBS|WDT&gt;*’)
]
</equation>
<bodyText confidence="0.994972333333333">
In the third example, the student constructed
a single chunk containing the entire text, and
then excised all elements that did not belong.
</bodyText>
<equation confidence="0.89785875">
cascade = [
ChunkRule(’&lt;.*&gt;+’)
ChinkRule(’&lt;VB.*|IN|CC|R.*|MD|WRB|TO|.|,&gt;+’)
]
</equation>
<subsectionHeader confidence="0.99898">
6.2 Class demonstrations
</subsectionHeader>
<bodyText confidence="0.999981153846154">
NLTK provides graphical tools that can be used
in class demonstrations to help explain basic
NLP concepts and algorithms. These interactive
tools can be used to display relevant data struc-
tures and to show the step-by-step execution of
algorithms. Both data structures and control
flow can be easily modified during the demon-
stration, in response to questions from the class.
Since these graphical tools are included with
the toolkit, they can also be used by students.
This allows students to experiment at home with
the algorithms that they have seen presented in
class.
</bodyText>
<subsectionHeader confidence="0.422226">
Example: The Chart Parsing Tool
</subsectionHeader>
<bodyText confidence="0.999762838709677">
The chart parsing tool is an example of a
graphical tool provided by NLTK. This tool can
be used to explain the basic concepts behind
chart parsing, and to show how the algorithm
works. Chart parsing is a flexible parsing algo-
rithm that uses a data structure called a chart to
record hypotheses about syntactic constituents.
Each hypothesis is represented by a single edge
on the chart. A set of rules determine when new
edges can be added to the chart. This set of rules
controls the overall behavior of the parser (e.g.,
whether it parses top-down or bottom-up).
The chart parsing tool demonstrates the pro-
cess of parsing a single sentence, with a given
grammar and lexicon. Its display is divided into
three sections: the bottom section displays the
chart; the middle section displays the sentence;
and the top section displays the partial syntax
tree corresponding to the selected edge. But-
tons along the bottom of the window are used
to control the execution of the algorithm. The
main display window for the chart parsing tool
is shown in Figure 1.
This tool can be used to explain several dif-
ferent aspects of chart parsing. First, it can be
used to explain the basic chart data structure,
and to show how edges can represent hypothe-
ses about syntactic constituents. It can then
be used to demonstrate and explain the indi-
vidual rules that the chart parser uses to create
new edges. Finally, it can be used to show how
</bodyText>
<figureCaption confidence="0.999344">
Figure 1: Chart Parsing Tool
</figureCaption>
<bodyText confidence="0.999839555555556">
these individual rules combine to find a complete
parse for a given sentence.
To reduce the overhead of setting up demon-
strations during lecture, the user can define a
list of preset charts. The tool can then be reset
to any one of these charts at any time.
The chart parsing tool allows for flexible con-
trol of the parsing algorithm. At each step of
the algorithm, the user can select which rule or
strategy they wish to apply. This allows the user
to experiment with mixing different strategies
(e.g., top-down and bottom-up). The user can
exercise fine-grained control over the algorithm
by selecting which edge they wish to apply a rule
to. This flexibility allows lecturers to use the
tool to respond to a wide variety of questions;
and allows students to experiment with different
variations on the chart parsing algorithm.
</bodyText>
<subsectionHeader confidence="0.998842">
6.3 Advanced Projects
</subsectionHeader>
<bodyText confidence="0.9999272">
NLTK provides students with a flexible frame-
work for advanced projects. Typical projects
involve the development of entirely new func-
tionality for a previously unsupported NLP task,
or the development of a complete system out of
existing and new modules.
The toolkit’s broad coverage allows students
to explore a wide variety of topics. In our intro-
ductory computational linguistics course, topics
for student projects included text generation,
word sense disambiguation, collocation analysis,
and morphological analysis.
NLTK eliminates the tedious infrastructure-
building that is typically associated with
advanced student projects by providing
students with the basic data structures, tools,
and interfaces that they need. This allows the
students to concentrate on the problems that
interest them.
The collaborative, open-source nature of the
toolkit can provide students with a sense that
their projects are meaningful contributions, and
not just exercises. Several of the students in our
course have expressed interest in incorporating
their projects into the toolkit.
Finally, many of the modules included in the
toolkit provide students with good examples
of what projects should look like, with well
thought-out interfaces, clean code structure, and
thorough documentation.
</bodyText>
<subsectionHeader confidence="0.794828">
Example: Probabilistic Parsing
</subsectionHeader>
<bodyText confidence="0.999981333333334">
The probabilistic parsing module was created
as a class project for a statistical NLP course.
The toolkit provided the basic data types and
interfaces for parsing. The project extended
these, adding a new probabilistic parsing inter-
face, and using subclasses to create a prob-
abilistic version of the context free grammar
data structure. These new components were
used in conjunction with several existing compo-
nents, such as the chart data structure, to define
two implementations of the probabilistic parsing
interface. Finally, a tutorial was written that
explained the basic motivations and concepts
behind probabilistic parsing, and described the
new interfaces, data structures, and parsers.
</bodyText>
<sectionHeader confidence="0.994979" genericHeader="method">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.9999715">
We used NLTK as a basis for the assignments
and student projects in CIS-530, an introduc-
tory computational linguistics class taught at
the University of Pennsylvania. CIS-530 is a
graduate level class, although some advanced
undergraduates were also enrolled. Most stu-
dents had a background in either computer sci-
ence or linguistics (and occasionally both). Stu-
dents were required to complete five assign-
ments, two exams, and a final project. All class
materials are available from the course website
http://www.cis.upenn.edu/~cis530/.
The experience of using NLTK was very pos-
itive, both for us and for the students. The
students liked the fact that they could do inter-
esting projects from the outset. They also liked
being able to run everything on their computer
at home. The students found the extensive doc-
umentation very helpful for learning to use the
toolkit. They found the interfaces defined by
NLTK intuitive, and appreciated the ease with
which they could combine different components
to create complete NLP systems.
We did encounter a few difficulties during the
semester. One problem was finding large clean
corpora that the students could use for their
assignments. Several of the students needed
assistance finding suitable corpora for their
final projects. Another issue was the fact that
we were actively developing NLTK during the
semester; some modules were only completed
one or two weeks before the students used
them. As a result, students who worked at
home needed to download new versions of the
toolkit several times throughout the semester.
Luckily, Python has extensive support for
installation scripts, which made these upgrades
simple. The students encountered a couple of
bugs in the toolkit, but none were serious, and
all were quickly corrected.
</bodyText>
<sectionHeader confidence="0.996545" genericHeader="method">
8 Other Approaches
</sectionHeader>
<bodyText confidence="0.968853325">
The computational component of computational
linguistics courses takes many forms. In this sec-
tion we briefly review a selection of approaches,
classified according to the (original) target audi-
ence.
Linguistics Students. Various books intro-
duce programming or computing to linguists.
These are elementary on the computational side,
providing a gentle introduction to students hav-
ing no prior experience in computer science.
Examples of such books are: Using Computers
in Linguistics (Lawler and Dry, 1998), and Pro-
gramming for Linguistics: Java Technology for
Language Researchers (Hammond, 2002).
Grammar Developers. Infrastructure
for grammar development has a long history
in unification-based (or constraint-based)
grammar frameworks, from DCG (Pereira
and Warren, 1980) to HPSG (Pollard and
Sag, 1994). Recent work includes (Copestake,
2000; Baldridge et al., 2002a). A concurrent
development has been the finite state toolkits,
such as the Xerox toolkit (Beesley and
Karttunen, 2002). This work has found
widespread pedagogical application.
Other Researchers and Developers.
A variety of toolkits have been created for
research or R&amp;D purposes. Examples include
the CMU-Cambridge Statistical Language
Modeling Toolkit (Clarkson and Rosenfeld,
1997), the EMU Speech Database System
(Harrington and Cassidy, 1999), the General
Architecture for Text Engineering (Bontcheva
et al., 2002), the Maxent Package for Maximum
Entropy Models (Baldridge et al., 2002b), and
the Annotation Graph Toolkit (Maeda et al.,
2002). Although not originally motivated by
pedagogical needs, all of these toolkits have
pedagogical applications and many have already
been used in teaching.
</bodyText>
<sectionHeader confidence="0.992169" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999974470588235">
NLTK provides a simple, extensible, uniform
framework for assignments, projects, and class
demonstrations. It is well documented, easy to
learn, and simple to use. We hope that NLTK
will allow computational linguistics classes to
include more hands-on experience with using
and building NLP components and systems.
NLTK is unique in its combination of three
factors. First, it was deliberately designed as
courseware and gives pedagogical goals primary
status. Second, its target audience consists of
both linguists and computer scientists, and it
is accessible and challenging at many levels of
prior computational skill. Finally, it is based on
an object-oriented scripting language support-
ing rapid prototyping and literate programming.
We plan to continue extending the breadth
of materials covered by the toolkit. We are
currently working on NLTK modules for Hidden
Markov Models, language modeling, and tree
adjoining grammars. We also plan to increase
the number of algorithms implemented by some
existing modules, such as the text classification
module.
Finding suitable corpora is a prerequisite for
many student assignments and projects. We are
therefore putting together a collection of corpora
containing data appropriate for every module
defined by the toolkit.
NLTK is an open source project, and we wel-
come any contributions. Readers who are inter-
ested in contributing to NLTK, or who have
suggestions for improvements, are encouraged to
contact the authors.
</bodyText>
<sectionHeader confidence="0.996681" genericHeader="acknowledgments">
10 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999984">
We are indebted to our students for feedback
on the toolkit, and to anonymous reviewers, Jee
Bang, and the workshop organizers for com-
ments on an earlier version of this paper. We are
grateful to Mitch Marcus and the Department of
Computer and Information Science at the Uni-
versity of Pennsylvania for sponsoring the work
reported here.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999688476190476">
Jason Baldridge, John Dowding, and Susana Early.
2002a. Leo: an architecture for sharing resources
for unification-based grammars. In Proceedings
of the Third Language Resources and Evaluation
Conference. Paris: European Language Resources
Association.
http://www.iccs.informatics.ed.ac.uk/
~jmb/leo-lrec.ps.gz.
Jason Baldridge, Thomas Morton, and Gann
Bierner. 2002b. The MaxEnt project.
http://maxent.sourceforge.net/.
Kenneth R. Beesley and Lauri Karttunen. 2002.
Finite-State Morphology: Xerox Tools and Tech-
niques. Studies in Natural Language Processing.
Cambridge University Press.
Kalina Bontcheva, Hamish Cunningham, Valentin
Tablan, Diana Maynard, and Oana Hamza. 2002.
Using GATE as an environment for teaching NLP.
In Proceedings of the ACL Workshop on Effective
Tools and Methodologies for Teaching NLP and
CL. Somerset, NJ: Association for Computational
Linguistics.
Philip R. Clarkson and Ronald Rosenfeld.
1997. Statistical language modeling using
the CMU-Cambridge Toolkit. In Proceedings
of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH
’97). http://svr-www.eng.cam.ac.uk/~prc14/
eurospeech97.ps.
Ann Copestake. 2000. The (new) LKB system.
http://www-csli.stanford.edu/~aac/doc5-2.
pdf.
Michael Hammond. 2002. Programming for Linguis-
tics: Java Technology for Language Researchers.
Oxford: Blackwell. In press.
Jonathan Harrington and Steve Cassidy. 1999. Tech-
niques in Speech Acoustics. Kluwer.
John M. Lawler and Helen Aristar Dry, editors.
1998. Using Computers in Linguistics. London:
Routledge.
Edward Loper. 2002. Epydoc.
http://epydoc.sourceforge.net/.
Fredrik Lundh. 1999. An introduction to tkinter.
http://www.pythonware.com/library/
tkinter/introduction/index.htm.
Kazuaki Maeda, Steven Bird, Xiaoyi Ma, and Hae-
joong Lee. 2002. Creating annotation tools with
the annotation graph toolkit. In Proceedings of
the Third International Conference on Language
Resources and Evaluation. http://arXiv.org/
abs/cs/0204005.
Fernando C. N. Pereira and David H. D. Warren.
1980. Definite clause grammars for language anal-
ysis – a survey of the formalism and a comparison
with augmented transition grammars. Artificial
Intelligence, 13:231–78.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Chicago University
Press.
Guido van Rossum. 1999. Computer program-
ming for everybody. Technical report, Corpo-
ration for National Research Initiatives. http:
//www.python.org/doc/essays/cp4e.html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.751177">
<title confidence="0.999824">NLTK: The Natural Language Toolkit</title>
<author confidence="0.99973">Edward Loper</author>
<author confidence="0.99973">Steven</author>
<affiliation confidence="0.999982">Department of Computer and Information Science</affiliation>
<address confidence="0.7677">University of Pennsylvania, Philadelphia, PA 19104-6389, USA</address>
<abstract confidence="0.998337461538461">NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>John Dowding</author>
<author>Susana Early</author>
</authors>
<title>Leo: an architecture for sharing resources for unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third Language Resources and Evaluation Conference. Paris: European Language Resources Association.</booktitle>
<contexts>
<context position="23333" citStr="Baldridge et al., 2002" startWordPosition="3514" endWordPosition="3517">ogramming or computing to linguists. These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and t</context>
</contexts>
<marker>Baldridge, Dowding, Early, 2002</marker>
<rawString>Jason Baldridge, John Dowding, and Susana Early. 2002a. Leo: an architecture for sharing resources for unification-based grammars. In Proceedings of the Third Language Resources and Evaluation Conference. Paris: European Language Resources Association.</rawString>
</citation>
<citation valid="false">
<note>http://www.iccs.informatics.ed.ac.uk/ ~jmb/leo-lrec.ps.gz.</note>
<marker></marker>
<rawString>http://www.iccs.informatics.ed.ac.uk/ ~jmb/leo-lrec.ps.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Thomas Morton</author>
<author>Gann Bierner</author>
</authors>
<date>2002</date>
<note>The MaxEnt project. http://maxent.sourceforge.net/.</note>
<contexts>
<context position="23333" citStr="Baldridge et al., 2002" startWordPosition="3514" endWordPosition="3517">ogramming or computing to linguists. These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and t</context>
</contexts>
<marker>Baldridge, Morton, Bierner, 2002</marker>
<rawString>Jason Baldridge, Thomas Morton, and Gann Bierner. 2002b. The MaxEnt project. http://maxent.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
<author>Lauri Karttunen</author>
</authors>
<title>Finite-State Morphology: Xerox Tools and Techniques. Studies in Natural Language Processing.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="23453" citStr="Beesley and Karttunen, 2002" startWordPosition="3532" endWordPosition="3535">on to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002). Although not originally motivated by pedagogical needs, all of these t</context>
</contexts>
<marker>Beesley, Karttunen, 2002</marker>
<rawString>Kenneth R. Beesley and Lauri Karttunen. 2002. Finite-State Morphology: Xerox Tools and Techniques. Studies in Natural Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Hamish Cunningham</author>
<author>Valentin Tablan</author>
<author>Diana Maynard</author>
<author>Oana Hamza</author>
</authors>
<title>Using GATE as an environment for teaching NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching NLP</booktitle>
<contexts>
<context position="23853" citStr="Bontcheva et al., 2002" startWordPosition="3586" endWordPosition="3589">n, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002). Although not originally motivated by pedagogical needs, all of these toolkits have pedagogical applications and many have already been used in teaching. 9 Conclusions and Future Work NLTK provides a simple, extensible, uniform framework for assignments, projects, and class demonstrations. It is well documented, easy to learn, and simple to use. We hope that NLTK will allow computational linguistics classes to include more hands-on experience with using and building </context>
</contexts>
<marker>Bontcheva, Cunningham, Tablan, Maynard, Hamza, 2002</marker>
<rawString>Kalina Bontcheva, Hamish Cunningham, Valentin Tablan, Diana Maynard, and Oana Hamza. 2002. Using GATE as an environment for teaching NLP. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL. Somerset, NJ: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge Toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology (EUROSPEECH ’97). http://svr-www.eng.cam.ac.uk/~prc14/ eurospeech97.ps.</booktitle>
<contexts>
<context position="23718" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="3567" endWordPosition="3570">ructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002). Although not originally motivated by pedagogical needs, all of these toolkits have pedagogical applications and many have already been used in teaching. 9 Conclusions and Future Work NLTK provides a simple, extensible, uniform framework for assignments, projects, and class demonstrations. It is well documented, easy to learn, and sim</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip R. Clarkson and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge Toolkit. In Proceedings of the 5th European Conference on Speech Communication and Technology (EUROSPEECH ’97). http://svr-www.eng.cam.ac.uk/~prc14/ eurospeech97.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>The (new) LKB system.</title>
<date>2000</date>
<pages>5--2</pages>
<contexts>
<context position="23309" citStr="Copestake, 2000" startWordPosition="3512" endWordPosition="3513">ooks introduce programming or computing to linguists. These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldrid</context>
</contexts>
<marker>Copestake, 2000</marker>
<rawString>Ann Copestake. 2000. The (new) LKB system. http://www-csli.stanford.edu/~aac/doc5-2. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hammond</author>
</authors>
<title>Programming for Linguistics: Java Technology for Language Researchers.</title>
<date>2002</date>
<publisher>Blackwell. In press.</publisher>
<location>Oxford:</location>
<contexts>
<context position="23060" citStr="Hammond, 2002" startWordPosition="3479" endWordPosition="3480">Other Approaches The computational component of computational linguistics courses takes many forms. In this section we briefly review a selection of approaches, classified according to the (original) target audience. Linguistics Students. Various books introduce programming or computing to linguists. These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistica</context>
</contexts>
<marker>Hammond, 2002</marker>
<rawString>Michael Hammond. 2002. Programming for Linguistics: Java Technology for Language Researchers. Oxford: Blackwell. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Harrington</author>
<author>Steve Cassidy</author>
</authors>
<title>Techniques in Speech Acoustics.</title>
<date>1999</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="23781" citStr="Harrington and Cassidy, 1999" startWordPosition="3576" endWordPosition="3579">on-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002). Although not originally motivated by pedagogical needs, all of these toolkits have pedagogical applications and many have already been used in teaching. 9 Conclusions and Future Work NLTK provides a simple, extensible, uniform framework for assignments, projects, and class demonstrations. It is well documented, easy to learn, and simple to use. We hope that NLTK will allow computational linguist</context>
</contexts>
<marker>Harrington, Cassidy, 1999</marker>
<rawString>Jonathan Harrington and Steve Cassidy. 1999. Techniques in Speech Acoustics. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M John</author>
</authors>
<date>1998</date>
<booktitle>Using Computers in Linguistics.</booktitle>
<editor>Lawler and Helen Aristar Dry, editors.</editor>
<publisher>Routledge.</publisher>
<location>London:</location>
<marker>John, 1998</marker>
<rawString>John M. Lawler and Helen Aristar Dry, editors. 1998. Using Computers in Linguistics. London: Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
</authors>
<date>2002</date>
<note>Epydoc. http://epydoc.sourceforge.net/.</note>
<contexts>
<context position="12551" citStr="Loper, 2002" startWordPosition="1861" endWordPosition="1862"> to use the toolkit, in the context of performing specific tasks. Each tutorial focuses on a single domain, such as tagging, probabilistic systems, or text classification. The tutorials include a high-level discussion that explains and motivates the domain, followed by a detailed walk-through that uses examples to show how NLTK can be used to perform specific tasks. Reference Documentation provides precise definitions for every module, interface, class, method, function, and variable in the toolkit. It is automatically extracted from docstring comments in the Python source code, using Epydoc (Loper, 2002). Technical Reports explain and justify the toolkit’s design and implementation. They are used by the developers of the toolkit to guide and document the toolkit’s construction. Students can also consult these reports if they would like further information about how the toolkit is designed, and why it is designed that way. 6 Uses of NLTK 6.1 Assignments NLTK can be used to create student assignments of varying difficulty and scope. In the simplest assignments, students experiment with an existing module. The wide variety of existing modules provide many opportunities for creating these simple </context>
</contexts>
<marker>Loper, 2002</marker>
<rawString>Edward Loper. 2002. Epydoc. http://epydoc.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrik Lundh</author>
</authors>
<title>An introduction to tkinter.</title>
<date>1999</date>
<note>http://www.pythonware.com/library/ tkinter/introduction/index.htm.</note>
<contexts>
<context position="4834" citStr="Lundh, 1999" startWordPosition="716" endWordPosition="717">uirements. Python is an object-oriented scripting language developed by Guido van Rossum and available on all platforms (www.python.org). Python offers a shallow learning curve; it was designed to be easily learnt by children (van Rossum, 1999). As an interpreted language, Python is suitable for rapid prototyping. Python code is exceptionally readable, and it has been praised as “executable pseudocode.” Python is an object-oriented language, but not punitively so, and it is easy to encapsulate data and methods inside Python classes. Finally, Python has an interface to the Tk graphics toolkit (Lundh, 1999), and writing graphical interfaces is straightforward. 3 Design Criteria Several criteria were considered in the design and implementation of the toolkit. These design criteria are listed in the order of their importance. It was also important to decide what goals the toolkit would not attempt to accomplish; we therefore include an explicit set of nonrequirements, which the toolkit is not expected to satisfy. 3.1 Requirements Ease of Use. The primary purpose of the toolkit is to allow students to concentrate on building natural language processing (NLP) systems. The more time students must spe</context>
</contexts>
<marker>Lundh, 1999</marker>
<rawString>Fredrik Lundh. 1999. An introduction to tkinter. http://www.pythonware.com/library/ tkinter/introduction/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Maeda</author>
<author>Steven Bird</author>
<author>Xiaoyi Ma</author>
<author>Haejoong Lee</author>
</authors>
<title>Creating annotation tools with the annotation graph toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation. http://arXiv.org/ abs/cs/0204005.</booktitle>
<contexts>
<context position="23981" citStr="Maeda et al., 2002" startWordPosition="3606" endWordPosition="3609">has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002). Although not originally motivated by pedagogical needs, all of these toolkits have pedagogical applications and many have already been used in teaching. 9 Conclusions and Future Work NLTK provides a simple, extensible, uniform framework for assignments, projects, and class demonstrations. It is well documented, easy to learn, and simple to use. We hope that NLTK will allow computational linguistics classes to include more hands-on experience with using and building NLP components and systems. NLTK is unique in its combination of three factors. First, it was deliberately designed as coursewar</context>
</contexts>
<marker>Maeda, Bird, Ma, Lee, 2002</marker>
<rawString>Kazuaki Maeda, Steven Bird, Xiaoyi Ma, and Haejoong Lee. 2002. Creating annotation tools with the annotation graph toolkit. In Proceedings of the Third International Conference on Language Resources and Evaluation. http://arXiv.org/ abs/cs/0204005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite clause grammars for language analysis – a survey of the formalism and a comparison with augmented transition grammars.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>13--231</pages>
<contexts>
<context position="23238" citStr="Pereira and Warren, 1980" startWordPosition="3499" endWordPosition="3502">fied according to the (original) target audience. Linguistics Students. Various books introduce programming or computing to linguists. These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontchev</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Fernando C. N. Pereira and David H. D. Warren. 1980. Definite clause grammars for language analysis – a survey of the formalism and a comparison with augmented transition grammars. Artificial Intelligence, 13:231–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="23270" citStr="Pollard and Sag, 1994" startWordPosition="3505" endWordPosition="3508">rget audience. Linguistics Students. Various books introduce programming or computing to linguists. These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&amp;D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Pack</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido van Rossum</author>
</authors>
<title>Computer programming for everybody.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Corporation for National Research Initiatives.</institution>
<note>http: //www.python.org/doc/essays/cp4e.html.</note>
<marker>van Rossum, 1999</marker>
<rawString>Guido van Rossum. 1999. Computer programming for everybody. Technical report, Corporation for National Research Initiatives. http: //www.python.org/doc/essays/cp4e.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>