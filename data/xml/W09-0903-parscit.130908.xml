<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.63798">
What’s in a Message?
Stergos D. Afantenos and Nicolas Hernandez
</title>
<bodyText confidence="0.6205765">
LINA, (UMR CNRS 6241)
Université de Nantes, France
stergos.afantenos@univ-nantes.fr
nicolas.hernandez@univ-nantes.fr
</bodyText>
<sectionHeader confidence="0.964079" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999402631578948">
In this paper we present the first step in a larger
series of experiments for the induction of pred-
icate/argument structures. The structures that
we are inducing are very similar to the con-
ceptual structures that are used in Frame Se-
mantics (such as FrameNet). Those structures
are called messages and they were previously
used in the context of a multi-document sum-
marization system of evolving events. The se-
ries of experiments that we are proposing are
essentially composed from two stages. In the
first stage we are trying to extract a represen-
tative vocabulary of words. This vocabulary
is later used in the second stage, during which
we apply to it various clustering approaches in
order to identify the clusters of predicates and
arguments—or frames and semantic roles, to
use the jargon of Frame Semantics. This paper
presents in detail and evaluates the first stage.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955619047619">
Take a sentence, any sentence for that matter; step back
for a while and try to perceive that sentence in its most
abstract form. What you will notice is that once you
try to abstract away sentences, several regularities be-
tween them will start to emerge. To start with, there is
almost always an action that is performed.1 Then, there
is most of the times an agent that is performing this ac-
tion and a patient or a benefactor that is receiving this
action, and it could be the case that this action is per-
formed with the aid of a certain instrument. In other
words, within a sentence—and in respect to its action-
denoting word, or predicate in linguistic terms—there
will be several entities that are associated with the pred-
icate, playing each time a specific semantic role.
The notion of semantic roles can be traced back to
Fillmore’s (1976) theory of Frame Semantics. Accord-
ing to this theory then, a frame is a conceptual structure
which tries to describe a stereotypical situation, event
or object along with its participants and props. Each
frame takes a name (e.g. COMMERCIAL TRANSAC-
TION) and contains a list of Lexical Units (LUs) which
</bodyText>
<footnote confidence="0.9281125">
1In linguistic terms, an action-denoting word is also
known as a predicate.
</footnote>
<bodyText confidence="0.999933333333334">
actually evoke this frame. An LU is nothing else than
a specific word or a specific meaning of a word in the
case of polysemous words. To continue the previous
example, some LUs that evoke the frame of COMMER-
CIAL TRANSACTION could be the verbs buy, sell,
etc. Finally, the frames contain several frame elements
or semantic roles which actually denote the abstract
conceptual entities that are involved with the particu-
lar frame.
Research in semantic roles can be distinguished into
two major branches. The first branch of research con-
sists in defining an ontology of semantic roles, the
frames in which the semantic roles are found as well as
defining the LUs that evoke those frames. The second
branch of research, on the other hand, stipulates the
existence of a set of frames, including semantic roles
and LUs; its goal then, is the creation of an algorithm
that given such a set of frames containing the semantic
roles, will be able to label the appropriate portions of
a sentence with the corresponding semantic roles. This
second branch of research is known as semantic role
labeling.
Most of the research concerning the definition of the
semantic roles has been carried out by linguists who are
manually examining a certain amount of frames before
finally defining the semantic roles and the frames that
contain those semantic roles. Two such projects that
are widely known are the FrameNet (Baker et al., 1998;
Ruppenhofer et al., 2006) and PropBank/NomBank 2
(Palmer et al., 2005; Meyers et al., 2004). Due to the
fact that the aforementioned projects are accompanied
by a large amount of annotated data, computer scien-
tists have started creating algorithms, mostly based on
statistics (Gildea and Jurafsky, 2002; Xue, 2008) in or-
der to automatically label the semantic roles in a sen-
tence. Those algorithms take as input the frame that
</bodyText>
<footnote confidence="0.9976697">
2We would like to note here that although the two ap-
proaches (FrameNet and PropBank/NomBank) share many
common elements, they have several differences as well.
Two major differences, for example, are the fact that the
Linguistic Units (FrameNet) are referred to as Relations
(PropBank/NomBank), and that for the definition of the se-
mantic roles in the case of PropBank/NomBank there is no
reference ontology. A detailed analysis of the differences be-
tween FrameNet and PropBank/NomBank would be out of
the scope of this paper.
</footnote>
<note confidence="0.966616">
Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 18–25,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998598">
18
</page>
<bodyText confidence="0.99997172">
contains the roles as well as the predicate3 of the sen-
tence.
Despite the fact that during the last years we have
seen an increasing interest concerning semantic role
labeling,4 we have not seen many advancements con-
cerning the issue of automatically inducing semantic
roles from raw textual corpora. Such a process of in-
duction would involve, firstly the identification of the
words that would serve as predicates and secondly the
creation of the appropriate clusters of word sequences,
within the limits of a sentence, that behave similarly
in relation to the given predicates. Although those
clusters of word sequences could not actually be said
to serve in themselves as the semantic roles,5 they
can nevertheless be viewed as containing characteris-
tic word sequences of specific semantic roles. The last
point has the implication that if one is looking for a
human intuitive naming of the semantic role that is im-
plied by the cluster then one should look elsewhere.
This is actually reminiscent of the approach that is car-
ried out by PropBank/NomBank in which each seman-
tic role is labeled as Arg1 through Arg5 with the se-
mantics given aside in a human readable natural lan-
guage sentence.
Our goal in this paper is to contribute to the research
problem of frame induction, that is of the creation of
frames, including their associated semantic roles, given
as input only a set of textual documents. More specifi-
cally we propose a general methodology to accomplish
this task, and we test its first stage which includes the
use of corpus statistics for the creation of a subset of
words, from the initial universe of initial words that are
present in the corpus. This subset will later be used
for the identification of the predicates as well as the
semantic roles. Knowing that the problem of frame in-
duction is very difficult in the general case, we limit
ourselves to a specific genre and domain trying to ex-
ploit the characteristics that exist in that domain. The
domain that we have chosen is that of the terroristic in-
cidents which involve hostages. Nevertheless, the same
methodology could be applied to other domains.
The rest of the paper is structured as follows. In sec-
tion 2 we describe the data on which we have applied
our methodology, which itself is described in detail in
section 3. Section 4 describes the actual experiments
that we have performed and the results obtained, while
a discussion of those results follows in section 5. Fi-
nally, section 6 contains a description of the related
work while we present our future work and conclusions
in section 7.
</bodyText>
<footnote confidence="0.999598285714286">
3In the case of FrameNet the predicate corresponds to a
“Linguistic Unit”, while in the case of PropBank/NomBank
it corresponds to what is named “Relation”.
4Cf, for example, the August 2008 issue of the journal
Computational Linguistics (34:2).
5At least as the notion of semantic roles is proposed and
used by FrameNet.
</footnote>
<sectionHeader confidence="0.845726" genericHeader="introduction">
2 The Annotated Data
</sectionHeader>
<bodyText confidence="0.999859038461538">
The annotated data that we have used in order to
perform our experiments come from a previous work
on automatic multi-document summarization of events
that evolve through time (Afantenos et al., 2008; Afan-
tenos et al., 2005; Afantenos et al., 2004). The method-
ology that is followed is based on the identification of
similarities and differences—between documents that
describe the evolution of an event—synchronically as
well as diachronically. In order to do so, the notion of
Synchronic and Diachronic cross document Relations
(SDRs),6 was introduced. SDRs connect not the doc-
uments themselves but some semantic structures that
were called messages. The connection of the messages
with the SDRs resulted in the creation of a semantic
graph that was then fed to a Natural Language Gener-
ation (NLG) system in order to produce the final sum-
mary. Although the notion of messages was originally
inspired by the notion of messages as used in the area of
NLG, for example during the stage of Content Determi-
nation as described in (Reiter and Dale, 1997), and in
general they do follow the spirit of the initial definition
by Reiter &amp; Dale, in the following section we would
like to make it clear what the notion of messages rep-
resents for us. In the rest of the paper, when we refer to
the notion of messages, it will be in the context of the
discussion that follows.
</bodyText>
<subsectionHeader confidence="0.95833">
2.1 Messages
</subsectionHeader>
<bodyText confidence="0.9805143">
The intuition behind messages, is the fact that during
the evolution of an event we have several activities that
take place and each activity is further decomposed into
a series of actions. Messages were created in order to
capture this abstract notion of actions. Of course, ac-
tions usually implicate several entities. In this case, en-
tities were represented with the aid of a domain ontol-
ogy. Thus, in more formal terms a message m can be
defined as follows:
M = message_type (argl, ... , arg.)
where argi E Topic Ontology, i E {1, ... , n}
In order to give a simple example, let us take for in-
stance the case of the hijacking of an airplane by ter-
rorists. In such a case, we are interested in knowing
if the airplane has arrived to its destination, or even to
another place. This action can be captured by a mes-
sage of type arrive whose arguments can be the en-
tity that arrives (the airplane in our case, or a vehicle,
in general) and the location that it arrives. The specifi-
cations of such a message can be expressed as follows:
</bodyText>
<footnote confidence="0.998119888888889">
6Although a full analysis of the notion of Synchronic and
Diachronic Relations is out of the scope of this paper, we
would like simply to mention that the premises on which
those relations are defined are similar to the ones which gov-
ern the notion of Rhetorical Structure Relations in Rhetorical
Structure Theory (RST) (Taboada and Mann, 2006), with the
difference that in the case of SDRs the relations hold across
documents, while in the case of RSTs the relation hold inside
a document.
</footnote>
<page confidence="0.99763">
19
</page>
<figure confidence="0.480815">
arrive (what, place)
what Vehicle
place Location
</figure>
<bodyText confidence="0.990546285714286">
The concepts Vehicle and Location belong to the
ontology of the topic; the concept Airplane is a sub-
concept of the Vehicle. A sentence that might in-
stantiate this message is the following:
The Boeing 747 arrived at the airport of
Stanstend.
The above sentence instantiates the following message:
arrive (&amp;quot;Boeing 747&amp;quot;, &amp;quot;airport of
Stanstend&amp;quot;)
The domain which was chosen was that of terroris-
tic incidents that involve hostages. An empirical study,
by three people, of 163 journalistic articles—written in
Greek—that fell in the above category, resulted in the
definition of 48 different message types that represent
the most important information in the domain. At this
point we would like to stress that what we mean by
“most important information” is the information that
one would normally expect to see in a typical summary
of such kinds of events. Some of the messages that
have been created are shown in Table 1; figure 1 pro-
vides full specifications for two messages.
</bodyText>
<table confidence="0.566592">
free explode
kill kidnap
enter arrest
negotiate encircle
escape_from block_the_way
give_deadline
</table>
<tableCaption confidence="0.989045">
Table 1: Some of the message types defined.
</tableCaption>
<listItem confidence="0.89056125">
negotiate (who, with_whom, about)
who: Person
with_whom : Person
about : Activity
free (who, whom, from)
who: Person
whom: Person
from: Place V Vehicle
</listItem>
<figureCaption confidence="0.99002">
Figure 1: An example of message specifications
</figureCaption>
<bodyText confidence="0.9999728">
Although in an abstract way the notion of messages,
as presented in this paper approaches the notion of
frame semantics—after all, both messages and frame
semantics are concerned with “who did what, to whom,
when, where and how”—it is our hope that our ap-
proach could ultimately be used for the problem of
frame induction. Nevertheless, the two structures have
several points in which they differ. In the following
section we would like to clarify those points in which
the two differ.
</bodyText>
<subsectionHeader confidence="0.993872">
2.2 How Messages differ from Frame Semantics
</subsectionHeader>
<bodyText confidence="0.999984466666667">
As it might have been evident until now, the notions
of messages and frame semantics are quite similar, at
least from an abstract point of view. In practical terms
though, the two notions exhibit several differences.
To start with, the notion of messages has been used
until now only in the context of automatic text summa-
rization of multiple documents. Thus, the aim of mes-
sages is to capture the essential information that one
would expect to see in a typical summary of this do-
main.7 In contrast, semantic roles and the frames in
which they exist do not have this limitation.
Another differentiating characteristic of frame se-
mantics and messages is the fact that semantic roles al-
ways get instantiated within the boundaries of the sen-
tence in which the predicate exists. By contrast, in mes-
sages although in the vast majority of the cases there is
a one-to-one mapping from sentences to messages, in
some of the cases the arguments of a message, which
correspond to the semantic roles, are found in neigh-
boring sentences. The overwhelming majority of those
cases (which in any case were but a few) concern re-
ferring expressions. Due to the nature of the machine
learning experiments that were performed, the actual
entities were annotated as arguments of the messages,
instead of the referring expressions that might exist in
the sentence in which the message’s predicate resided.
A final difference that exists between messages and
frame semantics is the fact that messages were meant
to exist within a certain domain, while the definition of
semantic roles is usually independent of a domain.8
</bodyText>
<sectionHeader confidence="0.979216" genericHeader="method">
3 The Approach Followed
</sectionHeader>
<bodyText confidence="0.997655230769231">
A schematic representation of our approach is shown
in Figure 2. As it can be seen from this figure, our ap-
proach comprises two stages. The first stage concerns
the creation of a lexicon which will contain as most as
possible—and, of course, as accurately as possible—
candidates that are characteristic either of the predi-
cates (message types) or of the semantic roles (argu-
ments of the messages). This stage can be thought of
as a filtering stage. The second stage involves the use
of unsupervised clustering techniques in order to create
the final clusters of words that are characteristic either
of the predicates or of the semantic roles that are asso-
7In this sense then, the notion of messages is reminiscent
of Schank &amp; Abelson’s (1977) notion of scripts, with the dif-
ference that messages are not meant to exist inside a struc-
ture similar to Schank &amp; Abelson’s “scenario”. We would
like also to note that the notion of messages shares certain
similarities with the notion of templates of Information Ex-
traction, as those structures are used in conferences such as
MUC. Incidentally, it is not by chance that the “M” in MUC
stands for Message (Understanding Conference).
8We would like to note at this point that this does not ex-
clude of course the fact that the notion of messages could be
used in a more general, domain independent way. Neverthe-
less, the notion of messages has for the moment been applied
in two specific domains (Afantenos et al., 2008).
</bodyText>
<page confidence="0.96791">
20
</page>
<bodyText confidence="0.999928266666667">
ciated with those predicates. The focus of this paper is
on the first stage.
As we have said, our aim in this paper is the use
of statistical measures in order to extract from a given
corpus a set of words that are most characteristic of
the messages that exist in this corpus. In the context
of this paper, a word will be considered as being char-
acteristic of a message if this word is employed in a
sentence that has been annotated with that message. If
a particular word does not appear in any message an-
notated sentence, then this word will not be considered
as being characteristic of this message. In more formal
terms then, we can define our task as follows. If by U
we designate the set of all the words that exist in our
corpus, then we are looking for a set M such that:
</bodyText>
<equation confidence="0.834651">
MCU n
w E M #� m appears at least once
in a message instance (1)
</equation>
<bodyText confidence="0.99159469047619">
In order to extract the set M we have employed the
following four statistical measures:
Collection Frequency: The set that results from the
union of the n% most frequent words that appear
in the corpus.
Document Frequency: The set that results from the
union of the n% most frequent words of each doc-
ument in the corpus.
tf.idf: For each word in the corpus we calculate its
tf.idf. Then we create a set which is the union of
words with the highest n% tf.idf score in each
document.
Inter-document Frequency: A word has inter-docu-
ment frequency n if it appears in at least n docu-
ments in the corpus. The set with inter-document
frequency n is the set that results from the union
of all the words that have inter-document fre-
quency n.
As we have previously said in this paper, our goal is
the exploitation of the characteristic vocabulary that
exists in a specific genre and domain in order to ulti-
mately achieve our goal of message induction, some-
thing which justifies the use of the above statistical
measures. The first three measures are known to be
used in context of Information retrieval to capture top-
ical informations. The latter measure has been pro-
posed by (Hernandez and Grau, 2003) in order to ex-
tract rhetorical indicator phrases from a genre depen-
dant corpus.
In order to calculate the aforementioned statistics,
and create the appropriate set of words, we ignored all
the stop-words. In addition we worked only with the
verbs and nouns. The intuition behind this decision lies
in the fact that the created set will later be used for the
identification of the predicates and the induction of the
semantic roles. As Gildea &amp; Jurafsky (2002)—among
others—have mentioned, predicates, or action denot-
ing words, are mostly represented by verbs or nouns.9
Thus, in this series of experiments we are mostly focus-
ing in the extraction of a set of words that approaches
the set that is obtained by the union of all the verbs and
nouns found in the annotated sentences.
</bodyText>
<sectionHeader confidence="0.998298" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99844075">
The corpus that we have consists of 163 journalistic
articles which describe the evolution of five different
terroristic incidents that involved hostages. The cor-
pus was initially used in the context of training a multi-
document summarization system. Out of the 3,027 sen-
tences that the corpus contains, about one third (1,017
sentences) were annotated with the 48 message types
that were mentioned in section 2.1.
</bodyText>
<table confidence="0.99979">
Number of Documents: 163
Number of Token: 71,888
Number of Sentences: 3,027
Annotated Sentences (messages): 1,017
Distinct Verbs and Nouns in the Corpus: 7,185
Distinct Verbs and Nouns in the Messages: 2,426
</table>
<tableCaption confidence="0.997333">
Table 2: Corpus Statistics.
</tableCaption>
<bodyText confidence="0.999944933333333">
The corpus contained 7,185 distinct verbs and nouns,
which actually constitute the U of the formula (1)
above. Out of those 7,185 distinct verbs and nouns
2,426 appear in the sentences that have been annotated
with the messages. Our goal was to create this set that
approached as much as possible to the set of 2,426 dis-
tinct verbs and nouns that are found in the messages.
Using the four different statistical measures pre-
sented in the previous section, we tried to reconstruct
that set. In order to understand how the statistical mea-
sures behaved, we varied for each one of them the value
of the threshold used. For each statistical measure used,
the threshold represents something different. For the
Collection Frequency measure the threshold represents
the n% most frequent words that appear in the cor-
pus. For the Document Frequency it represents the n%
most frequent words that appear in each document sep-
arately. For tf.idf it represents the words with the high-
est n% tf.idf score in each document. Finally for the
Inter-document Frequency the threshold represents the
verbs and nouns that appear in at least n documents.
Since for the first three measures the threshold repre-
sents a percentage, we varied it from 1 to 100 in order
to study how this measure behaves. For the case of
the Inter-document Frequency, we varied the threshold
from 1 to 73 which represents the maximum number of
documents in which a word appeared.
In order to measure the performance of the statistical
measures employed, we used four different evaluation
measures, often employed in the information retrieval
</bodyText>
<footnote confidence="0.998601">
9In some rare cases predicates can be represented by ad-
jectives as well.
</footnote>
<page confidence="0.997323">
21
</page>
<figure confidence="0.97189325">
Lexicon Extraction
(initial predicate filtering)
Unsupervised Clustering
Clusters of predicates and semantic roles
</figure>
<figureCaption confidence="0.999542">
Figure 2: Two different stages in the process of predicate clustering
</figureCaption>
<bodyText confidence="0.999898704918033">
field. Those measures are the Precision, Recall, F-
measure and Fallout. Precision represents the percent-
age of the correctly obtained verbs and nouns over the
total number of obtained verbs and nouns. Recall rep-
resents the percentage of the obtained verbs and nouns
over the target set of verbs and nouns. The F-measure
is the harmonic mean of Precision and Recall. Finally,
fallout represents the number of verbs and nouns that
were wrongly classified by the statistical measures as
belonging to a message, over the total number of verbs
and nouns that do not belong to a message. In an ideal
situation one expects a very high precision and recall
(and by consequence F-measure) and a very low Fall-
out.
The obtained graphs that combine the evaluation re-
sults for the four statistical measures presented in sec-
tion 3 are shown in Figures 3 through 6. A first remark
that we can make in respect to those graphs is that con-
cerning the collection frequency, document frequency
and tf.idf measures, for small threshold numbers we
have more or less high precision values while the recall
and fallout values are low. This implies that for smaller
threshold values the obtained sets are rather small, in
relation to M (and by consequence to Lf as well). As
the threshold increases we have the opposite situation,
that is the precision falls while the recall and the fall-
out increases, implying that we get much bigger sets of
verbs and nouns.
In terms of absolute numbers now, the best F-
measure is given by the Collection Frequency measure
with a threshold value of 46%. In other words, the
best results—in terms of F-measure—is given by the
union of the 46% most frequent verbs and nouns that
appear in the corpus. For this threshold the Precision
is 54.14%, the Recall is 72.18% and the F-measure is
61,87%. This high F-measure though comes at a cer-
tain cost since the Fallout is at 31.16%. This implies
that although we get a rather satisfying score in terms
of precision and recall, the number of false positives
that we get is rather high in relation to our universe.
As we have earlier said, a motivating factor of this pa-
per is the automatic induction of the structures that we
have called messages; the extracted lexicon of verbs
and messages will later be used by an unsupervised
clustering algorithm in order to create the classes of
words which will correspond to the message types. For
this reason, although we prefer to have an F-measure as
high as possible, we also want to have a fallout measure
as low as possible, so that the number of false positives
will not perturb the clustering algorithm.
If, on the other hand, we examine the relation be-
tween the F-measure and Fallout, we notice that for the
Inter-document Frequency with a threshold value of 4
we obtain a Precision of 71.60%, a recall of 43.86%
and an F-measure of 54.40%. Most importantly though
we get a fallout measure of 8.86% which implies that
the percentage of wrongly classified verbs and nouns
compose a small percentage of the total universe of
verbs and nouns. This combination of high F-measure
and very low Fallout is very important for later stages
during the process of message induction.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9998618">
As we have claimed in the introduction of this paper,
although we have applied our series of experiments in
a single domain, that of terroristic incidents which in-
volve hostages, we believe that the proposed procedure
can be viewed as a “general” one. In the section we
would like to clarify what exactly we mean by this
statement.
In order to proceed, we would like to suggest that
one can view two different kinds of generalization for
the proposed procedure:
</bodyText>
<listItem confidence="0.940006428571429">
1. The proposed procedure is a general one in the
sense that it can be applied in a large corpus of het-
erogeneous documents incorporating various do-
mains and genres, in order to yield “general”, i.e.
domain-independent, frames that can later be used
for any kind of domain.
2. The proposed procedure is a general one in the
sense that it can be used in any kind of domain
without any modifications. In contrast with the
first point, in this case the documents to which
the proposed procedure will be applied ought to
be homogeneous and rather representative of the
domain. The induced frames will not be general
ones, but instead will be domain dependent ones.
</listItem>
<page confidence="0.992513">
22
</page>
<figure confidence="0.999220375">
100.00% Precision
80.00% Recall
60.00% Fmeasure
40.00% Fallout
20.00%
0.00%
3 7 11 15 19 23 27 31 35 39 43 47 51 55 59 63 67 71 75 79 83 87 91 95 99
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
</figure>
<figureCaption confidence="0.992954">
Figure 3: Collection Frequency statistics
</figureCaption>
<figure confidence="0.981074724137931">
100.00% Precision
80.00% Recall
60.00% Fmeasure
40.00% Fallout
20.00%
0.00%
3
3 7 11 15 19 23 27 31 35 39 43 47 51 55 59 63 67 71 75 79 83 87 91 95 99
7
11
15
19
23
27
31
35
39
43
47
51
55
59
63
67
71
75
79
83
87
91
95
99
1
21
41
97
17
37
57
61
77
81
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
5
9
13
25
29
33
45
49
53
65
69
73
85
89
93
</figure>
<figureCaption confidence="0.999964">
Figure 4: Document Frequency statistics
</figureCaption>
<bodyText confidence="0.986837688888889">
Given the above two definitions of generality, we (2001). The algorithm is called (LP)2. The goal of the
Given the above two definitions of generality, we
(2001). The algorithm is called (LP)2. The goal of the
algorithm is to induce several symbolic rules given as
could say that the procedure proposed in this paper
could say that the procedure proposed in this paper algorithm is to induce several symbolic rules given as
input previous SGML tagged information by the user.
falls rather in the second category than in the first
falls rather in the second category than in the first input previous SGML tagged information by the user.
The induced rules will later be applied in new texts in
one. Ignoring for the moment the second stage of the
one. Ignoring for the moment the second stage of the The induced rules will later be applied in new texts in
order to tag it with the appropriate SGML tags. The
procedure—clustering of word sequences characteris-
procedure—clustering of word sequences characteris- order to tag it with the appropriate SGML tags. The
induced rules by (LP)2 fall into two distinct categories.
tic of specific semantic roles—and focusing on the ac- induced rules by (LP)2 fall into two distinct categories.
tic of specific semantic roles—and focusing on the ac-
In the first we have a bottom up procedure which gen-
tual work described in this paper, that is the use of
tual work described in this paper, that is the use of In the first we have a bottom up procedure which gen-
eralizes the tag instances found in the training corpus
statistical methods for the identification of candidate
statistical methods for the identification of candidate eralizes the tag instances found in the training corpus
which uses shallow NLP knowledge. A second set of
predicates, it becomes clear that the use of an hetero-
predicates, it becomes clear that the use of an hetero- which uses shallow NLP knowledge. A second set of
rules is also created which have a corrective character;
geneous, non-balanced corpus is prone to skewing the
geneous, non-balanced corpus is prone to skewing the rules is also created which have a corrective character;
that is, the application of this second set of rules aims
results. By consequence, we believe that the proposed
results. By consequence, we believe that the proposed that is, the application of this second set of rules aims
at correcting several of the mistakes that are performed
procedure is general in the sense that we can use it for
procedure is general in the sense that we can use it for at correcting several of the mistakes that are performed
by the first set of rules.
any kind of domain which is described by an homoge-
any kind of domain which is described by an homoge- by the first set of rules.
neous corpus of documents.
neous corpus of documents. On the other hand several researchers have pioneered
On the other hand several researchers have pioneered
the automatic acquisition of lexical and semantic re-
the automatic acquisition of lexical and semantic re-
sources (such as verb classes). Some approaches are
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="method">
6 Related Work sources (such as verb classes). Some approaches are
6 Related Work
</sectionHeader>
<bodyText confidence="0.950538431818182">
based on Harris’s (1951) distribution hypothesis: syn-
based on Harris’s (1951) distribution hypothesis: syn-
tactic structures with high occurrences can be used for
Teufel and Moens (2002) and Saggion and Lapalme tactic structures with high occurrences can be used for
Teufel and Moens (2002) and Saggion and Lapalme
identifying word clusters with common contexts (Lin
(2002) have shown that templates based on domain identifying word clusters with common contexts (Lin
(2002) have shown that templates based on domain
and Pantel, 2001). Some others perform analysis from
concepts and relations descriptions can be used for the
concepts and relations descriptions can be used for the and Pantel, 2001). Some others perform analysis from
semantic networks (Green et al., 2004). Poibeau and
task of automatic text summarization. The drawback
task of automatic text summarization. The drawback semantic networks (Green et al., 2004). Poibeau and
Dutoit (2002) showed that both can be used in a com-
of their work is that they rely on manual acquisition
of their work is that they rely on manual acquisition Dutoit (2002) showed that both can be used in a com-
plementary way.
of lexical resources and semantic classes’ definition. plementary way.
of lexical resources and semantic classes’ definition.
Consequently, they do not avoid the time-consuming
Consequently, they do not avoid the time-consuming Currently, our approach follows the first trend.
Currently, our approach follows the first trend.
task of elaborating linguistic resources. It is actually
task of elaborating linguistic resources. It is actually Based on Hernandez and Grau (2003; 2004)’s proposal,
Based on Hernandez and Grau (2003; 2004)’s proposal,
for this kind of reason—that is, the laborious manual
for this kind of reason—that is, the laborious manual we aim at explicitly using corpus characteristics such as
we aim at explicitly using corpus characteristics such as
work—that automatic induction of various structures is
work—that automatic induction of various structures is its genre and domain features to reduce the quantity of
its genre and domain features to reduce the quantity of
a recurrent theme in different research areas of Natural
a recurrent theme in different research areas of Natural considered data. In this paper we have explored various
considered data. In this paper we have explored various
Language Processing.
Language Processing. statistical measures which could be used as a filter for
statistical measures which could be used as a filter for
improving results obtained by the previous mentioned
An example of an inductive Information Extraction
An example of an inductive Information Extraction improving results obtained by the previous mentioned
works.
algorithm is the one presented by Fabio Ciravegna
algorithm is the one presented by Fabio Ciravegna works.
</bodyText>
<page confidence="0.989087">
23
</page>
<figure confidence="0.952955346153846">
3
3 7 11 15 19 23 27 31 35 39 43 47 51 55 59 63 67 71 75 79 83 87 91 95 99
7
11
15
19
23
27
31
35
39
43
47
51
55
59
63
67
71
75
79
83
87
91
95
99
1
17
21
37
41
57
61
5
9
13
25
29
33
45
49
53
65
69
73
77
81
85
89
93
97
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
</figure>
<figureCaption confidence="0.983661">
Figure 5: Tf.idf statistics
</figureCaption>
<figure confidence="0.994345760869565">
100.00%
80.00%
60.00%
40.00%
20.00%
0.00%
100.00%
80.00%
60.00%
40.00%
20.00%
0.00%
Precision
Recall
F-measure
Fallout
Precision
Recall
Fmeasure
Fallout
44
42
24
22
14
12
4
2
64
70
68
66
60
58
56
54
52
50
48
46
40
38
36
34
32
30
28
26
20
18
16
10
6
62
47
41
27
21
17
11
1
71
67
69
65
63
59
57
55
53
51
49
45
43
39
37
35
33
31
29
25
23
19
15
13
5
3
61
72
73
8
7 9
</figure>
<figureCaption confidence="0.999524">
Figure 6: Inter-document frequency statistics
</figureCaption>
<sectionHeader confidence="0.992679" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999603111111111">
In this paper we have presented a statistical approach
for the extraction of a lexicon which contains the verbs
and nouns that can be considered as candidates for use
as predicates for the induction of predicate/argument
structures that we call messages. Actually, the research
presented here can be considered as the first step in a
two-stages approach. The next step involves the use
of clustering algorithms on the extracted lexicon which
will provide the final clusters that will contain the pred-
icates and arguments for the messages. This process
is itself part of a larger process for the induction of
predicate/argument structures. Apart from messages,
such structures could as well be the structures that are
associated with frame semantics, that is the frames
and their associated semantic roles. Despite the great
resemblances that messages and frames have, one of
their great differences is the fact that messages were
firstly introduced in the context of automatic multi-
document summarization. By consequence they are
meant to capture the most important information in a
domain. Frames and semantic roles on the other hand,
do not have this restriction and thus are more general.
Nonetheless, it is our hope that the current research
could ultimately be useful for the induction of frame se-
mantics. In fact it is in our plans for the immediate fu-
ture work to apply the same procedure in FrameNet an-
notated data10 in order to extract a vocabulary of verbs
</bodyText>
<footnote confidence="0.893134">
10See http://framenet.icsi.berkeley.edu/
index.php?option=com_wrapper&amp;Itemid=84
</footnote>
<bodyText confidence="0.999724870967742">
and nouns which will be characteristic of the different
Linguistic Units (LUs) for the frames of FrameNet.
The proposed statistical measures are meant to be a
first step towards a fully automated process of mes-
sage induction. The immediate next step in the pro-
cess involves the application of various unsupervised
clustering techniques on the obtained lexicon in order
to create the 48 different classes each one of which
will represent a distinct vocabulary for the 48 differ-
ent message types. We are currently experimenting
with several algorithms such K-means, Expectation-
Minimization (EM), Cobweb and Farthest First. In ad-
dition to those clustering algorithms, we are also exam-
ining the use of various lexical association measures
such as Mutual Information, Dice coefficient, x2, etc.
Although this approach will provide us with clusters of
predicates and candidate arguments, still the problem
of linking the predicates with their arguments remains.
Undoubtedly, the use of more linguistically oriented
techniques, such as syntactic analysis, is inevitable. We
are currently experimenting with the use of a shallow
parser (chunker) in order to identify the chunks that
behave similarly in respect to a given cluster of pred-
icates.
Concerning the evaluation of our approach, the high-
est F-measure score (61,87%) was given by the Col-
lection Frequency statistical measure with a threshold
value of 46%. This high F-measure though came at the
cost of a high Fallout score (31.16%). Since the ex-
tracted lexicon will later be used as an input to a clus-
tering algorithm, we would like to minimize as much as
</bodyText>
<page confidence="0.994397">
24
</page>
<bodyText confidence="0.99981725">
possible the false positives. By consequence we have
opted in using the Inter-document Frequency measure
which presents an F-measure of 54.40% and a much
more limited Fallout of 8.86%.
</bodyText>
<sectionHeader confidence="0.987281" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99896825">
The authors would like to thank Konstantina Liontou and
Maria Salapata for their help on the annotation of the mes-
sages, as well as the anonymous reviewers for their insightful
and constructive comments.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99962511">
Stergos D. Afantenos, Irene Doura, Eleni Kapel-
lou, and Vangelis Karkaletsis. 2004. Exploit-
ing cross-document relations for multi-document
evolving summarization. In G. A. Vouros and
T. Panayiotopoulos, editors, Methods and Applica-
tions of Artificial Intelligence: Third Hellenic Con-
ference on AI, SETN 2004, volume 3025 of Lecture
Notes in Computer Science, pages 410–419, Samos,
Greece, May. Springer-Verlag Heidelberg.
Stergos D. Afantenos, Konstantina Liontou, Maria
Salapata, and Vangelis Karkaletsis. 2005. An in-
troduction to the summarization of evolving events:
Linear and non-linear evolution. In Bernadette
Sharp, editor, Proceedings of the 2nd International
Workshop on Natural Language Understanding and
Cognitive Science, NLUCS 2005, pages 91–99, Ma-
iami, Florida, USA, May. INSTICC Press.
Stergos D. Afantenos, Vangelis Karkaletsis, Panagio-
tis Stamatopoulos, and Constantin Halatsis. 2008.
Using synchronic and diachronic relations for sum-
marizing multiple documents describing evolving
events. Journal of Intelligent Information Systems,
30(3):183–226, June.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the COLING-ACL, Montreal, Canada.
Fabio Ciravegna. 2001. Adaptive information extrac-
tion from text by rule induction and generalisation.
In 17th International Joint Conference on Artificial
Intelligence (IJCAI2001), pages 1251–1256, Seat-
tle, USA, August.
C. J. Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences: Conference on the Origin and Develop-
ment of Language and Speech, 280:20–32.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing frame semantic verb classes from
wordnet and ldoce. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics (ACL’04), Main Volume, pages 375–382,
Barcelona, Spain, July.
Zelig Harris. 1951. Structural Linguistics. University
of Chicago Press.
Nicolas Hernandez and Brigitte Grau. 2003. Auto-
matic extraction of meta-descriptors for text descrip-
tion. In International Conference on Recent Ad-
vances In Natural Language Processing (RANLP),
Borovets, Bulgaria, 10-12 September.
Nicolas Hernandez. 2004. Détection et Description
Automatique de Structures de Texte. Ph.D. thesis,
Université de Paris-Sud XI.
Dekang Lin and Patrick Pantel. 2001. Induction of
semantic classes from natural language text. In Pro-
ceedings of ACM Conference on Knowledge Discov-
ery and Data Mining (KDD-01), pages 317–322, San
Francisco, CA.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Adam Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24–31, Boston, Massachusetts, USA,
May. Association for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Thierry Poibeau and Dominique Dutoit. 2002. In-
ferring knowledge from a large semantic network.
In Proceeding of the Semantic networks workshop,
during the Computational Linguistics Conference
(COLING 2002), Taipei, Taiwan.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Natural Lan-
guage Engineering, 3(1):57–87.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. Framenet ii: Extended theory and
practice. Unpublished manuscript; accessible at
http://framenet.icsi.berkeley.edu.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with sumum.
Computational Linguistics, 28(4):497–526.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: an Inquiry into
Human Knowledge Structures. L. Erlbaum, Hills-
dale, NJ.
Maite Taboada and William C. Mann. 2006. Rhetor-
ical structure theory: Looking back and moving
ahead. Discourse Studies, 8(3):423–459, June.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28:409–445.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225–255, June.
</reference>
<page confidence="0.998735">
25
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527437">
<title confidence="0.997253">What’s in a Message?</title>
<author confidence="0.984814">D Afantenos</author>
<affiliation confidence="0.771982">LINA, (UMR CNRS Université de Nantes, France</affiliation>
<email confidence="0.989993">nicolas.hernandez@univ-nantes.fr</email>
<abstract confidence="0.99887375">In this paper we present the first step in a larger series of experiments for the induction of predicate/argument structures. The structures that we are inducing are very similar to the conceptual structures that are used in Frame Semantics (such as FrameNet). Those structures are called messages and they were previously used in the context of a multi-document summarization system of evolving events. The series of experiments that we are proposing are essentially composed from two stages. In the first stage we are trying to extract a representative vocabulary of words. This vocabulary is later used in the second stage, during which we apply to it various clustering approaches in order to identify the clusters of predicates and arguments—or frames and semantic roles, to use the jargon of Frame Semantics. This paper presents in detail and evaluates the first stage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Stergos D Afantenos</author>
<author>Irene Doura</author>
<author>Eleni Kapellou</author>
<author>Vangelis Karkaletsis</author>
</authors>
<title>Exploiting cross-document relations for multi-document evolving summarization.</title>
<date>2004</date>
<booktitle>Methods and Applications of Artificial Intelligence: Third Hellenic Conference on AI, SETN 2004,</booktitle>
<volume>3025</volume>
<pages>410--419</pages>
<editor>In G. A. Vouros and T. Panayiotopoulos, editors,</editor>
<publisher>Springer-Verlag</publisher>
<location>Samos, Greece,</location>
<contexts>
<context position="8039" citStr="Afantenos et al., 2004" startWordPosition="1335" endWordPosition="1338">k and conclusions in section 7. 3In the case of FrameNet the predicate corresponds to a “Linguistic Unit”, while in the case of PropBank/NomBank it corresponds to what is named “Relation”. 4Cf, for example, the August 2008 issue of the journal Computational Linguistics (34:2). 5At least as the notion of semantic roles is proposed and used by FrameNet. 2 The Annotated Data The annotated data that we have used in order to perform our experiments come from a previous work on automatic multi-document summarization of events that evolve through time (Afantenos et al., 2008; Afantenos et al., 2005; Afantenos et al., 2004). The methodology that is followed is based on the identification of similarities and differences—between documents that describe the evolution of an event—synchronically as well as diachronically. In order to do so, the notion of Synchronic and Diachronic cross document Relations (SDRs),6 was introduced. SDRs connect not the documents themselves but some semantic structures that were called messages. The connection of the messages with the SDRs resulted in the creation of a semantic graph that was then fed to a Natural Language Generation (NLG) system in order to produce the final summary. Al</context>
</contexts>
<marker>Afantenos, Doura, Kapellou, Karkaletsis, 2004</marker>
<rawString>Stergos D. Afantenos, Irene Doura, Eleni Kapellou, and Vangelis Karkaletsis. 2004. Exploiting cross-document relations for multi-document evolving summarization. In G. A. Vouros and T. Panayiotopoulos, editors, Methods and Applications of Artificial Intelligence: Third Hellenic Conference on AI, SETN 2004, volume 3025 of Lecture Notes in Computer Science, pages 410–419, Samos, Greece, May. Springer-Verlag Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stergos D Afantenos</author>
<author>Konstantina Liontou</author>
<author>Maria Salapata</author>
<author>Vangelis Karkaletsis</author>
</authors>
<title>An introduction to the summarization of evolving events: Linear and non-linear evolution.</title>
<date>2005</date>
<booktitle>Proceedings of the 2nd International Workshop on Natural Language Understanding and Cognitive Science, NLUCS 2005,</booktitle>
<pages>91--99</pages>
<editor>In Bernadette Sharp, editor,</editor>
<publisher>INSTICC Press.</publisher>
<location>Maiami, Florida, USA,</location>
<contexts>
<context position="8014" citStr="Afantenos et al., 2005" startWordPosition="1330" endWordPosition="1334">e present our future work and conclusions in section 7. 3In the case of FrameNet the predicate corresponds to a “Linguistic Unit”, while in the case of PropBank/NomBank it corresponds to what is named “Relation”. 4Cf, for example, the August 2008 issue of the journal Computational Linguistics (34:2). 5At least as the notion of semantic roles is proposed and used by FrameNet. 2 The Annotated Data The annotated data that we have used in order to perform our experiments come from a previous work on automatic multi-document summarization of events that evolve through time (Afantenos et al., 2008; Afantenos et al., 2005; Afantenos et al., 2004). The methodology that is followed is based on the identification of similarities and differences—between documents that describe the evolution of an event—synchronically as well as diachronically. In order to do so, the notion of Synchronic and Diachronic cross document Relations (SDRs),6 was introduced. SDRs connect not the documents themselves but some semantic structures that were called messages. The connection of the messages with the SDRs resulted in the creation of a semantic graph that was then fed to a Natural Language Generation (NLG) system in order to prod</context>
</contexts>
<marker>Afantenos, Liontou, Salapata, Karkaletsis, 2005</marker>
<rawString>Stergos D. Afantenos, Konstantina Liontou, Maria Salapata, and Vangelis Karkaletsis. 2005. An introduction to the summarization of evolving events: Linear and non-linear evolution. In Bernadette Sharp, editor, Proceedings of the 2nd International Workshop on Natural Language Understanding and Cognitive Science, NLUCS 2005, pages 91–99, Maiami, Florida, USA, May. INSTICC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stergos D Afantenos</author>
<author>Vangelis Karkaletsis</author>
<author>Panagiotis Stamatopoulos</author>
<author>Constantin Halatsis</author>
</authors>
<title>Using synchronic and diachronic relations for summarizing multiple documents describing evolving events.</title>
<date>2008</date>
<journal>Journal of Intelligent Information Systems,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="7990" citStr="Afantenos et al., 2008" startWordPosition="1326" endWordPosition="1329">the related work while we present our future work and conclusions in section 7. 3In the case of FrameNet the predicate corresponds to a “Linguistic Unit”, while in the case of PropBank/NomBank it corresponds to what is named “Relation”. 4Cf, for example, the August 2008 issue of the journal Computational Linguistics (34:2). 5At least as the notion of semantic roles is proposed and used by FrameNet. 2 The Annotated Data The annotated data that we have used in order to perform our experiments come from a previous work on automatic multi-document summarization of events that evolve through time (Afantenos et al., 2008; Afantenos et al., 2005; Afantenos et al., 2004). The methodology that is followed is based on the identification of similarities and differences—between documents that describe the evolution of an event—synchronically as well as diachronically. In order to do so, the notion of Synchronic and Diachronic cross document Relations (SDRs),6 was introduced. SDRs connect not the documents themselves but some semantic structures that were called messages. The connection of the messages with the SDRs resulted in the creation of a semantic graph that was then fed to a Natural Language Generation (NLG)</context>
<context position="15677" citStr="Afantenos et al., 2008" startWordPosition="2640" endWordPosition="2643">imilar to Schank &amp; Abelson’s “scenario”. We would like also to note that the notion of messages shares certain similarities with the notion of templates of Information Extraction, as those structures are used in conferences such as MUC. Incidentally, it is not by chance that the “M” in MUC stands for Message (Understanding Conference). 8We would like to note at this point that this does not exclude of course the fact that the notion of messages could be used in a more general, domain independent way. Nevertheless, the notion of messages has for the moment been applied in two specific domains (Afantenos et al., 2008). 20 ciated with those predicates. The focus of this paper is on the first stage. As we have said, our aim in this paper is the use of statistical measures in order to extract from a given corpus a set of words that are most characteristic of the messages that exist in this corpus. In the context of this paper, a word will be considered as being characteristic of a message if this word is employed in a sentence that has been annotated with that message. If a particular word does not appear in any message annotated sentence, then this word will not be considered as being characteristic of this </context>
</contexts>
<marker>Afantenos, Karkaletsis, Stamatopoulos, Halatsis, 2008</marker>
<rawString>Stergos D. Afantenos, Vangelis Karkaletsis, Panagiotis Stamatopoulos, and Constantin Halatsis. 2008. Using synchronic and diachronic relations for summarizing multiple documents describing evolving events. Journal of Intelligent Information Systems, 30(3):183–226, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3720" citStr="Baker et al., 1998" startWordPosition="618" endWordPosition="621">roles and LUs; its goal then, is the creation of an algorithm that given such a set of frames containing the semantic roles, will be able to label the appropriate portions of a sentence with the corresponding semantic roles. This second branch of research is known as semantic role labeling. Most of the research concerning the definition of the semantic roles has been carried out by linguists who are manually examining a certain amount of frames before finally defining the semantic roles and the frames that contain those semantic roles. Two such projects that are widely known are the FrameNet (Baker et al., 1998; Ruppenhofer et al., 2006) and PropBank/NomBank 2 (Palmer et al., 2005; Meyers et al., 2004). Due to the fact that the aforementioned projects are accompanied by a large amount of annotated data, computer scientists have started creating algorithms, mostly based on statistics (Gildea and Jurafsky, 2002; Xue, 2008) in order to automatically label the semantic roles in a sentence. Those algorithms take as input the frame that 2We would like to note here that although the two approaches (FrameNet and PropBank/NomBank) share many common elements, they have several differences as well. Two major d</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Ciravegna</author>
</authors>
<title>Adaptive information extraction from text by rule induction and generalisation.</title>
<date>2001</date>
<booktitle>In 17th International Joint Conference on Artificial Intelligence (IJCAI2001),</booktitle>
<pages>1251--1256</pages>
<location>Seattle, USA,</location>
<marker>Ciravegna, 2001</marker>
<rawString>Fabio Ciravegna. 2001. Adaptive information extraction from text by rule induction and generalisation. In 17th International Joint Conference on Artificial Intelligence (IJCAI2001), pages 1251–1256, Seattle, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language. Annals of the New York Academy</title>
<date>1976</date>
<booktitle>of Sciences: Conference on the Origin and Development of Language and Speech,</booktitle>
<pages>280--20</pages>
<marker>Fillmore, 1976</marker>
<rawString>C. J. Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech, 280:20–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="4024" citStr="Gildea and Jurafsky, 2002" startWordPosition="665" endWordPosition="668">of the research concerning the definition of the semantic roles has been carried out by linguists who are manually examining a certain amount of frames before finally defining the semantic roles and the frames that contain those semantic roles. Two such projects that are widely known are the FrameNet (Baker et al., 1998; Ruppenhofer et al., 2006) and PropBank/NomBank 2 (Palmer et al., 2005; Meyers et al., 2004). Due to the fact that the aforementioned projects are accompanied by a large amount of annotated data, computer scientists have started creating algorithms, mostly based on statistics (Gildea and Jurafsky, 2002; Xue, 2008) in order to automatically label the semantic roles in a sentence. Those algorithms take as input the frame that 2We would like to note here that although the two approaches (FrameNet and PropBank/NomBank) share many common elements, they have several differences as well. Two major differences, for example, are the fact that the Linguistic Units (FrameNet) are referred to as Relations (PropBank/NomBank), and that for the definition of the semantic roles in the case of PropBank/NomBank there is no reference ontology. A detailed analysis of the differences between FrameNet and PropBa</context>
<context position="18180" citStr="Gildea &amp; Jurafsky (2002)" startWordPosition="3096" endWordPosition="3099">easures are known to be used in context of Information retrieval to capture topical informations. The latter measure has been proposed by (Hernandez and Grau, 2003) in order to extract rhetorical indicator phrases from a genre dependant corpus. In order to calculate the aforementioned statistics, and create the appropriate set of words, we ignored all the stop-words. In addition we worked only with the verbs and nouns. The intuition behind this decision lies in the fact that the created set will later be used for the identification of the predicates and the induction of the semantic roles. As Gildea &amp; Jurafsky (2002)—among others—have mentioned, predicates, or action denoting words, are mostly represented by verbs or nouns.9 Thus, in this series of experiments we are mostly focusing in the extraction of a set of words that approaches the set that is obtained by the union of all the verbs and nouns found in the annotated sentences. 4 Experiments and Results The corpus that we have consists of 163 journalistic articles which describe the evolution of five different terroristic incidents that involved hostages. The corpus was initially used in the context of training a multidocument summarization system. Out</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Green</author>
<author>Bonnie J Dorr</author>
<author>Philip Resnik</author>
</authors>
<title>Inducing frame semantic verb classes from wordnet and ldoce.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>375--382</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="29941" citStr="Green et al., 2004" startWordPosition="5148" endWordPosition="5151">used for Teufel and Moens (2002) and Saggion and Lapalme tactic structures with high occurrences can be used for Teufel and Moens (2002) and Saggion and Lapalme identifying word clusters with common contexts (Lin (2002) have shown that templates based on domain identifying word clusters with common contexts (Lin (2002) have shown that templates based on domain and Pantel, 2001). Some others perform analysis from concepts and relations descriptions can be used for the concepts and relations descriptions can be used for the and Pantel, 2001). Some others perform analysis from semantic networks (Green et al., 2004). Poibeau and task of automatic text summarization. The drawback task of automatic text summarization. The drawback semantic networks (Green et al., 2004). Poibeau and Dutoit (2002) showed that both can be used in a comof their work is that they rely on manual acquisition of their work is that they rely on manual acquisition Dutoit (2002) showed that both can be used in a complementary way. of lexical resources and semantic classes’ definition. plementary way. of lexical resources and semantic classes’ definition. Consequently, they do not avoid the time-consuming Consequently, they do not avo</context>
</contexts>
<marker>Green, Dorr, Resnik, 2004</marker>
<rawString>Rebecca Green, Bonnie J. Dorr, and Philip Resnik. 2004. Inducing frame semantic verb classes from wordnet and ldoce. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 375–382, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig Harris</author>
</authors>
<title>Structural Linguistics.</title>
<date>1951</date>
<publisher>University of Chicago Press.</publisher>
<marker>Harris, 1951</marker>
<rawString>Zelig Harris. 1951. Structural Linguistics. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Hernandez</author>
<author>Brigitte Grau</author>
</authors>
<title>Automatic extraction of meta-descriptors for text description.</title>
<date>2003</date>
<booktitle>In International Conference on Recent Advances In Natural Language Processing (RANLP), Borovets,</booktitle>
<contexts>
<context position="17720" citStr="Hernandez and Grau, 2003" startWordPosition="3018" endWordPosition="3021">east n documents in the corpus. The set with inter-document frequency n is the set that results from the union of all the words that have inter-document frequency n. As we have previously said in this paper, our goal is the exploitation of the characteristic vocabulary that exists in a specific genre and domain in order to ultimately achieve our goal of message induction, something which justifies the use of the above statistical measures. The first three measures are known to be used in context of Information retrieval to capture topical informations. The latter measure has been proposed by (Hernandez and Grau, 2003) in order to extract rhetorical indicator phrases from a genre dependant corpus. In order to calculate the aforementioned statistics, and create the appropriate set of words, we ignored all the stop-words. In addition we worked only with the verbs and nouns. The intuition behind this decision lies in the fact that the created set will later be used for the identification of the predicates and the induction of the semantic roles. As Gildea &amp; Jurafsky (2002)—among others—have mentioned, predicates, or action denoting words, are mostly represented by verbs or nouns.9 Thus, in this series of exper</context>
<context position="30808" citStr="Hernandez and Grau (2003" startWordPosition="5282" endWordPosition="5285">ey rely on manual acquisition of their work is that they rely on manual acquisition Dutoit (2002) showed that both can be used in a complementary way. of lexical resources and semantic classes’ definition. plementary way. of lexical resources and semantic classes’ definition. Consequently, they do not avoid the time-consuming Consequently, they do not avoid the time-consuming Currently, our approach follows the first trend. Currently, our approach follows the first trend. task of elaborating linguistic resources. It is actually task of elaborating linguistic resources. It is actually Based on Hernandez and Grau (2003; 2004)’s proposal, Based on Hernandez and Grau (2003; 2004)’s proposal, for this kind of reason—that is, the laborious manual for this kind of reason—that is, the laborious manual we aim at explicitly using corpus characteristics such as we aim at explicitly using corpus characteristics such as work—that automatic induction of various structures is work—that automatic induction of various structures is its genre and domain features to reduce the quantity of its genre and domain features to reduce the quantity of a recurrent theme in different research areas of Natural a recurrent theme in dif</context>
</contexts>
<marker>Hernandez, Grau, 2003</marker>
<rawString>Nicolas Hernandez and Brigitte Grau. 2003. Automatic extraction of meta-descriptors for text description. In International Conference on Recent Advances In Natural Language Processing (RANLP), Borovets, Bulgaria, 10-12 September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Hernandez</author>
</authors>
<title>Détection et Description Automatique de Structures de Texte.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Université de Paris-Sud XI.</institution>
<marker>Hernandez, 2004</marker>
<rawString>Nicolas Hernandez. 2004. Détection et Description Automatique de Structures de Texte. Ph.D. thesis, Université de Paris-Sud XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Induction of semantic classes from natural language text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM Conference on Knowledge Discovery and Data Mining (KDD-01),</booktitle>
<pages>317--322</pages>
<location>San Francisco, CA.</location>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Induction of semantic classes from natural language text. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining (KDD-01), pages 317–322, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>HLTNAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<editor>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts, USA,</location>
<marker>Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In Adam Meyers, editor, HLTNAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts, USA, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="3791" citStr="Palmer et al., 2005" startWordPosition="629" endWordPosition="632">en such a set of frames containing the semantic roles, will be able to label the appropriate portions of a sentence with the corresponding semantic roles. This second branch of research is known as semantic role labeling. Most of the research concerning the definition of the semantic roles has been carried out by linguists who are manually examining a certain amount of frames before finally defining the semantic roles and the frames that contain those semantic roles. Two such projects that are widely known are the FrameNet (Baker et al., 1998; Ruppenhofer et al., 2006) and PropBank/NomBank 2 (Palmer et al., 2005; Meyers et al., 2004). Due to the fact that the aforementioned projects are accompanied by a large amount of annotated data, computer scientists have started creating algorithms, mostly based on statistics (Gildea and Jurafsky, 2002; Xue, 2008) in order to automatically label the semantic roles in a sentence. Those algorithms take as input the frame that 2We would like to note here that although the two approaches (FrameNet and PropBank/NomBank) share many common elements, they have several differences as well. Two major differences, for example, are the fact that the Linguistic Units (FrameN</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Poibeau</author>
<author>Dominique Dutoit</author>
</authors>
<title>Inferring knowledge from a large semantic network.</title>
<date>2002</date>
<booktitle>In Proceeding of the Semantic networks workshop, during the Computational Linguistics Conference (COLING</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="30122" citStr="Poibeau and Dutoit (2002)" startWordPosition="5174" endWordPosition="5177">rd clusters with common contexts (Lin (2002) have shown that templates based on domain identifying word clusters with common contexts (Lin (2002) have shown that templates based on domain and Pantel, 2001). Some others perform analysis from concepts and relations descriptions can be used for the concepts and relations descriptions can be used for the and Pantel, 2001). Some others perform analysis from semantic networks (Green et al., 2004). Poibeau and task of automatic text summarization. The drawback task of automatic text summarization. The drawback semantic networks (Green et al., 2004). Poibeau and Dutoit (2002) showed that both can be used in a comof their work is that they rely on manual acquisition of their work is that they rely on manual acquisition Dutoit (2002) showed that both can be used in a complementary way. of lexical resources and semantic classes’ definition. plementary way. of lexical resources and semantic classes’ definition. Consequently, they do not avoid the time-consuming Consequently, they do not avoid the time-consuming Currently, our approach follows the first trend. Currently, our approach follows the first trend. task of elaborating linguistic resources. It is actually task</context>
</contexts>
<marker>Poibeau, Dutoit, 2002</marker>
<rawString>Thierry Poibeau and Dominique Dutoit. 2002. Inferring knowledge from a large semantic network. In Proceeding of the Semantic networks workshop, during the Computational Linguistics Conference (COLING 2002), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="8840" citStr="Reiter and Dale, 1997" startWordPosition="1466" endWordPosition="1469">as diachronically. In order to do so, the notion of Synchronic and Diachronic cross document Relations (SDRs),6 was introduced. SDRs connect not the documents themselves but some semantic structures that were called messages. The connection of the messages with the SDRs resulted in the creation of a semantic graph that was then fed to a Natural Language Generation (NLG) system in order to produce the final summary. Although the notion of messages was originally inspired by the notion of messages as used in the area of NLG, for example during the stage of Content Determination as described in (Reiter and Dale, 1997), and in general they do follow the spirit of the initial definition by Reiter &amp; Dale, in the following section we would like to make it clear what the notion of messages represents for us. In the rest of the paper, when we refer to the notion of messages, it will be in the context of the discussion that follows. 2.1 Messages The intuition behind messages, is the fact that during the evolution of an event we have several activities that take place and each activity is further decomposed into a series of actions. Messages were created in order to capture this abstract notion of actions. Of cour</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. Natural Language Engineering, 3(1):57–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>Framenet ii: Extended theory and practice. Unpublished manuscript; accessible at http://framenet.icsi.berkeley.edu.</title>
<date>2006</date>
<contexts>
<context position="3747" citStr="Ruppenhofer et al., 2006" startWordPosition="622" endWordPosition="625">oal then, is the creation of an algorithm that given such a set of frames containing the semantic roles, will be able to label the appropriate portions of a sentence with the corresponding semantic roles. This second branch of research is known as semantic role labeling. Most of the research concerning the definition of the semantic roles has been carried out by linguists who are manually examining a certain amount of frames before finally defining the semantic roles and the frames that contain those semantic roles. Two such projects that are widely known are the FrameNet (Baker et al., 1998; Ruppenhofer et al., 2006) and PropBank/NomBank 2 (Palmer et al., 2005; Meyers et al., 2004). Due to the fact that the aforementioned projects are accompanied by a large amount of annotated data, computer scientists have started creating algorithms, mostly based on statistics (Gildea and Jurafsky, 2002; Xue, 2008) in order to automatically label the semantic roles in a sentence. Those algorithms take as input the frame that 2We would like to note here that although the two approaches (FrameNet and PropBank/NomBank) share many common elements, they have several differences as well. Two major differences, for example, ar</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2006. Framenet ii: Extended theory and practice. Unpublished manuscript; accessible at http://framenet.icsi.berkeley.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Guy Lapalme</author>
</authors>
<title>Generating indicative-informative summaries with sumum.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>Horacio Saggion and Guy Lapalme. 2002. Generating indicative-informative summaries with sumum. Computational Linguistics, 28(4):497–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures.</title>
<date>1977</date>
<journal>L. Erlbaum,</journal>
<location>Hillsdale, NJ.</location>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures. L. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>William C Mann</author>
</authors>
<title>Rhetorical structure theory: Looking back and moving ahead.</title>
<date>2006</date>
<journal>Discourse Studies,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="10546" citStr="Taboada and Mann, 2006" startWordPosition="1780" endWordPosition="1783">even to another place. This action can be captured by a message of type arrive whose arguments can be the entity that arrives (the airplane in our case, or a vehicle, in general) and the location that it arrives. The specifications of such a message can be expressed as follows: 6Although a full analysis of the notion of Synchronic and Diachronic Relations is out of the scope of this paper, we would like simply to mention that the premises on which those relations are defined are similar to the ones which govern the notion of Rhetorical Structure Relations in Rhetorical Structure Theory (RST) (Taboada and Mann, 2006), with the difference that in the case of SDRs the relations hold across documents, while in the case of RSTs the relation hold inside a document. 19 arrive (what, place) what Vehicle place Location The concepts Vehicle and Location belong to the ontology of the topic; the concept Airplane is a subconcept of the Vehicle. A sentence that might instantiate this message is the following: The Boeing 747 arrived at the airport of Stanstend. The above sentence instantiates the following message: arrive (&amp;quot;Boeing 747&amp;quot;, &amp;quot;airport of Stanstend&amp;quot;) The domain which was chosen was that of terroristic inciden</context>
</contexts>
<marker>Taboada, Mann, 2006</marker>
<rawString>Maite Taboada and William C. Mann. 2006. Rhetorical structure theory: Looking back and moving ahead. Discourse Studies, 8(3):423–459, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--409</pages>
<contexts>
<context position="29354" citStr="Teufel and Moens (2002)" startWordPosition="5057" endWordPosition="5060">y an homoge- by the first set of rules. neous corpus of documents. neous corpus of documents. On the other hand several researchers have pioneered On the other hand several researchers have pioneered the automatic acquisition of lexical and semantic rethe automatic acquisition of lexical and semantic resources (such as verb classes). Some approaches are 6 Related Work sources (such as verb classes). Some approaches are 6 Related Work based on Harris’s (1951) distribution hypothesis: synbased on Harris’s (1951) distribution hypothesis: syntactic structures with high occurrences can be used for Teufel and Moens (2002) and Saggion and Lapalme tactic structures with high occurrences can be used for Teufel and Moens (2002) and Saggion and Lapalme identifying word clusters with common contexts (Lin (2002) have shown that templates based on domain identifying word clusters with common contexts (Lin (2002) have shown that templates based on domain and Pantel, 2001). Some others perform analysis from concepts and relations descriptions can be used for the concepts and relations descriptions can be used for the and Pantel, 2001). Some others perform analysis from semantic networks (Green et al., 2004). Poibeau and</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28:409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling chinese predicates with semantic roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="4036" citStr="Xue, 2008" startWordPosition="669" endWordPosition="670">the definition of the semantic roles has been carried out by linguists who are manually examining a certain amount of frames before finally defining the semantic roles and the frames that contain those semantic roles. Two such projects that are widely known are the FrameNet (Baker et al., 1998; Ruppenhofer et al., 2006) and PropBank/NomBank 2 (Palmer et al., 2005; Meyers et al., 2004). Due to the fact that the aforementioned projects are accompanied by a large amount of annotated data, computer scientists have started creating algorithms, mostly based on statistics (Gildea and Jurafsky, 2002; Xue, 2008) in order to automatically label the semantic roles in a sentence. Those algorithms take as input the frame that 2We would like to note here that although the two approaches (FrameNet and PropBank/NomBank) share many common elements, they have several differences as well. Two major differences, for example, are the fact that the Linguistic Units (FrameNet) are referred to as Relations (PropBank/NomBank), and that for the definition of the semantic roles in the case of PropBank/NomBank there is no reference ontology. A detailed analysis of the differences between FrameNet and PropBank/NomBank w</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling chinese predicates with semantic roles. Computational Linguistics, 34(2):225–255, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>