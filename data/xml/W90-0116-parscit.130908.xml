<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000818">
<title confidence="0.988649">
The Local Organization of Text
</title>
<author confidence="0.821202">
Penelope Sibun
</author>
<affiliation confidence="0.851835">
Department of Computer and Information Science
University of Massachusetts
</affiliation>
<address confidence="0.537659">
Amherst, MA 01003
</address>
<email confidence="0.896621">
sibun@cs.umass.edu or penni@umass.bitnet
</email>
<sectionHeader confidence="0.996104" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929538461538">
In this paper, I present a model of the local organization
of extended text. I show that texts with weak rhetorical
structure and strong domain structure, such as descrip-
tions of houses, digital circuits, and families, are best
analyzed in terms of local domain structure, and ar-
gue that global structures that may be inferred from
a domain are not always appropriate for constructing
descriptions in the domain. I present a system I am im-
plementing that uses short-range strategies to organize
text, and show how part of a description is organized by
these strategies. I also briefly discuss a model of incre-
mental text generation that dovetails with the model of
local organization presented here.
</bodyText>
<subsectionHeader confidence="0.946838">
Motivation for local organization
</subsectionHeader>
<bodyText confidence="0.985014338235294">
The approach to organizing extended text described
here has both psychological and computational moti-
vation. It aims both to model how people use language
and to provide a flexible architecture for a system&apos;s lan-
guage use. In this section, I describe the empirical data
that form the basis of this research, and characterize
the local organization of the collected texts. In the
next two sections, I describe a computational architec-
ture to implement local text organization and discuss
its advantages of generality and flexibility, and give an
example of how this architecture works.
An extended text has a structure; this structure is a
description of how the components relate so that sense
can be made of the whole. Two sources of this organiza-
tion are rhetoricial structure, which describes the way
elements of the text fit together, and domain structure,
which describes relations among domain objects. For
this research I chose three domains with strong domain
structure, and a task—description—with weak rhetor-
ical structure. I have tape-recorded 29 people giving
descriptions of house layouts, electronic circuit layouts,
and family relationships. Description fragments of a
house and of a family, and the questions asked to ob-
tain the descriptions, are given in figure 1. (Because of
space considerations, the fragments are somewhat ab-
breviated.)
Many approaches to text organization&apos; are based
on analyses of text in terms of rhetorical structure.
However, there are few segments of text with inter-
esting rhetorical structure in my corpus. For exam-
ple, an analysis of the texts using Mann and Thomp-
son&apos;s (1987) Rhetorical Structure Theory (RST) would
result primarily in the relations sequence and joint
and would contain few of the the relations like evi-
dence or justify that give RST its descriptive power.
Similarly, it is unclear what work a system like that of
Grosz and Sidner (1986) would do in analyzing a de-
scription.
Since the structure of descriptions cannot be ana-
lyzed adequately with rhetorical relations, perhaps it
can be explained in terms of the domain. Houses, chips,
and families are strongly structured. A family&apos;s rela-
tionships can be captured in a family tree; one might
suppose that a description of the family would also be
organized in this way. A house can be encoded in a
number of ways; for instance, it has a component hier-
archy, being composed of rooms composed of furnish-
ings. Linde (1974) has proposed another comprehensive
structure for houses: a phrase structure grammar that
determines how the rooms may be visited in a traversal
of a house layout.
Surprisingly, these global, hierarchical domain struc-
tures are not exploited in the organization of descrip-
tions in my corpus. While family trees and composition
hierarchies can be inferred from descriptions of families
and houses, this does not mean that these structures
guide the process of organizing them. For instance,
my family informants did not simply construct their
descriptions by starting at the root of the appropriate
tree and doing a depth-first or breadth-first traversal
of it. Instead, to select a next family member to talk
about, they would apply one of several criteria. Gener-
ally, a sibling, spouse, parent, or child would be the next
choice, and this might incidentally constitute part of a
tree walk. But that this choice is local is evidenced by
the next choice, which may not be construable, in any
&apos;What I call text organization is usually referred to as
text planning.
</bodyText>
<page confidence="0.993333">
120
</page>
<bodyText confidence="0.966012090909091">
on our righthand side would be a door,
which leads to Penni&apos;s room
and you walk in there...
and there are two windows,
in the.. .opposite corner from the one in
which you enter
one&apos;s on the lefthand wall,
and one&apos;s on the wall that you would be
facing
than, on the righthand side...of her room,
is the closet
Fragment of a house description.
In response to the question: &amp;quot;Could you please describe for me the layout of your house.&amp;quot;
there&apos;s my mother Katharine, my father John,
my sister Penni, and me
it&apos;s my mother&apos;s relatives that we go to see
Margaret and Bill,
who are Mommy&apos;s...
urn.. .Margaret&apos;s my great-aunt
so it must be Mommy&apos;s aunt
Fragment of a family description.
In response to the question: &amp;quot;Can you tell me how everyone who comes to Thanksgiving is related to each other?&amp;quot;
</bodyText>
<figureCaption confidence="0.999152">
Figure 1: Sample descriptions.
</figureCaption>
<bodyText confidence="0.990771103448276">
principled way, as part of the overall structure of the
description that one might have postulated at the pre-
vious step. Where to begin the family description also
appears to be a locally conditioned choice. Informants
begin at various points, such as themselves or long-dead
progenitrixes, but the majority start their description
of the family by mentioning the hostess and host of the
Thanksgiving dinner they are attending; we may sup-
pose that at a different time of year the descriptions are
likely to start off differently.
Further evidence that people do not structure their
descriptions using obvious global domain structures
may be adduced from examples in which speakers ex-
plicitly deny knowledge of such structures, as in the
following fragment.
and also.. .um...
Eleanor and Elizabeth come
who are.. .cousins of.. .all of us.. .um...
I don&apos;t know what generation cousins they
are
Here, the speaker shows by her description of two
family members that she does not know her relationship
to them, even though it would be clear if a family tree
were being used to organize the description (the women
in question are in fact first cousins twice removed of the
speaker).
Genealogical trees, phrase structure grammars, and
component hierarchies are useful for succinctly repre-
senting information about houses, chips, and families
But there is no a priori reason to suppose that a de-
scription of such things are the products of such easily
articulable schemas or grammars. When we examine
texts of the sort that we wish to generate, we must
distinguish the mechanisms that direct the process of
choosing what to say next from a retrospective descrip-
tion of its result.
The texts I have collected can be best analyzed as
locally organized by a process of deciding what to say
next. This decision is based principally on what has
already been said and what is currently available to
say. For example, if one has just mentioned a large
window in the kitchen, one can mention whatever is to
the left of the window, whatever is to the right of it,
which way it is facing, what it looks like, or how it is
similar to a window in another room of the house. If
one has mentioned an aunt, one can give her name, say
whether she is married, mention her sister, enumerate
her children, or talk about how much money she earns.
The strong domain structure of subjects like houses,
chips, and families ensures that a description can be
continued from any point: once a description has been
started, there is always something, often many things,
that can be said next. In structured domains, there is
always a default choice for the next thing to say. In
spatial domains like houses, spatial proximity provides
this default. Everything in a house, be it a room, a
wall, or a kitchen appliance, is next to something else.
Spatial proximity does not constrain house descriptions,
</bodyText>
<page confidence="0.996824">
121
</page>
<bodyText confidence="0.999946552631579">
but it ensures that a description does not come to a
premature dead end.
Descriptions are finite. Though there may always be
more to say, there are points at which a description
may stop, when the task may be considered accom-
plished. Linde (1974) proposed a completeness criterion
for house descriptions, which is reflected in my data as
well as hers. It states that a description may stop any
time after all the rooms have been mentioned, but it is
not complete if it stops before. A similar criterion holds
in the family descriptions collected: they were given in
answer to the question, &amp;quot;Can you tell me how everyone
who comes to Thanksgiving is related to each other?&amp;quot;
In this case, then, the criterion is mentioning everyone
who attends.
Knowing how to continue and knowing when to stop
together ensure that a description can be generated de-
pending solely on local organization. The strong do-
main structure of houses and families makes the work-
ing of these mechanisms for continuation and termina-
tion particularly clear, and thus these domains are a
good site for studying this approach to organizing text.
However, the local organization of text is also evident
in many other uses of language. People&apos;s conversation
is often locally organized (Levinson, 1983); some inter-
active systems are currently being designed with this
approach (Frohlich &amp; Luff, 1989).
Because I am interested not only in how a program
may organize text but also in how people do so, I study
people speaking rather than people writing. A written
text may be edited and reorganized, and this process
often involves explicitly thinking about rhetorical struc-
ture. A spoken description is likely to require that the
speaker organize her text locally—she cannot plan it
out ahead of time. Studying spoken text reveals more
of the underlying mechanisms of language, because time
constraints and the inability to edit what has already
been said make post-processing impossible.
</bodyText>
<subsectionHeader confidence="0.97643">
Computational architecture
</subsectionHeader>
<bodyText confidence="0.999973040540541">
I am implementing a system that employs local orga-
nization of text as described in this paper. The imple-
mentation comprises: a semantic net knowledge base;
an organizer composed of strategies and metastrategies;
and a generator. Local organization is achieved using
short-range strategies, each of which is responsible for
organizing only a short segment of text, between a word
and a clause in length.
Until recently, I have employed Mumble-86 (Meteer
et al., 1987) as the generator for this system. Construc-
tion is underway, however, on a simpler generator that
more accurately implements the principles of generation
implied by the structure of the organizer. The system is
currently implemented only for descriptions of houses;
the examples in this section will thus be drawn from
the house domain.
The knowledge base is a semantic net that encodes
the objects, properties, and relations of a domain. The
organizer keeps a pointer to the current node in the de-
scription. A strategy describes the current node and
others related to it via local connections in the net-
work. Strategies are selected sequentially, based on
local conditions; if there is no strategy available or if
more than one strategy is appropriate, metastrategies
(Davis, 1980) resolve the conflict, using a technique
similar to universal subgoaling in Soar (Laird, Newell
&amp; Rosenbloom, 1987). Metastrategies are responsible
for control: they sequence and combine strategies. Like
strategies, they can cause the production of text.
This architecture has several advantages. First, it is
flexible: because there is no fixed priority scheme and
because strategies are small, the strategies can be com-
bined in a variety of ways to produce texts that are
constrained only by the appropriateness of each strat-
egy as determined by the strategies&apos; interaction with the
knowledge base. Second, the architecture is extensible:
new strategies can easily be added to extend the orga-
nizer to different types of text. Finally, the organizer is
mainly domain-independent: while some strategies may
be particular to houses, most strategies are not.
The strategies are applied to the knowledge base and
select the items that make up the description. The
strategies find the appropriate lexical items(s) for each
knowledge base item that is expressed; these lexical
items and the knowledge base items themselves are de-
termined by the domain. While the strategies are sim-
ple, complex behavior emerges from their interaction
with the knowledge base; this locally organizes the ex-
tended text.
Each strategy falls into one of four classes, with
varying degrees of domain independence: discourse cue
strategies; linguistic strategies; parameterizable domain-
independent strategies; and semi-domain-independent
strategies. Figure 2 gives examples of each.
Of the domain-independent strategies, discourse cue
strategies focus attention, in a way similar to the clue
words described by Reichman (1985), and linguistic
strategies mention objects and associated properties.
mention-salient-object, used to say &amp;quot;there is a win-
dow,&amp;quot; may as easily express &amp;quot;there is a penguin&amp;quot; or
&amp;quot;there is a policy.&amp;quot; describe-object is similarly all-
purpose, and can produce &amp;quot;the window with two flank-
ing windows&amp;quot; or &amp;quot;the man with one black shoe.&amp;quot;
The parameterizable domain-independent strategies
have slightly different textual realizations in different
domains, but these differences can be captured by pa-
rameters. The example given in figure 2 is typical: the
strategy is realized as a prepositional phrase in each
domain, and only the preposition changes.
The semi-domain-independent strategies accomplish
tasks such as a sweep that seem particular to the do-
main, but are similar to tasks in other domains. A
sweep begins at an object and names another bearing
some spatial relationship to it, and then another object
</bodyText>
<page confidence="0.985552">
122
</page>
<table confidence="0.9151654">
Discourse Cue Strategies
introduce/shift-attention &amp;quot;then,&amp;quot; &amp;quot;and,&amp;quot; &amp;quot;now&amp;quot;
refer-back/reinforce &amp;quot;again,&amp;quot; &amp;quot;once more&amp;quot;
Linguistic Strategies
mention-salient-object &amp;quot;there is z&amp;quot;
describe-object &amp;quot;we have z&amp;quot;
&amp;quot;the z is y,&amp;quot;
&amp;quot;the y z&amp;quot;
&amp;quot;the z (which) has z&amp;quot;
&amp;quot;the z with z&amp;quot;
</table>
<subsectionHeader confidence="0.885928">
Parameterizable Domain-independent Strategies
</subsectionHeader>
<bodyText confidence="0.984510333333333">
situate &amp;quot;in the kitchen&amp;quot;
&amp;quot;during the morning&amp;quot;
&amp;quot;about the election&amp;quot;
Semi-domain-indepe ndent Strategies
connected each to the
sweep Enumerate objects
next by the same relation.
follow a path Traverse a natural connection be-
tween parts of the knowledge to be described.
</bodyText>
<figureCaption confidence="0.959228">
Figure 2: Strategies.
</figureCaption>
<bodyText confidence="0.9737751">
that bears the same relationship to the just-mentioned
one, until there are none left; for example, &amp;quot;to the left
of the window is a stove and then a refrigerator&amp;quot; is
a sweep-left. Similar constructions may be found in
other domains. A description of one&apos;s day may start at
some event and mention the next event and the event
after that. In this case, the relationship is temporal,
rather than physical.
Metastrategies select strategies based on the context,
which comprises:
</bodyText>
<subsectionHeader confidence="0.790534">
Local Context
</subsectionHeader>
<listItem confidence="0.9979143">
• What has just been said.
• What is immediately connected to the current item
in the knowledge base.
• What strategies are applicable.
Global Context
• Discourse and speaker parameters. (For example, a
speaker&apos;s propensity to mention objects to the right
before objects to the left.) It is here that anything
considered a global goal would be encoded.
• The completeness criterion.
</listItem>
<bodyText confidence="0.998383941176471">
In future implementations, the context may also in-
volve some model of the hearer. This would be part of
the local context: what one knows about one&apos;s hearer
and about what one&apos;s hearer knows changes, particu-
larly under the assumption that hearer and speaker are
engaged in a two-way interaction.
A strategy conflict set contains whatever strategies
are currently applicable. If it contains a single strat-
egy, that one is selected. If more than one is in the
set, metastrategies may resolve the conflict by selecting
one or by combining some or all of them. Finally, if
no strategy presents itself, the metastrategies apply a
default strategy.
The metastrategy find-&amp;quot;interesting&amp;quot;-link is trig-
gered after the introduction of a new topic, which has
links to many other items in the knowledge base. What
is &amp;quot;interesting&amp;quot; or salient depends on:
</bodyText>
<listItem confidence="0.9279088">
• The domain: In spatial description, objects that
are large or have many features are interesting.
• The structure of the domain: Objects that are
more connected to other objects are more interesting.
• The local context: If a window has just been men-
tioned, there is reason to mention other windows.
• The global context: There may be an inclination
to mention furnishings but not structural features of
the house, or vice versa; there may be differing levels
of detail required.
</listItem>
<page confidence="0.998404">
123
</page>
<bodyText confidence="0.9984175">
Some metastrategies combine strategies. Such metas-
trategies apply when several strategies are appropriate
at some point in the description and there is a felicitous
way to combine them. circular-sweep is an example:
it combines a number of sweep strategies (sweep-left,
sweep-right, and sweep-under), and includes addi-
tional orientation strategies to orient hearers between
sweeps. kitty-corner is a metastrategy that is used
to describe a room in which the most salient feature
is diagonally opposite the current location. The object
that is &amp;quot;kitty corner&amp;quot; from it is mentioned first, and
the rest of the room is described in relation to it.2 The
first fragment in figure 1 exemplifies the kitty-corner
metastrategy.
find-new-topic is the default metastrategy just in
case there is nothing &amp;quot;interesting&amp;quot; to say. In a spatial
domain, the default is to select an object to describe
using spatial proximity.
</bodyText>
<subsectionHeader confidence="0.604815">
Example of description organization
</subsectionHeader>
<bodyText confidence="0.999900181818182">
In this section, I describe the organisation of a fragment
of a description in my corpus. While the system is
not yet fully implemented to handle all the details, this
example is sufficiently complex to show the operation of
the architecture in selecting appropriate strategies and
metastrategies. Though the strategies used are simple,
complex choices, varying with context, are made. On
the following page are the text and a sketch of the area
being described.
In the fragment in figure 3, the speaker is describing
the bedroom he shares with his wife Carol. Each line
of the fragment, in most cases, is the result of a single
strategy.
The sketch in figure 4 of the bedroom is provided as
an aid to the reader in understanding John&apos;s descrip-
tion. There is no corresponding representation in the
system.
The global context used by this speaker includes pa-
rameters that predispose him to mention rather than
ignore room furnishings, as well as &amp;quot;stuff&amp;quot;—small arti-
cles than can be found in, on, or near pieces of furniture.
The strategy mention-stuff is a particular form of
describe-object, and as such is concerned with men-
tioning associated properties of the object rather than
physical proximity; this is suggested by the speaker&apos;s
typically using the preposition &amp;quot;with,&amp;quot; rather than an
obviously spatial one.
The global context also of course includes the com-
pleteness criterion which is unsatisfied throughout this
stretch of text. The fragment starts when the speaker
has just finished describing the kitchen and the next
spatially proximate thing is the door to the bedroom.
When the node to be described is a physical object,
</bodyText>
<tableCaption confidence="0.471524333333333">
2For a fuller treatment of how the system computes de-
ictic and other spatial terms like &amp;quot;kitty corner,&amp;quot; &amp;quot;left,&amp;quot; and
&amp;quot;right,&amp;quot; see (Sibun &amp; Huettner, 1989).
</tableCaption>
<bodyText confidence="0.996134714285714">
the mention-object strategy is always available; be-
cause this object is a door, mention-room is available
to talk about the room that the door leads into. The
metastrategies resolve this conflict in favor of the more
particular mention-room {1}.
Because the last strategy used was mention-room,
mention-salient-object becomes available; if there
is a particularly &amp;quot;salient&amp;quot; object in a room, descrip-
tions can then be organized around it. As it happens,
salience in this domain tends to depend primarily on
size; the kingsize bed fits the bill {2}. There are two
objects spatially proximate to the bed; one is selected
and the strategy mention-object is used to mention
the endtable {3}. The endtable is connected to several
other items in the knowledge base, but because there
is a context parameter to mention &amp;quot;stuff,&amp;quot; this is what
happens next {4}.
Now, there are two unmentioned objects spatially
proximate to the endtable—the window and the &amp;quot;wall&amp;quot;
(which is actually a covered-up chimney). The &amp;quot;wall&amp;quot;
is mentioned because, like furnishings, it has extent in
the room, and the context disposes the process toward
mentioning such objects {5}. The local context keeps
track of what has just been mentioned; another feature
it records is the direction in which the spatial proximity
links have been followed. The &amp;quot;wall&amp;quot; is next along the
trajectory from the bed through the endtable.
The &amp;quot;wall&amp;quot; is spatially linked to three things: the
window on the endtable side of it, the window that is
along the same trajectory that has been followed, and
the chests, which are also along that trajectory. The
window along the trajectory is the choice selected from
these three for two reasons: there is a tendency to main-
tain trajectories;3 and the last thing mentioned is part
of the structure of the room, as is the window, but not
the chests. But this selection of the window presents a
problem (at least, we can infer that it presents a prob-
lem to the speaker), because this window is connected
via a similarity link to the previous window, which has
not been mentioned. So the speaker performs a repair
consisting of backing up, mentioning the overlooked
window, reiterating mention of the wall, and, finally,
mentioning the selected window {6}.
While the next window is a promising candidate be-
cause it is the same sort of thing as that just mentioned,
the parameter for mentioning furniture overrides this,
and the cedar chests come next in the description {7},
followed by their associated &amp;quot;stuff&amp;quot; {8}. The window
is again available but so is the bureau, which is the
preferred choice because it is furniture {9}. The bu-
reau has spatial proximity links to two windows and
the &amp;quot;small thing,&amp;quot; as well as links to its &amp;quot;stuff,&amp;quot; so the
set of available strategies comprises ones that mention
each of these things. A metastrategy can resolve this
2Ullmer-Ehrich (1982) notes a similar tendency in the
dorm room descriptions that she collected.
</bodyText>
<page confidence="0.99661">
124
</page>
<bodyText confidence="0.9915412">
and then there&apos;s the bedroom {1}
and there&apos;s a huge kingsize bed {2}
and there&apos;s an endtable next to it {3}
with a lamp and a clock radio {4}
and some stuff of mine
like the Boston University cup
and...then there&apos;s a wall-- {5}
and then there&apos;s a window behind that {6}
and there&apos;s a wall, and there&apos;s another window
and there&apos;s some.. .cedar chests of Carol&apos;s {7}
that have blankets and sheets in them {8}
and...there&apos;s her bureau {9}
in the middle of two windows on either side {10}
with all of her makeup on top of it and clothes
and there&apos;s.. .a small thing with all her clothes
and there&apos;s another great big bookshelf
and all her spare books
and there&apos;s a small end table over on her side
um...a small digital clock and more kleenex
and...OK
</bodyText>
<figureCaption confidence="0.997573">
Figure 3: John&apos;s description.
</figureCaption>
<figure confidence="0.995802833333333">
window window
end table
chest chest
bed
bookshelf
I I
end table
I
*
0.
Oman*
thing
</figure>
<figureCaption confidence="0.99976">
Figure 4: Sketch of bedroom.
</figureCaption>
<page confidence="0.99596">
125
</page>
<bodyText confidence="0.998792">
conflict by realizing that strategies for saying that there
is an object of the same sort on two different sides of
the current object can be combined by saying that the
bureau is between the two windows {10}.4
The description for the rest of the room continues in
a manner similar to that already discussed.
</bodyText>
<subsectionHeader confidence="0.989061">
Incremental generation
</subsectionHeader>
<bodyText confidence="0.996588096153846">
The organizer described here is composed of strategies
that are often responsible for sub-clausal units of text;
furthermore, the strategies have already imposed an or-
ganization on the text, obviating the need for the gen-
erator to do more than enforce grammaticality. The
model of local text organization I am developing is cou-
pled with a model of incremental generation, in which
the increments are often smaller than a sentence. (Gar-
rett&apos;s investigation of speech errors (1975) constitutes
early work in this area; De Smedt &amp; Kempen (1987),
and Kempen &amp; Hoenkamp (1987) discuss a similar,
more fully-developed incremental generation project.)
A typical generator produces a sentence at a time. How-
ever, spoken text is replete with restarts, fragments,
and ungrammaticalities. This suggests that not only
do people organize their text incrementally, but gener-
ate it in increments as well.
Usually, an incremental generator is successful in gen-
erating grammatical text. The text
then /
in the kitchen /
there is a large window
is the result of three sequentially operating strategies:
introduce/shift-attention, situate, and mention-
salient-object. An incremental generator will be able
to produce this text in the increments specified by the
strategies.
A system that generates in strategy-sized increments
can result, in principled ways, in ungrammatical text.
A common error, exemplified by
and a lot of other people /
who /
I wasn&apos;t quite sure what they did
can be explained by the operation of the strategies
mention-object, add-clausal-modifier, and a set
of strategies to express some additional information,
which, because a dependent clause has already been
started, happens to result in the ungrammatical re-
sumptive pronoun &amp;quot;they.&amp;quot;
Locally-organized, occasionally ungrammatical text
may be prototypical, but it is certainly not the only sort
4Note that the previous window conflict was not resolved
by saying that the &amp;quot;wall&amp;quot; was between the two windows.
The difference can be explained by the observation that the
&amp;quot;wall,&amp;quot; despite its extent into the room, is a structural ob-
ject, while the bureau is furniture.
of text we wish to generate. To be comprehensive, my
system will require some capability to post-process text
after it is organized and before it is generated (Hovy,
1989, suggests a post-processing model). Output of my
system might also be appropriate input for a text revi-
sion system (e.g., Meteer &amp; McDonald, 1986).
</bodyText>
<subsectionHeader confidence="0.561444">
Related research
</subsectionHeader>
<bodyText confidence="0.99992393877551">
There are many other projects whose goal is to organize,
or plan, extended text. The main difference between
these and mine is flexibility and level of organization:
most text planning systems rely on global structures to
organize paragraph-sized text. These structures, which
are usually schemas or plans, constrain the text to re-
flect particular rhetorical and domain relations, but at
the expense of flexibility. My system builds structure
locally with no recourse or reference to an overall struc-
ture.
Through analysis of written texts, McKeown (1985)
has developed a small set of schemas, built from rhetor-
ical predicates, that would provide types of descriptions
(for example, constituency) for objects in a knowledge
base. Paris has extended this work to use a process
trace which derives its structure in part from the do-
main (Paris &amp; McKeown, 1987; Paris, 1988). This al-
ternative strategy builds and traverses a path through
the knowledge base when it is determined, on the basis
of a sparse model of user expertise, that a process de-
scription of an object is more appropriate than a declar-
ative one; the two strategies may be interleaved. Dale
(1989) similarly organizes text by means of a domain
structure, in this case, a recipe plan.
Rhetorical Structure Theory (Mann &amp; Thompson,
1987) is also drawn from an analysis of written texts;
it differs from McKeown&apos;s work in that it is composed
of a large number of rhetorical relations, rather than
a small number of schemas. RST may thus be more
flexible, but is is still assumed that the relations will be
combined into a single global tree covering an extended
text.
While most text planning systems have worked on
producing single texts in response to queries, some re-
search has been more particularly concerned with inter-
active text. Much recent work in this area has been for
explanation systems (e.g., Maybury, 1989), and some
of this work explicitly addresses allowing a human user
to ask follow-up questions (Moore &amp; Swartout, 1989).
However, such systems still build and use global struc-
tures for extended texts.
An argument is sometimes made that global struc-
ture is needed to capture high-level rhetorical goals in
the output text (see Appelt, 1985); Gricean Maxims
(Grice, 1975) are often invoked. But, as Hovy (1988)
points out, &amp;quot;be polite&amp;quot; is not a decomposable goal; the
objective of being polite is achieved through local de-
cisions. Such local decisions can comfortably be inte-
grated with a model of local organization of text.
</bodyText>
<page confidence="0.997946">
126
</page>
<sectionHeader confidence="0.996581" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.594982375">
I thank Bonnie Webber and David Chapman for much
useful discussion in the preparation of this paper. I also
thank Bruce Leban for helping to make the Mac draw
the picture. This work was supported in part by the Air
Force Systems Command, Rome Air Development Cen-
ter, Griffiss AFB, New York, 13441 under contract No.
F30602-85-C-0008, as part of the Northeast Artificial
Intelligence Consortium (NAIC).
</bodyText>
<sectionHeader confidence="0.967698" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977106097561">
Appelt, D., &amp;quot;Planning English Referring Expres-
sions.&amp;quot; Artificial Intelligence 26, pp 1-33, 1985.
Dale, R., &amp;quot;Generating Recipes: An Overview of Epi-
cure.&amp;quot; In Extended Abstracts Presented at the Sec-
ond European Natural Language Generation Workshop,
pp 7-13, 1989.
Davis, R., &amp;quot;Meta-rules: Reasoning about Control.&amp;quot;
Artificial Intelligence 15, pp 179-222, 1980.
De Smedt, K. and G. Kempen, &amp;quot;Incremental Sen-
tence Production, Self-correction, and Coordination.&amp;quot;
In G. Kempen (ed), Natural Language Generation: New
Results in Artificial Intelligence, Psychology and Lin-
guistics, Martinus Nijhoff Publishers, pp 365-376, 1987.
Frohlich, D. and P. Luff, &amp;quot;Applying the Technology of
Conversation to the Technology for Conversation.&amp;quot; In
Proceedings of the Computers and Conversation Sym-
posium, University of Surrey, England, 1989.
Garrett, M., &amp;quot;The Analysis of Sentence Produc-
tion.&amp;quot; Psychology of Learning and Motivation, Volume
9, pp 133-177, 1975.
Grice, H., &amp;quot;Logic and Conversation,&amp;quot; In P. Cole and
J. Morgan (eds.), Syntax and Semantics 3: Speech Acts,
pp 41-58, 1975.
Grosz, B. and C. Sidner, &amp;quot;Attention, Intentions, and
the Structure of Discourse.&amp;quot; Computational Linguis-
tics, (12)3, 1986.
Hovy, E., &amp;quot;Two Types of Planning in Language Gen-
eration.&amp;quot; In Proceedings of the 26th Annual Meeting of
the Association for Computational Linguistics, pp 179-
186, 1988.
Hovy, E., &amp;quot;Unresolved Issues In Paragraph Plan-
ning.&amp;quot; To appear in Proceedings of the Second European
Natural Language Generation Workshop, 1989.
Kempen, G. and E. Hoenkamp, &amp;quot;An Incremental
Procedural Grammar for Sentence Formulation.&amp;quot; Cog-
nitive Science 11, pp 201-258, 1987.
Laird, J., A. Newell, and P. Rosenbloom, &amp;quot;Soar: An
Architecture for General Intelligence.&amp;quot; Artificial Intel-
ligence, 33, pp 1-64, 1987.
Levinson, S., Pragmatics. Cambridge University
Press, 1983.
Linde, C., The Linguistic Encoding of Spatial Infor-
mation. Doctoral Dissertation, Columbia University,
1974.
Mann, W. and S. Thompson, Rhetorical Structure
Theory: A Theory of Text Organization. Report
ISI/RS-87-190, Information Sciences Institute, 1987.
Maybury, M., &amp;quot;Enhancing Explanation Coherence
with Rhetorical Strategies.&amp;quot; In Proceedings of the Euro-
pean Association of Computational Linguistics„ pp 168-
173, 1989.
McKeown, K., Text Generation. Cambridge Univer-
sity Press, 1985.
Meteer (Vaughan), M. and D. McDonald, &amp;quot;A Model
of Revision in Natural Language Generation.&amp;quot; In Pro-
ceedings of the 24th Annual Meeting of the Association
for Computational Linguistics, pp 90-96, 1986.
Meteer, M., D. McDonald, S. Anderson, D. Forster,
L. Gay, A. Huettner, and P. Sibun, MUMBLE-86: De-
sign and Implementation. COINS Technical Report 87-
87, University of Massachusetts, 1987.
Moore, J. and W. Swartout, &amp;quot;A Reactive Approach
to Explanation.&amp;quot; In Proceedings of the International
Joint Conference on Text Generation IJCAI, 1989.
Paris, C., &amp;quot;Tailoring Object Descriptions to a User&apos;s
Level of Expertise.&amp;quot; Computational Linguistics 14(3),
pp 64-78, 1988.
Paris, C. and K. McKeown, &amp;quot;Discourse Strategies for
Describing Complex Physical Objects.&amp;quot; In G. Kempen
(ed), Natural Language Generation: New Results in Ar-
tificial Intelligence, Psychology and Linguistics, Marti-
nus Nijhoff Publishers, pp 97-115, 1987.
Reichman, R., Getting Computers to Talk Like You
and Me. MIT Press, 1985.
Sibun, P. and A. Huettner, &amp;quot;Spatial Deixis in Gen-
erating Descriptions.&amp;quot; COINS Technical Report 89-34,
Department of Computer and Information Science, Uni-
versity of Massachusetts, 1989.
Ullmer-Ehrich, V., &amp;quot;The Structure of Living Space
Descriptions.&amp;quot; In R. Jarvella and W. Klein, eds.,
Speech, Place, and Action, John Wiley &amp; Sons, Ltd.,
1982.
</reference>
<page confidence="0.997318">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.571316">
<title confidence="0.995525">The Local Organization of Text</title>
<author confidence="0.972001">Penelope</author>
<affiliation confidence="0.9995645">Department of Computer and Information University of</affiliation>
<address confidence="0.999877">Amherst, MA 01003</address>
<abstract confidence="0.999142888217523">In this paper, I present a model of the local organization of extended text. I show that texts with weak rhetorical structure and strong domain structure, such as descriptions of houses, digital circuits, and families, are best analyzed in terms of local domain structure, and argue that global structures that may be inferred from a domain are not always appropriate for constructing descriptions in the domain. I present a system I am imthat uses strategies organize text, and show how part of a description is organized by these strategies. I also briefly discuss a model of incremental text generation that dovetails with the model of local organization presented here. Motivation for local organization The approach to organizing extended text described here has both psychological and computational motivation. It aims both to model how people use language and to provide a flexible architecture for a system&apos;s language use. In this section, I describe the empirical data that form the basis of this research, and characterize the local organization of the collected texts. In the next two sections, I describe a computational architecture to implement local text organization and discuss its advantages of generality and flexibility, and give an example of how this architecture works. An extended text has a structure; this structure is a description of how the components relate so that sense can be made of the whole. Two sources of this organizaare structure, describes the way of the text fit together, and structure, which describes relations among domain objects. For this research I chose three domains with strong domain structure, and a task—description—with weak rhetorical structure. I have tape-recorded 29 people giving descriptions of house layouts, electronic circuit layouts, and family relationships. Description fragments of a house and of a family, and the questions asked to obtain the descriptions, are given in figure 1. (Because of space considerations, the fragments are somewhat abbreviated.) Many approaches to text organization&apos; are based on analyses of text in terms of rhetorical structure. However, there are few segments of text with interesting rhetorical structure in my corpus. For example, an analysis of the texts using Mann and Thompson&apos;s (1987) Rhetorical Structure Theory (RST) would result primarily in the relations sequence and joint and would contain few of the the relations like evidence or justify that give RST its descriptive power. Similarly, it is unclear what work a system like that of Grosz and Sidner (1986) would do in analyzing a description. Since the structure of descriptions cannot be analyzed adequately with rhetorical relations, perhaps it can be explained in terms of the domain. Houses, chips, and families are strongly structured. A family&apos;s relationships can be captured in a family tree; one might suppose that a description of the family would also be organized in this way. A house can be encoded in a number of ways; for instance, it has a component hierarchy, being composed of rooms composed of furnishings. Linde (1974) has proposed another comprehensive structure for houses: a phrase structure grammar that determines how the rooms may be visited in a traversal of a house layout. Surprisingly, these global, hierarchical domain structures are not exploited in the organization of descriptions in my corpus. While family trees and composition hierarchies can be inferred from descriptions of families and houses, this does not mean that these structures guide the process of organizing them. For instance, my family informants did not simply construct their descriptions by starting at the root of the appropriate tree and doing a depth-first or breadth-first traversal of it. Instead, to select a next family member to talk about, they would apply one of several criteria. Generally, a sibling, spouse, parent, or child would be the next choice, and this might incidentally constitute part of a tree walk. But that this choice is local is evidenced by the next choice, which may not be construable, in any &apos;What I call text organization is usually referred to as text planning. 120 on our righthand side would be a door, which leads to Penni&apos;s room and you walk in there... and there are two windows, in the.. .opposite corner from the one in which you enter one&apos;s on the lefthand wall, and one&apos;s on the wall that you would be facing than, on the righthand side...of her room, is the closet Fragment of a house description. In response to the question: &amp;quot;Could you please describe for me the layout of your house.&amp;quot; there&apos;s my mother Katharine, my father John, my sister Penni, and me it&apos;s my mother&apos;s relatives that we go to see Margaret and Bill, who are Mommy&apos;s... urn.. .Margaret&apos;s my great-aunt so it must be Mommy&apos;s aunt Fragment of a family description. In response to the question: &amp;quot;Can you tell me how everyone who comes to Thanksgiving is related to each other?&amp;quot; Figure 1: Sample descriptions. principled way, as part of the overall structure of the description that one might have postulated at the previous step. Where to begin the family description also appears to be a locally conditioned choice. Informants begin at various points, such as themselves or long-dead progenitrixes, but the majority start their description of the family by mentioning the hostess and host of the Thanksgiving dinner they are attending; we may suppose that at a different time of year the descriptions are likely to start off differently. Further evidence that people do not structure their descriptions using obvious global domain structures may be adduced from examples in which speakers explicitly deny knowledge of such structures, as in the following fragment. and also.. .um... Eleanor and Elizabeth come who are.. .cousins of.. .all of us.. .um... know what generation cousins they are Here, the speaker shows by her description of two family members that she does not know her relationship to them, even though it would be clear if a family tree were being used to organize the description (the women in question are in fact first cousins twice removed of the speaker). Genealogical trees, phrase structure grammars, and component hierarchies are useful for succinctly representing information about houses, chips, and families there is no a to suppose that a description of such things are the products of such easily articulable schemas or grammars. When we examine texts of the sort that we wish to generate, we must distinguish the mechanisms that direct the process of choosing what to say next from a retrospective description of its result. The texts I have collected can be best analyzed as organized a process of deciding to say decision is based principally on what has already been said and what is currently available to say. For example, if one has just mentioned a large window in the kitchen, one can mention whatever is to the left of the window, whatever is to the right of it, which way it is facing, what it looks like, or how it is similar to a window in another room of the house. If one has mentioned an aunt, one can give her name, say whether she is married, mention her sister, enumerate her children, or talk about how much money she earns. The strong domain structure of subjects like houses, chips, and families ensures that a description can be continued from any point: once a description has been started, there is always something, often many things, that can be said next. In structured domains, there is always a default choice for the next thing to say. In spatial domains like houses, spatial proximity provides this default. Everything in a house, be it a room, a wall, or a kitchen appliance, is next to something else. proximity does not descriptions, 121 but it ensures that a description does not come to a premature dead end. Descriptions are finite. Though there may always be more to say, there are points at which a description may stop, when the task may be considered accom- Linde (1974) proposed a criterion for house descriptions, which is reflected in my data as well as hers. It states that a description may stop any time after all the rooms have been mentioned, but it is not complete if it stops before. A similar criterion holds in the family descriptions collected: they were given in answer to the question, &amp;quot;Can you tell me how everyone who comes to Thanksgiving is related to each other?&amp;quot; In this case, then, the criterion is mentioning everyone who attends. Knowing how to continue and knowing when to stop together ensure that a description can be generated depending solely on local organization. The strong domain structure of houses and families makes the working of these mechanisms for continuation and termination particularly clear, and thus these domains are a good site for studying this approach to organizing text. However, the local organization of text is also evident in many other uses of language. People&apos;s conversation is often locally organized (Levinson, 1983); some interactive systems are currently being designed with this approach (Frohlich &amp; Luff, 1989). Because I am interested not only in how a program may organize text but also in how people do so, I study people speaking rather than people writing. A written text may be edited and reorganized, and this process often involves explicitly thinking about rhetorical structure. A spoken description is likely to require that the speaker organize her text locally—she cannot plan it out ahead of time. Studying spoken text reveals more of the underlying mechanisms of language, because time constraints and the inability to edit what has already been said make post-processing impossible. Computational architecture I am implementing a system that employs local organization of text as described in this paper. The implementation comprises: a semantic net knowledge base; an organizer composed of strategies and metastrategies; and a generator. Local organization is achieved using strategies, of which is responsible for organizing only a short segment of text, between a word and a clause in length. Until recently, I have employed Mumble-86 (Meteer 1987) as the generator for this system. Construction is underway, however, on a simpler generator that more accurately implements the principles of generation implied by the structure of the organizer. The system is currently implemented only for descriptions of houses; the examples in this section will thus be drawn from the house domain. The knowledge base is a semantic net that encodes the objects, properties, and relations of a domain. The organizer keeps a pointer to the current node in the description. A strategy describes the current node and others related to it via local connections in the network. Strategies are selected sequentially, based on local conditions; if there is no strategy available or if than one strategy is appropriate, (Davis, 1980) resolve the conflict, using a technique similar to universal subgoaling in Soar (Laird, Newell &amp; Rosenbloom, 1987). Metastrategies are responsible for control: they sequence and combine strategies. Like strategies, they can cause the production of text. This architecture has several advantages. First, it is flexible: because there is no fixed priority scheme and because strategies are small, the strategies can be combined in a variety of ways to produce texts that are constrained only by the appropriateness of each strategy as determined by the strategies&apos; interaction with the knowledge base. Second, the architecture is extensible: new strategies can easily be added to extend the organizer to different types of text. Finally, the organizer is mainly domain-independent: while some strategies may be particular to houses, most strategies are not. The strategies are applied to the knowledge base and select the items that make up the description. The strategies find the appropriate lexical items(s) for each knowledge base item that is expressed; these lexical items and the knowledge base items themselves are determined by the domain. While the strategies are simple, complex behavior emerges from their interaction with the knowledge base; this locally organizes the extended text. Each strategy falls into one of four classes, with degrees of domain independence: cue strategies; linguistic strategies; parameterizable domainstrategies; 2 gives examples of each. Of the domain-independent strategies, discourse cue strategies focus attention, in a way similar to the clue words described by Reichman (1985), and linguistic strategies mention objects and associated properties. mention-salient-object, used to say &amp;quot;there is a window,&amp;quot; may as easily express &amp;quot;there is a penguin&amp;quot; or &amp;quot;there is a policy.&amp;quot; describe-object is similarly allpurpose, and can produce &amp;quot;the window with two flanking windows&amp;quot; or &amp;quot;the man with one black shoe.&amp;quot; The parameterizable domain-independent strategies have slightly different textual realizations in different domains, but these differences can be captured by parameters. The example given in figure 2 is typical: the strategy is realized as a prepositional phrase in each domain, and only the preposition changes. The semi-domain-independent strategies accomplish tasks such as a sweep that seem particular to the domain, but are similar to tasks in other domains. A sweep begins at an object and names another bearing some spatial relationship to it, and then another object 122 Discourse Cue Strategies introduce/shift-attention &amp;quot;then,&amp;quot; &amp;quot;and,&amp;quot; &amp;quot;now&amp;quot; refer-back/reinforce &amp;quot;again,&amp;quot; &amp;quot;once more&amp;quot; Linguistic Strategies mention-salient-object describe-object &amp;quot;there is z&amp;quot; &amp;quot;we have z&amp;quot; &amp;quot;the z is y,&amp;quot; &amp;quot;the y z&amp;quot; &amp;quot;the z (which) has z&amp;quot; &amp;quot;the z with z&amp;quot; Parameterizable Domain-independent Strategies situate &amp;quot;in the kitchen&amp;quot; &amp;quot;during the morning&amp;quot; &amp;quot;about the election&amp;quot; Semi-domain-indepe ndent Strategies connected each to the sweep Enumerate objects next by the same relation. follow a path Traverse a natural connection between parts of the knowledge to be described. Figure 2: Strategies. that bears the same relationship to the just-mentioned one, until there are none left; for example, &amp;quot;to the left of the window is a stove and then a refrigerator&amp;quot; is a sweep-left. Similar constructions may be found in other domains. A description of one&apos;s day may start at some event and mention the next event and the event after that. In this case, the relationship is temporal, rather than physical. select strategies based on the which comprises: Local Context • What has just been said. What is to the current item in the knowledge base. • What strategies are applicable. Global Context • Discourse and speaker parameters. (For example, a speaker&apos;s propensity to mention objects to the right before objects to the left.) It is here that anything considered a global goal would be encoded. • The completeness criterion. In future implementations, the context may also involve some model of the hearer. This would be part of the local context: what one knows about one&apos;s hearer and about what one&apos;s hearer knows changes, particularly under the assumption that hearer and speaker are engaged in a two-way interaction. A strategy conflict set contains whatever strategies are currently applicable. If it contains a single strategy, that one is selected. If more than one is in the set, metastrategies may resolve the conflict by selecting one or by combining some or all of them. Finally, if no strategy presents itself, the metastrategies apply a default strategy.</abstract>
<intro confidence="0.740405">The metastrategy find-&amp;quot;interesting&amp;quot;-link is trig-</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Appelt</author>
</authors>
<title>Planning English Referring Expressions.&amp;quot;</title>
<date>1985</date>
<journal>Artificial Intelligence</journal>
<volume>26</volume>
<pages>1--33</pages>
<contexts>
<context position="28305" citStr="Appelt, 1985" startWordPosition="4623" endWordPosition="4624">ing an extended text. While most text planning systems have worked on producing single texts in response to queries, some research has been more particularly concerned with interactive text. Much recent work in this area has been for explanation systems (e.g., Maybury, 1989), and some of this work explicitly addresses allowing a human user to ask follow-up questions (Moore &amp; Swartout, 1989). However, such systems still build and use global structures for extended texts. An argument is sometimes made that global structure is needed to capture high-level rhetorical goals in the output text (see Appelt, 1985); Gricean Maxims (Grice, 1975) are often invoked. But, as Hovy (1988) points out, &amp;quot;be polite&amp;quot; is not a decomposable goal; the objective of being polite is achieved through local decisions. Such local decisions can comfortably be integrated with a model of local organization of text. 126 Acknowledgements I thank Bonnie Webber and David Chapman for much useful discussion in the preparation of this paper. I also thank Bruce Leban for helping to make the Mac draw the picture. This work was supported in part by the Air Force Systems Command, Rome Air Development Center, Griffiss AFB, New York, 1344</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Appelt, D., &amp;quot;Planning English Referring Expressions.&amp;quot; Artificial Intelligence 26, pp 1-33, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
</authors>
<title>Generating Recipes: An Overview of Epicure.&amp;quot; In Extended Abstracts Presented at the Second European Natural Language Generation Workshop,</title>
<date>1989</date>
<pages>7--13</pages>
<contexts>
<context position="27246" citStr="Dale (1989)" startWordPosition="4446" endWordPosition="4447">(1985) has developed a small set of schemas, built from rhetorical predicates, that would provide types of descriptions (for example, constituency) for objects in a knowledge base. Paris has extended this work to use a process trace which derives its structure in part from the domain (Paris &amp; McKeown, 1987; Paris, 1988). This alternative strategy builds and traverses a path through the knowledge base when it is determined, on the basis of a sparse model of user expertise, that a process description of an object is more appropriate than a declarative one; the two strategies may be interleaved. Dale (1989) similarly organizes text by means of a domain structure, in this case, a recipe plan. Rhetorical Structure Theory (Mann &amp; Thompson, 1987) is also drawn from an analysis of written texts; it differs from McKeown&apos;s work in that it is composed of a large number of rhetorical relations, rather than a small number of schemas. RST may thus be more flexible, but is is still assumed that the relations will be combined into a single global tree covering an extended text. While most text planning systems have worked on producing single texts in response to queries, some research has been more particula</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Dale, R., &amp;quot;Generating Recipes: An Overview of Epicure.&amp;quot; In Extended Abstracts Presented at the Second European Natural Language Generation Workshop, pp 7-13, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Davis</author>
</authors>
<title>Meta-rules: Reasoning about Control.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>15</volume>
<pages>179--222</pages>
<contexts>
<context position="11338" citStr="Davis, 1980" startWordPosition="1873" endWordPosition="1874">tructure of the organizer. The system is currently implemented only for descriptions of houses; the examples in this section will thus be drawn from the house domain. The knowledge base is a semantic net that encodes the objects, properties, and relations of a domain. The organizer keeps a pointer to the current node in the description. A strategy describes the current node and others related to it via local connections in the network. Strategies are selected sequentially, based on local conditions; if there is no strategy available or if more than one strategy is appropriate, metastrategies (Davis, 1980) resolve the conflict, using a technique similar to universal subgoaling in Soar (Laird, Newell &amp; Rosenbloom, 1987). Metastrategies are responsible for control: they sequence and combine strategies. Like strategies, they can cause the production of text. This architecture has several advantages. First, it is flexible: because there is no fixed priority scheme and because strategies are small, the strategies can be combined in a variety of ways to produce texts that are constrained only by the appropriateness of each strategy as determined by the strategies&apos; interaction with the knowledge base.</context>
</contexts>
<marker>Davis, 1980</marker>
<rawString>Davis, R., &amp;quot;Meta-rules: Reasoning about Control.&amp;quot; Artificial Intelligence 15, pp 179-222, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K De Smedt</author>
<author>G Kempen</author>
</authors>
<title>Incremental Sentence Production, Self-correction, and Coordination.&amp;quot;</title>
<date>1987</date>
<booktitle>In G. Kempen (ed), Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, Martinus Nijhoff Publishers,</booktitle>
<pages>365--376</pages>
<marker>De Smedt, Kempen, 1987</marker>
<rawString>De Smedt, K. and G. Kempen, &amp;quot;Incremental Sentence Production, Self-correction, and Coordination.&amp;quot; In G. Kempen (ed), Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, Martinus Nijhoff Publishers, pp 365-376, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Frohlich</author>
<author>P Luff</author>
</authors>
<title>Applying the Technology of Conversation to the Technology for Conversation.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the Computers and Conversation Symposium,</booktitle>
<institution>University of Surrey,</institution>
<location>England,</location>
<contexts>
<context position="9475" citStr="Frohlich &amp; Luff, 1989" startWordPosition="1574" endWordPosition="1577">. Knowing how to continue and knowing when to stop together ensure that a description can be generated depending solely on local organization. The strong domain structure of houses and families makes the working of these mechanisms for continuation and termination particularly clear, and thus these domains are a good site for studying this approach to organizing text. However, the local organization of text is also evident in many other uses of language. People&apos;s conversation is often locally organized (Levinson, 1983); some interactive systems are currently being designed with this approach (Frohlich &amp; Luff, 1989). Because I am interested not only in how a program may organize text but also in how people do so, I study people speaking rather than people writing. A written text may be edited and reorganized, and this process often involves explicitly thinking about rhetorical structure. A spoken description is likely to require that the speaker organize her text locally—she cannot plan it out ahead of time. Studying spoken text reveals more of the underlying mechanisms of language, because time constraints and the inability to edit what has already been said make post-processing impossible. Computationa</context>
</contexts>
<marker>Frohlich, Luff, 1989</marker>
<rawString>Frohlich, D. and P. Luff, &amp;quot;Applying the Technology of Conversation to the Technology for Conversation.&amp;quot; In Proceedings of the Computers and Conversation Symposium, University of Surrey, England, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garrett</author>
</authors>
<title>The Analysis of Sentence Production.&amp;quot;</title>
<date>1975</date>
<journal>Psychology of Learning and Motivation,</journal>
<volume>9</volume>
<pages>133--177</pages>
<marker>Garrett, 1975</marker>
<rawString>Garrett, M., &amp;quot;The Analysis of Sentence Production.&amp;quot; Psychology of Learning and Motivation, Volume 9, pp 133-177, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Grice</author>
</authors>
<title>Logic and Conversation,&amp;quot;</title>
<date>1975</date>
<booktitle>Syntax and Semantics 3: Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and J. Morgan (eds.),</editor>
<contexts>
<context position="28335" citStr="Grice, 1975" startWordPosition="4627" endWordPosition="4628">t text planning systems have worked on producing single texts in response to queries, some research has been more particularly concerned with interactive text. Much recent work in this area has been for explanation systems (e.g., Maybury, 1989), and some of this work explicitly addresses allowing a human user to ask follow-up questions (Moore &amp; Swartout, 1989). However, such systems still build and use global structures for extended texts. An argument is sometimes made that global structure is needed to capture high-level rhetorical goals in the output text (see Appelt, 1985); Gricean Maxims (Grice, 1975) are often invoked. But, as Hovy (1988) points out, &amp;quot;be polite&amp;quot; is not a decomposable goal; the objective of being polite is achieved through local decisions. Such local decisions can comfortably be integrated with a model of local organization of text. 126 Acknowledgements I thank Bonnie Webber and David Chapman for much useful discussion in the preparation of this paper. I also thank Bruce Leban for helping to make the Mac draw the picture. This work was supported in part by the Air Force Systems Command, Rome Air Development Center, Griffiss AFB, New York, 13441 under contract No. F30602-85</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Grice, H., &amp;quot;Logic and Conversation,&amp;quot; In P. Cole and J. Morgan (eds.), Syntax and Semantics 3: Speech Acts, pp 41-58, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, Intentions, and the Structure of Discourse.&amp;quot;</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<contexts>
<context position="2823" citStr="Grosz and Sidner (1986)" startWordPosition="444" endWordPosition="447">e 1. (Because of space considerations, the fragments are somewhat abbreviated.) Many approaches to text organization&apos; are based on analyses of text in terms of rhetorical structure. However, there are few segments of text with interesting rhetorical structure in my corpus. For example, an analysis of the texts using Mann and Thompson&apos;s (1987) Rhetorical Structure Theory (RST) would result primarily in the relations sequence and joint and would contain few of the the relations like evidence or justify that give RST its descriptive power. Similarly, it is unclear what work a system like that of Grosz and Sidner (1986) would do in analyzing a description. Since the structure of descriptions cannot be analyzed adequately with rhetorical relations, perhaps it can be explained in terms of the domain. Houses, chips, and families are strongly structured. A family&apos;s relationships can be captured in a family tree; one might suppose that a description of the family would also be organized in this way. A house can be encoded in a number of ways; for instance, it has a component hierarchy, being composed of rooms composed of furnishings. Linde (1974) has proposed another comprehensive structure for houses: a phrase s</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. and C. Sidner, &amp;quot;Attention, Intentions, and the Structure of Discourse.&amp;quot; Computational Linguistics, (12)3, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Two Types of Planning in Language Generation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>179--186</pages>
<contexts>
<context position="28374" citStr="Hovy (1988)" startWordPosition="4634" endWordPosition="4635">roducing single texts in response to queries, some research has been more particularly concerned with interactive text. Much recent work in this area has been for explanation systems (e.g., Maybury, 1989), and some of this work explicitly addresses allowing a human user to ask follow-up questions (Moore &amp; Swartout, 1989). However, such systems still build and use global structures for extended texts. An argument is sometimes made that global structure is needed to capture high-level rhetorical goals in the output text (see Appelt, 1985); Gricean Maxims (Grice, 1975) are often invoked. But, as Hovy (1988) points out, &amp;quot;be polite&amp;quot; is not a decomposable goal; the objective of being polite is achieved through local decisions. Such local decisions can comfortably be integrated with a model of local organization of text. 126 Acknowledgements I thank Bonnie Webber and David Chapman for much useful discussion in the preparation of this paper. I also thank Bruce Leban for helping to make the Mac draw the picture. This work was supported in part by the Air Force Systems Command, Rome Air Development Center, Griffiss AFB, New York, 13441 under contract No. F30602-85-C-0008, as part of the Northeast Artif</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Hovy, E., &amp;quot;Two Types of Planning in Language Generation.&amp;quot; In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pp 179-186, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Unresolved Issues In Paragraph Planning.&amp;quot; To appear in</title>
<date>1989</date>
<booktitle>Proceedings of the Second European Natural Language Generation Workshop,</booktitle>
<contexts>
<context position="25920" citStr="Hovy, 1989" startWordPosition="4230" endWordPosition="4231">ns to result in the ungrammatical resumptive pronoun &amp;quot;they.&amp;quot; Locally-organized, occasionally ungrammatical text may be prototypical, but it is certainly not the only sort 4Note that the previous window conflict was not resolved by saying that the &amp;quot;wall&amp;quot; was between the two windows. The difference can be explained by the observation that the &amp;quot;wall,&amp;quot; despite its extent into the room, is a structural object, while the bureau is furniture. of text we wish to generate. To be comprehensive, my system will require some capability to post-process text after it is organized and before it is generated (Hovy, 1989, suggests a post-processing model). Output of my system might also be appropriate input for a text revision system (e.g., Meteer &amp; McDonald, 1986). Related research There are many other projects whose goal is to organize, or plan, extended text. The main difference between these and mine is flexibility and level of organization: most text planning systems rely on global structures to organize paragraph-sized text. These structures, which are usually schemas or plans, constrain the text to reflect particular rhetorical and domain relations, but at the expense of flexibility. My system builds s</context>
</contexts>
<marker>Hovy, 1989</marker>
<rawString>Hovy, E., &amp;quot;Unresolved Issues In Paragraph Planning.&amp;quot; To appear in Proceedings of the Second European Natural Language Generation Workshop, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kempen</author>
<author>E Hoenkamp</author>
</authors>
<title>An Incremental Procedural Grammar for Sentence Formulation.&amp;quot;</title>
<date>1987</date>
<journal>Cognitive Science</journal>
<volume>11</volume>
<pages>201--258</pages>
<contexts>
<context position="24201" citStr="Kempen &amp; Hoenkamp (1987)" startWordPosition="3961" endWordPosition="3964">ar to that already discussed. Incremental generation The organizer described here is composed of strategies that are often responsible for sub-clausal units of text; furthermore, the strategies have already imposed an organization on the text, obviating the need for the generator to do more than enforce grammaticality. The model of local text organization I am developing is coupled with a model of incremental generation, in which the increments are often smaller than a sentence. (Garrett&apos;s investigation of speech errors (1975) constitutes early work in this area; De Smedt &amp; Kempen (1987), and Kempen &amp; Hoenkamp (1987) discuss a similar, more fully-developed incremental generation project.) A typical generator produces a sentence at a time. However, spoken text is replete with restarts, fragments, and ungrammaticalities. This suggests that not only do people organize their text incrementally, but generate it in increments as well. Usually, an incremental generator is successful in generating grammatical text. The text then / in the kitchen / there is a large window is the result of three sequentially operating strategies: introduce/shift-attention, situate, and mentionsalient-object. An incremental generato</context>
</contexts>
<marker>Kempen, Hoenkamp, 1987</marker>
<rawString>Kempen, G. and E. Hoenkamp, &amp;quot;An Incremental Procedural Grammar for Sentence Formulation.&amp;quot; Cognitive Science 11, pp 201-258, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Laird</author>
<author>A Newell</author>
<author>P Rosenbloom</author>
</authors>
<title>Soar: An Architecture for General Intelligence.&amp;quot;</title>
<date>1987</date>
<journal>Artificial Intelligence,</journal>
<volume>33</volume>
<pages>1--64</pages>
<marker>Laird, Newell, Rosenbloom, 1987</marker>
<rawString>Laird, J., A. Newell, and P. Rosenbloom, &amp;quot;Soar: An Architecture for General Intelligence.&amp;quot; Artificial Intelligence, 33, pp 1-64, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Levinson</author>
<author>Pragmatics</author>
</authors>
<date>1983</date>
<publisher>Cambridge University Press,</publisher>
<marker>Levinson, Pragmatics, 1983</marker>
<rawString>Levinson, S., Pragmatics. Cambridge University Press, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Linde</author>
</authors>
<title>The Linguistic Encoding of Spatial Information. Doctoral Dissertation,</title>
<date>1974</date>
<location>Columbia University,</location>
<contexts>
<context position="3355" citStr="Linde (1974)" startWordPosition="538" endWordPosition="539">Similarly, it is unclear what work a system like that of Grosz and Sidner (1986) would do in analyzing a description. Since the structure of descriptions cannot be analyzed adequately with rhetorical relations, perhaps it can be explained in terms of the domain. Houses, chips, and families are strongly structured. A family&apos;s relationships can be captured in a family tree; one might suppose that a description of the family would also be organized in this way. A house can be encoded in a number of ways; for instance, it has a component hierarchy, being composed of rooms composed of furnishings. Linde (1974) has proposed another comprehensive structure for houses: a phrase structure grammar that determines how the rooms may be visited in a traversal of a house layout. Surprisingly, these global, hierarchical domain structures are not exploited in the organization of descriptions in my corpus. While family trees and composition hierarchies can be inferred from descriptions of families and houses, this does not mean that these structures guide the process of organizing them. For instance, my family informants did not simply construct their descriptions by starting at the root of the appropriate tre</context>
<context position="8358" citStr="Linde (1974)" startWordPosition="1392" endWordPosition="1393">something, often many things, that can be said next. In structured domains, there is always a default choice for the next thing to say. In spatial domains like houses, spatial proximity provides this default. Everything in a house, be it a room, a wall, or a kitchen appliance, is next to something else. Spatial proximity does not constrain house descriptions, 121 but it ensures that a description does not come to a premature dead end. Descriptions are finite. Though there may always be more to say, there are points at which a description may stop, when the task may be considered accomplished. Linde (1974) proposed a completeness criterion for house descriptions, which is reflected in my data as well as hers. It states that a description may stop any time after all the rooms have been mentioned, but it is not complete if it stops before. A similar criterion holds in the family descriptions collected: they were given in answer to the question, &amp;quot;Can you tell me how everyone who comes to Thanksgiving is related to each other?&amp;quot; In this case, then, the criterion is mentioning everyone who attends. Knowing how to continue and knowing when to stop together ensure that a description can be generated de</context>
</contexts>
<marker>Linde, 1974</marker>
<rawString>Linde, C., The Linguistic Encoding of Spatial Information. Doctoral Dissertation, Columbia University, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical Structure Theory: A Theory of Text Organization.</title>
<date>1987</date>
<tech>Report ISI/RS-87-190,</tech>
<institution>Information Sciences Institute,</institution>
<contexts>
<context position="27384" citStr="Mann &amp; Thompson, 1987" startWordPosition="4466" endWordPosition="4469">ample, constituency) for objects in a knowledge base. Paris has extended this work to use a process trace which derives its structure in part from the domain (Paris &amp; McKeown, 1987; Paris, 1988). This alternative strategy builds and traverses a path through the knowledge base when it is determined, on the basis of a sparse model of user expertise, that a process description of an object is more appropriate than a declarative one; the two strategies may be interleaved. Dale (1989) similarly organizes text by means of a domain structure, in this case, a recipe plan. Rhetorical Structure Theory (Mann &amp; Thompson, 1987) is also drawn from an analysis of written texts; it differs from McKeown&apos;s work in that it is composed of a large number of rhetorical relations, rather than a small number of schemas. RST may thus be more flexible, but is is still assumed that the relations will be combined into a single global tree covering an extended text. While most text planning systems have worked on producing single texts in response to queries, some research has been more particularly concerned with interactive text. Much recent work in this area has been for explanation systems (e.g., Maybury, 1989), and some of thi</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>Mann, W. and S. Thompson, Rhetorical Structure Theory: A Theory of Text Organization. Report ISI/RS-87-190, Information Sciences Institute, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maybury</author>
</authors>
<title>Enhancing Explanation Coherence with Rhetorical Strategies.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the European Association of Computational Linguistics„</booktitle>
<pages>168--173</pages>
<contexts>
<context position="27967" citStr="Maybury, 1989" startWordPosition="4568" endWordPosition="4569"> Theory (Mann &amp; Thompson, 1987) is also drawn from an analysis of written texts; it differs from McKeown&apos;s work in that it is composed of a large number of rhetorical relations, rather than a small number of schemas. RST may thus be more flexible, but is is still assumed that the relations will be combined into a single global tree covering an extended text. While most text planning systems have worked on producing single texts in response to queries, some research has been more particularly concerned with interactive text. Much recent work in this area has been for explanation systems (e.g., Maybury, 1989), and some of this work explicitly addresses allowing a human user to ask follow-up questions (Moore &amp; Swartout, 1989). However, such systems still build and use global structures for extended texts. An argument is sometimes made that global structure is needed to capture high-level rhetorical goals in the output text (see Appelt, 1985); Gricean Maxims (Grice, 1975) are often invoked. But, as Hovy (1988) points out, &amp;quot;be polite&amp;quot; is not a decomposable goal; the objective of being polite is achieved through local decisions. Such local decisions can comfortably be integrated with a model of local </context>
</contexts>
<marker>Maybury, 1989</marker>
<rawString>Maybury, M., &amp;quot;Enhancing Explanation Coherence with Rhetorical Strategies.&amp;quot; In Proceedings of the European Association of Computational Linguistics„ pp 168-173, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Text Generation.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="26641" citStr="McKeown (1985)" startWordPosition="4342" endWordPosition="4343">n system (e.g., Meteer &amp; McDonald, 1986). Related research There are many other projects whose goal is to organize, or plan, extended text. The main difference between these and mine is flexibility and level of organization: most text planning systems rely on global structures to organize paragraph-sized text. These structures, which are usually schemas or plans, constrain the text to reflect particular rhetorical and domain relations, but at the expense of flexibility. My system builds structure locally with no recourse or reference to an overall structure. Through analysis of written texts, McKeown (1985) has developed a small set of schemas, built from rhetorical predicates, that would provide types of descriptions (for example, constituency) for objects in a knowledge base. Paris has extended this work to use a process trace which derives its structure in part from the domain (Paris &amp; McKeown, 1987; Paris, 1988). This alternative strategy builds and traverses a path through the knowledge base when it is determined, on the basis of a sparse model of user expertise, that a process description of an object is more appropriate than a declarative one; the two strategies may be interleaved. Dale (</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, K., Text Generation. Cambridge University Press, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M</author>
<author>D McDonald</author>
</authors>
<title>A Model of Revision in Natural Language Generation.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>90--96</pages>
<marker>M, McDonald, 1986</marker>
<rawString>Meteer (Vaughan), M. and D. McDonald, &amp;quot;A Model of Revision in Natural Language Generation.&amp;quot; In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pp 90-96, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
<author>D McDonald</author>
<author>S Anderson</author>
<author>D Forster</author>
<author>L Gay</author>
<author>A Huettner</author>
<author>P Sibun</author>
</authors>
<date>1987</date>
<booktitle>MUMBLE-86: Design and Implementation. COINS</booktitle>
<tech>Technical Report 87-87,</tech>
<institution>University of Massachusetts,</institution>
<contexts>
<context position="10556" citStr="Meteer et al., 1987" startWordPosition="1745" endWordPosition="1748">ms of language, because time constraints and the inability to edit what has already been said make post-processing impossible. Computational architecture I am implementing a system that employs local organization of text as described in this paper. The implementation comprises: a semantic net knowledge base; an organizer composed of strategies and metastrategies; and a generator. Local organization is achieved using short-range strategies, each of which is responsible for organizing only a short segment of text, between a word and a clause in length. Until recently, I have employed Mumble-86 (Meteer et al., 1987) as the generator for this system. Construction is underway, however, on a simpler generator that more accurately implements the principles of generation implied by the structure of the organizer. The system is currently implemented only for descriptions of houses; the examples in this section will thus be drawn from the house domain. The knowledge base is a semantic net that encodes the objects, properties, and relations of a domain. The organizer keeps a pointer to the current node in the description. A strategy describes the current node and others related to it via local connections in the</context>
</contexts>
<marker>Meteer, McDonald, Anderson, Forster, Gay, Huettner, Sibun, 1987</marker>
<rawString>Meteer, M., D. McDonald, S. Anderson, D. Forster, L. Gay, A. Huettner, and P. Sibun, MUMBLE-86: Design and Implementation. COINS Technical Report 87-87, University of Massachusetts, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Moore</author>
<author>W Swartout</author>
</authors>
<title>A Reactive Approach to Explanation.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the International Joint Conference on Text Generation IJCAI,</booktitle>
<contexts>
<context position="28085" citStr="Moore &amp; Swartout, 1989" startWordPosition="4585" endWordPosition="4588">rk in that it is composed of a large number of rhetorical relations, rather than a small number of schemas. RST may thus be more flexible, but is is still assumed that the relations will be combined into a single global tree covering an extended text. While most text planning systems have worked on producing single texts in response to queries, some research has been more particularly concerned with interactive text. Much recent work in this area has been for explanation systems (e.g., Maybury, 1989), and some of this work explicitly addresses allowing a human user to ask follow-up questions (Moore &amp; Swartout, 1989). However, such systems still build and use global structures for extended texts. An argument is sometimes made that global structure is needed to capture high-level rhetorical goals in the output text (see Appelt, 1985); Gricean Maxims (Grice, 1975) are often invoked. But, as Hovy (1988) points out, &amp;quot;be polite&amp;quot; is not a decomposable goal; the objective of being polite is achieved through local decisions. Such local decisions can comfortably be integrated with a model of local organization of text. 126 Acknowledgements I thank Bonnie Webber and David Chapman for much useful discussion in the p</context>
</contexts>
<marker>Moore, Swartout, 1989</marker>
<rawString>Moore, J. and W. Swartout, &amp;quot;A Reactive Approach to Explanation.&amp;quot; In Proceedings of the International Joint Conference on Text Generation IJCAI, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
</authors>
<title>Tailoring Object Descriptions to a User&apos;s Level of Expertise.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<volume>14</volume>
<issue>3</issue>
<pages>64--78</pages>
<contexts>
<context position="26956" citStr="Paris, 1988" startWordPosition="4395" endWordPosition="4396"> structures, which are usually schemas or plans, constrain the text to reflect particular rhetorical and domain relations, but at the expense of flexibility. My system builds structure locally with no recourse or reference to an overall structure. Through analysis of written texts, McKeown (1985) has developed a small set of schemas, built from rhetorical predicates, that would provide types of descriptions (for example, constituency) for objects in a knowledge base. Paris has extended this work to use a process trace which derives its structure in part from the domain (Paris &amp; McKeown, 1987; Paris, 1988). This alternative strategy builds and traverses a path through the knowledge base when it is determined, on the basis of a sparse model of user expertise, that a process description of an object is more appropriate than a declarative one; the two strategies may be interleaved. Dale (1989) similarly organizes text by means of a domain structure, in this case, a recipe plan. Rhetorical Structure Theory (Mann &amp; Thompson, 1987) is also drawn from an analysis of written texts; it differs from McKeown&apos;s work in that it is composed of a large number of rhetorical relations, rather than a small numbe</context>
</contexts>
<marker>Paris, 1988</marker>
<rawString>Paris, C., &amp;quot;Tailoring Object Descriptions to a User&apos;s Level of Expertise.&amp;quot; Computational Linguistics 14(3), pp 64-78, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
<author>K McKeown</author>
</authors>
<title>Discourse Strategies for Describing Complex Physical Objects.&amp;quot;</title>
<date>1987</date>
<booktitle>In G. Kempen (ed), Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, Martinus Nijhoff Publishers,</booktitle>
<pages>97--115</pages>
<contexts>
<context position="26942" citStr="Paris &amp; McKeown, 1987" startWordPosition="4391" endWordPosition="4394">graph-sized text. These structures, which are usually schemas or plans, constrain the text to reflect particular rhetorical and domain relations, but at the expense of flexibility. My system builds structure locally with no recourse or reference to an overall structure. Through analysis of written texts, McKeown (1985) has developed a small set of schemas, built from rhetorical predicates, that would provide types of descriptions (for example, constituency) for objects in a knowledge base. Paris has extended this work to use a process trace which derives its structure in part from the domain (Paris &amp; McKeown, 1987; Paris, 1988). This alternative strategy builds and traverses a path through the knowledge base when it is determined, on the basis of a sparse model of user expertise, that a process description of an object is more appropriate than a declarative one; the two strategies may be interleaved. Dale (1989) similarly organizes text by means of a domain structure, in this case, a recipe plan. Rhetorical Structure Theory (Mann &amp; Thompson, 1987) is also drawn from an analysis of written texts; it differs from McKeown&apos;s work in that it is composed of a large number of rhetorical relations, rather than</context>
</contexts>
<marker>Paris, McKeown, 1987</marker>
<rawString>Paris, C. and K. McKeown, &amp;quot;Discourse Strategies for Describing Complex Physical Objects.&amp;quot; In G. Kempen (ed), Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, Martinus Nijhoff Publishers, pp 97-115, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichman</author>
</authors>
<title>Getting Computers to Talk Like You and Me.</title>
<date>1985</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="13036" citStr="Reichman (1985)" startWordPosition="2127" endWordPosition="2128">he knowledge base items themselves are determined by the domain. While the strategies are simple, complex behavior emerges from their interaction with the knowledge base; this locally organizes the extended text. Each strategy falls into one of four classes, with varying degrees of domain independence: discourse cue strategies; linguistic strategies; parameterizable domainindependent strategies; and semi-domain-independent strategies. Figure 2 gives examples of each. Of the domain-independent strategies, discourse cue strategies focus attention, in a way similar to the clue words described by Reichman (1985), and linguistic strategies mention objects and associated properties. mention-salient-object, used to say &amp;quot;there is a window,&amp;quot; may as easily express &amp;quot;there is a penguin&amp;quot; or &amp;quot;there is a policy.&amp;quot; describe-object is similarly allpurpose, and can produce &amp;quot;the window with two flanking windows&amp;quot; or &amp;quot;the man with one black shoe.&amp;quot; The parameterizable domain-independent strategies have slightly different textual realizations in different domains, but these differences can be captured by parameters. The example given in figure 2 is typical: the strategy is realized as a prepositional phrase in each doma</context>
</contexts>
<marker>Reichman, 1985</marker>
<rawString>Reichman, R., Getting Computers to Talk Like You and Me. MIT Press, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sibun</author>
<author>A Huettner</author>
</authors>
<title>Spatial Deixis in Generating Descriptions.&amp;quot;</title>
<date>1989</date>
<tech>COINS Technical Report 89-34,</tech>
<institution>Department of Computer and Information Science, University of Massachusetts,</institution>
<contexts>
<context position="19464" citStr="Sibun &amp; Huettner, 1989" startWordPosition="3155" endWordPosition="3158">ect rather than physical proximity; this is suggested by the speaker&apos;s typically using the preposition &amp;quot;with,&amp;quot; rather than an obviously spatial one. The global context also of course includes the completeness criterion which is unsatisfied throughout this stretch of text. The fragment starts when the speaker has just finished describing the kitchen and the next spatially proximate thing is the door to the bedroom. When the node to be described is a physical object, 2For a fuller treatment of how the system computes deictic and other spatial terms like &amp;quot;kitty corner,&amp;quot; &amp;quot;left,&amp;quot; and &amp;quot;right,&amp;quot; see (Sibun &amp; Huettner, 1989). the mention-object strategy is always available; because this object is a door, mention-room is available to talk about the room that the door leads into. The metastrategies resolve this conflict in favor of the more particular mention-room {1}. Because the last strategy used was mention-room, mention-salient-object becomes available; if there is a particularly &amp;quot;salient&amp;quot; object in a room, descriptions can then be organized around it. As it happens, salience in this domain tends to depend primarily on size; the kingsize bed fits the bill {2}. There are two objects spatially proximate to the b</context>
</contexts>
<marker>Sibun, Huettner, 1989</marker>
<rawString>Sibun, P. and A. Huettner, &amp;quot;Spatial Deixis in Generating Descriptions.&amp;quot; COINS Technical Report 89-34, Department of Computer and Information Science, University of Massachusetts, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ullmer-Ehrich</author>
</authors>
<title>The Structure of Living Space Descriptions.&amp;quot;</title>
<date>1982</date>
<editor>In R. Jarvella and W. Klein, eds., Speech, Place, and Action,</editor>
<publisher>John Wiley &amp; Sons, Ltd.,</publisher>
<contexts>
<context position="22325" citStr="Ullmer-Ehrich (1982)" startWordPosition="3634" endWordPosition="3635"> While the next window is a promising candidate because it is the same sort of thing as that just mentioned, the parameter for mentioning furniture overrides this, and the cedar chests come next in the description {7}, followed by their associated &amp;quot;stuff&amp;quot; {8}. The window is again available but so is the bureau, which is the preferred choice because it is furniture {9}. The bureau has spatial proximity links to two windows and the &amp;quot;small thing,&amp;quot; as well as links to its &amp;quot;stuff,&amp;quot; so the set of available strategies comprises ones that mention each of these things. A metastrategy can resolve this 2Ullmer-Ehrich (1982) notes a similar tendency in the dorm room descriptions that she collected. 124 and then there&apos;s the bedroom {1} and there&apos;s a huge kingsize bed {2} and there&apos;s an endtable next to it {3} with a lamp and a clock radio {4} and some stuff of mine like the Boston University cup and...then there&apos;s a wall-- {5} and then there&apos;s a window behind that {6} and there&apos;s a wall, and there&apos;s another window and there&apos;s some.. .cedar chests of Carol&apos;s {7} that have blankets and sheets in them {8} and...there&apos;s her bureau {9} in the middle of two windows on either side {10} with all of her makeup on top of it</context>
</contexts>
<marker>Ullmer-Ehrich, 1982</marker>
<rawString>Ullmer-Ehrich, V., &amp;quot;The Structure of Living Space Descriptions.&amp;quot; In R. Jarvella and W. Klein, eds., Speech, Place, and Action, John Wiley &amp; Sons, Ltd., 1982.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>