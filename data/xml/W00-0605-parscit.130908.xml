<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.9977065">
A Question Answering System Developed as a Project in a
Natural Language Processing Course*
</title>
<note confidence="0.498982">
W. Wang, J. Auer, R. Parasuraman, I. Zubarev, D. Brandyberry, and M. P. Harpei
Purdue University
West Lafayette, IN 47907
</note>
<email confidence="0.979249">
{wang28,jauer,pram,dbrandyb,harper}@ecn.purdue.edu and zubarevi@cs.purdue.edu
</email>
<sectionHeader confidence="0.986593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995965">
This paper describes the Question Answering Sys-
tem constructed during a one semester graduate-
level course on Natural Language Processing (NLP).
We hypothesized that by using a combination of syn-
tactic and semantic features and machine learning
techniques, we could improve the accuracy of ques-
tion answering on the test set of the Remedia corpus
over the reported levels. The approach, although
novel, was not entirely successful in the time frame
of the course.
</bodyText>
<figure confidence="0.9794558">
---
Plain Text ( Story and Questions)
—
Brill POS Tagger
Tagged Text
•
2 i Name Identification
Word Lomeli
Lexiced and Role
— • ink&amp;quot;rnatj*n ------- Label Intonation...7
Lexicon --
Grammar Partial
WOxSel Rules Parser
•
Grammar Pronouns
Resolved
Propernoun Identified
Tagged Text
Pronoun
Resolution
</figure>
<sectionHeader confidence="0.870929" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895772727273">
This paper describes a preliminary reading com-
prehension system constructed as a semester-long
project for a natural language processing course.
This was the first exposure to this material for
all but one student, and so much of the semester
was spent learning about and constructing the tools
that would be needed to attack this comprehen-
sive problem. The course was structured around
the project of building a question answering system
following the HumSent evaluation as used by the
Deep Read system (Hirschman et al., 1999). The
Deep Read reading comprehension prototype system
(Hirschman et al., 1999) achieves a level of 36% of
the answers correct using a bag-of-words approach
together with limited linguistic processing. Since the
average number of sentences per passage is 19.41,
this performance is much better than chance (i.e.,
5%). We hypothesized that by using a combina-
tion of syntactic and semantic features and machine
learning techniques, we could improve the accuracy
of question answering on the test set of the Remedia
corpus over these reported levels.
</bodyText>
<sectionHeader confidence="0.967317" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.99813675">
The overall architecture of our system is depicted
in Figure 1. The story sentences and its five ques-
tions (who, what, where, when, and why) are first
preprocessed and tagged by the Brill part-of-speech
</bodyText>
<footnote confidence="0.5200555">
* We would like to thank the Deep Read group for giving us
access to their test bed.
</footnote>
<figure confidence="0.997383">
•
Sentence-to-Question
5 I
Comparison
Voting
Answer with
Highest Scores
</figure>
<figureCaption confidence="0.994915">
Figure 1: The architecture for our question answer-
ing system.
</figureCaption>
<bodyText confidence="0.998971428571429">
(POS) tagger distributed with the Deep Read sys-
tem. This tagged text is then passed to the Name
Identification Module, which updates the tags of
named entities with semantic information and gen-
der when appropriate. - The Partial Parser Mod-
ule then takes this updated text and breaks it into
phrases while attempting to lexically disambiguate
the text. The Pronoun Resolution Module is con-
sulted by the parser in order to resolve pronouns be-
fore passing partially parsed sentences and questions
to the Sentence-to-Question Comparison Module.
The Comparison Module determines how strongly
the phrases of a sentence are related to those of a
question, and this information is passed to several
</bodyText>
<figure confidence="0.6040588">
Rule-Based Neuro-- Neural Genetic
Classifier Fuzzy Net Network Algorithm .&apos;
- - ANS ANS
•
ANS ANS
</figure>
<page confidence="0.998025">
28
</page>
<bodyText confidence="0.999959647058823">
modules which attempt to learn which features of
the comparison are the most important for identify-
ing whether a sentence is a strong answer candidate.
We intended to set up a voting scheme among vari-
ous modules; however, this part of the work has not
been completed (as indicated by the dashed lines).
Our system, like Deep Read, uses as the develop-
ment set 28 stories from grade 2 and 27 from grade
5, each with five short answer questions (who, what,
when, where, and why), and 60 stories with ques-
tions from grades 3 and 4 for testing 1. We will refer
to the development and testing data as the Remedia
corpus. The following example shows the informa-
tion added to a plain text sentence as it progresses
through each module of the system we have created.
Each module is described in more detail in the fol-
lowing sections.
</bodyText>
<subsectionHeader confidence="0.676102">
2.1 Name Identification Module
</subsectionHeader>
<bodyText confidence="0.995634583333333">
The Name Identification Module expects as in-
put a file that has been tagged by the Brill
tagger distributed with the Deep Read system.
The most important named entities in the Re-
media corpus are the names of people and the
names of places. To distinguish between these
two types, we created dictionaries for names of
people and names of places. The first and last
name dictionaries were derived from the files
at http://www.census.gov/genealogy/names/.
First names had an associated gender feature;
names that were either male or female included
gender frequency. Place names were extracted from
atlases and other references, and included names
of countries, major cities and capital cities, major
attractions and parks, continents, etc. WordNet
was also consulted because of its coverage of place
names. There are 5,165 first name entries, 88,798
last name entries, and 1,086 place name entries in
the dictionaries used by this module.
The module looks up possible names to decide
whether a word is a person&apos;s name or a location. If it
cannot find the word in the dictionaries, it then looks
at the POS tags provided in the input file to deter-
mine whether or not it is a propernoun. Heuristics
(e.g., looking at titles like Mr. or word endings like
vine) are then applied to decide the semantic type
of the propernoun, and if the type cannot be deter-
mined, the module returns both person and location
as its type. The accuracy of the Name Identification
Module on the testing set was 79.6%. The accuracy
adjusted to take into account incorrect tagging was
83.6%.
&apos;There were differences between the Deep Read electronic
version of the passages and the Remedia published passages.
We used the electronic passages.
</bodyText>
<figure confidence="0.935095625">
Ilin3-5 (... The club is for boys who are under 12 years old.) They are called Cub Scouts.
(Answer to Question I)
POS tagging
. They are called •
POS=&amp;quot;PRP&amp;quot; POS=&amp;quot;VBP&amp;quot;
Name Identification
They • are called -----
POS=&amp;quot;PRP&amp;quot; POS=&amp;quot;VBP&amp;quot; • POS=&amp;quot;VBN&apos; Cub Scouts
P05.1.4145
, PROPTWE.Loeme.
Initial Partial Parsing
are called
TYPE.VP
10.2
LABEL.rnvb
BASE.call
AGR.3p
TENSE-paste
I VOICE-passive .
SEM TYPE.equste•
Pronoun Resolution
They are , called , Cub Scouts
TYPE.VP , TYPE.VP •
10.1 . I0.2
LABEL.aux 1 LABEL=nwb
BASE.be &apos; BASE.cell
AGR.3p i AOR.3p
TENSEmpresent , TENSE.putp
VOICEective . VOICE-P..0&apos;.
SEM_TYPE•be-aux . SEM_TYPE.equate.
Bracketed Sentence
are
• TYPE.VP
ID.I
LABEL.rnvb
BASE.be
AGR.3p
TENSE.present
VOICE.active
Bracketed Question 1
</figure>
<figureCaption confidence="0.9981345">
Figure 2: Processing an example sentence for match-
ing with a question in our system.
</figureCaption>
<subsectionHeader confidence="0.998478">
2.2 Partial Parser Module
</subsectionHeader>
<bodyText confidence="0.999390333333333">
The Partial Parser Module follows sequentially af-
ter the Name Identification Module. The input is
the set of story sentences and questions, such that
the words in each are tagged with POS tags and
the names are marked with type and gender infor-
mation. Initially pronouns have not been resolved;
the partial parser provides segmented text with rich
lexical information and role labels directly to the
Pronoun Resolution Module. After pronoun reso-
lution, the segmented text with resolved pronouns
is returned to the partial parser for the parser to
update the feature values corresponding to the pro-
nouns. Finally, the partial parser provides bracketed
text to the Comparison Module, which extracts fea-
tures that will be used to construct modules for an-
swering questions.
The Partial Parser Module utilizes information
in a lexicon and a grammar to provide the partial
</bodyText>
<figure confidence="0.999382137931034">
POS=&amp;quot;VBN&amp;quot;•
Cub Scouts
POS,21■1Nr POS=&amp;quot;NNS&amp;quot;,
TYPE=NP
10=1
LABEL.subjeot
BASE=Ihey
AGR=Llp
GENDER=male
SEM_TYPE*erson.
TYPE.VP
10=1
LABEL.aux
BASE..
AGR.3p
TENSE.present
VOICE=adive
SEILt_TYPE.beema
They
Cub Scouts
;
TYPE =NP
, 10=2
LABEL=object
&apos; BASE=CubScouts
AGR=3p
GENDER=male
SEM TYPE5,ersonj
TYPE.NP
10.1
LABEL.subj.. .
:
AGR.3p
GENDER.neutral
; SEJe_TYPE.person •
PRONFIEZ,boys
They
TYPE=NP
, 10=1
LABEL=subject
BASE.Joy
, AGR=3p
GENDER=Inale
• SEM,Ty_pg_q,erso_n.
Who
&apos; TYPE=NP
10=1
• LABEL=subject
• BASE=who
; AGR..3p
; GENDER=male
TypEverson
TYPE.VP
BASE..
00R..31,
TENSE.p.sem
V010E...We
TYPE.VP
10.2
LABEL.rtwb
BASE.cs.
i AOR.3o
TENSE.1..1P
L SEM TYPE.e
• Updating Features
TYPE.NP
LABEL.object
BASE=CubScouts
• AGR.3p
GENDER.male
• SEM_TYPE.P.0.0
are called • Cub Scouts &apos;
TYPE.NP
10=2
LABEL=objecl
BASE=CubSocuts &apos;
AGF1=3p
GENDER=rnale :
SEM_TYPE=pusce&apos;
Cub Scouts
&apos;--TYPE=NP
10=2
LABELwabject
BASE=CubScouts
AGR=4
GENDER:4,We
TA=1-Y.P_E±-Pflol
</figure>
<page confidence="0.994257">
29
</page>
<bodyText confidence="0.9995465">
parses. The lexicon and the parser will be detailed
in the next two subsections.
</bodyText>
<subsubsectionHeader confidence="0.973521">
2.2.1 The Lexicon
</subsubsectionHeader>
<bodyText confidence="0.999311807017544">
There were two methods we used to construct the
lexicon: open lexicon, which includes all words
from the development set along with all determiners,
pronouns, prepositions, particles, and conjunctions
(these words are essential to achieving good sentence
segmentation), and closed lexicon, which includes
all of the development and testing words2. We con-
structed the closed lexicon with the benefit of the
development corpus only (i.e., we did not consult the
test materials to design the entries). To improve cov-
erage in the case of the open lexicon, we constructed
a module for obtaining features for words that do
not appear in the development set (unknown words)
that interfaces with WordNet to determine a word&apos;s
base/stem, semantic type, and synonyms. When an
unknown word has multiple senses, we have opted to
choose the first sense because WordNet orders senses
by frequency of use. Ignoring numbers, there are
1,999 unique words in the development set of the
Remedia corpus, and 2,067 in the testing data, of
which 1,008 do not appear in the development set.
Overall, there are 3,007 unique words across both
training and testing.
One of our hypotheses was that by creating a lex-
icon with a rich set of features, we would improve
the accuracy of question answering. The entries in
the lexicon were constructed using the conventions
adopted for the Parsec parser (Harper and Belzer-
man, 1995; Harper et al., 1995; Harper et al., 2000).
Each word entry contains information about its root
word (if there is one), its lexical category (or cate-
gories) along with a corresponding set of allowable
features and their corresponding values. Lexical cat-
egories include noun, verb, pronoun, propernoun,
adjective, adverb, preposition, particle, conjunction,
determiner, cardinal, ordinal, predeterminer, noun
modifier, and month. Feature types used in the
lexicon include subcat, gender, agr, case, vtype
(e.g., progressive), mood, gap, inverted, voice,
behavior (e.g., mass), type (e.g., interrogative, rel-
ative), semtype, and conjtype (e.g., noun-type,
verb-type, etc.). We hypothesized that semtype
should play a significant role in improving question
answering performance, but the choice of semantic
granularity is a difficult problem. We chose to keep
the number of semantic values relatively small. By
using the lexicographers&apos; files in WordNet to group
the semantic values, we selected 25 possible seman-
tic values for the nouns and 15 for the verbs. A
2Initially, we created the closed lexicon because this list
of words was in the Deep Read materials. Once we spotted
that the list contained words not in the development material,
we kept it as an alternative to see how important full lexical
knowledge would be for answering questions.
script was created to semi-automate the construc-
tion of the lexicon from information extracted from
previously existing dictionaries and from WordNet.
</bodyText>
<subsubsectionHeader confidence="0.939409">
2.2.2 The Partial Parser
</subsubsectionHeader>
<bodyText confidence="0.994118583333333">
The parser segments each sentence into either a noun
phrase (NP), a verb phrase (VP), or a prepositional
phrase (PP), each with various feature sets. NPs
have the feature types: Base (the root word of the
head word of the NP), AGR (number/person infor-
mation), SemType (the semtype of the root form in
the lexicon, e.g., person, object, event, artifact, or-
ganization), Label (the role type of the word in the
sentence, e.g., subject), and Gender. Verb phrases
(VPs) have the feature types: Base, AGR, SemType
(the semtype of the root form in the lexicon, e.g.,
contact, act, possession), Tense (e.g., present, past),
and Voice. Prepositional phrases (PPs) have the
feature types: Prep (the root form of the preposition
word), SemType (the semtype of the root form in the
lexicon, e.g., at-lc, at-time), Need (the object of the
preposition), and NeedSemType (the semtype of the
object of the preposition). Feature values are as-
signed using the lexicon, Pronoun Resolution Mod-
ule, and grammar rules.
We implemented a bottom-up partial parser to
segment each sentence into syntactic subparts. The
grammar used in the bottom-up parser is shown be-
low:
</bodyText>
<listItem confidence="0.999735428571429">
1. NP DET ADJ-t- NOUN+
2. NP DET NOUN
3. NP ADJ PROPERNOUN+
4. VP -4 (AUX-VERB) MAIN-VERB
5. PP -4 ADV
6. PP -4 ADJ (PRED)
7. PP —&gt; PREP NP
</listItem>
<bodyText confidence="0.998689818181818">
At the outset, the parser checks whether there are
any punctuation marks in the sentence, with com-
mas and periods being the most helpful. A comma
is used in two ways in the Remedia corpus: it acts
as a signal for the conjunction of a group of nouns
or propernouns, or it acts as punctuation signalling
an auxiliary phrase (usually a PP) or sentence. In
the NP conjunction case, the parser groups the con-
joined nouns or propernouns together as a plural NP.
In the second case, the sentence is partially parsed.
The partial parser operates in a bottom-up fashion
taking as input a POS-tagged and name-identified
sentence and matching it to the right-hand side of
the grammar rules. Starting from the beginning of
the sentence or auxiliary phrase (or sentence), the
parser looks for the POS tags of the words, trans-
forming the POS tags into corresponding lexical cat-
egories and tries to match the RHS of the rules.
Phrases are maintained on an agenda until they are
finalized.
NPs often require merging since some consecutive
NPs form a single multi-word token (i.e., multi-word
</bodyText>
<page confidence="0.99172">
30
</page>
<bodyText confidence="0.999918277777778">
names and conjunctions). An NP that results from
merging two tokens into a single multi-word token
has its Base as the rootword of the combined token,
and AGR. and SemType features are updated according
to the information retrieved from the lexicon based
on the multi-word token. In the case of an NP con-
junction, the Base is the union of the Base of each
NP, AGR is set to 3p, and SemType is assigned as that
of the head word of the merged NP. The rule for find-
ing the head word of an NP is: find the FIRST con-
secutive noun (propernoun) group in the NP, then
the LAST noun (propernoun) in this group is de-
fined as the head word of the NP.
The partial parser performs word-sense disam-
biguation as it parses. Words such as Washington
have multiple .semtype values in the lexicon for one
lexical category. The following are rules for word-
sense disambiguation used by the parser:
</bodyText>
<listItem confidence="0.843394">
• NP plus VP rules for word-sense disambiguation:
If there are verbs such as name, call, or be,
which have the semtype of equate, then the NPs
that precede and follow the VP have the same
semtype.
</listItem>
<bodyText confidence="0.89844925">
If a noun is the object of a verb, then the subcat
feature value of the verb can be used to disam-
biguate its word sense (e.g., take generally has
the subcat of obj+time).
</bodyText>
<listItem confidence="0.9684746">
• PP rules for word-sense disambiguation:
For some nouns (propernouns) which are the
object of a preposition, the intersection of the
semtype value sets of the preposition word and
its object determines their semtype.
• NPs in the date line of each passage are all ei-
ther dates or places with the typical order be-
ing place then time. For example, in (WASH-
INGTON, June, 1989), Washington is assigned
semtype of location rather than person.
</listItem>
<bodyText confidence="0.999546230769231">
To process unknown words (the 1,008 words in the
testing set that don&apos;t appear in the development set)
in the case of the open lexicon, WordNet is used to
assign the semtype feature for nouns and verbs, the
AGR feature for verbs can be obtained in part from
the POS tag, and AUL for unknown noun words can
be determined when they are used as the subject of
a sentence. For the closed lexicon, the only unknown
words are numbers. If a number is a four-digit num-
ber starting with 16 to 19 or is followed by A.D or
B.C. then generally it is a year, so its semtype is de-
fined as time. Other numbers tend to be modifiers
or predicates and have the semtype of num.
</bodyText>
<subsectionHeader confidence="0.991209">
2.3 Pronoun Resolution Module
</subsectionHeader>
<bodyText confidence="0.986893090909091">
A pronoun resolution module was developed using
the rules given in Allen&apos;s text (Allen, 1995) along
with other rules described in the work of Hobbs
(Hobbs, 1979). The module takes as input the
feature-augmented and segmented text provided by
the partial parser. Hence, the words are marked
with lexical (including gender) and semantic feature
information, and the phrase structure is also avail-
able. After the input file is provided by the Par-
tial Parser Module, the Pronoun Resolution Module
searches for the pronouns by looking through the
NPs identified by the partial parser. Candidate an-
tecedents are identified and a comparison of the fea-
tures is made between the pronoun and the possible
antecedent. The phrase that passes the most rule
filters is chosen as the antecedent. First and second
person pronouns are handled by using default val-
ues (i.e., writer and reader). If the system fails to
arrive at an antecedent, the pronoun is marked as
non-referential, which is often the case for pronouns
like it or they. Some of the most useful rules are
listed below:
</bodyText>
<listItem confidence="0.985073388888889">
• Reflexives must refer to an antecedent in the same
sentence. For simplicity, we chose the closest
noun preceding the pronoun in the sentence with
matching Gender, AGR, and SemType.
• Two NPs that co-refer must agree in AGR, Gender,
and SemType (e.g., person, location). Since, in
many cases the gender cannot be determined, this
information was used only when available.
• A subject was preferred over the object when the
pronoun occurred as the subject in a sentence.
• When it occurs in the beginning of a paragraph,
it is considered non-referential.
• We prefer a global entity (the first named entity in
a paragraph) when there is a feature match. In the
absence of such, we prefer the closest propernoun
preceding the pronoun with a feature match. If
that fails, we prefer the closest preceding noun or
pronoun with a feature match.
</listItem>
<bodyText confidence="0.999993">
The accuracy of our pronoun resolution module
on the training corpus was 79.5% for grade 2 and
79.4% for grade 5. On testing, it was 81.33% for
grade 3 and 80.39% for grade 4. The overall accu-
racy of this module on both the testing and train-
ing corpus was 80.17%. This was an improvement
over the baseline Deep Read coreference module
which achieved a 51.61% accuracy on training and
a 50.91% accuracy on testing, giving an overall ac-
curacy of 51.26%. This accuracy was determined
based on Professor Harper&apos;s manual pronoun reso-
lution of both the training and testing set (the per-
fect coreference information was not included in the
distribution of the Deep Read system).
</bodyText>
<subsectionHeader confidence="0.876523">
2.4 Sentence-to-Question Comparison
Module
</subsectionHeader>
<bodyText confidence="0.999792142857143">
The Sentence-to-Question Comparison Module
takes as input a set of tagged stories, for which
phrase types and features have been identified.
The semantic and syntactic information is coded as
shown in Figure 2 (using XML tags). A mechanism
to quantify a qualitative comparison of questions
and sentences has been developed. The comparison
</bodyText>
<page confidence="0.999741">
31
</page>
<bodyText confidence="0.9618213125">
provides data about how questions compare to their
answers and how questions compare to non-answers.
The classification of answers and non-answers is im-
plemented by using feature comparison vectors of
phrase-to-phrase comparisons in questions and po-
tential answer sentences.
A comparison is made using phrase-to-phrase
comparisons between each sentence and each ques-
tion in a passage. In particular, NP-to-NP, VP-to-
VP, PP-to-PP, and NP-to-PP comparisons are made
between each sentence and each of the five questions.
These comparisons are stored for each sentence in
the following arrays. Note that in these arrays Q
varies from 1 to 5, signifying the question that the
sentence matches. F varies over the features for the
phrase match.
</bodyText>
<figureCaption confidence="0.949983076923077">
CNNJ[F] Comparison of NP features (F = 1{Base,
AGR, and SemType}I) between question
Q and the sentence.
CV[Q][F] Comparison of VP features (F = I{Base,
AGR, SemType, Tense} l) between
question Q and the sentence.
CP[Q][F] Comparison of PP features (F = [{NeedBase,
Prep, PPSemType, NeedSemType}l)
between question Q and the sentence.
CPN[Q][F] Comparison of PP features in sentence
to NP features in question Q. Here F=2,
comparing ReedBase and Base, and
NeedSemType and SemType.
</figureCaption>
<bodyText confidence="0.999372045454545">
Values for these comparison matrices were calcu-
lated for each sentence by comparing the features of
each phrase type in the sentence to features of the
indicated phrase types in each of the five questions.
The individual matrix values describe the compari-
son of the best match between a sentence and a ques-
tion for NP-to-NP (the three feature match scores
for the best matching NP pair of the sentence and
question Q are stored in CN[Q]), VP-to-VP (stored
in CV[Q1), PP-to-PP (stored in CP[Q]), and PP-to-
NP (store in CPN[Q1). Selecting the phrase compar-
ison vector for a phrase type that best matches a
sentence phrase to a question phrase was chosen as
a heuristic to avoid placing more importance on a
sentence only because it contains more information.
Comparisons between features were calculated us-
ing the following equations. The first is used when
comparing features such as Base, NeedBase, and
Prep, where a partial match must be quantified.
The second is used when comparing features such as
SemType, AGR, and Tense where only exact matches
make sense.
</bodyText>
<construct confidence="0.966879666666667">
if Stri = Str2
length(Stri) length(Str2)
A(Stri E Str2 V Str2 E Stri
if Stri Str2
if Stri = Str2
if Stri Str2
</construct>
<bodyText confidence="0.999757857142857">
The matrices for the development set were pro-
vided to the algorithms in the Answer Module for
training the component answer classifiers. The ma-
trices for the testing set were also passed to the al-
gorithms for testing. Additionally, specific informa-
tion about the feature values for each sentence was
passed to the Answer Module.
</bodyText>
<subsectionHeader confidence="0.990012">
2.5 Answer Modules
</subsectionHeader>
<bodyText confidence="0.99969825">
Several methods were developed in parallel in an
attempt to learn the features that were central to
identifying the sentence from a story that correctly
answer a question. These methods are described in
the following subsections. Due to time constraints,
the evaluations of these Answer Modules were car-
ried out with a closed lexicon and perfect pronoun
resolution.
</bodyText>
<subsubsectionHeader confidence="0.58464">
2.5.1 A Neuro-Fuzzy Network Classifier
</subsubsectionHeader>
<bodyText confidence="0.999052974358974">
An Adaptive Network-based Fuzzy Inference System
(ANFIS) (Jang, 1993) from the Matlab Fuzzy Logic
Toolbox was used as one method to resolve the story
questions. A separate network was trained for each
question type in an attempt to make the networks
learn relationships between phrases that classify an-
swer sentences and non-answer sentences differently.
ANFIS has the ability to learn complex relationships
between its input variables. It was expected that
by learning the relationships in the training set, the
resolution of questions could be performed on the
testing set.
For ANFIS, the set of sentence-question pairs was
divided into five groups according to question type.
Currently the implementation of ANFIS on Matlab
is restricted to 4 inputs. Hence, we needed to devise
a way to aggregate the feature comparison informa-
tion for each comparison vector. The comparison
vectors for each phrase-to-phrase comparison were
reduced to a single number for each comparison pair
(i.e., NP-NP, VP-VP, PP-PP, NP-PP). This reduc-
tion was performed by multiplying the vector values
by a normalized weighting constant for the feature
values (e.g., NP-comparison = (Base weight)*(Base
comparison value) ± (AGR weight)*(AGR compari-
son value) (SemType weight)*(SemType compari-
son value), with the weights summing to 1). In most
cases that a match is found, the comparison values
are 1 (exact match). So weights were chosen that al-
lowed the ANFIS to tellSomething about the match
characteristics (e.g., if the AGR weight is 0.15 and
the SemType weight is 0.1, and the NP-comparison
value was 0.25, it can be concluded that the NP
that matched best between in the sentence-question
pair had the same AGR. and SemType features). The
aggregation weights were chosen so that all com-
binations of exact matches on features would have
unique values and the magnitude a the weights were
chosen based on the belief that the higher weighted
</bodyText>
<figure confidence="0.44866625">
1
min length(Stri ,Str2)
max length(Stri ,Str2)
0
</figure>
<page confidence="0.997957">
32
</page>
<bodyText confidence="0.999866275">
features contribute more useful information. The
weights, ordered to correspond to the features in the
table on the previous page are: (.55, .15, .3) for CN,
(.55, .1, .22, .13) for CV, (.55, .15, .2, .1) for CP, and
(.55, .45) for CPN.
ANFIS was trained using the update on the de-
velopment set provided by the Sentence-to-Question
Comparison Module as described above. During
testing, the data, provided by the Comparison Mod-
ule and updated as described above, is used as input
to ANFIS. The output is a confidence value that de-
scribes the likelihood of a sentence being a answer.
Every sentence is compared with every question in
ANFIS, and then within question, the sentences are
ranked by the likelihood that they are a question&apos;s
answer.
_
The accuracy of the best classifier produced with
ANFIS was quite poor. In the grade 3 set, we
achieved an accuracy of 13.33% on who questions,
6.67% on what questions, 0% on where questions,
6.67% on when questions, and 3.33% on why ques-
tions. In the grade 4 set, we achieved an accuracy of
3.54% on who questions, 10.34% on what questions,
10.34% on where questions, 0% on when questions,
and 6.9% on why questions. Although the best rank-
ing sentence produced poor accuracy results on the
testing set, with some additional knowledge the top-
ranking incorrect answers may be able to be elimi-
nated. The plots in Figure 3 display the number of
times the answer sentence was assigned a particular
rank by ANFIS. The rank of the correct sentence
tends to be in the top 10 fairly often for most ques-
tion types. This rank tendency is most noticeable
for who, what and when questions, but it is also
present for where questions. The rank distribution
for why questions appears to be random, which is
consistent with our belief that they require a deeper
analysis than would be possible with simple feature
comparisons.
</bodyText>
<subsectionHeader confidence="0.762761">
2.5.2 A Neural Network Classifier
</subsectionHeader>
<bodyText confidence="0.999949632653061">
Like ANFIS, this module uses a neural network, but
it has a different topology and uses an extended fea-
ture set. The nn (Neureka) neural network sim-
ulation system (Mat, 1998) was used to create a
multi-layer (one hidden layer) back-propagation net-
work. A single training/testing instance was gener-
ated from each story sentence. The network contains
an input layer with two groups of features. The sen-
tence/question feature vectors that compare a sen-
tence to each of the five story questions comprise the
first group. Sentence features that are independent
of the questions, i.e., contains a location, contains a
time/date, and contains a human, comprise the sec-
ond group. The hidden layer contains a number of
nodes that was experimentally varied to achieve best
performance. The output layer contains five nodes,
each of which has a binary outpiit value which indi-
cates whether or not the sentence is the answer to
the corresponding question (i.e., question 1 through
5).
Several training trials were performed to deter-
mine the optimum parameters for the network. We
trained using various subsets of the full input fea-
ture set since some features could be detrimental to
creating a good classifier. However, in the end, the
full set of features performed better than or equiva-
lently to the various subsets. Increasing the number
of hidden nodes can often improve the accuracy of
the network because it can learn more complex re-
lationships; however, this did not help much in the
current domain, and so the number of hidden nodes
was set to 16. For this domain, there are many more
sentences that are not the answer to a question than
that are. An effort was made to artificially change
this distribution by replicating the answer sentences
in the training set; however, no additional accuracy
was gained by this experimentation. Finally, we cre-
ated a neural network for each question type as in
ANFIS; however, these small networks had lower ac-
curacy than the single network approach.
The overall test set accuracy of the best neural
network classifier was 14%. In the grade 3 set, we
achieved an accuracy of 30% on who questions, 0%
on what questions, 23.3% on when questions, 13.3%
on where questions, and 3.3% on why questions. In
the grade 4 set, we achieved an accuracy of 17.2%
on who questions, 10.3% on what questions, 23.6%
on when questions, 10.3% on where questions, and
3.4% on why questions.
</bodyText>
<subsectionHeader confidence="0.467463">
2.5.3 A Rule-based Classifier based on C5.0
</subsectionHeader>
<bodyText confidence="0.999993909090909">
We attempted to learn rules for filtering out sen-
tences that are not good candidates as answers to
questions using C5.0 (Rul, 1999). First we ex-
tracted information from the sentence-to-question
correspondence data ignoring the comparison values
to make the input C5.0-compatible, and produced
five different files (one for each question type). These
files were then fed to C5.0; however, the program did
not produce a useful tree. The problem may have
been that most sentences in the passages are nega-
tive instances of answers to questions.
</bodyText>
<sectionHeader confidence="0.769072" genericHeader="method">
2.5.4 GAS
</sectionHeader>
<bodyText confidence="0.999557454545455">
GAS (Jelasity and Dombi, 1998) is a steady genetic
algorithm with subpopulation support. It is capa-
ble of optimizing functions with a high number of
local optima. The initial parameters were set theo-
retically. In the current matching problem, because
the number of local optima can be high due to the
coarse level of sentence information (there can be
several sentence candidates with very close scores),
this algorithm is preferred over other common ge-
netic algorithms. This algorithm was trained on the
training set, but due to the high noise level in the
</bodyText>
<page confidence="0.992945">
33
</page>
<figure confidence="0.991424263157895">
Who Questions
What Questions
.) &lt;3 A
N-best order of sentences
Where Questions
nnnn
A 9
N-best order of sentences
When Questions
J&apos;Jlln
N-best order of sentences
010
2
0 11.
eb &lt;3 A 9 415 ,1&gt; „.9 43
N-best order of sentences
Why Questions
n,
N-best order of sentences
</figure>
<figureCaption confidence="0.986202">
Figure 3: Correct answers ordered by ANFIS preference.
</figureCaption>
<figure confidence="0.8911855">
0.15
00
</figure>
<bodyText confidence="0.972673333333333">
training data, the algorithm fails to produce a win-
ning population based on the mean square minimiza-
tion function.
</bodyText>
<sectionHeader confidence="0.979341" genericHeader="method">
3 A Closer Look at the Features
</sectionHeader>
<bodyText confidence="0.999285347826087">
After observing the question answer accuracy results
of the above classifiers, we concluded that the fea-
tures we extracted for the classifiers are affected by
noise. The fact that we take into consideration only
the top matching phrase-to-phrase matches on a spe-
cific set of features may have contributed to this
noisiness. To analyze the noise source of features,
given that SemType was hypothesized to be essen-
tial for answer candidate discrimination, we exam-
ined those SemType values that occurred most fre-
quently and calculated statistics on how often the
values occurred in story sentences that are answers
versus non-answers to the questions. We observed
the following phenomena:
1. For who questions, the SemType value person
plays an important role in identifying answer
sentences, since 83.64% answers have person as
its NP SemType value, and 21.82% have it as
its PP NeedSemType value. However, 66.83% of
the non-answer sentences also have person as its
NP SemType and 15.85% as its PP NeedSemType.
Phrases with person SemType appear in most sen-
tences, whether they are answers or not, and this
</bodyText>
<page confidence="0.996502">
34
</page>
<bodyText confidence="0.992565166666667">
weakens its ability to act as an effective filter.
2. For what questions, the SemType value person ap-
pears as the NP SemType of most answer and non-
answer sentences. The next most dominant fea-
ture is the SemType value object, which appears
in the NP for 29.41% of the answer sentences and
PP NeedSemType for 15.68% of the answer sen-
tences. Most of the other SemType values such as
time contribute trivially to distinguishing answers
from non-answers, as might be expected.
3. For when questions, person appears dominant
among NP SemType values; however, time fea-
tures appear to be second most dominant since
19.30% of the answer sentences have time as their
NP SemType, and 26.32% have at-time as their PP
SemType. Note that the PP NeedSemType and VP
SemType appear to be less capable of guiding the
selection of the correct answer.
4. For where questions, location features are impor-
tant with 24.07% answer sentences having loca-
tion as their NP SemType value, and 20.37% hav-
ing at-lc as their PP SemType. However, the
distribution of values for VP SemType and PP
NeedSemType shows no interesting patterns.
The current training strategy weights the NP-NP,
VP-VP, PP-PP, and NP-PP comparisons equiva-
lently. The above observations suggest that training
classifiers based on these equally weighted compar-
isons may have prevented the detection of a clear
class boundary, resulting in poor classification per-
formance. Since different phrase types do not appear
to contribute in the same way across different ques-
tion types, it may be better to generate a rule base
as a prefilter to assign more weight to certain phrases
or discard others before inputting the feature vector
into the classifier for training.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="conclusions">
4 Future Directions
</sectionHeader>
<bodyText confidence="0.999985272727272">
As a next step, we will try to tame our feature set.
One possibility is to use a rule-based classifier that
is less impacted by the serious imbalance between
negative and positive instances than C5.0 in order
to learn more effective feature sets for answer candi-
date discrimination corresponding to different ques-
tion types. We could then use the classifier as a pre-
processing filter to discard those less relevant com-
parison vector elements before inputting them into
the classifiers, instead of inputting comparison re-
sults based on the complete feature sets. This should
help to reduce noise generated by irrelevant features.
Also, we will perform additional data analysis on the
classification results to gain further insight into the
noise sources.
The classifiers we developed covered a wide range
of approaches. To optimize the classification perfor-
mance, we would like to implement a voting mod-
ule to process the answer candidates from different
classifiers. The confidence rankings of the classifiers
would be determined from• their corresponding an-
swer selection accuracy in the training set, and will
be used horizontally over the classifiers to provide
a weighted confidence measure for each sentence,
giving a final ordered list, where the head of the
list is the proposed answer sentence. We propose
to use a voting neural network to train the confi-
dence weights on different classifiers based on differ-
ent question types, since we also want to explore the
relationship of classifier performance with question
types. We believe this voting scheme will optimize
the bagging of different classifiers and improve the
hypothesis accuracy.
</bodyText>
<sectionHeader confidence="0.998443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988912342105263">
J. Allen. 1995. Natural Language Understanding.
The Benjamin/Cummings Publishing Company,
Menlo Park, CA.
M. P. Harper and R. A. Helzerman. 1995. Man-
aging multiple knowledge sources in constraint-
based parsing spoken language. Fundamenta In-
formaticae, 23(2,3,4):303-353.
M. P. Harper, R. A. Helzerman, C. B. Zoltowski,
B. L. Yeo, Y. Chan, T. Stewart, and B. L. Pellom.
1995. Implementation issues in the development
of the parsec parser. SOFTWARE - Practice and
Experience, 25:831-862.
M. P. Harper, C. M. White, W. Wang, M. T. John-
son, and R. A. Helzerman. 2000. Effectiveness
of corpus-induced dependency grammars for post-
processing speech. In Proceedings of the 1st An-
nual Meeting of the North American Association
for Computational Linguistics.
L. Hirschman, M. Light, E. Breck, and J.D. Burger.
1999. Deep Read: A reading comprehension sys-
tem. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 325-332.
J. R. Hobbs. 1979. Coherence and coreference. Cog-
nitive Science, 1:67-90.
J-SR Jang. 1993. ANFIS: Adaptive-Network-based
Fuzzy Inference System. IEEE Transactions on
System, Man, and Cybernetics, 23(3):665-685.
M. Jelasity and J. Dombi. 1998. GAS, a concept on
modeling species in genetic algorithms. Artificial
Intelligence, 99(1):1-19.
The MathWorks, Inc., 1998. Neural Network Tool-
box, v3.0.1.
Rulequest Research, 1999.
Data
Mining Tools See5 and
C5.0.
http://www.rulequest . com/see5—info.html.
</reference>
<page confidence="0.997575">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998067">A Question Answering System Developed as a Project in Natural Language Processing Course*</title>
<author confidence="0.969884">W Wang</author>
<author confidence="0.969884">J Auer</author>
<author confidence="0.969884">R Parasuraman</author>
<author confidence="0.969884">I Zubarev</author>
<author confidence="0.969884">D Brandyberry</author>
<author confidence="0.969884">M P</author>
<affiliation confidence="0.377203">Purdue</affiliation>
<address confidence="0.515114">West Lafayette, IN</address>
<email confidence="0.999079">zubarevi@cs.purdue.edu</email>
<abstract confidence="0.999851636363636">This paper describes the Question Answering System constructed during a one semester graduatelevel course on Natural Language Processing (NLP). We hypothesized that by using a combination of syntactic and semantic features and machine learning techniques, we could improve the accuracy of question answering on the test set of the Remedia corpus over the reported levels. The approach, although novel, was not entirely successful in the time frame of the course.</abstract>
<title confidence="0.894160555555556">Plain Text ( Story and Questions) — Brill POS Tagger Tagged Text • 2 i Name Identification Lexiced and Role • ink&amp;quot;rnatj*n------- Intonation...7 Lexicon -- Grammar Partial Parser • Resolved Propernoun Identified Tagged Text Pronoun Resolution</title>
<abstract confidence="0.9804972">This paper describes a preliminary reading comprehension system constructed as a semester-long project for a natural language processing course. This was the first exposure to this material for all but one student, and so much of the semester was spent learning about and constructing the tools that would be needed to attack this comprehensive problem. The course was structured around the project of building a question answering system following the HumSent evaluation as used by the Deep Read system (Hirschman et al., 1999). The Deep Read reading comprehension prototype system (Hirschman et al., 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. Since the average number of sentences per passage is 19.41, this performance is much better than chance (i.e., 5%). We hypothesized that by using a combination of syntactic and semantic features and machine learning techniques, we could improve the accuracy of question answering on the test set of the Remedia corpus over these reported levels. 2 System Description The overall architecture of our system is depicted in Figure 1. The story sentences and its five questions (who, what, where, when, and why) are first preprocessed and tagged by the Brill part-of-speech * We would like to thank the Deep Read group for giving us access to their test bed. •</abstract>
<title confidence="0.845192">Sentence-to-Question 5 I Comparison Voting Answer with Highest Scores</title>
<abstract confidence="0.979271626666667">Figure 1: The architecture for our question answering system. (POS) tagger distributed with the Deep Read system. This tagged text is then passed to the Name Identification Module, which updates the tags of named entities with semantic information and gender when appropriate. - The Partial Parser Module then takes this updated text and breaks it into phrases while attempting to lexically disambiguate the text. The Pronoun Resolution Module is consulted by the parser in order to resolve pronouns before passing partially parsed sentences and questions to the Sentence-to-Question Comparison Module. The Comparison Module determines how strongly the phrases of a sentence are related to those of a question, and this information is passed to several Rule-Based Classifier Neuro-- Neural Genetic Fuzzy Net Network Algorithm .&apos; - - • ANS ANS 28 modules which attempt to learn which features of the comparison are the most important for identifying whether a sentence is a strong answer candidate. We intended to set up a voting scheme among various modules; however, this part of the work has not been completed (as indicated by the dashed lines). Our system, like Deep Read, uses as the development set 28 stories from grade 2 and 27 from grade 5, each with five short answer questions (who, what, when, where, and why), and 60 stories with quesfrom grades 3 and 4 for testing We will refer to the development and testing data as the Remedia corpus. The following example shows the information added to a plain text sentence as it progresses through each module of the system we have created. Each module is described in more detail in the following sections. 2.1 Name Identification Module The Name Identification Module expects as input a file that has been tagged by the Brill tagger distributed with the Deep Read system. The most important named entities in the Remedia corpus are the names of people and the names of places. To distinguish between these two types, we created dictionaries for names of people and names of places. The first and last name dictionaries were derived from the files First names had an associated gender feature; names that were either male or female included gender frequency. Place names were extracted from atlases and other references, and included names of countries, major cities and capital cities, major attractions and parks, continents, etc. WordNet was also consulted because of its coverage of place names. There are 5,165 first name entries, 88,798 last name entries, and 1,086 place name entries in the dictionaries used by this module. The module looks up possible names to decide whether a word is a person&apos;s name or a location. If it cannot find the word in the dictionaries, it then looks at the POS tags provided in the input file to determine whether or not it is a propernoun. Heuristics looking at titles like word endings like then applied to decide the semantic type of the propernoun, and if the type cannot be determined, the module returns both person and location as its type. The accuracy of the Name Identification Module on the testing set was 79.6%. The accuracy adjusted to take into account incorrect tagging was 83.6%. &apos;There were differences between the Deep Read electronic version of the passages and the Remedia published passages. used the electronic</abstract>
<note confidence="0.668592">Ilin3-5 (... The club is for boys who are under 12 years old.) They are called Cub Scouts. (Answer to Question I)</note>
<title confidence="0.8284902">POS tagging . They are called • POS=&amp;quot;PRP&amp;quot; POS=&amp;quot;VBP&amp;quot; Name Identification They POS=&amp;quot;PRP&amp;quot; • are called Cub Scouts</title>
<abstract confidence="0.988568044871795">POS=&amp;quot;VBP&amp;quot; • POS=&amp;quot;VBN&apos; P05.1.4145 , PROPTWE.Loeme. Initial Partial Parsing are called TYPE.VP 10.2 LABEL.rnvb BASE.call AGR.3p TENSE-paste I VOICE-passive Pronoun Resolution They are , called , Cub Scouts TYPE.VP , TYPE.VP • 10.1 . I0.2 LABEL.aux 1 LABEL=nwb BASE.be &apos; BASE.cell AGR.3p i AOR.3p TENSEmpresent , TENSE.putp VOICEective . VOICE-P..0&apos;. SEM_TYPE•be-aux . SEM_TYPE.equate. Bracketed Sentence are • TYPE.VP ID.I LABEL.rnvb BASE.be AGR.3p TENSE.present VOICE.active Bracketed Question 1 Figure 2: Processing an example sentence for matching with a question in our system. 2.2 Partial Parser Module The Partial Parser Module follows sequentially after the Name Identification Module. The input is the set of story sentences and questions, such that the words in each are tagged with POS tags and the names are marked with type and gender information. Initially pronouns have not been resolved; the partial parser provides segmented text with rich lexical information and role labels directly to the Pronoun Resolution Module. After pronoun resolution, the segmented text with resolved pronouns is returned to the partial parser for the parser to update the feature values corresponding to the pronouns. Finally, the partial parser provides bracketed text to the Comparison Module, which extracts features that will be used to construct modules for answering questions. The Partial Parser Module utilizes information in a lexicon and a grammar to provide the partial CubScouts POS=&amp;quot;NNS&amp;quot;, TYPE=NP 10=1 LABEL.subjeot BASE=Ihey AGR=Llp GENDER=male SEM_TYPE*erson. TYPE.VP 10=1 LABEL.aux BASE.. AGR.3p TENSE.present VOICE=adive SEILt_TYPE.beema They Cub Scouts ; TYPE =NP LABEL=object &apos; BASE=CubScouts AGR=3p GENDER=male TYPE.NP 10.1 LABEL.subj.. . : AGR.3p GENDER.neutral ; SEJe_TYPE.person • PRONFIEZ,boys They TYPE=NP , 10=1 LABEL=subject BASE.Joy , AGR=3p GENDER=Inale • SEM,Ty_pg_q,erso_n. Who &apos; TYPE=NP 10=1 • LABEL=subject • BASE=who ; AGR..3p ; GENDER=male TypEverson TYPE.VP BASE.. TENSE.p.sem V010E...We TYPE.VP 10.2 LABEL.rtwb BASE.cs. i AOR.3o TENSE.1..1P TYPE.e • Updating Features TYPE.NP LABEL.object BASE=CubScouts • AGR.3p GENDER.male • SEM_TYPE.P.0.0 are called • Cub Scouts &apos; TYPE.NP 10=2 LABEL=objecl BASE=CubSocuts &apos; AGF1=3p GENDER=rnale : SEM_TYPE=pusce&apos; Cub Scouts 10=2 LABELwabject BASE=CubScouts AGR=4 GENDER:4,We 29 parses. The lexicon and the parser will be detailed in the next two subsections. 2.2.1 The Lexicon There were two methods we used to construct the lexicon, includes all words from the development set along with all determiners, pronouns, prepositions, particles, and conjunctions (these words are essential to achieving good sentence and lexicon, includes of the development and testing We constructed the closed lexicon with the benefit of the development corpus only (i.e., we did not consult the test materials to design the entries). To improve coverage in the case of the open lexicon, we constructed a module for obtaining features for words that do not appear in the development set (unknown words) that interfaces with WordNet to determine a word&apos;s base/stem, semantic type, and synonyms. When an unknown word has multiple senses, we have opted to choose the first sense because WordNet orders senses by frequency of use. Ignoring numbers, there are 1,999 unique words in the development set of the Remedia corpus, and 2,067 in the testing data, of which 1,008 do not appear in the development set. Overall, there are 3,007 unique words across both training and testing. One of our hypotheses was that by creating a lexicon with a rich set of features, we would improve the accuracy of question answering. The entries in the lexicon were constructed using the conventions adopted for the Parsec parser (Harper and Belzerman, 1995; Harper et al., 1995; Harper et al., 2000). Each word entry contains information about its root word (if there is one), its lexical category (or categories) along with a corresponding set of allowable features and their corresponding values. Lexical categories include noun, verb, pronoun, propernoun, adjective, adverb, preposition, particle, conjunction, determiner, cardinal, ordinal, predeterminer, noun modifier, and month. Feature types used in the include gender, agr, case, vtype progressive), gap, inverted, voice, mass), interrogative, relnoun-type, etc.). We that semtype should play a significant role in improving question answering performance, but the choice of semantic granularity is a difficult problem. We chose to keep the number of semantic values relatively small. By using the lexicographers&apos; files in WordNet to group the semantic values, we selected 25 possible semantic values for the nouns and 15 for the verbs. A we created the closed lexicon because this list of words was in the Deep Read materials. Once we spotted that the list contained words not in the development material, we kept it as an alternative to see how important full lexical knowledge would be for answering questions. script was created to semi-automate the construction of the lexicon from information extracted from previously existing dictionaries and from WordNet. 2.2.2 The Partial Parser The parser segments each sentence into either a noun phrase (NP), a verb phrase (VP), or a prepositional with various feature sets. NPs the feature types: root word of the word of the NP), inforthe root form in the lexicon, e.g., person, object, event, artifact, orrole type of the word in the e.g., subject), and phrases have the feature types: AGR, SemType the root form in the lexicon, e.g., act, possession), present, past), phrases (PPs) have the types: root form of the preposition the root the e.g., at-lc, at-time), object of the and the object of the preposition). Feature values are assigned using the lexicon, Pronoun Resolution Module, and grammar rules. We implemented a bottom-up partial parser to segment each sentence into syntactic subparts. The grammar used in the bottom-up parser is shown below: 1. NP DET ADJ-t- NOUN+ 2. NP DET NOUN 3. NP ADJ PROPERNOUN+ 4. VP -4 (AUX-VERB) MAIN-VERB 5. PP -4 ADV 6. PP -4 ADJ (PRED) 7. PP —&gt; PREP NP At the outset, the parser checks whether there are any punctuation marks in the sentence, with commas and periods being the most helpful. A comma is used in two ways in the Remedia corpus: it acts as a signal for the conjunction of a group of nouns or propernouns, or it acts as punctuation signalling an auxiliary phrase (usually a PP) or sentence. In the NP conjunction case, the parser groups the conjoined nouns or propernouns together as a plural NP. In the second case, the sentence is partially parsed. The partial parser operates in a bottom-up fashion taking as input a POS-tagged and name-identified and matching the right-hand side of the grammar rules. Starting from the beginning of the sentence or auxiliary phrase (or sentence), the looks for the of the words, transthe into corresponding lexical catand tries to match the the rules. Phrases are maintained on an agenda until they are finalized. NPs often require merging since some consecutive NPs form a single multi-word token (i.e., multi-word 30 names and conjunctions). An NP that results from merging two tokens into a single multi-word token its the rootword of the combined token, are updated according to the information retrieved from the lexicon based on the multi-word token. In the case of an NP conthe the union of the each AGR set to 3p, and assigned as that of the head word of the merged NP. The rule for finding the head word of an NP is: find the FIRST consecutive noun (propernoun) group in the NP, then the LAST noun (propernoun) in this group is defined as the head word of the NP. The partial parser performs word-sense disamas it parses. Words such as have multiple .semtype values in the lexicon for one lexical category. The following are rules for wordsense disambiguation used by the parser: • NP plus VP rules for word-sense disambiguation: there are verbs such as name, have the equate, then the NPs that precede and follow the VP have the same semtype. a noun is the object of a verb, then the feature value of the verb can be used to disamits word sense (e.g., has obj+time). • PP rules for word-sense disambiguation: For some nouns (propernouns) which are the object of a preposition, the intersection of the sets of the preposition word and object determines their • NPs in the date line of each passage are all either dates or places with the typical order beplace then time. For example, in (WASH- June, 1989), Washington assigned location rather than person. To process unknown words (the 1,008 words in the testing set that don&apos;t appear in the development set) in the case of the open lexicon, WordNet is used to the for nouns and verbs, the for verbs can be obtained in part from POS tag, and unknown noun words can be determined when they are used as the subject of a sentence. For the closed lexicon, the only unknown words are numbers. If a number is a four-digit numstarting with 16 to 19 or is followed by generally it is a year, so its defined as time. Other numbers tend to be modifiers predicates and have the 2.3 Pronoun Resolution Module resolution module was developed using the rules given in Allen&apos;s text (Allen, 1995) along with other rules described in the work of Hobbs (Hobbs, 1979). The module takes as input the feature-augmented and segmented text provided by the partial parser. Hence, the words are marked with lexical (including gender) and semantic feature information, and the phrase structure is also available. After the input file is provided by the Partial Parser Module, the Pronoun Resolution Module searches for the pronouns by looking through the NPs identified by the partial parser. Candidate antecedents are identified and a comparison of the features is made between the pronoun and the possible antecedent. The phrase that passes the most rule filters is chosen as the antecedent. First and second person pronouns are handled by using default values (i.e., writer and reader). If the system fails to arrive at an antecedent, the pronoun is marked as non-referential, which is often the case for pronouns they. Some of the most useful rules are listed below: • Reflexives must refer to an antecedent in the same sentence. For simplicity, we chose the closest noun preceding the pronoun in the sentence with AGR, Two NPs that co-refer must agree in Gender, SemType person, location). Since, in many cases the gender cannot be determined, this information was used only when available. • A subject was preferred over the object when the pronoun occurred as the subject in a sentence. When in the beginning of a paragraph, it is considered non-referential. • We prefer a global entity (the first named entity in a paragraph) when there is a feature match. In the absence of such, we prefer the closest propernoun preceding the pronoun with a feature match. If that fails, we prefer the closest preceding noun or pronoun with a feature match. The accuracy of our pronoun resolution module on the training corpus was 79.5% for grade 2 and 79.4% for grade 5. On testing, it was 81.33% for grade 3 and 80.39% for grade 4. The overall accuracy of this module on both the testing and training corpus was 80.17%. This was an improvement over the baseline Deep Read coreference module which achieved a 51.61% accuracy on training and a 50.91% accuracy on testing, giving an overall accuracy of 51.26%. This accuracy was determined based on Professor Harper&apos;s manual pronoun resolution of both the training and testing set (the perfect coreference information was not included in the distribution of the Deep Read system). 2.4 Sentence-to-Question Comparison Module The Sentence-to-Question Comparison Module takes as input a set of tagged stories, for which phrase types and features have been identified. The semantic and syntactic information is coded as shown in Figure 2 (using XML tags). A mechanism to quantify a qualitative comparison of questions and sentences has been developed. The comparison 31 provides data about how questions compare to their answers and how questions compare to non-answers. The classification of answers and non-answers is implemented by using feature comparison vectors of phrase-to-phrase comparisons in questions and potential answer sentences. A comparison is made using phrase-to-phrase comparisons between each sentence and each question in a passage. In particular, NP-to-NP, VP-to- VP, PP-to-PP, and NP-to-PP comparisons are made between each sentence and each of the five questions. These comparisons are stored for each sentence in the following arrays. Note that in these arrays Q varies from 1 to 5, signifying the question that the sentence matches. F varies over the features for the phrase match. CNNJ[F] Comparison of NP features (F = 1{Base, AGR, and SemType}I) between question the sentence. CV[Q][F] Comparison of VP features (F = I{Base, AGR, SemType, Tense} l) between question Q and the sentence. CP[Q][F] Comparison of PP features (F = [{NeedBase, Prep, PPSemType, NeedSemType}l) question the sentence. CPN[Q][F] Comparison of PP features in sentence to NP features in question Q. Here F=2, comparing ReedBase and Base, and NeedSemType and SemType. Values for these comparison matrices were calculated for each sentence by comparing the features of each phrase type in the sentence to features of the indicated phrase types in each of the five questions. The individual matrix values describe the comparison of the best match between a sentence and a question for NP-to-NP (the three feature match scores for the best matching NP pair of the sentence and question Q are stored in CN[Q]), VP-to-VP (stored CV[Q1), PP-to-PP (stored and PP-to- NP (store in CPN[Q1). Selecting the phrase comparison vector for a phrase type that best matches a sentence phrase to a question phrase was chosen as a heuristic to avoid placing more importance on a sentence only because it contains more information. Comparisons between features were calculated using the following equations. The first is used when features such as NeedBase, a partial match must be quantified. The second is used when comparing features such as AGR, only exact matches make sense. if Stri = Str2 length(Stri) length(Str2) A(Stri E Str2 V Str2 E Stri if Stri Str2 Stri = if Stri Str2 The matrices for the development set were provided to the algorithms in the Answer Module for training the component answer classifiers. The matrices for the testing set were also passed to the algorithms for testing. Additionally, specific information about the feature values for each sentence was passed to the Answer Module. 2.5 Answer Modules Several methods were developed in parallel in an attempt to learn the features that were central to identifying the sentence from a story that correctly answer a question. These methods are described in the following subsections. Due to time constraints, the evaluations of these Answer Modules were carried out with a closed lexicon and perfect pronoun resolution. 2.5.1 A Neuro-Fuzzy Network Classifier An Adaptive Network-based Fuzzy Inference System (ANFIS) (Jang, 1993) from the Matlab Fuzzy Logic Toolbox was used as one method to resolve the story questions. A separate network was trained for each question type in an attempt to make the networks learn relationships between phrases that classify answer sentences and non-answer sentences differently. ANFIS has the ability to learn complex relationships between its input variables. It was expected that by learning the relationships in the training set, the resolution of questions could be performed on the testing set. For ANFIS, the set of sentence-question pairs was divided into five groups according to question type. Currently the implementation of ANFIS on Matlab is restricted to 4 inputs. Hence, we needed to devise a way to aggregate the feature comparison information for each comparison vector. The comparison vectors for each phrase-to-phrase comparison were reduced to a single number for each comparison pair (i.e., NP-NP, VP-VP, PP-PP, NP-PP). This reduction was performed by multiplying the vector values by a normalized weighting constant for the feature values (e.g., NP-comparison = (Base weight)*(Base value) ± comparivalue) (SemType weight)*(SemType value), with the weights summing to In cases that a match is found, the comparison values are 1 (exact match). So weights were chosen that allowed the ANFIS to tellSomething about the match characteristics (e.g., if the AGR weight is 0.15 and the SemType weight is 0.1, and the NP-comparison value was 0.25, it can be concluded that the NP that matched best between in the sentence-question had the same SemType features). The aggregation weights were chosen so that all combinations of exact matches on features would have values and the magnitude weights were chosen based on the belief that the higher weighted</abstract>
<note confidence="0.87879375">1 ,Str2) ,Str2) 0</note>
<abstract confidence="0.992442293103448">32 features contribute more useful information. The weights, ordered to correspond to the features in the table on the previous page are: (.55, .15, .3) for CN, (.55, .1, .22, .13) for CV, (.55, .15, .2, .1) for CP, and (.55, .45) for CPN. ANFIS was trained using the update on the development set provided by the Sentence-to-Question Comparison Module as described above. During testing, the data, provided by the Comparison Module and updated as described above, is used as input to ANFIS. The output is a confidence value that describes the likelihood of a sentence being a answer. Every sentence is compared with every question in ANFIS, and then within question, the sentences are ranked by the likelihood that they are a question&apos;s answer. _ The accuracy of the best classifier produced with ANFIS was quite poor. In the grade 3 set, we achieved an accuracy of 13.33% on who questions, 6.67% on what questions, 0% on where questions, 6.67% on when questions, and 3.33% on why questions. In the grade 4 set, we achieved an accuracy of 3.54% on who questions, 10.34% on what questions, 10.34% on where questions, 0% on when questions, and 6.9% on why questions. Although the best ranking sentence produced poor accuracy results on the testing set, with some additional knowledge the topranking incorrect answers may be able to be eliminated. The plots in Figure 3 display the number of times the answer sentence was assigned a particular rank by ANFIS. The rank of the correct sentence tends to be in the top 10 fairly often for most question types. This rank tendency is most noticeable for who, what and when questions, but it is also present for where questions. The rank distribution for why questions appears to be random, which is consistent with our belief that they require a deeper analysis than would be possible with simple feature comparisons. 2.5.2 A Neural Network Classifier Like ANFIS, this module uses a neural network, but it has a different topology and uses an extended feature set. The nn (Neureka) neural network simulation system (Mat, 1998) was used to create a multi-layer (one hidden layer) back-propagation network. A single training/testing instance was generated from each story sentence. The network contains an input layer with two groups of features. The sentence/question feature vectors that compare a sentence to each of the five story questions comprise the first group. Sentence features that are independent of the questions, i.e., contains a location, contains a time/date, and contains a human, comprise the second group. The hidden layer contains a number of nodes that was experimentally varied to achieve best performance. The output layer contains five nodes, of which has a binary outpiit value which indicates whether or not the sentence is the answer to the corresponding question (i.e., question 1 through 5). Several training trials were performed to determine the optimum parameters for the network. We trained using various subsets of the full input feature set since some features could be detrimental to creating a good classifier. However, in the end, the full set of features performed better than or equivalently to the various subsets. Increasing the number of hidden nodes can often improve the accuracy of the network because it can learn more complex relationships; however, this did not help much in the current domain, and so the number of hidden nodes was set to 16. For this domain, there are many more sentences that are not the answer to a question than that are. An effort was made to artificially change this distribution by replicating the answer sentences in the training set; however, no additional accuracy was gained by this experimentation. Finally, we created a neural network for each question type as in ANFIS; however, these small networks had lower accuracy than the single network approach. The overall test set accuracy of the best neural network classifier was 14%. In the grade 3 set, we achieved an accuracy of 30% on who questions, 0% on what questions, 23.3% on when questions, 13.3% on where questions, and 3.3% on why questions. In the grade 4 set, we achieved an accuracy of 17.2% on who questions, 10.3% on what questions, 23.6% on when questions, 10.3% on where questions, and 3.4% on why questions. 2.5.3 A Rule-based Classifier based on C5.0 We attempted to learn rules for filtering out sentences that are not good candidates as answers to questions using C5.0 (Rul, 1999). First we extracted information from the sentence-to-question correspondence data ignoring the comparison values to make the input C5.0-compatible, and produced five different files (one for each question type). These files were then fed to C5.0; however, the program did not produce a useful tree. The problem may have been that most sentences in the passages are negative instances of answers to questions. 2.5.4 GAS GAS (Jelasity and Dombi, 1998) is a steady genetic algorithm with subpopulation support. It is capable of optimizing functions with a high number of local optima. The initial parameters were set theoretically. In the current matching problem, because the number of local optima can be high due to the coarse level of sentence information (there can be several sentence candidates with very close scores), this algorithm is preferred over other common genetic algorithms. This algorithm was trained on the training set, but due to the high noise level in the 33</abstract>
<title confidence="0.827874333333333">Who Questions What Questions A N-best order of sentences Where Questions nnnn A 9 N-best order of sentences When Questions</title>
<abstract confidence="0.968422462264151">N-best order of sentences 2 A 9 415 43 N-best order of sentences Why Questions N-best order of sentences Figure 3: Correct answers ordered by ANFIS preference. 00 training data, the algorithm fails to produce a winning population based on the mean square minimization function. 3 A Closer Look at the Features After observing the question answer accuracy results of the above classifiers, we concluded that the features we extracted for the classifiers are affected by noise. The fact that we take into consideration only the top matching phrase-to-phrase matches on a specific set of features may have contributed to this noisiness. To analyze the noise source of features, that hypothesized to be essenfor answer candidate discrimination, we examthose that occurred most frequently and calculated statistics on how often the values occurred in story sentences that are answers versus non-answers to the questions. We observed the following phenomena: For who questions, the person plays an important role in identifying answer sentences, since 83.64% answers have person as NP and 21.82% have it as PP However, 66.83% of the non-answer sentences also have person as its SemType 15.85% as its NeedSemType. with person in most sentences, whether they are answers or not, and this 34 weakens its ability to act as an effective filter. For what questions, the person apas the NP most answer and nonanswer sentences. The next most dominant feais the object, which appears in the NP for 29.41% of the answer sentences and 15.68% of the answer sen- Most of the other such as time contribute trivially to distinguishing answers from non-answers, as might be expected. 3. For when questions, person appears dominant NP however, time features appear to be second most dominant since 19.30% of the answer sentences have time as their 26.32% have at-time as their PP that the PP VP to be less capable of guiding the selection of the correct answer. 4. For where questions, location features are important with 24.07% answer sentences having locaas their NP and 20.37% havat-lc as their PP the of values for VP PP no interesting patterns. The current training strategy weights the NP-NP, VP-VP, PP-PP, and NP-PP comparisons equivalently. The above observations suggest that training classifiers based on these equally weighted comparisons may have prevented the detection of a clear class boundary, resulting in poor classification performance. Since different phrase types do not appear to contribute in the same way across different question types, it may be better to generate a rule base as a prefilter to assign more weight to certain phrases or discard others before inputting the feature vector into the classifier for training. 4 Future Directions As a next step, we will try to tame our feature set. One possibility is to use a rule-based classifier that is less impacted by the serious imbalance between negative and positive instances than C5.0 in order to learn more effective feature sets for answer candidate discrimination corresponding to different question types. We could then use the classifier as a preprocessing filter to discard those less relevant comparison vector elements before inputting them into the classifiers, instead of inputting comparison results based on the complete feature sets. This should help to reduce noise generated by irrelevant features. Also, we will perform additional data analysis on the classification results to gain further insight into the noise sources. The classifiers we developed covered a wide range of approaches. To optimize the classification performance, we would like to implement a voting module to process the answer candidates from different classifiers. The confidence rankings of the classifiers would be determined from• their corresponding answer selection accuracy in the training set, and will be used horizontally over the classifiers to provide a weighted confidence measure for each sentence, giving a final ordered list, where the head of the list is the proposed answer sentence. We propose to use a voting neural network to train the confidence weights on different classifiers based on different question types, since we also want to explore the relationship of classifier performance with question types. We believe this voting scheme will optimize the bagging of different classifiers and improve the hypothesis accuracy.</abstract>
<title confidence="0.612432">References</title>
<author confidence="0.600446">Language Understanding</author>
<affiliation confidence="0.906743">The Benjamin/Cummings Publishing Company,</affiliation>
<address confidence="0.919539">Menlo Park, CA.</address>
<author confidence="0.571399">Man-</author>
<abstract confidence="0.84391325">aging multiple knowledge sources in constraintparsing spoken language. In- M. P. Harper, R. A. Helzerman, C. B. Zoltowski, B. L. Yeo, Y. Chan, T. Stewart, and B. L. Pellom. 1995. Implementation issues in the development the parsec parser. - Practice M. P. Harper, C. M. White, W. Wang, M. T. Johnson, and R. A. Helzerman. 2000. Effectiveness of corpus-induced dependency grammars for postspeech. In of the 1st Annual Meeting of the North American Association for Computational Linguistics. L. Hirschman, M. Light, E. Breck, and J.D. Burger. 1999. Deep Read: A reading comprehension sys- In of the 37th of the Association for Computational Linguistics,</abstract>
<note confidence="0.671789533333333">pages 325-332. R. Hobbs. 1979. Coherence and coreference. Cog- Science, J-SR Jang. 1993. ANFIS: Adaptive-Network-based Inference System. Transactions on Man, and Cybernetics, M. Jelasity and J. Dombi. 1998. GAS, a concept on species in genetic algorithms. MathWorks, Inc., 1998. Network Toolbox, v3.0.1. Rulequest Research, 1999. Data Mining Tools See5 and C5.0. http://www.rulequest . com/see5—info.html.</note>
<intro confidence="0.507693">35</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>Natural Language Understanding.</title>
<date>1995</date>
<publisher>The Benjamin/Cummings Publishing Company,</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="16493" citStr="Allen, 1995" startWordPosition="2698" endWordPosition="2699">ype feature for nouns and verbs, the AGR feature for verbs can be obtained in part from the POS tag, and AUL for unknown noun words can be determined when they are used as the subject of a sentence. For the closed lexicon, the only unknown words are numbers. If a number is a four-digit number starting with 16 to 19 or is followed by A.D or B.C. then generally it is a year, so its semtype is defined as time. Other numbers tend to be modifiers or predicates and have the semtype of num. 2.3 Pronoun Resolution Module A pronoun resolution module was developed using the rules given in Allen&apos;s text (Allen, 1995) along with other rules described in the work of Hobbs (Hobbs, 1979). The module takes as input the feature-augmented and segmented text provided by the partial parser. Hence, the words are marked with lexical (including gender) and semantic feature information, and the phrase structure is also available. After the input file is provided by the Partial Parser Module, the Pronoun Resolution Module searches for the pronouns by looking through the NPs identified by the partial parser. Candidate antecedents are identified and a comparison of the features is made between the pronoun and the possibl</context>
</contexts>
<marker>Allen, 1995</marker>
<rawString>J. Allen. 1995. Natural Language Understanding. The Benjamin/Cummings Publishing Company, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>R A Helzerman</author>
</authors>
<title>Managing multiple knowledge sources in constraintbased parsing spoken language. Fundamenta Informaticae,</title>
<date>1995</date>
<pages>23--2</pages>
<marker>Harper, Helzerman, 1995</marker>
<rawString>M. P. Harper and R. A. Helzerman. 1995. Managing multiple knowledge sources in constraintbased parsing spoken language. Fundamenta Informaticae, 23(2,3,4):303-353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>R A Helzerman</author>
<author>C B Zoltowski</author>
<author>B L Yeo</author>
<author>Y Chan</author>
<author>T Stewart</author>
<author>B L Pellom</author>
</authors>
<date>1995</date>
<booktitle>Implementation issues in the development of the parsec parser. SOFTWARE - Practice and Experience,</booktitle>
<pages>25--831</pages>
<contexts>
<context position="10160" citStr="Harper et al., 1995" startWordPosition="1610" endWordPosition="1613">we have opted to choose the first sense because WordNet orders senses by frequency of use. Ignoring numbers, there are 1,999 unique words in the development set of the Remedia corpus, and 2,067 in the testing data, of which 1,008 do not appear in the development set. Overall, there are 3,007 unique words across both training and testing. One of our hypotheses was that by creating a lexicon with a rich set of features, we would improve the accuracy of question answering. The entries in the lexicon were constructed using the conventions adopted for the Parsec parser (Harper and Belzerman, 1995; Harper et al., 1995; Harper et al., 2000). Each word entry contains information about its root word (if there is one), its lexical category (or categories) along with a corresponding set of allowable features and their corresponding values. Lexical categories include noun, verb, pronoun, propernoun, adjective, adverb, preposition, particle, conjunction, determiner, cardinal, ordinal, predeterminer, noun modifier, and month. Feature types used in the lexicon include subcat, gender, agr, case, vtype (e.g., progressive), mood, gap, inverted, voice, behavior (e.g., mass), type (e.g., interrogative, relative), semtyp</context>
</contexts>
<marker>Harper, Helzerman, Zoltowski, Yeo, Chan, Stewart, Pellom, 1995</marker>
<rawString>M. P. Harper, R. A. Helzerman, C. B. Zoltowski, B. L. Yeo, Y. Chan, T. Stewart, and B. L. Pellom. 1995. Implementation issues in the development of the parsec parser. SOFTWARE - Practice and Experience, 25:831-862.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>C M White</author>
<author>W Wang</author>
<author>M T Johnson</author>
<author>R A Helzerman</author>
</authors>
<title>Effectiveness of corpus-induced dependency grammars for postprocessing speech.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10182" citStr="Harper et al., 2000" startWordPosition="1614" endWordPosition="1617">se the first sense because WordNet orders senses by frequency of use. Ignoring numbers, there are 1,999 unique words in the development set of the Remedia corpus, and 2,067 in the testing data, of which 1,008 do not appear in the development set. Overall, there are 3,007 unique words across both training and testing. One of our hypotheses was that by creating a lexicon with a rich set of features, we would improve the accuracy of question answering. The entries in the lexicon were constructed using the conventions adopted for the Parsec parser (Harper and Belzerman, 1995; Harper et al., 1995; Harper et al., 2000). Each word entry contains information about its root word (if there is one), its lexical category (or categories) along with a corresponding set of allowable features and their corresponding values. Lexical categories include noun, verb, pronoun, propernoun, adjective, adverb, preposition, particle, conjunction, determiner, cardinal, ordinal, predeterminer, noun modifier, and month. Feature types used in the lexicon include subcat, gender, agr, case, vtype (e.g., progressive), mood, gap, inverted, voice, behavior (e.g., mass), type (e.g., interrogative, relative), semtype, and conjtype (e.g.,</context>
</contexts>
<marker>Harper, White, Wang, Johnson, Helzerman, 2000</marker>
<rawString>M. P. Harper, C. M. White, W. Wang, M. T. Johnson, and R. A. Helzerman. 2000. Effectiveness of corpus-induced dependency grammars for postprocessing speech. In Proceedings of the 1st Annual Meeting of the North American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>M Light</author>
<author>E Breck</author>
<author>J D Burger</author>
</authors>
<title>Deep Read: A reading comprehension system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>325--332</pages>
<contexts>
<context position="1604" citStr="Hirschman et al., 1999" startWordPosition="241" endWordPosition="244"> Grammar Pronouns Resolved Propernoun Identified Tagged Text Pronoun Resolution 1 Introduction This paper describes a preliminary reading comprehension system constructed as a semester-long project for a natural language processing course. This was the first exposure to this material for all but one student, and so much of the semester was spent learning about and constructing the tools that would be needed to attack this comprehensive problem. The course was structured around the project of building a question answering system following the HumSent evaluation as used by the Deep Read system (Hirschman et al., 1999). The Deep Read reading comprehension prototype system (Hirschman et al., 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. Since the average number of sentences per passage is 19.41, this performance is much better than chance (i.e., 5%). We hypothesized that by using a combination of syntactic and semantic features and machine learning techniques, we could improve the accuracy of question answering on the test set of the Remedia corpus over these reported levels. 2 System Description The overall architecture of our</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>L. Hirschman, M. Light, E. Breck, and J.D. Burger. 1999. Deep Read: A reading comprehension system. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 325-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Coherence and coreference.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>1--67</pages>
<contexts>
<context position="16561" citStr="Hobbs, 1979" startWordPosition="2710" endWordPosition="2711">tained in part from the POS tag, and AUL for unknown noun words can be determined when they are used as the subject of a sentence. For the closed lexicon, the only unknown words are numbers. If a number is a four-digit number starting with 16 to 19 or is followed by A.D or B.C. then generally it is a year, so its semtype is defined as time. Other numbers tend to be modifiers or predicates and have the semtype of num. 2.3 Pronoun Resolution Module A pronoun resolution module was developed using the rules given in Allen&apos;s text (Allen, 1995) along with other rules described in the work of Hobbs (Hobbs, 1979). The module takes as input the feature-augmented and segmented text provided by the partial parser. Hence, the words are marked with lexical (including gender) and semantic feature information, and the phrase structure is also available. After the input file is provided by the Partial Parser Module, the Pronoun Resolution Module searches for the pronouns by looking through the NPs identified by the partial parser. Candidate antecedents are identified and a comparison of the features is made between the pronoun and the possible antecedent. The phrase that passes the most rule filters is chosen</context>
</contexts>
<marker>Hobbs, 1979</marker>
<rawString>J. R. Hobbs. 1979. Coherence and coreference. Cognitive Science, 1:67-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-SR Jang</author>
</authors>
<title>ANFIS: Adaptive-Network-based Fuzzy Inference System.</title>
<date>1993</date>
<journal>IEEE Transactions on System, Man, and Cybernetics,</journal>
<pages>23--3</pages>
<contexts>
<context position="22577" citStr="Jang, 1993" startWordPosition="3700" endWordPosition="3701">sting. Additionally, specific information about the feature values for each sentence was passed to the Answer Module. 2.5 Answer Modules Several methods were developed in parallel in an attempt to learn the features that were central to identifying the sentence from a story that correctly answer a question. These methods are described in the following subsections. Due to time constraints, the evaluations of these Answer Modules were carried out with a closed lexicon and perfect pronoun resolution. 2.5.1 A Neuro-Fuzzy Network Classifier An Adaptive Network-based Fuzzy Inference System (ANFIS) (Jang, 1993) from the Matlab Fuzzy Logic Toolbox was used as one method to resolve the story questions. A separate network was trained for each question type in an attempt to make the networks learn relationships between phrases that classify answer sentences and non-answer sentences differently. ANFIS has the ability to learn complex relationships between its input variables. It was expected that by learning the relationships in the training set, the resolution of questions could be performed on the testing set. For ANFIS, the set of sentence-question pairs was divided into five groups according to quest</context>
</contexts>
<marker>Jang, 1993</marker>
<rawString>J-SR Jang. 1993. ANFIS: Adaptive-Network-based Fuzzy Inference System. IEEE Transactions on System, Man, and Cybernetics, 23(3):665-685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jelasity</author>
<author>J Dombi</author>
</authors>
<title>GAS, a concept on modeling species in genetic algorithms.</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<pages>99--1</pages>
<contexts>
<context position="29405" citStr="Jelasity and Dombi, 1998" startWordPosition="4840" endWordPosition="4843">2.5.3 A Rule-based Classifier based on C5.0 We attempted to learn rules for filtering out sentences that are not good candidates as answers to questions using C5.0 (Rul, 1999). First we extracted information from the sentence-to-question correspondence data ignoring the comparison values to make the input C5.0-compatible, and produced five different files (one for each question type). These files were then fed to C5.0; however, the program did not produce a useful tree. The problem may have been that most sentences in the passages are negative instances of answers to questions. 2.5.4 GAS GAS (Jelasity and Dombi, 1998) is a steady genetic algorithm with subpopulation support. It is capable of optimizing functions with a high number of local optima. The initial parameters were set theoretically. In the current matching problem, because the number of local optima can be high due to the coarse level of sentence information (there can be several sentence candidates with very close scores), this algorithm is preferred over other common genetic algorithms. This algorithm was trained on the training set, but due to the high noise level in the 33 Who Questions What Questions .) &lt;3 A N-best order of sentences Where </context>
</contexts>
<marker>Jelasity, Dombi, 1998</marker>
<rawString>M. Jelasity and J. Dombi. 1998. GAS, a concept on modeling species in genetic algorithms. Artificial Intelligence, 99(1):1-19.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Neural Network Toolbox, v3.0.1.</booktitle>
<institution>The MathWorks, Inc.,</institution>
<marker>1998</marker>
<rawString>The MathWorks, Inc., 1998. Neural Network Toolbox, v3.0.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rulequest Research</author>
</authors>
<date>1999</date>
<booktitle>Mining Tools See5 and C5.0.</booktitle>
<marker>Research, 1999</marker>
<rawString>Rulequest Research, 1999. Mining Tools See5 and C5.0.</rawString>
</citation>
<citation valid="false">
<authors>
<author>http www rulequest</author>
</authors>
<pages>5</pages>
<marker>rulequest, </marker>
<rawString>http://www.rulequest . com/see5—info.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>