<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.067738">
<title confidence="0.996532">
Paraphrase Generation as Monolingual Translation: Data and Evaluation
</title>
<author confidence="0.99795">
Sander Wubben, Antal van den Bosch, Emiel Krahmer
</author>
<affiliation confidence="0.983181666666667">
Tilburg centre for Cognition and Communication
Tilburg University
Tilburg, The Netherlands
</affiliation>
<email confidence="0.998401">
{s.wubben,antal.vdnbosch,e.j.krahmer}@uvt.nl
</email>
<sectionHeader confidence="0.997406" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999265823529412">
In this paper we investigate the auto-
matic generation and evaluation of senten-
tial paraphrases. We describe a method
for generating sentential paraphrases by
using a large aligned monolingual cor-
pus of news headlines acquired automat-
ically from Google News and a stan-
dard Phrase-Based Machine Translation
(PBMT) framework. The output of this
system is compared to a word substitu-
tion baseline. Human judges prefer the
PBMT paraphrasing system over the word
substitution system. We demonstrate that
BLEU correlates well with human judge-
ments provided that the generated para-
phrased sentence is sufficiently different
from the source sentence.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968696428571">
Text-to-text generation is an increasingly studied
subfield in natural language processing. In con-
trast with the typical natural language generation
paradigm of converting concepts to text, in text-
to-text generation a source text is converted into a
target text that approximates the meaning of the
source text. Text-to-text generation extends to
such varied tasks as summarization (Knight and
Marcu, 2002), question-answering (Lin and Pan-
tel, 2001), machine translation, and paraphrase
generation.
Sentential paraphrase generation (SPG) is the
process of transforming a source sentence into a
target sentence in the same language which dif-
fers in form from the source sentence, but approx-
imates its meaning. Paraphrasing is often used as
a subtask in more complex NLP applications to
allow for more variation in text strings presented
as input, for example to generate paraphrases of
questions that in their original form cannot be an-
swered (Lin and Pantel, 2001; Riezler et al., 2007),
or to generate paraphrases of sentences that failed
to translate (Callison-Burch et al., 2006). Para-
phrasing has also been used in the evaluation of
machine translation system output (Russo-Lassner
et al., 2006; Kauchak and Barzilay, 2006; Zhou
et al., 2006). Adding certain constraints to para-
phrasing allows for additional useful applications.
When a constraint is specified that a paraphrase
should be shorter than the input text, paraphras-
ing can be used for sentence compression (Knight
and Marcu, 2002; Barzilay and Lee, 2003) as well
as for text simplification for question answering or
subtitle generation (Daelemans et al., 2004).
We regard SPG as a monolingual machine trans-
lation task, where the source and target languages
are the same (Quirk et al., 2004). However, there
are two problems that have to be dealt with to
make this approach work, namely obtaining a suf-
ficient amount of examples, and a proper eval-
uation methodology. As Callison-Burch et al.
(2008) argue, automatic evaluation of paraphras-
ing is problematic. The essence of SPG is to gen-
erate a sentence that is structurally different from
the source. Automatic evaluation metrics in re-
lated fields such as machine translation operate on
a notion of similarity, while paraphrasing centers
around achieving dissimilarity. Besides the eval-
uation issue, another problem is that for an data-
driven MT account of paraphrasing to work, a
large collection of data is required. In this case,
this would have to be pairs of sentences that are
paraphrases of each other. So far, paraphrasing
data sets of sufficient size have been mostly lack-
ing. We argue that the headlines aggregated by
Google News offer an attractive avenue.
</bodyText>
<sectionHeader confidence="0.974469" genericHeader="method">
2 Data Collection
</sectionHeader>
<bodyText confidence="0.92825575">
Currently not many resources are available for
paraphrasing; one example is the Microsoft Para-
phrase Corpus (MSR) (Dolan et al., 2004; Nelken
and Shieber, 2006), which with its 139,000 aligned
</bodyText>
<figureCaption confidence="0.9818275">
Figure 1: Part of a sample headline cluster, with
aligned paraphrases
</figureCaption>
<bodyText confidence="0.998762833333333">
paraphrases can be considered relatively small. In
this study we explore the use of a large, automat-
ically acquired aligned paraphrase corpus. Our
method consists of crawling the headlines aggre-
gated and clustered by Google News and then
aligning paraphrases within each of these clusters.
An example of such a cluster is given in Figure 1.
For each pair of headlines in a cluster, we calcu-
late the Cosine similarity over the word vectors of
the two headlines. If the similarity exceeds a de-
fined upper threshold it is accepted; if it is below
a defined lower threshold it is rejected. In the case
that it lies between the thresholds, the process is
repeated but then with word vectors taken from a
snippet from the corresponding news article. This
method, described in earlier work Wubben et al.
(2009), was reported to yield a precision of 0.76
and a recall of 0.41 on clustering actual Dutch
paraphrases in a headline corpus. We adapted this
method to English. Our data consists of English
headlines that appeared in Google News over the
period of April to September 2006. Using this
method we end up with a corpus of 7,400,144 pair-
wise alignments of 1,025,605 unique headlines1.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="method">
3 Paraphrasing methods
</sectionHeader>
<bodyText confidence="0.978751333333333">
In our approach we use the collection of au-
tomatically obtained aligned headlines to train
a paraphrase generation model using a Phrase-
Based MT framework. We compare this ap-
proach to a word substitution baseline. The gen-
erated paraphrases along with their source head-
</bodyText>
<footnote confidence="0.73258">
1This list of aligned pairs is available at
http://ilk.uvt.nl/∼swubben/resources.html
</footnote>
<bodyText confidence="0.99902625">
lines are presented to human judges, whose rat-
ings are compared to the BLEU (Papineni et al.,
2002), METEOR (Banerjee and Lavie, 2005) and
ROUGE (Lin, 2004) automatic evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.991949">
3.1 Phrase-Based MT
</subsectionHeader>
<bodyText confidence="0.999548714285714">
We use the MOSES package to train a
Phrase-Based Machine Translation model
(PBMT) (Koehn et al., 2007). Such a model
normally finds a best translation e˜ of a text in
language f to a text in language e by combining
a translation model p(f|e) with a language model
p(e):
</bodyText>
<equation confidence="0.977603">
e˜ = arg max p(f|e)p(e)
e∈e∗
</equation>
<bodyText confidence="0.999974074074074">
GIZA++ is used to perform the word align-
ments (Och and Ney, 2003) which are then used in
the Moses pipeline to generate phrase alignments
in order to build the paraphrase model. We first to-
kenize our data before training a recaser. We then
lowercase all data and use all unique headlines in
the training data to train a language model with the
SRILM toolkit (Stolcke, 2002). Then we invoke
the GIZA++ aligner using the 7M training para-
phrase pairs. We run GIZA++ with standard set-
tings and we perform no optimization. Finally, we
use the MOSES decoder to generate paraphrases
for our test data.
Instead of assigning equal weights to language
and translation model, we assign a larger weight
of 0.7 to the language model to generate better
formed (but more conservative) paraphrases. Be-
cause dissimilarity is a factor that is very impor-
tant for paraphrasing but not implemented in a
PBMT model, we perform post-hoc reranking of
the different candidate outputs based on dissimi-
larity. For each headline in the testset we generate
the ten best paraphrases as scored by the decoder
and then rerank them according to dissimilarity to
the source using the Levenshtein distance measure
at the word level. The resulting headlines are re-
cased using the previously trained recaser.
</bodyText>
<subsectionHeader confidence="0.999685">
3.2 Word Substitution
</subsectionHeader>
<bodyText confidence="0.999219285714286">
We compare the PBMT results with a simple word
substitution baseline. For each noun, adjective and
verb in the sentence this model takes that word and
its Part of Speech tag and retrieves from Word-
Net its most frequent synonym from the most fre-
quent synset containing the input word. We use the
Memory Based Tagger (Daelemans et al., 1996)
</bodyText>
<table confidence="0.990309555555555">
Police investigate Doherty drug pics
Doherty under police investigation
Police to probe Pete pics
Pete Doherty arrested in drug-photo probe
Rocker photographed injecting unconscious fan
Doherty ʼinjected unconscious fan with drugʼ
Photos may show Pete Doherty injecting passed-out fan
Doherty ʼinjected female fanʼ
System Headline
Source Florida executes notorious serial killer
PBMT Serial killer executed in Florida
Word Sub. Florida executes ill-famed series slayer
Source Dublin evacuates airport due to bomb scare
PBMT Dublin airport evacuated after bomb threat
Word Sub. Dublin evacuates airdrome due to bomb panic
Source N. Korea blasts nuclear sanctions
PBMT N. Korea nuclear blast of sanctions
Word Sub. N. Korea blasts atomic sanctions
</table>
<tableCaption confidence="0.9667695">
Table 1: Examples of generated paraphrased head-
lines
</tableCaption>
<bodyText confidence="0.9921312">
trained on the Brown corpus to generate the POS-
tags. The WordNet::QueryData2 Perl module is
used to query WordNet (Fellbaum, 1998). Gener-
ated headlines and their source for both systems
are given in Table 1.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9992572">
For the evaluation of the generated paraphrases
we set up a human judgement study, and compare
the human judges’ ratings to automatic evaluation
measures in order to gain more insight in the auto-
matic evaluation of paraphrasing.
</bodyText>
<subsectionHeader confidence="0.997083">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.999923304347826">
We randomly select 160 headlines that meet the
following criteria: the headline has to be compre-
hensible without reading the corresponding news
article, both systems have to be able to produce a
paraphrase for each headline, and there have to be
a minimum of eight paraphrases for each headline.
We need these paraphrases as multiple references
for our automatic evaluation measures to account
for the diversity in real-world paraphrases, as the
aligned paraphrased headlines in Figure 1 witness.
The judges are presented with the 160 head-
lines, along with the paraphrases generated by
both systems. The order of the headlines is ran-
domized, and the order of the two paraphrases for
each headline is also randomized to prevent a bias
towards one of the paraphrases. The judges are
asked to rate the paraphrases on a 1 to 7 scale,
where 1 means that the paraphrase is very bad and
7 means that the paraphrase is very good. The
judges were instructed to base their overall quality
judgement on whether the meaning was retained,
the paraphrase was grammatical and fluent, and
whether the paraphrase was in fact different from
</bodyText>
<footnote confidence="0.7289705">
2http://search.cpan.org/dist/WordNet-
QueryData/QueryData.pm
</footnote>
<table confidence="0.962686666666667">
system mean stdev.
PBMT 4.60 0.44
Word Substitution 3.59 0.64
</table>
<tableCaption confidence="0.995851">
Table 2: Results of human judgements (N = 10)
</tableCaption>
<bodyText confidence="0.996837">
the source sentence. Ten judges rated two para-
phrases per headline, resulting in a total of 3,200
scores. All judges were blind to the purpose of the
evaluation and had no background in paraphrasing
research.
</bodyText>
<sectionHeader confidence="0.643643" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999952303030303">
The average scores assigned by the human judges
to the output of the two systems are displayed in
Table 2. These results show that the judges rated
the quality of the PBMT paraphrases significantly
higher than those generated by the word substitu-
tion system (t(18) = 4.11,p &lt; .001).
Results from the automatic measures as well
as the Levenshtein distance are listed in Table 3.
We use a Levenshtein distance over tokens. First,
we observe that both systems perform roughly the
same amount of edit operations on a sentence, re-
sulting in a Levenshtein distance over words of
2.76 for the PBMT system and 2.67 for the Word
Substitution system. BLEU, METEOR and three
typical ROUGE metrics3 all rate the PBMT sys-
tem higher than the Word Substitution system.
Notice also that the all metrics assign the high-
est scores to the original sentences, as is to be ex-
pected: because every operation we perform is in
the same language, the source sentence is also a
paraphrase of the reference sentences that we use
for scoring our generated headline. If we pick a
random sentence from the reference set and score
it against the rest of the set, we obtain similar
scores. This means that this score can be regarded
as an upper bound score for paraphrasing: we can
not expect our paraphrases to be better than those
produced by humans. However, this also shows
that these measures cannot be used directly as an
automatic evaluation method of paraphrasing, as
they assign the highest score to the “paraphrase” in
which nothing has changed. The scores observed
in Table 3 do indicate that the paraphrases gener-
</bodyText>
<footnote confidence="0.970947">
3ROUGE-1, ROUGE-2 and ROUGE-SU4 are also
adopted for the DUC 2007 evaluation campaign,
http://www-nlpir.nist.gov/projects/duc/
duc2007/tasks.html
</footnote>
<table confidence="0.99971725">
System BLEU ROUGE-1 ROUGE-2 ROUGE-SU4 METEOR Lev.dist. Lev. stdev.
PBMT 50.88 0.76 0.36 0.42 0.71 2.76 1.35
Wordsub. 24.80 0.59 0.22 0.26 0.54 2.67 1.50
Source 60.58 0.80 0.45 0.47 0.77 0 0
</table>
<tableCaption confidence="0.999688">
Table 3: Automatic evaluation and sentence Levenshtein scores
</tableCaption>
<figure confidence="0.9911165">
0 1 2 3 4 5 6
Levenshtein distance
</figure>
<figureCaption confidence="0.8168406">
Figure 2: Correlations between human judge-
ments and automatic evaluation metrics for vari-
ous edit distances
ated by PBMT are less well formed than the orig-
inal source sentence.
</figureCaption>
<bodyText confidence="0.999866625">
There is an overall medium correlation between
the BLEU measure and human judgements (r =
0.41, p &lt; 0.001). We see a lower correlation
between the various ROUGE scores and human
judgements, with ROUGE-1 showing the highest
correlation (r = 0.29,p &lt; 0.001). Between the
two lies the METEOR correlation (r = 0.35, p &lt;
0.001). However, if we split the data according to
Levenshtein distance, we observe that we gener-
ally get a higher correlation for all the tested met-
rics when the Levenshtein distance is higher, as
visualized in Figure 2. At Levenshtein distance 5,
the BLEU score achieves a correlation of 0.78 with
human judgements, while ROUGE-1 manages to
achieve a 0.74 correlation. Beyond edit distance
5, data sparsity occurs.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9997675625">
In this paper we have shown that with an automat-
ically obtained parallel monolingual corpus with
several millions of paired examples, it is possi-
ble to develop an SPG system based on a PBMT
framework. Human judges preferred the output
of our PBMT system over the output of a word
substitution system. We have also addressed the
problem of automatic paraphrase evaluation. We
measured BLEU, METEOR and ROUGE scores,
and observed that these automatic scores corre-
late with human judgements to some degree, but
that the correlation is highly dependent on edit
distance. At low edit distances automatic metrics
fail to properly assess the quality of paraphrases,
whereas at edit distance 5 the correlation of BLEU
with human judgements is 0.78, indicating that at
higher edit distances these automatic measures can
be utilized to rate the quality of the generated para-
phrases. From edit distance 2, BLEU correlates
best with human judgements, indicating that MT
evaluation metrics might be best for SPG evalua-
tion.
The data we used for paraphrasing consists of
headlines. Paraphrase patterns we learn are those
used in headlines and therefore different from
standard language. The advantage of our approach
is that it paraphrases those parts of sentences that
it can paraphrase, and leaves the unknown parts
intact. It is straightforward to train a language
model on in-domain text and use the translation
model acquired from the headlines to generate
paraphrases for other domains. We are also inter-
ested in capturing paraphrase patterns from other
domains, but acquiring parallel corpora for these
domains is not trivial.
Instead of post-hoc dissimilarity reranking of
the candidate paraphrase sentences we intend to
develop a proper paraphrasing model that takes
dissimilarity into account in the decoding pro-
cess. In addition, we plan to investigate if our
paraphrase generation approach is applicable to
sentence compression and simplification. On the
topic of automatic evaluation, we aim to define
an automatic paraphrase generation assessment
score. A paraphrase evaluation measure should be
able to recognize that a good paraphrase is a well-
formed sentence in the source language, yet it is
clearly dissimilar to the source.
</bodyText>
<figure confidence="0.995717">
correlation
0.8
0.6
0.4
0.2
0
BLEU
ROUGE-1
ROUGE-2
ROUGE-SU4
METEOR
</figure>
<sectionHeader confidence="0.927846" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995912901960785">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, June.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In NAACL ’03: Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
16–23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17–24.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. Parametric: an automatic evaluation met-
ric for paraphrasing. In COLING ’08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 97–104.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part of
speech tagger-generator. In Proc. of Fourth Work-
shop on Very Large Corpora, pages 14–27.
Walter Daelemans, Anja Hothker, and Erik Tjong
Kim Sang. 2004. Automatic sentence simplification
for subtitling in dutch and english. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 1045–1048.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
COLING ’04: Proceedings of the 20th international
conference on Computational Linguistics, page 350.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database, May.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 455–462, June.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91–107.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL.
Dekang Lin and Patrick Pantel. 2001. Dirt: Discov-
ery of inference rules from text. In KDD ’01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323–328.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, page 10.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-06), 3–7 April.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19–51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ’02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311–318.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 142–
149, July.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2006. A paraphrase-based approach to machine
translation evaluation. Technical report, University
of Maryland, College Park.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In In Proc. Int. Conf. on Spoken
Language Processing, pages 901–904.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
ENLG ’09: Proceedings of the 12th European Work-
shop on Natural Language Generation, pages 122–
125.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77–84, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657071">
<title confidence="0.999962">Paraphrase Generation as Monolingual Translation: Data and Evaluation</title>
<author confidence="0.998087">Sander Wubben</author>
<author confidence="0.998087">Antal van_den_Bosch</author>
<author confidence="0.998087">Emiel</author>
<affiliation confidence="0.845431">Tilburg centre for Cognition and Tilburg</affiliation>
<address confidence="0.944945">Tilburg, The</address>
<abstract confidence="0.999025111111111">In this paper we investigate the automatic generation and evaluation of sentential paraphrases. We describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard Phrase-Based Machine Translation (PBMT) framework. The output of this system is compared to a word substitution baseline. Human judges prefer the PBMT paraphrasing system over the word substitution system. We demonstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="5579" citStr="Banerjee and Lavie, 2005" startWordPosition="876" endWordPosition="879">r 2006. Using this method we end up with a corpus of 7,400,144 pairwise alignments of 1,025,605 unique headlines1. 3 Paraphrasing methods In our approach we use the collection of automatically obtained aligned headlines to train a paraphrase generation model using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1This list of aligned pairs is available at http://ilk.uvt.nl/∼swubben/resources.html lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. 3.1 Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f|e) with a language model p(e): e˜ = arg max p(f|e)p(e) e∈e∗ GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recas</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2442" citStr="Barzilay and Lee, 2003" startWordPosition="359" endWordPosition="362">at in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Aut</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="2004" citStr="Callison-Burch et al., 2006" startWordPosition="291" endWordPosition="294">hine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation tas</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Parametric: an automatic evaluation metric for paraphrasing.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="2885" citStr="Callison-Burch et al. (2008)" startWordPosition="433" endWordPosition="436">hen a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of suff</context>
</contexts>
<marker>Callison-Burch, Cohn, Lapata, 2008</marker>
<rawString>Chris Callison-Burch, Trevor Cohn, and Mirella Lapata. 2008. Parametric: an automatic evaluation metric for paraphrasing. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gillis</author>
</authors>
<title>Mbt: A memory-based part of speech tagger-generator.</title>
<date>1996</date>
<booktitle>In Proc. of Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<contexts>
<context position="7584" citStr="Daelemans et al., 1996" startWordPosition="1218" endWordPosition="1221">stset we generate the ten best paraphrases as scored by the decoder and then rerank them according to dissimilarity to the source using the Levenshtein distance measure at the word level. The resulting headlines are recased using the previously trained recaser. 3.2 Word Substitution We compare the PBMT results with a simple word substitution baseline. For each noun, adjective and verb in the sentence this model takes that word and its Part of Speech tag and retrieves from WordNet its most frequent synonym from the most frequent synset containing the input word. We use the Memory Based Tagger (Daelemans et al., 1996) Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. Florida executes ill-famed series slayer Source Dublin evacuates airport due to bomb scare PBMT Dublin airport evacuated after bomb threat Word Sub. Dublin evacuates airdr</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. 1996. Mbt: A memory-based part of speech tagger-generator. In Proc. of Fourth Workshop on Very Large Corpora, pages 14–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Anja Hothker, and Erik Tjong Kim Sang.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1045--1048</pages>
<marker>Daelemans, 2004</marker>
<rawString>Walter Daelemans, Anja Hothker, and Erik Tjong Kim Sang. 2004. Automatic sentence simplification for subtitling in dutch and english. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<contexts>
<context position="3756" citStr="Dolan et al., 2004" startWordPosition="574" endWordPosition="577">imilarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned paraphrase corpus. Our method consists of crawling the headlines aggregated and clustered by Google News and then aligning paraphrases within each of these clusters. An example of such a cluster is given in Figure 1. For each pair of headlines in a cluster, we calculate the Cosine similarity over the word vectors of the two headlines. If the si</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<contexts>
<context position="8514" citStr="Fellbaum, 1998" startWordPosition="1357" endWordPosition="1358"> System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. Florida executes ill-famed series slayer Source Dublin evacuates airport due to bomb scare PBMT Dublin airport evacuated after bomb threat Word Sub. Dublin evacuates airdrome due to bomb panic Source N. Korea blasts nuclear sanctions PBMT N. Korea nuclear blast of sanctions Word Sub. N. Korea blasts atomic sanctions Table 1: Examples of generated paraphrased headlines trained on the Brown corpus to generate the POStags. The WordNet::QueryData2 Perl module is used to query WordNet (Fellbaum, 1998). Generated headlines and their source for both systems are given in Table 1. 4 Evaluation For the evaluation of the generated paraphrases we set up a human judgement study, and compare the human judges’ ratings to automatic evaluation measures in order to gain more insight in the automatic evaluation of paraphrasing. 4.1 Method We randomly select 160 headlines that meet the following criteria: the headline has to be comprehensible without reading the corresponding news article, both systems have to be able to produce a paraphrase for each headline, and there have to be a minimum of eight para</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="2148" citStr="Kauchak and Barzilay, 2006" startWordPosition="313" endWordPosition="316"> sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make t</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 455–462, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artif. Intell.,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="1328" citStr="Knight and Marcu, 2002" startWordPosition="185" endWordPosition="188">em over the word substitution system. We demonstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence. 1 Introduction Text-to-text generation is an increasingly studied subfield in natural language processing. In contrast with the typical natural language generation paradigm of converting concepts to text, in textto-text generation a source text is converted into a target text that approximates the meaning of the source text. Text-to-text generation extends to such varied tasks as summarization (Knight and Marcu, 2002), question-answering (Lin and Pantel, 2001), machine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate par</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artif. Intell., 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris C Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="5754" citStr="Koehn et al., 2007" startWordPosition="903" endWordPosition="906">tomatically obtained aligned headlines to train a paraphrase generation model using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1This list of aligned pairs is available at http://ilk.uvt.nl/∼swubben/resources.html lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. 3.1 Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f|e) with a language model p(e): e˜ = arg max p(f|e)p(e) e∈e∗ GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase all data and use all unique headlines in the training data to train a language model with the SRILM toolkit (Stolcke, 2002). Then we invoke the GIZA++ al</context>
</contexts>
<marker>Koehn, Hoang, Birch, Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C. Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt: Discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In KDD ’01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="1371" citStr="Lin and Pantel, 2001" startWordPosition="190" endWordPosition="194">nstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence. 1 Introduction Text-to-text generation is an increasingly studied subfield in natural language processing. In contrast with the typical natural language generation paradigm of converting concepts to text, in textto-text generation a source text is converted into a target text that approximates the meaning of the source text. Text-to-text generation extends to such varied tasks as summarization (Knight and Marcu, 2002), question-answering (Lin and Pantel, 2001), machine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to transl</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt: Discovery of inference rules from text. In KDD ’01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. ACL workshop on Text Summarization Branches Out,</booktitle>
<pages>10</pages>
<contexts>
<context position="5601" citStr="Lin, 2004" startWordPosition="882" endWordPosition="883">ith a corpus of 7,400,144 pairwise alignments of 1,025,605 unique headlines1. 3 Paraphrasing methods In our approach we use the collection of automatically obtained aligned headlines to train a paraphrase generation model using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1This list of aligned pairs is available at http://ilk.uvt.nl/∼swubben/resources.html lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. 3.1 Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f|e) with a language model p(e): e˜ = arg max p(f|e)p(e) e∈e∗ GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proc. ACL workshop on Text Summarization Branches Out, page 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart M Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06),</booktitle>
<pages>3--7</pages>
<contexts>
<context position="3783" citStr="Nelken and Shieber, 2006" startWordPosition="578" endWordPosition="581">aphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned paraphrase corpus. Our method consists of crawling the headlines aggregated and clustered by Google News and then aligning paraphrases within each of these clusters. An example of such a cluster is given in Figure 1. For each pair of headlines in a cluster, we calculate the Cosine similarity over the word vectors of the two headlines. If the similarity exceeds a defined </context>
</contexts>
<marker>Nelken, Shieber, 2006</marker>
<rawString>Rani Nelken and Stuart M. Shieber. 2006. Towards robust context-sensitive sentence alignment for monolingual corpora. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), 3–7 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6016" citStr="Och and Ney, 2003" startWordPosition="954" endWordPosition="957">e at http://ilk.uvt.nl/∼swubben/resources.html lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. 3.1 Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f|e) with a language model p(e): e˜ = arg max p(f|e)p(e) e∈e∗ GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase all data and use all unique headlines in the training data to train a language model with the SRILM toolkit (Stolcke, 2002). Then we invoke the GIZA++ aligner using the 7M training paraphrase pairs. We run GIZA++ with standard settings and we perform no optimization. Finally, we use the MOSES decoder to generate paraphrases for our test data. Instead of assigning equal weights to language and translation model, </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="5544" citStr="Papineni et al., 2002" startWordPosition="871" endWordPosition="874"> the period of April to September 2006. Using this method we end up with a corpus of 7,400,144 pairwise alignments of 1,025,605 unique headlines1. 3 Paraphrasing methods In our approach we use the collection of automatically obtained aligned headlines to train a paraphrase generation model using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1This list of aligned pairs is available at http://ilk.uvt.nl/∼swubben/resources.html lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. 3.1 Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f|e) with a language model p(e): e˜ = arg max p(f|e)p(e) e∈e∗ GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokeni</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="2678" citStr="Quirk et al., 2004" startWordPosition="398" endWordPosition="401">machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadrive</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 142– 149, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu O Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1908" citStr="Riezler et al., 2007" startWordPosition="277" endWordPosition="280">as summarization (Knight and Marcu, 2002), question-answering (Lin and Pantel, 2001), machine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subt</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grazia Russo-Lassner</author>
<author>Jimmy Lin</author>
<author>Philip Resnik</author>
</authors>
<title>A paraphrase-based approach to machine translation evaluation.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>University of Maryland, College Park.</institution>
<contexts>
<context position="2120" citStr="Russo-Lassner et al., 2006" startWordPosition="309" endWordPosition="312">ource sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that hav</context>
</contexts>
<marker>Russo-Lassner, Lin, Resnik, 2006</marker>
<rawString>Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 2006. A paraphrase-based approach to machine translation evaluation. Technical report, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit. In</title>
<date>2002</date>
<booktitle>In Proc. Int. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="6324" citStr="Stolcke, 2002" startWordPosition="1010" endWordPosition="1011">ranslation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f|e) with a language model p(e): e˜ = arg max p(f|e)p(e) e∈e∗ GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase all data and use all unique headlines in the training data to train a language model with the SRILM toolkit (Stolcke, 2002). Then we invoke the GIZA++ aligner using the 7M training paraphrase pairs. We run GIZA++ with standard settings and we perform no optimization. Finally, we use the MOSES decoder to generate paraphrases for our test data. Instead of assigning equal weights to language and translation model, we assign a larger weight of 0.7 to the language model to generate better formed (but more conservative) paraphrases. Because dissimilarity is a factor that is very important for paraphrasing but not implemented in a PBMT model, we perform post-hoc reranking of the different candidate outputs based on dissi</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In In Proc. Int. Conf. on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
<author>Erwin Marsi</author>
</authors>
<title>Clustering and matching headlines for automatic paraphrase acquisition.</title>
<date>2009</date>
<booktitle>In ENLG ’09: Proceedings of the 12th European Workshop on Natural Language Generation,</booktitle>
<pages>122--125</pages>
<marker>Wubben, van den Bosch, Krahmer, Marsi, 2009</marker>
<rawString>Sander Wubben, Antal van den Bosch, Emiel Krahmer, and Erwin Marsi. 2009. Clustering and matching headlines for automatic paraphrase acquisition. In ENLG ’09: Proceedings of the 12th European Workshop on Natural Language Generation, pages 122– 125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Re-evaluating machine translation results with paraphrase support.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>77--84</pages>
<contexts>
<context position="2168" citStr="Zhou et al., 2006" startWordPosition="317" endWordPosition="320">ge which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, n</context>
</contexts>
<marker>Zhou, Lin, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-evaluating machine translation results with paraphrase support. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 77–84, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>