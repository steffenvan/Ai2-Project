<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000301">
<title confidence="0.806128">
CMU: Arc-Factored, Discriminative Semantic Dependency Parsing
</title>
<author confidence="0.8331895">
Sam Thomson Brendan O’Connor Jeffrey Flanigan David Bamman
Jesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. Smith
</author>
<affiliation confidence="0.904923">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.8075655">
{sthomson,brenocon,jflanigan,dbamman,jessed,
swabha,nschneid,cdyer,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999860666666667">
We present an arc-factored statistical model
for semantic dependency parsing, as de-
fined by the SemEval 2014 Shared Task 8
on Broad-Coverage Semantic Dependency
Parsing. Our entry in the open track placed
second in the competition.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993563592592593">
The task of broad coverage semantic dependency
parsing aims to provide a shallow semantic analysis
of text not limited to a specific domain. As distinct
from deeper semantic analysis (e.g., parsing to a
full lambda-calculus logical form), shallow seman-
tic parsing captures relationships between pairs
of words or concepts in a sentence, and has wide
application for information extraction, knowledge
base population, and question answering (among
others).
We present here two systems that produce seman-
tic dependency parses in the three formalisms of the
SemEval 2014 Shared Task 8 on Broad-Coverage
Semantic Dependency Parsing (Oepen et al., 2014).
These systems generate parses by extracting fea-
tures for each potential dependency arc and learn-
ing a statistical model to discriminate between good
arcs and bad; the first treats each labeled edge de-
cision as an independent multiclass logistic regres-
sion (§3.2.1), while the second predicts arcs as part
of a graph-based structured support vector machine
(§3.2.2). Common to both models is a rich set of
features on arcs, described in §3.2.3. We include a
discussion of features found to have no discernable
effect, or negative effect, during development (§4).
Our system placed second in the open track of
the Broad-Coverage Semantic Dependency Parsing
</bodyText>
<footnote confidence="0.94989525">
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<figureCaption confidence="0.9979795">
Figure 1: Example annotations for DM (top), PAS (middle),
and PCEDT (bottom).
</figureCaption>
<bodyText confidence="0.997901666666667">
task (in which output from syntactic parsers and
other outside resources can be used). We present
our results in §5.
</bodyText>
<sectionHeader confidence="0.99915" genericHeader="introduction">
2 Formalisms
</sectionHeader>
<bodyText confidence="0.999971333333333">
The Shared Task 8 dataset consists of annota-
tions of the WSJ Corpus in three different se-
mantic dependency formalisms. DM is derived
from LinGO English Resource Grammar (ERG)
annotations in DeepBank (Flickinger et al., 2012).
PAS is derived from the Enju HPSG treebank us-
ing the conversion rules of Miyao et al. (2004).
PCEDT is derived from the tectogrammatical layer
of the Prague Czech-English Dependency Treebank
(Hajiˇc, 1998). See Figure 1 for an example.
The three formalisms come from very different
linguistic theories, but all are represented as labeled
directed graphs, with words as vertices, and all
have “top” annotations, corresponding roughly to
the semantic focus of the sentence. (A “top” need
not be a root of the graph.) This allows us to use
the same machinery (§3) for training and testing
statistical models for the three formalisms.
</bodyText>
<sectionHeader confidence="0.99537" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.9996944">
We treat the problem as a three-stage pipeline. The
first stage prunes words by predicting whether they
have any incoming or outgoing edges at all (§3.1);
if a word does not, then it is not considered for
any attachments in later stages. The second stage
</bodyText>
<page confidence="0.98245">
176
</page>
<note confidence="0.7306155">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 176–180,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999944714285714">
predicts where edges are present, and their labels
(§3.2). The third stage predicts whether a predicate
word is a top or not (§3.3). Formalisms sometimes
annotate more than one “top” per sentence, but we
found that we achieve the best performance on all
formalisms by predicting only the one best-scoring
“top” under the model.
</bodyText>
<subsectionHeader confidence="0.999766">
3.1 Singleton Classification
</subsectionHeader>
<bodyText confidence="0.999842692307692">
For each formalism, we train a classifier to rec-
ognize singletons, nodes that have no parents or
children. (For example, punctuation tokens are of-
ten singletons.) This makes the system faster with-
out affecting accuracy. For singleton prediction,
we use a token-level logistic regression classifier,
with features including the word, its lemma, and
its part-of-speech tag. If the classifier predicts a
probability of 99% or higher the token is pruned;
this removes around 10% of tokens. (The classi-
fier performs differently on different formalisms;
on PAS it has perfect accuracy, while on DM and
PCEDT accuracy is in the mid-90’s.)
</bodyText>
<subsectionHeader confidence="0.999547">
3.2 Edge Prediction
</subsectionHeader>
<bodyText confidence="0.999991105263158">
In the second stage of the pipeline, we predict the
set of labeled directed edges in the graph. We use
the same set of edge-factored features (§3.2.3) in
two alternative models: an edge-independent mul-
ticlass logistic regression model (LOGISTICEDGE,
§3.2.1); and a structured SVM (Taskar et al., 2003;
Tsochantaridis et al., 2004) that enforces a deter-
minism constraint for certain labels, which allows
each word to have at most one outgoing edge with
that label (SVMEDGE, §3.2.2). For each formalism,
we trained both models with varying features en-
abled and hyperparameter settings and submitted
the configuration that produced the best labeled F1
on the development set. For DM and PCEDT, this
was LOGISTICEDGE; for PAS, this was SVMEDGE.
We report results only for the submitted configu-
rations, with different features enabled. Due to
time constraints, full hyperparameter sweeps and
comparable feature sweeps were not possible.
</bodyText>
<subsectionHeader confidence="0.888042">
3.2.1 LOGISTICEDGE Parser
</subsectionHeader>
<bodyText confidence="0.99937875">
The LOGISTICEDGE model considers only token
index pairs (i, j) where |i − j |G 10, i =� j,
and both ti and tj have been predicted to be non-
singletons by the first stage. Although this prunes
some gold edges, among the formalisms, 95%–97%
of all gold edges are between tokens of distance
10 or less. Both directions i —* j and j —* i are
considered between every pair.
Let L be the set of K + 1 possible output labels:
the formalism’s original K edge labels, plus the
additional label NOEDGE, which indicates that no
edge exists from i to j. The model treats every pair
of token indices (i, j) as an independent multiclass
logistic regression over output space L. Let x be
an input sentence. For candidate parent index i,
child index j, and edge label E, we extract a feature
vector f(x, i, j, E), where E is conjoined with every
feature described in §3.2.3. The multiclass logis-
tic regression model defines a distribution over L,
parametrized by weights φ:
</bodyText>
<equation confidence="0.769832">
explφ - f(x, i, j, E)}
P(E  |φ, x, i, j) = �t,∈L explφ - f(x, i, j, E&apos;)}.
</equation>
<bodyText confidence="0.999412733333333">
φ is learned by minimizing total negative log-
likelihood of the above (with weighting; see be-
low), plus E2 regularization. AdaGrad (Duchi et al.,
2011) is used for optimization. This seemed to opti-
mize faster than L-BFGS (Liu and Nocedal, 1989),
at least for earlier iterations, though we did no sys-
tematic comparison. Stochastic gradient steps are
applied one at a time from individual examples,
and a gradient step for the regularizer is applied
once per epoch.
The output labels have a class imbalance; in all
three formalisms, there are many more NOEDGE
examples than true edge examples. We improved
F1 performance by downweighting NOEDGE
examples through a weighted log-likelihood
</bodyText>
<equation confidence="0.69848175">
objective, � �t wt log P(E  |φ, x, i, j), with
i,j
wNOEDGE = 0.3 (selected on development set) and
wt = 1 otherwise.
</equation>
<bodyText confidence="0.9997583">
Decoding: To predict a graph structure at test-time
for a new sentence, the most likely edge label is pre-
dicted for every candidate (i, j) pair of unpruned
tokens. If an edge is predicted for both directions
for a single (i, j) pair, only the edge with the higher
score is chosen. (There are no such bidirectional
edges in the training data.) This post-processing ac-
tually did not improve accuracy on DM or PCEDT;
it did improve PAS by Pz�0.2% absolute F1, but we
did not submit LOGISTICEDGE for PAS.
</bodyText>
<subsectionHeader confidence="0.805702">
3.2.2 SVMEDGE Parser
</subsectionHeader>
<bodyText confidence="0.9999522">
In the SVMEDGE model, we use a structured SVM
with a determinism constraint. This constraint en-
sures that each word token has at most one outgoing
edge for each label in a set of deterministic labels
Ld. For example, in DM a predicate never has more
</bodyText>
<page confidence="0.989632">
177
</page>
<bodyText confidence="0.997837461538461">
than one child with edge label “ARG1.” Ld was
chosen to be the set of edges that were &gt; 99.9%
deterministic in the training data.1
Consider the fully dense graph of all edges be-
tween all words predicted as not singletons by the
singleton classifier §3.1 (in all directions with all
possible labels). Unlike LOGISTICEDGE, the la-
bel set L does not include an explicit NOEDGE
label. If ψ denotes the model weights, and f de-
notes the features, then an edge from i to j with
label E in the dense graph has a weight c(i, j, E)
assigned to it using the linear scoring function
c(i, j, E) = ψ &apos; f(x, i, j, E).
Decoding: For each node and each label E, if E E
Ld, the decoder adds the highest scoring outgoing
edge, if its weight is positive. For E E� Ld, every
outgoing edge with positive weight is added. This
procedure is guaranteed to find the highest scoring
subgraph (largest sum of edge weights) of the dense
graph subject to the determinism constraints. Its
runtime is O(n2).
The model weights are trained using the struc-
tured SVM loss. If x is a sentence and y is a
graph over that sentence, let the features be de-
noted f(x, y) = E(i,j,`)∈y f(x, i, j, E). The SVM
loss for each training example (xi, yi) is:
</bodyText>
<equation confidence="0.894457">
−ψTf(xi, yi)+max ψTf(xi, y)+cost(y, yi)
y
</equation>
<bodyText confidence="0.9988336">
where cost(y, yi) = α|y \ yi |+ Q|yi \ y|. α and
Q trade off between precision and recall for the
edges (Gimpel and Smith, 2010). The loss is min-
imized with AdaGrad using early-stopping on a
development set.
</bodyText>
<subsectionHeader confidence="0.549246">
3.2.3 Edge Features
</subsectionHeader>
<bodyText confidence="0.999825166666667">
Table 1 describes the features we used for predict-
ing edges. These features were computed over an
edge e with parent token s at index i and child
token t at index j. Unless otherwise stated, each
feature template listed has an indicator feature that
fires for each value it can take on. For the sub-
mitted results, LOGISTICEDGE uses all features
except Dependency Path v2, POS Path, and Dis-
tance Thresholds, and SVMEDGE uses all features
except Dependency Path v1. This was due to
SVMEDGE being faster to train than LOGISTIC-
EDGE when including POS Path features, and due
</bodyText>
<footnote confidence="0.9201934">
1 By this we mean that of the nodes that have at least
one outgoing f edge, 99.9% of them have only one outgo-
ing f edge. For DM, Ld = L\{“ and c,” “ or c,” “ then c,”
“loc,” “mwe,” “subord”}; for PAS, Ld = L; and for PCEDT,
Ld ={“DPHR,” “INTF,” “VOCAT”}.
</footnote>
<table confidence="0.909992166666667">
Tokens: The tokens s and t themselves.
Lemmas: Lemmas of s and t.
POS tags: Part of speech tags of s and t.
Linear Order: Fires if i &lt; j.
Linear Distance: i − j.
Dependency Path v1 (LOGISTICEDGE only): The
</table>
<figureCaption confidence="0.822809666666667">
concatenation of all POS tags, arc labels and up/down
directions on the path in the syntactic dependency tree
from s to t. Conjoined with s, with t, and without either.
Dependency Path v2 (SVMEDGE only): Same as De-
pendency Path v1, but with the lemma of s or t instead
of the word, and substituting the token for any “IN” POS
tag.
Up/Down Dependency Path: The sequence of upward
and downward moves needed to get from s to t in the
syntactic dependency tree.
Up/Down/Left/Right Dependency Path: The unla-
beled path through the syntactic dependency tree from s
to t, annotated with whether each step through the tree
was up or down, and whether it was to the right or left in
the sentence.
Is Parent: Fires if s is the parent of t in the syntactic
dependency parse.
Dependency Path Length: Distance between s and t in
the syntactic dependency parse.
POS Context: Concatenated POS tags of tokens at i−1,
i, i + 1, j − 1, j, and j + 1. Concatenated POS tags of
tokens at i − 1, i, j − 1, and j. Concatenated POS tags
of tokens at i, i + 1, j, and j + 1.
Subcategorization Sequence: The sequence of depen-
dency arc labels out of s, ordered by the index of the
child. Distinguish left children from right children. If t
is a direct child of s, distinguish its arc label with a “+”.
Conjoin this sequence with the POS tag of s.
Subcategorization Sequence with POS: As above, but
add the POS tag of each child to its arc label.
POS Path (SVMEDGE only): Concatenated POS tags
between and including i and j. Conjoined with head
lemma, with dependent lemma, and without either.
Distance Thresholds (SVMEDGE only): Fires for ev-
ery integer between 1 and Llog(|i − j |+ 1)/ log(1.39)J
inclusive.
</figureCaption>
<tableCaption confidence="0.996111">
Table 1: Features used in edge prediction
</tableCaption>
<bodyText confidence="0.998441">
to time constraints for the submission we were un-
able to retrain LOGISTICEDGE with these features.
</bodyText>
<subsectionHeader confidence="0.808691">
3.2.4 Feature Hashing
</subsectionHeader>
<bodyText confidence="0.999957714285714">
The biggest memory usage was in the map from
feature names to integer indices during feature
extraction. For experimental expedience, we im-
plemented multitask feature hashing (Weinberger
et al., 2009), which hashes feature names to indices,
under the theory that errors due to collisions tend
to cancel. No drop in accuracy was observed.
</bodyText>
<subsectionHeader confidence="0.998586">
3.3 Top Prediction
</subsectionHeader>
<bodyText confidence="0.99980975">
We trained a separate token-level binary logistic
regression model to classify whether a token’s node
had the “top” attribute or not. At decoding time, all
predicted predicates (i.e., nodes where there is at
</bodyText>
<page confidence="0.996618">
178
</page>
<bodyText confidence="0.999968">
least one outbound edge) are possible candidates
to be “top”; the classifier probabilities are evalu-
ated, and the highest-scoring node is chosen to be
“top.” This is suboptimal, since some graphs have
multiple tops (in PCEDT this is more common);
but selection rules based on probability thresholds
gave worse F1 performance on the dev set. For a
given token t at index i, the top classifier’s features
included t’s POS tag, i, those two conjoined, and
the depth of t in the syntactic dependency tree.
</bodyText>
<sectionHeader confidence="0.990638" genericHeader="method">
4 Negative Results
</sectionHeader>
<bodyText confidence="0.999958153846154">
We followed a forward-selection process during
feature engineering. For each potential feature,
we tested the current feature set versus the current
feature set plus the new potential feature. If the
new feature did not improve performance, we did
not add it. We list in table 2 some of the features
which we tested but did not improve performance.
In order to save time, we ran these feature se-
lection experiments on a subsample of the training
data, for a reduced number of iterations. These re-
sults thus have a strong caveat that the experiments
were not exhaustive. It may be that some of these
features could help under more careful study.
</bodyText>
<sectionHeader confidence="0.995362" genericHeader="evaluation">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999945">
We participated in the Open Track, and used the
syntactic dependency parses supplied by the orga-
nizers. Feature engineering was performed on a
development set (§20), training on §§00–19. We
evaluate labeled precision (LP), labeled recall (LR),
labeled F1 (LF), and labeled whole-sentence match
(LM) on the held-out test data using the evaluation
script provided by the organizers. LF was aver-
aged over the formalisms to determine the winning
system. Table 3 shows our scores.
</bodyText>
<sectionHeader confidence="0.989075" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99994525">
We found that feature-rich discriminative models
perform well at the task of mapping from sentences
to semantic dependency parses. While our final
approach is fairly standard for work in parsing,
we note here additional features and constraints
which did not appear to help (contrary to expecta-
tion). There are a number of clear extensions to
this work that could improve performance. While
an edge-factored model allows for efficient infer-
ence, there is much to be gained from higher-order
features (McDonald and Pereira, 2006; Martins
et al., 2013). The amount of information shared
</bodyText>
<tableCaption confidence="0.9537766875">
Word vectors: Features derived from 64-dimensional
vectors from (Faruqui and Dyer, 2014), including the
concatenation, difference, inner product, and element-
wise multiplication of the two vectors associated with
a parent-child edge. We also trained a Random Forest
on the word vectors using Liaw and Wiener’s (2002) R
implementation. The predicted labels were then used as
features in LOGISTICEDGE.
Brown clusters Features derived from Brown clusters
(Brown et al., 1992) trained on a large corpus of web data.
Parent, child, and conjoined parent-child edge features
from cluster prefixes of length 2, 4, 6, 8, 10, and 12.
Conjunctions of those features with the POS tags of the
parent and child tokens.
Active/passive: Active/passive voice feature (as in Jo-
hansson and Nugues (2008)) conjoined with both the
Linear Distance features and the Subcategorization Se-
quence features. Voice information may already be cap-
tured by features from the Stanford dependency–style
parses, which include passivization information in arc
labels such as nsubjpass and auxpass (de Marneffe and
Manning, 2008).
Connectivity constraint: Enforcing that the graph is
connected (ignoring singletons), similar to Flanigan et al.
(2014). Almost all semantic dependency graphs in the
training data are connected (ignoring singletons), but
we found that enforcing this constraint significantly hurt
precision.
Tree constraint: Enforces that the graph is a tree. Un-
surprisingly, we found that enforcing a tree constraint
hurt performance.
Table 2: Features and constraints giving negative results.
</tableCaption>
<table confidence="0.9998944">
LP LR LF LM
DM 0.8446 0.8348 0.8397 0.0875
PAS 0.9078 0.8851 0.8963 0.2604
PCEDT 0.7681 0.7072 0.7364 0.0712
Average 0.8402 0.8090 0.8241 0.1397
</table>
<tableCaption confidence="0.992207">
Table 3: Labeled precision (LP), recall (LR), Fl (LF), and
whole-sentence match (LM) on the held-out test data.
</tableCaption>
<bodyText confidence="0.9999638">
between the three formalisms suggests that a multi-
task learning (Evgeniou and Pontil, 2004) frame-
work could lead to gains. And finally, there is
additional structure in the formalisms which could
be exploited (such as the deterministic processes
by which an original PCEDT tree annotation was
converted into a graph); formulating more subtle
graph constraints to capture this a priori knowl-
edge could lead to improved performance. We
leave such explorations to future work.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999575125">
We are grateful to Manaal Faruqui for his help in word vector
experiments, and to reviewers for helpful comments. The re-
search reported in this paper was sponsored by the U.S. Army
Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533, DARPA
grant FA8750-12-2-0342 funded under the DEFT program,
U.S. NSF grants IIS-1251131 and IIS-1054319, and Google’s
support of the Reading is Believing project at CMU.
</bodyText>
<page confidence="0.998346">
179
</page>
<sectionHeader confidence="0.996113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999714844155844">
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Lin-
guistics, 18(4):467–479.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
Coling 2008: Proc. of the Workshop on Cross-Framework
and Cross-Domain Parser Evaluation, pages 1–8. Manch-
ester, UK.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adap-
tive subgradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Research,
12:2121–2159.
Theodoros Evgeniou and Massimiliano Pontil. 2004. Regular-
ized multitask learning. In Proc. of KDD, pages 109–117.
Seattle, WA, USA.
Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correlation.
In Proc. of EACL, pages 462–471. Gothenburg, Sweden.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer,
and Noah A. Smith. 2014. A discriminative graph-based
parser for the Abstract Meaning Representation. In Proc.
of ACL, pages 1426–1436. Baltimore, MD, USA.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. Deep-
Bank: a dynamically annotated treebank of the Wall Street
Journal. In Proc. of the Eleventh International Workshop on
Treebanks and Linguistic Theories, pages 85–96. Lisbon,
Portugal.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity. URL http://lti.cs.cmu.edu/sites/
default/files/research/reports/2010/
cmulti10008.pdf.
Jan Hajiˇc. 1998. Building a syntactically annotated corpus:
the Prague Dependency Treebank. In Eva Hajiˇcov´a, ed-
itor, Issues of Valency and Meaning. Studies in Honour
of Jarmila Panevov´a, pages 106–132. Prague Karolinum,
Charles University Press, Prague.
Richard Johansson and Pierre Nugues. 2008. Dependency-
based semantic role labeling of PropBank. In Proc. of
EMNLP, pages 69–78. Honolulu, HI, USA.
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomForest. R News, 2(3):18–
22. URL http://cran.r-project.org/web/
packages/randomForest/.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45(3):503–528.
Andr´e F. T. Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL, pages 617–622. Sofia,
Bulgaria.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proc. of
EACL, pages 81–88. Trento, Italy.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
head-driven phrase structure grammar from the Penn Tree-
bank. In Proc. of IJCNLP, pages 684–693. Hainan Island,
China.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel
Zeman, Dan Flickinger, Jan Hajiˇc, Angelina Ivanova, and
Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage
semantic dependency parsing. In Proc. of SemEval. Dublin,
Ireland.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin Markov networks. In Proc. of NIPS, pages 25–32.
Vancouver, British Columbia, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
of ICML, pages 104–111. Banff, Alberta, Canada.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In Proc. of ICML, pages
1113–1120. Montreal, Quebec, Canada.
</reference>
<page confidence="0.997763">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.470922">
<title confidence="0.998168">CMU: Arc-Factored, Discriminative Semantic Dependency Parsing</title>
<author confidence="0.984203">Sam Thomson Brendan O’Connor Jeffrey Flanigan David</author>
<affiliation confidence="0.835436333333333">Jesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.992788">Pittsburgh, PA 15213,</address>
<abstract confidence="0.930331142857143">We present an arc-factored statistical model for semantic dependency parsing, as defined by the SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing. Our entry in the open track placed second in the competition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="15853" citStr="Brown et al., 1992" startWordPosition="2662" endWordPosition="2665">ference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in LOGISTICEDGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneff</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proc. of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<location>Manchester, UK.</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proc. of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="6757" citStr="Duchi et al., 2011" startWordPosition="1067" endWordPosition="1070">ry pair of token indices (i, j) as an independent multiclass logistic regression over output space L. Let x be an input sentence. For candidate parent index i, child index j, and edge label E, we extract a feature vector f(x, i, j, E), where E is conjoined with every feature described in §3.2.3. The multiclass logistic regression model defines a distribution over L, parametrized by weights φ: explφ - f(x, i, j, E)} P(E |φ, x, i, j) = �t,∈L explφ - f(x, i, j, E&apos;)}. φ is learned by minimizing total negative loglikelihood of the above (with weighting; see below), plus E2 regularization. AdaGrad (Duchi et al., 2011) is used for optimization. This seemed to optimize faster than L-BFGS (Liu and Nocedal, 1989), at least for earlier iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch. The output labels have a class imbalance; in all three formalisms, there are many more NOEDGE examples than true edge examples. We improved F1 performance by downweighting NOEDGE examples through a weighted log-likelihood objective, � �t wt log P(E |φ, x, i, j), with i,j wNOEDGE = 0.3 (</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodoros Evgeniou</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Regularized multitask learning.</title>
<date>2004</date>
<booktitle>In Proc. of KDD,</booktitle>
<pages>109--117</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="17301" citStr="Evgeniou and Pontil, 2004" startWordPosition="2881" endWordPosition="2884">ing singletons), but we found that enforcing this constraint significantly hurt precision. Tree constraint: Enforces that the graph is a tree. Unsurprisingly, we found that enforcing a tree constraint hurt performance. Table 2: Features and constraints giving negative results. LP LR LF LM DM 0.8446 0.8348 0.8397 0.0875 PAS 0.9078 0.8851 0.8963 0.2604 PCEDT 0.7681 0.7072 0.7364 0.0712 Average 0.8402 0.8090 0.8241 0.1397 Table 3: Labeled precision (LP), recall (LR), Fl (LF), and whole-sentence match (LM) on the held-out test data. between the three formalisms suggests that a multitask learning (Evgeniou and Pontil, 2004) framework could lead to gains. And finally, there is additional structure in the formalisms which could be exploited (such as the deterministic processes by which an original PCEDT tree annotation was converted into a graph); formulating more subtle graph constraints to capture this a priori knowledge could lead to improved performance. We leave such explorations to future work. Acknowledgements We are grateful to Manaal Faruqui for his help in word vector experiments, and to reviewers for helpful comments. The research reported in this paper was sponsored by the U.S. Army Research Laboratory</context>
</contexts>
<marker>Evgeniou, Pontil, 2004</marker>
<rawString>Theodoros Evgeniou and Massimiliano Pontil. 2004. Regularized multitask learning. In Proc. of KDD, pages 109–117. Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation.</title>
<date>2014</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>462--471</pages>
<location>Gothenburg,</location>
<contexts>
<context position="15470" citStr="Faruqui and Dyer, 2014" startWordPosition="2605" endWordPosition="2608">erform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in LOGISTICEDGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent an</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proc. of EACL, pages 462–471. Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the Abstract Meaning Representation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1426--1436</pages>
<location>Baltimore, MD, USA.</location>
<contexts>
<context position="16594" citStr="Flanigan et al. (2014)" startWordPosition="2774" endWordPosition="2777">f length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision. Tree constraint: Enforces that the graph is a tree. Unsurprisingly, we found that enforcing a tree constraint hurt performance. Table 2: Features and constraints giving negative results. LP LR LF LM DM 0.8446 0.8348 0.8397 0.0875 PAS 0.9078 0.8851 0.8963 0.2604 PCEDT 0.7681 0.7072 0.7364 0.0712 Average 0.8402 0.8090 0.8241 0.1397 Table 3: Labeled precision (LP), recall (LR), Fl (LF), and whole-sentence match (LM) on the held</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the Abstract Meaning Representation. In Proc. of ACL, pages 1426–1436. Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
</authors>
<title>DeepBank: a dynamically annotated treebank of the Wall Street Journal.</title>
<date>2012</date>
<booktitle>In Proc. of the Eleventh International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>85--96</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2572" citStr="Flickinger et al., 2012" startWordPosition="370" endWordPosition="373"> under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ Figure 1: Example annotations for DM (top), PAS (middle), and PCEDT (bottom). task (in which output from syntactic parsers and other outside resources can be used). We present our results in §5. 2 Formalisms The Shared Task 8 dataset consists of annotations of the WSJ Corpus in three different semantic dependency formalisms. DM is derived from LinGO English Resource Grammar (ERG) annotations in DeepBank (Flickinger et al., 2012). PAS is derived from the Enju HPSG treebank using the conversion rules of Miyao et al. (2004). PCEDT is derived from the tectogrammatical layer of the Prague Czech-English Dependency Treebank (Hajiˇc, 1998). See Figure 1 for an example. The three formalisms come from very different linguistic theories, but all are represented as labeled directed graphs, with words as vertices, and all have “top” annotations, corresponding roughly to the semantic focus of the sentence. (A “top” need not be a root of the graph.) This allows us to use the same machinery (§3) for training and testing statistical </context>
</contexts>
<marker>Flickinger, Zhang, Kordoni, 2012</marker>
<rawString>Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. DeepBank: a dynamically annotated treebank of the Wall Street Journal. In Proc. of the Eleventh International Workshop on Treebanks and Linguistic Theories, pages 85–96. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Softmax-margin training for structured log-linear models.</title>
<date>2010</date>
<tech>Technical Report CMU-LTI-10-008,</tech>
<institution>Carnegie Mellon University.</institution>
<note>URL http://lti.cs.cmu.edu/sites/ default/files/research/reports/2010/ cmulti10008.pdf.</note>
<contexts>
<context position="9562" citStr="Gimpel and Smith, 2010" startWordPosition="1579" endWordPosition="1582">tgoing edge with positive weight is added. This procedure is guaranteed to find the highest scoring subgraph (largest sum of edge weights) of the dense graph subject to the determinism constraints. Its runtime is O(n2). The model weights are trained using the structured SVM loss. If x is a sentence and y is a graph over that sentence, let the features be denoted f(x, y) = E(i,j,`)∈y f(x, i, j, E). The SVM loss for each training example (xi, yi) is: −ψTf(xi, yi)+max ψTf(xi, y)+cost(y, yi) y where cost(y, yi) = α|y \ yi |+ Q|yi \ y|. α and Q trade off between precision and recall for the edges (Gimpel and Smith, 2010). The loss is minimized with AdaGrad using early-stopping on a development set. 3.2.3 Edge Features Table 1 describes the features we used for predicting edges. These features were computed over an edge e with parent token s at index i and child token t at index j. Unless otherwise stated, each feature template listed has an indicator feature that fires for each value it can take on. For the submitted results, LOGISTICEDGE uses all features except Dependency Path v2, POS Path, and Distance Thresholds, and SVMEDGE uses all features except Dependency Path v1. This was due to SVMEDGE being faster</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2010. Softmax-margin training for structured log-linear models. Technical Report CMU-LTI-10-008, Carnegie Mellon University. URL http://lti.cs.cmu.edu/sites/ default/files/research/reports/2010/ cmulti10008.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Building a syntactically annotated corpus: the Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>Issues of Valency and Meaning. Studies in Honour of Jarmila Panevov´a,</booktitle>
<pages>106--132</pages>
<editor>In Eva Hajiˇcov´a, editor,</editor>
<publisher>Charles University Press,</publisher>
<location>Prague.</location>
<marker>Hajiˇc, 1998</marker>
<rawString>Jan Hajiˇc. 1998. Building a syntactically annotated corpus: the Prague Dependency Treebank. In Eva Hajiˇcov´a, editor, Issues of Valency and Meaning. Studies in Honour of Jarmila Panevov´a, pages 106–132. Prague Karolinum, Charles University Press, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependencybased semantic role labeling of PropBank.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>69--78</pages>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="16165" citStr="Johansson and Nugues (2008)" startWordPosition="2713" endWordPosition="2717">twise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in LOGISTICEDGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision.</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependencybased semantic role labeling of PropBank. In Proc. of EMNLP, pages 69–78. Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andy Liaw</author>
<author>Matthew Wiener</author>
</authors>
<title>Classification and regression by randomForest.</title>
<date>2002</date>
<journal>R News,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>22</pages>
<note>URL http://cran.r-project.org/web/ packages/randomForest/.</note>
<marker>Liaw, Wiener, 2002</marker>
<rawString>Andy Liaw and Matthew Wiener. 2002. Classification and regression by randomForest. R News, 2(3):18– 22. URL http://cran.r-project.org/web/ packages/randomForest/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="6850" citStr="Liu and Nocedal, 1989" startWordPosition="1083" endWordPosition="1086">ut space L. Let x be an input sentence. For candidate parent index i, child index j, and edge label E, we extract a feature vector f(x, i, j, E), where E is conjoined with every feature described in §3.2.3. The multiclass logistic regression model defines a distribution over L, parametrized by weights φ: explφ - f(x, i, j, E)} P(E |φ, x, i, j) = �t,∈L explφ - f(x, i, j, E&apos;)}. φ is learned by minimizing total negative loglikelihood of the above (with weighting; see below), plus E2 regularization. AdaGrad (Duchi et al., 2011) is used for optimization. This seemed to optimize faster than L-BFGS (Liu and Nocedal, 1989), at least for earlier iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch. The output labels have a class imbalance; in all three formalisms, there are many more NOEDGE examples than true edge examples. We improved F1 performance by downweighting NOEDGE examples through a weighted log-likelihood objective, � �t wt log P(E |φ, x, i, j), with i,j wNOEDGE = 0.3 (selected on development set) and wt = 1 otherwise. Decoding: To predict a graph structure at </context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Miguel Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order non-projective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>617--622</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="15347" citStr="Martins et al., 2013" startWordPosition="2588" endWordPosition="2591">winning system. Table 3 shows our scores. 6 Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in LOGISTICEDGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge feature</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andr´e F. T. Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proc. of ACL, pages 617–622. Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>81--88</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="15324" citStr="McDonald and Pereira, 2006" startWordPosition="2584" endWordPosition="2587">formalisms to determine the winning system. Table 3 shows our scores. 6 Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in LOGISTICEDGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined pa</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of EACL, pages 81–88. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the Penn Treebank.</title>
<date>2004</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>684--693</pages>
<location>Hainan Island, China.</location>
<contexts>
<context position="2666" citStr="Miyao et al. (2004)" startWordPosition="388" endWordPosition="391">r are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ Figure 1: Example annotations for DM (top), PAS (middle), and PCEDT (bottom). task (in which output from syntactic parsers and other outside resources can be used). We present our results in §5. 2 Formalisms The Shared Task 8 dataset consists of annotations of the WSJ Corpus in three different semantic dependency formalisms. DM is derived from LinGO English Resource Grammar (ERG) annotations in DeepBank (Flickinger et al., 2012). PAS is derived from the Enju HPSG treebank using the conversion rules of Miyao et al. (2004). PCEDT is derived from the tectogrammatical layer of the Prague Czech-English Dependency Treebank (Hajiˇc, 1998). See Figure 1 for an example. The three formalisms come from very different linguistic theories, but all are represented as labeled directed graphs, with words as vertices, and all have “top” annotations, corresponding roughly to the semantic focus of the sentence. (A “top” need not be a root of the graph.) This allows us to use the same machinery (§3) for training and testing statistical models for the three formalisms. 3 Models We treat the problem as a three-stage pipeline. The </context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the Penn Treebank. In Proc. of IJCNLP, pages 684–693. Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Marco Kuhlmann</author>
<author>Yusuke Miyao</author>
<author>Daniel Zeman</author>
<author>Dan Flickinger</author>
<author>Jan Hajiˇc</author>
<author>Angelina Ivanova</author>
<author>Yi Zhang</author>
</authors>
<title>Task 8: Broad-coverage semantic dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of SemEval.</booktitle>
<location>Dublin, Ireland.</location>
<marker>Oepen, Kuhlmann, Miyao, Zeman, Flickinger, Hajiˇc, Ivanova, Zhang, 2014</marker>
<rawString>Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina Ivanova, and Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage semantic dependency parsing. In Proc. of SemEval. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Maxmargin Markov networks.</title>
<date>2003</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>25--32</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4919" citStr="Taskar et al., 2003" startWordPosition="747" endWordPosition="750">its lemma, and its part-of-speech tag. If the classifier predicts a probability of 99% or higher the token is pruned; this removes around 10% of tokens. (The classifier performs differently on different formalisms; on PAS it has perfect accuracy, while on DM and PCEDT accuracy is in the mid-90’s.) 3.2 Edge Prediction In the second stage of the pipeline, we predict the set of labeled directed edges in the graph. We use the same set of edge-factored features (§3.2.3) in two alternative models: an edge-independent multiclass logistic regression model (LOGISTICEDGE, §3.2.1); and a structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2004) that enforces a determinism constraint for certain labels, which allows each word to have at most one outgoing edge with that label (SVMEDGE, §3.2.2). For each formalism, we trained both models with varying features enabled and hyperparameter settings and submitted the configuration that produced the best labeled F1 on the development set. For DM and PCEDT, this was LOGISTICEDGE; for PAS, this was SVMEDGE. We report results only for the submitted configurations, with different features enabled. Due to time constraints, full hyperparameter sweeps and comparable fe</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Maxmargin Markov networks. In Proc. of NIPS, pages 25–32. Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>104--111</pages>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="4949" citStr="Tsochantaridis et al., 2004" startWordPosition="751" endWordPosition="754">rt-of-speech tag. If the classifier predicts a probability of 99% or higher the token is pruned; this removes around 10% of tokens. (The classifier performs differently on different formalisms; on PAS it has perfect accuracy, while on DM and PCEDT accuracy is in the mid-90’s.) 3.2 Edge Prediction In the second stage of the pipeline, we predict the set of labeled directed edges in the graph. We use the same set of edge-factored features (§3.2.3) in two alternative models: an edge-independent multiclass logistic regression model (LOGISTICEDGE, §3.2.1); and a structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2004) that enforces a determinism constraint for certain labels, which allows each word to have at most one outgoing edge with that label (SVMEDGE, §3.2.2). For each formalism, we trained both models with varying features enabled and hyperparameter settings and submitted the configuration that produced the best labeled F1 on the development set. For DM and PCEDT, this was LOGISTICEDGE; for PAS, this was SVMEDGE. We report results only for the submitted configurations, with different features enabled. Due to time constraints, full hyperparameter sweeps and comparable feature sweeps were not possible</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proc. of ICML, pages 104–111. Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>1113--1120</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="12736" citStr="Weinberger et al., 2009" startWordPosition="2159" endWordPosition="2162">OS Path (SVMEDGE only): Concatenated POS tags between and including i and j. Conjoined with head lemma, with dependent lemma, and without either. Distance Thresholds (SVMEDGE only): Fires for every integer between 1 and Llog(|i − j |+ 1)/ log(1.39)J inclusive. Table 1: Features used in edge prediction to time constraints for the submission we were unable to retrain LOGISTICEDGE with these features. 3.2.4 Feature Hashing The biggest memory usage was in the map from feature names to integer indices during feature extraction. For experimental expedience, we implemented multitask feature hashing (Weinberger et al., 2009), which hashes feature names to indices, under the theory that errors due to collisions tend to cancel. No drop in accuracy was observed. 3.3 Top Prediction We trained a separate token-level binary logistic regression model to classify whether a token’s node had the “top” attribute or not. At decoding time, all predicted predicates (i.e., nodes where there is at 178 least one outbound edge) are possible candidates to be “top”; the classifier probabilities are evaluated, and the highest-scoring node is chosen to be “top.” This is suboptimal, since some graphs have multiple tops (in PCEDT this i</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In Proc. of ICML, pages 1113–1120. Montreal, Quebec, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>