<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001690">
<title confidence="0.983731">
Automatic Word Spacing Using Hidden Markov Model
for Refining Korean Text Corpora
</title>
<note confidence="0.817355333333333">
Do-Gil Lee and Sang-Zoo Lee and Hae-Chang Rim
NLP Lab., Dept. of Computer Science and Engineering, Korea University
1, 5-ka, Anam-dong, Seongbuk-ku, Seoul 136-701, Korea
</note>
<author confidence="0.989816">
Heui-Seok Lim
</author>
<affiliation confidence="0.995095">
Dept. of Information and Communications, Chonan University
</affiliation>
<address confidence="0.884359">
115 AnSeo-dong, CheonAn 330-704, Korea
</address>
<sectionHeader confidence="0.941826" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999881875">
This paper proposes a word spacing model using
a hidden Markov model (HMM) for refining Ko-
rean raw text corpora. Previous statistical ap-
proaches for automatic word spacing have used
models that make use of inaccurate probabilities
because they do not consider the previous spac-
ing state. We consider word spacing problem as
a classification problem such as Part-of-Speech
(POS) tagging and have experimented with var-
ious models considering extended context. Ex-
perimental result shows that the performance
of the model becomes better as the more con-
text considered. In case of the same number
of parameters are used with other method, it
is proved that our model is more effective by
showing the better results.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988664142857143">
Automatic word spacing is a process to de-
cide correct boundaries between words in a sen-
tence containing spacing errors. In Korean,
word spacing is very important to increase the
readability and to communicate the accurate
meaning of a text. For example, if a sentence
�O}!QtI7} 1:1�.\• t
[þ #Q 7} V L}(Father entered the
room)&amp;quot; is written as &amp;quot;a}!Qtj [þt#Q7}V
L}(Father entered the bag)&amp;quot;, then its meaning
is changed a lot.
There are many word spacing errors in doc-
uments on the Internet, which is the principal
source of information. To deal with these docu-
ments properly, an automatic word spacing sys-
tem is absolutely necessary. Besides, it plays
an important role as a preprocessor of a mor-
phological analyzer that is a fundamental tool
for natural language processing applications, a
postprocessor to restore line boundaries from
an OCR, a postprocessor for continuous-syllable
sentence from a speech recognition system, and
one module for an orthographic error revision
system.
In Korean, spacing unit is Eojeol. Each Eo-
jeol consists of one or more words and a word
consists of one or more morphemes. Figure
1 represents their relationships for a sentence
���� Ãº4 s1a}l1x11_`�¦ 01�Vut&amp;quot;. According to the
rules of Korean spelling, the main principle for
word spacing is to split every word in a sen-
tence. Because one morpheme may form a word
and several morphemes too, there are confusing
cases to distinguish among words. Even though
postpositions belong to words, they should be
concatenated with the preceding word. Besides,
there are many conflicting (but can be permit-
ted) cases with the principles. For example,
spacing or concatenating individual nouns in-
cluding a compound noun are both considered
as right. As mentioned, word spacing is impor-
tant for some reasons, but it is difficult for even
man to space words correctly by spelling rules
because of the characteristics of Korean and the
inconsistent rules. Especially, it is much more
confused in the case of having no influence on
understanding the meaning of a sentence.
In this paper, we propose a word spacing
model 1 using an HMM. HMM is a widely used
statistical model to solve various NLP prob-
lems such as POS tagging(Charniak et al., 1993;
Merialdo, 1994; Kim et al., 1998a; Lee, 1999).
We regard the word spacing problem as a classi-
fication problem such as the POS tagging prob-
lem. When using an HMM for automatic word
spacing task, raw texts can be used as training
</bodyText>
<footnote confidence="0.613665">
&apos;Strictly speaking, our model described here is an Eo-
jeol spacing model rather than a word spacing model
because spacing unit of Korean is Eojeol. But we in
this paper do not distinguish between Eojeol and word
for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as
word, spacing unit in English.
</footnote>
<figureCaption confidence="0.999955">
Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot;
</figureCaption>
<figure confidence="0.989604894736842">
T
010P1
M
V
Q
morpheme
7f
�
OI0[)I z
7f
OIOPI�
z
MT)[
T
Eojeol
word
���
���
z
</figure>
<bodyText confidence="0.985530333333333">
data. Therefore, we expect that HMM can be
applied to the task effectively without bothering
to construct training data.
</bodyText>
<sectionHeader confidence="0.998756" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.981829392857143">
Previous approaches for automatic word spac-
ing can be classified into two groups: rule based
approach and statistical approach. The rule-
based approach uses lexical information and
heuristic rules(Choi, 1997; Kim et al., 1998b;
Kang, 1998; Kang, 2000). Lexical information
consists of postposition and Eomi2 information,
a list of spaced word examples, etc. Heuristic
rules are composed of longest match or short-
est match rule, morphological rules, and error
patterns. This approach has disadvantage re-
quiring higher computational complexity than
the statistical approach. It also costs too much
in constructing and maintaining lexical informa-
tion. Most of rule-based systems use a morpho-
logical analyzer to recognize word boundaries.
Another disadvantages of rule-based approach
are resulted from using morphological analyzer.
First, if ambiguous analyses are possible, fre-
quent backtracking may be caused and many
errors are propagated by an erroneous analy-
sis. Second, results of automatic word spacing
are highly dependent on the morphological an-
alyzer; false word boundary recognition occurs
if morphological analysis fails due to unknown
words. In addition, if an erroneous word is suc-
cessfully analyzed through overgeneration, the
error cannot even be detected. Finally, if a word
</bodyText>
<footnote confidence="0.9737505">
2Eomi is a grammatical morpheme of Korean which
is attached to verbal root
</footnote>
<bodyText confidence="0.999002571428571">
spacing system is used as a preprocessor of a
morphological analyzer, the same morphologi-
cal analyzing process should be repeated twice.
The statistical approach uses syllable statis-
tics extracted from large amount of corpora to
decide whether two adjacent syllables should be
spaced or not(Shim, 1996; Shin and Park, 1997;
Chung and Lee, 1999; Jeon and Park, 2000;
Kang and Woo, 2001). In contrast to the rule-
based approach, it does not require many costs
to construct and to maintain statistics because
they can be acquired automatically. It is more
robust against unknown words than rule-based
approach that uses a morphological analyzer.
A statistical method proposed in Kang and
Woo (2001) has shown the best performance so
far. In this method, word spacing probability
P(xi; xi+1), between two adjacent syllables xi
and xi+1, is in Equation 1. If the probability is
greater than 0:375, a space is inserted between
xi and xi+1.
</bodyText>
<equation confidence="0.999484666666667">
P(xi; xi+1) = 0:25 X PR(xi-1; xi) +
0:5 X PM(xi; xi+1) +
0:25 X PL(xi+1; xi+2) (1)
</equation>
<bodyText confidence="0.9999245">
In Equation 1, PR, PM, and PL denote the
probability of a space being inserted in the right,
middle, and left of the two syllables, respec-
tively. They are calculated as follows:
</bodyText>
<equation confidence="0.9993005">
PR(xi-1; xi) = freq(xi-1; xi)
freq(xi; SPACE; xi+1)
freq(xi-1; xi; SPACE)
PM(xi; xi+1) = freq(xi; xi+1)
PL(xi+1; xi+2) = freq(xi+1; xi+2)
freq(SPACE; xi+1; xi+2)
</equation>
<bodyText confidence="0.999921181818182">
In the above equations, freq(x) denotes a fre-
quency of a string x from training data, and
SPACE denotes a white space.
Similar to this method, other statistical sys-
tems usually use the word spacing probability
estimated from every syllable bigram3 in the
corpora. They calculate the probability by com-
bining PR, PM, and PL and compare it with a
certain threshold. If the probability is higher
than the threshold, then a space is inserted be-
tween two syllables.
It is reported that the performance is so sensi-
tive to training data: it shows somewhat differ-
ent performance according to similarity between
input document and training data. And there is
a crucial problem in the statistical method re-
sulted from not considering the previous spacing
state. For example, consider a sentence &amp;quot;o&apos;�
&apos;�e”~L}&amp;quot; of which correctly word spaced sen-
tence is &amp;quot;o&apos;�1/2+É T e”~L}&amp;quot;. According to Equa-
tion 1, the word spacing probability of &amp;quot;-&apos;F&amp;quot; and
&amp;quot;e”�&amp;quot; will be calculated as follows:
</bodyText>
<equation confidence="0.75256">
P(Ãº;e”~) = 0:25 x PR(1/2+ É; Ãº) + 0:5 x PM(Ãº; e”~)
+ 0:25 x PL(e”~; 4)
The probability PR(1/2+É; Ãº) as follows:
</equation>
<sectionHeader confidence="0.982766" genericHeader="method">
3 Word Spacing Model based on
Hidden Markov Model
</sectionHeader>
<bodyText confidence="0.999929230769231">
POS tagging is the most representative area
for HMM. Before explaining our word spacing
model using HMM, let&apos;s consider the POS tag-
ging model using an HMM. POS tagging func-
tion F(W) is to find the most likely sequence
of POS tags T = (t1; t2; ::: ; tn) for a given sen-
tence of words W = (w1; w2;::: ; wn) and is de-
fined in Equation 2:
Using Bayes&apos; rule, Equation 2 becomes Equa-
tion 3. Since P(W) is a constant for T, Equa-
tion 3 is transformed into Equation 4.
The probability P(T; W) is broken down into
the following equations by using the chain rule:
</bodyText>
<equation confidence="0.999443047619047">
P(T; W) = P(t1;n; w1;n) (6)
P(ti I t1;i-1; w1;i-1)) (7)
xP(wi I t1;i; w1;i�1)
F(W) def P(T I W) (2)
= argmax
T
= argmax
T
= argmax P(T)P(W I T) (4)
T
= argmax P(T; W) (5)
T
P(T)P(W I T)
(3)
P(W)
1/2+É
n
=
i=1
n
PR(1/2+ É; Ãº) = freq(1/2+ É; Ãº; SPACE) freq(1/2+É; Ãº) N i=1 P(ti I ti-K;i-1)P(wi I ti) (8)
</equation>
<bodyText confidence="0.999501133333333">
But a space should have been inserted be-
tween &amp;quot;1/2+É&amp;quot; and &amp;quot;&apos;�&amp;quot; in the correct sentence,
we should use freq(SPACE; Ãº; SPACE) in-
stead of freq(1/2+É; Ãº; SPACE) in order to get
the correct word spacing probability. This phe-
nomenon comes from not considering the previ-
ous spacing state. To alleviate this problem, we
can consider the previous spacing state that the
system has decided before. But errors can be
propagated from the previous false word spac-
ing result. Eventually, to avoid such propagated
errors, the system has to generate all possible in-
terpretations from a given sentence and choose
the best one. To choose the best state from all
possible states, we use an HMM in this paper.
</bodyText>
<footnote confidence="0.947445">
3syllable bigram is defined to be any combination of
two syllables with or without a space.
</footnote>
<bodyText confidence="0.993608944444445">
Markov assumptions (conditional indepen-
dence) used in Equation 8 are that the prob-
ability of a current tag ti conditionally depends
on only the previous K tags and that the prob-
ability of a current word wi conditionally de-
pends on only the current tag. In Equation 8,
P(ti I ti—K;i-1) is called transition probability
and P(wi I ti) is called lexical probability. Mod-
els are classified in terms of K. The larger K
is, the more context can be considered. Because
of the data sparseness problem, bigram model
(K is 1) and trigram model (K is 2) are used in
general.
The word spacing problem can be consid-
ered similar to POS tagging. We define a
word spacing task as a task to find the most
likely sequence of word spacing tags T =
(t1; t2;:::; tn) for a given sentence of syllables
</bodyText>
<equation confidence="0.98993275">
S = (s1; s2; ::: ; sn). Our word spacing model is
defined as in Equation 9:
argmax P(T j S) (9)
T
</equation>
<bodyText confidence="0.99182875">
Word spacing tag is a tag to indicate whether
the current syllable and the next one should
be spaced or not. Tag, 1 means that a space
should be put after the current syllable. Tag,
0 means that the current and the next syllable
should not be spaced. For example, if we at-
tach the word spacing tags to a sentence &amp;quot;/BNÂÒ
ºÃ e”�L}. (I can study)&amp;quot;, then it is tagged as
&amp;quot;/BN/0+ÂÒ/0+1/2+É/1+Ãº/1+e”~/0+U+/0+./1&amp;quot;.
Our proposed word spacing model is to find
the tag sequence T for maximizing the proba-
bility P(T; S).
</bodyText>
<equation confidence="0.999624153846154">
P(T; S )
= P(t1;n; s1;n) (10)
=(P(t1) X p(s1 j t1))
X(P(t2 j t1; s1) X P(s2 j t1;2; s1))
P(t3 j t1;2; s1;2)
X XP(s3 j t1;3; s1;2) X .. .
X P(tn j t1;n—1; s1;n-1) �(11)
XP(sn j t1;n; s1;n-1)
P(ti j t1;i-1; s1;i-1)1 (12)
XP(si t1;i; s1;i-1) J
P(ti j ti-K;i-1; si-J;i-1) �(13)
XP(si
j tiL;i; si�I;i�1)
</equation>
<bodyText confidence="0.9997701">
There are two Markov assumptions in Equa-
tion 13. One is that the probability of a current
tag ti conditionally depends on only the previ-
ous K (word spacing) tags and the previous J
syllables. The other is that the probability of
a current syllable si conditionally depends on
only the previous L tags, the current tag ti, and
the previous I tags. This model is denoted by
A(T(K:J); S(L:I)). Similar to the POS tagging
model, P(ti j ti-K;i-1; si-J;i_1) is called tran-
sition probability, and P(si j ti_L;i; si_I;i_1) is
called syllable probability in Equation 13. On
the other hand, our word spacing model uses
less strict Markov assumptions to consider a
larger context. The larger the values of K, J,
L, and I are, the more context can be consid-
ered. In order to avoid the data sparseness and
excessively increasing parameters of a model, it
is important to select proper values. In our cur-
rent work, they are restricted as follows:
</bodyText>
<equation confidence="0.594038">
0 &lt; K; J; L; I &lt; 2
</equation>
<bodyText confidence="0.999696583333333">
Thus, 3x3x3x3 = 81 models are possible. But
we do not use the case of (K; J) = (0; 0) in the
trasition probabilities. As a result, we actually
use 72 models. It has not yet been known that
which model is the best. We can verify this only
by means of experiments. Some possible models
and their equations are listed in Table 1.
Probabilities can be estimated simply by the
maximum likelihood estimator (MLE) from raw
texts. The syllable probabilities and the tran-
sition probabilities of the model A(T(1:2); S(1:2))
are estimated as follows:
</bodyText>
<equation confidence="0.9982405">
PMLE(ti j ti—1; si-2;i-1)
freq(si-2; ti-1; si-1; ti)
=
freq(si-2; ti-1; si-1)
PMLE(si j ti—1;i; si-2;i-1)
freq(si-2; ti-1; si-1; ti; si)
=
freq(si-2; ti-1; si-1; ti)
</equation>
<bodyText confidence="0.997383714285714">
To avoid zero probability, we just set very low
value such as 0:00001 if an estimated probability
is 0.
The probability that the model
A(T(1:1); S(0:1)) outputs &amp;quot;/BN/0+ÂÒ/0+1/2+É/1+
Ãº/1+e”~/0+Ut/0+./1&amp;quot; from a sentence &amp;quot;/BNÂÒ
Ãºe”~L}.&amp;quot; is calculated as follows:
</bodyText>
<equation confidence="0.999972111111111">
P(T; S) = P(t0 = 0 j s_1 = $; t_1 = 1)
X P(s0 = /BN j s-1 = $; t0 = 0)
X P(t1 = 0 j s0 = /BN; t0 = 0)
X P(s1 = ÒÂ j s0 = /BN; t1 = 0)
X P(1 j ÂÒ0) P(1/2+ É j ÂÒ1)
X P(1 j 1/2+É1) &apos; P(Ãº j1/2+É1)
X P(0 j Ãº1) P(e”~ j Ãº0)
X P(0 je”,0) &apos; P(C4 j e”�0)
X P(1 j 0}0) • P(. j 41)
</equation>
<bodyText confidence="0.988089">
&amp;quot;$&amp;quot; is a pseudo syllable which denotes the start
of a sentence, and its tag is always 1.4 The
</bodyText>
<footnote confidence="0.788721">
4Because any two adjacent sentences should always
be spaced.
</footnote>
<equation confidence="0.988504285714286">
1/2+É
= n
i=1
n
ti
i=1
1/2+É
</equation>
<tableCaption confidence="0.94172">
Table 1: Some models and their equations
</tableCaption>
<figure confidence="0.5459264">
Model Equation
A(T(1:0), S(0:0))
A(T(1:1), S(0:1))
A(T(1:1), S(1:1))
A(T(1:2), S(1:2))
A(T(2:2), S(2:2))
Fn i=1 P(ti j ti-1) -P(si j ti)
n
FUL P (ti j ti-1, si-1) •Psi j ti, si-1
�7 n
11i-1 P (ti j ti-1, si-1) • P (si j ti-1;i, si-1
��77
l 1in-1 P (ti jti-1, si-2;i—1) &apos;Psi j ti—1;i, si-2;i—1)
77��77 n
11i=1 P (ti j ti-2;i-1, si-2;i—1) •Psi j ti-2;i, si-2;i—1)
</figure>
<bodyText confidence="0.986563333333333">
most probable sequence of word spacing tags is
efficiently computed by using the Viterbi algo-
rithm.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999971666666667">
We used balanced 21st Century Sejong Project&apos;s
raw corpus of 26 million word size. As the bal-
anced corpus is used as training data, we ex-
pect that the performance would not be sensi-
tive too much to a certain document genre. The
ETRI POS tagged corpus of 288,269 word size
was used for evaluation. We modified the cor-
pus with no word boundary form for automatic
word spacing evaluation.
We used three kinds of evaluation measures:
syllable-unit accuracy (Psyl), word-unit recall
(Rword), and word-unit precision (Pword). The
word-unit recall is the rate of the number of cor-
rectly spaced words compared to the number of
total words in a test document. The word-unit
precision measures how accurate the system&apos;s
results are. The reason why we do not divide the
syllable-unit accuracy as recall and precision is
that the number of syllables in a document and
that of the system created are the same. Each
measure is defined as follows:
</bodyText>
<equation confidence="0.920299222222222">
Scorrect
Psyl = X 100(%�
Stotal
Wcorrect
Rword =
WDtotal
Wcorrect
Pword =
WStotal
</equation>
<bodyText confidence="0.999837872340426">
Where, Scorrect is the number of correctly
spaced syllables, Stotal is the total number of
syllables in a document, Wcorrect is the number
of correctly spaced words, WDtotal is the total
number of words in a document, and WStotal is
the total number of words created by a system.
To investigate every model, we calculated the
two accuracies for different K, J, L, and I. Ac-
curacies for each model are listed in Table 2.
According to the experimental results, we
are sure that models considering more contexts
show better results. The model A(T(2:2), S(1:2))
is the best for all measures.
Note that some models show the better ac-
curacies than the model A(T(2:2), S(2:2)), which
uses the largest context. It seems that this is
caused by sparseness of data. After evaluat-
ing the method of Kang and Woo (2001) for
our training and test data, it shows 93:06%
syllable-unit accuracy, 76:71% word-unit recall,
and 67:80% word-unit precision. Compared
with these results, our model shows much better
performance. If I is two in A(S(K:J), T(L:I)), syl-
lable trigrams are used. Although I is less than
two (such as the model A(T(2:1), S(1:1), which
uses syllable bigrams), our model is better than
Kang and Woo (2001)&apos;s. This fact tells us that
our model is also more effective even when used
the same number of parameters of the model.
There are two questions that we want to
know about the word spacing models: First,
how much training data is required to get the
best performance of a given model. Second,
which model best fits a given training cor-
pus. To answer these questions, we compare
the performance of various models according to
the size of training corpus in Figure 2. The
left plot shows the syllable-unit precision and
the right plot shows the word-unit precision.
In the figure, &amp;quot;HMM&amp;quot; denotes the proposed
model, and its number decides the model&apos;s
type. &amp;quot;Kang&amp;quot; denotes Kang and Woo (2001)&apos;s
model. &amp;quot;HMM2110&amp;quot; uses syllable unigrams,
&amp;quot;HMM2111&amp;quot; and &amp;quot;Kang&amp;quot; use syllable bigrams,
and &amp;quot;HMM2212&amp;quot; uses syllable trigrams. The
models used here are the models that show the
best accuracies among the models that use same
</bodyText>
<equation confidence="0.956095">
X 100(%)
X 100(%)
</equation>
<tableCaption confidence="0.990933">
Table 2: Experimental results according to (K, J, L, I)
</tableCaption>
<table confidence="0.99749164">
Model Psyl Rword Pword Model Psyl Rword Pword Model Psyl Rword Pword
(0,1,0,0) 84.26 41.28 44.06 (0,1,0,1) 88.93 55.38 57.10 (0,1,0,2) 88.45 53.83 55.88
(0,1,1,0) 89.44 56.91 61.34 (0,1,1,1) 95.58 79.31 82.58 (0,1,1,2) 95.74 79.76 83.68
(0,1,2,0) 84.44 42.15 47.02 (0,1,2,1) 92.86 70.26 71.63 (0,1,2,2) 94.97 76.90 79.45
(0,2,0,0) 85.48 45.65 47.52 (0,2,0,1) 88.93 56.24 57.21 (0,2,0,2) 89.59 58.23 59.88
(0,2,1,0) 90.22 59.12 63.74 (0,2,1,1) 95.60 79.26 82.94 (0,2,1,2) 95.92 80.41 84.56
(0,2,2,0) 86.46 47.62 52.15 (0,2,2,1) 93.44 72.06 73.90 (0,2,2,2) 95.22 77.84 80.59
(1,0,0,0) 85.75 47.05 48.96 (1,0,0,1) 90.24 60.73 62.20 (1,0,0,2) 89.74 58.68 61.09
(1,0,1,0) 89.28 59.80 59.98 (1,0,1,1) 95.64 81.17 81.81 (1,0,1,2) 95.90 81.50 83.56
(1,0,2,0) 82.85 45.10 45.38 (1,0,2,1) 93.30 73.04 73.39 (1,0,2,2) 94.94 77.52 78.88
(1,1,0,0) 85.83 49.95 50.43 (1,1,0,1) 90.96 63.18 64.89 (1,1,0,2) 90.21 62.99 62.58
(1,1,1,0) 89.85 61.47 62.80 (1,1,1,1) 96.15 82.88 84.10 (1,1,1,2) 96.17 82.67 84.86
(1,1,2,0) 84.21 49.44 49.29 (1,1,2,1) 94.07 75.54 76.87 (1,1,2,2) 95.62 80.32 82.13
(1,2,0,0) 87.21 54.25 54.85 (1,2,0,1) 90.83 63.34 64.59 (1,2,0,2) 91.54 66.39 67.00
(1,2,1,0) 90.74 64.14 65.63 (1,2,1,1) 96.07 82.44 84.09 (1,2,1,2) 96.39 83.51 85.91
(1,2,2,0) 86.96 55.50 55.95 (1,2,2,1) 94.67 77.53 79.28 (1,2,2,2) 95.90 81.39 83.42
(2,0,0,0) 86.18 50.25 51.42 (2,0,0,1) 90.44 61.97 63.61 (2,0,0,2) 89.77 61.52 62.17
(2,0,1,0) 89.49 61.07 61.32 (2,0,1,1) 95.83 82.11 82.73 (2,0,1,2) 95.91 82.09 83.39
(2,0,2,0) 83.37 46.52 47.15 (2,0,2,1) 93.55 73.91 74.63 (2,0,2,2) 95.03 78.36 78.96
(2,1,0,0) 86.51 52.60 53.46 (2,1,0,1) 91.10 64.81 65.85 (2,1,0,2) 90.69 65.11 65.10
(2,1,1,0) 90.34 64.04 64.90 (2,1,1,1) 96.29 83.73 84.74 (2,1,1,2) 96.28 83.43 85.21
(2,1,2,0) 85.07 52.32 52.63 (2,1,2,1) 94.31 76.69 77.82 (2,1,2,2) 95.91 81.51 83.45
(2,2,0,0) 88.58 58.94 59.84 (2,2,0,1) 91.78 67.07 68.32 (2,2,0,2) 92.44 69.88 70.54
(2,2,1,0) 91.65 67.82 69.14 (2,2,1,1) 96.26 83.46 84.88 (2,2,1,2) 96.69 84.93 86.82
(2,2,2,0) 88.97 61.20 62.28 (2,2,2,1) 95.01 78.99 80.60 (2,2,2,2) 96.04 82.05 83.96
</table>
<page confidence="0.698229">
98
</page>
<figure confidence="0.99183716">
97
96
95
94
93
92
91
syllable-unit precision (%)
90
89
88
87
86
85
84
83
82
81
80
79
78
77
76
75
74
85
80
70
65
word-unit precision (%)
60
55
50
45
40
75
35
30
25
20
HMM2110
HMM2111
HMM2212
Kang
HMM2110
HMM2111
HMM2212
Kang
10000 100000 1e+06 1e+07 10000 100000 1e+06 1e+07
size of training corpus (# of words) size of training corpus (# of words)
</figure>
<figureCaption confidence="0.999793">
Figure 2: Accuracies according to the size of training corpus
</figureCaption>
<bodyText confidence="0.995924714285714">
syllable ngrams.
We can observe the changes of the accura-
cies according to the size of the training data.
&amp;quot;HMM2110&amp;quot; using syllable unigrams converges
quickly on small training data. &amp;quot;HMM2111&amp;quot;
and &amp;quot;Kang&amp;quot; using syllable bigrams converge
on much more training data. Note that
&amp;quot;HMM2212&amp;quot; does not converge in these plots.
Therefore, there is a possibility of improve-
ment of this model&apos;s performance on more large
training data. &amp;quot;HMM2212&amp;quot; shows lower per-
formance than other models on small training
data. The reason is that the data sparseness
problem occurs.
</bodyText>
<sectionHeader confidence="0.997866" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999196">
Recently, text resources available from the In-
ternet have been rapidly increased. However,
there are many word spacing errors in those re-
sources, which cannot be used before correct-
ing errors. Therefore, the need for automatic
word spacing system to refine text corpora has
been raised. In this paper, we have proposed an
automatic word spacing model using an HMM.
Our method is a statistical approach and does
not require complex processes and costs in con-
structing and maintaining lexical information
as in the rule-based approach. The proposed
model can effectively solve the word spacing
problem by using only syllable statistics auto-
matically extracted from raw corpora. Accord-
ing to the experimental results, our model shows
higher performance than the previous method
even when using the same number of parame-
ters. We used just MLE to estimate probability,
but the more a model extends the context; the
more the data sparseness problem may arise.
In future work, we plan to adopt a smoothing
technique to increase the performance. Further
research on an effective evaluation method for
conflicting cases is also necessary.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719627118644">
E. Charniak, C. Hendrickson, N. Jacobson, and
M. Perkowitz. 1993. Equations for part-of-
speech tagging. In National Conference on
Artificial Intelligence, pages 784-789.
J.-H. Choi. 1997. Automatic Korean spacing
words correction system with bidirectional
longest match strategy. In Proceedings of the
9th Conference on Hangul and Korean Infor-
mation Processing, pages 145-151.
Y.-M. Chung and J.-Y. Lee. 1999. Automatic
word-segmentation at line-breaks for Korean
text processing. In Proceedings of the 6th
Conference of Korea Society for Information
Mangement, pages 21-24.
N.-Y. Jeon and H.-R. Park. 2000. Automatic
word-spacing of syllable bi-gram information
for Korean OCR postprocessing. In Proceed-
ings of the 12th Conference on Hangul and
Korean Information Processing, pages 95-
100.
S.-S. Kang and C.-W. Woo. 2001. Automatic
segmentation of words using syllable bigram
statistics. In Proceedings of the 6th Natural
Language Processing Pacific Rim Symposium,
pages 729-732.
S.-S. Kang. 1998. Automatic word-
segmentation for Hangul sentences. In
Proceedings of the 10th Conference on
Hangul and Korean Information Processing,
pages 137-142.
S.-S. Kang. 2000. Eojeol-block bidirectional
algorithm for automatic word spacing of
Hangul sentences. Journal of the Korea In-
formation Science Society, 27(4):441-447.
J.-D. Kim, H.-S. Lim, S.-Z. Lee, and H.-C. Rim.
1998a. Twoply hidden markov model: A Ko-
rean pos tagging model based on morpheme-
unit with word-unit context. Computer Pro-
cessing of Oriental Languages, 11(3):277-290.
K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998b.
Three-stage spacing system for Korean in
sentence with no word boundaries. Journal
of the Korea Information Science Society,
25(12):1838-1844.
S.-Z. Lee. 1999. New statistical models for au-
tomatic part-of-speech tagging. Ph.D. thesis,
Korea University.
B. Merialdo. 1994. Tagging english text with a
probabilistic model. Computational Linguis-
tics, 20(2):155-172.
Kwangseob Shim. 1996. Automated word-
segmentation for Korean using mutual infor-
mation of syllables. Journal of the Korea In-
formation Science Society, 23(9):991-1000.
J.-H. Shin and H.-R. Park. 1997. A statisti-
cal model for Korean text segmentation using
syllable-level bigrams. In Proceedings of the
9th Conference on Hangul and Korean Infor-
mation Processing, pages 255-260.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265779">
<title confidence="0.996727">Automatic Word Spacing Using Hidden Markov for Refining Korean Text Corpora</title>
<author confidence="0.998972">Lee Lee Rim</author>
<affiliation confidence="0.425093">NLP Lab., Dept. of Computer Science and Engineering, Korea</affiliation>
<address confidence="0.472115">1, 5-ka, Anam-dong, Seongbuk-ku, Seoul 136-701, Korea</address>
<author confidence="0.856623">Heui-Seok Lim</author>
<affiliation confidence="0.998215">Dept. of Information and Communications, Chonan</affiliation>
<address confidence="0.969283">115 AnSeo-dong, CheonAn 330-704, Korea</address>
<abstract confidence="0.999214647058824">This paper proposes a word spacing model using a hidden Markov model (HMM) for refining Korean raw text corpora. Previous statistical approaches for automatic word spacing have used models that make use of inaccurate probabilities because they do not consider the previous spacing state. We consider word spacing problem as a classification problem such as Part-of-Speech (POS) tagging and have experimented with various models considering extended context. Experimental result shows that the performance of the model becomes better as the more context considered. In case of the same number of parameters are used with other method, it is proved that our model is more effective by showing the better results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>C Hendrickson</author>
<author>N Jacobson</author>
<author>M Perkowitz</author>
</authors>
<title>Equations for part-ofspeech tagging.</title>
<date>1993</date>
<booktitle>In National Conference on Artificial Intelligence,</booktitle>
<pages>784--789</pages>
<contexts>
<context position="3312" citStr="Charniak et al., 1993" startWordPosition="534" endWordPosition="537">h the principles. For example, spacing or concatenating individual nouns including a compound noun are both considered as right. As mentioned, word spacing is important for some reasons, but it is difficult for even man to space words correctly by spelling rules because of the characteristics of Korean and the inconsistent rules. Especially, it is much more confused in the case of having no influence on understanding the meaning of a sentence. In this paper, we propose a word spacing model 1 using an HMM. HMM is a widely used statistical model to solve various NLP problems such as POS tagging(Charniak et al., 1993; Merialdo, 1994; Kim et al., 1998a; Lee, 1999). We regard the word spacing problem as a classification problem such as the POS tagging problem. When using an HMM for automatic word spacing task, raw texts can be used as training &apos;Strictly speaking, our model described here is an Eojeol spacing model rather than a word spacing model because spacing unit of Korean is Eojeol. But we in this paper do not distinguish between Eojeol and word for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M </context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>E. Charniak, C. Hendrickson, N. Jacobson, and M. Perkowitz. 1993. Equations for part-ofspeech tagging. In National Conference on Artificial Intelligence, pages 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-H Choi</author>
</authors>
<title>Automatic Korean spacing words correction system with bidirectional longest match strategy.</title>
<date>1997</date>
<booktitle>In Proceedings of the 9th Conference on Hangul and Korean Information Processing,</booktitle>
<pages>145--151</pages>
<contexts>
<context position="4323" citStr="Choi, 1997" startWordPosition="712" endWordPosition="713">not distinguish between Eojeol and word for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M V Q morpheme 7f � OI0[)I z 7f OIOPI� z MT)[ T Eojeol word ��� ��� z data. Therefore, we expect that HMM can be applied to the task effectively without bothering to construct training data. 2 Related Works Previous approaches for automatic word spacing can be classified into two groups: rule based approach and statistical approach. The rulebased approach uses lexical information and heuristic rules(Choi, 1997; Kim et al., 1998b; Kang, 1998; Kang, 2000). Lexical information consists of postposition and Eomi2 information, a list of spaced word examples, etc. Heuristic rules are composed of longest match or shortest match rule, morphological rules, and error patterns. This approach has disadvantage requiring higher computational complexity than the statistical approach. It also costs too much in constructing and maintaining lexical information. Most of rule-based systems use a morphological analyzer to recognize word boundaries. Another disadvantages of rule-based approach are resulted from using mor</context>
</contexts>
<marker>Choi, 1997</marker>
<rawString>J.-H. Choi. 1997. Automatic Korean spacing words correction system with bidirectional longest match strategy. In Proceedings of the 9th Conference on Hangul and Korean Information Processing, pages 145-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-M Chung</author>
<author>J-Y Lee</author>
</authors>
<title>Automatic word-segmentation at line-breaks for Korean text processing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th Conference of Korea Society for Information Mangement,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="5822" citStr="Chung and Lee, 1999" startWordPosition="937" endWordPosition="940">on occurs if morphological analysis fails due to unknown words. In addition, if an erroneous word is successfully analyzed through overgeneration, the error cannot even be detected. Finally, if a word 2Eomi is a grammatical morpheme of Korean which is attached to verbal root spacing system is used as a preprocessor of a morphological analyzer, the same morphological analyzing process should be repeated twice. The statistical approach uses syllable statistics extracted from large amount of corpora to decide whether two adjacent syllables should be spaced or not(Shim, 1996; Shin and Park, 1997; Chung and Lee, 1999; Jeon and Park, 2000; Kang and Woo, 2001). In contrast to the rulebased approach, it does not require many costs to construct and to maintain statistics because they can be acquired automatically. It is more robust against unknown words than rule-based approach that uses a morphological analyzer. A statistical method proposed in Kang and Woo (2001) has shown the best performance so far. In this method, word spacing probability P(xi; xi+1), between two adjacent syllables xi and xi+1, is in Equation 1. If the probability is greater than 0:375, a space is inserted between xi and xi+1. P(xi; xi+1</context>
</contexts>
<marker>Chung, Lee, 1999</marker>
<rawString>Y.-M. Chung and J.-Y. Lee. 1999. Automatic word-segmentation at line-breaks for Korean text processing. In Proceedings of the 6th Conference of Korea Society for Information Mangement, pages 21-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N-Y Jeon</author>
<author>H-R Park</author>
</authors>
<title>Automatic word-spacing of syllable bi-gram information for Korean OCR postprocessing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 12th Conference on Hangul and Korean Information Processing,</booktitle>
<pages>95--100</pages>
<contexts>
<context position="5843" citStr="Jeon and Park, 2000" startWordPosition="941" endWordPosition="944">gical analysis fails due to unknown words. In addition, if an erroneous word is successfully analyzed through overgeneration, the error cannot even be detected. Finally, if a word 2Eomi is a grammatical morpheme of Korean which is attached to verbal root spacing system is used as a preprocessor of a morphological analyzer, the same morphological analyzing process should be repeated twice. The statistical approach uses syllable statistics extracted from large amount of corpora to decide whether two adjacent syllables should be spaced or not(Shim, 1996; Shin and Park, 1997; Chung and Lee, 1999; Jeon and Park, 2000; Kang and Woo, 2001). In contrast to the rulebased approach, it does not require many costs to construct and to maintain statistics because they can be acquired automatically. It is more robust against unknown words than rule-based approach that uses a morphological analyzer. A statistical method proposed in Kang and Woo (2001) has shown the best performance so far. In this method, word spacing probability P(xi; xi+1), between two adjacent syllables xi and xi+1, is in Equation 1. If the probability is greater than 0:375, a space is inserted between xi and xi+1. P(xi; xi+1) = 0:25 X PR(xi-1; x</context>
</contexts>
<marker>Jeon, Park, 2000</marker>
<rawString>N.-Y. Jeon and H.-R. Park. 2000. Automatic word-spacing of syllable bi-gram information for Korean OCR postprocessing. In Proceedings of the 12th Conference on Hangul and Korean Information Processing, pages 95-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-S Kang</author>
<author>C-W Woo</author>
</authors>
<title>Automatic segmentation of words using syllable bigram statistics.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>729--732</pages>
<contexts>
<context position="5864" citStr="Kang and Woo, 2001" startWordPosition="945" endWordPosition="948">due to unknown words. In addition, if an erroneous word is successfully analyzed through overgeneration, the error cannot even be detected. Finally, if a word 2Eomi is a grammatical morpheme of Korean which is attached to verbal root spacing system is used as a preprocessor of a morphological analyzer, the same morphological analyzing process should be repeated twice. The statistical approach uses syllable statistics extracted from large amount of corpora to decide whether two adjacent syllables should be spaced or not(Shim, 1996; Shin and Park, 1997; Chung and Lee, 1999; Jeon and Park, 2000; Kang and Woo, 2001). In contrast to the rulebased approach, it does not require many costs to construct and to maintain statistics because they can be acquired automatically. It is more robust against unknown words than rule-based approach that uses a morphological analyzer. A statistical method proposed in Kang and Woo (2001) has shown the best performance so far. In this method, word spacing probability P(xi; xi+1), between two adjacent syllables xi and xi+1, is in Equation 1. If the probability is greater than 0:375, a space is inserted between xi and xi+1. P(xi; xi+1) = 0:25 X PR(xi-1; xi) + 0:5 X PM(xi; xi+</context>
<context position="16021" citStr="Kang and Woo (2001)" startWordPosition="2793" endWordPosition="2796"> number of words in a document, and WStotal is the total number of words created by a system. To investigate every model, we calculated the two accuracies for different K, J, L, and I. Accuracies for each model are listed in Table 2. According to the experimental results, we are sure that models considering more contexts show better results. The model A(T(2:2), S(1:2)) is the best for all measures. Note that some models show the better accuracies than the model A(T(2:2), S(2:2)), which uses the largest context. It seems that this is caused by sparseness of data. After evaluating the method of Kang and Woo (2001) for our training and test data, it shows 93:06% syllable-unit accuracy, 76:71% word-unit recall, and 67:80% word-unit precision. Compared with these results, our model shows much better performance. If I is two in A(S(K:J), T(L:I)), syllable trigrams are used. Although I is less than two (such as the model A(T(2:1), S(1:1), which uses syllable bigrams), our model is better than Kang and Woo (2001)&apos;s. This fact tells us that our model is also more effective even when used the same number of parameters of the model. There are two questions that we want to know about the word spacing models: Fir</context>
</contexts>
<marker>Kang, Woo, 2001</marker>
<rawString>S.-S. Kang and C.-W. Woo. 2001. Automatic segmentation of words using syllable bigram statistics. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, pages 729-732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-S Kang</author>
</authors>
<title>Automatic wordsegmentation for Hangul sentences.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th Conference on Hangul and Korean Information Processing,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="4354" citStr="Kang, 1998" startWordPosition="718" endWordPosition="719">and word for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M V Q morpheme 7f � OI0[)I z 7f OIOPI� z MT)[ T Eojeol word ��� ��� z data. Therefore, we expect that HMM can be applied to the task effectively without bothering to construct training data. 2 Related Works Previous approaches for automatic word spacing can be classified into two groups: rule based approach and statistical approach. The rulebased approach uses lexical information and heuristic rules(Choi, 1997; Kim et al., 1998b; Kang, 1998; Kang, 2000). Lexical information consists of postposition and Eomi2 information, a list of spaced word examples, etc. Heuristic rules are composed of longest match or shortest match rule, morphological rules, and error patterns. This approach has disadvantage requiring higher computational complexity than the statistical approach. It also costs too much in constructing and maintaining lexical information. Most of rule-based systems use a morphological analyzer to recognize word boundaries. Another disadvantages of rule-based approach are resulted from using morphological analyzer. First, if </context>
</contexts>
<marker>Kang, 1998</marker>
<rawString>S.-S. Kang. 1998. Automatic wordsegmentation for Hangul sentences. In Proceedings of the 10th Conference on Hangul and Korean Information Processing, pages 137-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-S Kang</author>
</authors>
<title>Eojeol-block bidirectional algorithm for automatic word spacing of Hangul sentences.</title>
<date>2000</date>
<journal>Journal of the Korea Information Science Society,</journal>
<pages>27--4</pages>
<contexts>
<context position="4367" citStr="Kang, 2000" startWordPosition="720" endWordPosition="721"> convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M V Q morpheme 7f � OI0[)I z 7f OIOPI� z MT)[ T Eojeol word ��� ��� z data. Therefore, we expect that HMM can be applied to the task effectively without bothering to construct training data. 2 Related Works Previous approaches for automatic word spacing can be classified into two groups: rule based approach and statistical approach. The rulebased approach uses lexical information and heuristic rules(Choi, 1997; Kim et al., 1998b; Kang, 1998; Kang, 2000). Lexical information consists of postposition and Eomi2 information, a list of spaced word examples, etc. Heuristic rules are composed of longest match or shortest match rule, morphological rules, and error patterns. This approach has disadvantage requiring higher computational complexity than the statistical approach. It also costs too much in constructing and maintaining lexical information. Most of rule-based systems use a morphological analyzer to recognize word boundaries. Another disadvantages of rule-based approach are resulted from using morphological analyzer. First, if ambiguous ana</context>
</contexts>
<marker>Kang, 2000</marker>
<rawString>S.-S. Kang. 2000. Eojeol-block bidirectional algorithm for automatic word spacing of Hangul sentences. Journal of the Korea Information Science Society, 27(4):441-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>H-S Lim</author>
<author>S-Z Lee</author>
<author>H-C Rim</author>
</authors>
<title>Twoply hidden markov model: A Korean pos tagging model based on morphemeunit with word-unit context.</title>
<date>1998</date>
<booktitle>Computer Processing of Oriental Languages,</booktitle>
<pages>11--3</pages>
<contexts>
<context position="3346" citStr="Kim et al., 1998" startWordPosition="540" endWordPosition="543">or concatenating individual nouns including a compound noun are both considered as right. As mentioned, word spacing is important for some reasons, but it is difficult for even man to space words correctly by spelling rules because of the characteristics of Korean and the inconsistent rules. Especially, it is much more confused in the case of having no influence on understanding the meaning of a sentence. In this paper, we propose a word spacing model 1 using an HMM. HMM is a widely used statistical model to solve various NLP problems such as POS tagging(Charniak et al., 1993; Merialdo, 1994; Kim et al., 1998a; Lee, 1999). We regard the word spacing problem as a classification problem such as the POS tagging problem. When using an HMM for automatic word spacing task, raw texts can be used as training &apos;Strictly speaking, our model described here is an Eojeol spacing model rather than a word spacing model because spacing unit of Korean is Eojeol. But we in this paper do not distinguish between Eojeol and word for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M V Q morpheme 7f � OI0[)I z 7f OIOP</context>
</contexts>
<marker>Kim, Lim, Lee, Rim, 1998</marker>
<rawString>J.-D. Kim, H.-S. Lim, S.-Z. Lee, and H.-C. Rim. 1998a. Twoply hidden markov model: A Korean pos tagging model based on morphemeunit with word-unit context. Computer Processing of Oriental Languages, 11(3):277-290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-S Kim</author>
<author>H-J Lee</author>
<author>S-J Lee</author>
</authors>
<title>Three-stage spacing system for Korean in sentence with no word boundaries.</title>
<date>1998</date>
<journal>Journal of the Korea Information Science Society,</journal>
<pages>25--12</pages>
<contexts>
<context position="3346" citStr="Kim et al., 1998" startWordPosition="540" endWordPosition="543">or concatenating individual nouns including a compound noun are both considered as right. As mentioned, word spacing is important for some reasons, but it is difficult for even man to space words correctly by spelling rules because of the characteristics of Korean and the inconsistent rules. Especially, it is much more confused in the case of having no influence on understanding the meaning of a sentence. In this paper, we propose a word spacing model 1 using an HMM. HMM is a widely used statistical model to solve various NLP problems such as POS tagging(Charniak et al., 1993; Merialdo, 1994; Kim et al., 1998a; Lee, 1999). We regard the word spacing problem as a classification problem such as the POS tagging problem. When using an HMM for automatic word spacing task, raw texts can be used as training &apos;Strictly speaking, our model described here is an Eojeol spacing model rather than a word spacing model because spacing unit of Korean is Eojeol. But we in this paper do not distinguish between Eojeol and word for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M V Q morpheme 7f � OI0[)I z 7f OIOP</context>
</contexts>
<marker>Kim, Lee, Lee, 1998</marker>
<rawString>K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998b. Three-stage spacing system for Korean in sentence with no word boundaries. Journal of the Korea Information Science Society, 25(12):1838-1844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-Z Lee</author>
</authors>
<title>New statistical models for automatic part-of-speech tagging.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Korea University.</institution>
<contexts>
<context position="3359" citStr="Lee, 1999" startWordPosition="544" endWordPosition="545">dividual nouns including a compound noun are both considered as right. As mentioned, word spacing is important for some reasons, but it is difficult for even man to space words correctly by spelling rules because of the characteristics of Korean and the inconsistent rules. Especially, it is much more confused in the case of having no influence on understanding the meaning of a sentence. In this paper, we propose a word spacing model 1 using an HMM. HMM is a widely used statistical model to solve various NLP problems such as POS tagging(Charniak et al., 1993; Merialdo, 1994; Kim et al., 1998a; Lee, 1999). We regard the word spacing problem as a classification problem such as the POS tagging problem. When using an HMM for automatic word spacing task, raw texts can be used as training &apos;Strictly speaking, our model described here is an Eojeol spacing model rather than a word spacing model because spacing unit of Korean is Eojeol. But we in this paper do not distinguish between Eojeol and word for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M V Q morpheme 7f � OI0[)I z 7f OIOPI� z MT)[ T E</context>
<context position="5822" citStr="Lee, 1999" startWordPosition="939" endWordPosition="940">if morphological analysis fails due to unknown words. In addition, if an erroneous word is successfully analyzed through overgeneration, the error cannot even be detected. Finally, if a word 2Eomi is a grammatical morpheme of Korean which is attached to verbal root spacing system is used as a preprocessor of a morphological analyzer, the same morphological analyzing process should be repeated twice. The statistical approach uses syllable statistics extracted from large amount of corpora to decide whether two adjacent syllables should be spaced or not(Shim, 1996; Shin and Park, 1997; Chung and Lee, 1999; Jeon and Park, 2000; Kang and Woo, 2001). In contrast to the rulebased approach, it does not require many costs to construct and to maintain statistics because they can be acquired automatically. It is more robust against unknown words than rule-based approach that uses a morphological analyzer. A statistical method proposed in Kang and Woo (2001) has shown the best performance so far. In this method, word spacing probability P(xi; xi+1), between two adjacent syllables xi and xi+1, is in Equation 1. If the probability is greater than 0:375, a space is inserted between xi and xi+1. P(xi; xi+1</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>S.-Z. Lee. 1999. New statistical models for automatic part-of-speech tagging. Ph.D. thesis, Korea University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="3328" citStr="Merialdo, 1994" startWordPosition="538" endWordPosition="539">xample, spacing or concatenating individual nouns including a compound noun are both considered as right. As mentioned, word spacing is important for some reasons, but it is difficult for even man to space words correctly by spelling rules because of the characteristics of Korean and the inconsistent rules. Especially, it is much more confused in the case of having no influence on understanding the meaning of a sentence. In this paper, we propose a word spacing model 1 using an HMM. HMM is a widely used statistical model to solve various NLP problems such as POS tagging(Charniak et al., 1993; Merialdo, 1994; Kim et al., 1998a; Lee, 1999). We regard the word spacing problem as a classification problem such as the POS tagging problem. When using an HMM for automatic word spacing task, raw texts can be used as training &apos;Strictly speaking, our model described here is an Eojeol spacing model rather than a word spacing model because spacing unit of Korean is Eojeol. But we in this paper do not distinguish between Eojeol and word for convenience. Therefore, we use the term &amp;quot;word&amp;quot; as word, spacing unit in English. Figure 1: Constitution of the sentence &amp;quot; T7t o]a}7]N � °;ovlut&amp;quot; T 010P1 M V Q morpheme 7f </context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging english text with a probabilistic model. Computational Linguistics, 20(2):155-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwangseob Shim</author>
</authors>
<title>Automated wordsegmentation for Korean using mutual information of syllables.</title>
<date>1996</date>
<journal>Journal of the Korea Information Science Society,</journal>
<pages>23--9</pages>
<contexts>
<context position="5780" citStr="Shim, 1996" startWordPosition="931" endWordPosition="932">er; false word boundary recognition occurs if morphological analysis fails due to unknown words. In addition, if an erroneous word is successfully analyzed through overgeneration, the error cannot even be detected. Finally, if a word 2Eomi is a grammatical morpheme of Korean which is attached to verbal root spacing system is used as a preprocessor of a morphological analyzer, the same morphological analyzing process should be repeated twice. The statistical approach uses syllable statistics extracted from large amount of corpora to decide whether two adjacent syllables should be spaced or not(Shim, 1996; Shin and Park, 1997; Chung and Lee, 1999; Jeon and Park, 2000; Kang and Woo, 2001). In contrast to the rulebased approach, it does not require many costs to construct and to maintain statistics because they can be acquired automatically. It is more robust against unknown words than rule-based approach that uses a morphological analyzer. A statistical method proposed in Kang and Woo (2001) has shown the best performance so far. In this method, word spacing probability P(xi; xi+1), between two adjacent syllables xi and xi+1, is in Equation 1. If the probability is greater than 0:375, a space i</context>
</contexts>
<marker>Shim, 1996</marker>
<rawString>Kwangseob Shim. 1996. Automated wordsegmentation for Korean using mutual information of syllables. Journal of the Korea Information Science Society, 23(9):991-1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-H Shin</author>
<author>H-R Park</author>
</authors>
<title>A statistical model for Korean text segmentation using syllable-level bigrams.</title>
<date>1997</date>
<booktitle>In Proceedings of the 9th Conference on Hangul and Korean Information Processing,</booktitle>
<pages>255--260</pages>
<contexts>
<context position="5801" citStr="Shin and Park, 1997" startWordPosition="933" endWordPosition="936">rd boundary recognition occurs if morphological analysis fails due to unknown words. In addition, if an erroneous word is successfully analyzed through overgeneration, the error cannot even be detected. Finally, if a word 2Eomi is a grammatical morpheme of Korean which is attached to verbal root spacing system is used as a preprocessor of a morphological analyzer, the same morphological analyzing process should be repeated twice. The statistical approach uses syllable statistics extracted from large amount of corpora to decide whether two adjacent syllables should be spaced or not(Shim, 1996; Shin and Park, 1997; Chung and Lee, 1999; Jeon and Park, 2000; Kang and Woo, 2001). In contrast to the rulebased approach, it does not require many costs to construct and to maintain statistics because they can be acquired automatically. It is more robust against unknown words than rule-based approach that uses a morphological analyzer. A statistical method proposed in Kang and Woo (2001) has shown the best performance so far. In this method, word spacing probability P(xi; xi+1), between two adjacent syllables xi and xi+1, is in Equation 1. If the probability is greater than 0:375, a space is inserted between xi</context>
</contexts>
<marker>Shin, Park, 1997</marker>
<rawString>J.-H. Shin and H.-R. Park. 1997. A statistical model for Korean text segmentation using syllable-level bigrams. In Proceedings of the 9th Conference on Hangul and Korean Information Processing, pages 255-260.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>