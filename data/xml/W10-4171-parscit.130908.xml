<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9899765">
Soochow University: Description and Analysis of the Chinese
Word Sense Induction System for CLP2010
</title>
<author confidence="0.993784">
Hua Xu Bing Liu Longhua Qian∗ Guodong Zhou
</author>
<affiliation confidence="0.983297">
Natural Language Processing Lab
School of Computer Science and Technology
</affiliation>
<address confidence="0.936702">
Soochow University, Suzhou, China 215006
</address>
<email confidence="0.9163705">
Email:
{20094227034,20084227065055,qianlonghua,gdzhou}@suda.edu.cn
</email>
<sectionHeader confidence="0.996555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999449333333334">
Recent studies on word sense induction
(WSI) mainly concentrate on European
languages, Chinese word sense induction
is becoming popular as it presents a new
challenge to WSI. In this paper, we
propose a feature-based approach using
the spectral clustering algorithm to this
problem. We also compare various
clustering algorithms and similarity
metrics. Experimental results show that
our system achieves promising
performance in F-score.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994500362318841">
Word sense induction (WSI) is an open problem
of natural language processing (NLP), which
governs the process of automatic discovery of
the possible senses of a word. WSI is similar to
word sense disambiguation (WSD) both in
methods employed and in problem encountered.
In the procedure of WSD, the senses are as-
sumed to be known and the task focuses on
choosing the correct one for an ambiguous word
in a context. The main difference between them
is that the task of WSD generally requires large-
scale manually annotated lexical resources while
WSI does not. As WSI doesn’t rely on the
manually annotated corpus, it has become one of
the most important topics in current NLP re-
search (Pantel and Lin, 2002; Neill, 2002; Rapp,
2003). Typically, the input to a WSI algorithm is
a target word to be disambiguated. The task of
WSI is to distinguish which target words share
the same meaning when they appear in different
contexts. Such result can be at the very least
used as empirically grounded suggestions for
lexicographers or as input for WSD algorithm.
Other possible uses include automatic thesaurus
or ontology construction, machine translation or
information retrieval. Compared with European
languages, the study of WSI in Chinese is scarce.
Furthermore, as Chinese has its special writing
style and Chinese word senses have their own
characteristics, the methods that work well in
English may not perform effectively in Chinese
and the usefulness of WSI in real-world applica-
tions has yet to be tested and proved.
The core idea behind word sense induction is
that contextual information provides important
cues regarding a word’s meaning. The idea dates
back to (at least) Firth (1957) ( You shall know
a word by the company it keeps ), and under-
lies most WSD and lexicon acquisition work to
date. For example, when the adverb phrase oc-
curring prior to the ambiguous word ,
then the target word is more likely to be a verb
and the meaning of which is “to hold something”;
Otherwise, if an adjective phrase locates in the
same position, then it probably means “confi-
dence” in English. Thus, the words surrounds
the target word are main contributor to sense
induction.
The bake off task 4 on WSI in the first CIPS-
SIGHAN Joint Conference on Chinese Lan-
guage Processing (CLP2010) is intended to
promote the exchange of ideas among partici-
pants and improve the performance of Chinese
WSI systems. Generally, our WSI system also
adopts a clustering algorithm to group the con-
texts of a target word. Differently, after generat-
∗ Corresponding author
ing feature vectors of words, we compute a simi-
larity matrix with each cell denoting the similar-
ity between two contexts. Furthermore, the set of
similarity values of a context with other contexts
is viewed as another kind of feature vector,
which we refer to as similarity vector. Both fea-
ture vectors and similarity vectors can be sepa-
rately used as the input to clustering algorithms.
Experimental results show our system achieves
good performances on the development dataset
as well as on the final test dataset provided by
the CLP2010.
</bodyText>
<sectionHeader confidence="0.978733" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9991905">
This section sequentially describes the architec-
ture of our WSI system and its main components.
</bodyText>
<subsectionHeader confidence="0.993607">
2.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.99976">
Figure 1 shows the architecture of our WSI
system. The first step is to preprocess the raw
dataset for feature extraction. After that, we
extract “bag of words” from the sentence
containing a target word (feature extraction) and
transform them into high-dimension vectors
(feature vector generation). Then, similarities of
every two vectors could be computed based on
the feature vectors (similarity measurement). the
similarities of an instance can be viewed as
another vector—similarity vector. Both feature
vectors and similarity vectors can be served as
the input for clustering algorithms. Finally, we
perform three clustering algorithms, namely, k-
means, HAC and spectral clustering.
</bodyText>
<figureCaption confidence="0.939411">
Figure 1 Architecture of our Chinese
</figureCaption>
<sectionHeader confidence="0.449438" genericHeader="method">
WSI system
</sectionHeader>
<subsectionHeader confidence="0.999247">
2.2 Feature Engineering
</subsectionHeader>
<bodyText confidence="0.999901">
In the task of WSI, the target words with their
topical context are first transformed into multi-
dimensional vectors with various features, and
then applying clustering algorithm to detect the
relevance of each other.
</bodyText>
<subsectionHeader confidence="0.981017">
Corpus Preprocessing
</subsectionHeader>
<bodyText confidence="0.999955941176471">
For each raw file, we first extract each sentence
embedded in the tag &lt;instance&gt;, including
the &lt;head&gt; and &lt;/head&gt; tags which are used
to identify the ambiguous word. Then, we put all
the sentences related to one target word into a
file, ordered by their instance IDs. The next step
is word segmentation, which segments each sen-
tence into a sequence of Chinese words and is
unique for Chinese WSI. Here, we use the soft-
ware from Hylanda1 since it is ready to use and
considered an efficient word segmentation tool.
Finally, since we retain the &lt;head&gt; tag in the
sentence, the &lt;head&gt; and &lt;/head&gt; tags are
usually separated after word segmentation, thus
we have to restore them in order to correctly lo-
cate the target word during the process of feature
extraction.
</bodyText>
<subsectionHeader confidence="0.62413">
Feature Extraction
</subsectionHeader>
<bodyText confidence="0.9999804">
After word segmentation, for a context of a par-
ticular word, we extract all the words around it
in the sentence and build a feature vector based
on a “bag-of-words” Boolean model. “Bag-of-
words” means that we don’t consider the order
of words. Meanwhile, in the Boolean model,
each word in the context is used to generate a
feature. This feature will be set to 1 if the word
appears in the context or 0 if it does not. Finally,
we get a number of feature vectors, each of them
corresponds to an instance of the target word.
One problem with this feature-based method is
that, since the size of word set may be huge, the
dimension is also very high, which might lead to
data sparsity problem.
</bodyText>
<subsectionHeader confidence="0.892075">
Similarity measurement
</subsectionHeader>
<bodyText confidence="0.9998694">
One commonly used metric for similarity meas-
urement is cosine similarity, which measures the
angle between two feature vectors in a high-
dimensional space. Formally, the cosine similar-
ity can be computed as follows:
</bodyText>
<equation confidence="0.776952333333333">
cos ine similarity ,
&lt; x
x •y
</equation>
<bodyText confidence="0.996799">
where x , y are two vectors in the vector space
and x , y are the lengths of x , y respectively.
</bodyText>
<figure confidence="0.9581071875">
1 http://www.hylanda.com/
Dataset
Preprocess
Feature
Extraction
WSI
Results
Vector
Generation
Clustering
Similarity
Measurement
Similarity
As Vector
y &gt; =
x •y
</figure>
<bodyText confidence="0.955214285714286">
Some clustering algorithms takes feature vec-
tors as the input and use cosine similarity as the
similarity measurement between two vectors.
This may lead to performance degradation due
to data sparsity in feature vectors. To avoid this
problem, we compute the similarities of every
two vectors and generate an N * N similarity
matrix, where is the number of all the in-
N
stances containing the ambiguous word. Gener-
ally, N is usually much smaller than the dimen-
sion size and may alleviate the data sparsity
problem. Moreover, we view every row of this
matrix (i.e., an ordered set of similarities of an
instance with other instances) as another kind of
feature vector. In other words, each instance it-
self is regarded as a feature, and the similarity
with this instance reflects the weight of the fea-
ture. We call this vector similarity vector, which
we believe will more properly represent the in-
stance and achieve promising performance.
</bodyText>
<subsectionHeader confidence="0.998897">
2.3 Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.999923625">
Clustering is a very popular technique which
aims to partition a dataset into such subgroups
that samples in the same group share more simi-
larities than those from different groups. Our
system explores various cluster algorithms for
Chinese WSI, including K-means, hierarchical
agglomerative clustering (HAC), and spectral
clustering (SC).
</bodyText>
<sectionHeader confidence="0.755657" genericHeader="method">
K-means (KM)
</sectionHeader>
<bodyText confidence="0.9996569">
K-means is a very popular method for general
clustering used to automatically partition a data
set into k groups. K-means works by assigning
multidimensional vectors to one of K clusters,
where K is given as a priori. The aim of the al-
gorithm is to minimize the variance of the vec-
tors assigned to each cluster.
K-means proceeds by selecting k initial clus-
ter centers and then iteratively refining them as
follows:
</bodyText>
<listItem confidence="0.9934968">
(1) Choose cluster centers to coincide with
k
k randomly-chosen patterns or k ran-
domly defined points.
(2) Assign each pattern to the closest cluster
center.
(3) Recompute the cluster centers using the
current cluster memberships.
(4) If a convergence criterion is not met, go
to step 2.
</listItem>
<subsectionHeader confidence="0.766874">
Hierarchical Agglomerative Clustering (HAC)
</subsectionHeader>
<bodyText confidence="0.995832529411765">
Different from K-means, hierarchical clustering
creates a hierarchy of clusters which can be
represented in a tree structure called a
dendrogram. The root of the tree consists of a
single cluster containing all objects, and the
leaves correspond to individual object.
Typically, hierarchical agglomerative
clustering (HAC) starts at the leaves and
successively merges two clusters together as
long as they have the shortest distance among all
the pair-wise distances between any two clusters.
Given a specified number of clusters, the key
problem is to determine where to cut the hierar-
chical tree into clusters. In this paper, we gener-
ate the final flat cluster structures greedily by
maximizing the equal distribution of instances
among different clusters.
</bodyText>
<subsectionHeader confidence="0.90654">
Spectral Clustering (SC)
</subsectionHeader>
<bodyText confidence="0.999752307692308">
Spectral clustering refers to a class of techniques
which rely on the eigen-structure of a similarity
matrix to partition points into disjoint clusters
with points in the same cluster having high simi-
larity and points in different clusters having low
similarity.
Compared to the “traditional algorithms” such
as K-means or single linkage, spectral clustering
has many fundamental advantages. Results ob-
tained by spectral clustering often outperform
the traditional approaches, spectral clustering is
very simple to implement and can be solved ef-
ficiently by standard linear algebra methods.
</bodyText>
<sectionHeader confidence="0.986494" genericHeader="method">
3 System Evaluation
</sectionHeader>
<bodyText confidence="0.999763333333333">
This section reports the evaluation dataset and
system performance for our feature-based Chi-
nese WSI system.
</bodyText>
<subsectionHeader confidence="0.994103">
3.1 Dataset and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.943724611111111">
We use the CLP2010 bake off task 4 sample
dataset as our development dataset. There are
2500 examples containing 50 target words and
each word has 50 sentences with different mean-
ings. The exact meanings of the target words are
blind, only the number of the meanings is pro-
vided in the data. We compute the system per-
formance with the sample dataset because it con-
tains the answers of each candidate meaning.
The test dataset provided by the CLP2010 is
similar to the sample dataset. It contains 100
target words and 5000 instances in total. How-
ever, it doesn’t provide the answers.
The F-score measurement is the same as Zhao
and Karypis (2005). Given a particular
class Lr of size and a particular cluster of
nr Si
size , suppose in the cluster belong to
</bodyText>
<equation confidence="0.877423583333333">
ni nir Si Lr,
then the F value of this class and cluster is de-
fined to be
F L S
( , ) =
r iR L S P L S
2x R(Lr,S)x P(Lr,S)
( , ) ( , )
r i r i
+
R(Lr,Si) = nir / nr
P(Lr, Si) = nir / ni
</equation>
<bodyText confidence="0.859122875">
where R(Lr, Si) is the recall value and P(Lr , Si )
is the precision value. The F-score of class Lr is
the maximum F value and F-score value follow:
F − score L r = S i F L r S i
( ) max ( , )
where is the total number of classes and n is
c
the total size.
</bodyText>
<subsectionHeader confidence="0.999008">
3.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.995739931034483">
Table 1 reports the F-score of our feature-based
Chinese WSI for different feature sets with
various window sizes using K-means clustering.
Since there are different results for each run of
K-means clustering algorithm, we perform 20
trials and compute their average as the final
results. The columns denote different window
size n, that is, the n words before and after the
target word are extracted as features. Particularly,
the size of infinity (∞) means that all the words
in the sentence except the target word are
considered. The rows represent various
combinations of feature sets and similarity
measurements, currently, four of which are
considered as follows:
F-All: all the words are considered as features
and from them feature vectors are constructed.
F-Stop: the top 150 most frequently occurring
words in the total “word bags” of the corpus are
regarded as stop words and thus removed from
the feature set. Feature vectors are then formed
from these words.
S-All: the feature set and the feature vector
are the same as those of F-All, but instead the
similarity vector is used for clustering (c.f. Sec-
tion 2.2).
S-Stop: the feature set and the feature vector
are the same as those of F-Stop, but instead the
similarity vector is used for clustering.
</bodyText>
<table confidence="0.9807165">
Feature/ 3 7 10 ∞
Similarity
F-All 0.5949 0.6199 0.6320 0.6575
F-Stop 0.6384 0.6500 0.6493 0.6428
S-All 0.5856 0.6044 0.6186 0.6843
S-Stop 0.6532 0.6696 0.6804 0.7320
</table>
<tableCaption confidence="0.993913">
Table 1 Experimental results for differ-
</tableCaption>
<bodyText confidence="0.961278285714286">
ent feature sets with different window sizes us-
ing K-means clustering
This table shows that S-Stop achieves the best
performance of 0.7320 in F-score. This suggests
that for K-means clustering, Chinese WSI can
benefit much from removing stop words and
adopting similarity vector. It also shows that:
</bodyText>
<listItem confidence="0.958923857142857">
• As the window size increases, the perform-
ance is almost consistently enhanced. This
indicates that all the words in the sentence
more or less help disambiguate the target
word.
• Removing stop words consistently improves
the F-score for both similarity metrics. This
means some high frequent words do not help
discriminate the meaning of the target words,
and further work on feature selection is thus
encouraged.
• Similarity vector consistently outperforms
feature vector for stop-removed features, but
not so for all-words features. This may be
</listItem>
<bodyText confidence="0.9722095">
due to the fact that, when the window size is
limited, the influence of frequently occur-
ring stop words is relatively high, thus the
similarity vector misrepresent the context of
the target word. On the contrary, when stop
words are removed or the context is wide,
the similarity vector can better reflect the
target word’s context, leading to better per-
formance.
In order to intuitively explain why the simi-
larity vector is more discriminative than the fea-
ture vector, we take two sentences containing
</bodyText>
<equation confidence="0.632175833333333">
F − score = ∑n r F − score
r= n
1
c
( )
Lr
</equation>
<bodyText confidence="0.991611875">
the Chinese word “ ” (hold, grasp) as an ex-
ample (Figure 2). These two sentences have few
common words, so clustering via feature vectors
puts them into different classes. However, since
the similarities of these two feature vectors with
other feature vectors are much similar, cluster-
ing via similarity vectors group them into the
same class.
</bodyText>
<figure confidence="0.996517444444444">
&lt;lexelt item=&amp;quot; &amp;quot; snum=&amp;quot;4&amp;quot;&gt;
&lt;instance id=&amp;quot;0012&amp;quot;&gt;
&lt;head&gt; &lt;/head&gt;“
&lt;/instance&gt;
&lt;instance id=&amp;quot;0015&amp;quot;&gt;
&lt;head&gt;
&lt;/head&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
</figure>
<figureCaption confidence="0.998561">
Figure 2 An example from the dataset
</figureCaption>
<bodyText confidence="0.9993647">
According to the conclusion of the above ex-
periments, it is better to include all the words
except stop words in the sentence as the features
in the subsequent experiment. Table 2 lists the
results using various clustering algorithms with
this same experimental setting. It shows that the
spectral clustering algorithm achieves the best
performance of 0.7692 in F-score for Chinese
WSI using the S-All setup. Additionally, there
are some interesting findings:
</bodyText>
<listItem confidence="0.986090166666667">
• Although SC performs best, KM with simi-
larity vectors achieves comparable results of
0.7320 units in F-score, slightly lower than
that of SC.
• HAC performs worst among all clustering
algorithms. An observation reveals that this
</listItem>
<bodyText confidence="0.969036071428571">
algorithm always groups the instances into
highly skewed clusters, i.e., one or two clus-
ters are extremely large while others usually
have only one instance in each cluster.
• It is surprising that S-All slightly outper-
forms F-All by only 0.0006 units in F-score.
The truth is that, as discussed in the first ex-
periment, KM using F-All doesn’t consider
instance density while S-All does. On the
contrary, SC identifies the eign-structure in
the instance space and thus already consid-
ers the density information, therefore S-All
will not significantly improve the perform-
ance.
</bodyText>
<table confidence="0.9626145">
Feature/ KM HAC SC
Similarity
F-All 0.6428 0.6280 0.7686
S-All 0.7320 0.6332 0.7692
</table>
<tableCaption confidence="0.818215">
Table 2 Experiments results using dif-
ferent clustering algorithms
</tableCaption>
<subsectionHeader confidence="0.998521">
3.3 Final System Performance
</subsectionHeader>
<bodyText confidence="0.999993916666667">
For the CLP2010 task 4 test dataset which con-
tains 100 target words and 5000 instances in to-
tal, we first extract all the words except stop
words in a sentence containing the target word,
then produce the feature vector for each context
and generate the similarity matrix, finally we
perform the spectral cluster algorithm. Probably
because the distribution of the target word in the
test dataset is different from that in the develop-
ment dataset, the F-score of our system on the
test dataset is 0.7108, about 0.05 units lower
than that we got on the sample dataset.
</bodyText>
<sectionHeader confidence="0.998921" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999937176470588">
In our Chinese WSI system, we extract all the
words except stop words in the sentence, con-
struct feature vectors and similarity vectors, and
apply the spectral clustering algorithm to this
problem. Experimental results show that our
simple and efficient system achieve a promising
result. Moreover, we also compare various clus-
tering algorithms and similarity metrics. We find
that although the spectral clustering algorithm
outperforms other clustering algorithms, the K-
means clustering with similarity vectors can also
achieve comparable results.
For future work, we will incorporate more
linguistic features, such as base chunking, parse
tree feature as well as dependency information
into our system to further improve the perform-
ance.
</bodyText>
<sectionHeader confidence="0.986642" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.8322286">
This research is supported by Project 60873150,
60970056 and 90920004 under the National
Natural Science Foundation of China. We would
also like to thank other contributors in the NLP
lab at Soochow University.
</bodyText>
<sectionHeader confidence="0.976128" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999011710526315">
Jain A, Murty M. 1999.Flynn P. Data clustering : A
Review [J]. ACM Computing Surveys,1999,31
(3) :2642323
F. Bach and M. Jordan.2004. Learning spectral clus-
tering. In Proc. of NIPS-16. MIT Press, 2004.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the ACL
(EACL 2009), pages 103–111.
Neill, D. B. 2002. Fully Automatic Word Sense In-
duction by Semantic Clustering. Cambridge Uni-
versity, Master’s Thesis, M.Phil. in Computer
Speech.
Agirre, E. and Soroa, A. 2007. Semeval-2007 task 02:
Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations:7-12
Ioannis P. Klapaftis and Suresh Manandhar. 2008.
Word sense induction using graphs of collocations.
In Proceedings of the 18th European Conference
On Artificial Intelligence (ECAI-2008), Patras,
Greece, July. IOS Press.
Kannan, R., Vempala, S and Vetta, A. 2004. On clus-
terings: Good, bad and spectral. J. ACM, 51(3),
497–515.
Reinhard Rapp.2004. A practical solution to the
problem of automatic word sense induction. Pro-
ceedings of the ACL 2004 on Interactive poster
and demonstration sessions, p.26-es, July 21-26,
2004, Barcelona, Spain
Bordag, S. 2006. Word sense induction: Triplet-based
clustering and automatic evaluation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics (EACL, Trento, Italy). 137--144.
Ying Zhao, and George Karypis.2005. Hierarchical
Clustering Algorithms for Document Datasets. Da-
ta Mining and Knowledge Discovery, 10, 141–168.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.180121">
<title confidence="0.491864">Soochow University: Description and Analysis of the Word Sense Induction System for CLP2010</title>
<author confidence="0.483394">Xu Bing Liu Longhua Guodong</author>
<affiliation confidence="0.727515">Natural Language Processing School of Computer Science and</affiliation>
<address confidence="0.965392">Soochow University, Suzhou, China 215006</address>
<email confidence="0.922895">20094227034,20084227065055,qianlonghua,gdzhou}@suda.edu.cn</email>
<abstract confidence="0.998322769230769">Recent studies on word sense induction (WSI) mainly concentrate on European languages, Chinese word sense induction is becoming popular as it presents a new challenge to WSI. In this paper, we propose a feature-based approach using the spectral clustering algorithm to this problem. We also compare various clustering algorithms and similarity metrics. Experimental results show that our system achieves promising performance in F-score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Jain</author>
<author>M Murty</author>
</authors>
<title>P. Data clustering : A Review [J].</title>
<date>1999</date>
<journal>ACM Computing Surveys,1999,31</journal>
<volume>3</volume>
<pages>2642323</pages>
<marker>Jain, Murty, 1999</marker>
<rawString>Jain A, Murty M. 1999.Flynn P. Data clustering : A Review [J]. ACM Computing Surveys,1999,31 (3) :2642323</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bach</author>
<author>M Jordan 2004</author>
</authors>
<title>Learning spectral clustering.</title>
<date>2004</date>
<booktitle>In Proc. of NIPS-16.</booktitle>
<publisher>MIT Press,</publisher>
<marker>Bach, 2004, 2004</marker>
<rawString>F. Bach and M. Jordan.2004. Learning spectral clustering. In Proc. of NIPS-16. MIT Press, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>103--111</pages>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Neill</author>
</authors>
<title>Fully Automatic Word Sense Induction by Semantic Clustering. Cambridge University, Master’s Thesis, M.Phil. in Computer Speech.</title>
<date>2002</date>
<contexts>
<context position="1505" citStr="Neill, 2002" startWordPosition="227" endWordPosition="228">ss of automatic discovery of the possible senses of a word. WSI is similar to word sense disambiguation (WSD) both in methods employed and in problem encountered. In the procedure of WSD, the senses are assumed to be known and the task focuses on choosing the correct one for an ambiguous word in a context. The main difference between them is that the task of WSD generally requires largescale manually annotated lexical resources while WSI does not. As WSI doesn’t rely on the manually annotated corpus, it has become one of the most important topics in current NLP research (Pantel and Lin, 2002; Neill, 2002; Rapp, 2003). Typically, the input to a WSI algorithm is a target word to be disambiguated. The task of WSI is to distinguish which target words share the same meaning when they appear in different contexts. Such result can be at the very least used as empirically grounded suggestions for lexicographers or as input for WSD algorithm. Other possible uses include automatic thesaurus or ontology construction, machine translation or information retrieval. Compared with European languages, the study of WSI in Chinese is scarce. Furthermore, as Chinese has its special writing style and Chinese word</context>
</contexts>
<marker>Neill, 2002</marker>
<rawString>Neill, D. B. 2002. Fully Automatic Word Sense Induction by Semantic Clustering. Cambridge University, Master’s Thesis, M.Phil. in Computer Speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations:7-12</booktitle>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Agirre, E. and Soroa, A. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the 4th International Workshop on Semantic Evaluations:7-12</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis P Klapaftis</author>
<author>Suresh Manandhar</author>
</authors>
<title>Word sense induction using graphs of collocations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference On Artificial Intelligence (ECAI-2008),</booktitle>
<publisher>IOS Press.</publisher>
<location>Patras, Greece,</location>
<marker>Klapaftis, Manandhar, 2008</marker>
<rawString>Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word sense induction using graphs of collocations. In Proceedings of the 18th European Conference On Artificial Intelligence (ECAI-2008), Patras, Greece, July. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kannan</author>
<author>S Vempala</author>
<author>A Vetta</author>
</authors>
<title>On clusterings: Good, bad and spectral.</title>
<date>2004</date>
<journal>J. ACM,</journal>
<volume>51</volume>
<issue>3</issue>
<pages>497--515</pages>
<marker>Kannan, Vempala, Vetta, 2004</marker>
<rawString>Kannan, R., Vempala, S and Vetta, A. 2004. On clusterings: Good, bad and spectral. J. ACM, 51(3), 497–515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp 2004</author>
</authors>
<title>A practical solution to the problem of automatic word sense induction.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, p.26-es,</booktitle>
<location>Barcelona, Spain</location>
<marker>2004, 2004</marker>
<rawString>Reinhard Rapp.2004. A practical solution to the problem of automatic word sense induction. Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, p.26-es, July 21-26, 2004, Barcelona, Spain</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bordag</author>
</authors>
<title>Word sense induction: Triplet-based clustering and automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL,</booktitle>
<pages>137--144</pages>
<location>Trento,</location>
<marker>Bordag, 2006</marker>
<rawString>Bordag, S. 2006. Word sense induction: Triplet-based clustering and automatic evaluation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL, Trento, Italy). 137--144.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ying Zhao</author>
<author>George Karypis 2005</author>
</authors>
<title>Hierarchical Clustering Algorithms for Document Datasets.</title>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>10</volume>
<pages>141--168</pages>
<marker>Zhao, 2005, </marker>
<rawString>Ying Zhao, and George Karypis.2005. Hierarchical Clustering Algorithms for Document Datasets. Data Mining and Knowledge Discovery, 10, 141–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>