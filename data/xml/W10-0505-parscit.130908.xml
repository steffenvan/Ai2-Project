<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007526">
<title confidence="0.999235">
Towards Automatic Question Answering over Social Media by Learning
Question Equivalence Patterns
</title>
<author confidence="0.994028">
Tianyong Hao1 Wenyin Liu Eugene Agichtein
</author>
<affiliation confidence="0.956018">
City University of Hong Kong City University of Hong Kong Emory University
</affiliation>
<note confidence="0.425904">
81 Tat Chee Avenue 81 Tat Chee Avenue 201 Dowman Drive
Kowloon, Hong Kong SAR Kowloon, Hong Kong SAR Atlanta, Georgia 30322 USA
</note>
<email confidence="0.96779">
haotianyong@gmail.com csliuwy@cityu.edu.hk eugene@mathcs.emory.edu
</email>
<sectionHeader confidence="0.993402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990721636363636">
Many questions submitted to Collaborative
Question Answering (CQA) sites have been
answered before. We propose an approach to
automatically generating an answer to such
questions based on automatically learning to
identify “equivalent” questions. Our main
contribution is an unsupervised method for
automatically learning question equivalence
patterns from CQA archive data. These pat-
terns can be used to match new questions to
their equivalents that have been answered be-
fore, and thereby help suggest answers auto-
matically. We experimented with our method
approach over a large collection of more than
200,000 real questions drawn from the Yahoo!
Answers archive, automatically acquiring
over 300 groups of question equivalence pat-
terns. These patterns allow our method to ob-
tain over 66% precision on automatically
suggesting answers to new questions, signifi-
cantly outperforming conventional baseline
approaches to question matching.
</bodyText>
<sectionHeader confidence="0.998003" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995421923076923">
Social media in general exhibit a rich variety of
information sources. Question answering (QA) has
been particularly amenable to social media, as it
allows a potentially more effective alternative to
web search by directly connecting users with the
information needs to users willing to share the in-
formation directly (Bian, 2008). One of the useful
by-products of this process is the resulting large
archives of data – which in turn could be good
sources of information for automatic question an-
swering. Yahoo! Answers, as a collaborative QA
system (CQA), has acquired an archive of more
than 40 Million Questions and 500 Million an-
</bodyText>
<footnote confidence="0.420825">
1 Work done while visiting Emory University
</footnote>
<bodyText confidence="0.999249828571429">
swers, as of 2008 estimates.
The main premise of this paper is that there are
many questions that are syntactically different
while semantically similar. The key problem is
how to identify such question groups. Our method
is based on the key observation that when the best
non-trivial answers chosen by asker in the same
domain are exactly the same, the corresponding
questions are semantically similar. Based on this
observation, we propose answering new method
for learning question equivalence patterns from
CQA archives. First, we retrieve “equivalent”
question groups from a large dataset by grouping
them by the text of the best answers (as chosen by
the askers). The equivalence patterns are then gen-
erated by learning common syntactic and lexical
patterns for each group. To avoid generating pat-
terns from questions that were grouped together by
chance, we estimate the group’s topic diversity to
filter the candidate patterns. These equivalence
patterns are then compared against newly submit-
ted questions. In case of a match, the new question
can be answered by proposing the “best” answer
from a previously answered equivalent question.
We performed large-scale experiments over a
more than 200,000 questions from Yahoo! An-
swers. Our method generated over 900 equivalence
patterns in 339 groups and allows to correctly sug-
gest an answer to a new question, roughly 70% of
the time – outperforming conventional similarity-
based baselines for answer suggestion.
Moreover, for the newly submitted questions,
our method can identify equivalent questions and
generate equivalent patterns incrementally, which
can greatly improve the feasibility of our method.
</bodyText>
<sectionHeader confidence="0.928337" genericHeader="method">
2 Learning Equivalence Patterns
</sectionHeader>
<bodyText confidence="0.999565333333333">
While most questions that share exactly the same
“best” answer are indeed semantically equivalent,
some may share the same answer by chance. To
</bodyText>
<page confidence="0.974438">
9
</page>
<note confidence="0.873936">
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 9–10,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999867409090909">
filter out such cases, we propose an estimate of
Topical Diversity (TD), calculated based on the
shared topics for all pairs of questions in the group.
If the diversity is larger than a threshold, the ques-
tions in this group are considered not equivalent,
and no patterns are generated. To calculate this
measure, we consider as topics the “notional
words” (NW) in the question, which are the head
nouns and the heads of verb phrases recognized by
the OpenNLP parser. Using these words as “top-
ics”, TD for a group of questions G is calculated as:
where Qi and Qj are the notional words in each
question in within group G with n questions total.
Based on the question groups, we can generate
equivalence patterns to extend the matching cover-
age – thus retrieving similar questions with differ-
ent syntactic structure. OpenNLP is used to
generate the basic syntactic structures by phrase
chunking. After that, only the chunks which con-
tain NWs are analyzed to acquire the phrase labels
as the syntactic pattern. Table 1 shows an example
of a generated pattern.
</bodyText>
<table confidence="0.507536">
Question: What was the first book you discovered that
made you think reading wasn&apos;t a complete waste of time?
Pattern: [NP]-[VP]-[NP]-[NP]-[VP]-[VP]-[NP]-[VP]-•••
NW: (Disjoint: read waste time) (Shared: book think)
Question: What book do you think everyone should have
at home?
Pattern: [NP]-[NP]-[VP]-[NP]-[VP]-[PP]-[NP]
NW: (Disjoint: do everyone have home) (Shared: book
think)
</table>
<tableCaption confidence="0.999312">
Table 1. A group of equivalence patterns
</tableCaption>
<sectionHeader confidence="0.996252" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.99987105">
Our dataset is 216,563 questions and 2,044,296
answers crawled from Yahoo! Answers. From this
we acquired 833 groups of similar questions dis-
tributed in 65 categories. After filtering by topical
diversity, 339 groups remain to generate equiva-
lence patterns. These groups contain 979 questions,
with, 2.89 questions per group on average.
After that, we split our data into 413 questions
for training (200 groups) and 566 questions, with
randomly selected an additional 10,000 questions,
for testing (the remainder) to compare three vari-
ants of our system Equivalence patterns only (EP),
Notional words only (NW), and the weighted com-
bination (EP+NW). To match question, both
equivalence patterns and notional words are used
with different weights. The weight of pattern, dis-
joint NW and shared NW are 0.7, 0.4 and 0.6 after
parameter training. We then compare the variants
and results are reported in Table 2, showing that
EP+NW achieves the highest performance.
</bodyText>
<table confidence="0.9998575">
Recall Precision F1 score
EP 0.811 0.385 0.522
NW 0.378 0.559 0.451
EP+NW 0.726 0.663 0.693
</table>
<tableCaption confidence="0.999928">
Table 2. Performance comparison of three variants
</tableCaption>
<bodyText confidence="0.809202666666667">
Using EP+NW as our best method, we now
compare it to traditional similarity-based methods
on whole question set. TF*IDF-based vector space
model (TFIDF), and a more highly tuned Cosine
model (that only keeps the same “notional words”
filtered by phrase chunking) are used as baselines.
Figure 3 reports the results, which indicate that
EP+NW, outperforms both Cosine and TFIDF me-
thods on all metrics.
</bodyText>
<figureCaption confidence="0.998453">
Figure 3. Performance of EP+NW vs. baselines
</figureCaption>
<bodyText confidence="0.999974142857143">
Our work expands on previous significant ef-
forts on CQA retrieval (e.g., Bian et al., Jeon et al.,
Kosseim et al.). Our contribution is a new unsu-
pervised and effective method for learning ques-
tion equivalence patterns that exploits the structure
of the collaborative question answering archives –
an important part of social media.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
4 References
</sectionHeader>
<reference confidence="0.9678584">
Bian, J., Liu, Y., Agichtein, E., and Zha, H. 2008. Find-
ing the right facts in the crowd: factoid question an-
swering over social media. WWW.
Jeon, J., Croft, B.W. and Lee, J.H. 2005. Finding simi-
lar questions in large question and answer archives
Export Find Similar. CIKM.
Kosseim, L. and Yousefi, J. 2008.Improving the per-
formance of question answering with semantically
equivalent answer patterns, Journal of Data &amp;
Knowledge Engineering.
</reference>
<figure confidence="0.991841810810811">
−1
2
n
n
∑∑
=
×
1
(
Q
−
1)
2
n(n
= =
1 j
i
Q Q
i j
− ) ( )
i j
&lt;
iyQj
TD (G)
0.8
0.6
0.4
0.3
0.2
0.7
0.5
0.1
0
Recall Precision F1
TFIDF(NW)
COSINE
EP+NW
</figure>
<page confidence="0.814724">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948580">
<title confidence="0.9994035">Towards Automatic Question Answering over Social Media by Learning Question Equivalence Patterns</title>
<author confidence="0.99299">Wenyin Liu Agichtein</author>
<affiliation confidence="0.999819">City University of Hong Kong City University of Hong Kong Emory University</affiliation>
<address confidence="0.988843">81 Tat Chee Avenue 81 Tat Chee Avenue 201 Dowman Drive Kowloon, Hong Kong SAR Kowloon, Hong Kong SAR Atlanta, Georgia 30322</address>
<email confidence="0.997628">haotianyong@gmail.comcsliuwy@cityu.edu.hkeugene@mathcs.emory.edu</email>
<abstract confidence="0.999067304347826">Many questions submitted to Collaborative Question Answering (CQA) sites have been answered before. We propose an approach to automatically generating an answer to such questions based on automatically learning to Our main is unsupervised method for automatically learning question equivalence from CQA archive These patterns can be used to match new questions to their equivalents that have been answered before, and thereby help suggest answers automatically. We experimented with our method approach over a large collection of more than 200,000 real questions drawn from the Yahoo! Answers archive, automatically acquiring over 300 groups of question equivalence patterns. These patterns allow our method to obtain over 66% precision on automatically suggesting answers to new questions, significantly outperforming conventional baseline approaches to question matching.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bian</author>
<author>Y Liu</author>
<author>E Agichtein</author>
<author>H Zha</author>
</authors>
<title>Finding the right facts in the crowd: factoid question answering over social media.</title>
<date>2008</date>
<publisher>WWW.</publisher>
<marker>Bian, Liu, Agichtein, Zha, 2008</marker>
<rawString>Bian, J., Liu, Y., Agichtein, E., and Zha, H. 2008. Finding the right facts in the crowd: factoid question answering over social media. WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>B W Croft</author>
<author>J H Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives Export Find Similar.</title>
<date>2005</date>
<publisher>CIKM.</publisher>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jeon, J., Croft, B.W. and Lee, J.H. 2005. Finding similar questions in large question and answer archives Export Find Similar. CIKM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Kosseim</author>
<author>J Yousefi</author>
</authors>
<title>2008.Improving the performance of question answering with semantically equivalent answer patterns,</title>
<journal>Journal of Data &amp; Knowledge Engineering.</journal>
<marker>Kosseim, Yousefi, </marker>
<rawString>Kosseim, L. and Yousefi, J. 2008.Improving the performance of question answering with semantically equivalent answer patterns, Journal of Data &amp; Knowledge Engineering.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>