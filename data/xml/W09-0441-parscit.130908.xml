<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009726">
<title confidence="0.7991365">
Fluency, Adequacy, or HTER?
Exploring Different Human Judgments with a Tunable MT Metric
</title>
<author confidence="0.940113">
Matthew Snover§, Nitin Madnani§, Bonnie J. Dorr§ † &amp; Richard Schwartz† t
</author>
<affiliation confidence="0.996476">
§Laboratory for Computational Linguistics and Information Processing
§Institute for Advanced Computer Studies
§University of Maryland, College Park
†Human Language Technology Center of Excellence
</affiliation>
<sectionHeader confidence="0.414467" genericHeader="abstract">
$BBN Technologies
</sectionHeader>
<email confidence="0.989701">
{snover,nmadnani,bonnie}@umiacs.umd.edu schwartz@bbn.com
</email>
<sectionHeader confidence="0.997257" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998539692307692">
Automatic Machine Translation (MT)
evaluation metrics have traditionally been
evaluated by the correlation of the scores
they assign to MT output with human
judgments of translation performance.
Different types of human judgments, such
as Fluency, Adequacy, and HTER, mea-
sure varying aspects of MT performance
that can be captured by automatic MT
metrics. We explore these differences
through the use of a new tunable MT met-
ric: TER-Plus, which extends the Transla-
tion Edit Rate evaluation metric with tun-
able parameters and the incorporation of
morphology, synonymy and paraphrases.
TER-Plus was shown to be one of the
top metrics in NIST’s Metrics MATR
2008 Challenge, having the highest aver-
age rank in terms of Pearson and Spear-
man correlation. Optimizing TER-Plus
to different types of human judgments
yields significantly improved correlations
and meaningful changes in the weight of
different types of edits, demonstrating sig-
nificant differences between the types of
human judgments.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959382978723">
Since the introduction of the BLEU metric (Pa-
pineni et al., 2002), statistical MT systems have
moved away from human evaluation of their per-
formance and towards rapid evaluation using au-
tomatic metrics. These automatic metrics are
themselves evaluated by their ability to generate
scores for MT output that correlate well with hu-
man judgments of translation quality. Numer-
ous methods of judging MT output by humans
have been used, including Fluency, Adequacy,
and, more recently, Human-mediated Translation
Edit Rate (HTER) (Snover et al., 2006). Fluency
measures whether a translation is fluent, regard-
less of the correct meaning, while Adequacy mea-
sures whether the translation conveys the correct
meaning, even if the translation is not fully flu-
ent. Fluency and Adequacy are frequently mea-
sured together on a discrete 5 or 7 point scale,
with their average being used as a single score
of translation quality. HTER is a more complex
and semi-automatic measure in which humans do
not score translations directly, but rather generate
a new reference translation that is closer to the
MT output but retains the fluency and meaning
of the original reference. This new targeted refer-
ence is then used as the reference translation when
scoring the MT output using Translation Edit Rate
(TER) (Snover et al., 2006) or when used with
other automatic metrics such as BLEU or ME-
TEOR (Banerjee and Lavie, 2005). One of the
difficulties in the creation of targeted references
is a further requirement that the annotator attempt
to minimize the number of edits, as measured by
TER, between the MT output and the targeted ref-
erence, creating the reference that is as close as
possible to the MT output while still being ade-
quate and fluent. In this way, only true errors in
the MT output are counted. While HTER has been
shown to be more consistent and finer grained than
individual human annotators of Fluency and Ade-
quacy, it is much more time consuming and tax-
ing on human annotators than other types of hu-
man judgments, making it difficult and expensive
to use. In addition, because HTER treats all edits
equally, no distinction is made between serious er-
rors (errors in names or missing subjects) and mi-
nor edits (such as a difference in verb agreement
</bodyText>
<note confidence="0.937869">
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 259–268,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997659">
259
</page>
<bodyText confidence="0.997410757575758">
or a missing determinator).
Different types of translation errors vary in im-
portance depending on the type of human judg-
ment being used to evaluate the translation. For
example, errors in tense might barely affect the ad-
equacy of a translation but might cause the trans-
lation be scored as less fluent. On the other hand,
deletion of content words might not lower the flu-
ency of a translation but the adequacy would suf-
fer. In this paper, we examine these differences
by taking an automatic evaluation metric and tun-
ing it to these these human judgments and exam-
ining the resulting differences in the parameteri-
zation of the metric. To study this we introduce
a new evaluation metric, TER-Plus (TERp)1 that
improves over the existing Translation Edit Rate
(TER) metric (Snover et al., 2006), incorporating
morphology, synonymy and paraphrases, as well
as tunable costs for different types of errors that
allow for easy interpretation of the differences be-
tween human judgments.
Section 2 summarizes the TER metric and dis-
cusses how TERp improves on it. Correlation re-
sults with human judgments, including indepen-
dent results from the 2008 NIST Metrics MATR
evaluation, where TERp was consistently one of
the top metrics, are presented in Section 3 to show
the utility of TERp as an evaluation metric. The
generation of paraphrases, as well as the effect of
varying the source of paraphrases, is discussed in
Section 4. Section 5 discusses the results of tuning
TERp to Fluency, Adequacy and HTER, and how
this affects the weights of various edit types.
</bodyText>
<sectionHeader confidence="0.87729" genericHeader="method">
2 TER and TERp
</sectionHeader>
<bodyText confidence="0.999968714285714">
Both TER and TERp are automatic evaluation
metrics for machine translation that score a trans-
lation, the hypothesis, of a foreign language text,
the source, against a translation of the source text
that was created by a human translator, called a
reference translation. The set of possible cor-
rect translations is very large—possibly infinite—
and any single reference translation is just a sin-
gle point in that space. Usually multiple refer-
ence translations, typically 4, are provided to give
broader sampling of the space of correct transla-
tions. Automatic MT evaluation metrics compare
the hypothesis against this set of reference trans-
lations and assign a score to the similarity; higher
</bodyText>
<footnote confidence="0.9504655">
1Named after the nickname–”terp”–of the University of
Maryland, College Park, mascot: the diamondback terrapin.
</footnote>
<bodyText confidence="0.999963571428572">
scores are given to hypotheses that are more simi-
lar to the references.
In addition to assigning a score to a hypothe-
sis, the TER metric also provides an alignment be-
tween the hypothesis and the reference, enabling it
to be useful beyond general translation evaluation.
While TER has been shown to correlate well with
human judgments of translation quality, it has sev-
eral flaws, including the use of only a single ref-
erence translation and the measuring of similarity
only by exact word matches between the hypoth-
esis and the reference. The handicap of using a
single reference can be addressed by the construc-
tion of a lattice of reference translations. Such a
technique has been used with TER to combine the
output of multiple translation systems (Rosti et al.,
2007). TERp does not utilize this methodology2
and instead focuses on addressing the exact match-
ing flaw of TER. A brief description of TER is pre-
sented in Section 2.1, followed by a discussion of
how TERp differs from TER in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.950421">
2.1 TER
</subsectionHeader>
<bodyText confidence="0.99996264">
One of the first automatic metrics used to evaluate
automatic machine translation (MT) systems was
Word Error Rate (WER) (Niessen et al., 2000),
which is the standard evaluation metric for Au-
tomatic Speech Recognition. WER is computed
as the Levenshtein (Levenshtein, 1966) distance
between the words of the system output and the
words of the reference translation divided by the
length of the reference translation. Unlike speech
recognition, there are many correct translations for
any given foreign sentence. These correct transla-
tions differ not only in their word choice but also
in the order in which the words occur. WER is
generally seen as inadequate for evaluation for ma-
chine translation as it fails to combine knowledge
from multiple reference translations and also fails
to model the reordering of words and phrases in
translation.
TER addresses the latter failing of WER by al-
lowing block movement of words, called shifts.
within the hypothesis. Shifting a phrase has the
same edit cost as inserting, deleting or substitut-
ing a word, regardless of the number of words
being shifted. While a general solution to WER
with block movement is NP-Complete (Lopresti
</bodyText>
<footnote confidence="0.9654265">
2The technique of combining references in this fashion
has not been evaluated in terms of its benefit when correlating
with human judgments. The authors hope to examine and
incorporate such a technique in future versions of TERp.
</footnote>
<page confidence="0.996124">
260
</page>
<bodyText confidence="0.99995835">
and Tomkins, 1997), TER addresses this by using
a greedy search to select the words to be shifted,
as well as further constraints on the words to be
shifted. These constraints are intended to simu-
late the way in which a human editor might choose
the words to shift. For exact details on these con-
straints, see Snover et al. (2006). There are other
automatic metrics that follow the general formu-
lation as TER but address the complexity of shift-
ing in different ways, such as the CDER evaluation
metric (Leusch et al., 2006).
When TER is used with multiple references, it
does not combine the references. Instead, it scores
the hypothesis against each reference individually.
The reference against which the hypothesis has the
fewest number of edits is deemed the closet refer-
ence, and that number of edits is used as the nu-
merator for calculating the TER score. For the de-
nominator, TER uses the average number of words
across all the references.
</bodyText>
<subsectionHeader confidence="0.999619">
2.2 TER-Plus
</subsectionHeader>
<bodyText confidence="0.999624296296297">
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms. In ad-
dition, it uses probabilistic phrasal substitutions
to align phrases in the hypothesis and reference.
These phrases are generated by considering possi-
ble paraphrases of the reference words. Matching
using stems and synonyms (Banerjee and Lavie,
2005) and using paraphrases (Zhou et al., 2006;
Kauchak and Barzilay, 2006) have previously been
shown to be beneficial for automatic MT evalu-
ation. Paraphrases have also been shown to be
useful in expanding the number of references used
for parameter tuning (Madnani et al., 2007; Mad-
nani et al., 2008) although they are not used di-
rectly in this fashion within TERp. While all edit
costs in TER are constant, all edit costs in TERp
are optimized to maximize correlation with human
judgments. This is because while a set of constant
weights might prove adequate for the purpose of
measuring translation quality—as evidenced by
correlation with human judgments both for TER
and HTER—they may not be ideal for maximiz-
ing correlation.
TERp uses all the edit operations of TER—
Matches, Insertions, Deletions, Substitutions and
Shifts—as well as three new edit operations: Stem
Matches, Synonym Matches and Phrase Substitu-
tions. TERp identifies words in the hypothesis and
reference that share the same stem using the Porter
stemming algorithm (Porter, 1980). Two words
are determined to be synonyms if they share the
same synonym set according to WordNet (Fell-
baum, 1998). Sequences of words in the reference
are considered to be paraphrases of a sequence of
words in the hypothesis if that phrase pair occurs
in the TERp phrase table. The TERp phrase table
is discussed in more detail in Section 4.
With the exception of the phrase substitutions,
the cost for all other edit operations is the same re-
gardless of what the words in question are. That
is, once the edit cost of an operation is determined
via optimization, that operation costs the same no
matter what words are under consideration. The
cost of a phrase substitution, on the other hand,
is a function of the probability of the paraphrase
and the number of edits needed to align the two
phrases according to TERp. In effect, the proba-
bility of the paraphrase is used to determine how
much to discount the alignment of the two phrases.
Specifically, the cost of a phrase substitution be-
tween the reference phrase, p1 and the hypothesis
phrase p2 is:
</bodyText>
<equation confidence="0.999464">
cost(p1,p2) =w1+
edit(p1, p2)x
(w2 log(Pr(p1, p2))
+ w3 Pr(p1,p2) + w4)
</equation>
<bodyText confidence="0.999952809523809">
where w1, w2, w3, and w4 are the 4 free param-
eters of the edit cost, edit(p1,p2) is the edit cost
according to TERp of aligning p1 to p2 (excluding
phrase substitutions) and Pr(p1,p2) is the prob-
ability of paraphrasing p1 as p2, obtained from
the TERp phrase table. The w parameters of the
phrase substitution cost may be negative while still
resulting in a positive phrase substitution cost, as
w2 is multiplied by the log probability, which is al-
ways a negative number. In practice this term will
dominate the phrase substitution edit cost.
This edit cost for phrasal substitutions is, there-
fore, specified by four parameters, w1, w2, w3
and w4. Only paraphrases specified in the TERp
phrase table are considered for phrase substitu-
tions. In addition, the cost for a phrasal substi-
tution is limited to values greater than or equal to
0, i.e., the substitution cost cannot be negative. In
addition, the shifting constraints of TERp are also
relaxed to allow shifting of paraphrases, stems,
and synonyms.
</bodyText>
<page confidence="0.983641">
261
</page>
<bodyText confidence="0.9999824375">
In total TERp uses 11 parameters out of which
four represent the cost of phrasal substitutions.
The match cost is held fixed at 0, so that only the
10 other parameters can vary during optimization.
All edit costs, except for the phrasal substitution
parameters, are also restricted to be positive. A
simple hill-climbing search is used to optimize the
edit costs by maximizing the correlation of human
judgments with the TERp score. These correla-
tions are measured at the sentence, or segment,
level. Although it was done for the experiments
described in this paper, optimization could also
be performed to maximize document level correla-
tion – such an optimization would give decreased
weight to shorter segments as compared to the seg-
ment level optimization.
</bodyText>
<sectionHeader confidence="0.971027" genericHeader="method">
3 Correlation Results
</sectionHeader>
<bodyText confidence="0.99998775">
The optimization of the TERp edit costs, and com-
parisons against several standard automatic eval-
uation metrics, using human judgments of Ade-
quacy is first described in Section 3.1. We then
summarize, in Section 3.2, results of the NIST
Metrics MATR workshop where TERp was eval-
uated as one of 39 automatic metrics using many
test conditions and types of human judgments.
</bodyText>
<subsectionHeader confidence="0.9791885">
3.1 Optimization of Edit Costs and
Correlation Results
</subsectionHeader>
<bodyText confidence="0.999986557692308">
As part of the 2008 NIST Metrics MATR work-
shop (Przybocki et al., 2008), a development sub-
set of translations from eight Arabic-to-English
MT systems submitted to NIST’s MTEval 2006
was released that had been annotated for Ade-
quacy. We divided this development set into an
optimization set and a test set, which we then used
to optimize the edit costs of TERp and compare it
against other evaluation metrics. TERp was op-
timized to maximize the segment level Pearson
correlation with adequacy on the optimization set.
The edit costs determined by this optimization are
shown in Table 1.
We can compare TERp with other metrics by
comparing their Pearson and Spearman corre-
lations with Adequacy, at the segment, docu-
ment and system level. Document level Ade-
quacy scores are determined by taking the length
weighted average of the segment level scores. Sys-
tem level scores are determined by taking the
weighted average of the document level scores in
the same manner.
We compare TERp with BLEU (Papineni et al.,
2002), METEOR (Banerjee and Lavie, 2005), and
TER (Snover et al., 2006). The IBM version of
BLEU was used in case insensitive mode with
an ngram-size of 4 to calculate the BLEU scores.
Case insensitivity was used with BLEU as it was
found to have much higher correlation with Ade-
quacy. In addition, we also examined BLEU using
an ngram-size of 2 (labeled as BLEU-2), instead
of the default ngram-size of 4, as it often has a
higher correlation with human judgments. When
using METEOR, the exact matching, porter stem-
ming matching, and WordNet synonym matching
modules were used. TER was also used in case
insensitive mode.
We show the Pearson and Spearman correlation
numbers of TERp and the other automatic metrics
on the optimization set and the test set in Tables 2
and 3. Correlation numbers that are statistically
indistinguishable from the highest correlation, us-
ing a 95% confidence interval, are shown in bold
and numbers that are actually not statistically sig-
nificant correlations are marked with a †. TERp
has the highest Pearson correlation in all condi-
tions, although not all differences are statistically
significant. When examining the Spearman cor-
relation, TERp has the highest correlation on the
segment and system levels, but performs worse
than METEOR on the document level Spearman
correlatons.
</bodyText>
<subsectionHeader confidence="0.975936">
3.2 NIST Metrics MATR 2008 Results
</subsectionHeader>
<bodyText confidence="0.999930421052632">
TERp was one of 39 automatic metrics evaluated
in the 2008 NIST Metrics MATR Challenge. In
order to evaluate the state of automatic MT eval-
uation, NIST tested metrics across a number of
conditions across 8 test sets. These conditions in-
cluded segment, document and system level corre-
lations with human judgments of preference, flu-
ency, adequacy and HTER. The test sets included
translations from Arabic-to-English, Chinese-to-
English, Farsi-to-English, Arabic-to-French, and
English-to-French MT systems involved in NIST’s
MTEval 2008, the GALE (Olive, 2005) Phase 2
and Phrase 2.5 program, Transtac January and July
2007, and CESTA run 1 and run 2, covering mul-
tiple genres. The version of TERp submitted to
this workshop was optimized as described in Sec-
tion 3.1. The development data upon which TERp
was optimized was not part of the test sets evalu-
ated in the Challenge.
</bodyText>
<page confidence="0.975276">
262
</page>
<table confidence="0.999496">
Phrase Substitution
Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4
0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
</table>
<tableCaption confidence="0.995925">
Table 1: Optimized TERp Edit Costs
</tableCaption>
<table confidence="0.999118">
Metric Optimization Set Test Set Optimization+Test
Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.623 0.867 0.952 0.563 0.852 0.948 0.603 0.861 0.954
BLEU-2 0.661 0.888 0.946 0.591 0.876 0.953 0.637 0.883 0.952
METEOR 0.731 0.894 0.952 0.751 0.904 0.957 0.739 0.898 0.958
TER -0.609 -0.864 -0.957 -0.607 -0.860 -0.959 -0.609 -0.863 -0.961
TERp -0.782 -0.912 -0.996 -0.787 -0.918 -0.985 -0.784 -0.914 -0.994
</table>
<tableCaption confidence="0.999736">
Table 2: Optimization &amp; Test Set Pearson Correlation Results
</tableCaption>
<bodyText confidence="0.99992534375">
Due to the wealth of testing conditions, a sim-
ple overall view of the official MATR08 results re-
leased by NIST is difficult. To facilitate this anal-
ysis, we examined the average rank of each metric
across all conditions, where the rank was deter-
mined by their Pearson and Spearman correlation
with human judgments. To incorporate statistical
significance, we calculated the 95% confidence in-
terval for each correlation coefficient and found
the highest and lowest rank from which the cor-
relation coefficient was statistically indistinguish-
able, resulting in lower and upper bounds of the
rank for each metric in each condition. The aver-
age lower bound, actual, and upper bound ranks
(where a rank of 1 indicates the highest correla-
tion) of the top metrics, as well as BLEU and TER,
are shown in Table 4, sorted by the average upper
bound Pearson correlation. Full descriptions of the
other metrics3, the evaluation results, and the test
set composition are available from NIST (Przy-
bocki et al., 2008).
This analysis shows that TERp was consistently
one of the top metrics across test conditions and
had the highest average rank both in terms of Pear-
son and Spearman correlations. While this anal-
ysis is not comprehensive, it does give a general
idea of the performance of all metrics by syn-
thesizing the results into a single table. There
are striking differences between the Spearman and
Pearson correlations for other metrics, in particu-
lar the CDER metric (Leusch et al., 2006) had the
second highest rank in Spearman correlations (af-
</bodyText>
<footnote confidence="0.972684666666667">
3System description of metrics are also distributed
by AMTA: http://www.amtaweb.org/AMTA2008.
html
</footnote>
<bodyText confidence="0.99971425">
ter TERp), but was the sixth ranked metric accord-
ing to the Pearson correlation. In several cases,
TERp was not the best metric (if a metric was the
best in all conditions, its average rank would be 1),
although it performed well on average. In partic-
ular, TERp did significantly better than the TER
metric, indicating the benefit of the enhancements
made to TER.
</bodyText>
<sectionHeader confidence="0.999181" genericHeader="method">
4 Paraphrases
</sectionHeader>
<bodyText confidence="0.99989275">
TERp uses probabilistic phrasal substitutions to
align phrases in the hypothesis with phrases in the
reference. It does so by looking up—in a pre-
computed phrase table—paraphrases of phrases in
the reference and using its associated edit cost as
the cost of performing a match against the hy-
pothesis. The paraphrases used in TERp were ex-
tracted using the pivot-based method as described
in (Bannard and Callison-Burch, 2005) with sev-
eral additional filtering mechanisms to increase
the precision. The pivot-based method utilizes the
inherent monolingual semantic knowledge from
bilingual corpora: we first identify English-to-F
phrasal correspondences, then map from English
to English by following translation units from En-
glish to F and back. For example, if the two En-
glish phrases e1 and e2 both correspond to the
same foreign phrase f, then they may be consid-
ered to be paraphrases of each other with the fol-
lowing probability:
</bodyText>
<equation confidence="0.987333">
p(e1|e2) pz� p(e1|f) * p(f|e2)
</equation>
<bodyText confidence="0.9926435">
If there are several pivot phrases that link the two
English phrases, then they are all used in comput-
</bodyText>
<page confidence="0.997325">
263
</page>
<table confidence="0.995884428571428">
Metric Optimization Set Test Set Optimization+Test
Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.635 0.816 0.7141 0.550 0.740 0.6901 0.606 0.794 0.7381
BLEU-2 0.643 0.823 0.7861 0.558 0.747 0.6901 0.614 0.799 0.7381
METEOR 0.729 0.886 0.881 0.727 0.853 0.7381 0.730 0.876 0.922
TER -0.630 -0.794 -0.8101 -0.630 -0.797 -0.6671 -0.631 -0.801 -0.7861
TERp -0.760 -0.834 -0.976 -0.737 -0.818 -0.881 -0.754 -0.834 -0.929
</table>
<tableCaption confidence="0.996916">
Table 3: MT06 Dev. Optimization &amp; Test Set Spearman Correlation Results
</tableCaption>
<table confidence="0.999925230769231">
Metric Average Rank by Pearson Average Rank by Spearman
TERp 1.49 « 6.07 « 17.31 1.60 « 6.44 « 17.76
METEOR v0.7 1.82 « 7.64 « 18.70 1.73 « 8.21 « 19.33
METEOR ranking 2.39 « 9.45 « 19.91 2.18 « 10.18 « 19.67
METEOR v0.6 2.42 « 10.67 « 19.11 2.47 « 11.27 « 19.60
EDPM 2.45 « 8.21 « 20.97 2.79 « 7.61 « 20.52
CDER 2.93 « 8.53 « 19.67 1.69 « 8.00 « 18.80
BleuSP 3.67 « 9.93 « 21.40 3.16 « 8.29 « 20.80
NIST-v11b 3.82 « 11.13 « 21.96 4.64 « 12.29 « 23.38
BLEU-1 (IBM) 4.42 « 12.47 « 22.18 4.98 « 14.87 « 24.00
BLEU-4 (IBM) 6.93 « 15.40 « 24.69 6.98 « 14.38 « 25.11
TER v0.7.25 8.87 « 16.27 « 25.29 6.93 « 17.33 « 24.80
BLEU-4 v12 (NIST) 10.16 « 18.02 « 27.64 10.96 « 17.82 « 28.16
</table>
<tableCaption confidence="0.999263">
Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results
</tableCaption>
<bodyText confidence="0.580086">
ing the probability:
</bodyText>
<equation confidence="0.9963855">
p(e1|e2) � � p(e1|f&apos;) * p(f&apos;|e2)
P
</equation>
<bodyText confidence="0.994704333333333">
The corpus used for extraction was an Arabic-
English newswire bitext containing a million sen-
tences. A few examples of the extracted para-
phrase pairs that were actually used in a run of
TERp on the Metrics MATR 2008 development
set are shown below:
(brief -* short)
(controversy over -* polemic about)
(by using power -* by force)
(response -* reaction)
A discussion of paraphrase quality is presented
in Section 4.1, followed by a brief analysis of the
effect of varying the pivot corpus used by the auto-
matic paraphrase generation upon the correlation
performance of the TERp metric in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.999763">
4.1 Analysis of Paraphrase Quality
</subsectionHeader>
<bodyText confidence="0.998707">
We analyzed the utility of the paraphrase probabil-
ity and found that it was not always a very reliable
estimate of the degree to which the pair was se-
mantically related. For example, we looked at all
paraphrase pairs that had probabilities greater than
0.9, a set that should ideally contain pairs that are
paraphrastic to a large degree. In our analysis, we
found the following five kinds of paraphrases in
this set:
</bodyText>
<listItem confidence="0.992327142857143">
(a) Lexical Paraphrases. These paraphrase
pairs are not phrasal paraphrases but instead
differ in at most one word and may be con-
sidered as lexical paraphrases for all practical
purposes. While these pairs may not be very
valuable for TERp due to the obvious overlap
with WordNet, they may help in increasing
the coverage of the paraphrastic phenomena
that TERp can handle. Here are some exam-
ples:
(2500 polish troops -* 2500 polish soldiers)
(accounting firms -* auditing firms)
(armed source -* military source)
(b) Morphological Variants. These phrasal
</listItem>
<bodyText confidence="0.57243">
pairs only differ in the morphological form
</bodyText>
<page confidence="0.995865">
264
</page>
<bodyText confidence="0.788378166666666">
for one of the words. As the examples show,
any knowledge that these pairs may provide
is already available to TERp via stemming.
(50 ton → 50 tons)
(caused clouds → causing clouds)
(syria deny → syria denies)
</bodyText>
<listItem confidence="0.90762440625">
(c) Approximate Phrasal Paraphrases. This
set included pairs that only shared partial se-
mantic content. Most paraphrases extracted
by the pivot method are expected to be of this
nature. These pairs are not directly beneficial
to TERp since they cannot be substituted for
each other in all contexts. However, the fact
that they share at least some semantic content
does suggest that they may not be entirely
useless either. Examples include:
(mutual proposal → suggest)
(them were exiled → them abroad)
(my parents → my father)
(d) Phrasal Paraphrases. We did indeed find
a large number of pairs in this set that were
truly paraphrastic and proved the most useful
for TERp. For example:
(agence presse → news agency)
(army roadblock → military barrier)
(staff walked out → team withdrew)
(e) Noisy Co-occurrences. There are also pairs
that are completely unrelated and happen
to be extracted as paraphrases based on the
noise inherent in the pivoting process. These
pairs are much smaller in number than the
four sets described above and are not signif-
icantly detrimental to TERp since they are
rarely chosen for phrasal substitution. Exam-
ples:
(counterpart salam → peace)
(regulation dealing → list)
(recall one → deported)
</listItem>
<bodyText confidence="0.999962769230769">
Given this distribution of the pivot-based para-
phrases, we experimented with a variant of TERp
that did not use the paraphrase probability at all
but instead only used the actual edit distance be-
tween the two phrases to determine the final cost
of a phrase substitution. The results for this exper-
iment are shown in the second row of Table 5. We
can see that this variant works as well as the full
version of TERp that utilizes paraphrase probabil-
ities. This confirms our intuition that the proba-
bility computed via the pivot-method is not a very
useful predictor of semantic equivalence for use in
TERp.
</bodyText>
<subsectionHeader confidence="0.999491">
4.2 Varying Paraphrase Pivot Corpora
</subsectionHeader>
<bodyText confidence="0.999977192307693">
To determine the effect that the pivot language
might have on the quality and utility of the ex-
tracted paraphrases in TERp, we used paraphrase
pairsmade available by Callison-Burch (2008).
These paraphrase pairs were extracted from Eu-
roparl data using each of 10 European languages
(German, Italian, French etc.) as a pivot language
separately and then combining the extracted para-
phrase pairs. Callison-Burch (2008) also extracted
and made available syntactically constrained para-
phrase pairs from the same data that are more
likely to be semantically related.
We used both sets of paraphrases in TERp as al-
ternatives to the paraphrase pairs that we extracted
from the Arabic newswire bitext. The results are
shown in the last four rows of Table 5 and show
that using a pivot language other than the one that
the MT system is actually translating yields results
that are almost as good. It also shows that the
syntactic constraints imposed by Callison-Burch
(2008) on the pivot-based paraphrase extraction
process are useful and yield improved results over
the baseline pivot-method. The results further sup-
port our claim that the pivot paraphrase probability
is not a very useful indicator of semantic related-
ness.
</bodyText>
<sectionHeader confidence="0.926868" genericHeader="method">
5 Varying Human Judgments
</sectionHeader>
<bodyText confidence="0.999005714285714">
To evaluate the differences between human judg-
ment types we first align the hypothesis to the ref-
erences using a fixed set of edit costs, identical to
the weights in Table 1, and then optimize the edit
costs to maximize the correlation, without realign-
ing. The separation of the edit costs used for align-
ment from those used for scoring allows us to re-
move the confusion of edit costs selected for align-
ment purposes from those selected to increase cor-
relation.
For Adequacy and Fluency judgments, the
MTEval 2002 human judgement set4 was used.
This set consists of the output of ten MT sys-
tems, 3 Arabic-to-English systems and 7 Chinese-
</bodyText>
<footnote confidence="0.950181">
4Distributed to the authors by request from NIST.
</footnote>
<page confidence="0.989129">
265
</page>
<table confidence="0.99615825">
Paraphrase Setup Pearson Spearman
Seg Doc Sys Seg Doc Sys
Arabic pivot -0.787 -0.918 -0.985 -0.737 -0.818 -0.881
Arabic pivot and no prob -0.787 -0.933 -0.986 -0.737 -0.841 -0.881
Europarl pivot -0.775 -0.940 -0.983 -0.738 -0.865 -0.905
Europarl pivot and no prob -0.775 -0.940 -0.983 -0.737 -0.860 -0.905
Europarl pivot and syntactic constraints -0.781 -0.941 -0.985 -0.739 -0.859 -0.881
Europarl pivot, syntactic constraints and no prob -0.779 -0.946 -0.985 -0.737 -0.866 -0.976
</table>
<tableCaption confidence="0.997757">
Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.
</tableCaption>
<table confidence="0.999877166666667">
Human Match Insert Deletion Subst. Stem Syn. Shift Phrase Substitution
Judgment w1 w2 w3 w4
Alignment 0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
Adequacy 0.0 0.18 1.42 1.71 0.0 0.0 0.19 -0.38 -0.03 0.22 0.47
Fluency 0.0 0.12 1.37 1.81 0.0 0.0 0.43 -0.63 -0.07 0.12 0.46
HTER 0.0 0.84 0.76 1.55 0.90 0.75 1.07 -0.03 -0.17 -0.08 -0.09
</table>
<tableCaption confidence="0.998345">
Table 6: Optimized Edit Costs
</tableCaption>
<bodyText confidence="0.999854140350877">
to-English systems, consisting of a total, across
all systems and both language pairs, of 7,452 seg-
ments across 900 documents. To evaluate HTER,
the GALE (Olive, 2005) 2007 (Phase 2.0) HTER
scores were used. This set consists of the out-
put of 6 MT systems, 3 Arabic-to-English systems
and 3 Chinese-to-English systems, although each
of the systems in question is the product of system
combination. The HTER data consisted of a total,
across all systems and language pairs, of 16,267
segments across a total of 1,568 documents. Be-
cause HTER annotation is especially expensive
and difficult, it is rarely performed, and the only
source, to the authors’ knowledge, of available
HTER annotations is on GALE evaluation data for
which no Fluency and Adequacy judgments have
been made publicly available.
The edit costs learned for each of these human
judgments, along with the alignment edit costs are
shown in Table 6. While all three types of human
judgements differ from the alignment costs used
in alignment, the HTER edit costs differ most sig-
nificantly. Unlike Adequacy and Fluency which
have a low edit cost for insertions and a very high
cost for deletions, HTER has a balanced cost for
the two edit types. Inserted words are strongly pe-
nalized against in HTER, as opposed to in Ade-
quacy and Fluency, where such errors are largely
forgiven. Stem and synonym edits are also penal-
ized against while these are considered equivalent
to a match for both Adequacy and Fluency. This
penalty against stem matches can be attributed to
Fluency requirements in HTER that specifically
penalize against incorrect morphology. The cost
of shifts is also increased in HTER, strongly penal-
izing the movement of phrases within the hypoth-
esis, while Adequacy and Fluency give a much
lower cost to such errors. Some of the differences
between HTER and both fluency and adequacy
can be attributed to the different systems used. The
MT systems evaluated with HTER are all highly
performing state of the art systems, while the sys-
tems used for adequacy and fluency are older MT
systems.
The differences between Adequacy and Fluency
are smaller, but there are still significant differ-
ences. In particular, the cost of shifts is over twice
as high for the fluency optimized system than the
adequacy optimized system, indicating that the
movement of phrases, as expected, is only slightly
penalized when judging meaning, but can be much
more harmful to the fluency of a translation. Flu-
ency however favors paraphrases more strongly
than the edit costs optimized for adequacy. This
might indicate that paraphrases are used to gener-
ate a more fluent translation although at the poten-
tial loss of meaning.
</bodyText>
<page confidence="0.996996">
266
</page>
<sectionHeader confidence="0.999787" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999993596491228">
We introduced a new evaluation metric, TER-Plus,
and showed that it is competitive with state-of-the-
art evaluation metrics when its predictions are cor-
related with human judgments. The inclusion of
stem, synonym and paraphrase edits allows TERp
to overcome some of the weaknesses of the TER
metric and better align hypothesized translations
with reference translations. These new edit costs
can then be optimized to allow better correlation
with human judgments. In addition, we have ex-
amined the use of other paraphrasing techniques,
and shown that the paraphrase probabilities esti-
mated by the pivot-method may not be fully ad-
equate for judgments of whether a paraphrase in
a translation indicates a correct translation. This
line of research holds promise as an external eval-
uation method of various paraphrasing methods.
However promising correlation results for an
evaluation metric may be, the evaluation of the
final output of an MT system is only a portion
of the utility of an automatic translation metric.
Optimization of the parameters of an MT system
is now done using automatic metrics, primarily
BLEU. It is likely that some features that make an
evaluation metric good for evaluating the final out-
put of a system would make it a poor metric for use
in system tuning. In particular, a metric may have
difficulty distinguishing between outputs of an MT
system that been optimized for that same metric.
BLEU, the metric most frequently used to opti-
mize systems, might therefore perform poorly in
evaluation tasks compared to recall oriented met-
rics such as METEOR and TERp (whose tuning
in Table 1 indicates a preference towards recall).
Future research into the use of TERp and other
metrics as optimization metrics is needed to better
understand these metrics and the interaction with
parameter optimization.
Finally, we explored the difference between
three types of human judgments that are often
used to evaluate both MT systems and automatic
metrics, by optimizing TERp to these human
judgments and examining the resulting edit costs.
While this can make no judgement as to the pref-
erence of one type of human judgment over an-
other, it indicates differences between these hu-
man judgment types, and in particular the differ-
ence between HTER and Adequacy and Fluency.
This exploration is limited by the the lack of a
large amount of diverse data annotated for all hu-
man judgment types, as well as the small num-
ber of edit types used by TERp. The inclusion
of additional more specific edit types could lead
to a more detailed understanding of which trans-
lation phenomenon and translation errors are most
emphasized or ignored by which types of human
judgments.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999492">
This work was supported, in part, by BBN Tech-
nologies under the GALE Program, DARPA/IPTO
Contract No. HR0011-06-C-0022 and in part by
the Human Language Technology Center of Ex-
cellence.. TERp is available on the web for down-
load at: http://www.umiacs.umd.edu/∼snover/terp/.
</bodyText>
<sectionHeader confidence="0.999658" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999854205882353">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL 2005 Workshop on Intrinsic and
Extrinsic Evaulation Measures for MT and/or Sum-
marization.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005),
pages 597–604, Ann Arbor, Michigan, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196–
205, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
http://www.cogsci.princeton.edu/˜wn [2000,
September 7].
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the ACL, pages 455–
462.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block
Movements. In Proceedings of the 11th Confer-
enceof the European Chapter of the Association for
Computational Linguistics (EACL 2006).
V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Soviet
Physics Doklady, 10:707–710.
</reference>
<page confidence="0.961253">
267
</page>
<reference confidence="0.999594240740741">
Daniel Lopresti and Andrew Tomkins. 1997. Block
edit models for approximate string matching. Theo-
retical Computer Science, 181(1):159–179, July.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are Multiple Reference
Translations Necessary? Investigating the Value
of Paraphrased Reference Translations in Parameter
Optimization. In Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, October.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalua-
tion for MT research. In Proceedings of the 2nd In-
ternational Conference on Language Resources and
Evaluation (LREC-2000), pages 39–45.
Joseph Olive. 2005. Global Autonomous Language
Exploitation (GALE). DARPA/IPTO Proposer In-
formation Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Traslation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Martin F. Porter. 1980. An algorithm for suffic strip-
ping. Program, 14(3):130–137.
Mark Przybocki, Kay Peterson, and Sebas-
tian Bronsart. 2008. Official results
of the NIST 2008 ”Metrics for MAchine
TRanslation” Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/,
October.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 312–319, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings ofAssociation for Machine
Translation in the Americas.
Liang Zhou, Chon-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating Machine Translation Results with
Paraphrase Support. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2006), pages 77–84.
</reference>
<page confidence="0.99676">
268
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.183994">
<title confidence="0.652138875">Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric Nitin Bonnie J. † Richard for Computational Linguistics and Information for Advanced Computer of Maryland, College Language Technology Center of Technologies</title>
<email confidence="0.996484">schwartz@bbn.com</email>
<abstract confidence="0.999658407407407">Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaulation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="2852" citStr="Banerjee and Lavie, 2005" startWordPosition="432" endWordPosition="435"> frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy, it is much more time consuming and taxing on human annotators than other types of human </context>
<context position="10064" citStr="Banerjee and Lavie, 2005" startWordPosition="1635" endWordPosition="1638">reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation q</context>
<context position="15507" citStr="Banerjee and Lavie, 2005" startWordPosition="2547" endWordPosition="2550">ized to maximize the segment level Pearson correlation with adequacy on the optimization set. The edit costs determined by this optimization are shown in Table 1. We can compare TERp with other metrics by comparing their Pearson and Spearman correlations with Adequacy, at the segment, document and system level. Document level Adequacy scores are determined by taking the length weighted average of the segment level scores. System level scores are determined by taking the weighted average of the document level scores in the same manner. We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). The IBM version of BLEU was used in case insensitive mode with an ngram-size of 4 to calculate the BLEU scores. Case insensitivity was used with BLEU as it was found to have much higher correlation with Adequacy. In addition, we also examined BLEU using an ngram-size of 2 (labeled as BLEU-2), instead of the default ngram-size of 4, as it often has a higher correlation with human judgments. When using METEOR, the exact matching, porter stemming matching, and WordNet synonym matching modules were used. TER was also used in case insensitive mode. We show the Pears</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaulation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>597--604</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="20764" citStr="Bannard and Callison-Burch, 2005" startWordPosition="3410" endWordPosition="3413">ons, its average rank would be 1), although it performed well on average. In particular, TERp did significantly better than the TER metric, indicating the benefit of the enhancements made to TER. 4 Paraphrases TERp uses probabilistic phrasal substitutions to align phrases in the hypothesis with phrases in the reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrases used in TERp were extracted using the pivot-based method as described in (Bannard and Callison-Burch, 2005) with several additional filtering mechanisms to increase the precision. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify English-to-F phrasal correspondences, then map from English to English by following translation units from English to F and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) pz� p(e1|f) * p(f|e2) If there are several pivot phrases that link the two English</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 597–604, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>196--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="26621" citStr="Callison-Burch (2008)" startWordPosition="4421" endWordPosition="4422"> phrases to determine the final cost of a phrase substitution. The results for this experiment are shown in the second row of Table 5. We can see that this variant works as well as the full version of TERp that utilizes paraphrase probabilities. This confirms our intuition that the probability computed via the pivot-method is not a very useful predictor of semantic equivalence for use in TERp. 4.2 Varying Paraphrase Pivot Corpora To determine the effect that the pivot language might have on the quality and utility of the extracted paraphrases in TERp, we used paraphrase pairsmade available by Callison-Burch (2008). These paraphrase pairs were extracted from Europarl data using each of 10 European languages (German, Italian, French etc.) as a pivot language separately and then combining the extracted paraphrase pairs. Callison-Burch (2008) also extracted and made available syntactically constrained paraphrase pairs from the same data that are more likely to be semantically related. We used both sets of paraphrases in TERp as alternatives to the paraphrase pairs that we extracted from the Arabic newswire bitext. The results are shown in the last four rows of Table 5 and show that using a pivot language o</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 196– 205, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11228" citStr="Fellbaum, 1998" startWordPosition="1825" endWordPosition="1827">adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and Phrase Substitutions. TERp identifies words in the hypothesis and reference that share the same stem using the Porter stemming algorithm (Porter, 1980). Two words are determined to be synonyms if they share the same synonym set according to WordNet (Fellbaum, 1998). Sequences of words in the reference are considered to be paraphrases of a sequence of words in the hypothesis if that phrase pair occurs in the TERp phrase table. The TERp phrase table is discussed in more detail in Section 4. With the exception of the phrase substitutions, the cost for all other edit operations is the same regardless of what the words in question are. That is, once the edit cost of an operation is determined via optimization, that operation costs the same no matter what words are under consideration. The cost of a phrase substitution, on the other hand, is a function of the</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="false">
<date>2000</date>
<location>http://www.cogsci.princeton.edu/˜wn</location>
<marker>2000</marker>
<rawString>http://www.cogsci.princeton.edu/˜wn [2000, September 7].</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for Automatic Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="10134" citStr="Kauchak and Barzilay, 2006" startWordPosition="1646" endWordPosition="1649">culating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER a</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 455– 462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDER: Efficient MT Evaluation Using Block Movements.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conferenceof the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<contexts>
<context position="9192" citStr="Leusch et al., 2006" startWordPosition="1491" endWordPosition="1494">ts. The authors hope to examine and incorporate such a technique in future versions of TERp. 260 and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem </context>
<context position="19807" citStr="Leusch et al., 2006" startWordPosition="3256" endWordPosition="3259">ll descriptions of the other metrics3, the evaluation results, and the test set composition are available from NIST (Przybocki et al., 2008). This analysis shows that TERp was consistently one of the top metrics across test conditions and had the highest average rank both in terms of Pearson and Spearman correlations. While this analysis is not comprehensive, it does give a general idea of the performance of all metrics by synthesizing the results into a single table. There are striking differences between the Spearman and Pearson correlations for other metrics, in particular the CDER metric (Leusch et al., 2006) had the second highest rank in Spearman correlations (af3System description of metrics are also distributed by AMTA: http://www.amtaweb.org/AMTA2008. html ter TERp), but was the sixth ranked metric according to the Pearson correlation. In several cases, TERp was not the best metric (if a metric was the best in all conditions, its average rank would be 1), although it performed well on average. In particular, TERp did significantly better than the TER metric, indicating the benefit of the enhancements made to TER. 4 Paraphrases TERp uses probabilistic phrasal substitutions to align phrases in </context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In Proceedings of the 11th Conferenceof the European Chapter of the Association for Computational Linguistics (EACL 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions, and Reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="7534" citStr="Levenshtein, 1966" startWordPosition="1214" endWordPosition="1215">nique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference translation divided by the length of the reference translation. Unlike speech recognition, there are many correct translations for any given foreign sentence. These correct translations differ not only in their word choice but also in the order in which the words occur. WER is generally seen as inadequate for evaluation for machine translation as it fails to combine knowledge from multiple reference translations and also fails to model the reordering of words and phrases in translation. TER addresses the latter fai</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions, and Reversals. Soviet Physics Doklady, 10:707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lopresti</author>
<author>Andrew Tomkins</author>
</authors>
<title>Block edit models for approximate string matching.</title>
<date>1997</date>
<journal>Theoretical Computer Science,</journal>
<volume>181</volume>
<issue>1</issue>
<marker>Lopresti, Tomkins, 1997</marker>
<rawString>Daniel Lopresti and Andrew Tomkins. 1997. Block edit models for approximate string matching. Theoretical Computer Science, 181(1):159–179, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil Ayan</author>
<author>Philip Resnik</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10339" citStr="Madnani et al., 2007" startWordPosition="1680" endWordPosition="1683">ot only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Match</context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie J. Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Philip Resnik</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas,</booktitle>
<contexts>
<context position="10362" citStr="Madnani et al., 2008" startWordPosition="1684" endWordPosition="1688">exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and</context>
</contexts>
<marker>Madnani, Resnik, Dorr, Schwartz, 2008</marker>
<rawString>Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and Richard Schwartz. 2008. Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization. In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC-2000),</booktitle>
<pages>39--45</pages>
<contexts>
<context position="7404" citStr="Niessen et al., 2000" startWordPosition="1193" endWordPosition="1196">ce. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference translation divided by the length of the reference translation. Unlike speech recognition, there are many correct translations for any given foreign sentence. These correct translations differ not only in their word choice but also in the order in which the words occur. WER is generally seen as inadequate for evaluation for machine translation as it fails to combine knowledge from multip</context>
</contexts>
<marker>Niessen, Och, Leusch, Ney, 2000</marker>
<rawString>S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC-2000), pages 39–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Olive</author>
</authors>
<title>Global Autonomous Language Exploitation (GALE). DARPA/IPTO Proposer Information Pamphlet.</title>
<date>2005</date>
<contexts>
<context position="17374" citStr="Olive, 2005" startWordPosition="2847" endWordPosition="2848">earman correlatons. 3.2 NIST Metrics MATR 2008 Results TERp was one of 39 automatic metrics evaluated in the 2008 NIST Metrics MATR Challenge. In order to evaluate the state of automatic MT evaluation, NIST tested metrics across a number of conditions across 8 test sets. These conditions included segment, document and system level correlations with human judgments of preference, fluency, adequacy and HTER. The test sets included translations from Arabic-to-English, Chinese-toEnglish, Farsi-to-English, Arabic-to-French, and English-to-French MT systems involved in NIST’s MTEval 2008, the GALE (Olive, 2005) Phase 2 and Phrase 2.5 program, Transtac January and July 2007, and CESTA run 1 and run 2, covering multiple genres. The version of TERp submitted to this workshop was optimized as described in Section 3.1. The development data upon which TERp was optimized was not part of the test sets evaluated in the Challenge. 262 Phrase Substitution Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4 0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18 Table 1: Optimized TERp Edit Costs Metric Optimization Set Test Set Optimization+Test Seg Doc Sys Seg Doc Sys Seg Doc Sys BLEU 0.623 0.867 0.952 0.</context>
<context position="29483" citStr="Olive, 2005" startWordPosition="4894" endWordPosition="4895">e NIST MATR 2008 test set for several variations of paraphrase usage. Human Match Insert Deletion Subst. Stem Syn. Shift Phrase Substitution Judgment w1 w2 w3 w4 Alignment 0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18 Adequacy 0.0 0.18 1.42 1.71 0.0 0.0 0.19 -0.38 -0.03 0.22 0.47 Fluency 0.0 0.12 1.37 1.81 0.0 0.0 0.43 -0.63 -0.07 0.12 0.46 HTER 0.0 0.84 0.76 1.55 0.90 0.75 1.07 -0.03 -0.17 -0.08 -0.09 Table 6: Optimized Edit Costs to-English systems, consisting of a total, across all systems and both language pairs, of 7,452 segments across 900 documents. To evaluate HTER, the GALE (Olive, 2005) 2007 (Phase 2.0) HTER scores were used. This set consists of the output of 6 MT systems, 3 Arabic-to-English systems and 3 Chinese-to-English systems, although each of the systems in question is the product of system combination. The HTER data consisted of a total, across all systems and language pairs, of 16,267 segments across a total of 1,568 documents. Because HTER annotation is especially expensive and difficult, it is rarely performed, and the only source, to the authors’ knowledge, of available HTER annotations is on GALE evaluation data for which no Fluency and Adequacy judgments have</context>
</contexts>
<marker>Olive, 2005</marker>
<rawString>Joseph Olive. 2005. Global Autonomous Language Exploitation (GALE). DARPA/IPTO Proposer Information Pamphlet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Traslation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1514" citStr="Papineni et al., 2002" startWordPosition="214" endWordPosition="218">ds the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. 1 Introduction Since the introduction of the BLEU metric (Papineni et al., 2002), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics. These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality. Numerous methods of judging MT output by humans have been used, including Fluency, Adequacy, and, more recently, Human-mediated Translation Edit Rate (HTER) (Snover et al., 2006). Fluency measures whether a translation is fluent, regardless of the correct meaning, while Adequacy measures whether t</context>
<context position="15472" citStr="Papineni et al., 2002" startWordPosition="2542" endWordPosition="2545">aluation metrics. TERp was optimized to maximize the segment level Pearson correlation with adequacy on the optimization set. The edit costs determined by this optimization are shown in Table 1. We can compare TERp with other metrics by comparing their Pearson and Spearman correlations with Adequacy, at the segment, document and system level. Document level Adequacy scores are determined by taking the length weighted average of the segment level scores. System level scores are determined by taking the weighted average of the document level scores in the same manner. We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). The IBM version of BLEU was used in case insensitive mode with an ngram-size of 4 to calculate the BLEU scores. Case insensitivity was used with BLEU as it was found to have much higher correlation with Adequacy. In addition, we also examined BLEU using an ngram-size of 2 (labeled as BLEU-2), instead of the default ngram-size of 4, as it often has a higher correlation with human judgments. When using METEOR, the exact matching, porter stemming matching, and WordNet synonym matching modules were used. TER was also used in case </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Traslation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffic stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="11114" citStr="Porter, 1980" startWordPosition="1806" endWordPosition="1807">mized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and Phrase Substitutions. TERp identifies words in the hypothesis and reference that share the same stem using the Porter stemming algorithm (Porter, 1980). Two words are determined to be synonyms if they share the same synonym set according to WordNet (Fellbaum, 1998). Sequences of words in the reference are considered to be paraphrases of a sequence of words in the hypothesis if that phrase pair occurs in the TERp phrase table. The TERp phrase table is discussed in more detail in Section 4. With the exception of the phrase substitutions, the cost for all other edit operations is the same regardless of what the words in question are. That is, once the edit cost of an operation is determined via optimization, that operation costs the same no mat</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffic stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>Sebastian Bronsart</author>
</authors>
<date>2008</date>
<booktitle>Official results of the NIST 2008 ”Metrics for MAchine TRanslation” Challenge (MetricsMATR08). http://nist.gov/speech/tests/metricsmatr/2008/results/,</booktitle>
<contexts>
<context position="14530" citStr="Przybocki et al., 2008" startWordPosition="2385" endWordPosition="2388">mization would give decreased weight to shorter segments as compared to the segment level optimization. 3 Correlation Results The optimization of the TERp edit costs, and comparisons against several standard automatic evaluation metrics, using human judgments of Adequacy is first described in Section 3.1. We then summarize, in Section 3.2, results of the NIST Metrics MATR workshop where TERp was evaluated as one of 39 automatic metrics using many test conditions and types of human judgments. 3.1 Optimization of Edit Costs and Correlation Results As part of the 2008 NIST Metrics MATR workshop (Przybocki et al., 2008), a development subset of translations from eight Arabic-to-English MT systems submitted to NIST’s MTEval 2006 was released that had been annotated for Adequacy. We divided this development set into an optimization set and a test set, which we then used to optimize the edit costs of TERp and compare it against other evaluation metrics. TERp was optimized to maximize the segment level Pearson correlation with adequacy on the optimization set. The edit costs determined by this optimization are shown in Table 1. We can compare TERp with other metrics by comparing their Pearson and Spearman correl</context>
<context position="19327" citStr="Przybocki et al., 2008" startWordPosition="3174" endWordPosition="3178">confidence interval for each correlation coefficient and found the highest and lowest rank from which the correlation coefficient was statistically indistinguishable, resulting in lower and upper bounds of the rank for each metric in each condition. The average lower bound, actual, and upper bound ranks (where a rank of 1 indicates the highest correlation) of the top metrics, as well as BLEU and TER, are shown in Table 4, sorted by the average upper bound Pearson correlation. Full descriptions of the other metrics3, the evaluation results, and the test set composition are available from NIST (Przybocki et al., 2008). This analysis shows that TERp was consistently one of the top metrics across test conditions and had the highest average rank both in terms of Pearson and Spearman correlations. While this analysis is not comprehensive, it does give a general idea of the performance of all metrics by synthesizing the results into a single table. There are striking differences between the Spearman and Pearson correlations for other metrics, in particular the CDER metric (Leusch et al., 2006) had the second highest rank in Spearman correlations (af3System description of metrics are also distributed by AMTA: ht</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>Mark Przybocki, Kay Peterson, and Sebastian Bronsart. 2008. Official results of the NIST 2008 ”Metrics for MAchine TRanslation” Challenge (MetricsMATR08). http://nist.gov/speech/tests/metricsmatr/2008/results/, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>312--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7019" citStr="Rosti et al., 2007" startWordPosition="1126" endWordPosition="1129">an alignment between the hypothesis and the reference, enabling it to be useful beyond general translation evaluation. While TER has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference trans</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="1994" citStr="Snover et al., 2006" startWordPosition="288" endWordPosition="291">gnificant differences between the types of human judgments. 1 Introduction Since the introduction of the BLEU metric (Papineni et al., 2002), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics. These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality. Numerous methods of judging MT output by humans have been used, including Fluency, Adequacy, and, more recently, Human-mediated Translation Edit Rate (HTER) (Snover et al., 2006). Fluency measures whether a translation is fluent, regardless of the correct meaning, while Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent. Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the origin</context>
<context position="4667" citStr="Snover et al., 2006" startWordPosition="741" endWordPosition="744">example, errors in tense might barely affect the adequacy of a translation but might cause the translation be scored as less fluent. On the other hand, deletion of content words might not lower the fluency of a translation but the adequacy would suffer. In this paper, we examine these differences by taking an automatic evaluation metric and tuning it to these these human judgments and examining the resulting differences in the parameterization of the metric. To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments. Section 2 summarizes the TER metric and discusses how TERp improves on it. Correlation results with human judgments, including independent results from the 2008 NIST Metrics MATR evaluation, where TERp was consistently one of the top metrics, are presented in Section 3 to show the utility of TERp as an evaluation metric. The generation of paraphrases, as well as the effect of varying the source of paraphrases</context>
<context position="8999" citStr="Snover et al. (2006)" startWordPosition="1458" endWordPosition="1461">ion to WER with block movement is NP-Complete (Lopresti 2The technique of combining references in this fashion has not been evaluated in terms of its benefit when correlating with human judgments. The authors hope to examine and incorporate such a technique in future versions of TERp. 260 and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all</context>
<context position="15538" citStr="Snover et al., 2006" startWordPosition="2553" endWordPosition="2556">earson correlation with adequacy on the optimization set. The edit costs determined by this optimization are shown in Table 1. We can compare TERp with other metrics by comparing their Pearson and Spearman correlations with Adequacy, at the segment, document and system level. Document level Adequacy scores are determined by taking the length weighted average of the segment level scores. System level scores are determined by taking the weighted average of the document level scores in the same manner. We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). The IBM version of BLEU was used in case insensitive mode with an ngram-size of 4 to calculate the BLEU scores. Case insensitivity was used with BLEU as it was found to have much higher correlation with Adequacy. In addition, we also examined BLEU using an ngram-size of 2 (labeled as BLEU-2), instead of the default ngram-size of 4, as it often has a higher correlation with human judgments. When using METEOR, the exact matching, porter stemming matching, and WordNet synonym matching modules were used. TER was also used in case insensitive mode. We show the Pearson and Spearman correlation num</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings ofAssociation for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chon-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Re-evaluating Machine Translation Results with Paraphrase Support.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>77--84</pages>
<contexts>
<context position="10105" citStr="Zhou et al., 2006" startWordPosition="1642" endWordPosition="1645">e numerator for calculating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with h</context>
</contexts>
<marker>Zhou, Lin, Hovy, 2006</marker>
<rawString>Liang Zhou, Chon-Yew Lin, and Eduard Hovy. 2006. Re-evaluating Machine Translation Results with Paraphrase Support. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 77–84.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>