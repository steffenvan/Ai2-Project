<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043422">
<title confidence="0.874873666666667">
Towards Automatic Scoring of a Test of Spoken Language with
Heterogeneous Task Types
Klaus Zechner and Xiaoming Xi
</title>
<author confidence="0.840698">
Educational Testing Service
</author>
<affiliation confidence="0.894154">
Rosedale Road, Princeton, NJ 08541, USA
</affiliation>
<email confidence="0.996146">
{kzechner,xxi}@ets.org
</email>
<sectionHeader confidence="0.993837" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999366">
This paper describes a system aimed at auto-
matically scoring two task types of high and
medium-high linguistic entropy from a spoken
English test with a total of six widely differing
task types.
We describe the speech recognizer used for
this system and its acoustic model and lan-
guage model adaptation; the speech features
computed based on the recognition output;
and finally the scoring models based on mul-
tiple regression and classification trees.
For both tasks, agreement measures between
machine and human scores (correlation,
kappa) are close to or reach inter-human
agreements.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978482758621">
As demand for spoken language testing and cost of
human scoring have increased in recent years,
there is a growing interest in building both research
and industrial systems for automatically scoring
non-native speech (Bernstein, 1999, Zechner and
Bejar, 2006, Zechner et al, 2007).
However, past approaches have focused typi-
cally only on one type of spoken language, or on a
range of types similar in linguistic entropy. En-
tropy in this context can be seen as a measure for
how predictable the language in the expected spo-
ken response is: Some tests, such as SET-10 (Bern-
stein 1999), are focused mostly on the lower
entropy aspects of language, using tasks such as
“reading” or “repetition”, where the expected se-
quence of words is highly predictable. Other as-
sessments, such as the TOEFL® Practice Online
Speaking test, on the other hand, focus on more
spontaneous, high-entropy responses (Zechner et
al., 2007).
In this paper, we describe a spoken language test
with heterogeneous task types, ranging from read
speech to tasks that require candidates to give their
opinions on an issue, whose goal is to assess com-
municative competence (Bachman, 1990; Bach-
man &amp; Palmer, 1996); we call this test THT (Test
with Heterogeneous Tasks). Communicative com-
petence, in this context, refers to a speaker&apos;s ability
to use the language for communicative purposes.
The effectiveness of the communication typically
consists of a few aspects including comprehensibil-
ity, accuracy, clarity, coherence and appropriate-
ness, and is evident in a speaker&apos;s pronunciation,
fluency, use of grammar and vocabulary, develop-
ment of ideas, and sensitivity to the context of the
communication.
This test has the advantage of being able to as-
sess a wide range of non-native speakers’ profi-
ciencies by using tasks of varying difficulty levels
to allow even low proficiency speakers some de-
gree of success on easier task types.
We select two tasks from this test, one of higher
and one of medium to high entropy, and first adapt
a non-native English speech recognizer (trained on
TOEFL® Practice Online data) to transcribed THT
task responses, then compute a set of relevant
speech features based on the recognition output,
and finally build a scoring model using a subset of
these features to predict trained human rater scores.
In this paper, we will demonstrate that the ma-
chine-human score agreements on these two task
types come close to or even exceed the level of
inter-human agreement.
This paper is organized as follows: Section 2
discusses related work, Section 3 describes the test
and the challenges for automatic scoring involved,
Section 4 discusses the speech recognizer and the
acoustic and language model adaptations per-
</bodyText>
<page confidence="0.863138">
98
</page>
<note confidence="0.74187">
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 98–106,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999756833333333">
formed, and Section 5 describes the speech fea-
tures selected for use in the scoring model. In Sec-
tion 6, we report the construction of the scoring
model and its results, Section 7 contains a general
discussion and Section 8 concludes the paper with
a brief discussion of future research.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99995516">
There has been previous work to automatically
characterize aspects of communicative competence
such as fluency, pronunciation, and prosody.
Franco et al. (2000) present a system for automatic
evaluation of the pronunciation quality of both na-
tive and non-native speakers of English on a phone
level and a sentence level (EduSpeak). Candidates
read English texts and a forced alignment between
the speech signal and the ideal path through the
Hidden Markov Model (HMM) is computed. Next,
the log posterior probabilities for pronouncing a
certain phone at a certain position in the signal are
computed to achieve a local pronunciation score.
These scores are then combined with other auto-
matically derived measures such as the rate of
speech (number of words per second) or the dura-
tion of phonemes to yield global pronunciation
scores.
Cucchiarini et al. (1997a, 1997b) describe a sys-
tem for Dutch pronunciation scoring along similar
lines. Their feature set, however, is more extensive
and contains, in addition to log likelihood Hidden
Markov Model scores, various duration scores, and
information on pauses, word stress, syllable struc-
ture, and intonation. In an evaluation, correlations
between four human scores and five machine
scores range from 0.67 to 0.92.
Bernstein (1999) presents a test for spoken Eng-
lish (SET-10) that uses the following types of task-
s: reading, sentence repetition, sentence building,
opposites, short questions, and open-ended ques-
tions. All types except for the last are scored auto-
matically and a score is reported that can be
interpreted as an indicator of how native-like a
speaker’s speech is. In Bernstein et al. (2000), an
experiment is performed to investigate the per-
formance of the SET-10 test in predicting speak-
ers’ oral proficiency. It is shown that the SET-10
test scores can predict different levels on the Oral
Interaction Scale of the Council of Europe’s
Framework (North, 2000) for describing oral pro-
ficiency of second/foreign language speakers with
reasonable accuracy. This paper further reports on
studies done to correlate the SET-10 automated
scores with the human scores from two other tests
of oral English communication skills. Correlations
are found to be between 0.73 and 0.88.
Zechner and Bejar (2006) investigate the auto-
mated scoring of unrestricted, spontaneous speech
of non-native speakers. They focus on exploring a
number of different fluency features for the auto-
mated scoring of short (one minute) responses to
test questions in a TOEFL-related program. They
explore scoring models based on classification and
regression trees (CART) as well as support vector
machines (SVM). Their findings are that the SVM
models are more useful for a quantitative analysis,
whereas the CART models allow for a more trans-
parent summary of the patterns underlying the
data.
In this paper, we use CART to build the scoring
model for one task type. We also adopt multiple
regression for another task type which has the ad-
vantage of being more easily interpreted than, for
example, SVMs. Another major difference be-
tween previous work and the work reported in this
paper is that we use feature normalization and
transformation to obtain statistically more mean-
ingful input variables for the scoring model. In ad-
dition, we do not use the whole set of features in an
exploratory fashion. Instead, we have carefully
selected a subset of features that are both good pre-
dictors of human scores and maximize the repre-
sentation of the concept of communicative
competence.
</bodyText>
<sectionHeader confidence="0.999018" genericHeader="method">
3 The THT test
</sectionHeader>
<subsectionHeader confidence="0.9997275">
3.1 Task types and scoring rubrics of the THT
Speaking test
</subsectionHeader>
<bodyText confidence="0.9997915">
There are six task types in the THT Speaking test,
ranging from reading-aloud tasks to tasks that re-
quire short answers and tasks that require extended
responses of one minute. The rubrics differ in both
the dimensions of speaking skills measured and the
possible score points. (Rubrics are characteriza-
tions of candidates’ competence at given score lev-
els and are used by human raters to determine the
appropriate score for a response.) Below is a brief
description of the task types and the rubrics.
</bodyText>
<page confidence="0.997773">
99
</page>
<bodyText confidence="0.98774387012987">
Task type 1: Reading-aloud (Planning time: 45
seconds; Response time: 45 seconds; zero/very-
low entropy)
There are two read-aloud tasks. Each task requires
the test-taker to read a short paragraph of 40-60
words aloud. The reading materials include an-
nouncements, advertisements, introductions, etc.
These two tasks are rated analytically on pronun-
ciation and intonation and stress on a 3-point scale.
That is to say, two separate scores are given on
each task – one for pronunciation and one for into-
nation and stress.
Task type 2: Picture description (Planning time:
30 seconds; Response time: 45 seconds; me-
dium-high entropy)
This task requires the test-taker to describe a pic-
ture in as much detail as possible.
This task is rated holistically on the combined
impact of delivery (fluency, pronunciation etc.),
use of structures, vocabulary, content relevance
and fullness on a 3-point scale.
Task type 3: Open-ended short-answer ques-
tions (Planning time: none; Response time: 15-
30 seconds; low/low-medium entropy)
The test-taker responds, without preparation, to
three questions about familiar and accessible topics
that draw on immediate personal experience. The
first two questions each elicit a 15-second response
that covers one or two pieces of information re-
lated to the specified topic. The third question re-
quires a 30-second response that expresses an
opinion or gives an explanation related to the topic.
This task is rated holistically on the combined im-
pact of delivery, use of structures, vocabulary, and
task appropriateness on a 3-point scale.
Task type 4: Constrained short-answer ques-
tions (Planning time: none; Response time: 15-
30 seconds; low/low-medium entropy)
The test-taker responds to three questions about a
schedule/agenda that is provided in written form.
All the information needed to answer the questions
should be included on or easily inferred from the
schedule. The test-taker has 15 seconds to respond
to each of the first two questions. These questions
ask for specific information on the schedule or eas-
ily inferred information about the schedule. The
test-taker has 30 seconds to respond to the last
question which requires a summary of multiple
events or multiple pieces of information on the
schedule. This task is rated holistically on the
combined impact of delivery, use of structures,
vocabulary, task appropriateness and content accu-
racy on a 3-point scale.
Task type 5: Respond to a voice mail (Planning
time: 30 seconds; Response time: 60 seconds;
high entropy)
In this task, the test-taker listens to a voicemail that
describes a problem, question or situation and then
assumes a particular role (bank teller, office assis-
tant, etc.) to respond with a proposed solution or
answer. This task is rated holistically on the com-
bined impact of fluency, pronunciation, intonation
and stress, grammar, vocabulary, register, content
relevance, and cohesion and idea progression on a
5-point scale.
Task type 6: Opinion task (Planning time: 15
seconds; Response time: 60 seconds; high en-
tropy)
In this task, the test-taker is expected to state an
opinion or position on an issue that is familiar and
accessible and to express support for the opinion or
position with reasons, examples, arguments, etc.
This task is rated holistically on the combined im-
pact of fluency, pronunciation, intonation and
stress, grammar, vocabulary, content relevance,
and cohesion and idea progression on a 5-point
scale.
</bodyText>
<subsectionHeader confidence="0.9830045">
3.2 Challenges of the THT test design to auto-
matic scoring
</subsectionHeader>
<bodyText confidence="0.992935352941177">
1. Some of the tasks require responses that are ex-
pected to vary very little in vocabulary and content
across examinees (e.g., Reading-aloud and Con-
strained short-answer questions) whereas others
allow much more flexibility and variation in the
use of vocabulary and grammatical structure and
topical content (e.g. Respond to a voicemail and
Opinion task). The predictability of the expected
response will dictate what type of language model-
ing technique is preferable to optimize speech rec-
ognition results. Therefore, unlike in other systems
focusing either on high or low entropy speech
(e.g., Zechner and Bejar, 2006; Bernstein, 1999),
in which a single speech recognizer is employed, it
is anticipated that different types of speech recog-
nizers are needed to suit different THT task types.
This may increase both the amount of development
</bodyText>
<page confidence="0.95665">
100
</page>
<bodyText confidence="0.923304133333333">
work and the complexity in integrating different
types of recognizers into the real-time automated
scoring system.
2. Furthermore, the scoring criteria of these six
different task types are somewhat different. This
suggests that different scoring models may need to
be developed for different task types since the
relevant speech features to be included in the scor-
ing model for each task type may differ.
3. THT speaking tasks use two kinds of score
scales: 0-3 and 0-5. Classification techniques, such
as classification trees or cumulative logit models
(Agresti, 2002; Menard, 2001), may be more ap-
propriate for task types that use a 3-point scale.
Prediction techniques such as multiple regression
may be better suited for task types that are on a 5-
point scale. Training different types of scoring
models will certainly increase the complexity and
the amount of scoring model development and
evaluation work.
In summary, the complexity of the design
of the THT Speaking test is expected to have a ma-
jor impact on our efforts to develop an automated
scoring system. Given these challenges and the
research resources available, we decided on a strat-
egy of starting with high entropy task types and
proceeding to low entropy task types. For this pa-
per, we selected the high entropy Opinion task and
the medium-high entropy Picture tasks for system
development.
</bodyText>
<sectionHeader confidence="0.718867" genericHeader="method">
4 Adaptation of the speech recognizer
</sectionHeader>
<bodyText confidence="0.999979021276596">
For this work, we are using a state-of-the-art gen-
der-independent Hidden Markov Model speech
recognizer whose acoustic model was trained on
about 30 hours of non-native speech and whose
language model was built on several hundred hours
of both native and non-native speech. The non-
native data came from the TOEFL® Practice
Online system, a web-based practice program for
prospective takers of the Test Of English as a For-
eign Language (TOEFL) (Zechner et al., 2007).
This data is somewhat different from the THT, as
there are only high-entropy tasks in TOEFL Speak-
ing and as the speakers are generally more profi-
cient. Due to this difference, the baseline word
accuracy was fairly low (see Table 1).
Therefore, as a first step, we needed to adapt the
automatic speech recognition engine to the THT
speech data.
We had approximately 1,000 responses each
from the Picture and Opinion tasks transcribed. As
mentioned above, while the Opinion task responses
are generally more spontaneous, the Picture task
requires the candidate to accurately describe a pic-
ture and thus restricts the possible answer space
considerably. Still, there is more room for individ-
ual choice and variation in the vocabulary, gram-
mar and content produced than there is in the more
restricted low-medium and low entropy task types
in the THT Speaking test.
When using our baseline automatic speech rec-
ognition (ASR) engine without any adaptation to
the THT speech data, we only obtained word accu-
racies between 25% and 33%, which was clearly
inadequate, and far below a word accuracy where,
at least for some speakers, meaningful information
can be drawn from the ASR hypothesis.
Therefore, we undertook a series of adaptation
and optimization steps with the goal of maximizing
the word accuracy on the two task types for the
THT Speaking test. We first adapted the acoustic
model in batch mode with supervised maximum a-
posteriori (MAP) adaptation using the combined
data from both tasks, then the language model, op-
timized the filler cost parameter and finally con-
ducted unsupervised maximum likelihood linear
regression (MLLR) acoustic model adaptation
based on individual speakers.
</bodyText>
<subsectionHeader confidence="0.946822">
4.1 Acoustic model batch adaptation
</subsectionHeader>
<bodyText confidence="0.999916444444444">
We randomly selected about 90% of Picture and
Opinion task response data for acoustic model
(AM) adaptation, which contained 1,800 response
files (over 25 hours of speech, adult speakers with
typically low to intermediate English proficiency).
Results are always reported on the held-out evalua-
tion data containing 100 files for the Picture task
and 80 files for the Opinion task.
We performed supervised maximum a posteriori
(MAP) adaptation which is the method of choice
for larger amounts of data and is typically per-
formed in batch mode (Tomokiyo and Waibel,
2001; Wang et al., 2003). After one cycle of adap-
tation, word accuracy improved by about 8%, as is
shown in Table 1. We also performed unsupervised
maximum likelihood linear regression (MLLR)
adaptation, which is discussed in Section 4.4 be-
low.
</bodyText>
<page confidence="0.979307">
101
</page>
<table confidence="0.99995945">
Picture task word Opinion task word
accuracy accuracy
Method Absolute Increase Absolute Increase
from from
previous previous
step step
Baseline 25.8% NA 32.2% NA
recognizer
AM MAP 33.6% 7.8% 40.0% 7.8%
adaptation
LM adap- 50.4% 16.8% 51.0% 11.0%
tation
Filler 57.0% 6.6% 56.3% 5.3%
optimiza-
tion
Ignoring 60.5% 3.5% 59.2% 2.9%
fillers
MLLR 62.4% 1.9% 61.2% 2.0%
Speaker
adaptation
</table>
<tableCaption confidence="0.9512455">
Table 1. Word accuracies after each incremental
step of adaptation or optimization and performance
improvement within each step for Picture and Opin-
ion task types.
</tableCaption>
<subsectionHeader confidence="0.949779">
4.2 Language model adaptation
</subsectionHeader>
<bodyText confidence="0.999910230769231">
The second step was language model (LM) adapta-
tion. The Picture and Opinion tasks were adapted
separately using the same training sets as above.
We built interpolated models between the task-
specific LM and the baseline LM (from the origi-
nal recognizer).
We obtained the best results using only the task-
specific LM trained on the THT data set (given in
Table 1). This indicates that the domain of each of
the tasks is narrow enough that it can be suffi-
ciently described with a set of about 900 tran-
scribed examples each and it does not benefit from
a larger LM such as our baseline LM.
</bodyText>
<subsectionHeader confidence="0.995168">
4.3 Filler cost optimization
</subsectionHeader>
<bodyText confidence="0.999986930232558">
“Filler cost” is a recognizer-internal parameter that
determines the likelihood of filler and noise words
to be inserted into the hypothesis before or after
“real” words. The higher the parameter’s value, the
less likely fillers will be inserted.
The experiments with the filler cost parameter
grew out of an observation that the baseline recog-
nizer has a tendency to hypothesize too many
words when faced with different kinds of “uncer-
tain” audio, such as mumbled words, noises or fill-
ers. Therefore we conjectured that having the
recognizer hypothesize more filler and noise words
in these cases and be more restrictive with actual
word hypotheses might increase the word accuracy
overall.
We varied the filler cost parameter from its de-
fault, 3, down to its lowest meaningful value, 0.
Our experiments show that for fillercost=0, a
maximum word accuracy was achieved (given in
Table 1), albeit at the cost of more than doubling
the length of the recognizer’s hypothesis by intro-
ducing a large amount of fillers (such as “um” or
“uh”, noises, mumbles etc.). We observe that using
such a low filler cost parameter setting can nega-
tively affect some speech features which are can-
didates for being used in a scoring model, such as
“language model score”. Therefore we have to
carefully assess whether achieving a higher word
accuracy is more beneficial to the overall perform-
ance of the feature set or whether it has too many
negative effects on some important speech fea-
tures. In future work we will attempt to tune the
recognizer in such a way that it is not only opti-
mized for a high word accuracy, but also for high
accuracy in filler (and noise) prediction.
Word accuracy was computed with the fillers
included or excluded. Since fillers are not real
words, and in this round of scoring model devel-
opment we did not use any features based on fill-
ers, it was reasonable to compute the overall word
accuracy with the fillers removed from the human
and recognizer transcriptions, resulting in a moder-
ate performance gain (see Table 1).
</bodyText>
<subsectionHeader confidence="0.997003">
4.4 Unsupervised speaker adaptation
</subsectionHeader>
<bodyText confidence="0.999987866666667">
We used unsupervised maximum likelihood lin-
ear regression (MLLR) AM adaptation on top of
the previous adaptation and optimization steps (To-
mokiyo and Waibel, 2001; Wang et al., 2003). In
this step, all words whose confidence score was
higher than a pre-set threshold were collected and
their acoustic information was used to adapt the
acoustic model. All adaptations were done based
on the utterances of a single speaker and pertained
to that speaker only, i.e., it was not incremental or
cumulative. Since a second decoding run is needed
after the actual MLLR adaptations, the recog-
nizer’s response time more than doubles when this
method is employed. The unsupervised speaker
adaptation led to an additional increase of
</bodyText>
<page confidence="0.995047">
102
</page>
<table confidence="0.998345875">
Feature Feature Feature Description Used in
Number Name Class
1 hmmscore Pronuncia- Acoustic Model score: sum of the log probabilities of Opinion &amp; Picture
tion every frame, normalized for length
2 typesper- Fluency &amp; Number of unique words in response (“types”) di- Opinion &amp; Picture
second Vocabulary vided by length of response
diversity
3 silences- Fluency Number of silences per second Opinion &amp; Picture
persecond
4 repetitions Fluency Number of repetitions divided by number of words Opinion
5 relevance- Vocabulary Cosine word vector product between a response and Opinion
cos5 &amp; Content all responses in the training set that have the highest
score (5 for the Opinion task)
6 relevance- Vocabulary Cosine word vector product between a response and Picture
cos3 &amp; Content all responses in the training set that have the highest
score (3 for the Picture task)
</table>
<tableCaption confidence="0.999551">
Table 2. Final features used for the scoring models for the Opinion and Picture tasks
</tableCaption>
<bodyText confidence="0.9995144">
approximately 2% for the Picture and Opinion
tasks (see Table 1). There were large differences
between different speakers in terms of the per-
formance gain of MLLR adaptation on our data
set, however. There was also a large variation of
word accuracies between speakers (13-100%). The
variation in accuracy across speakers can be due to
many different factors, including the degree of ac-
cent, the grammaticality of the response, the voice
quality and the recording quality.
</bodyText>
<sectionHeader confidence="0.986993" genericHeader="method">
5 Speech features
</sectionHeader>
<bodyText confidence="0.999981772727273">
Based on the output of the ASR engine, a feature
computation module computes a set of about 40
features for each response, mostly in the fluency
domain (e.g. “average silence duration”), but also
some features related to pronunciation, vocabulary
diversity and content.
Instead of using all of these features in a scoring
model, we used a process of iterative refinement
and selection to narrow down the feature set, based
on both the coverage of the concept of communica-
tive competence and empirical performance (corre-
lations with human scores) of the features.
Following this process, five features were selected
to be included in developing the scoring models for
the Opinion task type and four for the Picture task
type (see Table 2).
When we look at the correlations of these fea-
tures to the human scores, we find that hmmscore,
after being transformed to improve normality, was
the strongest predictor of human scores for both
the Opinion and Picture tasks with typespersecond
as the second strongest (0.5 &lt;= Pearson r &lt;= 0.7).
</bodyText>
<sectionHeader confidence="0.964646" genericHeader="method">
6 Scoring models
</sectionHeader>
<bodyText confidence="0.9999265">
All the responses were double scored by a ran-
domly selected pair of raters who were trained for
scoring this test. The agreements between the two
ratings (both kappa and Pearson r correlation) were
around 0.50 for the Picture and 0.72 for the Opin-
ion task. (Note that the fewer points a scale has, the
lower correlation we can expect due to less score
variability, everything else being equal.)
While we use the same training sets for the scor-
ing model experiments as for the above ASR ex-
periments (sm-train), we add about 600 responses
each to the evaluation sets (these responses were
untranscribed) to yield a scoring model evaluation
set size of about 700 responses each (sm-eval).
Scoring models were developed and evaluated
for the Opinion and Picture task types separately.
The Opinion tasks are on a 0-5 point scale whereas
the Picture tasks are on a 0-3 point scale. There
were only a handful of 0s on each task and they
were excluded in building the scoring models.
For the Opinion tasks, multiple regression mod-
els employing different weights for the features
were developed, namely an Equal Weights model,
an Expert Weights model and an Optimal Weights
model. In the Equal Weights model, each feature
was assigned the same weight, indicating that all
features are equally important in the prediction. In
the Expert Weights model, different weights were
assigned to different features that reflected our un-
derstanding of the different roles features play in
indicating the overall speech quality. In the Opti-
mal Weights model, weights were determined by
</bodyText>
<page confidence="0.998638">
103
</page>
<bodyText confidence="0.99996544">
the least squares optimization procedure using the
sm-train data. All features were normalized to
have a mean of 0 and a standard deviation of 1,
such that their respective baseline influence on the
model is comparable across features.
For the Picture task type, CART was used to
predict the score class each response should be
assigned to. CART 5.0 (Steinberg &amp; Colla, 1997)
was used to build the classification trees.
In addition, generic and task-specific models
were developed for both task types. The task-
specific models made use of task-specific vocabu-
lary features (Features 5 and 6 in Table 2) which
required using previous response data to each of
the tasks within a particular task type. (Both task
types had 4 different tasks each). The generic
models, in contrast, used features that were the
same across all tasks for a particular task type and
did not use any task-specific vocabulary features.
As it would be much more time-consuming and
costly to build task-specific models, it is worth-
while to investigate how much more predictive
power the task-specific vocabulary features could
add over and beyond the features in the generic
models.
</bodyText>
<subsectionHeader confidence="0.997404">
6.1 Opinion task type
</subsectionHeader>
<bodyText confidence="0.999818545454545">
For the Opinion tasks, four features were used in
building the generic models and five in developing
the task-specific models. The following features
Were used: hmmscore, typespersecond, silences-
persecond, repetitions and relevancecos5 (the latter
only in the task-specific model).
Table 3 shows the results on the sm-eval set.
The Expert Weights model and the Optimal
Weights models yielded very similar results
(weighted kappa and correlation = 0.61-0.63) if we
look at predicted scores that were rounded to the
nearest integer. The agreements between regres-
sion model predicted scores and scores of human
rater 1 were just a little below the agreements be-
tween two human raters (weighted kappa and cor-
relation = 0.72). However, the results for the Equal
Weights model were inferior.
The results for the task-specific models showed
no improvement over the generic models, suggest-
ing that the task-specific vocabulary feature did not
contribute more predictive power beyond the four
features already in the generic models.
</bodyText>
<table confidence="0.999839230769231">
Model Multiple Regres- Multiple
sion Weights) Regres-
(Equal Multiple sion (Op-
Regression timal
(Expert Weights)
Weights)
Weighted κ 0.53 0.62 0.61
Pearson r 0.62 0.68 0.69
Correlation
(unrounded)
Pearson r 0.56 0.63 0.63
Correlation
(rounded)
</table>
<tableCaption confidence="0.985504">
Table 3. Performance of different weighting schemes
on THT scoring model evaluation set for Opinion
tasks (generic model)
</tableCaption>
<subsectionHeader confidence="0.995142">
6.2 Picture task type
</subsectionHeader>
<bodyText confidence="0.9999713">
As mentioned earlier, the Picture tasks are on a 0-3
point scale and we removed a small number of 0-
scores from the analyses, making it a 3-point scale.
Given this particular score scale, multiple regres-
sion may not be appropriate for this data as it re-
quires a continuous or a quasi-continuous
dependent variable (i.e. a variable that has at least
5 or more data points). Some classification tech-
niques such as CART (Brieman et al., 1984) or
logistic regression, which can take ordered score
categories as the outcome variable, are better
suited for this data. In this study, we analyzed the
data with CART models.
CART 5.0 (Steinberg and Colla, 1997) was used
to build the classification trees. We built two sets
of CART models, one set with the task-specific
vocabulary feature (relevancecos3) and one set
without it. We explored different model configura-
tions, i.e., different combinations of priors and
splitting rules. For each combination, a 10-fold
cross-validation was conducted. Subsequently, the
optimal sub tree that was a relatively small tree
with the highest or near-highest agreement with the
human scores (weighted kappa) on the cross-
validation sample was identified. Then the cases in
the sm-eval data set were dropped down the opti-
mal tree to obtain the evaluation results on the
held-out data.
The results for the generic model vs. task-
specific models are compared in Table 4. For both
</bodyText>
<page confidence="0.996931">
104
</page>
<bodyText confidence="0.999702285714286">
models, CART trees built using the Twoing1 split-
ting rule combined with mixed priors (average of
equal priors for different score classes and sm-train
sample priors) yielded the best kappa values on the
cross-validation data and were selected as the op-
timal trees. The agreements between the CART
model predicted scores and first rater scores
slightly exceeded that between two human raters
on the sm-eval data set. Another observation from
Table 4 was that for this task type, the task-specific
CART model did not demonstrate an advantage
over the generic model; actually, its performance
was slightly worse than that of the generic model, a
finding in line with the Opinion task.
</bodyText>
<table confidence="0.9979306">
Generic Task- Inter-human
specific agreement
Weighted κ 0.51 0.50 0.49
Pearson r 0.52 0.50 0.50
Correlation
</table>
<tableCaption confidence="0.991871">
Table 4. Performance of CART models on THT
scoring model evaluation set for Picture tasks (ge-
neric model vs. task-specific model)
</tableCaption>
<sectionHeader confidence="0.998595" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999790148148148">
This paper investigates the feasibility of develop-
ing an automatic scoring system for the THT
Speaking test, focusing on the particular challenges
posed by the design of the test. The main challenge
posed by the test design is the high variability in
task types -- ranging from low-entropy Reading-
aloud tasks to high-entropy Opinion tasks. While
previous tests of spoken language have focused
mainly on either high or low entropy tasks (Bern-
stein, 1999; Zechner and Bejar, 2006), we have
made an attempt at starting to address the whole
scale of entropy within a single test.
In this paper, we selected one high entropy task
(Opinion) and one medium-high entropy task (Pic-
ture) to start our explorations. While we found that
we could, for the most part, use a similar set of
features for both tasks, we had to address the dif-
ference in score scales between these two task
types. While we could use multiple regression for
scoring the 5-point-scale Opinion task, we had to
1 The Twoing rule divides the cases into two
groups, gathers similar classes together, and at-
tempts to separate the two groups in descendant
nodes.
employ CART trees for the 3-point-scale Picture
task, demonstrating that one can not necessarily
use one type of scoring model for all tasks.
When moving to low and low-medium entropy
tasks, we expect further adaptations, both in terms
of the feature set (e.g., the higher importance of
pronunciation features in Reading-aloud tasks),
and in speech recognition, where more restrictive
language models will be needed.
We have reported findings associated with the
performance of the scoring models for the Opinion
and Picture task types. Overall, the preliminary
findings are quite promising: with a few key
speech features, we were able to achieve prediction
accuracies that could almost emulate or slightly
exceed the agreements between two human raters
at task level. Once we have developed scoring
models for all task types, it is conceivable to ag-
gregate the task level scores to produce a total
summary score at the test level and it is very likely
we would see a much stronger association between
human scores and automated scores for the whole
test.
The findings also suggest that task-specific
modeling efforts did not seem to be necessary for
the two task types investigated. This does not pre-
clude the possibility, though, that task-specific
scoring models are superior for other task types in
which the expected content is much more restricted
(such as the Constrained short-answer questions).
</bodyText>
<sectionHeader confidence="0.988122" genericHeader="conclusions">
8 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9998535">
We have demonstrated that by using a three-stage
architecture of automatic speech recognition, fea-
ture computation, and scoring models, we are able
to achieve some degree of success in generating
automated scores for two task types of a spoken
language test with a wide variation in entropy in its
tasks. The agreement between machine scores and
human scores comes close to or reaches the inter-
human agreement levels for these two tasks.
In future work, we will switch our focus to task
types that elicit more constrained speech (such as
the Reading-aloud tasks and Constrained short-
answer questions). In the meantime, we will con-
tinue to refine and evaluate the preliminary scoring
models developed in this paper. In particular, we
will explore cumulative logit models for tasks that
are on a 0-3 point scale and compare the results to
those of CART models.
</bodyText>
<page confidence="0.998979">
105
</page>
<sectionHeader confidence="0.995896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999151661538461">
Agresti, A. (2002). Categorial data analysis (2nd
ed.). New York: Wiley.
Bachman, L.F. (1990). Fundamental Considera-
tions in Language Testing. New York: Oxford
University Press.
Bachman, L. F., and Palmer, A. S. (1996). Lan-
guage testing in practice. Ox-
ford:OxfordUniversity Press.
Bernstein, J. (1999). PhonePass testing: Structure
and construct. Menlo Park, CA: Ordinate Corpo-
ration.
Bernstein, J., DeJong, J., Pisoni, D., and Town-
shend, B. (2000). Two experiments in automatic
scoring of spoken language proficiency. In-
STILL2000, Dundee, Scotland.
Brieman, L., Jerome F., Olshen, R., and Stone, C.
(1984). Classification and Regression Trees. Pa-
cific Grove: Wadsworth.
Cucchiarini, C., Strik, H., &amp; Boves, L. (1997a).
Using speech recognition technology to assess
foreign speakers&apos; pronunciation of Dutch. Third
international symposium on the acquisition of
second language speech: NEW SOUNDS 97,
Klagenfurt, Austria.
Cucchiarini, C., Strik, S., and Boves, L. (1997b).
Automatic evaluation of Dutch pronunciation by
using speech recognition technology. IEEE
Automatic Speech Recognition and Understand-
ing Workshop, Santa Barbara, CA.
Franco, H., Abrash, V., Precoda, K., Bratt, H.,
Rao, R., and Butzberger, J. (2000). The SRI
EduSpeak system: Recognition and pronuncia-
tion scoring for language learning. InSTiLL-
2000 (Intelligent Speech Technology in Lan-
guage Learning), Dundee, Scotland.
Menard, S. (2001). Applied logistic regression
analysis. Sage University Paper Series on Quan-
titative Applications in the Social Sciences 07-
106, Thousand Oaks, CA: Sage.
North, B. (2000). The Development of a Common
Framework Scale of Language Proficiency.
New York, NY: Peter Lang.
Steinberg, D., and Colla, P. (1997). CART -- Clas-
sification and Regression Trees. San Diego, CA:
Salford Systems.
Tomokiyo, L. M., and Waibel, A. (2001). Adapta-
tion methods for non-native speech. Multilin-
guality in Spoken Language Processing,
Aalborg.
Wang, Z., Schultz, T., and Waibel, A. (2003).
Comparison of acoustic model adaptation tech-
niques on non-native speech. IEEE International
Conference on Acoustics, Speech, and Signal Proc-
essing (ICASSP-2003), Hong Kong, China.
Zechner, K., and Bejar, I. (2006). Towards Auto-
matic Scoring of Non-Native Spontaneous
Speech. HLT-NAACL-06, New York, NY.
Zechner, K., Higgins, D., and Xi, X. (2007).
SpeechRater®: A Construct-Driven Approach to
Score Spontaneous Non-Native Speech. Pro-
ceedings of the 2007 Workshop of the Interna-
tional Speech Communication Association
(ISCA) Special Interest Group on Speech and
Language Technology in Education (SLaTE),
Farmington, PA, October.
</reference>
<page confidence="0.997324">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875772">
<title confidence="0.9996725">Towards Automatic Scoring of a Test of Spoken Language with Heterogeneous Task Types</title>
<author confidence="0.999105">Zechner Xi</author>
<affiliation confidence="0.981474">Educational Testing Service</affiliation>
<address confidence="0.998078">Rosedale Road, Princeton, NJ 08541, USA</address>
<email confidence="0.995975">kzechner@ets.org</email>
<email confidence="0.995975">xxi@ets.org</email>
<abstract confidence="0.9931784375">This paper describes a system aimed at automatically scoring two task types of high and medium-high linguistic entropy from a spoken English test with a total of six widely differing task types. We describe the speech recognizer used for this system and its acoustic model and language model adaptation; the speech features computed based on the recognition output; and finally the scoring models based on multiple regression and classification trees. For both tasks, agreement measures between machine and human scores (correlation, kappa) are close to or reach inter-human agreements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Agresti</author>
</authors>
<title>Categorial data analysis (2nd ed.).</title>
<date>2002</date>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="13046" citStr="Agresti, 2002" startWordPosition="2074" endWordPosition="2075">increase both the amount of development 100 work and the complexity in integrating different types of recognizers into the real-time automated scoring system. 2. Furthermore, the scoring criteria of these six different task types are somewhat different. This suggests that different scoring models may need to be developed for different task types since the relevant speech features to be included in the scoring model for each task type may differ. 3. THT speaking tasks use two kinds of score scales: 0-3 and 0-5. Classification techniques, such as classification trees or cumulative logit models (Agresti, 2002; Menard, 2001), may be more appropriate for task types that use a 3-point scale. Prediction techniques such as multiple regression may be better suited for task types that are on a 5- point scale. Training different types of scoring models will certainly increase the complexity and the amount of scoring model development and evaluation work. In summary, the complexity of the design of the THT Speaking test is expected to have a major impact on our efforts to develop an automated scoring system. Given these challenges and the research resources available, we decided on a strategy of starting w</context>
</contexts>
<marker>Agresti, 2002</marker>
<rawString>Agresti, A. (2002). Categorial data analysis (2nd ed.). New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Bachman</author>
</authors>
<title>Fundamental Considerations in Language Testing.</title>
<date>1990</date>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="1969" citStr="Bachman, 1990" startWordPosition="310" endWordPosition="311">Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as “reading” or “repetition”, where the expected sequence of words is highly predictable. Other assessments, such as the TOEFL® Practice Online Speaking test, on the other hand, focus on more spontaneous, high-entropy responses (Zechner et al., 2007). In this paper, we describe a spoken language test with heterogeneous task types, ranging from read speech to tasks that require candidates to give their opinions on an issue, whose goal is to assess communicative competence (Bachman, 1990; Bachman &amp; Palmer, 1996); we call this test THT (Test with Heterogeneous Tasks). Communicative competence, in this context, refers to a speaker&apos;s ability to use the language for communicative purposes. The effectiveness of the communication typically consists of a few aspects including comprehensibility, accuracy, clarity, coherence and appropriateness, and is evident in a speaker&apos;s pronunciation, fluency, use of grammar and vocabulary, development of ideas, and sensitivity to the context of the communication. This test has the advantage of being able to assess a wide range of non-native spea</context>
</contexts>
<marker>Bachman, 1990</marker>
<rawString>Bachman, L.F. (1990). Fundamental Considerations in Language Testing. New York: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Bachman</author>
<author>A S Palmer</author>
</authors>
<title>Language testing in practice.</title>
<date>1996</date>
<publisher>Oxford:OxfordUniversity Press.</publisher>
<contexts>
<context position="1994" citStr="Bachman &amp; Palmer, 1996" startWordPosition="312" endWordPosition="316">h as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as “reading” or “repetition”, where the expected sequence of words is highly predictable. Other assessments, such as the TOEFL® Practice Online Speaking test, on the other hand, focus on more spontaneous, high-entropy responses (Zechner et al., 2007). In this paper, we describe a spoken language test with heterogeneous task types, ranging from read speech to tasks that require candidates to give their opinions on an issue, whose goal is to assess communicative competence (Bachman, 1990; Bachman &amp; Palmer, 1996); we call this test THT (Test with Heterogeneous Tasks). Communicative competence, in this context, refers to a speaker&apos;s ability to use the language for communicative purposes. The effectiveness of the communication typically consists of a few aspects including comprehensibility, accuracy, clarity, coherence and appropriateness, and is evident in a speaker&apos;s pronunciation, fluency, use of grammar and vocabulary, development of ideas, and sensitivity to the context of the communication. This test has the advantage of being able to assess a wide range of non-native speakers’ proficiencies by us</context>
</contexts>
<marker>Bachman, Palmer, 1996</marker>
<rawString>Bachman, L. F., and Palmer, A. S. (1996). Language testing in practice. Oxford:OxfordUniversity Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
</authors>
<title>PhonePass testing: Structure and construct. Menlo Park, CA: Ordinate Corporation.</title>
<date>1999</date>
<contexts>
<context position="1050" citStr="Bernstein, 1999" startWordPosition="156" endWordPosition="157">cognizer used for this system and its acoustic model and language model adaptation; the speech features computed based on the recognition output; and finally the scoring models based on multiple regression and classification trees. For both tasks, agreement measures between machine and human scores (correlation, kappa) are close to or reach inter-human agreements. 1 Introduction As demand for spoken language testing and cost of human scoring have increased in recent years, there is a growing interest in building both research and industrial systems for automatically scoring non-native speech (Bernstein, 1999, Zechner and Bejar, 2006, Zechner et al, 2007). However, past approaches have focused typically only on one type of spoken language, or on a range of types similar in linguistic entropy. Entropy in this context can be seen as a measure for how predictable the language in the expected spoken response is: Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as “reading” or “repetition”, where the expected sequence of words is highly predictable. Other assessments, such as the TOEFL® Practice Online Speaking test, on the other</context>
<context position="5310" citStr="Bernstein (1999)" startWordPosition="840" endWordPosition="841">bined with other automatically derived measures such as the rate of speech (number of words per second) or the duration of phonemes to yield global pronunciation scores. Cucchiarini et al. (1997a, 1997b) describe a system for Dutch pronunciation scoring along similar lines. Their feature set, however, is more extensive and contains, in addition to log likelihood Hidden Markov Model scores, various duration scores, and information on pauses, word stress, syllable structure, and intonation. In an evaluation, correlations between four human scores and five machine scores range from 0.67 to 0.92. Bernstein (1999) presents a test for spoken English (SET-10) that uses the following types of tasks: reading, sentence repetition, sentence building, opposites, short questions, and open-ended questions. All types except for the last are scored automatically and a score is reported that can be interpreted as an indicator of how native-like a speaker’s speech is. In Bernstein et al. (2000), an experiment is performed to investigate the performance of the SET-10 test in predicting speakers’ oral proficiency. It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of th</context>
<context position="12267" citStr="Bernstein, 1999" startWordPosition="1950" endWordPosition="1951">equire responses that are expected to vary very little in vocabulary and content across examinees (e.g., Reading-aloud and Constrained short-answer questions) whereas others allow much more flexibility and variation in the use of vocabulary and grammatical structure and topical content (e.g. Respond to a voicemail and Opinion task). The predictability of the expected response will dictate what type of language modeling technique is preferable to optimize speech recognition results. Therefore, unlike in other systems focusing either on high or low entropy speech (e.g., Zechner and Bejar, 2006; Bernstein, 1999), in which a single speech recognizer is employed, it is anticipated that different types of speech recognizers are needed to suit different THT task types. This may increase both the amount of development 100 work and the complexity in integrating different types of recognizers into the real-time automated scoring system. 2. Furthermore, the scoring criteria of these six different task types are somewhat different. This suggests that different scoring models may need to be developed for different task types since the relevant speech features to be included in the scoring model for each task t</context>
<context position="30314" citStr="Bernstein, 1999" startWordPosition="4900" endWordPosition="4902">50 0.50 Correlation Table 4. Performance of CART models on THT scoring model evaluation set for Picture tasks (generic model vs. task-specific model) 7 Discussion This paper investigates the feasibility of developing an automatic scoring system for the THT Speaking test, focusing on the particular challenges posed by the design of the test. The main challenge posed by the test design is the high variability in task types -- ranging from low-entropy Readingaloud tasks to high-entropy Opinion tasks. While previous tests of spoken language have focused mainly on either high or low entropy tasks (Bernstein, 1999; Zechner and Bejar, 2006), we have made an attempt at starting to address the whole scale of entropy within a single test. In this paper, we selected one high entropy task (Opinion) and one medium-high entropy task (Picture) to start our explorations. While we found that we could, for the most part, use a similar set of features for both tasks, we had to address the difference in score scales between these two task types. While we could use multiple regression for scoring the 5-point-scale Opinion task, we had to 1 The Twoing rule divides the cases into two groups, gathers similar classes tog</context>
</contexts>
<marker>Bernstein, 1999</marker>
<rawString>Bernstein, J. (1999). PhonePass testing: Structure and construct. Menlo Park, CA: Ordinate Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
<author>J DeJong</author>
<author>D Pisoni</author>
<author>B Townshend</author>
</authors>
<title>Two experiments in automatic scoring of spoken language proficiency.</title>
<date>2000</date>
<location>InSTILL2000, Dundee, Scotland.</location>
<contexts>
<context position="5685" citStr="Bernstein et al. (2000)" startWordPosition="900" endWordPosition="903">den Markov Model scores, various duration scores, and information on pauses, word stress, syllable structure, and intonation. In an evaluation, correlations between four human scores and five machine scores range from 0.67 to 0.92. Bernstein (1999) presents a test for spoken English (SET-10) that uses the following types of tasks: reading, sentence repetition, sentence building, opposites, short questions, and open-ended questions. All types except for the last are scored automatically and a score is reported that can be interpreted as an indicator of how native-like a speaker’s speech is. In Bernstein et al. (2000), an experiment is performed to investigate the performance of the SET-10 test in predicting speakers’ oral proficiency. It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of the Council of Europe’s Framework (North, 2000) for describing oral proficiency of second/foreign language speakers with reasonable accuracy. This paper further reports on studies done to correlate the SET-10 automated scores with the human scores from two other tests of oral English communication skills. Correlations are found to be between 0.73 and 0.88. Zechner and Bejar </context>
</contexts>
<marker>Bernstein, DeJong, Pisoni, Townshend, 2000</marker>
<rawString>Bernstein, J., DeJong, J., Pisoni, D., and Townshend, B. (2000). Two experiments in automatic scoring of spoken language proficiency. InSTILL2000, Dundee, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Brieman</author>
<author>F Jerome</author>
<author>R Olshen</author>
<author>C Stone</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth.</publisher>
<location>Pacific Grove:</location>
<contexts>
<context position="27958" citStr="Brieman et al., 1984" startWordPosition="4519" endWordPosition="4522">0.56 0.63 0.63 Correlation (rounded) Table 3. Performance of different weighting schemes on THT scoring model evaluation set for Opinion tasks (generic model) 6.2 Picture task type As mentioned earlier, the Picture tasks are on a 0-3 point scale and we removed a small number of 0- scores from the analyses, making it a 3-point scale. Given this particular score scale, multiple regression may not be appropriate for this data as it requires a continuous or a quasi-continuous dependent variable (i.e. a variable that has at least 5 or more data points). Some classification techniques such as CART (Brieman et al., 1984) or logistic regression, which can take ordered score categories as the outcome variable, are better suited for this data. In this study, we analyzed the data with CART models. CART 5.0 (Steinberg and Colla, 1997) was used to build the classification trees. We built two sets of CART models, one set with the task-specific vocabulary feature (relevancecos3) and one set without it. We explored different model configurations, i.e., different combinations of priors and splitting rules. For each combination, a 10-fold cross-validation was conducted. Subsequently, the optimal sub tree that was a rela</context>
</contexts>
<marker>Brieman, Jerome, Olshen, Stone, 1984</marker>
<rawString>Brieman, L., Jerome F., Olshen, R., and Stone, C. (1984). Classification and Regression Trees. Pacific Grove: Wadsworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cucchiarini</author>
<author>H Strik</author>
<author>L Boves</author>
</authors>
<title>Using speech recognition technology to assess foreign speakers&apos; pronunciation of Dutch. Third international symposium on the acquisition of second language speech: NEW SOUNDS 97,</title>
<date>1997</date>
<location>Klagenfurt, Austria.</location>
<contexts>
<context position="4888" citStr="Cucchiarini et al. (1997" startWordPosition="775" endWordPosition="778">both native and non-native speakers of English on a phone level and a sentence level (EduSpeak). Candidates read English texts and a forced alignment between the speech signal and the ideal path through the Hidden Markov Model (HMM) is computed. Next, the log posterior probabilities for pronouncing a certain phone at a certain position in the signal are computed to achieve a local pronunciation score. These scores are then combined with other automatically derived measures such as the rate of speech (number of words per second) or the duration of phonemes to yield global pronunciation scores. Cucchiarini et al. (1997a, 1997b) describe a system for Dutch pronunciation scoring along similar lines. Their feature set, however, is more extensive and contains, in addition to log likelihood Hidden Markov Model scores, various duration scores, and information on pauses, word stress, syllable structure, and intonation. In an evaluation, correlations between four human scores and five machine scores range from 0.67 to 0.92. Bernstein (1999) presents a test for spoken English (SET-10) that uses the following types of tasks: reading, sentence repetition, sentence building, opposites, short questions, and open-ended q</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 1997</marker>
<rawString>Cucchiarini, C., Strik, H., &amp; Boves, L. (1997a). Using speech recognition technology to assess foreign speakers&apos; pronunciation of Dutch. Third international symposium on the acquisition of second language speech: NEW SOUNDS 97, Klagenfurt, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cucchiarini</author>
<author>S Strik</author>
<author>L Boves</author>
</authors>
<title>Automatic evaluation of Dutch pronunciation by using speech recognition technology. IEEE Automatic Speech Recognition and Understanding Workshop,</title>
<date>1997</date>
<location>Santa Barbara, CA.</location>
<contexts>
<context position="4888" citStr="Cucchiarini et al. (1997" startWordPosition="775" endWordPosition="778">both native and non-native speakers of English on a phone level and a sentence level (EduSpeak). Candidates read English texts and a forced alignment between the speech signal and the ideal path through the Hidden Markov Model (HMM) is computed. Next, the log posterior probabilities for pronouncing a certain phone at a certain position in the signal are computed to achieve a local pronunciation score. These scores are then combined with other automatically derived measures such as the rate of speech (number of words per second) or the duration of phonemes to yield global pronunciation scores. Cucchiarini et al. (1997a, 1997b) describe a system for Dutch pronunciation scoring along similar lines. Their feature set, however, is more extensive and contains, in addition to log likelihood Hidden Markov Model scores, various duration scores, and information on pauses, word stress, syllable structure, and intonation. In an evaluation, correlations between four human scores and five machine scores range from 0.67 to 0.92. Bernstein (1999) presents a test for spoken English (SET-10) that uses the following types of tasks: reading, sentence repetition, sentence building, opposites, short questions, and open-ended q</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 1997</marker>
<rawString>Cucchiarini, C., Strik, S., and Boves, L. (1997b). Automatic evaluation of Dutch pronunciation by using speech recognition technology. IEEE Automatic Speech Recognition and Understanding Workshop, Santa Barbara, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Franco</author>
<author>V Abrash</author>
<author>K Precoda</author>
<author>H Bratt</author>
<author>R Rao</author>
<author>J Butzberger</author>
</authors>
<title>The SRI EduSpeak system: Recognition and pronunciation scoring for language learning.</title>
<date>2000</date>
<booktitle>InSTiLL2000 (Intelligent Speech Technology in Language Learning),</booktitle>
<location>Dundee, Scotland.</location>
<contexts>
<context position="4189" citStr="Franco et al. (2000)" startWordPosition="661" endWordPosition="664"> Innovative Use of NLP for Building Educational Applications, pages 98–106, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics formed, and Section 5 describes the speech features selected for use in the scoring model. In Section 6, we report the construction of the scoring model and its results, Section 7 contains a general discussion and Section 8 concludes the paper with a brief discussion of future research. 2 Related work There has been previous work to automatically characterize aspects of communicative competence such as fluency, pronunciation, and prosody. Franco et al. (2000) present a system for automatic evaluation of the pronunciation quality of both native and non-native speakers of English on a phone level and a sentence level (EduSpeak). Candidates read English texts and a forced alignment between the speech signal and the ideal path through the Hidden Markov Model (HMM) is computed. Next, the log posterior probabilities for pronouncing a certain phone at a certain position in the signal are computed to achieve a local pronunciation score. These scores are then combined with other automatically derived measures such as the rate of speech (number of words per</context>
</contexts>
<marker>Franco, Abrash, Precoda, Bratt, Rao, Butzberger, 2000</marker>
<rawString>Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R., and Butzberger, J. (2000). The SRI EduSpeak system: Recognition and pronunciation scoring for language learning. InSTiLL2000 (Intelligent Speech Technology in Language Learning), Dundee, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Menard</author>
</authors>
<title>Applied logistic regression analysis.</title>
<date>2001</date>
<booktitle>Sage University Paper Series on Quantitative Applications in the Social Sciences 07-106,</booktitle>
<publisher>Sage.</publisher>
<location>Thousand Oaks, CA:</location>
<contexts>
<context position="13061" citStr="Menard, 2001" startWordPosition="2076" endWordPosition="2077">he amount of development 100 work and the complexity in integrating different types of recognizers into the real-time automated scoring system. 2. Furthermore, the scoring criteria of these six different task types are somewhat different. This suggests that different scoring models may need to be developed for different task types since the relevant speech features to be included in the scoring model for each task type may differ. 3. THT speaking tasks use two kinds of score scales: 0-3 and 0-5. Classification techniques, such as classification trees or cumulative logit models (Agresti, 2002; Menard, 2001), may be more appropriate for task types that use a 3-point scale. Prediction techniques such as multiple regression may be better suited for task types that are on a 5- point scale. Training different types of scoring models will certainly increase the complexity and the amount of scoring model development and evaluation work. In summary, the complexity of the design of the THT Speaking test is expected to have a major impact on our efforts to develop an automated scoring system. Given these challenges and the research resources available, we decided on a strategy of starting with high entrop</context>
</contexts>
<marker>Menard, 2001</marker>
<rawString>Menard, S. (2001). Applied logistic regression analysis. Sage University Paper Series on Quantitative Applications in the Social Sciences 07-106, Thousand Oaks, CA: Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B North</author>
</authors>
<title>The Development of a Common Framework Scale of Language Proficiency.</title>
<date>2000</date>
<location>New York, NY: Peter Lang.</location>
<contexts>
<context position="5955" citStr="North, 2000" startWordPosition="946" endWordPosition="947">sh (SET-10) that uses the following types of tasks: reading, sentence repetition, sentence building, opposites, short questions, and open-ended questions. All types except for the last are scored automatically and a score is reported that can be interpreted as an indicator of how native-like a speaker’s speech is. In Bernstein et al. (2000), an experiment is performed to investigate the performance of the SET-10 test in predicting speakers’ oral proficiency. It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of the Council of Europe’s Framework (North, 2000) for describing oral proficiency of second/foreign language speakers with reasonable accuracy. This paper further reports on studies done to correlate the SET-10 automated scores with the human scores from two other tests of oral English communication skills. Correlations are found to be between 0.73 and 0.88. Zechner and Bejar (2006) investigate the automated scoring of unrestricted, spontaneous speech of non-native speakers. They focus on exploring a number of different fluency features for the automated scoring of short (one minute) responses to test questions in a TOEFL-related program. Th</context>
</contexts>
<marker>North, 2000</marker>
<rawString>North, B. (2000). The Development of a Common Framework Scale of Language Proficiency. New York, NY: Peter Lang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Steinberg</author>
<author>P Colla</author>
</authors>
<title>CART -- Classification and Regression Trees.</title>
<date>1997</date>
<location>San Diego, CA: Salford Systems.</location>
<contexts>
<context position="28171" citStr="Steinberg and Colla, 1997" startWordPosition="4554" endWordPosition="4557">icture tasks are on a 0-3 point scale and we removed a small number of 0- scores from the analyses, making it a 3-point scale. Given this particular score scale, multiple regression may not be appropriate for this data as it requires a continuous or a quasi-continuous dependent variable (i.e. a variable that has at least 5 or more data points). Some classification techniques such as CART (Brieman et al., 1984) or logistic regression, which can take ordered score categories as the outcome variable, are better suited for this data. In this study, we analyzed the data with CART models. CART 5.0 (Steinberg and Colla, 1997) was used to build the classification trees. We built two sets of CART models, one set with the task-specific vocabulary feature (relevancecos3) and one set without it. We explored different model configurations, i.e., different combinations of priors and splitting rules. For each combination, a 10-fold cross-validation was conducted. Subsequently, the optimal sub tree that was a relatively small tree with the highest or near-highest agreement with the human scores (weighted kappa) on the crossvalidation sample was identified. Then the cases in the sm-eval data set were dropped down the optima</context>
<context position="25308" citStr="Steinberg &amp; Colla, 1997" startWordPosition="4095" endWordPosition="4098">Expert Weights model, different weights were assigned to different features that reflected our understanding of the different roles features play in indicating the overall speech quality. In the Optimal Weights model, weights were determined by 103 the least squares optimization procedure using the sm-train data. All features were normalized to have a mean of 0 and a standard deviation of 1, such that their respective baseline influence on the model is comparable across features. For the Picture task type, CART was used to predict the score class each response should be assigned to. CART 5.0 (Steinberg &amp; Colla, 1997) was used to build the classification trees. In addition, generic and task-specific models were developed for both task types. The taskspecific models made use of task-specific vocabulary features (Features 5 and 6 in Table 2) which required using previous response data to each of the tasks within a particular task type. (Both task types had 4 different tasks each). The generic models, in contrast, used features that were the same across all tasks for a particular task type and did not use any task-specific vocabulary features. As it would be much more time-consuming and costly to build task-s</context>
</contexts>
<marker>Steinberg, Colla, 1997</marker>
<rawString>Steinberg, D., and Colla, P. (1997). CART -- Classification and Regression Trees. San Diego, CA: Salford Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Tomokiyo</author>
<author>A Waibel</author>
</authors>
<title>Adaptation methods for non-native speech.</title>
<date>2001</date>
<booktitle>Multilinguality in Spoken Language Processing,</booktitle>
<location>Aalborg.</location>
<contexts>
<context position="16638" citStr="Tomokiyo and Waibel, 2001" startWordPosition="2656" endWordPosition="2659"> individual speakers. 4.1 Acoustic model batch adaptation We randomly selected about 90% of Picture and Opinion task response data for acoustic model (AM) adaptation, which contained 1,800 response files (over 25 hours of speech, adult speakers with typically low to intermediate English proficiency). Results are always reported on the held-out evaluation data containing 100 files for the Picture task and 80 files for the Opinion task. We performed supervised maximum a posteriori (MAP) adaptation which is the method of choice for larger amounts of data and is typically performed in batch mode (Tomokiyo and Waibel, 2001; Wang et al., 2003). After one cycle of adaptation, word accuracy improved by about 8%, as is shown in Table 1. We also performed unsupervised maximum likelihood linear regression (MLLR) adaptation, which is discussed in Section 4.4 below. 101 Picture task word Opinion task word accuracy accuracy Method Absolute Increase Absolute Increase from from previous previous step step Baseline 25.8% NA 32.2% NA recognizer AM MAP 33.6% 7.8% 40.0% 7.8% adaptation LM adap- 50.4% 16.8% 51.0% 11.0% tation Filler 57.0% 6.6% 56.3% 5.3% optimization Ignoring 60.5% 3.5% 59.2% 2.9% fillers MLLR 62.4% 1.9% 61.2%</context>
<context position="20306" citStr="Tomokiyo and Waibel, 2001" startWordPosition="3270" endWordPosition="3274">for high accuracy in filler (and noise) prediction. Word accuracy was computed with the fillers included or excluded. Since fillers are not real words, and in this round of scoring model development we did not use any features based on fillers, it was reasonable to compute the overall word accuracy with the fillers removed from the human and recognizer transcriptions, resulting in a moderate performance gain (see Table 1). 4.4 Unsupervised speaker adaptation We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003). In this step, all words whose confidence score was higher than a pre-set threshold were collected and their acoustic information was used to adapt the acoustic model. All adaptations were done based on the utterances of a single speaker and pertained to that speaker only, i.e., it was not incremental or cumulative. Since a second decoding run is needed after the actual MLLR adaptations, the recognizer’s response time more than doubles when this method is employed. The unsupervised speaker adaptation led to an additional increase of 102 Feature Feature Feature Description </context>
</contexts>
<marker>Tomokiyo, Waibel, 2001</marker>
<rawString>Tomokiyo, L. M., and Waibel, A. (2001). Adaptation methods for non-native speech. Multilinguality in Spoken Language Processing, Aalborg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wang</author>
<author>T Schultz</author>
<author>A Waibel</author>
</authors>
<title>Comparison of acoustic model adaptation techniques on non-native speech.</title>
<date>2003</date>
<booktitle>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2003),</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="16658" citStr="Wang et al., 2003" startWordPosition="2660" endWordPosition="2663">coustic model batch adaptation We randomly selected about 90% of Picture and Opinion task response data for acoustic model (AM) adaptation, which contained 1,800 response files (over 25 hours of speech, adult speakers with typically low to intermediate English proficiency). Results are always reported on the held-out evaluation data containing 100 files for the Picture task and 80 files for the Opinion task. We performed supervised maximum a posteriori (MAP) adaptation which is the method of choice for larger amounts of data and is typically performed in batch mode (Tomokiyo and Waibel, 2001; Wang et al., 2003). After one cycle of adaptation, word accuracy improved by about 8%, as is shown in Table 1. We also performed unsupervised maximum likelihood linear regression (MLLR) adaptation, which is discussed in Section 4.4 below. 101 Picture task word Opinion task word accuracy accuracy Method Absolute Increase Absolute Increase from from previous previous step step Baseline 25.8% NA 32.2% NA recognizer AM MAP 33.6% 7.8% 40.0% 7.8% adaptation LM adap- 50.4% 16.8% 51.0% 11.0% tation Filler 57.0% 6.6% 56.3% 5.3% optimization Ignoring 60.5% 3.5% 59.2% 2.9% fillers MLLR 62.4% 1.9% 61.2% 2.0% Speaker adapta</context>
<context position="20326" citStr="Wang et al., 2003" startWordPosition="3275" endWordPosition="3278"> (and noise) prediction. Word accuracy was computed with the fillers included or excluded. Since fillers are not real words, and in this round of scoring model development we did not use any features based on fillers, it was reasonable to compute the overall word accuracy with the fillers removed from the human and recognizer transcriptions, resulting in a moderate performance gain (see Table 1). 4.4 Unsupervised speaker adaptation We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003). In this step, all words whose confidence score was higher than a pre-set threshold were collected and their acoustic information was used to adapt the acoustic model. All adaptations were done based on the utterances of a single speaker and pertained to that speaker only, i.e., it was not incremental or cumulative. Since a second decoding run is needed after the actual MLLR adaptations, the recognizer’s response time more than doubles when this method is employed. The unsupervised speaker adaptation led to an additional increase of 102 Feature Feature Feature Description Used in Number Name </context>
</contexts>
<marker>Wang, Schultz, Waibel, 2003</marker>
<rawString>Wang, Z., Schultz, T., and Waibel, A. (2003). Comparison of acoustic model adaptation techniques on non-native speech. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2003), Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>I Bejar</author>
</authors>
<title>Towards Automatic Scoring of Non-Native Spontaneous Speech. HLT-NAACL-06,</title>
<date>2006</date>
<location>New York, NY.</location>
<contexts>
<context position="1075" citStr="Zechner and Bejar, 2006" startWordPosition="158" endWordPosition="161"> this system and its acoustic model and language model adaptation; the speech features computed based on the recognition output; and finally the scoring models based on multiple regression and classification trees. For both tasks, agreement measures between machine and human scores (correlation, kappa) are close to or reach inter-human agreements. 1 Introduction As demand for spoken language testing and cost of human scoring have increased in recent years, there is a growing interest in building both research and industrial systems for automatically scoring non-native speech (Bernstein, 1999, Zechner and Bejar, 2006, Zechner et al, 2007). However, past approaches have focused typically only on one type of spoken language, or on a range of types similar in linguistic entropy. Entropy in this context can be seen as a measure for how predictable the language in the expected spoken response is: Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as “reading” or “repetition”, where the expected sequence of words is highly predictable. Other assessments, such as the TOEFL® Practice Online Speaking test, on the other hand, focus on more spon</context>
<context position="6291" citStr="Zechner and Bejar (2006)" startWordPosition="995" endWordPosition="998">tein et al. (2000), an experiment is performed to investigate the performance of the SET-10 test in predicting speakers’ oral proficiency. It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of the Council of Europe’s Framework (North, 2000) for describing oral proficiency of second/foreign language speakers with reasonable accuracy. This paper further reports on studies done to correlate the SET-10 automated scores with the human scores from two other tests of oral English communication skills. Correlations are found to be between 0.73 and 0.88. Zechner and Bejar (2006) investigate the automated scoring of unrestricted, spontaneous speech of non-native speakers. They focus on exploring a number of different fluency features for the automated scoring of short (one minute) responses to test questions in a TOEFL-related program. They explore scoring models based on classification and regression trees (CART) as well as support vector machines (SVM). Their findings are that the SVM models are more useful for a quantitative analysis, whereas the CART models allow for a more transparent summary of the patterns underlying the data. In this paper, we use CART to buil</context>
<context position="12249" citStr="Zechner and Bejar, 2006" startWordPosition="1946" endWordPosition="1949">ng 1. Some of the tasks require responses that are expected to vary very little in vocabulary and content across examinees (e.g., Reading-aloud and Constrained short-answer questions) whereas others allow much more flexibility and variation in the use of vocabulary and grammatical structure and topical content (e.g. Respond to a voicemail and Opinion task). The predictability of the expected response will dictate what type of language modeling technique is preferable to optimize speech recognition results. Therefore, unlike in other systems focusing either on high or low entropy speech (e.g., Zechner and Bejar, 2006; Bernstein, 1999), in which a single speech recognizer is employed, it is anticipated that different types of speech recognizers are needed to suit different THT task types. This may increase both the amount of development 100 work and the complexity in integrating different types of recognizers into the real-time automated scoring system. 2. Furthermore, the scoring criteria of these six different task types are somewhat different. This suggests that different scoring models may need to be developed for different task types since the relevant speech features to be included in the scoring mod</context>
<context position="30340" citStr="Zechner and Bejar, 2006" startWordPosition="4903" endWordPosition="4906">on Table 4. Performance of CART models on THT scoring model evaluation set for Picture tasks (generic model vs. task-specific model) 7 Discussion This paper investigates the feasibility of developing an automatic scoring system for the THT Speaking test, focusing on the particular challenges posed by the design of the test. The main challenge posed by the test design is the high variability in task types -- ranging from low-entropy Readingaloud tasks to high-entropy Opinion tasks. While previous tests of spoken language have focused mainly on either high or low entropy tasks (Bernstein, 1999; Zechner and Bejar, 2006), we have made an attempt at starting to address the whole scale of entropy within a single test. In this paper, we selected one high entropy task (Opinion) and one medium-high entropy task (Picture) to start our explorations. While we found that we could, for the most part, use a similar set of features for both tasks, we had to address the difference in score scales between these two task types. While we could use multiple regression for scoring the 5-point-scale Opinion task, we had to 1 The Twoing rule divides the cases into two groups, gathers similar classes together, and attempts to sep</context>
</contexts>
<marker>Zechner, Bejar, 2006</marker>
<rawString>Zechner, K., and Bejar, I. (2006). Towards Automatic Scoring of Non-Native Spontaneous Speech. HLT-NAACL-06, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>D Higgins</author>
<author>X Xi</author>
</authors>
<title>SpeechRater®: A Construct-Driven Approach to Score Spontaneous Non-Native Speech.</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 Workshop of the International Speech Communication Association (ISCA) Special Interest Group on Speech and Language Technology in Education (SLaTE),</booktitle>
<location>Farmington, PA,</location>
<contexts>
<context position="1097" citStr="Zechner et al, 2007" startWordPosition="162" endWordPosition="165">stic model and language model adaptation; the speech features computed based on the recognition output; and finally the scoring models based on multiple regression and classification trees. For both tasks, agreement measures between machine and human scores (correlation, kappa) are close to or reach inter-human agreements. 1 Introduction As demand for spoken language testing and cost of human scoring have increased in recent years, there is a growing interest in building both research and industrial systems for automatically scoring non-native speech (Bernstein, 1999, Zechner and Bejar, 2006, Zechner et al, 2007). However, past approaches have focused typically only on one type of spoken language, or on a range of types similar in linguistic entropy. Entropy in this context can be seen as a measure for how predictable the language in the expected spoken response is: Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as “reading” or “repetition”, where the expected sequence of words is highly predictable. Other assessments, such as the TOEFL® Practice Online Speaking test, on the other hand, focus on more spontaneous, high-entropy </context>
<context position="14343" citStr="Zechner et al., 2007" startWordPosition="2287" endWordPosition="2290">is paper, we selected the high entropy Opinion task and the medium-high entropy Picture tasks for system development. 4 Adaptation of the speech recognizer For this work, we are using a state-of-the-art gender-independent Hidden Markov Model speech recognizer whose acoustic model was trained on about 30 hours of non-native speech and whose language model was built on several hundred hours of both native and non-native speech. The nonnative data came from the TOEFL® Practice Online system, a web-based practice program for prospective takers of the Test Of English as a Foreign Language (TOEFL) (Zechner et al., 2007). This data is somewhat different from the THT, as there are only high-entropy tasks in TOEFL Speaking and as the speakers are generally more proficient. Due to this difference, the baseline word accuracy was fairly low (see Table 1). Therefore, as a first step, we needed to adapt the automatic speech recognition engine to the THT speech data. We had approximately 1,000 responses each from the Picture and Opinion tasks transcribed. As mentioned above, while the Opinion task responses are generally more spontaneous, the Picture task requires the candidate to accurately describe a picture and th</context>
</contexts>
<marker>Zechner, Higgins, Xi, 2007</marker>
<rawString>Zechner, K., Higgins, D., and Xi, X. (2007). SpeechRater®: A Construct-Driven Approach to Score Spontaneous Non-Native Speech. Proceedings of the 2007 Workshop of the International Speech Communication Association (ISCA) Special Interest Group on Speech and Language Technology in Education (SLaTE), Farmington, PA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>