<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000544">
<title confidence="0.998473">
A Selectionist Theory of Language Acquisition
</title>
<author confidence="0.971302">
Charles D. Yang*
</author>
<affiliation confidence="0.9647535">
Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.924934">
Cambridge, MA 02139
</address>
<email confidence="0.999326">
charles@ai.mit.edu
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970684210526">
This paper argues that developmental patterns in
child language be taken seriously in computational
models of language acquisition, and proposes a for-
mal theory that meets this criterion. We first present
developmental facts that are problematic for sta-
tistical learning approaches which assume no prior
knowledge of grammar, and for traditional learnabil-
ity models which assume the learner moves from one
UG-defined grammar to another. In contrast, we
view language acquisition as a population of gram-
mars associated with &amp;quot;weights&amp;quot;, that compete in a
Darwinian selectionist process. Selection is made
possible by the variational properties of individual
grammars; specifically, their differential compatibil-
ity with the primary linguistic data in the environ-
ment. In addition to a convergence proof, we present
empirical evidence in child language development,
that a learner is best modeled as multiple grammars
in co-existence and competition.
</bodyText>
<sectionHeader confidence="0.987741" genericHeader="categories and subject descriptors">
1 Learnability and Development
</sectionHeader>
<bodyText confidence="0.999887">
A central issue in linguistics and cognitive science
is the problem of language acquisition: How does
a human child come to acquire her language with
such ease, yet without high computational power or
favorable learning conditions? It is evident that any
adequate model of language acquisition must meet
the following empirical conditions:
</bodyText>
<listItem confidence="0.993775444444444">
• Learnability: such a model must converge to the
target grammar used in the learner&apos;s environ-
ment, under plausible assumptions about the
learner&apos;s computational machinery, the nature
of the input data, sample size, and so on.
• Developmental compatibility: the learner mod-
eled in such a theory must exhibit behaviors
that are analogous to the actual course of lan-
guage development (Pinker, 1979).
</listItem>
<bodyText confidence="0.997546">
I would like to thank Julie Legate, Sam Gutmann, Bob
Berwick, Noam Chomsky, John Frampton, and John Gold-
smith for comments and discussion. This work is supported
by an NSF graduate fellowship.
It is worth noting that the developmental compati-
bility condition has been largely ignored in the for-
mal studies of language acquisition. In the rest of
this section, I show that if this condition is taken se-
riously, previous models of language acquisition have
difficulties explaining certain developmental facts in
child language.
</bodyText>
<subsectionHeader confidence="0.999518">
1.1 Against Statistical Learning
</subsectionHeader>
<bodyText confidence="0.985607485714286">
An empiricist approach to language acquisition has
(re)gained popularity in computational linguistics
and cognitive science; see Stolcke (1994), Charniak
(1995), Klavans and Resnik (1996), de Marcken
(1996), Bates and Elman (1996), Seidenberg (1997),
among numerous others. The child is viewed as an
inductive and &amp;quot;generalized&amp;quot; data processor such as
a neural network, designed to derive structural reg-
ularities from the statistical distribution of patterns
in the input data without prior (innate) specific
knowledge of natural language. Most concrete pro-
posals of statistical learning employ expensive and
specific computational procedures such as compres-
sion, Bayesian inferences, propagation of learning
errors, and usually require a large corpus of (some-
times pre-processed) data. These properties imme-
diately challenge the psychological plausibility of the
statistical learning approach. In the present discus-
sion, however, we are not concerned with this but
simply grant that someday, someone might devise
a statistical learning scheme that is psychologically
plausible and also succeeds in converging to the tar-
get language. We show that even if such a scheme
were possible, it would still face serious challenges
from the important but often ignored requirement
of developmental compatibility.
One of the most significant findings in child lan-
guage research of the past decade is that different
aspects of syntactic knowledge are learned at differ-
ent rates. For example, consider the placement of
finite verb in French, where inflected verbs precede
negation and adverbs:
Jean voit souvent/pas Marie.
Jean sees often/not Marie.
This property of French is mastered as early as
</bodyText>
<page confidence="0.99873">
429
</page>
<bodyText confidence="0.999267906976745">
the 20th month, as evidenced by the extreme rarity
of incorrect verb placement in child speech (Pierce,
1992). In contrast, some aspects of language are ac-
quired relatively late. For example, the requirement
of using a sentential subject is not mastered by En-
glish children until as late as the 36th month (Valian,
1991), when English children stop producing a sig-
nificant number of subjectless sentences.
When we examine the adult speech to children
(transcribed in the CHILDES corpus; MacWhinney
and Snow, 1985), we find that more than 90% of
English input sentences contain an overt subject,
whereas only 7-8% of all French input sentences con-
tain an inflected verb followed by negation/adverb.
A statistical learner, one which builds knowledge
purely on the basis of the distribution of the input
data, predicts that English obligatory subject use
should be learned (much) earlier than French verb
placement — exactly the opposite of the actual find-
ings in child language.
Further evidence against statistical learning comes
from the Root Infinitive (RI) stage (Wexler, 1994;
inter alia) in children acquiring certain languages.
Children in the RI stage produce a large number of
sentences where matrix verbs are not finite — un-
grammatical in adult language and thus appearing
infrequently in the primary linguistic data if at all.
It is not clear how a statistical learner will induce
non-existent patterns from the training corpus. In
addition, in the acquisition of verb-second (V2) in
Germanic grammars, it is known (e.g. Haegeman,
1994) that at an early stage, children use a large
proportion (50%) of verb-initial (V1) sentences, a
marked pattern that appears only sparsely in adult
speech. Again, an inductive learner purely driven by
corpus data has no explanation for these disparities
between child and adult languages.
Empirical evidence as such poses a serious prob-
lem for the statistical learning approach. It seems
a mistake to view language acquisition as an induc-
tive procedure that constructs linguistic knowledge,
directly and exclusively, from the distributions of in-
put data.
</bodyText>
<subsectionHeader confidence="0.998828">
1.2 The Transformational Approach
</subsectionHeader>
<bodyText confidence="0.997511854545455">
Another leading approach to language acquisition,
largely in the tradition of generative linguistics, is
motivated by the fact that although child language is
different from adult language, it is different in highly
restrictive ways. Given the input to the child, there
are logically possible and computationally simple in-
ductive rules to describe the data that are never
attested in child language. Consider the following
well-known example. Forming a question in English
involves inversion of the auxiliary verb and the sub-
ject:
Is the man t tall?
where &amp;quot;is&amp;quot; has been fronted from the position t, the
position it assumes in a declarative sentence. A pos-
sible inductive rule to describe the above sentence is
this: front the first auxiliary verb in the sentence.
This rule, though logically possible and computa-
tionally simple, is never attested in child language
(Chomsky, 1975; Crain and Nakayama, 1987; Crain,
1991): that is, children are never seen to produce
sentences like:
* Is the cat that the dog t chasing is scared?
where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;),
instead of the auxiliary following the subject of the
sentence (here, the second &amp;quot;is&amp;quot; in the sentence).
Acquisition findings like these lead linguists to
postulate that the human language capacity is con-
strained in a finite prior space, the Universal Gram-
mar (UG). Previous models of language acquisi-
tion in the UG framework (Wexler and Culicover,
1980; Berwick, 1985; Gibson and Wexler, 1994) are
transformational, borrowing a term from evolution
(Lewontin, 1983), in the sense that the learner moves
from one hypothesis/grammar to another as input
sentences are processed.1 Learnability results can
be obtained for some psychologically plausible algo-
rithms (Niyogi and Berwick, 1996). However, the
developmental compatibility condition still poses se-
rious problems.
Since at any time the state of the learner is identi-
fied with a particular grammar defined by UG, it is
hard to explain (a) the inconsistent patterns in child
language, which cannot be described by any single
adult grammar (e.g. Brown, 1973); and (b) the
smoothness of language development (e.g. Pinker,
1984; Valiant, 1991; inter alia), whereby the child
gradually converges to the target grammar, rather
than the abrupt jumps that would be expected from
binary changes in hypotheses/grammars.
Having noted the inadequacies of the previous
approaches to language acquisition, we will pro-
pose a theory that aims to meet language learn-
ability and language development conditions simul-
taneously. Our theory draws inspirations from Dar-
winian evolutionary biology.
</bodyText>
<sectionHeader confidence="0.999612" genericHeader="method">
2 A Selectionist Model of Language
Acquisition
</sectionHeader>
<subsectionHeader confidence="0.999189">
2.1 The Dynamics of Darwinian Evolution
</subsectionHeader>
<bodyText confidence="0.977077333333333">
Essential to Darwinian evolution is the concept of
variational thinking (Lewontin, 1983). First, differ-
that the transformational approach is not restricted
to UG-based models; for example, Brill&apos;s influential work
(1993) is a corpus-based model which successively revises a
set of syntactic_ rules upon presentation of partially bracketed
sentences. Note that however, the state of the learning sys-
tem at any time is still a single set of rules, that is, a single
&amp;quot;grammar&amp;quot;.
</bodyText>
<page confidence="0.986659">
430
</page>
<bodyText confidence="0.999689">
ences among individuals are viewed as &amp;quot;real&amp;quot;, as op-
posed to deviant from some idealized archetypes, as
in pre-Darwinian thinking. Second, such differences
result in variance in operative functions among indi-
viduals in a population, thus allowing forces of evo-
lution such as natural selection to operate. Evolu-
tionary changes are therefore changes in the distri-
bution of variant individuals in the population. This
contrasts with Lamarckian transformational think-
ing, in which individuals themselves undergo direct
changes (transformations) (Lewontin, 1983).
</bodyText>
<subsectionHeader confidence="0.999781">
2.2 A population of grammars
</subsectionHeader>
<bodyText confidence="0.999645176470588">
Learning, including language acquisition, can be
characterized as a sequence of states in which the
learner moves from one state to another. Transfor-
mational models of language acquisition identify the
state of the learner as a single grammar/hypothesis.
As noted in section 1, this makes difficult to explain
the inconsistency in child language and the smooth-
ness of language development.
We propose that the learner be modeled as a pop-
ulation of &amp;quot;grammars&amp;quot;, the set of all principled lan-
guage variations made available by the biological en-
dowment of the human language faculty. Each gram-
mar Gi is associated with a weight pi, 0 &lt; pi &lt; 1,
and &gt;2p 1. In a linguistic environment E, the
weight pi(E,t) is a function of E and the time vari-
able t, the time since the onset of language acquisi-
tion. We say that
</bodyText>
<equation confidence="0.6251305">
Definition: Learning converges if
Ve,0 &lt;C &lt;1,VG,,jp,(E,t +1)— p,(E,t)1&lt;
</equation>
<bodyText confidence="0.9995386">
That is, learning converges when the composition
and distribution of the grammar population are sta-
bilized. Particularly, in a monolingual environment
ET in which a target grammar T is used, we say that
learning converges to T if limt, pT(ET,t) = 1.
</bodyText>
<subsectionHeader confidence="0.99986">
2.3 A Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999689">
Write E s to indicate that a sentence s is an ut-
terance in the linguistic environment E. Write s E G
if a grammar G can analyze s, which, in a narrow
sense, is parsability (Wexler and Culicover, 1980;
Berwick, 1985). Suppose that there are altogether
N grammars in the population. For simplicity, write
pi for pi(E,t) at time t, and p&apos;,„ for pi(E, t+1) at time
t + 1. Learning takes place as follows:
</bodyText>
<subsectionHeader confidence="0.968577">
The Algorithm:
</subsectionHeader>
<bodyText confidence="0.9814745">
Given an input sentence s, the child
with the probability pi, selects a grammar Gi
</bodyText>
<listItem confidence="0.981756666666667">
• if s E Pi = Pi + 7(1 — Pt)
t Pi = (1 — -Y)P.7 if j
• if s G,
</listItem>
<equation confidence="0.938328">
fp=(1 — 1&apos;)Pi
</equation>
<bodyText confidence="0.99985152173913">
Comment: The algorithm is the Linear reward-
penalty (LR_p) scheme (Bush and Mostellar, 1958),
one of the earliest and most extensively studied
stochastic algorithms in the psychology of learning.
It is real-time and on-line, and thus reflects the
rather limited computational capacity of the child
language learner, by avoiding sophisticated data pro-
cessing and the need for a large memory to store
previously seen examples. Many variants and gener-
alizations of this scheme are studied in Atkinson et
al. (1965), and their thorough mathematical treat-
ments can be found in Narendra and Thathachar
(1989).
The algorithm operates in a selectionist man-
ner: grammars that succeed in analyzing input sen-
tences are rewarded, and those that fail are pun-
ished. In addition to the psychological evidence for
such a scheme in animal and human learning, there
is neurological evidence (Hubei and Wiesel, 1962;
Changeux, 1983; Edelman, 1987; inter alia) that the
development of neural substrate is guided by the ex-
posure to specific stimulus in the environment in a
Darwinian selectionist fashion.
</bodyText>
<subsectionHeader confidence="0.994291">
2.4 A Convergence Proof
</subsectionHeader>
<bodyText confidence="0.984646">
For simplicity but without loss of generality, assume
that there are two grammars (N =- 2), the target
grammar T1 and a pretender T2. The results pre-
sented here generalize to the N-grammar case; see
Narendra and Thathachar (1989).
Definition: The penalty probability of grammar T,
in a linguistic environment E is
ci = Pr(s OTi E s)
In other words, cj represents the probability that
the grammar T, fails to analyze an incoming sen-
tence s and gets punished as a result. Notice that
the penalty probability, essentially a fitness measure
of individual grammars, is an intrinsic property of a
UG-defined grammar relative to a particular linguis-
tic environment E, determined by the distributional
patterns of linguistic expressions in E. It is not ex-
plicitly computed, as in (Clark, 1992) which uses the
Genetic Algorithm (GA).2
The main result is as follows:
</bodyText>
<equation confidence="0.82813475">
Theorem:
C2
lim pi(t) = if Ii — -y(ci +c2) l&lt; 1 (1)
t--000 c1+ c2
</equation>
<bodyText confidence="0.8544899">
Proof sketch: Computing E[pi(t + 1) I pi (t)] as
a function of pi (t) and taking expectations on both
2Clark&apos;s model and the present one share an important
feature: the outcome of acquisition is determined by the dif-
ferential compatibilities of individual grammars. The choice
of the GA introduces various psychological and linguistic as-
sumptions that can not be justified; see Dresher (1999) and
Yang (1999). Furthermore, no formal proof of convergence is
given.
if j
</bodyText>
<page confidence="0.948279">
431
</page>
<bodyText confidence="0.703731">
sides give
</bodyText>
<equation confidence="0.969281">
E[pi(t + 1) = [1 — 01(ci + c2)]E[pi (t)] + lic2 (2)
</equation>
<bodyText confidence="0.962273833333333">
Solving [2] yields [1].
Comment 1: It is easy to see that pi —■ 1 (and
P2 —&gt; 0) when ci = 0 and c2 &gt; 0; that is, the learner
converges to the target grammar T1, which has a
penalty probability of 0, by definition, in a mono-
lingual environment. Learning is robust. Suppose
that there is a small amount of noise in the input,
i.e. sentences such as speaker errors which are not
compatible with the target grammar. Then ci &gt; 0.
If ci &lt; c2, convergence to Ti is still ensured by [1].
Consider a non-uniform linguistic environment in
which the linguistic evidence does not unambigu-
ously identify any single grammar; an example of
this is a population in contact with two languages
(grammars), say, Ti and T2. Since ci &gt; 0 and c2 &gt; 0,
[1] entails that pi and p2 reach a stable equilibrium
at the end of language acquisition; that is, language
learners are essentially bi-lingual speakers as a result
of language contact. Kroch (1989) and his colleagues
have argued convincingly that this is what happened
in many cases of diachronic change. In Yang (1999),
we have been able to extend the acquisition model
to a population of learners, and formalize Kroch&apos;s
idea of grammar competition over time.
Comment 2: In the present model, one can di-
rectly measure the rate of change in the weight of the
target grammar, and compare with developmental
findings. Suppose T1 is the target grammar, hence
Cl = 0. The expected increase of pi, 41 is com-
puted as follows:
</bodyText>
<equation confidence="0.993069">
E[APi] = c2P1P2 (3)
</equation>
<bodyText confidence="0.9972985">
Since p2 = 1 — pi, Api [3] is obviously a quadratic
function of pi (t). Hence, the growth of pi will pro-
duce the familiar S-shape curve familiar in the psy-
chology of learning. There is evidence for an S-shape
pattern in child language development (Clahsen,
1986; Wijnen, 1999; inter alia), which, if true, sug-
gests that a selectionist learning algorithm adopted
here might indeed be what the child learner employs.
</bodyText>
<subsectionHeader confidence="0.942904">
2.5 Unambiguous Evidence is Unnecessary
</subsectionHeader>
<bodyText confidence="0.972678285714286">
One way to ensure convergence is to assume the ex-
istence of unambiguous evidence (cf. Fodor, 1998):
sentences that are only compatible with the target
grammar but not with any other grammar. Unam-
biguous evidence is, however, not necessary for the
proposed model to converge. It follows from the the-
orem [1] that even if no evidence can unambiguously
identify the target grammar from its competitors, it
is still possible to ensure convergence as long as all
competing grammars fail on some proportion of in-
put sentences; i.e. they all have positive penalty
probabilities. Consider the acquisition of the target,
a German V2 grammar, in a population of grammars
below:
</bodyText>
<listItem confidence="0.99893675">
1. German: SVO, OVS, XVSO
2. English: SVO, XSVO
3. Irish: VSO, XVSO
4. Hixkaryana: OVS, XOVS
</listItem>
<bodyText confidence="0.99979975">
We have used X to denote non-argument categories
such as adverbs, adjuncts, etc., which can quite
freely appear in sentence-initial positions. Note that
none of the patterns in (1) could conclusively distin-
guish German from the other three grammars. Thus,
no unambiguous evidence appears to exist. How-
ever, if SVO, OVS, and XVSO patterns appear in
the input data at positive frequencies, the German
grammar has a higher overall &amp;quot;fitness value&amp;quot; than
other grammars by the virtue of being compatible
with all input sentences. As a result, German will
eventually eliminate competing grammars.
</bodyText>
<subsectionHeader confidence="0.878252">
2.6 Learning in a Parametric Space
</subsectionHeader>
<bodyText confidence="0.999986142857143">
Suppose that natural language grammars vary in
a parametric space, as cross-linguistic studies sug-
gest.3 We can then study the dynamical behaviors
of grammar classes that are defined in these para-
metric dimensions. Following (Clark, 1992), we say
that a sentence s expresses a parameter a if a gram-
mar must have set a to some definite value in order
to assign a well-formed representation to s. Con-
vergence to the target value of a can be ensured by
the existence of evidence (s) defined in the sense of
parameter expression. The convergence to a single
grammar can then be viewed as the intersection of
parametric grammar classes, converging in parallel
to the target values of their respective parameters.
</bodyText>
<sectionHeader confidence="0.978438" genericHeader="method">
3 Some Developmental Predictions
</sectionHeader>
<bodyText confidence="0.996953333333333">
The present model makes two predictions that can-
not be made in the standard transformational theo-
ries of acquisition:
</bodyText>
<listItem confidence="0.678062">
1. As the target gradually rises to dominance, the
</listItem>
<bodyText confidence="0.981675">
child entertains a number of co-existing gram-
mars. This will be reflected in distributional
patterns of child language, under the null hy-
pothesis that the grammatical knowledge (in
our model, the population of grammars and
their respective weights) used in production is
that used in analyzing linguistic evidence. For
grammatical phenomena that are acquired rela-
tively late, child language consists of the output
of more than one grammar.
</bodyText>
<footnote confidence="0.7527495">
3Although different theories of grammar, e.g. GB, HPSG,
LFG, TAG, have different ways of instantiating this idea.
</footnote>
<page confidence="0.997267">
432
</page>
<bodyText confidence="0.986047176470588">
2. Other things being equal, the rate of develop-
ment is determined by the penalty probabili-
ties of competing grammars relative to the in-
put data in the linguistic environment [3].
In this paper, we present longitudinal evidence
concerning the prediction in (2).4 To evaluate de-
velopmental predictions, we must estimate the the
penalty probabilities of the competing grammars in
a particular linguistic environment. Here we exam-
ine the developmental rate of French verb placement,
an early acquisition (Pierce, 1992), that of English
subject use, a late acquisition (Valian, 1991), that of
Dutch V2 parameter, also a late acquisition (Haege-
man, 1994).
Using the idea of parameter expression (section
2.6), we estimate the frequency of sentences that
unambiguously identify the target value of a pa-
rameter. For example, sentences that contain finite
verbs preceding adverb or negation (&amp;quot;Jean voit sou-
vent/pas Marie&amp;quot;) are unambiguous indication for the
[-1-1 value of the verb raising parameter. A grammar
with the [-] value for this parameter is incompatible
with such sentences and if probabilistically selected
for the learner for grammatical analysis, will be pun-
ished as a result. Based on the CHILDES corpus,
we estimate that such sentences constitute 8% of all
French adult utterances to children. This suggests
that unambiguous evidence as 8% of all input data
is sufficient for a very early acquisition: in this case,
the target value of the verb-raising parameter is cor-
rectly set. We therefore have a direct explanation
of Brown&apos;s (1973) observation that in the acquisi-
tion of fixed word order languages such as English,
word order errors are &amp;quot;trifingly few&amp;quot;. For example,
English children are never to seen to produce word
order variations other than SVO, the target gram-
mar, nor do they fail to front Wh-words in question
formation. Virtually all English sentences display
rigid word order, e.g. verb almost always (immedi-
ately) precedes object, which give a very high (per-
haps close to 100%, far greater than 8%, which is
sufficient for a very early acquisition as in the case of
French verb raising) rate of unambiguous evidence,
sufficient to drive out other word order grammars
very early on.
Consider then the acquisition of the subject pa-
rameter in English, which requires a sentential sub-
ject. Languages like Italian, Spanish, and Chinese,
on the other hand, have the option of dropping the
subject. Therefore, sentences with an overt subject
are not necessarily useful in distinguishing English
</bodyText>
<footnote confidence="0.875027333333333">
41n Yang (1999), we show that a child learner, en route to
her target grammar, entertains multiple grammars. For ex-
ample, a significant portion of English child language shows
characteristics of a topic-drop optional subject grammar like
Chinese, before they learn that subject use in English is oblig-
atory at around the 3rd birthday.
</footnote>
<bodyText confidence="0.979191535714286">
from optional subject languages.&apos; However, there
exists a certain type of English sentence that is in-
dicative (Hyams, 1986):
There is a man in the room.
Are there toys on the floor?
The subject of these sentences is &amp;quot;there&amp;quot;, a non-
referential lexical item that is present for purely
structural reasons — to satisfy the requirement in
English that the pre-verbal subject position must
be filled. Optional subject languages do not have
this requirement, and do not have expletive-subject
sentences. Expletive sentences therefore express the
[+] value of the subject parameter. Based on the
CHILDES corpus, we estimate that expletive sen-
tences constitute 1% of all English adult utterances
to children.
Note that before the learner eliminates optional
subject grammars on the cumulative basis of exple-
tive sentences, she has probabilistic access to multi-
ple grammars. This is fundamentally different from
stochastic grammar models, in which the learner has
probabilistic access to generative rules. A stochastic
grammar is not a developmentally adequate model
of language acquisition. As discussed in section 1.1,
more than 90% of English sentences contain a sub-
ject: a stochastic grammar model will overwhelm-
ingly bias toward the rule that generates a subject.
English children, however, go through long period
of subject drop. In the present model, child sub-
ject drop is interpreted as the presence of the true
optional subject grammar, in co-existence with the
obligatory subject grammar.
Lastly, we consider the setting of the Dutch V2
parameter. As noted in section 2.5, there appears to
no unambiguous evidence for the [-E] value of the V2
parameter: SVO, VSO, and OVS grammars, mem-
bers of the [-V2] class, are each compatible with cer-
tain proportions of expressions produced by the tar-
get V2 grammar. However, observe that despite of
its compatibility with with some input patterns, an
OVS grammar can not survive long in the population
of competing grammars. This is because an OVS
grammar has an extremely high penalty probability.
Examination of CHILDES shows that OVS patterns
consist of only 1.3% of all input sentences to chil-
dren, whereas SVO patterns constitute about 65%
of all utterances, and XVSO, about 34%. There-
fore, only SVO and VSO grammar, members of the
[-V2] class, are &amp;quot;contenders&amp;quot; alongside the (target)
V2 grammar, by the virtue of being compatible with
significant portions of input data. But notice that
OVS patterns do penalize both SVO and VSO gram-
mars, and are only compatible with the [+V2] gram-
5Notice that this presupposes the child&apos;s prior knowledge
of and access to both obligatory and optional subject gram-
mars.
</bodyText>
<page confidence="0.998458">
433
</page>
<bodyText confidence="0.99999045">
mars. Therefore, OVS patterns are effectively un-
ambiguous evidence (among the contenders) for the
V2 parameter, which eventually drive SVO and VSO
grammars out of the population.
In the selectionist model, the rarity of OVS sen-
tences predicts that the acquisition of the V2 pa-
rameter in Dutch is a relatively late phenomenon.
Furthermore, because the frequency (1.3%) of Dutch
OVS sentences is comparable to the frequency (1%)
of English expletive sentences, we expect that Dutch
V2 grammar is successfully acquired roughly at the
same time when English children have adult-level
subject use (around age 3; Valian, 1991). Although
I am not aware of any report on the timing of the
correct setting of the Dutch V2 parameter, there is
evidence in the acquisition of German, a similar lan-
_ guage, that children are considered to have success-
fully acquired V2 by the 36-39th month (Clahsen,
1986). Under the model developed here, this is not
an coincidence.
</bodyText>
<sectionHeader confidence="0.999497" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999970088235294">
To capitulate, this paper first argues that consider-
ations of language development must be taken seri-
ously to evaluate computational models of language
acquisition. Once we do so, both statistical learn-
ing approaches and traditional UG-based learnabil-
ity studies are empirically inadequate. We proposed
an alternative model which views language acqui-
sition as a selectionist process in which grammars
form a population and compete to match linguis-
tic- expressions present in the environment. The
course and outcome of acquisition are determined by
the relative compatibilities of the grammars with in-
put data; such compatibilities, expressed in penalty
probabilities and unambiguous evidence, are quan-
tifiable and empirically testable, allowing us to make
direct predictions about language development.
The biologically endowed linguistic knowledge en-
ables the learner to go beyond unanalyzed distribu-
tional properties of the input data. We argued in
section 1.1 that it is a mistake to model language
acquisition as directly learning the probabilistic dis-
tribution of the linguistic data. Rather, language ac-
quisition is guided by particular input evidence that
serves to disambiguate the target grammar from the
competing grammars. The ability to use such evi-
dence for grammar selection is based on the learner&apos;s
linguistic knowledge. Once such knowledge is as-
sumed, the actual process of language acquisition is
no more remarkable than generic psychological mod-
els of learning. The selectionist theory, if correct,
show an example of the interaction between domain-
specific knowledge and domain-neutral mechanisms,
which combine to explain properties of language and
cognition.
</bodyText>
<sectionHeader confidence="0.993039" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.928333607142857">
Atkinson, R., G. Bower, and E. Crot,hers. (1965).
An Introduction to Mathematical Learning Theory.
New York: Wiley.
Bates, E. and J. Elman. (1996). Learning rediscov-
ered: A perspective on Saffran, Aslin, and Newport.
Science 274: 5294.
Berwick, R. (1985). The acquisition of syntactic
knowledge. Cambridge, MA: MIT Press.
Brill, E. (1993). Automatic grammar induction and
parsing free text: a transformation-based approach.
ACL Annual Meeting.
Brown, R. (1973). A first language. Cambridge,
MA: Harvard University Press.
Bush, R. and F. Mostellar. Stochastic models for
learning. New York: Wiley.
Charniak, E. (1995). Statistical language learning.
Cambridge, MA: MIT Press.
Chomsky, N. (1975). Reflections on language. New
York: Pantheon.
Changeux, J.-P. (1983). L&apos;Homme Neuronal. Paris:
Fayard.
Clahsen, H. (1986). Verbal inflections in German
child language: Acquisition of agreement markings
and the functions they encode. Linguistics 24: 79-
121.
Clark, R. (1992). The selection of syntactic knowl-
edge. Language Acquisition 2: 83-149.
CraM, S. and M. Nakayama (1987). Structure de-
pendency in grammar formation. Language 63: 522-
543.
Dresher, E. (1999). Charting the learning path: cues
to parameter setting. Linguistic Inquiry 30: 27-67.
Edelman, G. (1987). Neural Darwinism: The the-
ory of neuronal group selection. New York: Basic
Books.
Fodor, J. D. (1998). Unambiguous triggers. Lin-
guistic Inquiry 29: 1-36.
Gibson, E. and K. Wexler (1994). Triggers. Linguis-
tic Inquiry 25: 355-407.
Haegeman, L. (1994). Root infinitives, clitics, and
truncated structures. Language Acquisition.
Hubel, D. and T. Wiesel (1962). Receptive fields,
binocular interaction and functional architecture in
the cat&apos;s visual cortex. Journal of Physiology 160:
106-54.
Hyams, N. (1986) Language acquisition and the the-
ory of parameters. Reidel: Dordrecht.
Klavins, J. and P. Resnik (eds.) (1996). The balanc-
ing act. Cambridge, MA: MIT Press.
Kroch, A. (1989). Reflexes of grammar in patterns
of language change. Language variation and change
1: 199-244.
Lewontin, R. (1983). The organism as the subject
and object of evolution. Scientia 118: 65-82.
de Marcken, C. (1996). Unsupervised language ac-
quisition. Ph.D. dissertation, MIT.
</reference>
<page confidence="0.99076">
434
</page>
<reference confidence="0.994017051282051">
MacWhinney, B. and C. Snow (1985). The Child
Language Date Exchange System. Journal of Child
Language 12, 271-296.
Narendra, K. and M. Thathachar (1989). Learning
automata. Englewood Cliffs, NJ: Prentice Hall.
Niyogi, P. and R. Berwick (1996). A language learn-
ing model for finite parameter space. Cognition 61:
162-193.
Pierce, A. (1992). Language acquisition and and
syntactic theory: a comparative analysis of French
and English child grammar. Boston: Kluwer.
Pinker, S. (1979). Formal models of language learn-
ing. Cognition 7: 217-283.
Pinker, S. (1984). Language learnability and lan-
guage development. Cambridge, MA: Harvard Uni-
versity Press.
Seidenberg, M. (1997). Language acquisition and
use: Learning and applying probabilistic con-
straints. Science 275: 1599-1604.
Stolcke, A. (1994) Bayesian Learning of Probabilis-
tic Language Models. Ph.D. thesis, University of
California at Berkeley, Berkeley, CA.
Valian, V. (1991). Syntactic subjects in the early
speech of American and Italian children. Cognition
40: 21-82.
Wexler, K. (1994). Optional infinitives, head move-
ment, and the economy of derivation in child lan-
guage. In Lightfoot, D. and N. Hornstein (eds.)
Verb movement. Cambridge: Cambridge University
Press.
Wexler, K. and P. Culicover (1980). Formal princi-
ples of language acquisition. Cambridge, MA: MIT
Press.
Wijnen, F. (1999). Verb placement in Dutch child
language: A longitudinal analysis. Ms. University
of Utrecht.
Yang, C. (1999). The variational dynamics of natu-
ral language: Acquisition and use. Technical report,
MIT AT Lab.
</reference>
<page confidence="0.999206">
435
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000210">
<title confidence="0.999967">A Selectionist Theory of Language Acquisition</title>
<author confidence="0.999962">Charles D Yang</author>
<affiliation confidence="0.999993">Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<address confidence="0.999987">Cambridge, MA 02139</address>
<email confidence="0.999821">charles@ai.mit.edu</email>
<abstract confidence="0.994635070811746">This paper argues that developmental patterns in child language be taken seriously in computational models of language acquisition, and proposes a formal theory that meets this criterion. We first present developmental facts that are problematic for statistical learning approaches which assume no prior knowledge of grammar, and for traditional learnability models which assume the learner moves from one UG-defined grammar to another. In contrast, we view language acquisition as a population of grammars associated with &amp;quot;weights&amp;quot;, that compete in a Darwinian selectionist process. Selection is made by the of individual grammars; specifically, their differential compatibility with the primary linguistic data in the environment. In addition to a convergence proof, we present empirical evidence in child language development, that a learner is best modeled as multiple grammars in co-existence and competition. 1 Learnability and Development A central issue in linguistics and cognitive science is the problem of language acquisition: How does a human child come to acquire her language with such ease, yet without high computational power or favorable learning conditions? It is evident that any adequate model of language acquisition must meet the following empirical conditions: Learnability: a model must converge to the target grammar used in the learner&apos;s environment, under plausible assumptions about the learner&apos;s computational machinery, the nature of the input data, sample size, and so on. Developmental compatibility: learner modeled in such a theory must exhibit behaviors that are analogous to the actual course of language development (Pinker, 1979). I would like to thank Julie Legate, Sam Gutmann, Bob Berwick, Noam Chomsky, John Frampton, and John Goldsmith for comments and discussion. This work is supported by an NSF graduate fellowship. It is worth noting that the developmental compatibility condition has been largely ignored in the formal studies of language acquisition. In the rest of this section, I show that if this condition is taken seriously, previous models of language acquisition have difficulties explaining certain developmental facts in child language. 1.1 Against Statistical Learning An empiricist approach to language acquisition has (re)gained popularity in computational linguistics and cognitive science; see Stolcke (1994), Charniak (1995), Klavans and Resnik (1996), de Marcken (1996), Bates and Elman (1996), Seidenberg (1997), among numerous others. The child is viewed as an inductive and &amp;quot;generalized&amp;quot; data processor such as a neural network, designed to derive structural regularities from the statistical distribution of patterns the input data without specific knowledge of natural language. Most concrete proposals of statistical learning employ expensive and specific computational procedures such as compression, Bayesian inferences, propagation of learning errors, and usually require a large corpus of (sometimes pre-processed) data. These properties immediately challenge the psychological plausibility of the statistical learning approach. In the present discussion, however, we are not concerned with this but simply grant that someday, someone might devise a statistical learning scheme that is psychologically plausible and also succeeds in converging to the target language. We show that even if such a scheme were possible, it would still face serious challenges from the important but often ignored requirement of developmental compatibility. One of the most significant findings in child language research of the past decade is that different aspects of syntactic knowledge are learned at different rates. For example, consider the placement of finite verb in French, where inflected verbs precede negation and adverbs: Jean voit souvent/pas Marie. Marie. This property of French is mastered as early as 429 the 20th month, as evidenced by the extreme rarity of incorrect verb placement in child speech (Pierce, 1992). In contrast, some aspects of language are acquired relatively late. For example, the requirement of using a sentential subject is not mastered by English children until as late as the 36th month (Valian, 1991), when English children stop producing a significant number of subjectless sentences. When we examine the adult speech to children (transcribed in the CHILDES corpus; MacWhinney and Snow, 1985), we find that more than 90% of English input sentences contain an overt subject, whereas only 7-8% of all French input sentences contain an inflected verb followed by negation/adverb. A statistical learner, one which builds knowledge purely on the basis of the distribution of the input data, predicts that English obligatory subject use should be learned (much) earlier than French verb placement — exactly the opposite of the actual findings in child language. Further evidence against statistical learning comes from the Root Infinitive (RI) stage (Wexler, 1994; alia) children acquiring certain languages. Children in the RI stage produce a large number of sentences where matrix verbs are not finite — ungrammatical in adult language and thus appearing infrequently in the primary linguistic data if at all. It is not clear how a statistical learner will induce from the training corpus. In addition, in the acquisition of verb-second (V2) in Germanic grammars, it is known (e.g. Haegeman, 1994) that at an early stage, children use a large proportion (50%) of verb-initial (V1) sentences, a marked pattern that appears only sparsely in adult speech. Again, an inductive learner purely driven by corpus data has no explanation for these disparities between child and adult languages. Empirical evidence as such poses a serious problem for the statistical learning approach. It seems a mistake to view language acquisition as an inductive procedure that constructs linguistic knowledge, directly and exclusively, from the distributions of input data. 1.2 The Transformational Approach Another leading approach to language acquisition, largely in the tradition of generative linguistics, is motivated by the fact that although child language is different from adult language, it is different in highly Given the input to the child, there are logically possible and computationally simple inductive rules to describe the data that are never attested in child language. Consider the following well-known example. Forming a question in English involves inversion of the auxiliary verb and the subject: Is the man t tall? &amp;quot;is&amp;quot; has been fronted from the position position it assumes in a declarative sentence. A possible inductive rule to describe the above sentence is front the verb in the sentence. This rule, though logically possible and computationally simple, is never attested in child language (Chomsky, 1975; Crain and Nakayama, 1987; Crain, 1991): that is, children are never seen to produce sentences like: * Is the cat that the dog t chasing is scared? where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;), instead of the auxiliary following the subject of the sentence (here, the second &amp;quot;is&amp;quot; in the sentence). Acquisition findings like these lead linguists to postulate that the human language capacity is conin a finite the Universal Grammar (UG). Previous models of language acquisition in the UG framework (Wexler and Culicover, 1980; Berwick, 1985; Gibson and Wexler, 1994) are a term from evolution (Lewontin, 1983), in the sense that the learner moves from one hypothesis/grammar to another as input are Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be described by any single adult grammar (e.g. Brown, 1973); and (b) the smoothness of language development (e.g. Pinker, Valiant, 1991; alia), the child gradually converges to the target grammar, rather than the abrupt jumps that would be expected from binary changes in hypotheses/grammars. Having noted the inadequacies of the previous approaches to language acquisition, we will propose a theory that aims to meet language learnability and language development conditions simultaneously. Our theory draws inspirations from Darwinian evolutionary biology. 2 A Selectionist Model of Language Acquisition 2.1 The Dynamics of Darwinian Evolution Essential to Darwinian evolution is the concept of thinking (Lewontin, 1983). First, differthat the transformational approach is not restricted to UG-based models; for example, Brill&apos;s influential work (1993) is a corpus-based model which successively revises a set of syntactic_ rules upon presentation of partially bracketed sentences. Note that however, the state of the learning system at any time is still a single set of rules, that is, a single &amp;quot;grammar&amp;quot;. 430 ences among individuals are viewed as &amp;quot;real&amp;quot;, as opposed to deviant from some idealized archetypes, as in pre-Darwinian thinking. Second, such differences result in variance in operative functions among individuals in a population, thus allowing forces of evolution such as natural selection to operate. Evoluchanges are therefore changes in the distrivariant individuals in the population. This with Lamarckian thinkin which individuals direct changes (transformations) (Lewontin, 1983). 2.2 A population of grammars Learning, including language acquisition, can be characterized as a sequence of states in which the learner moves from one state to another. Transformational models of language acquisition identify the of the learner as a As noted in section 1, this makes difficult to explain the inconsistency in child language and the smoothness of language development. propose that the learner be modeled as a pop- &amp;quot;grammars&amp;quot;, the set of all principled language variations made available by the biological endowment of the human language faculty. Each gramis associated with a &lt; &lt; &gt;2p1. In a linguistic environment the a function of the time varitime since the onset of language acquisition. We say that Ve,0 &lt;C &lt;1,VG,,jp,(E,t +1)— p,(E,t)1&lt; That is, learning converges when the composition and distribution of the grammar population are stabilized. Particularly, in a monolingual environment in which a target grammar used, we say that to T = 1. 2.3 A Learning Algorithm s indicate that a sentence an utin the linguistic environment a grammar analyze in a narrow sense, is parsability (Wexler and Culicover, 1980; Berwick, 1985). Suppose that there are altogether in the population. For simplicity, write for time t+1) time + Learning takes place as follows: The Algorithm: an input sentence child the probability selects a grammar if sEPi = Pi + — Pt) Pi = — if G, — 1&apos;)Pi algorithm is the Linear reward- (Bush and Mostellar, 1958), one of the earliest and most extensively studied stochastic algorithms in the psychology of learning. It is real-time and on-line, and thus reflects the rather limited computational capacity of the child language learner, by avoiding sophisticated data processing and the need for a large memory to store previously seen examples. Many variants and generalizations of this scheme are studied in Atkinson et al. (1965), and their thorough mathematical treatments can be found in Narendra and Thathachar (1989). The algorithm operates in a selectionist manner: grammars that succeed in analyzing input sentences are rewarded, and those that fail are punished. In addition to the psychological evidence for such a scheme in animal and human learning, there is neurological evidence (Hubei and Wiesel, 1962; 1983; Edelman, 1987; alia) the development of neural substrate is guided by the exposure to specific stimulus in the environment in a Darwinian selectionist fashion. 2.4 A Convergence Proof For simplicity but without loss of generality, assume there are two grammars =the target T1 and a results presented here generalize to the N-grammar case; see Narendra and Thathachar (1989). probability grammar a linguistic environment = Pr(s In other words, cj represents the probability that grammar to analyze an incoming sengets punished as a result. Notice that the penalty probability, essentially a fitness measure individual grammars, is an of a UG-defined grammar relative to a particular linguisenvironment by the distributional of linguistic expressions in It not explicitly computed, as in (Clark, 1992) which uses the Algorithm The main result is as follows: Theorem: C2 lim pi(t) = if Ii — -y(ci +c2) l&lt; 1 (1) t--000 c1+ c2 sketch: + 1) (t)] as function of (t) taking expectations on both model and the present one share an important feature: the outcome of acquisition is determined by the differential compatibilities of individual grammars. The choice of the GA introduces various psychological and linguistic assumptions that can not be justified; see Dresher (1999) and Yang (1999). Furthermore, no formal proof of convergence is given. 431 sides give 1) = — + c2)]E[pi (t)] + lic2 (2) Solving [2] yields [1]. Comment 1: It is easy to see that pi —■ 1 (and P2 —&gt; 0) when ci = 0 and c2 &gt; 0; that is, the learner to the target grammar which has a penalty probability of 0, by definition, in a monoenvironment. Learning is that there is a small amount of noise in the input, i.e. sentences such as speaker errors which are not with the target grammar. Then &gt; 0. &lt; c2, convergence to Ti is still ensured by [1]. Consider a non-uniform linguistic environment in which the linguistic evidence does not unambiguously identify any single grammar; an example of this is a population in contact with two languages say, Ti and ci &gt; 0 and c2 &gt; 0, [1] entails that pi and p2 reach a stable equilibrium at the end of language acquisition; that is, language learners are essentially bi-lingual speakers as a result of language contact. Kroch (1989) and his colleagues have argued convincingly that this is what happened in many cases of diachronic change. In Yang (1999), we have been able to extend the acquisition model to a population of learners, and formalize Kroch&apos;s idea of grammar competition over time. Comment 2: In the present model, one can dimeasure the rate of the weight of the target grammar, and compare with developmental Suppose is the target grammar, hence Cl = 0. The expected increase of pi, 41 is computed as follows: c2P1P2 p2 = 1 — pi, [3] is obviously a quadratic of pi (t). Hence, the growth of will produce the familiar S-shape curve familiar in the psychology of learning. There is evidence for an S-shape pattern in child language development (Clahsen, Wijnen, 1999; alia), if true, suggests that a selectionist learning algorithm adopted here might indeed be what the child learner employs. 2.5 Unambiguous Evidence is Unnecessary One way to ensure convergence is to assume the exof evidence Fodor, 1998): that are with the target grammar but not with any other grammar. Unambiguous evidence is, however, not necessary for the proposed model to converge. It follows from the theorem [1] that even if no evidence can unambiguously identify the target grammar from its competitors, it is still possible to ensure convergence as long as all grammars fail on of input sentences; i.e. they all have positive penalty probabilities. Consider the acquisition of the target, a German V2 grammar, in a population of grammars below: 1. German: SVO, OVS, XVSO 2. English: SVO, XSVO 3. Irish: VSO, XVSO 4. Hixkaryana: OVS, XOVS We have used X to denote non-argument categories such as adverbs, adjuncts, etc., which can quite freely appear in sentence-initial positions. Note that none of the patterns in (1) could conclusively distinguish German from the other three grammars. Thus, no unambiguous evidence appears to exist. However, if SVO, OVS, and XVSO patterns appear in the input data at positive frequencies, the German grammar has a higher overall &amp;quot;fitness value&amp;quot; than other grammars by the virtue of being compatible sentences. As a result, German will eventually eliminate competing grammars. 2.6 Learning in a Parametric Space Suppose that natural language grammars vary in a parametric space, as cross-linguistic studies sug- We can then study the dynamical behaviors grammar are defined in these parametric dimensions. Following (Clark, 1992), we say a sentence expresses parameter a if a grammar must have set a to some definite value in order assign a well-formed representation to Convergence to the target value of a can be ensured by existence of evidence in the sense of parameter expression. The convergence to a single grammar can then be viewed as the intersection of parametric grammar classes, converging in parallel to the target values of their respective parameters. 3 Some Developmental Predictions The present model makes two predictions that cannot be made in the standard transformational theories of acquisition: 1. As the target gradually rises to dominance, the child entertains a number of co-existing grammars. This will be reflected in distributional patterns of child language, under the null hypothesis that the grammatical knowledge (in our model, the population of grammars and their respective weights) used in production is that used in analyzing linguistic evidence. For grammatical phenomena that are acquired relatively late, child language consists of the output of more than one grammar. different theories of grammar, e.g. GB, HPSG, LFG, TAG, have different ways of instantiating this idea. 432 2. Other things being equal, the rate of development is determined by the penalty probabilities of competing grammars relative to the input data in the linguistic environment [3]. In this paper, we present longitudinal evidence the prediction in To evaluate developmental predictions, we must estimate the the penalty probabilities of the competing grammars in a particular linguistic environment. Here we examine the developmental rate of French verb placement, an early acquisition (Pierce, 1992), that of English subject use, a late acquisition (Valian, 1991), that of Dutch V2 parameter, also a late acquisition (Haegeman, 1994). Using the idea of parameter expression (section 2.6), we estimate the frequency of sentences that unambiguously identify the target value of a parameter. For example, sentences that contain finite preceding adverb or negation (&amp;quot;Jean souvent/pas Marie&amp;quot;) are unambiguous indication for the [-1-1 value of the verb raising parameter. A grammar with the [-] value for this parameter is incompatible with such sentences and if probabilistically selected for the learner for grammatical analysis, will be punished as a result. Based on the CHILDES corpus, we estimate that such sentences constitute 8% of all French adult utterances to children. This suggests that unambiguous evidence as 8% of all input data is sufficient for a very early acquisition: in this case, the target value of the verb-raising parameter is correctly set. We therefore have a direct explanation of Brown&apos;s (1973) observation that in the acquisition of fixed word order languages such as English, word order errors are &amp;quot;trifingly few&amp;quot;. For example, English children are never to seen to produce word order variations other than SVO, the target grammar, nor do they fail to front Wh-words in question formation. Virtually all English sentences display rigid word order, e.g. verb almost always (immediately) precedes object, which give a very high (perhaps close to 100%, far greater than 8%, which is sufficient for a very early acquisition as in the case of French verb raising) rate of unambiguous evidence, sufficient to drive out other word order grammars very early on. Consider then the acquisition of the subject parameter in English, which requires a sentential subject. Languages like Italian, Spanish, and Chinese, on the other hand, have the option of dropping the subject. Therefore, sentences with an overt subject are not necessarily useful in distinguishing English Yang (1999), we show that a child learner, en route to her target grammar, entertains multiple grammars. For example, a significant portion of English child language shows characteristics of a topic-drop optional subject grammar like Chinese, before they learn that subject use in English is obligatory at around the 3rd birthday. from optional subject languages.&apos; However, there a certain type of English sentence that indicative (Hyams, 1986): There is a man in the room. Are there toys on the floor? The subject of these sentences is &amp;quot;there&amp;quot;, a nonreferential lexical item that is present for purely structural reasons — to satisfy the requirement in English that the pre-verbal subject position must be filled. Optional subject languages do not have this requirement, and do not have expletive-subject sentences. Expletive sentences therefore express the [+] value of the subject parameter. Based on the CHILDES corpus, we estimate that expletive sentences constitute 1% of all English adult utterances to children. Note that before the learner eliminates optional subject grammars on the cumulative basis of expletive sentences, she has probabilistic access to multiis fundamentally different from stochastic grammar models, in which the learner has access to rules. stochastic grammar is not a developmentally adequate model of language acquisition. As discussed in section 1.1, more than 90% of English sentences contain a subject: a stochastic grammar model will overwhelmingly bias toward the rule that generates a subject. English children, however, go through long period of subject drop. In the present model, child subject drop is interpreted as the presence of the true subject co-existence with the obligatory subject grammar. Lastly, we consider the setting of the Dutch V2 parameter. As noted in section 2.5, there appears to no unambiguous evidence for the [-E] value of the V2 parameter: SVO, VSO, and OVS grammars, members of the [-V2] class, are each compatible with certain proportions of expressions produced by the target V2 grammar. However, observe that despite of compatibility with with patterns, an OVS grammar can not survive long in the population of competing grammars. This is because an OVS grammar has an extremely high penalty probability. Examination of CHILDES shows that OVS patterns consist of only 1.3% of all input sentences to children, whereas SVO patterns constitute about 65% of all utterances, and XVSO, about 34%. Therefore, only SVO and VSO grammar, members of the [-V2] class, are &amp;quot;contenders&amp;quot; alongside the (target) V2 grammar, by the virtue of being compatible with significant portions of input data. But notice that patterns do penalize and VSO gramand are only compatible with the [+V2] gramthat this presupposes the child&apos;s prior knowledge of and access to both obligatory and optional subject grammars. 433 Therefore, OVS patterns are unambiguous evidence (among the contenders) for the V2 parameter, which eventually drive SVO and VSO grammars out of the population. In the selectionist model, the rarity of OVS sentences predicts that the acquisition of the V2 parameter in Dutch is a relatively late phenomenon. Furthermore, because the frequency (1.3%) of Dutch OVS sentences is comparable to the frequency (1%) of English expletive sentences, we expect that Dutch V2 grammar is successfully acquired roughly at the same time when English children have adult-level subject use (around age 3; Valian, 1991). Although I am not aware of any report on the timing of the correct setting of the Dutch V2 parameter, there is in the acquisition of German, a similar lan- _ guage, that children are considered to have successfully acquired V2 by the 36-39th month (Clahsen, 1986). Under the model developed here, this is not an coincidence. 4 Conclusion To capitulate, this paper first argues that considerations of language development must be taken seriously to evaluate computational models of language acquisition. Once we do so, both statistical learning approaches and traditional UG-based learnability studies are empirically inadequate. We proposed an alternative model which views language acquisition as a selectionist process in which grammars form a population and compete to match linguisexpressions present in the environment. The course and outcome of acquisition are determined by the relative compatibilities of the grammars with input data; such compatibilities, expressed in penalty probabilities and unambiguous evidence, are quantifiable and empirically testable, allowing us to make direct predictions about language development. The biologically endowed linguistic knowledge enables the learner to go beyond unanalyzed distributional properties of the input data. We argued in section 1.1 that it is a mistake to model language acquisition as directly learning the probabilistic distribution of the linguistic data. Rather, language acquisition is guided by particular input evidence that serves to disambiguate the target grammar from the grammars. The use such evidence for grammar selection is based on the learner&apos;s linguistic knowledge. Once such knowledge is asthe actual language acquisition is no more remarkable than generic psychological models of learning. The selectionist theory, if correct, show an example of the interaction between domainspecific knowledge and domain-neutral mechanisms, which combine to explain properties of language and cognition.</abstract>
<note confidence="0.759228609756098">References Atkinson, R., G. Bower, and E. Crot,hers. (1965). to Mathematical Learning Theory. New York: Wiley. Bates, E. and J. Elman. (1996). Learning rediscovered: A perspective on Saffran, Aslin, and Newport. 5294. R. (1985). acquisition of syntactic MA: MIT Press. Brill, E. (1993). Automatic grammar induction and parsing free text: a transformation-based approach. ACL Annual Meeting. R. (1973). first language. MA: Harvard University Press. R. and F. Mostellar. models for York: Wiley. E. (1995). language learning. Cambridge, MA: MIT Press. N. (1975). on language. New York: Pantheon. J.-P. (1983). Neuronal. Fayard. Clahsen, H. (1986). Verbal inflections in German child language: Acquisition of agreement markings the functions they encode. 79- 121. Clark, R. (1992). The selection of syntactic knowl- Acquisition 83-149. CraM, S. and M. Nakayama (1987). Structure dein grammar formation. 522- 543. Dresher, E. (1999). Charting the learning path: cues parameter setting. Inquiry 27-67. G. (1987). Darwinism: The theof neuronal group selection. York: Basic Books. J. D. (1998). Unambiguous triggers. Lin- Inquiry 1-36. E. and K. Wexler (1994). Triggers. Linguis- Inquiry 355-407. Haegeman, L. (1994). Root infinitives, clitics, and</note>
<abstract confidence="0.941919625">structures. Acquisition. Hubel, D. and T. Wiesel (1962). Receptive fields, binocular interaction and functional architecture in cat&apos;s visual cortex. of Physiology 106-54. N. (1986) acquisition and the theof parameters. Dordrecht. J. and P. Resnik (eds.) (1996). balancact. MA: MIT Press. Kroch, A. (1989). Reflexes of grammar in patterns language change. variation and change 1: 199-244. Lewontin, R. (1983). The organism as the subject object of evolution. 65-82. de Marcken, C. (1996). Unsupervised language acquisition. Ph.D. dissertation, MIT.</abstract>
<note confidence="0.772278846153846">434 MacWhinney, B. and C. Snow (1985). The Child Date Exchange System. of Child 271-296. K. and M. Thathachar (1989). Cliffs, NJ: Prentice Hall. Niyogi, P. and R. Berwick (1996). A language learnmodel for finite parameter space. 162-193. A. (1992). acquisition and and syntactic theory: a comparative analysis of French English child grammar. Kluwer. Pinker, S. (1979). Formal models of language learn- 217-283. S. (1984). learnability and landevelopment. MA: Harvard University Press. Seidenberg, M. (1997). Language acquisition and use: Learning and applying probabilistic con- 1599-1604. Stolcke, A. (1994) Bayesian Learning of Probabilistic Language Models. Ph.D. thesis, University of California at Berkeley, Berkeley, CA. Valian, V. (1991). Syntactic subjects in the early of American and Italian children. 40: 21-82. Wexler, K. (1994). Optional infinitives, head movement, and the economy of derivation in child language. In Lightfoot, D. and N. Hornstein (eds.) movement. Cambridge University Press. K. and P. Culicover (1980). princiof language acquisition. MA: MIT Press. Wijnen, F. (1999). Verb placement in Dutch child language: A longitudinal analysis. Ms. University of Utrecht. Yang, C. (1999). The variational dynamics of natural language: Acquisition and use. Technical report,</note>
<affiliation confidence="0.752449">MIT AT Lab.</affiliation>
<address confidence="0.68056">435</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Atkinson</author>
<author>G Bower</author>
<author>E Crot</author>
<author>hers</author>
</authors>
<title>An Introduction to Mathematical Learning Theory.</title>
<date>1965</date>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="12287" citStr="Atkinson et al. (1965)" startWordPosition="1927" endWordPosition="1930">obability pi, selects a grammar Gi • if s E Pi = Pi + 7(1 — Pt) t Pi = (1 — -Y)P.7 if j • if s G, fp=(1 — 1&apos;)Pi Comment: The algorithm is the Linear rewardpenalty (LR_p) scheme (Bush and Mostellar, 1958), one of the earliest and most extensively studied stochastic algorithms in the psychology of learning. It is real-time and on-line, and thus reflects the rather limited computational capacity of the child language learner, by avoiding sophisticated data processing and the need for a large memory to store previously seen examples. Many variants and generalizations of this scheme are studied in Atkinson et al. (1965), and their thorough mathematical treatments can be found in Narendra and Thathachar (1989). The algorithm operates in a selectionist manner: grammars that succeed in analyzing input sentences are rewarded, and those that fail are punished. In addition to the psychological evidence for such a scheme in animal and human learning, there is neurological evidence (Hubei and Wiesel, 1962; Changeux, 1983; Edelman, 1987; inter alia) that the development of neural substrate is guided by the exposure to specific stimulus in the environment in a Darwinian selectionist fashion. 2.4 A Convergence Proof Fo</context>
</contexts>
<marker>Atkinson, Bower, Crot, hers, 1965</marker>
<rawString>Atkinson, R., G. Bower, and E. Crot,hers. (1965). An Introduction to Mathematical Learning Theory. New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bates</author>
<author>J Elman</author>
</authors>
<title>Learning rediscovered: A perspective on</title>
<date>1996</date>
<journal>Saffran, Aslin, and Newport. Science</journal>
<volume>274</volume>
<pages>5294</pages>
<contexts>
<context position="2689" citStr="Bates and Elman (1996)" startWordPosition="394" endWordPosition="397"> NSF graduate fellowship. It is worth noting that the developmental compatibility condition has been largely ignored in the formal studies of language acquisition. In the rest of this section, I show that if this condition is taken seriously, previous models of language acquisition have difficulties explaining certain developmental facts in child language. 1.1 Against Statistical Learning An empiricist approach to language acquisition has (re)gained popularity in computational linguistics and cognitive science; see Stolcke (1994), Charniak (1995), Klavans and Resnik (1996), de Marcken (1996), Bates and Elman (1996), Seidenberg (1997), among numerous others. The child is viewed as an inductive and &amp;quot;generalized&amp;quot; data processor such as a neural network, designed to derive structural regularities from the statistical distribution of patterns in the input data without prior (innate) specific knowledge of natural language. Most concrete proposals of statistical learning employ expensive and specific computational procedures such as compression, Bayesian inferences, propagation of learning errors, and usually require a large corpus of (sometimes pre-processed) data. These properties immediately challenge the p</context>
</contexts>
<marker>Bates, Elman, 1996</marker>
<rawString>Bates, E. and J. Elman. (1996). Learning rediscovered: A perspective on Saffran, Aslin, and Newport. Science 274: 5294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
</authors>
<title>The acquisition of syntactic knowledge.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="7732" citStr="Berwick, 1985" startWordPosition="1187" endWordPosition="1188"> attested in child language (Chomsky, 1975; Crain and Nakayama, 1987; Crain, 1991): that is, children are never seen to produce sentences like: * Is the cat that the dog t chasing is scared? where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;), instead of the auxiliary following the subject of the sentence (here, the second &amp;quot;is&amp;quot; in the sentence). Acquisition findings like these lead linguists to postulate that the human language capacity is constrained in a finite prior space, the Universal Grammar (UG). Previous models of language acquisition in the UG framework (Wexler and Culicover, 1980; Berwick, 1985; Gibson and Wexler, 1994) are transformational, borrowing a term from evolution (Lewontin, 1983), in the sense that the learner moves from one hypothesis/grammar to another as input sentences are processed.1 Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be described by a</context>
<context position="11416" citStr="Berwick, 1985" startWordPosition="1772" endWordPosition="1773">ince the onset of language acquisition. We say that Definition: Learning converges if Ve,0 &lt;C &lt;1,VG,,jp,(E,t +1)— p,(E,t)1&lt; That is, learning converges when the composition and distribution of the grammar population are stabilized. Particularly, in a monolingual environment ET in which a target grammar T is used, we say that learning converges to T if limt, pT(ET,t) = 1. 2.3 A Learning Algorithm Write E s to indicate that a sentence s is an utterance in the linguistic environment E. Write s E G if a grammar G can analyze s, which, in a narrow sense, is parsability (Wexler and Culicover, 1980; Berwick, 1985). Suppose that there are altogether N grammars in the population. For simplicity, write pi for pi(E,t) at time t, and p&apos;,„ for pi(E, t+1) at time t + 1. Learning takes place as follows: The Algorithm: Given an input sentence s, the child with the probability pi, selects a grammar Gi • if s E Pi = Pi + 7(1 — Pt) t Pi = (1 — -Y)P.7 if j • if s G, fp=(1 — 1&apos;)Pi Comment: The algorithm is the Linear rewardpenalty (LR_p) scheme (Bush and Mostellar, 1958), one of the earliest and most extensively studied stochastic algorithms in the psychology of learning. It is real-time and on-line, and thus reflec</context>
</contexts>
<marker>Berwick, 1985</marker>
<rawString>Berwick, R. (1985). The acquisition of syntactic knowledge. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: a transformation-based approach.</title>
<date>1993</date>
<journal>ACL Annual Meeting.</journal>
<marker>Brill, 1993</marker>
<rawString>Brill, E. (1993). Automatic grammar induction and parsing free text: a transformation-based approach. ACL Annual Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brown</author>
</authors>
<title>A first language.</title>
<date>1973</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="8374" citStr="Brown, 1973" startWordPosition="1285" endWordPosition="1286"> transformational, borrowing a term from evolution (Lewontin, 1983), in the sense that the learner moves from one hypothesis/grammar to another as input sentences are processed.1 Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be described by any single adult grammar (e.g. Brown, 1973); and (b) the smoothness of language development (e.g. Pinker, 1984; Valiant, 1991; inter alia), whereby the child gradually converges to the target grammar, rather than the abrupt jumps that would be expected from binary changes in hypotheses/grammars. Having noted the inadequacies of the previous approaches to language acquisition, we will propose a theory that aims to meet language learnability and language development conditions simultaneously. Our theory draws inspirations from Darwinian evolutionary biology. 2 A Selectionist Model of Language Acquisition 2.1 The Dynamics of Darwinian Evo</context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Brown, R. (1973). A first language. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Bush</author>
<author>F Mostellar</author>
</authors>
<title>Stochastic models for learning.</title>
<publisher>Wiley.</publisher>
<location>New York:</location>
<marker>Bush, Mostellar, </marker>
<rawString>Bush, R. and F. Mostellar. Stochastic models for learning. New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical language learning.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="2619" citStr="Charniak (1995)" startWordPosition="385" endWordPosition="386">smith for comments and discussion. This work is supported by an NSF graduate fellowship. It is worth noting that the developmental compatibility condition has been largely ignored in the formal studies of language acquisition. In the rest of this section, I show that if this condition is taken seriously, previous models of language acquisition have difficulties explaining certain developmental facts in child language. 1.1 Against Statistical Learning An empiricist approach to language acquisition has (re)gained popularity in computational linguistics and cognitive science; see Stolcke (1994), Charniak (1995), Klavans and Resnik (1996), de Marcken (1996), Bates and Elman (1996), Seidenberg (1997), among numerous others. The child is viewed as an inductive and &amp;quot;generalized&amp;quot; data processor such as a neural network, designed to derive structural regularities from the statistical distribution of patterns in the input data without prior (innate) specific knowledge of natural language. Most concrete proposals of statistical learning employ expensive and specific computational procedures such as compression, Bayesian inferences, propagation of learning errors, and usually require a large corpus of (somet</context>
</contexts>
<marker>Charniak, 1995</marker>
<rawString>Charniak, E. (1995). Statistical language learning. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Reflections on language.</title>
<date>1975</date>
<publisher>Pantheon.</publisher>
<location>New York:</location>
<contexts>
<context position="7161" citStr="Chomsky, 1975" startWordPosition="1092" endWordPosition="1093">ere are logically possible and computationally simple inductive rules to describe the data that are never attested in child language. Consider the following well-known example. Forming a question in English involves inversion of the auxiliary verb and the subject: Is the man t tall? where &amp;quot;is&amp;quot; has been fronted from the position t, the position it assumes in a declarative sentence. A possible inductive rule to describe the above sentence is this: front the first auxiliary verb in the sentence. This rule, though logically possible and computationally simple, is never attested in child language (Chomsky, 1975; Crain and Nakayama, 1987; Crain, 1991): that is, children are never seen to produce sentences like: * Is the cat that the dog t chasing is scared? where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;), instead of the auxiliary following the subject of the sentence (here, the second &amp;quot;is&amp;quot; in the sentence). Acquisition findings like these lead linguists to postulate that the human language capacity is constrained in a finite prior space, the Universal Grammar (UG). Previous models of language acquisition in the UG framework (Wexler and Culicover, 1980; Berwick, 1985; Gibson and Wexler, 1994) ar</context>
</contexts>
<marker>Chomsky, 1975</marker>
<rawString>Chomsky, N. (1975). Reflections on language. New York: Pantheon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-P Changeux</author>
</authors>
<title>L&apos;Homme Neuronal.</title>
<date>1983</date>
<location>Paris: Fayard.</location>
<contexts>
<context position="12688" citStr="Changeux, 1983" startWordPosition="1993" endWordPosition="1994">language learner, by avoiding sophisticated data processing and the need for a large memory to store previously seen examples. Many variants and generalizations of this scheme are studied in Atkinson et al. (1965), and their thorough mathematical treatments can be found in Narendra and Thathachar (1989). The algorithm operates in a selectionist manner: grammars that succeed in analyzing input sentences are rewarded, and those that fail are punished. In addition to the psychological evidence for such a scheme in animal and human learning, there is neurological evidence (Hubei and Wiesel, 1962; Changeux, 1983; Edelman, 1987; inter alia) that the development of neural substrate is guided by the exposure to specific stimulus in the environment in a Darwinian selectionist fashion. 2.4 A Convergence Proof For simplicity but without loss of generality, assume that there are two grammars (N =- 2), the target grammar T1 and a pretender T2. The results presented here generalize to the N-grammar case; see Narendra and Thathachar (1989). Definition: The penalty probability of grammar T, in a linguistic environment E is ci = Pr(s OTi E s) In other words, cj represents the probability that the grammar T, fail</context>
</contexts>
<marker>Changeux, 1983</marker>
<rawString>Changeux, J.-P. (1983). L&apos;Homme Neuronal. Paris: Fayard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Clahsen</author>
</authors>
<title>Verbal inflections in German child language: Acquisition of agreement markings and the functions they encode.</title>
<date>1986</date>
<journal>Linguistics</journal>
<volume>24</volume>
<pages>79--121</pages>
<contexts>
<context position="16075" citStr="Clahsen, 1986" startWordPosition="2590" endWordPosition="2591">f learners, and formalize Kroch&apos;s idea of grammar competition over time. Comment 2: In the present model, one can directly measure the rate of change in the weight of the target grammar, and compare with developmental findings. Suppose T1 is the target grammar, hence Cl = 0. The expected increase of pi, 41 is computed as follows: E[APi] = c2P1P2 (3) Since p2 = 1 — pi, Api [3] is obviously a quadratic function of pi (t). Hence, the growth of pi will produce the familiar S-shape curve familiar in the psychology of learning. There is evidence for an S-shape pattern in child language development (Clahsen, 1986; Wijnen, 1999; inter alia), which, if true, suggests that a selectionist learning algorithm adopted here might indeed be what the child learner employs. 2.5 Unambiguous Evidence is Unnecessary One way to ensure convergence is to assume the existence of unambiguous evidence (cf. Fodor, 1998): sentences that are only compatible with the target grammar but not with any other grammar. Unambiguous evidence is, however, not necessary for the proposed model to converge. It follows from the theorem [1] that even if no evidence can unambiguously identify the target grammar from its competitors, it is </context>
<context position="25515" citStr="Clahsen, 1986" startWordPosition="4114" endWordPosition="4115">meter in Dutch is a relatively late phenomenon. Furthermore, because the frequency (1.3%) of Dutch OVS sentences is comparable to the frequency (1%) of English expletive sentences, we expect that Dutch V2 grammar is successfully acquired roughly at the same time when English children have adult-level subject use (around age 3; Valian, 1991). Although I am not aware of any report on the timing of the correct setting of the Dutch V2 parameter, there is evidence in the acquisition of German, a similar lan_ guage, that children are considered to have successfully acquired V2 by the 36-39th month (Clahsen, 1986). Under the model developed here, this is not an coincidence. 4 Conclusion To capitulate, this paper first argues that considerations of language development must be taken seriously to evaluate computational models of language acquisition. Once we do so, both statistical learning approaches and traditional UG-based learnability studies are empirically inadequate. We proposed an alternative model which views language acquisition as a selectionist process in which grammars form a population and compete to match linguistic- expressions present in the environment. The course and outcome of acquisi</context>
</contexts>
<marker>Clahsen, 1986</marker>
<rawString>Clahsen, H. (1986). Verbal inflections in German child language: Acquisition of agreement markings and the functions they encode. Linguistics 24: 79-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Clark</author>
</authors>
<title>The selection of syntactic knowledge.</title>
<date>1992</date>
<journal>Language Acquisition</journal>
<volume>2</volume>
<pages>83--149</pages>
<contexts>
<context position="13670" citStr="Clark, 1992" startWordPosition="2155" endWordPosition="2156">o the N-grammar case; see Narendra and Thathachar (1989). Definition: The penalty probability of grammar T, in a linguistic environment E is ci = Pr(s OTi E s) In other words, cj represents the probability that the grammar T, fails to analyze an incoming sentence s and gets punished as a result. Notice that the penalty probability, essentially a fitness measure of individual grammars, is an intrinsic property of a UG-defined grammar relative to a particular linguistic environment E, determined by the distributional patterns of linguistic expressions in E. It is not explicitly computed, as in (Clark, 1992) which uses the Genetic Algorithm (GA).2 The main result is as follows: Theorem: C2 lim pi(t) = if Ii — -y(ci +c2) l&lt; 1 (1) t--000 c1+ c2 Proof sketch: Computing E[pi(t + 1) I pi (t)] as a function of pi (t) and taking expectations on both 2Clark&apos;s model and the present one share an important feature: the outcome of acquisition is determined by the differential compatibilities of individual grammars. The choice of the GA introduces various psychological and linguistic assumptions that can not be justified; see Dresher (1999) and Yang (1999). Furthermore, no formal proof of convergence is given</context>
<context position="17893" citStr="Clark, 1992" startWordPosition="2881" endWordPosition="2882">hus, no unambiguous evidence appears to exist. However, if SVO, OVS, and XVSO patterns appear in the input data at positive frequencies, the German grammar has a higher overall &amp;quot;fitness value&amp;quot; than other grammars by the virtue of being compatible with all input sentences. As a result, German will eventually eliminate competing grammars. 2.6 Learning in a Parametric Space Suppose that natural language grammars vary in a parametric space, as cross-linguistic studies suggest.3 We can then study the dynamical behaviors of grammar classes that are defined in these parametric dimensions. Following (Clark, 1992), we say that a sentence s expresses a parameter a if a grammar must have set a to some definite value in order to assign a well-formed representation to s. Convergence to the target value of a can be ensured by the existence of evidence (s) defined in the sense of parameter expression. The convergence to a single grammar can then be viewed as the intersection of parametric grammar classes, converging in parallel to the target values of their respective parameters. 3 Some Developmental Predictions The present model makes two predictions that cannot be made in the standard transformational theo</context>
</contexts>
<marker>Clark, 1992</marker>
<rawString>Clark, R. (1992). The selection of syntactic knowledge. Language Acquisition 2: 83-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S CraM</author>
<author>M Nakayama</author>
</authors>
<title>Structure dependency in grammar formation.</title>
<date>1987</date>
<journal>Language</journal>
<volume>63</volume>
<pages>522--543</pages>
<marker>CraM, Nakayama, 1987</marker>
<rawString>CraM, S. and M. Nakayama (1987). Structure dependency in grammar formation. Language 63: 522-543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dresher</author>
</authors>
<title>Charting the learning path: cues to parameter setting.</title>
<date>1999</date>
<journal>Linguistic Inquiry</journal>
<volume>30</volume>
<pages>27--67</pages>
<publisher>Basic Books.</publisher>
<location>Edelman, G.</location>
<contexts>
<context position="14200" citStr="Dresher (1999)" startWordPosition="2247" endWordPosition="2248">s of linguistic expressions in E. It is not explicitly computed, as in (Clark, 1992) which uses the Genetic Algorithm (GA).2 The main result is as follows: Theorem: C2 lim pi(t) = if Ii — -y(ci +c2) l&lt; 1 (1) t--000 c1+ c2 Proof sketch: Computing E[pi(t + 1) I pi (t)] as a function of pi (t) and taking expectations on both 2Clark&apos;s model and the present one share an important feature: the outcome of acquisition is determined by the differential compatibilities of individual grammars. The choice of the GA introduces various psychological and linguistic assumptions that can not be justified; see Dresher (1999) and Yang (1999). Furthermore, no formal proof of convergence is given. if j 431 sides give E[pi(t + 1) = [1 — 01(ci + c2)]E[pi (t)] + lic2 (2) Solving [2] yields [1]. Comment 1: It is easy to see that pi —■ 1 (and P2 —&gt; 0) when ci = 0 and c2 &gt; 0; that is, the learner converges to the target grammar T1, which has a penalty probability of 0, by definition, in a monolingual environment. Learning is robust. Suppose that there is a small amount of noise in the input, i.e. sentences such as speaker errors which are not compatible with the target grammar. Then ci &gt; 0. If ci &lt; c2, convergence to Ti i</context>
</contexts>
<marker>Dresher, 1999</marker>
<rawString>Dresher, E. (1999). Charting the learning path: cues to parameter setting. Linguistic Inquiry 30: 27-67. Edelman, G. (1987). Neural Darwinism: The theory of neuronal group selection. New York: Basic Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<title>Unambiguous triggers.</title>
<date>1998</date>
<journal>Linguistic Inquiry</journal>
<volume>29</volume>
<pages>1--36</pages>
<contexts>
<context position="16367" citStr="Fodor, 1998" startWordPosition="2636" endWordPosition="2637">ease of pi, 41 is computed as follows: E[APi] = c2P1P2 (3) Since p2 = 1 — pi, Api [3] is obviously a quadratic function of pi (t). Hence, the growth of pi will produce the familiar S-shape curve familiar in the psychology of learning. There is evidence for an S-shape pattern in child language development (Clahsen, 1986; Wijnen, 1999; inter alia), which, if true, suggests that a selectionist learning algorithm adopted here might indeed be what the child learner employs. 2.5 Unambiguous Evidence is Unnecessary One way to ensure convergence is to assume the existence of unambiguous evidence (cf. Fodor, 1998): sentences that are only compatible with the target grammar but not with any other grammar. Unambiguous evidence is, however, not necessary for the proposed model to converge. It follows from the theorem [1] that even if no evidence can unambiguously identify the target grammar from its competitors, it is still possible to ensure convergence as long as all competing grammars fail on some proportion of input sentences; i.e. they all have positive penalty probabilities. Consider the acquisition of the target, a German V2 grammar, in a population of grammars below: 1. German: SVO, OVS, XVSO 2. E</context>
</contexts>
<marker>Fodor, 1998</marker>
<rawString>Fodor, J. D. (1998). Unambiguous triggers. Linguistic Inquiry 29: 1-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<date>1994</date>
<journal>Triggers. Linguistic Inquiry</journal>
<volume>25</volume>
<pages>355--407</pages>
<contexts>
<context position="7758" citStr="Gibson and Wexler, 1994" startWordPosition="1189" endWordPosition="1192">ild language (Chomsky, 1975; Crain and Nakayama, 1987; Crain, 1991): that is, children are never seen to produce sentences like: * Is the cat that the dog t chasing is scared? where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;), instead of the auxiliary following the subject of the sentence (here, the second &amp;quot;is&amp;quot; in the sentence). Acquisition findings like these lead linguists to postulate that the human language capacity is constrained in a finite prior space, the Universal Grammar (UG). Previous models of language acquisition in the UG framework (Wexler and Culicover, 1980; Berwick, 1985; Gibson and Wexler, 1994) are transformational, borrowing a term from evolution (Lewontin, 1983), in the sense that the learner moves from one hypothesis/grammar to another as input sentences are processed.1 Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be described by any single adult grammar (e</context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>Gibson, E. and K. Wexler (1994). Triggers. Linguistic Inquiry 25: 355-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Haegeman</author>
</authors>
<title>Root infinitives, clitics, and truncated structures.</title>
<date>1994</date>
<journal>Language</journal>
<volume>160</volume>
<pages>106--54</pages>
<contexts>
<context position="5693" citStr="Haegeman, 1994" startWordPosition="862" endWordPosition="863">site of the actual findings in child language. Further evidence against statistical learning comes from the Root Infinitive (RI) stage (Wexler, 1994; inter alia) in children acquiring certain languages. Children in the RI stage produce a large number of sentences where matrix verbs are not finite — ungrammatical in adult language and thus appearing infrequently in the primary linguistic data if at all. It is not clear how a statistical learner will induce non-existent patterns from the training corpus. In addition, in the acquisition of verb-second (V2) in Germanic grammars, it is known (e.g. Haegeman, 1994) that at an early stage, children use a large proportion (50%) of verb-initial (V1) sentences, a marked pattern that appears only sparsely in adult speech. Again, an inductive learner purely driven by corpus data has no explanation for these disparities between child and adult languages. Empirical evidence as such poses a serious problem for the statistical learning approach. It seems a mistake to view language acquisition as an inductive procedure that constructs linguistic knowledge, directly and exclusively, from the distributions of input data. 1.2 The Transformational Approach Another lea</context>
<context position="19772" citStr="Haegeman, 1994" startWordPosition="3183" endWordPosition="3185">ate of development is determined by the penalty probabilities of competing grammars relative to the input data in the linguistic environment [3]. In this paper, we present longitudinal evidence concerning the prediction in (2).4 To evaluate developmental predictions, we must estimate the the penalty probabilities of the competing grammars in a particular linguistic environment. Here we examine the developmental rate of French verb placement, an early acquisition (Pierce, 1992), that of English subject use, a late acquisition (Valian, 1991), that of Dutch V2 parameter, also a late acquisition (Haegeman, 1994). Using the idea of parameter expression (section 2.6), we estimate the frequency of sentences that unambiguously identify the target value of a parameter. For example, sentences that contain finite verbs preceding adverb or negation (&amp;quot;Jean voit souvent/pas Marie&amp;quot;) are unambiguous indication for the [-1-1 value of the verb raising parameter. A grammar with the [-] value for this parameter is incompatible with such sentences and if probabilistically selected for the learner for grammatical analysis, will be punished as a result. Based on the CHILDES corpus, we estimate that such sentences const</context>
</contexts>
<marker>Haegeman, 1994</marker>
<rawString>Haegeman, L. (1994). Root infinitives, clitics, and truncated structures. Language Acquisition. Hubel, D. and T. Wiesel (1962). Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex. Journal of Physiology 160: 106-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Hyams</author>
</authors>
<title>Language acquisition and the theory of parameters.</title>
<date>1986</date>
<location>Reidel: Dordrecht.</location>
<contexts>
<context position="22094" citStr="Hyams, 1986" startWordPosition="3558" endWordPosition="3559"> the other hand, have the option of dropping the subject. Therefore, sentences with an overt subject are not necessarily useful in distinguishing English 41n Yang (1999), we show that a child learner, en route to her target grammar, entertains multiple grammars. For example, a significant portion of English child language shows characteristics of a topic-drop optional subject grammar like Chinese, before they learn that subject use in English is obligatory at around the 3rd birthday. from optional subject languages.&apos; However, there exists a certain type of English sentence that is indicative (Hyams, 1986): There is a man in the room. Are there toys on the floor? The subject of these sentences is &amp;quot;there&amp;quot;, a nonreferential lexical item that is present for purely structural reasons — to satisfy the requirement in English that the pre-verbal subject position must be filled. Optional subject languages do not have this requirement, and do not have expletive-subject sentences. Expletive sentences therefore express the [+] value of the subject parameter. Based on the CHILDES corpus, we estimate that expletive sentences constitute 1% of all English adult utterances to children. Note that before the lea</context>
</contexts>
<marker>Hyams, 1986</marker>
<rawString>Hyams, N. (1986) Language acquisition and the theory of parameters. Reidel: Dordrecht.</rawString>
</citation>
<citation valid="true">
<title>The balancing act.</title>
<date>1996</date>
<editor>Klavins, J. and P. Resnik (eds.)</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="2646" citStr="(1996)" startWordPosition="390" endWordPosition="390">his work is supported by an NSF graduate fellowship. It is worth noting that the developmental compatibility condition has been largely ignored in the formal studies of language acquisition. In the rest of this section, I show that if this condition is taken seriously, previous models of language acquisition have difficulties explaining certain developmental facts in child language. 1.1 Against Statistical Learning An empiricist approach to language acquisition has (re)gained popularity in computational linguistics and cognitive science; see Stolcke (1994), Charniak (1995), Klavans and Resnik (1996), de Marcken (1996), Bates and Elman (1996), Seidenberg (1997), among numerous others. The child is viewed as an inductive and &amp;quot;generalized&amp;quot; data processor such as a neural network, designed to derive structural regularities from the statistical distribution of patterns in the input data without prior (innate) specific knowledge of natural language. Most concrete proposals of statistical learning employ expensive and specific computational procedures such as compression, Bayesian inferences, propagation of learning errors, and usually require a large corpus of (sometimes pre-processed) data. T</context>
</contexts>
<marker>1996</marker>
<rawString>Klavins, J. and P. Resnik (eds.) (1996). The balancing act. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kroch</author>
</authors>
<title>Reflexes of grammar in patterns of language change.</title>
<date>1989</date>
<journal>Language variation and change</journal>
<volume>1</volume>
<pages>199--244</pages>
<contexts>
<context position="15271" citStr="Kroch (1989)" startWordPosition="2447" endWordPosition="2448">e input, i.e. sentences such as speaker errors which are not compatible with the target grammar. Then ci &gt; 0. If ci &lt; c2, convergence to Ti is still ensured by [1]. Consider a non-uniform linguistic environment in which the linguistic evidence does not unambiguously identify any single grammar; an example of this is a population in contact with two languages (grammars), say, Ti and T2. Since ci &gt; 0 and c2 &gt; 0, [1] entails that pi and p2 reach a stable equilibrium at the end of language acquisition; that is, language learners are essentially bi-lingual speakers as a result of language contact. Kroch (1989) and his colleagues have argued convincingly that this is what happened in many cases of diachronic change. In Yang (1999), we have been able to extend the acquisition model to a population of learners, and formalize Kroch&apos;s idea of grammar competition over time. Comment 2: In the present model, one can directly measure the rate of change in the weight of the target grammar, and compare with developmental findings. Suppose T1 is the target grammar, hence Cl = 0. The expected increase of pi, 41 is computed as follows: E[APi] = c2P1P2 (3) Since p2 = 1 — pi, Api [3] is obviously a quadratic funct</context>
</contexts>
<marker>Kroch, 1989</marker>
<rawString>Kroch, A. (1989). Reflexes of grammar in patterns of language change. Language variation and change 1: 199-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lewontin</author>
</authors>
<title>The organism as the subject and object of evolution.</title>
<date>1983</date>
<journal>Scientia</journal>
<volume>118</volume>
<pages>65--82</pages>
<note>Ph.D. dissertation, MIT.</note>
<contexts>
<context position="7829" citStr="Lewontin, 1983" startWordPosition="1200" endWordPosition="1201">ildren are never seen to produce sentences like: * Is the cat that the dog t chasing is scared? where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;), instead of the auxiliary following the subject of the sentence (here, the second &amp;quot;is&amp;quot; in the sentence). Acquisition findings like these lead linguists to postulate that the human language capacity is constrained in a finite prior space, the Universal Grammar (UG). Previous models of language acquisition in the UG framework (Wexler and Culicover, 1980; Berwick, 1985; Gibson and Wexler, 1994) are transformational, borrowing a term from evolution (Lewontin, 1983), in the sense that the learner moves from one hypothesis/grammar to another as input sentences are processed.1 Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be described by any single adult grammar (e.g. Brown, 1973); and (b) the smoothness of language development (e.g. </context>
<context position="9069" citStr="Lewontin, 1983" startWordPosition="1386" endWordPosition="1387">91; inter alia), whereby the child gradually converges to the target grammar, rather than the abrupt jumps that would be expected from binary changes in hypotheses/grammars. Having noted the inadequacies of the previous approaches to language acquisition, we will propose a theory that aims to meet language learnability and language development conditions simultaneously. Our theory draws inspirations from Darwinian evolutionary biology. 2 A Selectionist Model of Language Acquisition 2.1 The Dynamics of Darwinian Evolution Essential to Darwinian evolution is the concept of variational thinking (Lewontin, 1983). First, differthat the transformational approach is not restricted to UG-based models; for example, Brill&apos;s influential work (1993) is a corpus-based model which successively revises a set of syntactic_ rules upon presentation of partially bracketed sentences. Note that however, the state of the learning system at any time is still a single set of rules, that is, a single &amp;quot;grammar&amp;quot;. 430 ences among individuals are viewed as &amp;quot;real&amp;quot;, as opposed to deviant from some idealized archetypes, as in pre-Darwinian thinking. Second, such differences result in variance in operative functions among indivi</context>
</contexts>
<marker>Lewontin, 1983</marker>
<rawString>Lewontin, R. (1983). The organism as the subject and object of evolution. Scientia 118: 65-82. de Marcken, C. (1996). Unsupervised language acquisition. Ph.D. dissertation, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
<author>C Snow</author>
</authors>
<title>The Child Language Date Exchange System.</title>
<date>1985</date>
<journal>Journal of Child Language</journal>
<volume>12</volume>
<pages>271--296</pages>
<contexts>
<context position="4661" citStr="MacWhinney and Snow, 1985" startWordPosition="696" endWordPosition="699">rbs: Jean voit souvent/pas Marie. Jean sees often/not Marie. This property of French is mastered as early as 429 the 20th month, as evidenced by the extreme rarity of incorrect verb placement in child speech (Pierce, 1992). In contrast, some aspects of language are acquired relatively late. For example, the requirement of using a sentential subject is not mastered by English children until as late as the 36th month (Valian, 1991), when English children stop producing a significant number of subjectless sentences. When we examine the adult speech to children (transcribed in the CHILDES corpus; MacWhinney and Snow, 1985), we find that more than 90% of English input sentences contain an overt subject, whereas only 7-8% of all French input sentences contain an inflected verb followed by negation/adverb. A statistical learner, one which builds knowledge purely on the basis of the distribution of the input data, predicts that English obligatory subject use should be learned (much) earlier than French verb placement — exactly the opposite of the actual findings in child language. Further evidence against statistical learning comes from the Root Infinitive (RI) stage (Wexler, 1994; inter alia) in children acquiring</context>
</contexts>
<marker>MacWhinney, Snow, 1985</marker>
<rawString>MacWhinney, B. and C. Snow (1985). The Child Language Date Exchange System. Journal of Child Language 12, 271-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Narendra</author>
<author>M Thathachar</author>
</authors>
<title>Learning automata. Englewood Cliffs,</title>
<date>1989</date>
<journal>Cognition</journal>
<volume>61</volume>
<pages>162--193</pages>
<publisher>Prentice</publisher>
<location>NJ:</location>
<contexts>
<context position="12378" citStr="Narendra and Thathachar (1989)" startWordPosition="1941" endWordPosition="1944">if j • if s G, fp=(1 — 1&apos;)Pi Comment: The algorithm is the Linear rewardpenalty (LR_p) scheme (Bush and Mostellar, 1958), one of the earliest and most extensively studied stochastic algorithms in the psychology of learning. It is real-time and on-line, and thus reflects the rather limited computational capacity of the child language learner, by avoiding sophisticated data processing and the need for a large memory to store previously seen examples. Many variants and generalizations of this scheme are studied in Atkinson et al. (1965), and their thorough mathematical treatments can be found in Narendra and Thathachar (1989). The algorithm operates in a selectionist manner: grammars that succeed in analyzing input sentences are rewarded, and those that fail are punished. In addition to the psychological evidence for such a scheme in animal and human learning, there is neurological evidence (Hubei and Wiesel, 1962; Changeux, 1983; Edelman, 1987; inter alia) that the development of neural substrate is guided by the exposure to specific stimulus in the environment in a Darwinian selectionist fashion. 2.4 A Convergence Proof For simplicity but without loss of generality, assume that there are two grammars (N =- 2), t</context>
</contexts>
<marker>Narendra, Thathachar, 1989</marker>
<rawString>Narendra, K. and M. Thathachar (1989). Learning automata. Englewood Cliffs, NJ: Prentice Hall. Niyogi, P. and R. Berwick (1996). A language learning model for finite parameter space. Cognition 61: 162-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pierce</author>
</authors>
<title>Language acquisition and and syntactic theory: a comparative analysis of French and English child grammar.</title>
<date>1992</date>
<journal>Cognition</journal>
<volume>7</volume>
<pages>217--283</pages>
<publisher>Kluwer. Pinker, S.</publisher>
<location>Boston:</location>
<contexts>
<context position="4257" citStr="Pierce, 1992" startWordPosition="633" endWordPosition="634"> would still face serious challenges from the important but often ignored requirement of developmental compatibility. One of the most significant findings in child language research of the past decade is that different aspects of syntactic knowledge are learned at different rates. For example, consider the placement of finite verb in French, where inflected verbs precede negation and adverbs: Jean voit souvent/pas Marie. Jean sees often/not Marie. This property of French is mastered as early as 429 the 20th month, as evidenced by the extreme rarity of incorrect verb placement in child speech (Pierce, 1992). In contrast, some aspects of language are acquired relatively late. For example, the requirement of using a sentential subject is not mastered by English children until as late as the 36th month (Valian, 1991), when English children stop producing a significant number of subjectless sentences. When we examine the adult speech to children (transcribed in the CHILDES corpus; MacWhinney and Snow, 1985), we find that more than 90% of English input sentences contain an overt subject, whereas only 7-8% of all French input sentences contain an inflected verb followed by negation/adverb. A statistic</context>
<context position="19638" citStr="Pierce, 1992" startWordPosition="3162" endWordPosition="3163">theories of grammar, e.g. GB, HPSG, LFG, TAG, have different ways of instantiating this idea. 432 2. Other things being equal, the rate of development is determined by the penalty probabilities of competing grammars relative to the input data in the linguistic environment [3]. In this paper, we present longitudinal evidence concerning the prediction in (2).4 To evaluate developmental predictions, we must estimate the the penalty probabilities of the competing grammars in a particular linguistic environment. Here we examine the developmental rate of French verb placement, an early acquisition (Pierce, 1992), that of English subject use, a late acquisition (Valian, 1991), that of Dutch V2 parameter, also a late acquisition (Haegeman, 1994). Using the idea of parameter expression (section 2.6), we estimate the frequency of sentences that unambiguously identify the target value of a parameter. For example, sentences that contain finite verbs preceding adverb or negation (&amp;quot;Jean voit souvent/pas Marie&amp;quot;) are unambiguous indication for the [-1-1 value of the verb raising parameter. A grammar with the [-] value for this parameter is incompatible with such sentences and if probabilistically selected for </context>
</contexts>
<marker>Pierce, 1992</marker>
<rawString>Pierce, A. (1992). Language acquisition and and syntactic theory: a comparative analysis of French and English child grammar. Boston: Kluwer. Pinker, S. (1979). Formal models of language learning. Cognition 7: 217-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Language learnability and language development.</title>
<date>1984</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="8441" citStr="Pinker, 1984" startWordPosition="1295" endWordPosition="1296">, in the sense that the learner moves from one hypothesis/grammar to another as input sentences are processed.1 Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be described by any single adult grammar (e.g. Brown, 1973); and (b) the smoothness of language development (e.g. Pinker, 1984; Valiant, 1991; inter alia), whereby the child gradually converges to the target grammar, rather than the abrupt jumps that would be expected from binary changes in hypotheses/grammars. Having noted the inadequacies of the previous approaches to language acquisition, we will propose a theory that aims to meet language learnability and language development conditions simultaneously. Our theory draws inspirations from Darwinian evolutionary biology. 2 A Selectionist Model of Language Acquisition 2.1 The Dynamics of Darwinian Evolution Essential to Darwinian evolution is the concept of variation</context>
</contexts>
<marker>Pinker, 1984</marker>
<rawString>Pinker, S. (1984). Language learnability and language development. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Seidenberg</author>
</authors>
<title>Language acquisition and use: Learning and applying probabilistic constraints.</title>
<date>1997</date>
<journal>Science</journal>
<volume>275</volume>
<pages>1599--1604</pages>
<contexts>
<context position="2708" citStr="Seidenberg (1997)" startWordPosition="398" endWordPosition="399">. It is worth noting that the developmental compatibility condition has been largely ignored in the formal studies of language acquisition. In the rest of this section, I show that if this condition is taken seriously, previous models of language acquisition have difficulties explaining certain developmental facts in child language. 1.1 Against Statistical Learning An empiricist approach to language acquisition has (re)gained popularity in computational linguistics and cognitive science; see Stolcke (1994), Charniak (1995), Klavans and Resnik (1996), de Marcken (1996), Bates and Elman (1996), Seidenberg (1997), among numerous others. The child is viewed as an inductive and &amp;quot;generalized&amp;quot; data processor such as a neural network, designed to derive structural regularities from the statistical distribution of patterns in the input data without prior (innate) specific knowledge of natural language. Most concrete proposals of statistical learning employ expensive and specific computational procedures such as compression, Bayesian inferences, propagation of learning errors, and usually require a large corpus of (sometimes pre-processed) data. These properties immediately challenge the psychological plausi</context>
</contexts>
<marker>Seidenberg, 1997</marker>
<rawString>Seidenberg, M. (1997). Language acquisition and use: Learning and applying probabilistic constraints. Science 275: 1599-1604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Bayesian Learning of Probabilistic Language Models.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="2602" citStr="Stolcke (1994)" startWordPosition="383" endWordPosition="384">n, and John Goldsmith for comments and discussion. This work is supported by an NSF graduate fellowship. It is worth noting that the developmental compatibility condition has been largely ignored in the formal studies of language acquisition. In the rest of this section, I show that if this condition is taken seriously, previous models of language acquisition have difficulties explaining certain developmental facts in child language. 1.1 Against Statistical Learning An empiricist approach to language acquisition has (re)gained popularity in computational linguistics and cognitive science; see Stolcke (1994), Charniak (1995), Klavans and Resnik (1996), de Marcken (1996), Bates and Elman (1996), Seidenberg (1997), among numerous others. The child is viewed as an inductive and &amp;quot;generalized&amp;quot; data processor such as a neural network, designed to derive structural regularities from the statistical distribution of patterns in the input data without prior (innate) specific knowledge of natural language. Most concrete proposals of statistical learning employ expensive and specific computational procedures such as compression, Bayesian inferences, propagation of learning errors, and usually require a large</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Stolcke, A. (1994) Bayesian Learning of Probabilistic Language Models. Ph.D. thesis, University of California at Berkeley, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Valian</author>
</authors>
<title>Syntactic subjects in the early speech of American and Italian children.</title>
<date>1991</date>
<journal>Cognition</journal>
<volume>40</volume>
<pages>21--82</pages>
<contexts>
<context position="4468" citStr="Valian, 1991" startWordPosition="669" endWordPosition="670">fferent aspects of syntactic knowledge are learned at different rates. For example, consider the placement of finite verb in French, where inflected verbs precede negation and adverbs: Jean voit souvent/pas Marie. Jean sees often/not Marie. This property of French is mastered as early as 429 the 20th month, as evidenced by the extreme rarity of incorrect verb placement in child speech (Pierce, 1992). In contrast, some aspects of language are acquired relatively late. For example, the requirement of using a sentential subject is not mastered by English children until as late as the 36th month (Valian, 1991), when English children stop producing a significant number of subjectless sentences. When we examine the adult speech to children (transcribed in the CHILDES corpus; MacWhinney and Snow, 1985), we find that more than 90% of English input sentences contain an overt subject, whereas only 7-8% of all French input sentences contain an inflected verb followed by negation/adverb. A statistical learner, one which builds knowledge purely on the basis of the distribution of the input data, predicts that English obligatory subject use should be learned (much) earlier than French verb placement — exactl</context>
<context position="19702" citStr="Valian, 1991" startWordPosition="3172" endWordPosition="3173">s of instantiating this idea. 432 2. Other things being equal, the rate of development is determined by the penalty probabilities of competing grammars relative to the input data in the linguistic environment [3]. In this paper, we present longitudinal evidence concerning the prediction in (2).4 To evaluate developmental predictions, we must estimate the the penalty probabilities of the competing grammars in a particular linguistic environment. Here we examine the developmental rate of French verb placement, an early acquisition (Pierce, 1992), that of English subject use, a late acquisition (Valian, 1991), that of Dutch V2 parameter, also a late acquisition (Haegeman, 1994). Using the idea of parameter expression (section 2.6), we estimate the frequency of sentences that unambiguously identify the target value of a parameter. For example, sentences that contain finite verbs preceding adverb or negation (&amp;quot;Jean voit souvent/pas Marie&amp;quot;) are unambiguous indication for the [-1-1 value of the verb raising parameter. A grammar with the [-] value for this parameter is incompatible with such sentences and if probabilistically selected for the learner for grammatical analysis, will be punished as a resu</context>
<context position="25243" citStr="Valian, 1991" startWordPosition="4065" endWordPosition="4066">herefore, OVS patterns are effectively unambiguous evidence (among the contenders) for the V2 parameter, which eventually drive SVO and VSO grammars out of the population. In the selectionist model, the rarity of OVS sentences predicts that the acquisition of the V2 parameter in Dutch is a relatively late phenomenon. Furthermore, because the frequency (1.3%) of Dutch OVS sentences is comparable to the frequency (1%) of English expletive sentences, we expect that Dutch V2 grammar is successfully acquired roughly at the same time when English children have adult-level subject use (around age 3; Valian, 1991). Although I am not aware of any report on the timing of the correct setting of the Dutch V2 parameter, there is evidence in the acquisition of German, a similar lan_ guage, that children are considered to have successfully acquired V2 by the 36-39th month (Clahsen, 1986). Under the model developed here, this is not an coincidence. 4 Conclusion To capitulate, this paper first argues that considerations of language development must be taken seriously to evaluate computational models of language acquisition. Once we do so, both statistical learning approaches and traditional UG-based learnabilit</context>
</contexts>
<marker>Valian, 1991</marker>
<rawString>Valian, V. (1991). Syntactic subjects in the early speech of American and Italian children. Cognition 40: 21-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wexler</author>
</authors>
<title>Optional infinitives, head movement, and the economy of derivation in child language.</title>
<date>1994</date>
<editor>In Lightfoot, D. and N. Hornstein (eds.)</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5226" citStr="Wexler, 1994" startWordPosition="787" endWordPosition="788"> the CHILDES corpus; MacWhinney and Snow, 1985), we find that more than 90% of English input sentences contain an overt subject, whereas only 7-8% of all French input sentences contain an inflected verb followed by negation/adverb. A statistical learner, one which builds knowledge purely on the basis of the distribution of the input data, predicts that English obligatory subject use should be learned (much) earlier than French verb placement — exactly the opposite of the actual findings in child language. Further evidence against statistical learning comes from the Root Infinitive (RI) stage (Wexler, 1994; inter alia) in children acquiring certain languages. Children in the RI stage produce a large number of sentences where matrix verbs are not finite — ungrammatical in adult language and thus appearing infrequently in the primary linguistic data if at all. It is not clear how a statistical learner will induce non-existent patterns from the training corpus. In addition, in the acquisition of verb-second (V2) in Germanic grammars, it is known (e.g. Haegeman, 1994) that at an early stage, children use a large proportion (50%) of verb-initial (V1) sentences, a marked pattern that appears only spa</context>
<context position="7758" citStr="Wexler, 1994" startWordPosition="1191" endWordPosition="1192">e (Chomsky, 1975; Crain and Nakayama, 1987; Crain, 1991): that is, children are never seen to produce sentences like: * Is the cat that the dog t chasing is scared? where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;), instead of the auxiliary following the subject of the sentence (here, the second &amp;quot;is&amp;quot; in the sentence). Acquisition findings like these lead linguists to postulate that the human language capacity is constrained in a finite prior space, the Universal Grammar (UG). Previous models of language acquisition in the UG framework (Wexler and Culicover, 1980; Berwick, 1985; Gibson and Wexler, 1994) are transformational, borrowing a term from evolution (Lewontin, 1983), in the sense that the learner moves from one hypothesis/grammar to another as input sentences are processed.1 Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be described by any single adult grammar (e</context>
</contexts>
<marker>Wexler, 1994</marker>
<rawString>Wexler, K. (1994). Optional infinitives, head movement, and the economy of derivation in child language. In Lightfoot, D. and N. Hornstein (eds.) Verb movement. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wexler</author>
<author>P Culicover</author>
</authors>
<title>Formal principles of language acquisition.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="7717" citStr="Wexler and Culicover, 1980" startWordPosition="1183" endWordPosition="1186">utationally simple, is never attested in child language (Chomsky, 1975; Crain and Nakayama, 1987; Crain, 1991): that is, children are never seen to produce sentences like: * Is the cat that the dog t chasing is scared? where the first auxiliary is fronted (the first &amp;quot;is&amp;quot;), instead of the auxiliary following the subject of the sentence (here, the second &amp;quot;is&amp;quot; in the sentence). Acquisition findings like these lead linguists to postulate that the human language capacity is constrained in a finite prior space, the Universal Grammar (UG). Previous models of language acquisition in the UG framework (Wexler and Culicover, 1980; Berwick, 1985; Gibson and Wexler, 1994) are transformational, borrowing a term from evolution (Lewontin, 1983), in the sense that the learner moves from one hypothesis/grammar to another as input sentences are processed.1 Learnability results can be obtained for some psychologically plausible algorithms (Niyogi and Berwick, 1996). However, the developmental compatibility condition still poses serious problems. Since at any time the state of the learner is identified with a particular grammar defined by UG, it is hard to explain (a) the inconsistent patterns in child language, which cannot be</context>
<context position="11400" citStr="Wexler and Culicover, 1980" startWordPosition="1768" endWordPosition="1771"> time variable t, the time since the onset of language acquisition. We say that Definition: Learning converges if Ve,0 &lt;C &lt;1,VG,,jp,(E,t +1)— p,(E,t)1&lt; That is, learning converges when the composition and distribution of the grammar population are stabilized. Particularly, in a monolingual environment ET in which a target grammar T is used, we say that learning converges to T if limt, pT(ET,t) = 1. 2.3 A Learning Algorithm Write E s to indicate that a sentence s is an utterance in the linguistic environment E. Write s E G if a grammar G can analyze s, which, in a narrow sense, is parsability (Wexler and Culicover, 1980; Berwick, 1985). Suppose that there are altogether N grammars in the population. For simplicity, write pi for pi(E,t) at time t, and p&apos;,„ for pi(E, t+1) at time t + 1. Learning takes place as follows: The Algorithm: Given an input sentence s, the child with the probability pi, selects a grammar Gi • if s E Pi = Pi + 7(1 — Pt) t Pi = (1 — -Y)P.7 if j • if s G, fp=(1 — 1&apos;)Pi Comment: The algorithm is the Linear rewardpenalty (LR_p) scheme (Bush and Mostellar, 1958), one of the earliest and most extensively studied stochastic algorithms in the psychology of learning. It is real-time and on-line,</context>
</contexts>
<marker>Wexler, Culicover, 1980</marker>
<rawString>Wexler, K. and P. Culicover (1980). Formal principles of language acquisition. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wijnen</author>
</authors>
<title>Verb placement in Dutch child language: A longitudinal analysis.</title>
<date>1999</date>
<institution>Ms. University of Utrecht.</institution>
<contexts>
<context position="16089" citStr="Wijnen, 1999" startWordPosition="2592" endWordPosition="2593"> formalize Kroch&apos;s idea of grammar competition over time. Comment 2: In the present model, one can directly measure the rate of change in the weight of the target grammar, and compare with developmental findings. Suppose T1 is the target grammar, hence Cl = 0. The expected increase of pi, 41 is computed as follows: E[APi] = c2P1P2 (3) Since p2 = 1 — pi, Api [3] is obviously a quadratic function of pi (t). Hence, the growth of pi will produce the familiar S-shape curve familiar in the psychology of learning. There is evidence for an S-shape pattern in child language development (Clahsen, 1986; Wijnen, 1999; inter alia), which, if true, suggests that a selectionist learning algorithm adopted here might indeed be what the child learner employs. 2.5 Unambiguous Evidence is Unnecessary One way to ensure convergence is to assume the existence of unambiguous evidence (cf. Fodor, 1998): sentences that are only compatible with the target grammar but not with any other grammar. Unambiguous evidence is, however, not necessary for the proposed model to converge. It follows from the theorem [1] that even if no evidence can unambiguously identify the target grammar from its competitors, it is still possible</context>
</contexts>
<marker>Wijnen, 1999</marker>
<rawString>Wijnen, F. (1999). Verb placement in Dutch child language: A longitudinal analysis. Ms. University of Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yang</author>
</authors>
<title>The variational dynamics of natural language: Acquisition and use.</title>
<date>1999</date>
<tech>Technical report, MIT AT Lab.</tech>
<contexts>
<context position="14216" citStr="Yang (1999)" startWordPosition="2250" endWordPosition="2251">ressions in E. It is not explicitly computed, as in (Clark, 1992) which uses the Genetic Algorithm (GA).2 The main result is as follows: Theorem: C2 lim pi(t) = if Ii — -y(ci +c2) l&lt; 1 (1) t--000 c1+ c2 Proof sketch: Computing E[pi(t + 1) I pi (t)] as a function of pi (t) and taking expectations on both 2Clark&apos;s model and the present one share an important feature: the outcome of acquisition is determined by the differential compatibilities of individual grammars. The choice of the GA introduces various psychological and linguistic assumptions that can not be justified; see Dresher (1999) and Yang (1999). Furthermore, no formal proof of convergence is given. if j 431 sides give E[pi(t + 1) = [1 — 01(ci + c2)]E[pi (t)] + lic2 (2) Solving [2] yields [1]. Comment 1: It is easy to see that pi —■ 1 (and P2 —&gt; 0) when ci = 0 and c2 &gt; 0; that is, the learner converges to the target grammar T1, which has a penalty probability of 0, by definition, in a monolingual environment. Learning is robust. Suppose that there is a small amount of noise in the input, i.e. sentences such as speaker errors which are not compatible with the target grammar. Then ci &gt; 0. If ci &lt; c2, convergence to Ti is still ensured </context>
<context position="21651" citStr="Yang (1999)" startWordPosition="3488" endWordPosition="3489"> always (immediately) precedes object, which give a very high (perhaps close to 100%, far greater than 8%, which is sufficient for a very early acquisition as in the case of French verb raising) rate of unambiguous evidence, sufficient to drive out other word order grammars very early on. Consider then the acquisition of the subject parameter in English, which requires a sentential subject. Languages like Italian, Spanish, and Chinese, on the other hand, have the option of dropping the subject. Therefore, sentences with an overt subject are not necessarily useful in distinguishing English 41n Yang (1999), we show that a child learner, en route to her target grammar, entertains multiple grammars. For example, a significant portion of English child language shows characteristics of a topic-drop optional subject grammar like Chinese, before they learn that subject use in English is obligatory at around the 3rd birthday. from optional subject languages.&apos; However, there exists a certain type of English sentence that is indicative (Hyams, 1986): There is a man in the room. Are there toys on the floor? The subject of these sentences is &amp;quot;there&amp;quot;, a nonreferential lexical item that is present for purel</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yang, C. (1999). The variational dynamics of natural language: Acquisition and use. Technical report, MIT AT Lab.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>