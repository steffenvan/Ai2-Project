<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.925636">
Chunking-based Chinese Word Tokenization
</title>
<author confidence="0.91326">
GuoDong ZHOU
</author>
<affiliation confidence="0.94206">
Institute for Infocomm Research
</affiliation>
<address confidence="0.974258">
21 Heng Mui Keng Terrace
Singapore, 119613
</address>
<email confidence="0.955144">
zhougd@i2r.a-star.edu.sg
</email>
<figure confidence="0.759777">
log (  |) log ( ) log
n n n
P T G = P T +
1 1 1 P T P G
( ) ( )
n n
⋅
1 1
Abstract
(Tn Gn )
P
1
, 1
</figure>
<bodyText confidence="0.9986858">
This paper introduces a Chinese word
tokenization system through HMM-based
chunking. Experiments show that such a
system can well deal with the unknown word
problem in Chinese word tokenization.
</bodyText>
<sectionHeader confidence="0.999389" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999886666666667">
Word Tokenization is regarded as one of major
bottlenecks in Chinese Language Processing.
Normally, word tokenization is implemented
through word segmentation in Chinese Language
Processing literature. This is also affected in the
title of this competition.
There exists two major problems in Chinese
word segmentation: ambiguity and unknown word
detection. While ngarm modeling and/or word
co-ocurrence has been successfully applied to deal
with ambiguity problem, unknown word detection
has become major bottleneck in word tokenization.
This paper proposes a HMM-based chunking
scheme to cope with unkown words in Chinese
word tokenization. The unknown word detection is
re-casted as chunking several words
(single-character word or multi-character word)
together to form a new word.
</bodyText>
<sectionHeader confidence="0.9764595" genericHeader="keywords">
2 HMM-based Chunking
2.1 HMM
</sectionHeader>
<bodyText confidence="0.870494285714286">
Given an input sequence G1n = g1g2 gn, the goal
of Chunking is to find a stochastic optimal tag
sequence T1n = t1t2 t that maximizes (Zhou and
n
Su 2000) (2-1)
The second term in (2-1) is the mutual
information between T and . In order to
</bodyText>
<equation confidence="0.994831">
n G1 n
1
</equation>
<bodyText confidence="0.9617345">
simplify the computation of this term, we assume
mutual information independence (2-2):
</bodyText>
<equation confidence="0.998966916666667">
n
MI T n G n
( 1 , 1 ) = ∑MI(ti , G1n ) or
i=1
P(
1n
G1)
T
,
= ∑= log
n n
⋅ i 1
</equation>
<bodyText confidence="0.915443">
That is, an individual tag is only dependent on the
token sequence G and independent on other tags
</bodyText>
<equation confidence="0.969307">
n
1
</equation>
<bodyText confidence="0.997254">
in the tag sequence T1n . This assumption is
reasonable because the dependence among the tags
in the tag sequence T has already been captured
</bodyText>
<equation confidence="0.8932549">
n
1
by the first term in equation (2-1). Applying it to
equation (2-1), we have (2-3):
n
n n
log P T G
(  |)
1 1
1
</equation>
<bodyText confidence="0.781501">
From equation (2-3), we can see that:
</bodyText>
<listItem confidence="0.999526">
• The first term can be computed by applying
chain rules. In ngram modeling, each tag is
assumed to be probabilistically dependent on
the N-1 previous tags.
• The second term is the summation of log
probabilities of all the individual tags.
log
• The third term corresponds to the “lexical”
component (dictionary) of the tagger.
</listItem>
<equation confidence="0.984536483870968">
P T P G
( ) ( )
1 1
n
)
P(ti , Gi
( i
) ( )
⋅ 1
P t P G
n
P log P(ti
1
(T
n )
1
)
n
=
log
i
=
+
n )
1
∑
log ( |
P t G
i
i
=
</equation>
<bodyText confidence="0.8548996">
We will not discuss either the first or the second
term further in this paper because ngram modeling
has been well studied in the literature. We will focus
n
on the third term∑= log (  |1 )
</bodyText>
<equation confidence="0.964947">
n
P ti G .
i 1
</equation>
<subsectionHeader confidence="0.999045">
2.2 Chinese Word Tokenization
</subsectionHeader>
<bodyText confidence="0.99998">
Given the previous HMM, for Chinese word
tokenization, we have (Zhou and Su 2002):
</bodyText>
<listItem confidence="0.973434">
• gi =&lt; pi, wi &gt; ; W1n = w1w2 w is the word
</listItem>
<equation confidence="0.6469435">
n n
sequence; P1 = p1p2 pn is the word
</equation>
<bodyText confidence="0.783161">
formation pattern sequence and is the word
pi
formation pattern of . Here consists of:
wi p i
o The percentage of w occurring as a whole
i
word (round to 10%)
o The percentage of w occurring at the
</bodyText>
<equation confidence="0.463">
i
</equation>
<bodyText confidence="0.8842045">
beginning of other words (round to 10%)
o The percentage of occurring at the end of
</bodyText>
<equation confidence="0.42221">
wi
</equation>
<bodyText confidence="0.907927">
other words (round to 10%)
o The length of wi
</bodyText>
<listItem confidence="0.942354333333333">
o The occurring frequence feature, which is set
to max(log(Frequence), 9 ).
• tag : Here, a word is regarded as a chunk
</listItem>
<bodyText confidence="0.973270769230769">
ti
(called &amp;quot;Word-Chunk&amp;quot;) and the tags are used to
bracket and differentiate various types of
Word-chunks. Chinese word tokenization can be
regarded as a bracketing process while
differentiation of different word types can help
the bracketing process. For convenience, here
the tag used in Chinese word tokenization is
called “Word-chunk tag”. The Word-chunk tag
ti is structural and consists of three parts:
o Boundary category (B): it is a set of four
values: 0,1,2,3, where 0 means that current
word is a whole entity and 1/2/3 means that
current word is at the beginning/in the
middle/at the end of a word.
o Word category (W): used to denote the class
of the word. In our system, word is classified
into two types: pure Chinese word type and
mixed word type (for example, including
English characters/Chinese
digits/Chinesenumbers).
o Word Formation Pattern(P): Because of
the limited number of boundary and word
categories, the word formation pattern is
added into the structural chunk tag to
represent more accurate models.
</bodyText>
<sectionHeader confidence="0.997866" genericHeader="method">
3 Context-dependent Lexicons
</sectionHeader>
<bodyText confidence="0.998557857142857">
The major problem with Chunking-based Chinese
word tokenization is how to effectively
approximate P(ti / Gn). This can be done by
adding lexical entries with more contextual
information into the lexicon Φ . In the following,
we will discuss five context-dependent lexicons
which consider different contextual information.
</bodyText>
<subsectionHeader confidence="0.8504585">
3.1 Context of current word formation pattern
and current word
</subsectionHeader>
<bodyText confidence="0.993436">
Here, we assume:
</bodyText>
<equation confidence="0.957296333333333">
/piwi) pi wi
=
P(ti /pi) pi wi
</equation>
<bodyText confidence="0.708657">
where
</bodyText>
<equation confidence="0.804941">
Φ = {piwi, piwi∃C} + {pi, pi∃C} and is a
pi wi
</equation>
<bodyText confidence="0.9838235">
word formation pattern and word pair existing in the
training data C .
</bodyText>
<subsectionHeader confidence="0.999832">
3.2 Context of previous word formation pattern
</subsectionHeader>
<bodyText confidence="0.9780395">
and current word formation pattern
Here, we assume :
</bodyText>
<equation confidence="0.99495725">
P(ti / G, )
=

P
P(ti /pi−1pi) pi−1pi
( / ) p p
t p
i i i − 1 i
</equation>
<bodyText confidence="0.787078">
where
</bodyText>
<equation confidence="0.997703">
Φ = {pi−1pi,pi−1pi∃C} + {pi,pi∃C} and
</equation>
<bodyText confidence="0.985799333333333">
pi−1pi is a pair of previous word formation pattern
and current word formation pattern existing in the
training data C .
</bodyText>
<equation confidence="0.404016363636364">
t G n )
P ( / 1
i

P(ti
∈ Φ
∉Φ
∈ Φ
∉Φ
accuracy by merging all the above
context-dependent lexicons in a single lexicon.
</equation>
<bodyText confidence="0.734841">
3.3 Context of previous word formation pattern,
previous word and current word formation
pattern
Here, we assume :
</bodyText>
<equation confidence="0.990360153846154">
P t p w p p w
( / )
i i− −
1 i i
1 i− −
1 i 1
= 

i/pi
pi
P(t
)
pi
</equation>
<bodyText confidence="0.9840925">
where
Φ _ {pi−1wi−1pi,pi−1wi−1pi∃C} + {pi,pi∃C} ,
where pi−1wi−1pi is a triple pattern existing in the
training corpus.
</bodyText>
<subsectionHeader confidence="0.941013">
3.4 Context of previous word formation pattern,
</subsectionHeader>
<bodyText confidence="0.875025666666667">
current word formation pattern and current
word
For a new lexical entry e , the effectiveness
i
is measured by the reduction in error which
results from adding the lexical entry to the lexicon :
</bodyText>
<equation confidence="0.918647333333333">
F Φ Error e − F Φ +∆Φ
( ) Error
i
</equation>
<bodyText confidence="0.6609409">
) is the chunking error number of the
lexical entry e for the old lexicon Φ and
i
Error is the chunking error number of the
FΦ+∆Φ e
( i)
lexical entry e for the new lexicon Φ+∆Φ
i
where ei ∈ ∆Φ ( ∆Φ is the list of new lexical
entries added to the old lexicon ). If
</bodyText>
<figure confidence="0.848004190476191">
Φ FΦ (ei ) &gt; 0,
we define the lexical entry e as positive for
i
)
P
(ti / G;

pi
∈ Φ
∉Φ
)
FΦ (ei
FΦ (ei )
FError e
Φ ( i
)
. Here,
( ei
)
lexicon Φ . Otherwise, the lexical entry e is
i
</figure>
<bodyText confidence="0.8287795">
negative for lexicon Φ .
Here, we assume :
</bodyText>
<equation confidence="0.963406555555556">
P(ti / G;
( / ) =



P t p p w p
( /
i i− 1 i i) i−1
P(ti / pi) pi−A
</equation>
<bodyText confidence="0.96823">
where
Φ _ {pi−1piwi,pi−1piwi∃C} + {pi,pi∃C} ,
where pi− 1pi wi is a triple pattern.
</bodyText>
<subsectionHeader confidence="0.788791">
3.5 Context of previous word formation pattern,
</subsectionHeader>
<bodyText confidence="0.9998278">
previous word, current word formation pattern
and current word
Here, the context of previous word formation
pattern, previous word, current word formation
pattern and current word is used as a lexical entry to
determine the current structural chunk tag and Φ _
{pi−1wi−1piwi,pi−1wi−1piwi∃C} + {pi,pi∃C} ,
where pi−1wi−1piwi is a pattern existing in the
training corpus. Due to memory limitation, only
lexical entries which occurs at least 3 times are kept.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
4 Error-Driven Learning
</sectionHeader>
<bodyText confidence="0.999937">
In order to reduce the size of lexicon effectively, an
error-driven learning approach is adopted to
examine the effectiveness of lexical entries and
make it possible to further improve the chunking
</bodyText>
<sectionHeader confidence="0.999195" genericHeader="method">
5 Implementation
</sectionHeader>
<bodyText confidence="0.966860714285715">
In training process, only the words occurs at least 5
times are kept in the training corpus and in the word
table while those less-freqently occurred words are
separated into short words (most of such short
words are single-character words) to simulate the
chunking. That is, those less-frequently words are
regarded as chunked from several short words.
In word tokenization process, the
Chunking-based Chinese word tokenization can be
implemented as follows:
1) Given an input sentence, a lattice of word and
word formation pattern pair is generated by
skimming the sentence from left-to-right,
looking up the word table to determine all the
possible words, and determining the word
formation pattern for each possible word.
2) Viterbi algorithm is applied to decode the
lattice to find the most possible tag sequence.
3) In this way, the given sentence is chunked into
words with word category information
discarded.
</bodyText>
<figure confidence="0.8675406">
pi
∈ Φ
wi
wi
∉Φ
</figure>
<sectionHeader confidence="0.97583" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.992381">
Table 1 shows the performance of our
chunking-based Chinese word tokenization in the
competition.
</bodyText>
<table confidence="0.998352555555555">
PK (closed, CTB (closed,
official) unofficial)
Precision 94.5 90.7
Recall 93.6 89.6
F 94.0 90.1
OOV 6.9 18.1
Recall on OOV 76.3 75.2
Recall on In-Voc 94.9 92.7
Speed on P1.8G 420 KB/min 390 KB/min
</table>
<bodyText confidence="0.9998625">
The most important advantage of
chunking-based Chinese word segmentation is the
ability to cope with the unknown words. Table 1
shows that about 75% of the unknown words can be
detected correctly using the chunking approach on
the PK and CTB corpus.
</bodyText>
<sectionHeader confidence="0.999412" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999889">
This paper proposes a HMM-based chunking
scheme to cope with the unkown words in Chinese
word tokenization. In the meantime, error-driven
learning is applied to effectively incorporate
various context-dependent information.
Experiments show that such a system can well deal
with the unknown word problem in Chinese word
tokenization.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999199857142857">
Rabiner L. 1989. A Tutorial on Hidden Markov
Models and Selected Applications in Speech
Recognition. IEEE 77(2), pages257-285.
Viterbi A.J. 1967. Error Bounds for Convolutional
Codes and an Asymptotically Optimum Decoding
Algorithm. IEEE Transactions on Information
Theory, IT 13(2), 260-269.
Zhou GuoDong and Su Jian. 2000. Error-driven
HMM-based Chunk Tagger with
Context-dependent Lexicon. Proceedings of the
Joint Conference on Empirical Methods on
Natural Language Processing and Very Large
Corpus (EMNLP/ VLC&apos;2000). Hong Kong, 7-8
Oct.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.089605">
<title confidence="0.997343">Chunking-based Chinese Word Tokenization</title>
<author confidence="0.570784">GuoDong</author>
<affiliation confidence="0.812484">Institute for Infocomm</affiliation>
<address confidence="0.8320475">21 Heng Mui Keng Singapore,</address>
<email confidence="0.508799">log(|)log()log</email>
<abstract confidence="0.864280733333333">n n n T G T 1 1P T P G ( ) ( ) n n ⋅ 1 1 Abstract ) P This paper introduces a Chinese word tokenization system through HMM-based chunking. Experiments show that such a system can well deal with the unknown word problem in Chinese word tokenization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.</title>
<date>1989</date>
<journal>IEEE</journal>
<volume>77</volume>
<issue>2</issue>
<pages>257--285</pages>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner L. 1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. IEEE 77(2), pages257-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory, IT</journal>
<volume>13</volume>
<issue>2</issue>
<pages>260--269</pages>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi A.J. 1967. Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm. IEEE Transactions on Information Theory, IT 13(2), 260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
</authors>
<title>Error-driven HMM-based Chunk Tagger with Context-dependent Lexicon.</title>
<date>2000</date>
<booktitle>Proceedings of the Joint Conference on Empirical Methods on Natural Language Processing and Very Large Corpus (EMNLP/ VLC&apos;2000). Hong Kong,</booktitle>
<pages>7--8</pages>
<marker>GuoDong, Jian, 2000</marker>
<rawString>Zhou GuoDong and Su Jian. 2000. Error-driven HMM-based Chunk Tagger with Context-dependent Lexicon. Proceedings of the Joint Conference on Empirical Methods on Natural Language Processing and Very Large Corpus (EMNLP/ VLC&apos;2000). Hong Kong, 7-8 Oct.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>