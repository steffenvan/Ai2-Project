<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994609">
A Structural Similarity Measure
</title>
<author confidence="0.930797">
Petr Homola and Vladislav Kubon
</author>
<affiliation confidence="0.769615">
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.6613055">
Malostranske ndmesti25
110 00 Praha 1, Czech republic
</address>
<email confidence="0.996968">
{homola,vk}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.979983" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939277777778">
This paper outlines a measure of lan-
guage similarity based on structural
similarity of surface syntactic depen-
dency trees. Unlike the more tradi-
tional string-based measures, this mea-
sure tries to reflect “deeper” correspon-
dences among languages. The develop-
ment of this measure has been inspired
by the experience from MT of syntac-
tically similar languages. This experi-
ence shows that the lexical similarity is
less important than syntactic similar-
ity. This claim is supported by a num-
ber of examples illustrating the prob-
lems which may arise when a measure
of language similarity relies too much
on a simple similarity of texts in differ-
ent languages.
</bodyText>
<sectionHeader confidence="0.996303" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999072113207547">
Although the similarity of natural languages is
in principal a very vague notion, the linguistic
literature seems to be full of claims classifying
two natural languages as being more or less
similar. These claims are in some cases a result
of a detailed comparative examination of lex-
ical and/or syntactic properties of languages
under question, in some cases they are based
on a very subjective opinion of the author, in
many other cases they reflect the application
of some mathematical formula on textual data
(a very nice example of such mathematical ap-
proach can be found at (Scannell, 2004)).
Especially in the last case the notion of lan-
guage similarity is very often confused with the
notion of text similarity. Even the well known
paper (Lebart and Rajman, 2000) deals more
with the text similarity than language similar-
ity. This general trend is quite understand-
able, the mathematical methods for measur-
ing text similarity are of a prominent impor-
tance especially for information retrieval and
similar fields. On the other hand, they con-
centrate too much on the surface similarity
of word forms and thus may not reflect the
similarity of languages properly. This paper
tries to advocate different approach, based on
the experience gained in MT experiments with
closely related (and similar) languages, where
it is possible to “measure” the similarity indi-
rectly by a complexity of modules we have to
use in order to achieve a reasonable transla-
tion quality. This experience led us to formu-
lating an evaluation measure trying to capture
not only textual, but also syntactic similarities
between natural languages.
2 Imperfections of measures based
on string similarity
There are many application areas in the NLP
in which it is useful to apply the measures ex-
ploiting the similarity of word forms (strings).
They serve very well for example for tasks
like spellchecking (where the choice of the best
candidates for correction of a spelling error is
typically based upon the Levenshtein metrics)
or estimating the similarity of a new source
sentence to those stored in the translation
memory of a Machine Aided Translation sys-
tem. They are a bit controversial in a “proper”
machine translation, where the popular BLEU
score (Papineni et al., 2002), although widely
accepted as a measure of translation accuracy,
seems to favor stochastic approaches based on
</bodyText>
<page confidence="0.984437">
91
</page>
<note confidence="0.695569">
Proceedings of the Workshop on Linguistic Distances, pages 91–99,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999927588235295">
an n-gram model over other MT methods (see
the results in (Nist, 2001)).
The controversies the BLEU score seems to
provoke arise due to the fact that the evalua-
tion of MT systems can be, in general, per-
formed from two different viewpoints. The
first one is that of a developer of such a sys-
tem, who needs to get a reliable feedback in
the process of development and debugging of
the system. The primary interest of such a
person is the grammar or dictionary coverage
and system performance and he needs a cheap,
fast and simple evaluation method in order to
allow frequent routine tests indicating the im-
provements of the system during the develop-
ment of the system.
The second viewpoint is that of a user, who
is primarily concerned with the capability of
the system to provide fast and reliable trans-
lation requiring as few post-editing efforts as
possible. The simplicity, speed and low costs
are not of such importance here. If the eval-
uation is performed only once, in the mo-
ment when the system is considered to be
ready, the evaluation method may even be rel-
atively complicated, expensive and slow. A
good example of such a complex measure is the
FEMTI framework (Framework for the Evalu-
ation of Machine Translation). The most com-
plete description of the FEMTI framework can
be found in (Hovy et al., 2002). Such mea-
sures are much more popular among transla-
tors than among language engineers and MT
systems developers.
If we aim at measuring the similarity of lan-
guages or language distances, our point of view
should be much more similar to that of a hu-
man translator than of a system developer, if
we’ll stick to our MT analogy. When looking
for clues concerning the desirable properties
of a language similarity (or distance) measure,
we can first try to formulate the reasons why
we consider the simple string-based (or word-
form-based) measures inadequate.
If we take into account a number of lan-
guages existing in the world, the number of
word forms existing in each of those languages
and a simple fact that a huge percentage of
those word forms is not longer than five or
six characters, it is quite clear that there is a
huge number of overlapping word forms which
have completely different meaning in all lan-
guages containing that particular word form.
Let us take for illustration some language pairs
of non-related languages.
For example for Czech and English (the lan-
guages very different with regard both to the
lexicon and syntax) we can find several exam-
ples of overlapping word forms. The English
word house means a duckling in Czech, the En-
glish indefinite article a is in Czech also very
frequent, because it represents a coordinating
conjunction and, while an is an archaic form
of a pronoun in Czech. On the other hand, if
we look at the identical (or nearly identical)
word forms in similar languages, we can find
many examples of totally different meaning.
For example, the word form zivot means life
in Czech and belly in Russian; godina means
year in Serbo-Croatian while hodina is an hour
in Czech (by the way, an hour in Russian is cas
— and the same word means time in Czech).
The overlapping word forms between rela-
tively distant languages are so frequent that it
is even possible to create (more or less) syntac-
tically correct sentences in one language con-
taining only word forms from the other lan-
guage. Again, let us look at the Czech-English
language pair. The English sentences Let my
pal to pile a lumpy paste on a metal pan. or
I had to let a house to a nosy patron. consist
entirely of word forms existing also in Czech,
while the Czech sentence Adept demise metal
hole pod led. — [A resignation candidate was
throwing sticks under the ice.] consists of En-
glish word forms.
Creating such a Czech sentence is more com-
plicated — as a highly inflected language it
uses a wide variety of endings, which make it
more difficult to create a syntactically correct
sentence from word forms of a language which
has incomparably smaller repertoire of end-
ings. This fact directly leads to another argu-
ment against the string similarity based mea-
sures — even though two languages may have
very similar syntactic properties and their ba-
sic word forms may also be very similar, then if
the languages are highly inflective and the only
difference between those languages are differ-
ent endings used for expressing identical mor-
phosyntactic properties, the string similarity
based methods will probably show a substan-
</bodyText>
<page confidence="0.992435">
92
</page>
<bodyText confidence="0.9997674">
tial difference between these languages.
This is highly probable especially for shorter
words — the words with a basic form
only four or five characters long may have
endings longer or equal to the length of
the basic form, for example: novd/novata
“new&amp;quot; (Cze/Mac), videny/vidimyj “seen”
(Cze/Rus), fotografujici/fotografuojantysis
“photographing” (Cze/Lit).
The last but not least indirect argument
against the use of string-based metrics can be
found in (Kubon and Bemova, 1990). The pa-
per describes so called transducing dictionary,
a set of rules designed for a direct transcrip-
tion of a certain category of source language
words into a target language. The system has
been tested on two language pairs (English-
to-Czech and Czech-to-Russian) and although
there was a natural original assumption that
such a system will cover substantially more ex-
pressions when applied to a pair of related lan-
guages (which are not only related, but also
quite similar), this assumption turned to be
wrong. The system covered almost identical
set of words for both language pairs — namely
the words with Greek or Latin origin. The
similarity of coverage even allowed to build an
English-to-Russian transducing dictionary us-
ing Czech as a pivot language with a negligible
loss of the coverage.
</bodyText>
<sectionHeader confidence="0.973043" genericHeader="method">
3 Experience from MT of similar
languages
</sectionHeader>
<bodyText confidence="0.999862821428571">
The Machine Translation field is a good testing
ground for any theory concerning the similar-
ity of natural languages. The systems dealing
with related languages usually achieve higher
translation quality than the systems aiming at
the translation of more distant language pairs
— the average MT quality for a given system
and a given language pair might therefore also
serve as some kind of a very rough metrics of
similarity of languages concerned.
Let us demonstrate this idea using an ex-
ample of a multilingual MT system described
in several recently published papers (see e.g.
(Hajic et al., 2003) or (Homola and Kubon,
2004)). The system aims at the translation
from a single source language (Czech) into
multiple more or less similar target languages,
namely into Slovak, Polish, Lithuanian, Lower
Sorbian and Macedonian.
The system is very simple — it doesn’t con-
tain any full-fledged parser, neither rule based,
nor stochastic one. It relies on the syntactic
similarity of the source and target languages.
It is transfer-based with the transfer being per-
formed as soon as possible, depending on the
similarity of both languages. In its simplest
form (Czech to Slovak translation) the system
consists of the following modules:
</bodyText>
<listItem confidence="0.9976006">
1. Morphological analysis of the source lan-
guage (Czech)
2. Morphological disambiguation of the
source language text by means of a
stochastic tagger
3. Transfer exploiting the domain-related
bilingual glossaries and a general (domain
independent) bilingual dictionary
4. Morphological synthesis of the target lan-
guage
</listItem>
<bodyText confidence="0.999942785714286">
The lower degree of similarity between Czech
and the remaining target languages led to
an inclusion of a shallow parsing module for
Czech for some of the language pairs. This
module directly follows the morphological dis-
ambiguation of Czech.
The evaluation results presented in (Homola
and Kubon, 2004) indicate that even though
Czech and Lithuanian are much less similar
at the lexical and morphological level (e.g. at
both levels actually dealing with strings), the
translation quality is very similar due to the
syntactic similarity between all languages con-
cerned.
</bodyText>
<sectionHeader confidence="0.922731" genericHeader="method">
4 Typology of language similarity
</sectionHeader>
<bodyText confidence="0.99919175">
The experience from the field of MT of closely
related languages presented in the previus sec-
tions shows that it is very useful to classify the
language similarity into several categories:
</bodyText>
<listItem confidence="0.99990675">
• typological
• morphological
• syntactic
• lexical
</listItem>
<bodyText confidence="0.9043335">
Let us now look at these categories from the
point of view of machine translation,
</bodyText>
<page confidence="0.992529">
93
</page>
<subsectionHeader confidence="0.990536">
4.1 Typological similarity
</subsectionHeader>
<bodyText confidence="0.999932">
The first type of similarity is probably the
most important one. If both the target and
the source language are of a different language
type, it is more difficult to obtain good MT
quality. The notions like word order, the ex-
istence or non-existence of articles, different
temporal system and several other properties
have direct consequences for the translation
quality. Let us take Czech and Lithuanian as
an example of the language pair, which doesn’t
belong to the same group of languages (Czech
is a Slavic and Lithuanian Baltic language).
Both languages have rich inflection and very
high degree of word order freedom, thus it is
not necessary to change the word order at the
constituent level. On the other hand, both
languages differ a lot in the lexics and mor-
phology.
For example, both (1) and (3) mean approx-
imately “The father read a/the book”. What
these sentences differ in is the information
structure. (1) should be translated as “The
father read a book”, whereas (3) means in
fact “The book has been read by the father&amp;quot;.1
The category of voice differs in both sentences
because of strict word order in English, al-
though in both Czech equivalents, active voice
is used.2 We see that in the Lithuanian trans-
lation, the word order is exactly the same.
</bodyText>
<listItem confidence="0.6073875">
(1) Otec cetl knihu
father-NOM read-3SG,PAST book-ACC
“The father read a book.” (Cze)
(2) Tevas skaite knyga�
father-NOM read-3SG,PAST book-ACC
“The father read a book.” (Lit)
(3) Knihu cetl otec
book-ACC read-3SG,PAST father-NOM
</listItem>
<bodyText confidence="0.674138">
“The father read a book.” (Cze)
&apos;Note that in the first sentence, an indefinite article
is used, whereas in the latter one, a definite article
stands in front of “book”. The reason is that in the first
sentence, the noun “book” is not contextually bound (it
belongs to the focus), in the latter one it belongs to the
topic.
2Passive voice (except of the reflexive one) occurs
rarely in Czech (and most other Slavonic languages).
It can be used if one would like to underline the di-
rect object or if there is no subject at all (for example,
Kniha byla ctena “The book has been read”).
</bodyText>
<subsectionHeader confidence="0.964328">
4.2 Lexical similarity
</subsectionHeader>
<bodyText confidence="0.999936272727273">
The lexical similarity does not mean that the
vocabulary has to have the same origin, i.e.,
that words have to be created from the same
(proto-)stem. What is important for shallow
MT (and for MT in general), is the seman-
tic correspondence (preferably one-to-one re-
lation).
Lexical similarity is the least important one
from the point of view of MT, because the lex-
ical differences are solved in the glossaries and
general dictionaries.
</bodyText>
<subsectionHeader confidence="0.97702">
4.3 Syntactic similarity
</subsectionHeader>
<bodyText confidence="0.999937214285714">
Syntactic similarity is also very important es-
pecially on higher levels, in particular on the
verbal level. The differences in verbal va-
lences have negative influence on the quality
of translation due to the fact that the trans-
fer thus requires a large scale valence lexicon
for both languages, which is extremely difficult
to build. Syntactic structure of smaller con-
stituents, such as nominal and prepositional
phrases, is not that important, because it is
possible to analyze those constituents syntac-
tically using a shallow syntactic analysis and
thus it is possible to adapt locally the syntactic
structure of a target sentence.
</bodyText>
<subsectionHeader confidence="0.994821">
4.4 Morphological similarity
</subsectionHeader>
<bodyText confidence="0.999731294117647">
Morphological similarity means similar struc-
ture of morphological hierarchy and paradigms
such as case system, verbal system etc. In
our understanding Baltic and Slavic languages
(except for Bulgarian and Macedonian) have
a similar case system and their verbal system
is quite similar as well. Some problems are
caused by synthetic forms, which have to be
expressed by analytical constructions in other
languages (e.g., future tense or conjunctive in
Czech and Lithuanian). The differences in
morphology can be relatively easily overcomed
by the exploitation of full-fledged morphology
of both languages (source and target).
Similar morphological systems simplify the
transfer. For example, Slavonic languages (ex-
cept of Bulgarian and Macedonian) have 6-7
</bodyText>
<figure confidence="0.965185">
(4) Knyga� skaite tevas
book-ACC read-3SG,PAST father-NOM
“The father read a book.” (Lit)
</figure>
<page confidence="0.994886">
94
</page>
<bodyText confidence="0.99819665">
cases. The case system of East Baltic lan-
guages is very similar, although it has been re-
duced formally in Latvian (instrumental forms
are equal as dative and accusative and the
function of instrumentral is expressed by the
preposition ar “with”, similarly as in Upper
Sorbian). (Ambrazas, 1996) gives seven cases
for Lithuanian, but there are in fact at least
eight cases in Lithuanian (or ten cases but only
eight of them are productive3). Nevertheless
the case systems of Slavonic and East Baltic
languages are very similar which makes the
languages quite similar even across the border
of different language groups.
Significant differences occur only in the ver-
bal system, East Baltic languages have a huge
amount of participles and half-participles that
have no direct counterpart in Czech. The
Lithuanian translation of an example from
(Gamut, 1991) is given in (5):
</bodyText>
<figure confidence="0.300240666666667">
(5) Gime vaikas,
was-born-3SG child-NOM
valdysiantis pasauli�
ruling-FUT,MASC,SG,NOM world-ACC
“A child was born which will rule the
world.” (Lit)
</figure>
<bodyText confidence="0.996530584905661">
The participle valdysiantis is used instead
of an embedded sentence, because Lithuanian
has future participles. These participles have
to be expresses by an embedded sentence in
Slavonic languages.
5 An outline of a structural
similarity measure
In this section, we propose a comparatively
simple measure of syntactic (structural) sim-
ilarity. There are generally two levels which
may serve as a basis for such a structural mea-
sure, the surface or deep syntactic level. Let us
first explain the reasons supporting our choice
of surface syntactic level.
Compared to deep syntactic representation,
the surface syntactic trees are much more
3Although some Balticists argue that illative forms
are adverbs, it is a fact that this case is productive and
used quite often (Erika Rimkute, personal communica-
tion), though it has been widely replaced by preposi-
tional phrases. Allative and adessive are used only in
some Lithuanian dialects, except of a few fixed allative
forms (e.g., vakarop(i) “in the evening”, velniop(i) “to
the hell”.)
closely related to the actual surface form of a
sentence. It is quite common that every word
form or punctuation sign is directly related to
a single node of a surface syntactic tree. The
deep syntactic trees, on the other hand, usu-
ally represent autosemantic words only, they
may even actually contain more nodes than
there are words in the input sentence (for ex-
ample, when the input sentence contains ellip-
sis). It is also quite clear that the deep syntac-
tic trees are much more closely related to the
meaning of the sentence than its original sur-
face form, therefore they may hide certain dif-
ferences between the languages concerned, it is
a generally accepted hypothesis that transfer
performed on the deep syntactic level is eas-
ier than the transfer at the surface syntactic
level, especially for syntactically and typolog-
ically less similar languages.
The second important decision we had to
make was to select the best type of surface
syntactic trees between the dependency and
phrase structure trees. For practical reasons
we have decided to use dependency trees. The
main motivation for this decision is the enor-
mous structural ambiguity of phrase structure
trees that represent sentences with identical
surface form. Let us have a look at the follow-
ing Polish sentence:
</bodyText>
<equation confidence="0.77849425">
(6) Pawe l czyta
Pawe l-NOM read-3SG
ksigik�
book-FEM,SG,ACC
</equation>
<bodyText confidence="0.9492238">
“Pawe l is reading a/the book.”
The syntactic structure of this sentence can
be expressed by two phrase structure trees rep-
resenting different order of attaching nominal
phrases to a verb.4
</bodyText>
<footnote confidence="0.9927495">
4The full line denotes the head of the phrase, the
dotted line a dependent.
</footnote>
<page confidence="0.999056">
95
</page>
<bodyText confidence="0.959549714285714">
The structural measure we are suggesting is
based on the analogy to the Levenshtein mea-
sure. It is therefore pretty simple — the dis-
tance of two trees is the minimal amount of
elementary operations that transform one tree
to the other. We consider the following ele-
mentary operations:
</bodyText>
<listItem confidence="0.99739225">
1. adding a node,
2. removing a node,
3. changing the order of a node,
4. changing the father of a node.
</listItem>
<figure confidence="0.982706090909091">
•
�
�
�
•
�
�
�
•
�
�
�
•
•
Pawe l czyta ksigik�
•
•
• • •
i i i
I I I
i i i
Pawe l czyta ksigik�
</figure>
<bodyText confidence="0.9995429">
There is no linguistically relevant difference
between these two trees. Although generally
useful, the information hidden in both trees
is purely superfluous for our goal of designing
a simple structural metrics. The dependency
tree obtained from the phrase structure ones
by contraction of all head edges seem to be
much more appropriate for our purpose. In our
example, we therefore get the following form
of the dependency tree:
</bodyText>
<equation confidence="0.780273333333333">
czyta
Zuuuuu JJJJJJJJ�J�
Pawe l ksigik�
</equation>
<bodyText confidence="0.999943363636364">
The nodes of the dependency trees repre-
senting surface syntactic level directly corre-
spond to word forms present in the sentence.
For the sake of simplicity, the punctuation
marks are not represented in our trees. They
would probably cause a lot of technical prob-
lems and might distort the whole similarity
measure. The node of a tree are ordered and
reflect the surface word-order of the sentence.
Different labels of nodes in both languages (see
the example below) don’t influence the value
of the measure, however they are important
for the identification of corresponding nodes
(a bilingual dictionary is used here).
The similarity of languages can be obtained
as an average distance of individual sentences
in a parallel corpus.
The following examples show the use of the
measure on individual trees. The correspon-
dence between individual nodes of both trees
can be handled by exploiting the bilingual dic-
tionary wherever necessary:
</bodyText>
<equation confidence="0.82729175">
(7) Vesna je
Vesna-NOM is-3SG
prisla
come-RESPART,FEM,SG
“Vesna has come.” (Slo)
(8) Vesna przysz la
Vesna-NOM come-RESPART,FEM,SG
“Vesna has come.” (Pol)
</equation>
<bodyText confidence="0.999831">
The distance between (7) and (8) is equal 1,
since one node has been removed (the dotted
line gives the removed node).
</bodyText>
<equation confidence="0.814934714285714">
prisla/przysz la
��jjjjjjjjjjjjjjjj
6
Vesna je
(9) Grem z adom
go-1SG with car-MASC,SG,INS
“I am going by car.&amp;quot; (Slo)
</equation>
<listItem confidence="0.707093">
(10) Jade samochodem
go-1SG car-MASC,SG,INS
</listItem>
<bodyText confidence="0.35781">
“I am going by car.&amp;quot; (Pol)
</bodyText>
<page confidence="0.987972">
96
</page>
<bodyText confidence="0.998488333333333">
The distance between (9) and (10) is equal
1, since one node has been removed (the dotted
line gives the removed node).
</bodyText>
<figure confidence="0.922932777777778">
grem/jade
�������������������
avtom/samochodem
z
5.1 Formalization
(11) On rdd plave
he-NOM with-pleasure swims-3SG
“He likes swimming.” (Cze)
plave
</figure>
<bodyText confidence="0.9997706875">
The Czech-English example (11) shows two
sentences which have a mutual distance equal
to 3 — if we start changing the Czech tree
into an English one, then the first elemen-
tary operation is the deletion of the node rdd,
the second operation adds the new node cor-
responding to the English word likes and the
third and last operation is the change of the
father of the node corresponding to the per-
sonal pronoun on [he] from swimming to likes.
As mentioned above, the node labels are not
taken into account, the fact that the Czech fi-
nite verbal form plave changes into an English
gerund has no effect on the distance.
A similar case are sentences with a dative
agent, for example:
</bodyText>
<equation confidence="0.883605333333333">
(12) Je mi zima
is me-DAT cold-F,SG,NOM
“I am cold” (Cze)
</equation>
<bodyText confidence="0.996282">
In this sentence, the Czech mi does not
match to I since it is no subject. Similarly,
the substantive zima does not match to cold,
since it is a different part of speech. Hence
two nodes are removed and two new nodes
are added, which gives us a distance of 4.
This example demonstrates that the measure
tends to behave naturally - even short sen-
tences containing syntactically different con-
structions get a relatively high score.
To formalize the process described above, let
us introduce a notion of lexical and analytical
equality of nodes in analytical trees:
</bodyText>
<listItem confidence="0.961941">
• Two nodes equal lexically if and only if
they share the same meaning in the given
context. Nevertheless to simplify auto-
matic processing, we treat two nodes as
lexically equal if they share a particular
meaning (defined e.g. as a non-empty in-
tersection of Wordnet classes).
• Two nodes equal analytically if and only
if they have the same analytical label (e.g.
subject, spacial adverbial etc.).
</listItem>
<bodyText confidence="0.998565111111111">
As for the measure, two nodes match to each
other if they 1) occur at the same position in
the subtree of their parent and 2) equal lexi-
cally and analytically.
If a subtree (greater than 1) is added or re-
moved, the operation contributes to the mea-
sure with the size of the subtree (the amount
of its nodes), for example in the following id-
iomatic phrase:
</bodyText>
<equation confidence="0.800908">
(13) puscic z dymem
leave-INF with smoke-MASC,SG,INS
“burn down” (Pol)
(14) zapolit
burn-down-INF
“burn down” (Cze)
</equation>
<bodyText confidence="0.998109">
In the above example, the distance is
equal 2.
The automatic procedure can be described
as follows (given two trees):
</bodyText>
<listItem confidence="0.995095">
1. Align all sons of the root node.
2. Count discrepancies.
3. For all matched nodes, go to step 1 to
process subtrees and sum up distances.
</listItem>
<figure confidence="0.9994484">
I
I
on rdd I
I
I
I
likes I
b �
I
I
I
11
he
swimming
��
</figure>
<page confidence="0.987723">
97
</page>
<subsectionHeader confidence="0.82189">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.995880666666667">
It is obvious that our measure expresses the ty-
pological similarity of languages. We get com-
paratively high values even for genetically re-
lated languages if their typology is different.
Let us demonstrate this fact on Czech and
Macedonian examples.
</bodyText>
<figure confidence="0.989">
(15) Ivan dal
Ivan-NOM gave-RESPART,MASC,SG
knihu Stojanovi
book-FEM.SG,ACC Stojan-DAT
“Ivan gave the book to Stojan.&amp;quot; (Cze)
�
���
�����������
����������
������
Ivan knihu Stojanovi
(16) Ivan mu ja
Ivan-NOM him her-FEM,SG,ACC
ima dadeno
has-3SG given-PPART,NEUT,SG
knigata na Stojan
book-FEM.SG,DEF on Stojan
</figure>
<bodyText confidence="0.9437025">
“Ivan gave the book to Stojan.&amp;quot; (Mac)
The distance equals 5. The score is rela-
tively high, taken into account that both lan-
guages are related. It indicates again that for
a given purpose the measure seems to provide
consistent results.
The proposed measure takes into account
only the structure of the trees, completely ig-
noring node and edge labels. Let us analyze
the following example:
</bodyText>
<figure confidence="0.9448095">
(17) Ta ksigika
this-FEM,SG,NOM book-FEM.SG,NOM
sib cz�sto czyta
REFL well read-3SG
“This book is read often.”
(18) T� ksigik�
this-FEM,SG,ACC book-FEM.SG,ACC
sib cz�sto czyta
</figure>
<footnote confidence="0.590335">
REFL well read-3SG
“This book is read often.”
</footnote>
<bodyText confidence="0.99998724">
The syntactic trees of both sentences have
the same structure, but (17) is passive and
(18) active (with a general subject). This is
of course a significant difference and as such
it should be captured in the measure, never-
theless our simple measure doesn’t reflect it.
There are several reasons why a current ver-
sion of the measure doesn’t include morpho-
logical and morphosyntactic labels. One of the
reasons is a different nature of the problem —
to design a reliable measure combining struc-
tural information with the information con-
tained in node labels is very difficult. From the
technical point of view, a great obstacle is also
the variety of systems of tags used for this pur-
pose for individual languages, which may not
be compatible. For example, Macedonian has
almost no cases at nouns, therefore it would
make no sense to use cases in the noun anno-
tation, while for other Slavic languages (and
not only for Slavic ones) is this information
very important. To find a good integration of
morphosyntactic features into the structural
measure is definitely a very interesting topic
for future research.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99121872">
This paper contains an outline of a simple lan-
guage similarity measure based upon the sur-
face syntactic dependency trees. According to
our opinion, such a measure expresses more
adequately the similarity of languages than
simple string-based measures used for the text
similarity. The measure is defined on pairs of
trees from a parallel corpus. In its current
form it doesn’t account for differences in mor-
phosyntactic labels of corresponding nodes or
edges, although it is an important parameter
of language similarity. The proper combina-
tion of our basic structural similarity measure
with some measure reflecting the differences of
labels opens a wide range of options for a fu-
ture research. Equally important seems to be
a task of gathering properly syntactically an-
notated parallel corpora of a reasonable size.
The only corpus of such kind which we have
at our disposal, the Prague Czech-English De-
pendency Treebank (Cutin et al., 2004) re-
lies on imperfect automatic annotation which
might distort the results. The human annota-
tion of the PCEDT is just starting, so there’s a
dal
</bodyText>
<page confidence="0.985909">
98
</page>
<figure confidence="0.99101">
dadeno �����
�����
��
LXX
�������
Ivan mu ` ja ` ima knigata Stojan
b
na
</figure>
<figureCaption confidence="0.999996">
Figure 1: The dependency tree of (16)
</figureCaption>
<bodyText confidence="0.958437666666667">
good chance that the measure will bring some
reliable results at least for those two lenguages
soon.
</bodyText>
<figure confidence="0.686461">
Jan
r
Martin
f
Cu
in,
mejrek,Ji
</figure>
<reference confidence="0.971728576923077">
iHavelka, Jan Ha-
jic, Vladislav Kubon, and Zdenek �Zabokrtsky.
2004. Prague Czech-English Dependency Tree-
bank Version 1.0. Linguistic Data Consortium.
LTF Gamut. 1991. Login, loanguage and meaning
2: Intensional logic and logical grammar. Uni-
versity of Chicago Press, Chicago.
Jan Hajic, Petr Homola, and Vladislav Kubon.
2003. A simple multilinguale machine transla-
tion system. In Proceedings of the MT Summit
IX, New Orleans.
Petr Homola and Vladislav
n
2004. A trans-
lation model for languages of accessing coun-
tries. In Proceedings of the 9th EAMT Work-
shop, La Valetta, Malta.
Eduard Hovy, Margaret King, an
Kubo
.
d Andrei
Popescu-Beli. 2002. Principles of Context-
Based Machine Translation Evaluation. Ma-
chine Translation, 1(17).
Vladislav
n and Alevtina
</reference>
<figure confidence="0.95102425">
e
a
1990.
Czech-to-
Kubo
B
mov
.
</figure>
<reference confidence="0.969729285714286">
Russian Transducing Dictionary. In
Proceedings of the XIIIth conference COLING
&apos;90, volume 3.
Ludovic Lebart and Martin Rajman
, 2000. Hand-
book of Natural Language Processing, chapter
Computing similarity. Dekker, New York.
7 Acknowledgements
This research was supported by the Min-
istry of Education of the Czech Repub-
lic, project MSM0021620838, by the grant
No. GAUK 351/2005 and by the grant
No. 1ET100300517. We would like to thank
the anonymous reviewers for their valuable
comments and recommendations.
References
Vytautas Ambrazas. 1996.
e
Dabartin
slietuviu� kal-
bos gramatika. Mokslo ir enciklopediju� leidykla,
Vilnius.
Nist. 2001. Automatic evaluation of machine
translation quality using n-gram co-
occurrence
statistics. Technical report, NIST.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for
Automatic Evaluation of Machine Translation.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
Philadelphia.
Kevin P. Scannell. 2004. Cor-
pus building for minority languages.
http://borel.slu.edu/crubadan/index.html.
</reference>
<page confidence="0.99876">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.500800">
<title confidence="0.999693">A Structural Similarity Measure</title>
<author confidence="0.981359">Homola</author>
<affiliation confidence="0.7761145">Institute of Formal and Applied Malostranske</affiliation>
<address confidence="0.962116">110 00 Praha 1, Czech</address>
<email confidence="0.963977">homola@ufal.mff.cuni.cz</email>
<email confidence="0.963977">vk@ufal.mff.cuni.cz</email>
<abstract confidence="0.996231315789474">This paper outlines a measure of language similarity based on structural similarity of surface syntactic dependency trees. Unlike the more traditional string-based measures, this measure tries to reflect “deeper” correspondences among languages. The development of this measure has been inspired by the experience from MT of syntactically similar languages. This experience shows that the lexical similarity is less important than syntactic similarity. This claim is supported by a number of examples illustrating the problems which may arise when a measure of language similarity relies too much on a simple similarity of texts in different languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Hajic iHavelka</author>
<author>Vladislav Kubon</author>
<author>Zdenek �Zabokrtsky</author>
</authors>
<title>Prague Czech-English Dependency Treebank Version 1.0. Linguistic Data Consortium.</title>
<date>2004</date>
<marker>iHavelka, Kubon, �Zabokrtsky, 2004</marker>
<rawString>iHavelka, Jan Hajic, Vladislav Kubon, and Zdenek �Zabokrtsky. 2004. Prague Czech-English Dependency Treebank Version 1.0. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LTF Gamut</author>
</authors>
<title>Login, loanguage and meaning 2: Intensional logic and logical grammar.</title>
<date>1991</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="16577" citStr="Gamut, 1991" startWordPosition="2722" endWordPosition="2723">h”, similarly as in Upper Sorbian). (Ambrazas, 1996) gives seven cases for Lithuanian, but there are in fact at least eight cases in Lithuanian (or ten cases but only eight of them are productive3). Nevertheless the case systems of Slavonic and East Baltic languages are very similar which makes the languages quite similar even across the border of different language groups. Significant differences occur only in the verbal system, East Baltic languages have a huge amount of participles and half-participles that have no direct counterpart in Czech. The Lithuanian translation of an example from (Gamut, 1991) is given in (5): (5) Gime vaikas, was-born-3SG child-NOM valdysiantis pasauli� ruling-FUT,MASC,SG,NOM world-ACC “A child was born which will rule the world.” (Lit) The participle valdysiantis is used instead of an embedded sentence, because Lithuanian has future participles. These participles have to be expresses by an embedded sentence in Slavonic languages. 5 An outline of a structural similarity measure In this section, we propose a comparatively simple measure of syntactic (structural) similarity. There are generally two levels which may serve as a basis for such a structural measure, the</context>
</contexts>
<marker>Gamut, 1991</marker>
<rawString>LTF Gamut. 1991. Login, loanguage and meaning 2: Intensional logic and logical grammar. University of Chicago Press, Chicago. Jan Hajic, Petr Homola, and Vladislav Kubon.</rawString>
</citation>
<citation valid="true">
<title>A simple multilinguale machine translation system.</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit IX,</booktitle>
<location>New Orleans.</location>
<marker>2003</marker>
<rawString>2003. A simple multilinguale machine translation system. In Proceedings of the MT Summit IX, New Orleans.</rawString>
</citation>
<citation valid="false">
<institution>Petr Homola and Vladislav n</institution>
<marker></marker>
<rawString>Petr Homola and Vladislav n</rawString>
</citation>
<citation valid="true">
<title>A translation model for languages of accessing countries.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th EAMT Workshop,</booktitle>
<location>La Valetta,</location>
<marker>2004</marker>
<rawString>2004. A translation model for languages of accessing countries. In Proceedings of the 9th EAMT Workshop, La Valetta, Malta.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eduard Hovy</author>
</authors>
<location>Margaret King, an Kubo .</location>
<marker>Hovy, </marker>
<rawString>Eduard Hovy, Margaret King, an Kubo .</rawString>
</citation>
<citation valid="true">
<authors>
<author>d Andrei Popescu-Beli</author>
</authors>
<title>Principles of ContextBased Machine Translation Evaluation.</title>
<date>2002</date>
<journal>Machine Translation,</journal>
<volume>1</volume>
<issue>17</issue>
<marker>Popescu-Beli, 2002</marker>
<rawString>d Andrei Popescu-Beli. 2002. Principles of ContextBased Machine Translation Evaluation. Machine Translation, 1(17).</rawString>
</citation>
<citation valid="false">
<title>Vladislav n and Alevtina</title>
<marker></marker>
<rawString>Vladislav n and Alevtina</rawString>
</citation>
<citation valid="false">
<authors>
<author>Russian</author>
</authors>
<title>Transducing Dictionary.</title>
<booktitle>In Proceedings of the XIIIth conference COLING &apos;90,</booktitle>
<volume>3</volume>
<marker>Russian, </marker>
<rawString>Russian Transducing Dictionary. In Proceedings of the XIIIth conference COLING &apos;90, volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Lebart</author>
<author>Martin Rajman</author>
</authors>
<title>Handbook of Natural Language Processing, chapter Computing similarity.</title>
<date>2000</date>
<publisher>Dekker,</publisher>
<location>New York.</location>
<contexts>
<context position="1639" citStr="Lebart and Rajman, 2000" startWordPosition="259" endWordPosition="262">two natural languages as being more or less similar. These claims are in some cases a result of a detailed comparative examination of lexical and/or syntactic properties of languages under question, in some cases they are based on a very subjective opinion of the author, in many other cases they reflect the application of some mathematical formula on textual data (a very nice example of such mathematical approach can be found at (Scannell, 2004)). Especially in the last case the notion of language similarity is very often confused with the notion of text similarity. Even the well known paper (Lebart and Rajman, 2000) deals more with the text similarity than language similarity. This general trend is quite understandable, the mathematical methods for measuring text similarity are of a prominent importance especially for information retrieval and similar fields. On the other hand, they concentrate too much on the surface similarity of word forms and thus may not reflect the similarity of languages properly. This paper tries to advocate different approach, based on the experience gained in MT experiments with closely related (and similar) languages, where it is possible to “measure” the similarity indirectly</context>
</contexts>
<marker>Lebart, Rajman, 2000</marker>
<rawString>Ludovic Lebart and Martin Rajman , 2000. Handbook of Natural Language Processing, chapter Computing similarity. Dekker, New York. 7 Acknowledgements This research was supported by the Ministry of Education of the Czech Republic, project MSM0021620838, by the grant</rawString>
</citation>
<citation valid="true">
<authors>
<author>GAUK</author>
</authors>
<title>351/2005 and by the grant No. 1ET100300517. We would like to thank the anonymous reviewers for their valuable comments and recommendations. References Vytautas Ambrazas.</title>
<date>1996</date>
<location>Vilnius.</location>
<marker>GAUK, 1996</marker>
<rawString>No. GAUK 351/2005 and by the grant No. 1ET100300517. We would like to thank the anonymous reviewers for their valuable comments and recommendations. References Vytautas Ambrazas. 1996. slietuviu� kalbos gramatika. Mokslo ir enciklopediju� leidykla, Vilnius.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nist</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<tech>Technical</tech>
<location>Philadelphia.</location>
<contexts>
<context position="3441" citStr="Nist, 2001" startWordPosition="549" endWordPosition="550">cally based upon the Levenshtein metrics) or estimating the similarity of a new source sentence to those stored in the translation memory of a Machine Aided Translation system. They are a bit controversial in a “proper” machine translation, where the popular BLEU score (Papineni et al., 2002), although widely accepted as a measure of translation accuracy, seems to favor stochastic approaches based on 91 Proceedings of the Workshop on Linguistic Distances, pages 91–99, Sydney, July 2006. c�2006 Association for Computational Linguistics an n-gram model over other MT methods (see the results in (Nist, 2001)). The controversies the BLEU score seems to provoke arise due to the fact that the evaluation of MT systems can be, in general, performed from two different viewpoints. The first one is that of a developer of such a system, who needs to get a reliable feedback in the process of development and debugging of the system. The primary interest of such a person is the grammar or dictionary coverage and system performance and he needs a cheap, fast and simple evaluation method in order to allow frequent routine tests indicating the improvements of the system during the development of the system. The</context>
</contexts>
<marker>Nist, 2001</marker>
<rawString>Nist. 2001. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. Technical report, NIST. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Scannell</author>
</authors>
<title>Corpus building for minority languages.</title>
<date>2004</date>
<note>http://borel.slu.edu/crubadan/index.html.</note>
<contexts>
<context position="1464" citStr="Scannell, 2004" startWordPosition="231" endWordPosition="232">es. 1 Introduction Although the similarity of natural languages is in principal a very vague notion, the linguistic literature seems to be full of claims classifying two natural languages as being more or less similar. These claims are in some cases a result of a detailed comparative examination of lexical and/or syntactic properties of languages under question, in some cases they are based on a very subjective opinion of the author, in many other cases they reflect the application of some mathematical formula on textual data (a very nice example of such mathematical approach can be found at (Scannell, 2004)). Especially in the last case the notion of language similarity is very often confused with the notion of text similarity. Even the well known paper (Lebart and Rajman, 2000) deals more with the text similarity than language similarity. This general trend is quite understandable, the mathematical methods for measuring text similarity are of a prominent importance especially for information retrieval and similar fields. On the other hand, they concentrate too much on the surface similarity of word forms and thus may not reflect the similarity of languages properly. This paper tries to advocate</context>
</contexts>
<marker>Scannell, 2004</marker>
<rawString>Kevin P. Scannell. 2004. Corpus building for minority languages. http://borel.slu.edu/crubadan/index.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>