<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016928">
<title confidence="0.993703">
CLaC-CORE: Exhaustive Feature Combination for Measuring Textual
Similarity
</title>
<author confidence="0.993413">
Ehsan Shareghi
</author>
<affiliation confidence="0.992262">
CLaC Laboratory
Concordia University
</affiliation>
<address confidence="0.933616">
Montreal, QC H3G 1M8, CANADA
</address>
<email confidence="0.98612">
eh share@cse.concordia.ca
</email>
<author confidence="0.980284">
Sabine Bergler
</author>
<affiliation confidence="0.988306">
CLaC Laboratory
Concordia University
</affiliation>
<address confidence="0.933235">
Montreal, QC H3G 1M8, CANADA
</address>
<email confidence="0.996961">
bergler@cse.concordia.ca
</email>
<sectionHeader confidence="0.995583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999795722222222">
CLaC-CORE, an exhaustive feature combina-
tion system ranked 4th among 34 teams in the
Semantic Textual Similarity shared task STS
2013. Using a core set of 11 lexical features
of the most basic kind, it uses a support vector
regressor which uses a combination of these
lexical features to train a model for predicting
similarity between sentences in a two phase
method, which in turn uses all combinations
of the features in the feature space and trains
separate models based on each combination.
Then it creates a meta-feature space and trains
a final model based on that. This two step pro-
cess improves the results achieved by single-
layer standard learning methodology over the
same simple features. We analyze the correla-
tion of feature combinations with the data sets
over which they are effective.
</bodyText>
<sectionHeader confidence="0.999123" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999814771428571">
The Semantic Textual Similarity (STS) shared task
aims to find a unified way of measuring similarity
between sentences. In fact, sentence similarity is
a core element of tasks trying to establish how two
pieces of text are related, such as Textual Entailment
(RTE) (Dagan et al., 2006), and Paraphrase Recog-
nition (Dolan et al., 2004). The STS shared task was
introduced for SemEval-2012 and was selected as its
first shared task. Similar in spirit, STS differs from
the well-known RTE shared tasks in two important
points: it defines a graded similarity scale to mea-
sure similarity of two texts, instead of RTE’s binary
yes/no decision and the similarity relation is consid-
ered to be symmetrical, whereas the entailment rela-
tion of RTE is inherently unidirectional.
The leading systems in the 2012 competition used
a variety of very simple lexical features. Each sys-
tem combines a different set of related features.
CLaC Labs investigated the different combination
possibilities of these simple lexical features and
measured their performance on the different data
sets. Originally conceived to explore the space of
all possible feature combinations for ‘feature com-
bination selection’, a two-step method emerged that
deliberately compiles and trains all feature combina-
tions exhaustively and then trains an SVM regressor
using all combination models as its input features.
It turns out that this technique is not nearly as pro-
hibitive as imagined and achieves statistically sig-
nificant improvements over the alternative of feature
selection or of using any one single combination in-
dividually.
We propose the method as a viable approach when
the characteristics of the data are not well under-
stood and no satisfactory training set is available.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9956494">
Recently, systems started to approach measuring
similarity by combining different resources and
methods. For example, the STS-2012 shared task’s
leading UKP (B¨ar et al., 2012) system uses n-grams,
string similarity, WordNet, and ESA, and a regres-
sor. In addition, they use MOSES, a statistical ma-
chine translation system (Koehn et al., 2007), to
translate each English sentence into Dutch, German,
and Spanish and back into English in an effort to in-
crease their training set of similar text pairs.
</bodyText>
<page confidence="0.967812">
202
</page>
<subsubsectionHeader confidence="0.252693">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.959563785714286">
and the Shared Task, pages 202–206, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
TakeLab (ˇSaric et al., 2012), in place two of the
2012 STS shared task, uses n-gram models, two
WordNet-based measures, LSA, and dependencies
to align subject-verb-object predicate structures. In-
cluding named-entities and number matching in the
feature space improved performance of their support
vector regressor.
(Shareghi and Bergler, 2013) illustrates two ex-
periments with STS-2012 training and test sets us-
ing the basic core features of these systems, outper-
forming the STS-2012 task’s highest ranking sys-
tems. The STS-2013 submission CLaC-CORE uses
the same two-step approach.
</bodyText>
<sectionHeader confidence="0.997414" genericHeader="method">
3 CLaC Methodology
</sectionHeader>
<bodyText confidence="0.99949775">
Preprocessing consists of tokenizing, lemmatizing,
sentence splitting, and part of speech (POS) tagging.
We extract two main categories of lexical features:
explicit and implicit.
</bodyText>
<subsectionHeader confidence="0.998984">
3.1 Explicit Lexical Features
</subsectionHeader>
<bodyText confidence="0.999887652173913">
Sentence similarity at the explicit level is based
solely on the input text and measures the similar-
ity between two sentences either by using an n-gram
model (ROUGE-1, ROUGE-2, ROUGE-SU4) or by
reverting to string similarity (longest common sub-
sequence, jaro, ROUGE-W):
Longest Common Subsequence (Allison and
Trevor, 1986) compare the length of the
longest sequence of characters, not necessarily
consecutive ones, in order to detect similarities
Jaro (Jaro, 1989) identifies spelling variation be-
tween two inputs based on the occurrence of
common characters between two text segments
at a certain distance
ROUGE-W (Lin et al., 2004a), a weighted version
of longest common subsequence, takes into ac-
count the number of the consecutive characters
in each match, giving higher score for those
matches that have larger number of consecu-
tive characters in common. This metric was de-
veloped to measure the similarity between ma-
chine generated text summaries and a manually
generated gold standard
</bodyText>
<note confidence="0.8529975">
ROUGE-1 unigrams (Lin et al., 2004a)
ROUGE-2 bigrams (Lin et al., 2004a)
</note>
<bodyText confidence="0.733028">
ROUGE-SU4 4-Skip bigrams (including Uni-
grams) (Lin et al., 2004a)
</bodyText>
<subsectionHeader confidence="0.999469">
3.2 Implicit Lexical Features
</subsectionHeader>
<bodyText confidence="0.999974658536585">
Sentence similarity at the implicit level uses exter-
nal resources to make up for the lexical gaps that
go otherwise undetected at the explicit level. The
synonymy of bag and suitcase is an example of an
implicit similarity. This type of implicit similarity
can be detected using knowledge sources such as
WordNet or Roget’s Thesaurus based on the Word-
Net::Similarity package (Pedersen et al., 2004) and
combination techniques (Mihalcea et al., 2006). For
the more semantically challenging non-ontologigal
relations, for example sanction and Iran, which lex-
ica do not provide, co-occurrence-based measures
like ESA are more robust. We use:
Lin (Lin, 1998) uses the Brown Corpus of Ameri-
can English to calculate information content of
two concepts’ least common subsumer. Then
he scales it using the sum of the information
content of the compared concepts
Jiang-Conrath (Jiang and Conrath, 1997) uses the
conditional probability of encountering a con-
cept given an instance of its parent to calculate
the information content. Then they define the
distance between two concepts to be the sum
of the difference between the information con-
tent of each of the two given concepts and their
least common subsumer
Roget’s Thesaurus is another lexical resource and
is based on well-crafted concept classifica-
tion and was created by professional lexicogra-
phers. It has a nine-level ontology and doesn’t
have one of the major drawbacks of WordNet,
which is lack of links between part of speeches.
According to the schema proposed by (Jarmasz
and Szpakowicz, 2003) the distance of two
terms decreases within the interval of [0,16],
as the the common head that subsumes them
moves from top to the bottom and becomes
more specific. The electronic version of Ro-
get’s Thesaurus which was developed by (Jar-
masz and Szpakowicz, 2003) was used for ex-
tracting this score
</bodyText>
<page confidence="0.993518">
203
</page>
<bodyText confidence="0.995562833333333">
Explicit Semantic Analyzer (Gabrilovich and
Markovitch, 2007) In order to have broader
coverage on word types not represented in
lexical resources, specifically for named enti-
ties, we add explicit semantic analyzer (ESA)
generated features to our feature space
</bodyText>
<subsectionHeader confidence="0.997553">
3.3 CLaC-CORE
</subsectionHeader>
<bodyText confidence="0.999876">
CLaC-CORE first generates all combinations of the
11 basic features (jaro, Lemma, lcsq, ROUGE-W,
ROUGE-1, ROUGE-2, ROUGE-SU4, roget, lin, jcn,
esa), that is 211 − 1 = 2047 non-empty combina-
tions. The Two Phase Model Training step trains
a separate Support Vector Regressor (SVR) for
each combination creating 2047 Phase One Models.
These 2N − 1 predicted scores per text data item
form a new feature vector called Phase Two Fea-
tures, which feed into a SVR to train our Phase Two
Model.
On a standard 2 core computer with ≤100 GB
of RAM using multi-threading (thread pool of size
200, a training process per thread) it took roughly 15
hours to train the 2047 Phase One Models on 5342
text pairs and another 17 hours to build the Phase
Two Feature Space for the training data. Building
the Phase Two Feature Space for the test sets took
roughly 7.5 hours for 2250 test pairs.
For the current submissions we combine all train-
ing sets into one single training set used in all of our
submissions for the STS 2013 task.
</bodyText>
<sectionHeader confidence="0.882381" genericHeader="method">
4 Analysis of Results
</sectionHeader>
<bodyText confidence="0.999496545454546">
Our three submission for STS-2013 compare a base-
line of Standard Learning (RUN-1)with two ver-
sions of our Two Phase Learning (RUN-2, RUN-
3). For the Standard Learning baseline, one regres-
sor was trained on the training set on all 11 Basic
Features and tested on the test sets. For the remain-
ing runs the Two Phase Learning method was used.
All our submissions use the same 11 Basic Features.
RUN-2 is our main contribution. RUN-3 is identical
to RUN-2 except for reducing the number of support
vectors and allowing larger training errors in an ef-
fort to assess the potential for speedup. This was
done by decreasing the value of -y (in the RBF ker-
nel) from 0.01 to 0.0001, and decreasing the value of
C (error weight) from 1 to 0.01. These parameters
resulted in a smoother and simpler decision surface
but negatively affected the performance for RUN-3
as shown in Table 1.
The STS shared task-2013 used the Pearson Cor-
relation Coefficient as the evaluation metric. The re-
sults of our experiments are presented in Table 1.
The results indicate that the proposed method, RUN-
</bodyText>
<table confidence="0.9987702">
rank headlines OnWN FNWN SMT
RUN-1 10 0.6774 0.7667 0.3793 0.3068
RUN-2 7 0.6921 0.7367 0.3793 0.3375
RUN-3 46 0.5276 0.6495 0.4158 0.3082
STS-bl 73 0.5399 0.2828 0.2146 0.2861
</table>
<tableCaption confidence="0.9900605">
Table 1: CLaC-CORE runs and STS baseline perfor-
mance
</tableCaption>
<bodyText confidence="0.999676">
2, was successful in improving the results achieved
by our baseline RUN-1 ever so slightly (the confi-
dence invervals at 5% differ to .016 at the upper end)
and far exceeds the reduced computation version of
RUN-3.
</bodyText>
<subsectionHeader confidence="0.996168">
4.1 Successful Feature Combinations
</subsectionHeader>
<bodyText confidence="0.99939">
Having trained separate models based on each sub-
set of features we can use the predicted scores gen-
erated by each of these models to calculate their cor-
relations to assess which of the feature combinations
were more effective in making predictions and how
this most successful combination varies bewteen the
different datasets.
</bodyText>
<figure confidence="0.667164571428571">
best worst
headlines [ ROUGE-1 ROUGE- [jcn lem lcsq]
SU4 esa lem]
0.7329 0.3375
OnWN [ROUGE-1 ROUGE- [jaro]
SU4 esa lin jcn roget
lem lcsq ROUGE-W ]
0.7768 0.1425
FNWN [roget ROUGE-1 [ROUGE-2 lem lcsq]
ROUGE-SU4]
0.4464 -0.0386
SMT [lin jcn roget [esa lcsq]
ROUGE-1]
0.3648 0.2305
</figure>
<tableCaption confidence="0.9520015">
Table 2: Best and worst feature combination performance
on test set
</tableCaption>
<bodyText confidence="0.6479835">
Table 2 lists the best and worst feature combina-
tions on each test set. ROUGE-1 (denoted by RO-
1), unigram overlap, is part of all four best perform-
ing subsets. The features ROUGE-SU4 and Roget’s
</bodyText>
<page confidence="0.997502">
204
</page>
<bodyText confidence="0.999838290322581">
appear in three of the best four feature combina-
tions, making Roget’s the best performing lexicon-
based feature outperforming WordNet features on
this task. esa, lin, jcn are part of two of the best
subsets, where lin and jcn occur together both times,
suggesting synergy. Looking at the worst perform-
ing feature combinations is also instructive and sug-
gests that lcsq was not an effective feature (despite
being at the heart of the more successful ROUGE-W
measure).
We also analyze performance of individual fea-
tures over different datasets. Table 3 lists all the fea-
tures and, instead of looking at only the best com-
bination, takes the top three best combinations for
each test and compares how many times each fea-
ture has occurred in the resulting 12 combinations
(first column). Three clear classes of effectiveness
emerge, high (10-7), medium (6-4), and low (3-0).
Next, we observe that the test sets differ in the aver-
age length of the data: headlines and OnWN glosses
are very short, in contrast to the other two. Table 3
shows in fact contrastive feature behavior for these
two categories (denoted by short and long). The last
column reports the number of time a feature has oc-
curred in the best combinations (out of 4). Again,
ROUGE-1, ROUGE-SU4, and roget prove effective
across different test sets. esa and lem seem most re-
liable when we deal with short text fragments, while
roget and ROUGE-SU4 are most valuable on longer
texts. The individual most valuable features overall
are ROUGE-1, ROUGE-SU4, and roget.
</bodyText>
<table confidence="0.998741333333333">
Features total (/12) short (/6) long (/6) best (/4)
esa 6 6 0 2
lin 6 3 3 2
jcn 4 1 3 2
roget 9 3 6 3
lem 6 6 0 2
jaro 0 0 0 0
lcsq 3 3 0 1
ROUGE-W 7 4 3 1
ROUGE-1 10 6 4 4
ROUGE-2 3 1 2 0
ROUGE-SU4 10 5 5 3
</table>
<sectionHeader confidence="0.978527" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99996440625">
CLaC-CORE investigated the performance possibil-
ities of different feature combinations for 11 basic
lexical features that are frequently used in seman-
tic distance measures. By exhaustively training all
combinations in a two-phase regressor, we were able
to establish a few interesting observations.
First, our own baseline of simply training a SVM
regressor on all 11 basic features achieves rank 10
and outperforms the baseline used for the shared
task. It should probably become the new standard
baseline.
Second, our two-phase exhaustive model, while
resource intensive, is not at all prohibitive. If the
knowledge to pick appropriate features is not avail-
able and if not enough training data exists to per-
form feature selection, the exhaustive method can
produce results that outperform our baseline and one
that is competitive in the current field (rank 7 of 88
submissions). But more importantly, this method al-
lows us to forensically analyze feature combination
behavior contrastively. We were able to establish
that unigrams and 4-skip bigrams are most versatile,
but surprisingly that Roget’s Thesaurus outperforms
the two leading WordNet-based distance measures.
In addition, ROUGE-W, a weighted longest com-
mon subsequence algorithm that to our knowledge
has not previously been used for similarity mea-
surements shows to be a fairly reliable measure for
all data sets, in contrast to longest common subse-
quence, which is among the lowest performers.
We feel that the insight we gained well justified
the expense of our approach.
</bodyText>
<sectionHeader confidence="0.984399" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982888">
We are grateful to Michelle Khalife and Jona Schu-
man for their comments and feedback on this work.
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada (NSERC).
</bodyText>
<tableCaption confidence="0.990441">
Table 3: Feature contribution to the three best results over
four datasets
</tableCaption>
<sectionHeader confidence="0.996268" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.966786">
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. ITRI-04-08 The Sketch Engine. In-
formation Technology.
</reference>
<page confidence="0.992445">
205
</page>
<reference confidence="0.997861617647059">
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A Tutorial
on Support Vector Regression. Statistics and Comput-
ing, 14(3).
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Semantic
Relatedness. Computational Linguistics, 32(1).
Chin-Yew Lin. 2004a. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop.
Chin-Yew Lin and Franz Josef Och. 2004b. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Christiane Fellbaum 2010. WordNet. Theory and
Applications of Ontology: Computer Applications.
Springer.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012), in
conjunction with the First Joint Conference on Lexical
and Computational Semantics.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proceedings of the 15th International
Conference on Machine Learning, volume 1.
Ehsan Shareghi, Sabine Bergler. 2013. Feature Combi-
nation for Sentence Similarity. To appear in Proceed-
ings of the 26st Conference of the Canadian Society
for Computational Studies of Intelligence (Canadian
AI’13). Advances in Artificial Intelligence, Regina,
SK, Canada. Springer-Verlag Berlin Heidelberg.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence.
Frane &amp;quot;Saric, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;sic. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), in conjunction with the
First Joint Conference on Lexical and Computational
Semantics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The Pascal Recognising Textual Entailment
Challenge. Machine Learning Challenges. Evaluat-
ing Predictive Uncertainty, Visual Object Classifica-
tion, and Recognising Tectual Entailment.
Jay J. Jiang and David W. Conrath. 1997. Semantic Sim-
ilarity Based on Corpus Statistics and Lexical Taxon-
omy. Proceedings of the 10th International Confer-
ence on Research on Computational Linguistics.
Lloyd Allison and Trevor I. Dix. 1986. A Bit-String
Longest-Common-Subsequence Algorithm. Informa-
tion Processing Letters, 23(5).
Mario Jarmasz and Stan Szpakowicz. 2003. Rogets The-
saurus and Semantic Similarity. In Proceedings of the
Conference on Recent Advances in Natural Language
Processing.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: an Update.
ACM SIGKDD Explorations Newsletter, 11(1).
Matthew A. Jaro. 1989. Advances in Record-Linkage
Methodology as Applied to Matching the 1985 Census
of Tampa, Florida. Journal of the American Statistical
Association.
Philip Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcelo Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond&amp;quot;rej Bojar. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation. In
Proceedings of the 45th Annual Meeting of the ACL on
Interactive Poster and Demonstration Sessions. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Ted Pedersen, Siddharth Patwardhan and Jason Miche-
lizzi. 2004. WordNet:: Similarity: Measuring the
Relatedness of Concepts. In Demonstration Papers at
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics.
William B. Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News Sources.
In Proceedings of the 20th International Conference
on Computational Linguistics. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.998892">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.056746">
<title confidence="0.990399">CLaC-CORE: Exhaustive Feature Combination for Measuring Textual Similarity</title>
<author confidence="0.971619">Ehsan</author>
<affiliation confidence="0.516358">CLaC Concordia</affiliation>
<address confidence="0.666638">Montreal, QC H3G 1M8,</address>
<email confidence="0.575634">ehshare@cse.concordia.ca</email>
<author confidence="0.836621">Sabine</author>
<affiliation confidence="0.4902605">CLaC Concordia</affiliation>
<address confidence="0.811571">Montreal, QC H3G 1M8,</address>
<email confidence="0.990419">bergler@cse.concordia.ca</email>
<abstract confidence="0.999566894736842">CLaC-CORE, an exhaustive feature combination system ranked 4th among 34 teams in the Semantic Textual Similarity shared task STS 2013. Using a core set of 11 lexical features of the most basic kind, it uses a support vector regressor which uses a combination of these lexical features to train a model for predicting similarity between sentences in a two phase method, which in turn uses all combinations of the features in the feature space and trains separate models based on each combination. Then it creates a meta-feature space and trains a final model based on that. This two step process improves the results achieved by singlelayer standard learning methodology over the same simple features. We analyze the correlation of feature combinations with the data sets over which they are effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Pavel Rychly</author>
<author>Pavel Smrz</author>
<author>David Tugwell</author>
</authors>
<title>The Sketch Engine. Information Technology.</title>
<date>2004</date>
<pages>04--08</pages>
<marker>Kilgarriff, Rychly, Smrz, Tugwell, 2004</marker>
<rawString>Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. ITRI-04-08 The Sketch Engine. Information Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A Tutorial on Support Vector Regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A Tutorial on Support Vector Regression. Statistics and Computing, 14(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based Measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</booktitle>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004a. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004b. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Theory and Applications of Ontology: Computer Applications.</title>
<publisher>Springer.</publisher>
<marker>Fellbaum, </marker>
<rawString>Christiane Fellbaum 2010. WordNet. Theory and Applications of Ontology: Computer Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics.</booktitle>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<volume>1</volume>
<contexts>
<context position="6246" citStr="Lin, 1998" startWordPosition="958" endWordPosition="959">es external resources to make up for the lexical gaps that go otherwise undetected at the explicit level. The synonymy of bag and suitcase is an example of an implicit similarity. This type of implicit similarity can be detected using knowledge sources such as WordNet or Roget’s Thesaurus based on the WordNet::Similarity package (Pedersen et al., 2004) and combination techniques (Mihalcea et al., 2006). For the more semantically challenging non-ontologigal relations, for example sanction and Iran, which lexica do not provide, co-occurrence-based measures like ESA are more robust. We use: Lin (Lin, 1998) uses the Brown Corpus of American English to calculate information content of two concepts’ least common subsumer. Then he scales it using the sum of the information content of the compared concepts Jiang-Conrath (Jiang and Conrath, 1997) uses the conditional probability of encountering a concept given an instance of its parent to calculate the information content. Then they define the distance between two concepts to be the sum of the difference between the information content of each of the two given concepts and their least common subsumer Roget’s Thesaurus is another lexical resource and </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proceedings of the 15th International Conference on Machine Learning, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehsan Shareghi</author>
<author>Sabine Bergler</author>
</authors>
<title>Feature Combination for Sentence Similarity. To appear in</title>
<date>2013</date>
<booktitle>Proceedings of the 26st Conference of the Canadian Society for Computational Studies of Intelligence (Canadian AI’13). Advances in Artificial Intelligence,</booktitle>
<publisher>Springer-Verlag</publisher>
<location>Regina, SK,</location>
<contexts>
<context position="3961" citStr="Shareghi and Bergler, 2013" startWordPosition="606" endWordPosition="609">their training set of similar text pairs. 202 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 202–206, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics TakeLab (ˇSaric et al., 2012), in place two of the 2012 STS shared task, uses n-gram models, two WordNet-based measures, LSA, and dependencies to align subject-verb-object predicate structures. Including named-entities and number matching in the feature space improved performance of their support vector regressor. (Shareghi and Bergler, 2013) illustrates two experiments with STS-2012 training and test sets using the basic core features of these systems, outperforming the STS-2012 task’s highest ranking systems. The STS-2013 submission CLaC-CORE uses the same two-step approach. 3 CLaC Methodology Preprocessing consists of tokenizing, lemmatizing, sentence splitting, and part of speech (POS) tagging. We extract two main categories of lexical features: explicit and implicit. 3.1 Explicit Lexical Features Sentence similarity at the explicit level is based solely on the input text and measures the similarity between two sentences eithe</context>
</contexts>
<marker>Shareghi, Bergler, 2013</marker>
<rawString>Ehsan Shareghi, Sabine Bergler. 2013. Feature Combination for Sentence Similarity. To appear in Proceedings of the 26st Conference of the Canadian Society for Computational Studies of Intelligence (Canadian AI’13). Advances in Artificial Intelligence, Regina, SK, Canada. Springer-Verlag Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics.</booktitle>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7506" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1161" endWordPosition="1164">ncept classification and was created by professional lexicographers. It has a nine-level ontology and doesn’t have one of the major drawbacks of WordNet, which is lack of links between part of speeches. According to the schema proposed by (Jarmasz and Szpakowicz, 2003) the distance of two terms decreases within the interval of [0,16], as the the common head that subsumes them moves from top to the bottom and becomes more specific. The electronic version of Roget’s Thesaurus which was developed by (Jarmasz and Szpakowicz, 2003) was used for extracting this score 203 Explicit Semantic Analyzer (Gabrilovich and Markovitch, 2007) In order to have broader coverage on word types not represented in lexical resources, specifically for named entities, we add explicit semantic analyzer (ESA) generated features to our feature space 3.3 CLaC-CORE CLaC-CORE first generates all combinations of the 11 basic features (jaro, Lemma, lcsq, ROUGE-W, ROUGE-1, ROUGE-2, ROUGE-SU4, roget, lin, jcn, esa), that is 211 − 1 = 2047 non-empty combinations. The Two Phase Model Training step trains a separate Support Vector Regressor (SVR) for each combination creating 2047 Phase One Models. These 2N − 1 predicted scores per text data item form </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Saric</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;sic.</title>
<date></date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics.</booktitle>
<marker>Saric, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Saric, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;sic. 2012. TakeLab: Systems for Measuring Semantic Text Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The Pascal Recognising Textual Entailment Challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment.</title>
<date>2006</date>
<contexts>
<context position="1399" citStr="Dagan et al., 2006" startWordPosition="213" endWordPosition="216">combination. Then it creates a meta-feature space and trains a final model based on that. This two step process improves the results achieved by singlelayer standard learning methodology over the same simple features. We analyze the correlation of feature combinations with the data sets over which they are effective. 1 Introduction The Semantic Textual Similarity (STS) shared task aims to find a unified way of measuring similarity between sentences. In fact, sentence similarity is a core element of tasks trying to establish how two pieces of text are related, such as Textual Entailment (RTE) (Dagan et al., 2006), and Paraphrase Recognition (Dolan et al., 2004). The STS shared task was introduced for SemEval-2012 and was selected as its first shared task. Similar in spirit, STS differs from the well-known RTE shared tasks in two important points: it defines a graded similarity scale to measure similarity of two texts, instead of RTE’s binary yes/no decision and the similarity relation is considered to be symmetrical, whereas the entailment relation of RTE is inherently unidirectional. The leading systems in the 2012 competition used a variety of very simple lexical features. Each system combines a dif</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The Pascal Recognising Textual Entailment Challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<date>1997</date>
<booktitle>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. Proceedings of the 10th International Conference on Research on Computational Linguistics.</booktitle>
<contexts>
<context position="6485" citStr="Jiang and Conrath, 1997" startWordPosition="994" endWordPosition="997">ted using knowledge sources such as WordNet or Roget’s Thesaurus based on the WordNet::Similarity package (Pedersen et al., 2004) and combination techniques (Mihalcea et al., 2006). For the more semantically challenging non-ontologigal relations, for example sanction and Iran, which lexica do not provide, co-occurrence-based measures like ESA are more robust. We use: Lin (Lin, 1998) uses the Brown Corpus of American English to calculate information content of two concepts’ least common subsumer. Then he scales it using the sum of the information content of the compared concepts Jiang-Conrath (Jiang and Conrath, 1997) uses the conditional probability of encountering a concept given an instance of its parent to calculate the information content. Then they define the distance between two concepts to be the sum of the difference between the information content of each of the two given concepts and their least common subsumer Roget’s Thesaurus is another lexical resource and is based on well-crafted concept classification and was created by professional lexicographers. It has a nine-level ontology and doesn’t have one of the major drawbacks of WordNet, which is lack of links between part of speeches. According</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. Proceedings of the 10th International Conference on Research on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A Bit-String Longest-Common-Subsequence Algorithm.</title>
<date>1986</date>
<journal>Information Processing Letters,</journal>
<volume>23</volume>
<issue>5</issue>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I. Dix. 1986. A Bit-String Longest-Common-Subsequence Algorithm. Information Processing Letters, 23(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Rogets Thesaurus and Semantic Similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="7142" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="1101" endWordPosition="1104">ility of encountering a concept given an instance of its parent to calculate the information content. Then they define the distance between two concepts to be the sum of the difference between the information content of each of the two given concepts and their least common subsumer Roget’s Thesaurus is another lexical resource and is based on well-crafted concept classification and was created by professional lexicographers. It has a nine-level ontology and doesn’t have one of the major drawbacks of WordNet, which is lack of links between part of speeches. According to the schema proposed by (Jarmasz and Szpakowicz, 2003) the distance of two terms decreases within the interval of [0,16], as the the common head that subsumes them moves from top to the bottom and becomes more specific. The electronic version of Roget’s Thesaurus which was developed by (Jarmasz and Szpakowicz, 2003) was used for extracting this score 203 Explicit Semantic Analyzer (Gabrilovich and Markovitch, 2007) In order to have broader coverage on word types not represented in lexical resources, specifically for named entities, we add explicit semantic analyzer (ESA) generated features to our feature space 3.3 CLaC-CORE CLaC-CORE first genera</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Rogets Thesaurus and Semantic Similarity. In Proceedings of the Conference on Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: an Update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: an Update. ACM SIGKDD Explorations Newsletter, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew A Jaro</author>
</authors>
<date>1989</date>
<journal>Census of Tampa, Florida. Journal of the American Statistical Association.</journal>
<booktitle>Advances in Record-Linkage Methodology as Applied to Matching the</booktitle>
<contexts>
<context position="4895" citStr="Jaro, 1989" startWordPosition="746" endWordPosition="747">tting, and part of speech (POS) tagging. We extract two main categories of lexical features: explicit and implicit. 3.1 Explicit Lexical Features Sentence similarity at the explicit level is based solely on the input text and measures the similarity between two sentences either by using an n-gram model (ROUGE-1, ROUGE-2, ROUGE-SU4) or by reverting to string similarity (longest common subsequence, jaro, ROUGE-W): Longest Common Subsequence (Allison and Trevor, 1986) compare the length of the longest sequence of characters, not necessarily consecutive ones, in order to detect similarities Jaro (Jaro, 1989) identifies spelling variation between two inputs based on the occurrence of common characters between two text segments at a certain distance ROUGE-W (Lin et al., 2004a), a weighted version of longest common subsequence, takes into account the number of the consecutive characters in each match, giving higher score for those matches that have larger number of consecutive characters in common. This metric was developed to measure the similarity between machine generated text summaries and a manually generated gold standard ROUGE-1 unigrams (Lin et al., 2004a) ROUGE-2 bigrams (Lin et al., 2004a)</context>
</contexts>
<marker>Jaro, 1989</marker>
<rawString>Matthew A. Jaro. 1989. Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida. Journal of the American Statistical Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcelo Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ond&amp;quot;rej Bojar.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3218" citStr="Koehn et al., 2007" startWordPosition="497" endWordPosition="500">nt improvements over the alternative of feature selection or of using any one single combination individually. We propose the method as a viable approach when the characteristics of the data are not well understood and no satisfactory training set is available. 2 Related Work Recently, systems started to approach measuring similarity by combining different resources and methods. For example, the STS-2012 shared task’s leading UKP (B¨ar et al., 2012) system uses n-grams, string similarity, WordNet, and ESA, and a regressor. In addition, they use MOSES, a statistical machine translation system (Koehn et al., 2007), to translate each English sentence into Dutch, German, and Spanish and back into English in an effort to increase their training set of similar text pairs. 202 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 202–206, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics TakeLab (ˇSaric et al., 2012), in place two of the 2012 STS shared task, uses n-gram models, two WordNet-based measures, LSA, and dependencies to align subject-verb-object predicate structures. Includi</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philip Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcelo Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6041" citStr="Mihalcea et al., 2006" startWordPosition="926" endWordPosition="929">ard ROUGE-1 unigrams (Lin et al., 2004a) ROUGE-2 bigrams (Lin et al., 2004a) ROUGE-SU4 4-Skip bigrams (including Unigrams) (Lin et al., 2004a) 3.2 Implicit Lexical Features Sentence similarity at the implicit level uses external resources to make up for the lexical gaps that go otherwise undetected at the explicit level. The synonymy of bag and suitcase is an example of an implicit similarity. This type of implicit similarity can be detected using knowledge sources such as WordNet or Roget’s Thesaurus based on the WordNet::Similarity package (Pedersen et al., 2004) and combination techniques (Mihalcea et al., 2006). For the more semantically challenging non-ontologigal relations, for example sanction and Iran, which lexica do not provide, co-occurrence-based measures like ESA are more robust. We use: Lin (Lin, 1998) uses the Brown Corpus of American English to calculate information content of two concepts’ least common subsumer. Then he scales it using the sum of the information content of the compared concepts Jiang-Conrath (Jiang and Conrath, 1997) uses the conditional probability of encountering a concept given an instance of its parent to calculate the information content. Then they define the dista</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet:: Similarity: Measuring the Relatedness of Concepts. In Demonstration Papers at North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="5990" citStr="Pedersen et al., 2004" startWordPosition="919" endWordPosition="922"> text summaries and a manually generated gold standard ROUGE-1 unigrams (Lin et al., 2004a) ROUGE-2 bigrams (Lin et al., 2004a) ROUGE-SU4 4-Skip bigrams (including Unigrams) (Lin et al., 2004a) 3.2 Implicit Lexical Features Sentence similarity at the implicit level uses external resources to make up for the lexical gaps that go otherwise undetected at the explicit level. The synonymy of bag and suitcase is an example of an implicit similarity. This type of implicit similarity can be detected using knowledge sources such as WordNet or Roget’s Thesaurus based on the WordNet::Similarity package (Pedersen et al., 2004) and combination techniques (Mihalcea et al., 2006). For the more semantically challenging non-ontologigal relations, for example sanction and Iran, which lexica do not provide, co-occurrence-based measures like ESA are more robust. We use: Lin (Lin, 1998) uses the Brown Corpus of American English to calculate information content of two concepts’ least common subsumer. Then he scales it using the sum of the information content of the compared concepts Jiang-Conrath (Jiang and Conrath, 1997) uses the conditional probability of encountering a concept given an instance of its parent to calculate </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan and Jason Michelizzi. 2004. WordNet:: Similarity: Measuring the Relatedness of Concepts. In Demonstration Papers at North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1448" citStr="Dolan et al., 2004" startWordPosition="221" endWordPosition="224"> and trains a final model based on that. This two step process improves the results achieved by singlelayer standard learning methodology over the same simple features. We analyze the correlation of feature combinations with the data sets over which they are effective. 1 Introduction The Semantic Textual Similarity (STS) shared task aims to find a unified way of measuring similarity between sentences. In fact, sentence similarity is a core element of tasks trying to establish how two pieces of text are related, such as Textual Entailment (RTE) (Dagan et al., 2006), and Paraphrase Recognition (Dolan et al., 2004). The STS shared task was introduced for SemEval-2012 and was selected as its first shared task. Similar in spirit, STS differs from the well-known RTE shared tasks in two important points: it defines a graded similarity scale to measure similarity of two texts, instead of RTE’s binary yes/no decision and the similarity relation is considered to be symmetrical, whereas the entailment relation of RTE is inherently unidirectional. The leading systems in the 2012 competition used a variety of very simple lexical features. Each system combines a different set of related features. CLaC Labs investi</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>William B. Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In Proceedings of the 20th International Conference on Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>