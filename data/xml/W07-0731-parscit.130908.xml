<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004326">
<title confidence="0.988248">
The Syntax Augmented MT (SAMT) System for the Shared Task in the 2007
ACL Workshop on Statistical Machine Translation
</title>
<author confidence="0.99092">
Andreas Zollmann and Ashish Venugopal and Matthias Paulik and Stephan Vogel
</author>
<affiliation confidence="0.980895">
School of Computer Science, Carnegie Mellon University, Pittsburgh
interACT Lab, University of Karlsruhe
</affiliation>
<email confidence="0.999407">
{ashishv,zollmann,paulik,vogel+}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784454545455">
We describe the CMU-UKA Syntax Augmented
Machine Translation system ‘SAMT’ used for the
shared task “Machine Translation for European Lan-
guages” at the ACL 2007 Workshop on Statistical
Machine Translation. Following an overview of syn-
tax augmented machine translation, we describe pa-
rameters for components in our open-source SAMT
toolkit that were used to generate translation results
for the Spanish to English in-domain track of the
shared task and discuss relative performance against
our phrase-based submission.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863724137931">
As Chiang (2005) and Koehn et al. (2003) note,
purely lexical “phrase-based” translation models
suffer from sparse data effects when translating con-
ceptual elements that span or skip across several
source language words. Phrase-based models also
rely on distance and lexical distortion models to rep-
resent the reordering effects across language pairs.
However, such models are typically applied over
limited source sentence ranges to prevent errors in-
troduced by these models and to maintain efficient
decoding (Och and Ney, 2004).
To address these concerns, hierarchically struc-
tured models as in Chiang (2005) define weighted
transduction rules, interpretable as components of
a probabilistic synchronous grammar (Aho and Ull-
man, 1969) that represent translation and reordering
operations. In this work, we describe results from
the open-source Syntax Augmented Machine Trans-
lation (SAMT) toolkit (Zollmann and Venugopal,
2006) applied to the Spanish-to-English in-domain
translation task of the ACL’07 workshop on statisti-
cal machine translation.
We begin by describing the probabilistic model of
translation applied by the SAMT toolkit. We then
present settings for the pipeline of SAMT tools that
we used in our shared task submission. Finally, we
compare our translation results to the CMU-UKA
phrase-based SMT system and discuss relative per-
formance.
</bodyText>
<sectionHeader confidence="0.97919" genericHeader="method">
2 Synchronous Grammars for SMT
</sectionHeader>
<bodyText confidence="0.9967954">
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS, a target terminal set (target
vocabulary) TT, a shared nonterminal set N and pro-
duction rules of the form
</bodyText>
<equation confidence="0.821872">
X → h-y, α,∼,wi
</equation>
<bodyText confidence="0.987642">
where following (Chiang, 2005)
</bodyText>
<listItem confidence="0.99991675">
• X ∈ N is a nonterminal
• -y ∈ (N ∪ TS)� : sequence of source nonterminals
and terminals
• α ∈ (N ∪ TT)* : sequence of target nonterminals
and terminals
• the count #NT(-y) of nonterminal tokens in -y is
equal to the count #NT(α) of nonterminal tokens
in α,
• ∼: {1, ... , #NT(-y)} → {1, ... , #NT(α)} one-
to-one mapping from nonterminal tokens in -y to
nonterminal tokens in α
• w ∈ [0, ∞) : nonnegative real-valued weight
</listItem>
<bodyText confidence="0.983457333333333">
Chiang (2005) uses a single nonterminal cate-
gory, Galley et al. (2004) use syntactic constituents
for the PSCFG nonterminal set, and Zollmann and
Venugopal (2006) take advantage of CCG (Combi-
natorial Categorical Grammar) (Steedman, 1999) in-
spired “slash” and “plus” categories, focusing on tar-
get (rather than source side) categories to generate
well formed translations.
We now describe the identification and estima-
tion of PSCFG rules from parallel sentence aligned
corpora under the framework proposed by Zollmann
and Venugopal (2006).
</bodyText>
<page confidence="0.985502">
216
</page>
<bodyText confidence="0.486144">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 216–219,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.95549">
2.1 Grammar Induction
</subsectionHeader>
<bodyText confidence="0.999353517241379">
Zollmann and Venugopal (2006) describe a process
to generate a PSCFG given parallel sentence pairs
(f, e), a parse tree 7r for each e, the maximum a
posteriori word alignment a over (f, e), and phrase
pairs Phrases(a) identified by any alignment-driven
phrase induction technique such as e.g. (Och and
Ney, 2004).
Each phrase in Phrases(a) (phrases identifiable
from a) is first annotated with a syntactic category
to produce initial rules. If the target span of the
phrase does not match a constituent in 7r, heuristics
are used to assign categories that correspond to par-
tial rewriting of the tree. These heuristics first con-
sider concatenation operations, forming categories
like “NP+VP”, and then resort to CCG style “slash”
categories like “NP/NN” giving preference to cate-
gories found closer to the leaves of the tree.
To illustrate this process, consider the following
French-English sentence pair and selected phrase
pairs obtained by phrase induction on an automat-
ically produced alignment a, and matching target
spans with 7r.
The alignment a with the associated target side
parse tree is shown in Fig. 1 in the alignment visual-
ization style defined by Galley et al. (2004).
Following the Data-Oriented Parsing inspired
rule generalization technique proposed by Chiang
(2005), one can now generalize each identified
rule (initial or already partially generalized) N —*
</bodyText>
<equation confidence="0.8912825">
f1 ... f,,/e1 ... e,,, for which there is an initial rule
M —* fi ... f,,/ej ... e„ where 1 &lt; i &lt; u &lt; m and
1 &lt; j &lt; v &lt; n, to obtain a new rule
N → f1 ... fi−1Mkfv.+1 ... f�/e1 ... ej−1Mkev+1 ... en
</equation>
<bodyText confidence="0.999226666666667">
where k is an index for the nonterminal M that in-
dicates the one-to-one correspondence between the
new M tokens on the two sides (it is not in the space
of word indices like i, j, u, v, m, n). The initial rules
listed above can be generalized to additionally ex-
tract the following rules from f, e.
</bodyText>
<figure confidence="0.5494006">
S → PRP1 ne va pas , PRP1 does not go
S → il ne VB1 pas , he does not VB1
S → il RB+VB1, he does RB+VB1
S → PRP1 RB+VB2, PRP1 does RB+VB2
RB+VB → ne VB1 pas, not VB1
</figure>
<figureCaption confidence="0.7140735">
Fig. 2 uses regions to identify the labeled, source
and target side span for all initial rules extracted on
</figureCaption>
<bodyText confidence="0.99818975">
our example sentence pair and parse. Under this rep-
resentation, generalization can be viewed as a pro-
cess that selects a region, and proceeds to subtract
out any sub-region to form a generalized rule.
</bodyText>
<figure confidence="0.578792555555556">
S
������� ������� �������
NP VP
�������
PRN AUX RB VB
he does not go
�������
������� �������
il ne va pas
</figure>
<figureCaption confidence="0.92558575">
Figure 1: Alignment graph (word alignment and target parse
tree) for a French-English sentence pair.
Figure 2: Spans of initial lexical phrases w.r.t. f, e. Each phrase
is labeled with a category derived from the tree in Fig. 1.
</figureCaption>
<subsectionHeader confidence="0.998619">
2.2 Decoding
</subsectionHeader>
<bodyText confidence="0.9991374">
Given a source sentence f, the translation task under
a PSCFG grammar can be expressed analogously to
monolingual parsing with a CFG. We find the most
likely derivation D with source-side f and read off
the English translation from this derivation:
</bodyText>
<equation confidence="0.7013145">
e = tgt arg max �p(D) (1)
(D:src(D)=f
</equation>
<bodyText confidence="0.988511">
where tgt(D) refers to the target terminals and
src(D) to the source terminals generated by deriva-
tion D.
Our distribution p over derivations is defined by a
log-linear model. The probability of a derivation D
</bodyText>
<figure confidence="0.987309892857143">
il ne va pas
he does not go
il, he
va, go
ne va pas, not go
S → il ne va pas, he does not go
f =
e =
PRP →
VB →
RB+VB →
S
il 1 ne 2 va 3 pas 4
does 2
not 3
go 4
he 1
�
�
�
�
�
�
NP+AUX
NP
RB+VB
VB
VP
</figure>
<page confidence="0.972979">
217
</page>
<equation confidence="0.859991">
is defined in terms of the rules r that are used in D:
p(D) = pLM(tgt(D))aLM H,CD Hi 0i(r)az (2)
Z(A)
</equation>
<bodyText confidence="0.999993307692308">
where 0i refers to features defined on each rule,
pLM is a language model (LM) probability applied to
the target terminal symbols generated by the deriva-
tion D, and Z(A) is a normalization constant cho-
sen such that the probabilities sum up to one. The
computational challenges of this search task (com-
pounded by the integration of the LM) are addressed
in (Chiang, 2007; Venugopal et al., 2007). The
feature weights Ai are trained in concert with the
LM weight via minimum error rate (MER) training
(Och, 2003).
We now describe the parameters for the SAMT
implementation of the model described above.
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="method">
3 SAMT Components
</sectionHeader>
<bodyText confidence="0.9999638">
SAMT provides tools to perform grammar induc-
tion ( “extractrules”, “filterrules”), from bilingual
phrase pairs and target language parse trees, as well
as translation (“FastTranslateChart”) of source sen-
tences given an induced grammar.
</bodyText>
<subsectionHeader confidence="0.990655">
3.1 extractrules
</subsectionHeader>
<bodyText confidence="0.996942861111111">
extractrules is the first step of the grammar induc-
tion pipeline, where rules are identified based on the
process described in section 2.1. This tool works on
a per sentence basis, considering phrases extracted
for the training sentence pair (si, ti) and the corre-
sponding target parse tree Tri. extractrules outputs
identified rules for each input sentence pair, along
with associated statistics that play a role in the esti-
mation of the rule features 0. These statistics take
the form of real-valued feature vectors for each rule
as well as summary information collected over the
corpus, such as the frequency of each nonterminal
symbol, or unique rule source sides encountered.
For the shared task evaluation, we ran extrac-
trules with the following extraction parameter
settings to limit the scope and number of rules
extracted. These settings produce the same initial
phrase table as the CMU-UKA phrase based sys-
tem. We limit the source-side length of the phrase
pairs considered as initial rules to 8 (parameter
MaxSourceLength). Further we set the max-
imum number of source and target terminals per
rule (MaxSource/MaxTargetWordCount)
to 5 and 8 respectively with 2 of nonter-
minal pairs (i.e., substitution sites) per rule
(MaxSubstititionCount). We limit the
total number of symbols in each rule to 8
(MaxSource/TargetSymbolCount) and
require all rules to contain at least one source-side
terminal symbol (noAllowAbstractRules,
noAllowRulesWithOnlyTargetTerminals)
since this reduces decoding time considerably. Ad-
ditionally, we discard all rules that contain source
word sequences that do not exist in the development
and test sets provided for the shared task (parameter
-r).
</bodyText>
<subsectionHeader confidence="0.993676">
3.2 filterrules
</subsectionHeader>
<bodyText confidence="0.9998578">
This tool takes as input the rules identified by ex-
tractrules, and associates each rule with a feature
vector 0, representing multiple criteria by which the
decoding process can judge the quality of each rule
and, by extension, each derivation. filterrules is also
in charge of pruning the resulting PSCFG to ensure
tractable decoding.
0 contains both real and Boolean valued features
for each rule. The following probabilistic features
are generated by filterrules:
</bodyText>
<listItem confidence="0.992928833333333">
• p(r |lhs(X)) : Probability of a rule given its left-
hand-side (“result”) nonterminal
• p(r |src(r)) : Prob. of a rule given its source side
• p(ul(src(r)), ul(tgt(r)) |ul(src(r)) : Probability
of the unlabeled source and target side of the rule
given its unlabeled source side.
</listItem>
<bodyText confidence="0.996292625">
Here, the function ul removes all syntactic la-
bels from its arguments, but retains ordering nota-
tion, producing relative frequencies similar to those
used in purely hierarchical systems. As in phrase-
based translation model estimation, 0 also contains
two lexical weights (Koehn et al., 2003), counters
for number of target terminals generated. 0 also
boolean features that describe rule types (i.e. purely
terminal vs purely nonterminal).
For the shared task submission, we pruned away
rules that share the same source side based on
p(r |src(r)) (the source conditioned relative fre-
quency). We prune away a rule if this value is
less that 0.5 times the one of the best performing
rule (parameters BeamFactorLexicalRules,
BeamFactorNonlexicalRules).
</bodyText>
<subsectionHeader confidence="0.996997">
3.3 FastTranslateChart
</subsectionHeader>
<bodyText confidence="0.999970583333333">
The FastTranslateChart decoder is a chart parser
based on the CYK+(Chappelier and Rajman, 1998)
algorithm. Translation experiments in this paper
are performed with a 4-gram SRI language model
trained on the target side of the corpus. Fast-
TranslateChart implements both methods of han-
dling the LM intersection described in (Venugopal
et al., 2007). For this submission, we use the Cube-
Pruning (Chiang, 2007) approach (the default set-
ting). LM and rule feature parameters A are trained
with the included MER training tool. Our prun-
ing settings allow up to 200 chart items per cell
</bodyText>
<page confidence="0.996417">
218
</page>
<bodyText confidence="0.999929714285714">
with left-hand side nonterminal ‘ S’ (the reserved
sentence spanning nonterminal), and 100 items per
cell for each other nonterminal. Beam pruning
based on an (LM-scaled) additive beam of neg-
lob probability 5 is used to prune the search fur-
ther. These pruning settings correspond to setting
’PruningMap=0-100-5-@_S-200-5’.
</bodyText>
<sectionHeader confidence="0.997528" genericHeader="method">
4 Empirical Results
</sectionHeader>
<bodyText confidence="0.999955416666667">
We trained our system on the Spanish-English in-
domain training data provided for the workshop. Ini-
tial data processing and normalizing is described
in the workshop paper for the CMU-UKA ISL
phrase-based system. NIST-BLEU scores are re-
ported on the 2K sentence development ‘dev06’ and
test ‘test06’ corpora as per the workshop guide-
lines (case sensitive, de-tokenized). We compare
our scores against the CMU-UKA ISL phrase-based
submission, a state-of-the art phrase-based SMT
system with part-of-speech (POS) based word re-
ordering (Paulik et al., 2007).
</bodyText>
<subsectionHeader confidence="0.985501">
4.1 Translation Results
</subsectionHeader>
<bodyText confidence="0.999988714285714">
The SAMT system achieves a BLEU score of
32.48% on the ‘dev06’ development corpus and
32.15% on the unseen ’test06’ corpus. This is
slightly better than the score of the CMU-UKA
phrase-based system, which achieves 32.20% and
31.85% when trained and tuned under the same in-
domain conditions. 1
To understand why the syntax augmented ap-
proach has limited additional impact on the Spanish-
to-English task, we consider the impact of reorder-
ing within our phrase-based system. Table 1 shows
the impact of increasing reordering window length
(Koehn et al., 2003) on translation quality for the
‘dev06’ data.2 Increasing the reordering window
past 2 has minimal impact on translation quality,
implying that most of the reordering effects across
Spanish and English are well modeled at the local or
phrase level. The benefit of syntax-based systems to
capture long-distance reordering phenomena based
on syntactic structure seems to be of limited value
for the Spanish to English translation task.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.997929">
In this work, we briefly summarized the Syntax-
augmented MT model, described how we trained
and ran our implementation of that model on
</bodyText>
<footnote confidence="0.969970333333333">
1The CMU-UKA phrase-based workshop submission was
tuned on out-of-domain data as well.
2Variant of the CMU-UKA ISL phrase-based system with-
out POS based reordering. With POS-based reordering turned
on, additional window-based reordering even for window length
1 had no improvement in NIST-BLEU.
</footnote>
<table confidence="0.9921585">
ReOrder 1 2 3 4 POS SAMT
BLEU 31.98 32.24 32.30 32.26 32.20 32.48
</table>
<tableCaption confidence="0.909253">
Table 1: Impact of phrase based reordering model settings com-
pared to SAMT on the ‘dev06’ corpus measured by NIST-
BLEU
</tableCaption>
<bodyText confidence="0.999542">
the MT’07 Spanish-to-English translation task.
We compared SAMT translation results to
a strong phrase-based system trained under
the same conditions. Our system is available
open-source under the GNU General Pub-
lic License (GPL) and can be downloaded at
www.cs.cmu.edu/zollmann/samt
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999497666666667">
Alfred Aho and Jeffrey Ullman. 1969. Syntax directed
translations and the pushdown assembler. Journal of
Computer and System Sciences.
Jean-Cedric. Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proc. of Tabulation in Parsing and Deduction
(TAPD’98), Paris, France.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics. To appear.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc.
of HLT/NAACL, Boston, Massachusetts.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL, Edmonton,Canada.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
put. Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Sap-
poro, Japan, July 6-7.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
phrase-based MT system for the 2007 ACL work-
shop on statistical MT. In Proc. of the Association
of Computational Linguistics Workshop on Statistical
Machine Translation.
Mark Steedman. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL, College Park, Maryland.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to syn-
chronous CFG driven MT. In Proc. of HLT/NAACL,
Rochester, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of the Workshop on Statistical Machine Transla-
tion, HLT/NAACL, New York, June.
</reference>
<page confidence="0.999213">
219
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434961">
<title confidence="0.8972345">The Syntax Augmented MT (SAMT) System for the Shared Task in the 2007 ACL Workshop on Statistical Machine Translation</title>
<author confidence="0.996977">Zollmann Venugopal Paulik</author>
<affiliation confidence="0.775897">School of Computer Science, Carnegie Mellon University, interACT Lab, University of</affiliation>
<abstract confidence="0.99947475">We describe the CMU-UKA Syntax Augmented Machine Translation system ‘SAMT’ used for the shared task “Machine Translation for European Languages” at the ACL 2007 Workshop on Statistical Machine Translation. Following an overview of syntax augmented machine translation, we describe parameters for components in our open-source SAMT toolkit that were used to generate translation results for the Spanish to English in-domain track of the shared task and discuss relative performance against our phrase-based submission.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred Aho</author>
<author>Jeffrey Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences.</journal>
<contexts>
<context position="1622" citStr="Aho and Ullman, 1969" startWordPosition="227" endWordPosition="231">ata effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on distance and lexical distortion models to represent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) define weighted transduction rules, interpretable as components of a probabilistic synchronous grammar (Aho and Ullman, 1969) that represent translation and reordering operations. In this work, we describe results from the open-source Syntax Augmented Machine Translation (SAMT) toolkit (Zollmann and Venugopal, 2006) applied to the Spanish-to-English in-domain translation task of the ACL’07 workshop on statistical machine translation. We begin by describing the probabilistic model of translation applied by the SAMT toolkit. We then present settings for the pipeline of SAMT tools that we used in our shared task submission. Finally, we compare our translation results to the CMU-UKA phrase-based SMT system and discuss r</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Alfred Aho and Jeffrey Ullman. 1969. Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A generalized CYK algorithm for parsing stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proc. of Tabulation in Parsing and Deduction (TAPD’98),</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="11375" citStr="Chappelier and Rajman, 1998" startWordPosition="1863" endWordPosition="1866">so contains two lexical weights (Koehn et al., 2003), counters for number of target terminals generated. 0 also boolean features that describe rule types (i.e. purely terminal vs purely nonterminal). For the shared task submission, we pruned away rules that share the same source side based on p(r |src(r)) (the source conditioned relative frequency). We prune away a rule if this value is less that 0.5 times the one of the best performing rule (parameters BeamFactorLexicalRules, BeamFactorNonlexicalRules). 3.3 FastTranslateChart The FastTranslateChart decoder is a chart parser based on the CYK+(Chappelier and Rajman, 1998) algorithm. Translation experiments in this paper are performed with a 4-gram SRI language model trained on the target side of the corpus. FastTranslateChart implements both methods of handling the LM intersection described in (Venugopal et al., 2007). For this submission, we use the CubePruning (Chiang, 2007) approach (the default setting). LM and rule feature parameters A are trained with the included MER training tool. Our pruning settings allow up to 200 chart items per cell 218 with left-hand side nonterminal ‘ S’ (the reserved sentence spanning nonterminal), and 100 items per cell for ea</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-Cedric. Chappelier and Martin Rajman. 1998. A generalized CYK algorithm for parsing stochastic CFG. In Proc. of Tabulation in Parsing and Deduction (TAPD’98), Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL. David Chiang.</booktitle>
<note>To appear.</note>
<contexts>
<context position="901" citStr="Chiang (2005)" startWordPosition="124" endWordPosition="125"> Karlsruhe {ashishv,zollmann,paulik,vogel+}@cs.cmu.edu Abstract We describe the CMU-UKA Syntax Augmented Machine Translation system ‘SAMT’ used for the shared task “Machine Translation for European Languages” at the ACL 2007 Workshop on Statistical Machine Translation. Following an overview of syntax augmented machine translation, we describe parameters for components in our open-source SAMT toolkit that were used to generate translation results for the Spanish to English in-domain track of the shared task and discuss relative performance against our phrase-based submission. 1 Introduction As Chiang (2005) and Koehn et al. (2003) note, purely lexical “phrase-based” translation models suffer from sparse data effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on distance and lexical distortion models to represent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) defi</context>
<context position="2544" citStr="Chiang, 2005" startWordPosition="368" endWordPosition="369">on. We begin by describing the probabilistic model of translation applied by the SAMT toolkit. We then present settings for the pipeline of SAMT tools that we used in our shared task submission. Finally, we compare our translation results to the CMU-UKA phrase-based SMT system and discuss relative performance. 2 Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and production rules of the form X → h-y, α,∼,wi where following (Chiang, 2005) • X ∈ N is a nonterminal • -y ∈ (N ∪ TS)� : sequence of source nonterminals and terminals • α ∈ (N ∪ TT)* : sequence of target nonterminals and terminals • the count #NT(-y) of nonterminal tokens in -y is equal to the count #NT(α) of nonterminal tokens in α, • ∼: {1, ... , #NT(-y)} → {1, ... , #NT(α)} oneto-one mapping from nonterminal tokens in -y to nonterminal tokens in α • w ∈ [0, ∞) : nonnegative real-valued weight Chiang (2005) uses a single nonterminal category, Galley et al. (2004) use syntactic constituents for the PSCFG nonterminal set, and Zollmann and Venugopal (2006) take advanta</context>
<context position="4973" citStr="Chiang (2005)" startWordPosition="761" endWordPosition="762">ories like “NP+VP”, and then resort to CCG style “slash” categories like “NP/NN” giving preference to categories found closer to the leaves of the tree. To illustrate this process, consider the following French-English sentence pair and selected phrase pairs obtained by phrase induction on an automatically produced alignment a, and matching target spans with 7r. The alignment a with the associated target side parse tree is shown in Fig. 1 in the alignment visualization style defined by Galley et al. (2004). Following the Data-Oriented Parsing inspired rule generalization technique proposed by Chiang (2005), one can now generalize each identified rule (initial or already partially generalized) N —* f1 ... f,,/e1 ... e,,, for which there is an initial rule M —* fi ... f,,/ej ... e„ where 1 &lt; i &lt; u &lt; m and 1 &lt; j &lt; v &lt; n, to obtain a new rule N → f1 ... fi−1Mkfv.+1 ... f�/e1 ... ej−1Mkev+1 ... en where k is an index for the nonterminal M that indicates the one-to-one correspondence between the new M tokens on the two sides (it is not in the space of word indices like i, j, u, v, m, n). The initial rules listed above can be generalized to additionally extract the following rules from f, e. S → PRP1 </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL. David Chiang. 2007. Hierarchical phrase based translation. Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="3039" citStr="Galley et al. (2004)" startWordPosition="464" endWordPosition="467">et vocabulary) TT, a shared nonterminal set N and production rules of the form X → h-y, α,∼,wi where following (Chiang, 2005) • X ∈ N is a nonterminal • -y ∈ (N ∪ TS)� : sequence of source nonterminals and terminals • α ∈ (N ∪ TT)* : sequence of target nonterminals and terminals • the count #NT(-y) of nonterminal tokens in -y is equal to the count #NT(α) of nonterminal tokens in α, • ∼: {1, ... , #NT(-y)} → {1, ... , #NT(α)} oneto-one mapping from nonterminal tokens in -y to nonterminal tokens in α • w ∈ [0, ∞) : nonnegative real-valued weight Chiang (2005) uses a single nonterminal category, Galley et al. (2004) use syntactic constituents for the PSCFG nonterminal set, and Zollmann and Venugopal (2006) take advantage of CCG (Combinatorial Categorical Grammar) (Steedman, 1999) inspired “slash” and “plus” categories, focusing on target (rather than source side) categories to generate well formed translations. We now describe the identification and estimation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by Zollmann and Venugopal (2006). 216 Proceedings of the Second Workshop on Statistical Machine Translation, pages 216–219, Prague, June 2007. c�2007 Association for</context>
<context position="4871" citStr="Galley et al. (2004)" startWordPosition="747" endWordPosition="750">ond to partial rewriting of the tree. These heuristics first consider concatenation operations, forming categories like “NP+VP”, and then resort to CCG style “slash” categories like “NP/NN” giving preference to categories found closer to the leaves of the tree. To illustrate this process, consider the following French-English sentence pair and selected phrase pairs obtained by phrase induction on an automatically produced alignment a, and matching target spans with 7r. The alignment a with the associated target side parse tree is shown in Fig. 1 in the alignment visualization style defined by Galley et al. (2004). Following the Data-Oriented Parsing inspired rule generalization technique proposed by Chiang (2005), one can now generalize each identified rule (initial or already partially generalized) N —* f1 ... f,,/e1 ... e,,, for which there is an initial rule M —* fi ... f,,/ej ... e„ where 1 &lt; i &lt; u &lt; m and 1 &lt; j &lt; v &lt; n, to obtain a new rule N → f1 ... fi−1Mkfv.+1 ... f�/e1 ... ej−1Mkev+1 ... en where k is an index for the nonterminal M that indicates the one-to-one correspondence between the new M tokens on the two sides (it is not in the space of word indices like i, j, u, v, m, n). The initial </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of HLT/NAACL, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL, Edmonton,Canada.</booktitle>
<contexts>
<context position="925" citStr="Koehn et al. (2003)" startWordPosition="127" endWordPosition="130">v,zollmann,paulik,vogel+}@cs.cmu.edu Abstract We describe the CMU-UKA Syntax Augmented Machine Translation system ‘SAMT’ used for the shared task “Machine Translation for European Languages” at the ACL 2007 Workshop on Statistical Machine Translation. Following an overview of syntax augmented machine translation, we describe parameters for components in our open-source SAMT toolkit that were used to generate translation results for the Spanish to English in-domain track of the shared task and discuss relative performance against our phrase-based submission. 1 Introduction As Chiang (2005) and Koehn et al. (2003) note, purely lexical “phrase-based” translation models suffer from sparse data effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on distance and lexical distortion models to represent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) define weighted transduction</context>
<context position="10799" citStr="Koehn et al., 2003" startWordPosition="1778" endWordPosition="1781">abilistic features are generated by filterrules: • p(r |lhs(X)) : Probability of a rule given its lefthand-side (“result”) nonterminal • p(r |src(r)) : Prob. of a rule given its source side • p(ul(src(r)), ul(tgt(r)) |ul(src(r)) : Probability of the unlabeled source and target side of the rule given its unlabeled source side. Here, the function ul removes all syntactic labels from its arguments, but retains ordering notation, producing relative frequencies similar to those used in purely hierarchical systems. As in phrasebased translation model estimation, 0 also contains two lexical weights (Koehn et al., 2003), counters for number of target terminals generated. 0 also boolean features that describe rule types (i.e. purely terminal vs purely nonterminal). For the shared task submission, we pruned away rules that share the same source side based on p(r |src(r)) (the source conditioned relative frequency). We prune away a rule if this value is less that 0.5 times the one of the best performing rule (parameters BeamFactorLexicalRules, BeamFactorNonlexicalRules). 3.3 FastTranslateChart The FastTranslateChart decoder is a chart parser based on the CYK+(Chappelier and Rajman, 1998) algorithm. Translation </context>
<context position="13339" citStr="Koehn et al., 2003" startWordPosition="2173" endWordPosition="2176"> word reordering (Paulik et al., 2007). 4.1 Translation Results The SAMT system achieves a BLEU score of 32.48% on the ‘dev06’ development corpus and 32.15% on the unseen ’test06’ corpus. This is slightly better than the score of the CMU-UKA phrase-based system, which achieves 32.20% and 31.85% when trained and tuned under the same indomain conditions. 1 To understand why the syntax augmented approach has limited additional impact on the Spanishto-English task, we consider the impact of reordering within our phrase-based system. Table 1 shows the impact of increasing reordering window length (Koehn et al., 2003) on translation quality for the ‘dev06’ data.2 Increasing the reordering window past 2 has minimal impact on translation quality, implying that most of the reordering effects across Spanish and English are well modeled at the local or phrase level. The benefit of syntax-based systems to capture long-distance reordering phenomena based on syntactic structure seems to be of limited value for the Spanish to English translation task. 5 Conclusions In this work, we briefly summarized the Syntaxaugmented MT model, described how we trained and ran our implementation of that model on 1The CMU-UKA phra</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT/NAACL, Edmonton,Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Comput. Linguistics.</journal>
<contexts>
<context position="1415" citStr="Och and Ney, 2004" startWordPosition="199" endWordPosition="202">sk and discuss relative performance against our phrase-based submission. 1 Introduction As Chiang (2005) and Koehn et al. (2003) note, purely lexical “phrase-based” translation models suffer from sparse data effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on distance and lexical distortion models to represent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) define weighted transduction rules, interpretable as components of a probabilistic synchronous grammar (Aho and Ullman, 1969) that represent translation and reordering operations. In this work, we describe results from the open-source Syntax Augmented Machine Translation (SAMT) toolkit (Zollmann and Venugopal, 2006) applied to the Spanish-to-English in-domain translation task of the ACL’07 workshop on statistical machine translation. We begin by describing the probabilistic model of translation applied by the SAM</context>
<context position="4000" citStr="Och and Ney, 2004" startWordPosition="607" endWordPosition="610">timation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by Zollmann and Venugopal (2006). 216 Proceedings of the Second Workshop on Statistical Machine Translation, pages 216–219, Prague, June 2007. c�2007 Association for Computational Linguistics 2.1 Grammar Induction Zollmann and Venugopal (2006) describe a process to generate a PSCFG given parallel sentence pairs (f, e), a parse tree 7r for each e, the maximum a posteriori word alignment a over (f, e), and phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. (Och and Ney, 2004). Each phrase in Phrases(a) (phrases identifiable from a) is first annotated with a syntactic category to produce initial rules. If the target span of the phrase does not match a constituent in 7r, heuristics are used to assign categories that correspond to partial rewriting of the tree. These heuristics first consider concatenation operations, forming categories like “NP+VP”, and then resort to CCG style “slash” categories like “NP/NN” giving preference to categories found closer to the leaves of the tree. To illustrate this process, consider the following French-English sentence pair and sel</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Comput. Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>6--7</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="7700" citStr="Och, 2003" startWordPosition="1302" endWordPosition="1303"> defined in terms of the rules r that are used in D: p(D) = pLM(tgt(D))aLM H,CD Hi 0i(r)az (2) Z(A) where 0i refers to features defined on each rule, pLM is a language model (LM) probability applied to the target terminal symbols generated by the derivation D, and Z(A) is a normalization constant chosen such that the probabilities sum up to one. The computational challenges of this search task (compounded by the integration of the LM) are addressed in (Chiang, 2007; Venugopal et al., 2007). The feature weights Ai are trained in concert with the LM weight via minimum error rate (MER) training (Och, 2003). We now describe the parameters for the SAMT implementation of the model described above. 3 SAMT Components SAMT provides tools to perform grammar induction ( “extractrules”, “filterrules”), from bilingual phrase pairs and target language parse trees, as well as translation (“FastTranslateChart”) of source sentences given an induced grammar. 3.1 extractrules extractrules is the first step of the grammar induction pipeline, where rules are identified based on the process described in section 2.1. This tool works on a per sentence basis, considering phrases extracted for the training sentence p</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, Sapporo, Japan, July 6-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Paulik</author>
<author>Kay Rottmann</author>
<author>Jan Niehues</author>
<author>Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<date>2007</date>
<booktitle>The ISL phrase-based MT system for the 2007 ACL workshop on statistical MT. In Proc. of the Association of Computational Linguistics Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="12758" citStr="Paulik et al., 2007" startWordPosition="2079" endWordPosition="2082">pond to setting ’PruningMap=0-100-5-@_S-200-5’. 4 Empirical Results We trained our system on the Spanish-English indomain training data provided for the workshop. Initial data processing and normalizing is described in the workshop paper for the CMU-UKA ISL phrase-based system. NIST-BLEU scores are reported on the 2K sentence development ‘dev06’ and test ‘test06’ corpora as per the workshop guidelines (case sensitive, de-tokenized). We compare our scores against the CMU-UKA ISL phrase-based submission, a state-of-the art phrase-based SMT system with part-of-speech (POS) based word reordering (Paulik et al., 2007). 4.1 Translation Results The SAMT system achieves a BLEU score of 32.48% on the ‘dev06’ development corpus and 32.15% on the unseen ’test06’ corpus. This is slightly better than the score of the CMU-UKA phrase-based system, which achieves 32.20% and 31.85% when trained and tuned under the same indomain conditions. 1 To understand why the syntax augmented approach has limited additional impact on the Spanishto-English task, we consider the impact of reordering within our phrase-based system. Table 1 shows the impact of increasing reordering window length (Koehn et al., 2003) on translation qua</context>
</contexts>
<marker>Paulik, Rottmann, Niehues, Hildebrand, Vogel, 2007</marker>
<rawString>Matthias Paulik, Kay Rottmann, Jan Niehues, Silja Hildebrand, and Stephan Vogel. 2007. The ISL phrase-based MT system for the 2007 ACL workshop on statistical MT. In Proc. of the Association of Computational Linguistics Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Alternative quantifier scope in CCG.</title>
<date>1999</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>College Park, Maryland.</location>
<contexts>
<context position="3206" citStr="Steedman, 1999" startWordPosition="489" endWordPosition="490">quence of source nonterminals and terminals • α ∈ (N ∪ TT)* : sequence of target nonterminals and terminals • the count #NT(-y) of nonterminal tokens in -y is equal to the count #NT(α) of nonterminal tokens in α, • ∼: {1, ... , #NT(-y)} → {1, ... , #NT(α)} oneto-one mapping from nonterminal tokens in -y to nonterminal tokens in α • w ∈ [0, ∞) : nonnegative real-valued weight Chiang (2005) uses a single nonterminal category, Galley et al. (2004) use syntactic constituents for the PSCFG nonterminal set, and Zollmann and Venugopal (2006) take advantage of CCG (Combinatorial Categorical Grammar) (Steedman, 1999) inspired “slash” and “plus” categories, focusing on target (rather than source side) categories to generate well formed translations. We now describe the identification and estimation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by Zollmann and Venugopal (2006). 216 Proceedings of the Second Workshop on Statistical Machine Translation, pages 216–219, Prague, June 2007. c�2007 Association for Computational Linguistics 2.1 Grammar Induction Zollmann and Venugopal (2006) describe a process to generate a PSCFG given parallel sentence pairs (f, e), a parse tre</context>
</contexts>
<marker>Steedman, 1999</marker>
<rawString>Mark Steedman. 1999. Alternative quantifier scope in CCG. In Proc. of ACL, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous CFG driven MT.</title>
<date>2007</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<location>Rochester, NY.</location>
<contexts>
<context position="7584" citStr="Venugopal et al., 2007" startWordPosition="1280" endWordPosition="1283"> he does not go f = e = PRP → VB → RB+VB → S il 1 ne 2 va 3 pas 4 does 2 not 3 go 4 he 1 � � � � � � NP+AUX NP RB+VB VB VP 217 is defined in terms of the rules r that are used in D: p(D) = pLM(tgt(D))aLM H,CD Hi 0i(r)az (2) Z(A) where 0i refers to features defined on each rule, pLM is a language model (LM) probability applied to the target terminal symbols generated by the derivation D, and Z(A) is a normalization constant chosen such that the probabilities sum up to one. The computational challenges of this search task (compounded by the integration of the LM) are addressed in (Chiang, 2007; Venugopal et al., 2007). The feature weights Ai are trained in concert with the LM weight via minimum error rate (MER) training (Och, 2003). We now describe the parameters for the SAMT implementation of the model described above. 3 SAMT Components SAMT provides tools to perform grammar induction ( “extractrules”, “filterrules”), from bilingual phrase pairs and target language parse trees, as well as translation (“FastTranslateChart”) of source sentences given an induced grammar. 3.1 extractrules extractrules is the first step of the grammar induction pipeline, where rules are identified based on the process describe</context>
<context position="11626" citStr="Venugopal et al., 2007" startWordPosition="1902" endWordPosition="1905">t share the same source side based on p(r |src(r)) (the source conditioned relative frequency). We prune away a rule if this value is less that 0.5 times the one of the best performing rule (parameters BeamFactorLexicalRules, BeamFactorNonlexicalRules). 3.3 FastTranslateChart The FastTranslateChart decoder is a chart parser based on the CYK+(Chappelier and Rajman, 1998) algorithm. Translation experiments in this paper are performed with a 4-gram SRI language model trained on the target side of the corpus. FastTranslateChart implements both methods of handling the LM intersection described in (Venugopal et al., 2007). For this submission, we use the CubePruning (Chiang, 2007) approach (the default setting). LM and rule feature parameters A are trained with the included MER training tool. Our pruning settings allow up to 200 chart items per cell 218 with left-hand side nonterminal ‘ S’ (the reserved sentence spanning nonterminal), and 100 items per cell for each other nonterminal. Beam pruning based on an (LM-scaled) additive beam of neglob probability 5 is used to prune the search further. These pruning settings correspond to setting ’PruningMap=0-100-5-@_S-200-5’. 4 Empirical Results We trained our syste</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, and Stephan Vogel. 2007. An efficient two-pass approach to synchronous CFG driven MT. In Proc. of HLT/NAACL, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation, HLT/NAACL,</booktitle>
<location>New York,</location>
<contexts>
<context position="1814" citStr="Zollmann and Venugopal, 2006" startWordPosition="254" endWordPosition="257">esent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) define weighted transduction rules, interpretable as components of a probabilistic synchronous grammar (Aho and Ullman, 1969) that represent translation and reordering operations. In this work, we describe results from the open-source Syntax Augmented Machine Translation (SAMT) toolkit (Zollmann and Venugopal, 2006) applied to the Spanish-to-English in-domain translation task of the ACL’07 workshop on statistical machine translation. We begin by describing the probabilistic model of translation applied by the SAMT toolkit. We then present settings for the pipeline of SAMT tools that we used in our shared task submission. Finally, we compare our translation results to the CMU-UKA phrase-based SMT system and discuss relative performance. 2 Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS, a target terminal set </context>
<context position="3131" citStr="Zollmann and Venugopal (2006)" startWordPosition="477" endWordPosition="480">h-y, α,∼,wi where following (Chiang, 2005) • X ∈ N is a nonterminal • -y ∈ (N ∪ TS)� : sequence of source nonterminals and terminals • α ∈ (N ∪ TT)* : sequence of target nonterminals and terminals • the count #NT(-y) of nonterminal tokens in -y is equal to the count #NT(α) of nonterminal tokens in α, • ∼: {1, ... , #NT(-y)} → {1, ... , #NT(α)} oneto-one mapping from nonterminal tokens in -y to nonterminal tokens in α • w ∈ [0, ∞) : nonnegative real-valued weight Chiang (2005) uses a single nonterminal category, Galley et al. (2004) use syntactic constituents for the PSCFG nonterminal set, and Zollmann and Venugopal (2006) take advantage of CCG (Combinatorial Categorical Grammar) (Steedman, 1999) inspired “slash” and “plus” categories, focusing on target (rather than source side) categories to generate well formed translations. We now describe the identification and estimation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by Zollmann and Venugopal (2006). 216 Proceedings of the Second Workshop on Statistical Machine Translation, pages 216–219, Prague, June 2007. c�2007 Association for Computational Linguistics 2.1 Grammar Induction Zollmann and Venugopal (2006) describe a pr</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of the Workshop on Statistical Machine Translation, HLT/NAACL, New York, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>