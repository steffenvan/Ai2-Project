<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001259">
<title confidence="0.9951245">
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
</title>
<author confidence="0.997408">
Cristina Butnariu Su Nam Kim Preslav Nakov
</author>
<affiliation confidence="0.998889">
University College Dublin University of Melbourne National University of Singapore
</affiliation>
<email confidence="0.928908">
ioana.butnariu@ucd.ie nkim@csse.unimelb.edu.au nakov@comp.nus.edu.sg
</email>
<author confidence="0.99636">
Diarmuid O´ S´eaghdha Stan Szpakowicz Tony Veale
</author>
<affiliation confidence="0.999758">
University of Cambridge University of Ottawa University College Dublin
</affiliation>
<address confidence="0.432202">
do242@cam.ac.uk Polish Academy of Sciences tony.veale@ucd.ie
</address>
<email confidence="0.992681">
szpak@site.uottawa.ca
</email>
<sectionHeader confidence="0.993739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816294117647">
Previous research has shown that the mean-
ing of many noun-noun compounds N1 N2
can be approximated reasonably well by
paraphrasing clauses of the form ‘N2 that
... N1’, where ‘... ’ stands for a verb
with or without a preposition. For exam-
ple, malaria mosquito is a ‘mosquito that
carries malaria’. Evaluating the quality of
such paraphrases is the theme of Task 9 at
SemEval-2010. This paper describes some
background, the task definition, the process
of data collection and the task results. We
also venture a few general conclusions be-
fore the participating teams present their
systems at the SemEval-2010 workshop.
There were 5 teams who submitted 7 sys-
tems.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999231235294118">
Noun compounds (NCs) are sequences of two or
more nouns that act as a single noun,1 e.g., stem
cell, stem cell research, stem cell research organi-
zation, etc. Lapata and Lascarides (2003) observe
that NCs pose syntactic and semantic challenges for
three basic reasons: (1) the compounding process
is extremely productive in English; (2) the seman-
tic relation between the head and the modifier is
implicit; (3) the interpretation can be influenced by
contextual and pragmatic factors. Corpus studies
have shown that while NCs are very common in
English, their frequency distribution follows a Zip-
fian or power-law distribution and the majority of
NCs encountered will be rare types (Tanaka and
Baldwin, 2003; Lapata and Lascarides, 2003; Bald-
win and Tanaka, 2004; O´ S6aghdha, 2008). As a
consequence, Natural Language Processing (NLP)
</bodyText>
<footnote confidence="0.799166">
1We follow the definition in (Downing, 1977).
</footnote>
<bodyText confidence="0.999571025641026">
applications cannot afford either to ignore NCs or
to assume that they can be handled by relying on a
dictionary or other static resource.
Trouble with lexical resources for NCs notwith-
standing, NC semantics plays a central role in com-
plex knowledge discovery and applications, includ-
ing but not limited to Question Answering (QA),
Machine Translation (MT), and Information Re-
trieval (IR). For example, knowing the (implicit)
semantic relation between the NC components can
help rank and refine queries in QA and IR, or select
promising translation pairs in MT (Nakov, 2008a).
Thus, robust semantic interpretation of NCs should
be of much help in broad-coverage semantic pro-
cessing.
Proposed approaches to modelling NC seman-
tics have used semantic similarity (Nastase and Sz-
pakowicz, 2003; Moldovan et al., 2004; Kim and
Baldwin, 2005; Nastase and Szpakowicz, 2006;
Girju, 2007; O´ S6aghdha and Copestake, 2007)
and paraphrasing (Vanderwende, 1994; Kim and
Baldwin, 2006; Butnariu and Veale, 2008; Nakov
and Hearst, 2008). The former body of work seeks
to measure the similarity between known and un-
seen NCs by considering various features, usually
context-related. In contrast, the latter group uses
verb semantics to interpret NCs directly, e.g., olive
oil as ‘oil that is extracted from olive(s)’, drug
death as ‘death that is caused by drug(s)’, flu shot
as a ‘shot that prevents flu’.
The growing popularity – and expected direct
utility – of paraphrase-based NC semantics has
encouraged us to propose an evaluation exercise
for the 2010 edition of SemEval. This paper gives
a bird’s-eye view of the task. Section 2 presents
its objective, data, data collection, and evaluation
method. Section 3 lists the participating teams.
Section 4 shows the results and our analysis. In
Section 5, we sum up our experience so far.
</bodyText>
<page confidence="0.993609">
39
</page>
<note confidence="0.543734">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39–44,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.936935" genericHeader="method">
2 Task Description
</sectionHeader>
<subsectionHeader confidence="0.993117">
2.1 The Objective
</subsectionHeader>
<bodyText confidence="0.999969580645161">
For the purpose of the task, we focused on two-
word NCs which are modifier-head pairs of nouns,
such as apple pie or malaria mosquito. There are
several ways to “attack” the paraphrase-based se-
mantics of such NCs.
We have proposed a rather simple problem: as-
sume that many paraphrases can be found – perhaps
via clever Web search – but their relevance is up in
the air. Given sufficient training data, we seek to es-
timate the quality of candidate paraphrases in a test
set. Each NC in the training set comes with a long
list of verbs in the infinitive (often with a prepo-
sition) which may paraphrase the NC adequately.
Examples of apt paraphrasing verbs: olive oil –
be extracted from, drug death – be caused by, flu
shot – prevent. These lists have been constructed
from human-proposed paraphrases. For the train-
ing data, we also provide the participants with a
quality score for each paraphrase, which is a simple
count of the number of human subjects who pro-
posed that paraphrase. At test time, given a noun
compound and a list of paraphrasing verbs, a partic-
ipating system needs to produce aptness scores that
correlate well (in terms of relative ranking) with
the held out human judgments. There may be a
diverse range of paraphrases for a given compound,
some of them in fact might be inappropriate, but
it can be expected that the distribution over para-
phrases estimated from a large number of subjects
will indeed be representative of the compound’s
meaning.
</bodyText>
<subsectionHeader confidence="0.999312">
2.2 The Datasets
</subsectionHeader>
<bodyText confidence="0.999932916666667">
Following Nakov (2008b), we took advantage of
the Amazon Mechanical Turk2 (MTurk) to acquire
paraphrasing verbs from human annotators. The
service offers inexpensive access to subjects for
tasks which require human intelligence. Its API
allows a computer program to run tasks easily and
collate the subjects’ responses. MTurk is becoming
a popular means of eliciting and collecting linguis-
tic intuitions for NLP research; see Snow et al.
(2008) for an overview and a further discussion.
Even though we recruited human subjects,
whom we required to take a qualification test,3
</bodyText>
<footnote confidence="0.6985275">
2www.mturk.com
3We soon realized that we also had to offer a version of
our assignments without a qualification test (at a lower pay
rate) since very few people were willing to take a test. Overall,
</footnote>
<bodyText confidence="0.981190980392157">
data collection was time-consuming since many
annotators did not follow the instructions. We had
to monitor their progress and to send them timely
messages, pointing out mistakes. Although the
MTurk service allows task owners to accept or re-
ject individual submissions, rejection was the last
resort since it has the triply unpleasant effect of
(1) denying the worker her fee, (2) negatively af-
fecting her rating, and (3) lowering our rating as
a requester. We thus chose to try and educate our
workers “on the fly”. Even so, we ended up with
many examples which we had to correct manu-
ally by labor-intensive post-processing. The flaws
were not different from those already described by
Nakov (2008b). Post-editing was also necessary to
lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we
released as trial data the previously collected para-
phrase sets (Nakov, 2008b) for the Levi-250 dataset
(after further review and cleaning). This dataset
consisted of 250 noun-noun compounds form (Levi,
1978), each paraphrased by 25-30 MTurk workers
(without a qualification test).
Training Data. The training dataset was an ex-
tension of the trial dataset. It consisted of the same
250 noun-noun compounds, but the number of an-
notators per compound increased significantly. We
aimed to recruit at least 30 additional MTurk work-
ers per compound; for some compounds we man-
aged to get many more. For example, when we
added the paraphrasing verbs from the trial dataset
to the newly collected verbs, we had 131 different
workers for neighborhood bars, compared to just
50 for tear gas. On the average, we had 72.7 work-
ers per compound. Each worker was instructed
to try to produce at least three paraphrasing verbs,
so we ended up with 191.8 paraphrasing verbs per
compound, 84.6 of them being unique. See Table 1
for more details.
Test Data. The test dataset consisted of 388
noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003) dataset;
and (2) the Lauer (1995) dataset. The former
contains 328 noun-noun compounds (there are
also a number of adjective-noun and adverb-noun
pairs), while the latter contains 266 noun-noun
compounds. Since these datasets overlap between
themselves and with the training dataset, we had
to exclude some examples. In the end, we had 388
we found little difference in the quality of work of subjects
recruited with and without the test.
</bodyText>
<page confidence="0.993839">
40
</page>
<table confidence="0.9863888">
Training: 250 NCs Testing: 388 NCs All: 638 NCs
Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg
MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0
Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3
Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1
</table>
<tableCaption confidence="0.997642">
Table 1: Statistics about the the training/test datasets. Shown are the total number of verbs proposed as
</tableCaption>
<bodyText confidence="0.968008833333333">
well as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.
unique noun-noun compounds for testing, distinct
from those used for training. We aimed for 100
human workers per testing NC, but we could only
get 68.3, with a minimum of 57 and a maximum of
96; there were 185.0 paraphrasing verbs per com-
pound, 70.9 of them being unique, which is close
to what we had for the training data.
Data format. We distribute the training data as
a raw text file. Each line has the following tab-
separated format:
NC paraphrase frequency
where NC is a noun-noun compound (e.g., ap-
ple cake, flu virus), paraphrase is a human-
proposed paraphrasing verb optionally followed
by a preposition, and frequency is the number
of annotators who proposed that paraphrase. Here
is an illustrative extract from the training dataset:
flu virus cause 38
flu virus spread 13
flu virus create 6
flu virus give 5
flu virus produce 5
...
flu virus be made up of 1
flu virus be observed in 1
flu virus exacerbate 1
The test file has a similar format, except that the
frequency is not included and the paraphrases for
each noun compound appear in random order:
...
chest pain originate
chest pain start in
chest pain descend in
chest pain be in
...
</bodyText>
<listItem confidence="0.3299675">
License. All datasets are released under the Cre-
ative Commons Attribution 3.0 Unported license.4
</listItem>
<page confidence="0.542991">
4creativecommons.org/licenses/by/3.0
</page>
<subsectionHeader confidence="0.957526">
2.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999829375">
All evaluation was performed by computing an ap-
propriate measure of similarity/correlation between
system predictions and the compiled judgements of
the human annotators. We did it on a compound-by-
compound basis and averaged over all compounds
in the test dataset. Section 4 shows results for three
measures: Spearman rank correlation, Pearson cor-
relation, and cosine similarity.
Spearman Rank Correlation (ρ) was adopted
as the official evaluation measure for the competi-
tion. As a rank correlation statistic, it does not use
the numerical values of the predictions or human
judgements, only their relative ordering encoded
as integer ranks. For a sample of n items ranked
by two methods x and y, the rank correlation ρ is
calculated as follows:
</bodyText>
<equation confidence="0.944155">
n P xiyi − (P xi)(P yi)
q q
n P x2 i − (P xi)2 n P y2i − (P yi)2
(1)
</equation>
<bodyText confidence="0.998323454545455">
where xi, yi are the ranks given by x and y to the
ith item, respectively. The value of ρ ranges be-
tween -1.0 (total negative correlation) and 1.0 (total
positive correlation).
Pearson Correlation (r) is a standard measure
of correlation strength between real-valued vari-
ables. The formula is the same as (1), but with
xi, yi taking real values rather than rank values;
just like ρ, r’s values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to
compare numerical vectors:
</bodyText>
<equation confidence="0.956341">
Pn i xiyi
cos = (2)
qP
</equation>
<bodyText confidence="0.999960714285714">
For non-negative data, the cosine similarity takes
values between 0.0 and 1.0. Pearson’s r can be
viewed as a version of the cosine similarity which
performs centering on x and y.
Baseline: To help interpret these evaluation mea-
sures, we implemented a simple baseline. A dis-
tribution over the paraphrases was estimated by
</bodyText>
<equation confidence="0.9925425">
ρ=
i x2 Pn
n i y2
i i
</equation>
<page confidence="0.997569">
41
</page>
<table confidence="0.999800176470588">
System Institution Team Description
NC-INTERP International Institute of Prashant Unsupervised model using verb-argument frequen-
Information Technology, Mathur cies from parsed Web snippets and WordNet
Hyderabad smoothing
UCAM University of Cambridge Clemens Hepp- Unsupervised model using verb-argument frequen-
ner cies from the British National Corpus
UCD-GOGGLE-I University College Guofu Li Unsupervised probabilistic model using pattern fre-
UCD-GOGGLE-II Dublin quencies estimated from the Google N-Gram corpus
UCD-GOGGLE-III Paraphrase ranking model learned from training
data
Combination of UCD-GOGGLE-I and UCD-
GOGGLE-II
UCD-PN University College Paul Nulty Scoring according to the probability of a paraphrase
Dublin appearing in the same set as other paraphrases pro-
vided
UVT-MEPHISTO Tilburg University Sander Supervised memory-based ranker using features
Wubben from Google N-Gram Corpus and WordNet
</table>
<tableCaption confidence="0.999712">
Table 2: Teams participating in SemEval-2010 Task 9
</tableCaption>
<bodyText confidence="0.9999646">
summing the frequencies for all compounds in the
training dataset, and the paraphrases for the test ex-
amples were scored according to this distribution.
Note that this baseline entirely ignores the identity
of the nouns in the compound.
</bodyText>
<sectionHeader confidence="0.998613" genericHeader="method">
3 Participants
</sectionHeader>
<bodyText confidence="0.9997788">
The task attracted five teams, one of which (UCD-
GOGGLE) submitted three runs. The participants
are listed in Table 2 along with brief system de-
scriptions; for more details please see the teams’
own description papers.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999959">
The task results appear in Table 3. In an evaluation
by Spearman’s ρ (the official ranking measure),
the winning system was UVT-MEPHISTO, which
scored 0.450. UVT also achieved the top Pear-
son’s r score. UCD-PN is the top-scoring system
according to the cosine measure. One participant
submitted part of his results after the official dead-
line, which is marked by an asterisk.
The participants used a variety of information
sources and estimation methods. UVT-MEPHISTO
is a supervised system that uses frequency informa-
tion from the Google N-Gram Corpus and features
from WordNet (Fellbaum, 1998) to rank candidate
paraphrases. On the other hand, UCD-PN uses
no external resources and no supervised training,
yet came within 0.009 of UVT-MEPHISTO in the
official evaluation. The basic idea of UCD-PN –
that one can predict the plausibility of a paraphrase
simply by knowing which other paraphrases have
been given for that compound regardless of their
frequency – is clearly a powerful one. Unlike the
other systems, UCD-PN used information about the
test examples (not their ranks, of course) for model
estimation; this has similarities to “transductive”
methods for semi-supervised learning. However,
post-hoc analysis shows that UCD-PN would have
preserved its rank if it had estimated its model on
the training data only. On the other hand, if the task
had been designed differently – by asking systems
to propose paraphrases from the set of all possi-
ble verb/preposition combinations – then we would
not expect UCD-PN’s approach to work as well as
models that use corpus information.
The other systems are comparable to UVT-
MEPHISTO in that they use corpus frequencies
to evaluate paraphrases and apply some kind of
semantic smoothing to handle sparsity. How-
ever, UCD-GOGGLE-I, UCAM and NC-INTERP
are unsupervised systems. UCAM uses the 100-
million word BNC corpus, while the other systems
use Web-scale resources; this has presumably ex-
acerbated sparsity issues and contributed to a rela-
tively poor performance.
The hybrid approach exemplified by UCD-
GOGGLE-III combines the predictions of a sys-
tem that models paraphrase correlations and one
that learns from corpus frequencies and thus at-
tains better performance. Given that the two top-
scoring systems can also be characterized as using
these two distinct information sources, it is natu-
ral to consider combining these systems. Simply
normalizing (to unit sum) and averaging the two
sets of prediction values for each compound does
</bodyText>
<page confidence="0.998041">
42
</page>
<table confidence="0.9739056">
Rank System Supervised? Hybrid? Spearman p Pearson r Cosine
1 UVT-MEPHISTO yes no 0.450 0.411 0.635
2 UCD-PN no no 0.441 0.361 0.669
3 UCD-GOGGLE-III yes yes 0.432 0.395 0.652
4 UCD-GOGGLE-II yes no 0.418 0.375 0.660
5 UCD-GOGGLE-I no no 0.380 0.252 0.629
6 UCAM no no 0.267 0.219 0.374
7 NC-INTERP* no no 0.186 0.070 0.466
Baseline yes no 0.425 0.344 0.524
Combining UVT and UCD-PN yes yes 0.472 0.431 0.685
</table>
<tableCaption confidence="0.999914">
Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).
</tableCaption>
<bodyText confidence="0.9994298">
indeed give better scores: Spearman p = 0.472,
r = 0.431, Cosine = 0.685.
The baseline from Section 2.3 turns out to be
very strong. Evaluating with Spearman’s p, only
three systems outperform it. It is less competitive
on the other evaluation measures though. This
suggests that global paraphrase frequencies may
be useful for telling sensible paraphrases from bad
ones, but will not do for quantifying the plausibility
of a paraphrase for a given noun compound.
</bodyText>
<sectionHeader confidence="0.993861" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992076923077">
Given that it is a newly-proposed task, this initial
experiment in paraphrasing noun compounds has
been a moderate success. The participation rate
has been sufficient for the purposes of comparing
and contrasting different approaches to the role
of paraphrases in the interpretation of noun-noun
compounds. We have seen a variety of approaches
applied to the same dataset, and we have been able
to compare the performance of pure approaches to
hybrid approaches, and of supervised approaches
to unsupervised approaches. The results reported
here are also encouraging, though clearly there is
considerable room for improvement.
This task has established a high baseline for sys-
tems to beat. We can take heart from the fact that
the best performance is apparently obtained from a
combination of corpus-derived usage features and
dictionary-derived linguistic knowledge. Although
clever but simple approaches can do quite well on
such a task, it is encouraging to note that the best
results await those who employ the most robust
and the most informed treatments of NCs and their
paraphrases. Despite a good start, this is a chal-
lenge that remains resolutely open. We expect that
the dataset created for the task will be a valuable
resource for future research.
</bodyText>
<sectionHeader confidence="0.994852" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.966893333333333">
This work is partially supported by grants from
Amazon and from the Bulgarian National Science
Foundation (D002-111/15.12.2008 – SmartBook).
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998963212121212">
Timothy Baldwin and Takaaki Tanaka. 2004. Trans-
lation by Machine of Compound Nominals: Getting
it Right. In Proceedings of the ACL-04 Workshop
on Multiword Expressions: Integrating Processing,
pages 24–31, Barcelona, Spain.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
08), pages 81–88, Manchester, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810–842.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Roxana Girju. 2007. Improving the Interpretation
of Noun Phrases with Cross-linguistic Information.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL-07),
pages 568–575, Prague, Czech Republic.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 945–956, Jeju Island, South Ko-
rea.
Su Nam Kim and Timothy Baldwin. 2006. Inter-
preting Semantic Relations in Noun Compounds via
Verb Semantics. In Proceedings of the COLING-
ACL-06 Main Conference Poster Sessions, pages
491–498, Sydney, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional evi-
dence. In Proceedings of the 10th Conference of the
</reference>
<page confidence="0.998383">
43
</page>
<reference confidence="0.998067703125">
European Chapter of the Association for Computa-
tional Linguistics (EACL-03), pages 235–242, Bu-
dapest, Hungary.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, NY.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on Computa-
tional Lexical Semantics, pages 60–67, Boston, MA.
Preslav Nakov and Marti A. Hearst. 2008. Solving
Relational Similarity Problems Using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL-08), pages 452–460, Columbus, OH.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence (ECAI-08), pages 338–342, Patras,
Greece.
Preslav Nakov. 2008b. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study.
In Proceedings of the 13th International Confer-
ence on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA-08), pages 103–117,
Varna, Bulgaria.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics (IWCS-03), pages 285–301, Tilburg, The
Netherlands.
Vivi Nastase and Stan Szpakowicz. 2006. Matching
syntactic-semantic graphs for semantic relation as-
signment. In Proceedings of the 1st Workshop on
Graph Based Methods for Natural Language Pro-
cessing (TextGraphs-06), pages 81–88, New York,
NY.
Diarmuid O´ S´eaghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proceedings of the ACL-07 Workshop
on A Broader Perspective on Multiword Expressions
(MWE-07), pages 57–64, Prague, Czech Republic.
Diarmuid O´ S´eaghdha. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, University of Cam-
bridge.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast – But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-08), pages 254–263, Honolulu, HI.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of the
ACL-03 Workshop on Multiword Expressions (MWE-
03), pages 17–24, Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics (COLING-94), pages 782–788,
Kyoto, Japan.
</reference>
<page confidence="0.999293">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448239">
<title confidence="0.9856785">SemEval-2010 Task 9: The Interpretation of Noun Compounds Using Paraphrasing Verbs and Prepositions</title>
<author confidence="0.999254">Cristina Butnariu Su Nam Kim Preslav Nakov</author>
<affiliation confidence="0.996519">University College Dublin University of Melbourne National University of Singapore</affiliation>
<email confidence="0.641722">ioana.butnariu@ucd.ienkim@csse.unimelb.edu.aunakov@comp.nus.edu.sg</email>
<author confidence="0.996429">O´S´eaghdha Stan Szpakowicz Tony Veale</author>
<affiliation confidence="0.9627675">University of Cambridge University of Ottawa University College Dublin Academy of Sciences tony.veale@ucd.ie</affiliation>
<email confidence="0.907069">szpak@site.uottawa.ca</email>
<abstract confidence="0.9890055">Previous research has shown that the meanof many noun-noun compounds can be approximated reasonably well by clauses of the form where ‘... ’ stands for a verb with or without a preposition. For exammosquito a that carriesEvaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Takaaki Tanaka</author>
</authors>
<title>Translation by Machine of Compound Nominals: Getting it Right.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL-04 Workshop on Multiword Expressions: Integrating Processing,</booktitle>
<pages>24--31</pages>
<location>Barcelona,</location>
<contexts>
<context position="1946" citStr="Baldwin and Tanaka, 2004" startWordPosition="287" endWordPosition="291">ch organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Baldwin and Tanaka, 2004; O´ S6aghdha, 2008). As a consequence, Natural Language Processing (NLP) 1We follow the definition in (Downing, 1977). applications cannot afford either to ignore NCs or to assume that they can be handled by relying on a dictionary or other static resource. Trouble with lexical resources for NCs notwithstanding, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help r</context>
</contexts>
<marker>Baldwin, Tanaka, 2004</marker>
<rawString>Timothy Baldwin and Takaaki Tanaka. 2004. Translation by Machine of Compound Nominals: Getting it Right. In Proceedings of the ACL-04 Workshop on Multiword Expressions: Integrating Processing, pages 24–31, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Tony Veale</author>
</authors>
<title>A ConceptCentered Approach to Noun-Compound Interpretation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING08),</booktitle>
<pages>81--88</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="3062" citStr="Butnariu and Veale, 2008" startWordPosition="460" endWordPosition="463">n Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This paper gives a bird’s-eye vie</context>
</contexts>
<marker>Butnariu, Veale, 2008</marker>
<rawString>Cristina Butnariu and Tony Veale. 2008. A ConceptCentered Approach to Noun-Compound Interpretation. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING08), pages 81–88, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Downing</author>
</authors>
<title>On the creation and use of English compound nouns.</title>
<date>1977</date>
<journal>Language,</journal>
<volume>53</volume>
<issue>4</issue>
<contexts>
<context position="2064" citStr="Downing, 1977" startWordPosition="307" endWordPosition="308">ns: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Baldwin and Tanaka, 2004; O´ S6aghdha, 2008). As a consequence, Natural Language Processing (NLP) 1We follow the definition in (Downing, 1977). applications cannot afford either to ignore NCs or to assume that they can be handled by relying on a dictionary or other static resource. Trouble with lexical resources for NCs notwithstanding, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic</context>
</contexts>
<marker>Downing, 1977</marker>
<rawString>Pamela Downing. 1977. On the creation and use of English compound nouns. Language, 53(4):810–842.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Improving the Interpretation of Noun Phrases with Cross-linguistic Information.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-07),</booktitle>
<pages>568--575</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2943" citStr="Girju, 2007" startWordPosition="445" endWordPosition="446">plications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics</context>
</contexts>
<marker>Girju, 2007</marker>
<rawString>Roxana Girju. 2007. Improving the Interpretation of Noun Phrases with Cross-linguistic Information. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-07), pages 568–575, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic interpretation of noun compounds using WordNet similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05),</booktitle>
<pages>945--956</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="2900" citStr="Kim and Baldwin, 2005" startWordPosition="437" endWordPosition="440"> a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct</context>
</contexts>
<marker>Kim, Baldwin, 2005</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2005. Automatic interpretation of noun compounds using WordNet similarity. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05), pages 945–956, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Interpreting Semantic Relations in Noun Compounds via Verb Semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLINGACL-06 Main Conference Poster Sessions,</booktitle>
<pages>491--498</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3036" citStr="Kim and Baldwin, 2006" startWordPosition="456" endWordPosition="459">on (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This pa</context>
</contexts>
<marker>Kim, Baldwin, 2006</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2006. Interpreting Semantic Relations in Noun Compounds via Verb Semantics. In Proceedings of the COLINGACL-06 Main Conference Poster Sessions, pages 491–498, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>Detecting novel compounds: The role of distributional evidence.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL-03),</booktitle>
<pages>235--242</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1372" citStr="Lapata and Lascarides (2003)" startWordPosition="198" endWordPosition="201">ample, malaria mosquito is a ‘mosquito that carries malaria’. Evaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems. 1 Introduction Noun compounds (NCs) are sequences of two or more nouns that act as a single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Baldwin and Tanaka, 2004; O´ S6aghdha, 2008). As a</context>
</contexts>
<marker>Lapata, Lascarides, 2003</marker>
<rawString>Mirella Lapata and Alex Lascarides. 2003. Detecting novel compounds: The role of distributional evidence. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL-03), pages 235–242, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Noun Compounds.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University.</institution>
<contexts>
<context position="8385" citStr="Lauer (1995)" startWordPosition="1345" endWordPosition="1346">t many more. For example, when we added the paraphrasing verbs from the trial dataset to the newly collected verbs, we had 131 different workers for neighborhood bars, compared to just 50 for tear gas. On the average, we had 72.7 workers per compound. Each worker was instructed to try to produce at least three paraphrasing verbs, so we ended up with 191.8 paraphrasing verbs per compound, 84.6 of them being unique. See Table 1 for more details. Test Data. The test dataset consisted of 388 noun compounds collected from two data sources: (1) the Nastase and Szpakowicz (2003) dataset; and (2) the Lauer (1995) dataset. The former contains 328 noun-noun compounds (there are also a number of adjective-noun and adverb-noun pairs), while the latter contains 266 noun-noun compounds. Since these datasets overlap between themselves and with the training dataset, we had to exclude some examples. In the end, we had 388 we found little difference in the quality of work of subjects recruited with and without the test. 40 Training: 250 NCs Testing: 388 NCs All: 638 NCs Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0 Verb types 32,832 2</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis, Macquarie University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="7401" citStr="Levi, 1978" startWordPosition="1178" endWordPosition="1179">ering our rating as a requester. We thus chose to try and educate our workers “on the fly”. Even so, we ended up with many examples which we had to correct manually by labor-intensive post-processing. The flaws were not different from those already described by Nakov (2008b). Post-editing was also necessary to lemmatize the paraphrasing verbs systematically. Trial Data. At the end of August 2009, we released as trial data the previously collected paraphrase sets (Nakov, 2008b) for the Levi-250 dataset (after further review and cleaning). This dataset consisted of 250 noun-noun compounds form (Levi, 1978), each paraphrased by 25-30 MTurk workers (without a qualification test). Training Data. The training dataset was an extension of the trial dataset. It consisted of the same 250 noun-noun compounds, but the number of annotators per compound increased significantly. We aimed to recruit at least 30 additional MTurk workers per compound; for some compounds we managed to get many more. For example, when we added the paraphrasing verbs from the trial dataset to the newly collected verbs, we had 131 different workers for neighborhood bars, compared to just 50 for tear gas. On the average, we had 72.</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Judith Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
<author>Roxana Girju</author>
</authors>
<title>Models for the Semantic Classification of Noun Phrases.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL-04 Workshop on Computational Lexical Semantics,</booktitle>
<pages>60--67</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2877" citStr="Moldovan et al., 2004" startWordPosition="433" endWordPosition="436">ing, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularit</context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe, and Roxana Girju. 2004. Models for the Semantic Classification of Noun Phrases. In Proceedings of the HLT-NAACL-04 Workshop on Computational Lexical Semantics, pages 60–67, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti A Hearst</author>
</authors>
<title>Solving Relational Similarity Problems Using the Web as a Corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics (ACL-08),</booktitle>
<pages>452--460</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="3087" citStr="Nakov and Hearst, 2008" startWordPosition="464" endWordPosition="467">ple, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This paper gives a bird’s-eye view of the task. Section 2 </context>
</contexts>
<marker>Nakov, Hearst, 2008</marker>
<rawString>Preslav Nakov and Marti A. Hearst. 2008. Solving Relational Similarity Problems Using the Web as a Corpus. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics (ACL-08), pages 452–460, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improved Statistical Machine Translation Using Monolingual Paraphrases.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference on Artificial Intelligence (ECAI-08),</booktitle>
<pages>338--342</pages>
<location>Patras, Greece.</location>
<contexts>
<context position="2639" citStr="Nakov, 2008" startWordPosition="399" endWordPosition="400">ow the definition in (Downing, 1977). applications cannot afford either to ignore NCs or to assume that they can be handled by relying on a dictionary or other static resource. Trouble with lexical resources for NCs notwithstanding, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In cont</context>
<context position="5611" citStr="Nakov (2008" startWordPosition="891" endWordPosition="892">se, which is a simple count of the number of human subjects who proposed that paraphrase. At test time, given a noun compound and a list of paraphrasing verbs, a participating system needs to produce aptness scores that correlate well (in terms of relative ranking) with the held out human judgments. There may be a diverse range of paraphrases for a given compound, some of them in fact might be inappropriate, but it can be expected that the distribution over paraphrases estimated from a large number of subjects will indeed be representative of the compound’s meaning. 2.2 The Datasets Following Nakov (2008b), we took advantage of the Amazon Mechanical Turk2 (MTurk) to acquire paraphrasing verbs from human annotators. The service offers inexpensive access to subjects for tasks which require human intelligence. Its API allows a computer program to run tasks easily and collate the subjects’ responses. MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al. (2008) for an overview and a further discussion. Even though we recruited human subjects, whom we required to take a qualification test,3 2www.mturk.com 3We soon realized that we also</context>
<context position="7063" citStr="Nakov (2008" startWordPosition="1128" endWordPosition="1129">s. We had to monitor their progress and to send them timely messages, pointing out mistakes. Although the MTurk service allows task owners to accept or reject individual submissions, rejection was the last resort since it has the triply unpleasant effect of (1) denying the worker her fee, (2) negatively affecting her rating, and (3) lowering our rating as a requester. We thus chose to try and educate our workers “on the fly”. Even so, we ended up with many examples which we had to correct manually by labor-intensive post-processing. The flaws were not different from those already described by Nakov (2008b). Post-editing was also necessary to lemmatize the paraphrasing verbs systematically. Trial Data. At the end of August 2009, we released as trial data the previously collected paraphrase sets (Nakov, 2008b) for the Levi-250 dataset (after further review and cleaning). This dataset consisted of 250 noun-noun compounds form (Levi, 1978), each paraphrased by 25-30 MTurk workers (without a qualification test). Training Data. The training dataset was an extension of the trial dataset. It consisted of the same 250 noun-noun compounds, but the number of annotators per compound increased significant</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008a. Improved Statistical Machine Translation Using Monolingual Paraphrases. In Proceedings of the 18th European Conference on Artificial Intelligence (ECAI-08), pages 338–342, Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Noun Compound Interpretation Using Paraphrasing Verbs: Feasibility Study.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th International Conference on Artificial Intelligence: Methodology, Systems and Applications (AIMSA-08),</booktitle>
<pages>103--117</pages>
<location>Varna, Bulgaria.</location>
<contexts>
<context position="2639" citStr="Nakov, 2008" startWordPosition="399" endWordPosition="400">ow the definition in (Downing, 1977). applications cannot afford either to ignore NCs or to assume that they can be handled by relying on a dictionary or other static resource. Trouble with lexical resources for NCs notwithstanding, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In cont</context>
<context position="5611" citStr="Nakov (2008" startWordPosition="891" endWordPosition="892">se, which is a simple count of the number of human subjects who proposed that paraphrase. At test time, given a noun compound and a list of paraphrasing verbs, a participating system needs to produce aptness scores that correlate well (in terms of relative ranking) with the held out human judgments. There may be a diverse range of paraphrases for a given compound, some of them in fact might be inappropriate, but it can be expected that the distribution over paraphrases estimated from a large number of subjects will indeed be representative of the compound’s meaning. 2.2 The Datasets Following Nakov (2008b), we took advantage of the Amazon Mechanical Turk2 (MTurk) to acquire paraphrasing verbs from human annotators. The service offers inexpensive access to subjects for tasks which require human intelligence. Its API allows a computer program to run tasks easily and collate the subjects’ responses. MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al. (2008) for an overview and a further discussion. Even though we recruited human subjects, whom we required to take a qualification test,3 2www.mturk.com 3We soon realized that we also</context>
<context position="7063" citStr="Nakov (2008" startWordPosition="1128" endWordPosition="1129">s. We had to monitor their progress and to send them timely messages, pointing out mistakes. Although the MTurk service allows task owners to accept or reject individual submissions, rejection was the last resort since it has the triply unpleasant effect of (1) denying the worker her fee, (2) negatively affecting her rating, and (3) lowering our rating as a requester. We thus chose to try and educate our workers “on the fly”. Even so, we ended up with many examples which we had to correct manually by labor-intensive post-processing. The flaws were not different from those already described by Nakov (2008b). Post-editing was also necessary to lemmatize the paraphrasing verbs systematically. Trial Data. At the end of August 2009, we released as trial data the previously collected paraphrase sets (Nakov, 2008b) for the Levi-250 dataset (after further review and cleaning). This dataset consisted of 250 noun-noun compounds form (Levi, 1978), each paraphrased by 25-30 MTurk workers (without a qualification test). Training Data. The training dataset was an extension of the trial dataset. It consisted of the same 250 noun-noun compounds, but the number of annotators per compound increased significant</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008b. Noun Compound Interpretation Using Paraphrasing Verbs: Feasibility Study. In Proceedings of the 13th International Conference on Artificial Intelligence: Methodology, Systems and Applications (AIMSA-08), pages 103–117, Varna, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring noun-modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the 5th International Workshop on Computational Semantics (IWCS-03),</booktitle>
<pages>285--301</pages>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="2854" citStr="Nastase and Szpakowicz, 2003" startWordPosition="428" endWordPosition="432">resources for NCs notwithstanding, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’</context>
<context position="8351" citStr="Nastase and Szpakowicz (2003)" startWordPosition="1337" endWordPosition="1340">s per compound; for some compounds we managed to get many more. For example, when we added the paraphrasing verbs from the trial dataset to the newly collected verbs, we had 131 different workers for neighborhood bars, compared to just 50 for tear gas. On the average, we had 72.7 workers per compound. Each worker was instructed to try to produce at least three paraphrasing verbs, so we ended up with 191.8 paraphrasing verbs per compound, 84.6 of them being unique. See Table 1 for more details. Test Data. The test dataset consisted of 388 noun compounds collected from two data sources: (1) the Nastase and Szpakowicz (2003) dataset; and (2) the Lauer (1995) dataset. The former contains 328 noun-noun compounds (there are also a number of adjective-noun and adverb-noun pairs), while the latter contains 266 noun-noun compounds. Since these datasets overlap between themselves and with the training dataset, we had to exclude some examples. In the end, we had 388 we found little difference in the quality of work of subjects recruited with and without the test. 40 Training: 250 NCs Testing: 388 NCs All: 638 NCs Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,2</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2003. Exploring noun-modifier semantic relations. In Proceedings of the 5th International Workshop on Computational Semantics (IWCS-03), pages 285–301, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Matching syntactic-semantic graphs for semantic relation assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the 1st Workshop on Graph Based Methods for Natural Language Processing (TextGraphs-06),</booktitle>
<pages>81--88</pages>
<location>New York, NY.</location>
<contexts>
<context position="2930" citStr="Nastase and Szpakowicz, 2006" startWordPosition="441" endWordPosition="444">lex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based</context>
</contexts>
<marker>Nastase, Szpakowicz, 2006</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2006. Matching syntactic-semantic graphs for semantic relation assignment. In Proceedings of the 1st Workshop on Graph Based Methods for Natural Language Processing (TextGraphs-06), pages 81–88, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Cooccurrence Contexts for Noun Compound Interpretation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-07 Workshop on A Broader Perspective on Multiword Expressions (MWE-07),</booktitle>
<pages>57--64</pages>
<location>Prague, Czech Republic.</location>
<marker>S´eaghdha, Copestake, 2007</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. 2007. Cooccurrence Contexts for Noun Compound Interpretation. In Proceedings of the ACL-07 Workshop on A Broader Perspective on Multiword Expressions (MWE-07), pages 57–64, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Learning Compound Noun Semantics.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<marker>S´eaghdha, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha. 2008. Learning Compound Noun Semantics. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<pages>254--263</pages>
<location>Honolulu, HI.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08), pages 254–263, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Timothy Baldwin</author>
</authors>
<title>Nounnoun compound machine translation: A feasibility study on shallow processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-03 Workshop on Multiword Expressions (MWE03),</booktitle>
<pages>17--24</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1891" citStr="Tanaka and Baldwin, 2003" startWordPosition="279" endWordPosition="282">1 e.g., stem cell, stem cell research, stem cell research organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Baldwin and Tanaka, 2004; O´ S6aghdha, 2008). As a consequence, Natural Language Processing (NLP) 1We follow the definition in (Downing, 1977). applications cannot afford either to ignore NCs or to assume that they can be handled by relying on a dictionary or other static resource. Trouble with lexical resources for NCs notwithstanding, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit)</context>
</contexts>
<marker>Tanaka, Baldwin, 2003</marker>
<rawString>Takaaki Tanaka and Timothy Baldwin. 2003. Nounnoun compound machine translation: A feasibility study on shallow processing. In Proceedings of the ACL-03 Workshop on Multiword Expressions (MWE03), pages 17–24, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Algorithm for Automatic Interpretation of Noun Sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94),</booktitle>
<pages>782--788</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="3013" citStr="Vanderwende, 1994" startWordPosition="454" endWordPosition="455">, Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; Girju, 2007; O´ S6aghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edit</context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>Lucy Vanderwende. 1994. Algorithm for Automatic Interpretation of Noun Sequences. In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94), pages 782–788, Kyoto, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>