<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004851">
<title confidence="0.9992">
It Depends: Dependency Parser Comparison
Using A Web-based Evaluation Tool
</title>
<author confidence="0.973066">
Jinho D. Choi
</author>
<affiliation confidence="0.974998">
Emory University
</affiliation>
<address confidence="0.980348">
400 Dowman Dr.
Atlanta, GA 30322, USA
</address>
<email confidence="0.997045">
jchoi31@emory.edu
</email>
<author confidence="0.868858">
Joel Tetreault
</author>
<affiliation confidence="0.794225">
Yahoo Labs
</affiliation>
<address confidence="0.889963">
229 West 43rd St.
New York, NY 10036, USA
</address>
<email confidence="0.751302">
tetreaul@yahoo-inc.com
</email>
<author confidence="0.721456">
Amanda Stent
</author>
<affiliation confidence="0.668006">
Yahoo Labs
</affiliation>
<address confidence="0.900298">
229 West 43rd St.
New York, NY 10036, USA
</address>
<email confidence="0.913348">
stent@yahoo-inc.com
</email>
<sectionHeader confidence="0.995834" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977647058824">
The last few years have seen a surge in the
number of accurate, fast, publicly avail-
able dependency parsers. At the same
time, the use of dependency parsing in
NLP applications has increased. It can be
difficult for a non-expert to select a good
“off-the-shelf” parser. We present a com-
parative analysis of ten leading statistical
dependency parsers on a multi-genre cor-
pus of English. For our analysis, we de-
veloped a new web-based tool that gives
a convenient way of comparing depen-
dency parser outputs. Our analysis will
help practitioners choose a parser to op-
timize their desired speed/accuracy trade-
off, and our tool will help practitioners ex-
amine and compare parser output.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910266666667">
Dependency parsing is a valuable form of syn-
tactic processing for NLP applications due to its
transparent lexicalized representation and robust-
ness with respect to flexible word order languages.
Thanks to over a decade of research on statisti-
cal dependency parsing, many dependency parsers
are now publicly available. In this paper, we re-
port on a comparative analysis of leading statis-
tical dependency parsers using a multi-genre cor-
pus. Our purpose is not to introduce a new pars-
ing algorithm but to assess the performance of ex-
isting systems across different genres of language
use and to provide tools and recommendations
that practitioners can use to choose a dependency
parser. The contributions of this work include:
</bodyText>
<listItem confidence="0.934601153846154">
• A comparison of the accuracy and speed of
ten state-of-the-art dependency parsers, cov-
ering a range of approaches, on a large multi-
genre corpus of English.
• A new web-based tool, DEPENDABLE, for
side-by-side comparison and visualization of
the output from multiple dependency parsers.
• A detailed error analysis for these parsers
using DEPENDABLE, with recommendations
for parser choice for different factors.
• The release of the set of dependencies used
in our experiments, the test outputs from all
parsers, and the parser-specific models.
</listItem>
<sectionHeader confidence="0.999728" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99950152173913">
There have been several shared tasks on de-
pendency parsing conducted by CoNLL (Buch-
holz and Marsi, 2006; Nivre and others, 2007;
Surdeanu and others, 2008; Hajiˇc and others,
2009), SANCL (Petrov and McDonald, 2012),
SPMRL (Seddah and others, 2013), and Se-
mEval (Oepen and others, 2014). These shared
tasks have led to the public release of numerous
statistical parsers. The primary metrics reported
in these shared tasks are: labeled attachment score
(LAS) – the percentage of predicted dependencies
where the arc and the label are assigned correctly;
unlabeled attachment score (UAS) – where the arc
is assigned correctly; label accuracy score (LS) –
where the label is assigned correctly; and exact
match (EM) – the percentage of sentences whose
predicted trees are entirely correct.
Although shared tasks have been tremendously
useful for advancing the state of the art in depen-
dency parsing, most English evaluation has em-
ployed a single-genre corpus, the WSJ portion of
the Penn Treebank (Marcus et al., 1993), so it
is not immediately clear how these results gen-
</bodyText>
<page confidence="0.968614">
387
</page>
<note confidence="0.987719666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 387–396,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.990657">
BC BN MZ NW PT TC WB ALL
Training 171,120 206,057 163,627 876,399 296,437 85,466 284,975 2,084,081
Development 29,962 25,274 15,422 147,958 25,206 11,467 36,351 291,640
Test 35,952 26,424 17,875 60,757 25,883 10,976 38,490 216,357
Training 10,826 10,349 6,672 34,492 21,419 8,969 12,452 105,179
Development 2,117 1,295 642 5,896 1,780 1,634 1,797 15,161
Test 2,211 1,357 780 2,327 1,869 1,366 1,787 11,697
</table>
<tableCaption confidence="0.99671">
Table 1: Distribution of data used for our experiments. The first three/last three rows show the number of
</tableCaption>
<bodyText confidence="0.946186333333333">
tokens/trees in each genre. BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine,
NW: newswire, PT: pivot text, TC: telephone conversation, WB: web text, ALL: all genres combined.
eralize.1 Furthermore, a detailed comparative er-
ror analysis is typically lacking. The most de-
tailed comparison of dependency parsers to date
was performed by McDonald and Nivre (2007;
2011); they analyzed accuracy as a function of
sentence length, dependency distance, valency,
non-projectivity, part-of-speech tags and depen-
dency labels.2 Since then, additional analyses of
dependency parsers have been performed, but ei-
ther with respect to specific linguistic phenom-
ena (e.g. (Nivre et al., 2010; Bender et al., 2011))
or to downstream tasks (e.g. (Miwa and others,
2010; Petrov et al., 2010; Yuret et al., 2013)).
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="method">
3 Data
</sectionHeader>
<subsectionHeader confidence="0.999257">
3.1 OntoNotes 5
</subsectionHeader>
<bodyText confidence="0.99993">
We used the English portion of the OntoNotes 5
corpus, a large multi-lingual, multi-genre cor-
pus annotated with syntactic structure, predicate-
argument structure, word senses, named entities,
and coreference (Weischedel and others, 2011;
Pradhan and others, 2013). We chose this corpus
rather than the Penn Treebank used in most pre-
vious work because it is larger (2.9M vs. 1M to-
kens) and more diverse (7 vs. 1 genres). We used
the standard data split used in CoNLL’12 3, but re-
moved sentences containing only one token so as
not to artificially inflate accuracy.
Table 1 shows the distribution across genres
of training, development, and test data. For the
most strict and realistic comparison, we trained all
ten parsers using automatically assigned POS tags
from the tagger in ClearNLP (Choi and Palmer,
2012a), which achieved accuracies of 97.34 and
97.52 on the development and test data, respec-
tively. We also excluded any “morphological” fea-
</bodyText>
<footnote confidence="0.9970566">
1The SANCL shared task used OntoNotes and the Web
Treebanks instead for better generalization.
2A detailed error analysis of constituency parsing was per-
formed by (Kummerfeld and others, 2012).
3conll.cemantix.org/2012/download/ids/
</footnote>
<bodyText confidence="0.994152">
ture from the input, as these are often not available
in non-annotated data.
</bodyText>
<subsectionHeader confidence="0.994865">
3.2 Dependency Conversion
</subsectionHeader>
<bodyText confidence="0.999945086956522">
OntoNotes provides annotation of constituency
trees only. Several programs are available for con-
verting constituency trees into dependency trees.
Table 2 shows a comparison between three of
the most widely used: the LTH (Johansson and
Nugues, 2007),4, Stanford (de Marneffe and Man-
ning, 2008),5 and ClearNLP (Choi and Palmer,
2012b)6 dependency converters. Compared to the
Stanford converter, the ClearNLP converter pro-
duces a similar set of dependency labels but gen-
erates fewer unclassified dependencies (0.23% vs.
3.62%), which makes the training data less noisy.
Both the LTH and ClearNLP converters pro-
duce long-distance dependencies and use function
tags for the generation of dependency relations,
which allows one to generate rich dependency
structures including non-projective dependencies.
However, only the ClearNLP converter adapted
the new Treebank guidelines used in OntoNotes.
It can also produce secondary dependencies (e.g.
right-node raising, referent), which can be used for
further analysis. We used the ClearNLP converter
to produce dependencies for our experiments.
</bodyText>
<table confidence="0.9941264">
LTH Stanford ClearNLP
Long-distance ✓ ✓
Secondary 1 2 4
Function tags ✓ ✓
New TB format ✓
</table>
<tableCaption confidence="0.789884666666667">
Table 2: Dependency converters. The “secondary”
row shows how many types of secondary depen-
dencies that can be produced by each converter.
</tableCaption>
<footnote confidence="0.999992333333333">
4http://nlp.cs.lth.se/software
5http://nlp.stanford.edu/software
6http://www.clearnlp.com
</footnote>
<page confidence="0.993715">
388
</page>
<table confidence="0.999863818181818">
Parser Approach Language License
ClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache
GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2
LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a
Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2
RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT
Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS
spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual
SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2
Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2
Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache
</table>
<tableCaption confidence="0.999561">
Table 3: Dependency parsers used in our experiments.
</tableCaption>
<sectionHeader confidence="0.993028" genericHeader="method">
4 Parsers
</sectionHeader>
<bodyText confidence="0.999935615384616">
We compared ten state of the art parsers repre-
senting a wide range of contemporary approaches
to statistical dependency parsing (Table 3). We
trained each parser using the training data from
OntoNotes. For all parsers we trained using the
automatic POS tags generated during data prepro-
cessing, as described above.
Training settings For most parsers, we used the
default settings for training. For the SNN parser,
following the recommendation of the developers,
we used the word embeddings from (Collobert and
others, 2011).
Development data ClearNLP, LTDP, SNN and
Yara make use of the development data (for pa-
rameter tuning). Mate and Turbo self-tune param-
eter settings using the training data. The others
were trained using their default/“standard” param-
eter settings.
Beam search ClearNLP, LTDP, Redshift and
Yara have the option of different beam settings.
The higher the beam size, the more accurate the
parser usually becomes, but typically at the ex-
pense of speed. For LTDP and Redshift, we ex-
perimented with beams of 1, 8, 16 and 64 and
found that the highest accuracy was achieved at
beam 8.17 For ClearNLP and Yara, a beam size of
</bodyText>
<footnote confidence="0.961024583333333">
7www.clearnlp.com
8cs.bgu.ac.il/˜yoavg/software/sdparser
9acl.cs.qc.edu/˜lhuang
10code.google.com/p/mate-tools
11github.com/taolei87/RBGParser
12github.com/syllog1sm/Redshift
13honnibal.github.io/spaCy
14nlp.stanford.edu/software/nndep.shtml
15www.ark.cs.cmu.edu/TurboParser
16https://github.com/yahoo/YaraParser
17Due to memory limitations we were unable to train Red-
shift on a beam size greater than 8.
</footnote>
<bodyText confidence="0.999835333333333">
64 produced the best accuracy, while a beam size
of 1 for LTDT, ClearNLP, and Yara produced the
best speed performance. Given this trend, we also
include how those three parsers perform at beam 1
in our analyses.
Feature Sets RBG, Turbo and Yara have the op-
tions of different feature sets. A more complex or
larger feature set has the advantage of accuracy,
but often at the expense of speed. For RBG and
Turbo, we use the ”Standard” setting and for Yara,
we use the default (”not basic”) feature setting.
Output All the parsers other than LTDP output
labeled dependencies. The ClearNLP, Mate, RBG,
and Turbo parsers can generate non-projective de-
pendencies.
</bodyText>
<sectionHeader confidence="0.7401535" genericHeader="method">
5 DEPENDABLE: Web-based Evaluation
and Visualization Tool
</sectionHeader>
<bodyText confidence="0.9998918125">
There are several very useful tools for evaluating
the output of dependency parsers, including the
venerable eval.pl18 script used in the CoNLL
shared tasks, and newer Java-based tools that sup-
port visualization of and search over parse trees
such as TedEval (Tsarfaty et al., 2011),19 Mal-
tEval (Nilsson and Nivre, 2008)20 and “What’s
wrong with my NLP?”.21 Recently, there is mo-
mentum towards web-based tools for annotation
and visualization of NLP pipelines (Stenetorp and
others, 2012). For this work, we used a new web-
based tool, DEPENDABLE, developed by the first
author of this paper. It requires no installation and
so provides a convenient way to evaluate and com-
pare dependency parsers. The following are key
features of DEPENDABLE:
</bodyText>
<footnote confidence="0.99847825">
18ilk.uvt.nl/conll/software.html
19www.tsarfaty.com/unipar/
20www.maltparser.org/malteval.html
21whatswrong.googlecode.com
</footnote>
<page confidence="0.986224">
389
</page>
<figureCaption confidence="0.999653">
Figure 1: Screenshot of our evaluation tool. Figure 2: Screenshot of our visualization tool.
</figureCaption>
<listItem confidence="0.99893125">
• It reads any type of Tab Separated Value
(TSV) format, including the CoNLL formats.
• It computes LAS, UAS and LS for parse out-
puts from multiple parsers against gold (man-
ual) parses.
• It computes exact match scores for multiple
parsers, and “oracle ensemble” output, the
upper bound performance obtainable by com-
bining all parser outputs.
• It allows the user to exclude symbol tokens,
projective trees, or non-projective trees.
• It produces detailed analyses by POS tags, de-
pendency labels, sentence lengths, and de-
pendency distances.
• It reports statistical significance values for all
parse outputs (using McNemar’s test).
</listItem>
<bodyText confidence="0.982618090909091">
DEPENDABLE can be also used for visualizing
and comparing multiple dependency trees together
(Figure 2). A key feature is that the user may
select parse trees by specifying a range of accu-
racy scores; this enabled us to perform the er-
ror analyses in Section 6.5. DEPENDABLE al-
lows one to filter trees by sentence length and
highlights arc and label errors. The evalua-
tion and comparison tools are publicly avail-
able at http://nlp.mathcs.emory.edu/
clearnlp/dependable.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
6 Results and Error Analysis
</sectionHeader>
<bodyText confidence="0.999862181818182">
In this section, we report overall parser accu-
racy and speed. We analyze parser accuracy
by sentence length, dependency distance, non-
projectivity, POS tags and dependency labels, and
genre. We report detailed manual error analy-
ses focusing on sentences that multiple parsers
parsed incorrectly.22 All analyses, other than pars-
ing speed, were conducted using the DEPEND-
ABLE tool.23 The full set of outputs from all
parsers, as well as the trained models for each
parser, available at http://amandastent.
com/dependable/.
We also include the greedy parsing results of
ClearNLP, LTDP, and Yara in two of our anal-
yses to better illustrate the differences between
the greedy and non-greedy settings. The greedy
parsing results are denoted by the subscript ‘g’.
These two analyses are the overall accuracy re-
sults, presented in Section 6.1 (Table 4), and the
overall speed results, presented in Section 6.2 (
(Table 5 and Figure ). All other analyses exclude
the ClearNLPg, LTDPg and Yarag.
</bodyText>
<footnote confidence="0.991596888888889">
22For one sentence in the NW data, the LTDP parser failed
to produce a complete parse containing all tokens, so we
removed this sentence for all parsers, leaving 11,696 trees
(216,313 tokens) in the test data.
23We compared the results produced by DEPENDABLE
with those produced by eval07.pl, and verified that LAS,
UAS, LA, and EM were the same when punctuation was
included. Our tool uses a slightly different symbol set than
eval07.pl: !&amp;quot;#$g&amp;’()*+,-./:;&lt;=&gt;?@[\] ‘{|}
</footnote>
<page confidence="0.95365">
390
</page>
<table confidence="0.999982">
With Punctuation Without Punctuation
Overall Exact Match Overall Exact Match
LAS UAS LS LAS UAS LS LAS UAS LS LAS UAS LS
ClearNLP9 89.19 90.63 94.94 47.65 53.00 61.17 90.09 91.72 94.29 49.12 55.01 61.31
GN13 87.59 89.17 93.99 43.78 48.89 56.71 88.75 90.54 93.32 45.44 51.20 56.88
LTDP9 n/a 85.75 n/a n/a 46.38 n/a n/a 87.16 n/a n/a 48.01 n/a
SNN 86.42 88.15 93.54 42.98 48.53 55.87 87.63 89.59 92.70 43.96 49.83 55.91
spaCy 87.92 89.61 94.08 43.36 48.79 55.67 88.95 90.86 93.32 44.97 51.28 55.70
Yara9 85.93 87.64 92.99 42.94 47.77 54.79 87.39 89.32 92.24 44.25 49.44 54.96
ClearNLP 89.87 91.30 95.28 49.38 55.18 63.18 90.64 92.26 94.67 50.61 56.88 63.24
LTDP n/a 88.18 n/a n/a 51.62 n/a n/a 89.17 n/a n/a 53.54 n/a
Mate 90.03 91.62 95.29 49.66 56.44 62.71 90.70 92.50 94.67 50.83 58.36 62.72
RBG 89.57 91.45 94.71 46.49 55.49 58.45 90.23 92.35 94.01 47.64 56.54 58.07
Redshift 89.48 91.01 95.04 49.71 55.82 62.70 90.27 92.00 94.42 50.88 57.28 62.78
Turbo 89.81 91.50 95.00 48.08 55.33 60.49 90.49 92.40 94.34 49.29 57.09 60.52
Yara 89.80 91.36 95.19 50.07 56.18 63.36 90.47 92.24 94.57 51.02 57.53 63.42
</table>
<tableCaption confidence="0.983125">
Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and
non-greedy parsers, respectively.
</tableCaption>
<subsectionHeader confidence="0.993453">
6.1 Overall Accuracy
</subsectionHeader>
<bodyText confidence="0.999984647058824">
In Table 4, we report overall accuracy for each
parser. For clarity, we report results separately
for greedy and non-greedy versions of the parsers.
Over all the different metrics, MATE is a clear
winner, though ClearNLP, RBG, Redshift, Turbo
and Yara are very close in performance. Look-
ing at only the greedy parsers, ClearNLPg shows a
significant advantage over the others.
We conducted a statistical significance test for
the the parsers (greedy versions excluded). All
LAS differences are statistically significant at p &lt;
.01 (using McNemar’s test), except for: RBG vs.
Redshift, Turbo vs. Yara, Turbo vs. ClearNLP and
Yara vs. ClearNLP. All UAS differences are sta-
tistically significant at p &lt; .01 (using McNemar’s
test), except for: SNN vs. LTDP, Turbo vs. Red-
shift, Yara vs. RBG and ClearNLP vs. Yara.
</bodyText>
<subsectionHeader confidence="0.999228">
6.2 Overall Speed
</subsectionHeader>
<bodyText confidence="0.980327176470588">
We ran timing experiments on a 64 core machine
with 16 Intel Xeon E5620 2.40 GHz processors
and 24G RAM, and used the unix time com-
mand to time each run. Some parsers are multi-
threaded; for these, we ran in single-thread mode
(since any parser can be externally parallelized).
Most parsers do not report model load time, so we
first ran each parser five times with a test set of
10 sentences, and then averaged the middle three
times to get the average model load time.24 Next,
we ran each parser five times with the entire test
set and derived the overall parse time by averag-
ing the middle three parse times. We then sub-
tracted the average model time from the average
24Recall we exclude single-token sentences from our tests.
parse time and averaged over the number of sen-
tences and tokens.
</bodyText>
<table confidence="0.999566928571428">
Sent/Sec Tokens/Sec Language
ClearNLP9 555 10,271 Java
GN13 95 1,757 Python
LTDP9 232 4,287 Python
SNN 465 8,602 Java
spaCy 755 13,963 Cython
Yara9 532 9,838 Java
ClearNLP 72 1,324 Java
LTDP 26 488 Python
Mate 30 550 Java
RBG 57 1,056 Java
Redshift 188 3,470 Cython
Turbo 19 349 C++
Yara 18 340 Java
</table>
<tableCaption confidence="0.997878">
Table 5: Overall parsing speed.
</tableCaption>
<figureCaption confidence="0.8813595">
Figure 3: Number of sentences parsed per second
by each parser with respect to sentence length.
</figureCaption>
<bodyText confidence="0.988302">
Table 5 shows overall parsing speed for each
parser. spaCy is the fastest greedy parser and Red-
shift is the fastest non-greedy parser. Figure 3
</bodyText>
<page confidence="0.994102">
391
</page>
<bodyText confidence="0.999808">
shows an analysis of parsing speed by sentence
length in bins of length 10. As expected, as sen-
tence length increases, parsing speed decreases re-
markably.
</bodyText>
<subsectionHeader confidence="0.998506">
6.3 Detailed Accuracy Analyses
</subsectionHeader>
<bodyText confidence="0.999663866666667">
For the following more detailed analyses, we used
all tokens (including punctuation). As mentioned
earlier, we exclude ClearNLPg, LTDPg and Yarag
from these analyses and instead use their respec-
tive non-greedy modes yielding higher accuracy.
Sentence Length We analyzed parser accuracy
by sentence length in bins of length 10 (Figure 4).
As expected, all parsers perform better on shorter
sentences. For sentences under length 10, UAS
ranges from 93.49 to 95.5; however, UAS de-
clines to a range of 81.66 and 86.61 for sen-
tence lengths greater than 50. The most accurate
parsers (ClearNLP, Mate, RBG, Redshift, Turbo,
and Yara) separate from the remaining when sen-
tence length is more than 20 tokens.
</bodyText>
<figureCaption confidence="0.941445">
Figure 4: UAS by sentence length.
</figureCaption>
<bodyText confidence="0.999637705882353">
Dependency Distance We analyzed parser ac-
curacy by dependency distance (depth from each
dependent to its head; Figure 5). Accuracy falls
off more slowly as dependency distance increases
for the top 6 parsers vs. the rest.
Projectivity Some of our parsers only produce
projective parses. Table 6 shows parsing accuracy
for trees containing only projective arcs (11,231
trees, 202,521 tokens) and for trees containing
non-projective arcs (465 trees, 13,792 tokens). As
before, all differences are statistically significant
at p &lt; .01 except for: Redshift vs. RBG for over-
all LAS; LTDP vs. SNN for overall UAS; and
Turbo vs. SpaCy for overall UAS. For strictly pro-
jective trees, the LTDP parser is 5th from the top in
UAS. Apart from this, the grouping between “very
good” and “good” parsers does not change.
</bodyText>
<figureCaption confidence="0.973161">
Figure 5: UAS by dependency distance.
</figureCaption>
<table confidence="0.99988025">
Projective only Non-proj. only
LAS UAS LAS UAS
ClearNLP 90.20 91.62 85.10 86.72
GN13 88.00 89.57 81.56 83.37
LTDP n/a 90.24 n/a 57.83
Mate 90.34 91.91 85.51 87.40
RBG 89.86 91.72 84.83 86.94
Redshift 89.90 91.41 83.30 85.12
SNN 86.83 88.55 80.37 82.32
spaCy 88.31 89.99 82.15 84.08
Turbo 88.36 89.90 83.50 85.30
Yara 90.20 91.74 83.92 85.74
</table>
<tableCaption confidence="0.994182">
Table 6: Accuracy for proj. and non-proj. trees.
</tableCaption>
<bodyText confidence="0.998844842105263">
Dependency Relations We were interested in
which dependency relations were computed with
high/low overall accuracy, and for which accuracy
varied between parsers. The dependency relations
with the highest average LAS scores (&gt; 97%)
were possessive, hyph, expl, hmod, aux,
det and poss. These relations have strong lexi-
cal clues (e.g. possessive) or occur very often
(e.g. det). Those with the lowest LAS scores
(&lt; 50%) were csubjpass, meta, dep, nmod
and parataxis. These either occur rarely or are
very general (dep).
The most “confusing” dependency relations
(those with the biggest range of accuracies across
parsers) were csubj, preconj, csubjpass,
parataxis, meta and oprd (all with a spread
of &gt; 20%). The Mate and Yara parsers each had
the highest accuracy for 3 out of the top 10 “con-
fusing” dependency relations. The RBG parser
</bodyText>
<page confidence="0.996569">
392
</page>
<bodyText confidence="0.999976423076923">
had the highest accuracy for 4 out of the top 10
“most accurate” dependency relations. SNN had
the lowest accuracy for 5 out of the top 10 “least
accurate” dependency relations, while the RBG
had the lowest accuracy for another 4.
POS Tags We also examined error types by part
of speech tag of the dependent. The POS tags with
the highest average LAS scores (&gt; 97%) were
the highly unambiguous tags POS, WP$, MD, TO,
HYPH, EX, PRP and PRP$. With the exception of
WP$, these tags occur frequently. Those with the
lowest average LAS scores (&lt; 75%) were punctu-
ation markers ((, ) and :, and the rare tags AFX,
FW, NFP and LS.
Genres Table 7 shows parsing accuracy for each
parser for each of the seven genres comprising
the English portion of OntoNotes 5. Mate and
ClearNLP are responsible for the highest accuracy
for some genres, although accuracy differences
among the top four parsers are generally small.
Accuracy is highest for PT (pivot text, the Bible)
and lowest for TC (telephone conversation) and
WB (web data). The web data is itself multi-genre
and includes translations from Arabic and Chi-
nese, while telephone conversation data includes
disfluencies and informal language.
</bodyText>
<subsectionHeader confidence="0.994451">
6.4 Oracle Ensemble Performance
</subsectionHeader>
<bodyText confidence="0.999536454545455">
One popular method for achieving higher accuracy
on a classification task is to use system combina-
tion (Bj¨orkelund and others, 2014; Le Roux and
others, 2012; Le Roux et al., 2013; Sagae and
Lavie, 2006; Sagae and Tsujii, 2010; Haffari et
al., 2011). DEPENDABLE reports ensemble upper
bound performance assuming that the best tree can
be identified by an oracle (macro), or that the best
arc can be identified by an oracle (micro). Ta-
ble 8 provides an upper bound on ensemble per-
formance for future work.
</bodyText>
<table confidence="0.988758333333333">
LAS UAS LS
Macro 94.66 96.00 97.82
Micro 96.52 97.61 98.40
</table>
<tableCaption confidence="0.999532">
Table 8: Oracle ensemble performance.
</tableCaption>
<bodyText confidence="0.999950857142857">
The highest match was achieved between the RBG
and Mate parser (62.22 UAS). ClearNLP, GN13
and LTDP all matched with Redshift the best, and
RBG, Redshift and Turbo matched with Mate the
best. SNN, spaCy and Turbo did not match well
with other parsers; their respective ”best match”
score was never higher than 55.
</bodyText>
<subsectionHeader confidence="0.966542">
6.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999576">
From the test data, we pulled out parses where
only one parser achieved very high accuracy, and
parses where only one parser had low accuracy
(Table 9). As with the detailed performance anal-
yses, we used the most accurate version of each
parser for this analysis. Mate has the highest num-
ber of “generally good” parses, while the SNN
parser has the highest number of “uniquely bad”
parses. The SNN parser tended to choose the
wrong root, but this did not appear to be tied to the
number of verbs in the sentence - rather, the SNN
parser just makes the earliest “reasonable” choice
of root.
</bodyText>
<table confidence="0.999886666666667">
Parser UAS &gt; 90 = 100 &lt; 90 &lt; 90
All others UAS &lt; 90 &lt; 90 &gt; 90 = 100
ClearNLP 42 11 45 15
LTDP 29 12 182 36
GN13 26 8 148 65
Mate 75 19 44 10
RBG 49 21 49 15
Redshift 38 17 28 8
SNN 70 23 417 142
spaCy 48 17 218 73
Turbo 54 15 28 14
Yara 33 15 27 7
</table>
<tableCaption confidence="0.997712">
Table 9: Differential parsing accuracies.
</tableCaption>
<bodyText confidence="0.99995780952381">
To further analyze these results, we first looked at
the parse trees for “errorful” sentences where the
parsers agreed. From the test data, we extracted
parses for sentences where at least two parsers got
UAS of &lt; 50%. This gave us 253 sentences. The
distribution of these errors across genres varied:
PT - 2.8%, MZ - 3.5%, BN - 9.8%, NW - 10.3%,
WB - 17.4%, BC - 25.3%, TC - 30.8%.
By manual comparison using the DEPEND-
ABLE tool, we identified frequently occurring po-
tential sources of error. We then manually anno-
tated all sentences for these error types. Figure 6
shows the number of “errorful” sentences of each
type. Punctuation attachment “errors” are preva-
lent. For genres with “noisy” text (e.g. broadcast
conversation, telephone conversation) a significant
proportion of errors come from fragmented sen-
tences or those containing backchannels or disflu-
encies. There are also a number of sentences with
what appeared to be manual dependency labeling
errors in the gold annotation.
</bodyText>
<page confidence="0.99744">
393
</page>
<table confidence="0.999320923076923">
BC BN MZ NW PT TC WB
LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
ClearNLP 88.95 90.36 89.59 91.01 89.56 91.24 89.79 91.08 95.88 96.68 87.17 88.93 87.93 89.83
GN13 86.75 88.40 87.38 88.87 87.31 89.10 87.36 88.84 94.06 95.00 85.68 87.60 85.20 87.19
LTDP n/a 86.81 n/a 87.43 n/a 88.87 n/a 88.40 n/a 93.52 n/a 85.85 n/a 86.37
Mate 89.03 90.73 89.30 90.82 90.09 91.92 90.28 91.68 95.71 96.64 87.86 89.87 87.86 89.89
RBG 88.64 90.58 88.99 90.86 89.28 91.45 89.85 91.47 95.27 96.41 87.36 89.65 87.12 89.61
Redshift 88.60 90.19 88.96 90.46 89.11 90.90 89.63 90.99 95.36 96.22 87.14 88.99 87.27 89.31
SNN 85.35 87.08 86.13 87.78 86.00 87.92 86.17 87.74 93.47 94.64 83.50 85.74 84.29 86.50
spaCy 87.27 89.05 87.70 89.31 87.37 89.29 88.00 89.52 94.28 95.27 85.67 87.65 85.16 87.40
Turbo 87.05 88.70 87.58 89.04 88.34 90.02 87.95 89.33 94.39 95.36 85.91 87.93 85.66 87.70
Yara 88.90 90.53 89.40 90.89 89.72 91.42 90.00 91.41 95.41 96.32 87.35 89.19 87.55 89.61
Total 2211 1357 780 2326 1869 1366 1787
</table>
<tableCaption confidence="0.99945">
Table 7: Parsing accuracy by genre.
</tableCaption>
<figureCaption confidence="0.994258">
Figure 6: Common error types in erroneous trees.
</figureCaption>
<subsectionHeader confidence="0.99132">
6.6 Recommendations
</subsectionHeader>
<bodyText confidence="0.999928076923077">
Each of the transition-based parsers that was in-
cluded in this evaluation can use varying beam
widths to trade off speed vs. accuracy, and each
parser has numerous other parameters that can be
tuned. Notwithstanding all these variables, we
can make some recommendations. Figure 7 illus-
trates the speed vs. accuracy tradeoff across the
parsers. For highest accuracy (e.g. in dialog sys-
tems), Mate, RBG, Turbo, ClearNLP and Yara are
good choices. For highest speed (e.g. in web-scale
NLP), spaCy and ClearNLPg are good choices;
SNN and Yarag are also good choices when ac-
curacy is relatively not as important.
</bodyText>
<sectionHeader confidence="0.988246" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.847751">
In this paper we have: (a) provided a detailed com-
parative analysis of several state-of-the-art statis-
tical dependency parsers, focusing on accuracy
</bodyText>
<figureCaption confidence="0.997489">
Figure 7: Speed with respect to accuracy.
</figureCaption>
<bodyText confidence="0.9997266">
and speed; and (b) presented DEPENDABLE, a
new web-based evaluation and visualization tool
for analyzing dependency parsers. DEPENDABLE
supports a wide range of useful functionalities.
In the future, we plan to add regular expression
search over parses, and sorting within results ta-
bles. Our hope is that the results from the eval-
uation as well as the tool will give non-experts
in parsing better insight into which parsing tool
works well under differing conditions. We also
hope that the tool can be used to facilitate evalua-
tion and be used as a teaching aid in NLP courses.
Supplements to this paper include the tool,
the parse outputs, the statistical models for each
parser, and the new set of dependency trees for
OntoNotes 5 created using the ClearNLP depen-
dency converter. We do recommend examining
one’s data and task before choosing and/or train-
ing a parser. Are non-projective parses likely or
desirable? Does the data contain disfluencies, sen-
tence fragments, and other “noisy text” phenom-
ena? What is the average and standard deviation
for sentence length and dependency length? The
analyses in this paper can be used to select a parser
if one has the answers to these questions.
</bodyText>
<page confidence="0.996329">
394
</page>
<bodyText confidence="0.9999785">
In this work we did not implement an ensemble
of parsers, partly because an ensemble necessarily
entails complexity and/or speed delays that render
it unusable by all but experts. However, our anal-
yses indicate that it may be possible to achieve
small but significant increases in accuracy of de-
pendency parsing through ensemble methods. A
good place to start would be with ClearNLP, Mate,
or Redshift in combination with LTDP and Turbo,
SNN or spaCy. In addition, it may be possible to
achieve good performance in particular genres by
doing “mini-ensembles” trained on general pur-
pose data (e.g. WB) and genre-specific data. We
leave this for future work. We also leave for fu-
ture work the comparison of these parsers across
languages.
It remains to be seen what downstream impact
differences in parsing accuracy of 2-5% have on
the goal task. If the impact is small, then speed
and ease of use are the criteria to optimize, and
here spaCy, ClearNLPg, Yarag and SNN are good
choices.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991">
We would like to thank the researchers who
have made available data (especially OntoNotes),
parsers (especially those compared in this work),
and evaluation and visualization tools. Special
thanks go to Boris Abramzon, Matthew Honnibal,
Tao Lei, Danqi Li and Mohammad Sadegh Rasooli
for assistance in installation, trouble-shooting and
general discussion. Additional thanks goes to the
kind folks from the SANCL-SPMRL community
for an informative discussion of evaluation and vi-
sualization tools. Finally, we would like to thank
the three reviewers, as well as Martin Chodorow,
Dean Foster, Joseph Le Roux and Robert Stine, for
feedback on this paper.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997170806451613">
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and
non-local deep dependencies in a large corpus. In
Proceedings of EMNLP.
Anders Bj¨orkelund et al. 2014. Introducing the IMS-
Wrocław-Szeged-CIS entry at the SPMRL 2014
shared task: Reranking and morpho-syntax meet
unlabeled data. In Proceedings of the First Joint
Workshop on Statistical Parsing of Morphologically
Rich Languages and Syntactic Analysis of Non-
Canonical Languages.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of COLING.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of EMNLP.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based Dependency Parsing with Selec-
tional Branching. In Proceedings of the 51stAnnual
Meeting of the Association for Computational Lin-
guistics, ACL’13, pages 1052–1062.
Jinho D. Choi and Martha Palmer. 2012a. Fast and Ro-
bust Part-of-Speech Tagging Using Dynamic Model
Selection. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL’12, pages 363–367.
Jinho D. Choi and Martha Palmer. 2012b. Guidelines
for the Clear Style Constituent to Dependency Con-
version. Technical Report 01-12, Institute of Cogni-
tive Science, University of Colorado Boulder, Boul-
der, CO, USA.
Ronan Collobert et al. 2011. Natural language pro-
cessing (almost) from scratch. Journal of Machine
Learning Research, 12:2493–2537.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Yoav Goldberg and Joakim Nivre. 2013. Training de-
terministic parser with non-deterministic oracles. In
Proceedings of TACL.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL-HLT.
Jan Hajiˇc et al. 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in mul-
tiple languages. In Proceedings of CoNLL.
Matthew Honnibal, Yoav Goldberg, and Mark Johnson.
2013. A non-monotonic arc-eager transition system
for dependency parsing. In Proceedings of CoNLL.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the NAACL.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA.
</reference>
<page confidence="0.987693">
395
</page>
<reference confidence="0.99861346">
Jonathan K. Kummerfeld et al. 2012. Parser show-
down at the wall street corral: An empirical inves-
tigation of error types in parser output. In Proceed-
ings of EMNLP.
Joseph Le Roux et al. 2012. DCU-Paris13 systems
for the SANCL 2012 shared task. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Joseph Le Roux, Antoine Rozenknop, and Jennifer
Foster. 2013. Combining PCFG-LA models with
dual decomposition: A case study with function la-
bels and binarization. In Proceedings of EMNLP.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1381–1391, Baltimore, Maryland, June. Association
for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the Penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
ACL.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proceedings of EMNLP-CoNLL.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197–230.
Makoto Miwa et al. 2010. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP.
Jens Nilsson and Joakim Nivre. 2008. MaltEval:
An evaluation and visualization tool for dependency
parsing. In Proceedings of LREC.
Joakim Nivre et al. 2007. The CoNLL 2007 shared
task on dependency parsing. In Proceedings of
CoNLL.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los G´omez Rodr´ıguez. 2010. Evaluation of de-
pendency parsers on unbounded dependencies. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
833–841, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
Stephan Oepen et al. 2014. SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 63–72.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Pro-
ceedings of the First Workshop on Syntactic Analysis
of Non-Canonical Language (SANCL) Shared Task.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705–713, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Sameer Pradhan et al. 2013. Towards robust linguis-
tic analysis using OntoNotes. In Proceedings of
CoNLL.
Mohammad Sadegh Rasooli and Joel R. Tetreault.
2015. Yara parser: A fast and accurate dependency
parser. CoRR, abs/1503.06733.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings HLT-NAACL.
Kenji Sagae and Jun’ichi Tsujii. 2010. Dependency
parsing and domain adaptation with data-driven LR
models and parser ensembles. In Trends in Parsing
Technology: Dependency Parsing, Domain Adapta-
tion, and Deep Parsing, pages 57–68. Springer.
Djam´e Seddah et al. 2013. Overview of the SPMRL
2013 shared task: A cross-framework evaluation of
parsing morphologically rich languages. In Pro-
ceedings of the 4th Workshop on Statistical Parsing
of Morphologically Rich Languages.
Pontus Stenetorp et al. 2012. BRAT: A web-based tool
for NLP-assisted text annotation. In Proceedings of
the EACL.
Mihai Surdeanu et al. 2008. The CoNLL-2008 shared
task on joint parsing of syntactic and semantic de-
pendencies. In Proceedings of CoNLL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-annotation evaluation. In Pro-
ceedings of EMNLP.
Ralph Weischedel et al. 2011. OntoNotes: A large
training corpus for enhanced processing. In Joseph
Olive, Caitlin Christianson, and John McCary, ed-
itors, Handbook of Natural Language Processing
and Machine Translation. Springer.
Deniz Yuret, Laura Rimell, and Aydin Han. 2013.
Parser evaluation using textual entailments. Lan-
guage Resources and Evaluation, 47(3).
</reference>
<page confidence="0.999086">
396
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.172041">
<title confidence="0.9972305">It Depends: Dependency Parser Using A Web-based Evaluation Tool</title>
<author confidence="0.999553">Jinho D Choi</author>
<affiliation confidence="0.999949">Emory University</affiliation>
<address confidence="0.994232">400 Dowman Atlanta, GA 30322,</address>
<email confidence="0.99924">jchoi31@emory.edu</email>
<author confidence="0.86823">Joel</author>
<affiliation confidence="0.808487">Yahoo</affiliation>
<address confidence="0.990347">229 West 43rd New York, NY 10036,</address>
<email confidence="0.998272">tetreaul@yahoo-inc.com</email>
<author confidence="0.639799">Amanda</author>
<affiliation confidence="0.366747">Yahoo</affiliation>
<address confidence="0.986168">229 West 43rd New York, NY 10036,</address>
<email confidence="0.999672">stent@yahoo-inc.com</email>
<abstract confidence="0.994612">The last few years have seen a surge in the number of accurate, fast, publicly available dependency parsers. At the same time, the use of dependency parsing in NLP applications has increased. It can be difficult for a non-expert to select a good “off-the-shelf” parser. We present a comparative analysis of ten leading statistical dependency parsers on a multi-genre corpus of English. For our analysis, we developed a new web-based tool that gives a convenient way of comparing dependency parser outputs. Our analysis will help practitioners choose a parser to optimize their desired speed/accuracy tradeoff, and our tool will help practitioners examine and compare parser output.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
<author>Yi Zhang</author>
</authors>
<title>Parser evaluation over local and non-local deep dependencies in a large corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4897" citStr="Bender et al., 2011" startWordPosition="761" endWordPosition="764">, NW: newswire, PT: pivot text, TC: telephone conversation, WB: web text, ALL: all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 Data 3.1 OntoNotes 5 We used the English portion of the OntoNotes 5 corpus, a large multi-lingual, multi-genre corpus annotated with syntactic structure, predicateargument structure, word senses, named entities, and coreference (Weischedel and others, 2011; Pradhan and others, 2013). We chose this corpus rather than the Penn Treebank used in most previous work because it is larger (2.9M vs. 1M tokens) and more diverse (7 vs. 1 genres). We used the standard data split used in CoNLL’12 3, but remo</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Zhang, 2011</marker>
<rawString>Emily M. Bender, Dan Flickinger, Stephan Oepen, and Yi Zhang. 2011. Parser evaluation over local and non-local deep dependencies in a large corpus. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
</authors>
<title>Introducing the IMSWrocław-Szeged-CIS entry at the SPMRL 2014 shared task: Reranking and morpho-syntax meet unlabeled data.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of NonCanonical Languages.</booktitle>
<marker>Bj¨orkelund, 2014</marker>
<rawString>Anders Bj¨orkelund et al. 2014. Introducing the IMSWrocław-Szeged-CIS entry at the SPMRL 2014 shared task: Reranking and morpho-syntax meet unlabeled data. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of NonCanonical Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="8073" citStr="Bohnet, 2010" startWordPosition="1232" endWordPosition="1233"> ✓ New TB format ✓ Table 2: Dependency converters. The “secondary” row shows how many types of secondary dependencies that can be produced by each converter. 4http://nlp.cs.lth.se/software 5http://nlp.stanford.edu/software 6http://www.clearnlp.com 388 Parser Approach Language License ClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache Table 3: Dependency parsers used in our experiments. 4 Parsers We compared ten state of the art</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2434" citStr="Buchholz and Marsi, 2006" startWordPosition="385" endWordPosition="389">of ten state-of-the-art dependency parsers, covering a range of approaches, on a large multigenre corpus of English. • A new web-based tool, DEPENDABLE, for side-by-side comparison and visualization of the output from multiple dependency parsers. • A detailed error analysis for these parsers using DEPENDABLE, with recommendations for parser choice for different factors. • The release of the set of dependencies used in our experiments, the test outputs from all parsers, and the parser-specific models. 2 Related Work There have been several shared tasks on dependency parsing conducted by CoNLL (Buchholz and Marsi, 2006; Nivre and others, 2007; Surdeanu and others, 2008; Hajiˇc and others, 2009), SANCL (Petrov and McDonald, 2012), SPMRL (Seddah and others, 2013), and SemEval (Oepen and others, 2014). These shared tasks have led to the public release of numerous statistical parsers. The primary metrics reported in these shared tasks are: labeled attachment score (LAS) – the percentage of predicted dependencies where the arc and the label are assigned correctly; unlabeled attachment score (UAS) – where the arc is assigned correctly; label accuracy score (LS) – where the label is assigned correctly; and exact m</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8384" citStr="Chen and Manning, 2014" startWordPosition="1270" endWordPosition="1273">ransition-based, selectional branching (Choi and McCallum, 2013) Java Apache GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache Table 3: Dependency parsers used in our experiments. 4 Parsers We compared ten state of the art parsers representing a wide range of contemporary approaches to statistical dependency parsing (Table 3). We trained each parser using the training data from OntoNotes. For all parsers we trained using the automatic POS tags generated during data preprocessing, as described above. Training settings For most p</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Transition-based Dependency Parsing with Selectional Branching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51stAnnual Meeting of the Association for Computational Linguistics, ACL’13,</booktitle>
<pages>1052--1062</pages>
<contexts>
<context position="7825" citStr="Choi and McCallum, 2013" startWordPosition="1195" endWordPosition="1198">lso produce secondary dependencies (e.g. right-node raising, referent), which can be used for further analysis. We used the ClearNLP converter to produce dependencies for our experiments. LTH Stanford ClearNLP Long-distance ✓ ✓ Secondary 1 2 4 Function tags ✓ ✓ New TB format ✓ Table 2: Dependency converters. The “secondary” row shows how many types of secondary dependencies that can be produced by each converter. 4http://nlp.cs.lth.se/software 5http://nlp.stanford.edu/software 6http://www.clearnlp.com 388 Parser Approach Language License ClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposit</context>
</contexts>
<marker>Choi, McCallum, 2013</marker>
<rawString>Jinho D. Choi and Andrew McCallum. 2013. Transition-based Dependency Parsing with Selectional Branching. In Proceedings of the 51stAnnual Meeting of the Association for Computational Linguistics, ACL’13, pages 1052–1062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL’12,</booktitle>
<pages>363--367</pages>
<contexts>
<context position="5828" citStr="Choi and Palmer, 2012" startWordPosition="915" endWordPosition="918">, and coreference (Weischedel and others, 2011; Pradhan and others, 2013). We chose this corpus rather than the Penn Treebank used in most previous work because it is larger (2.9M vs. 1M tokens) and more diverse (7 vs. 1 genres). We used the standard data split used in CoNLL’12 3, but removed sentences containing only one token so as not to artificially inflate accuracy. Table 1 shows the distribution across genres of training, development, and test data. For the most strict and realistic comparison, we trained all ten parsers using automatically assigned POS tags from the tagger in ClearNLP (Choi and Palmer, 2012a), which achieved accuracies of 97.34 and 97.52 on the development and test data, respectively. We also excluded any “morphological” fea1The SANCL shared task used OntoNotes and the Web Treebanks instead for better generalization. 2A detailed error analysis of constituency parsing was performed by (Kummerfeld and others, 2012). 3conll.cemantix.org/2012/download/ids/ ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into</context>
</contexts>
<marker>Choi, Palmer, 2012</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2012a. Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL’12, pages 363–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Guidelines for the Clear Style Constituent to Dependency Conversion.</title>
<date>2012</date>
<tech>Technical Report 01-12,</tech>
<institution>Institute of Cognitive Science, University of Colorado Boulder,</institution>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="5828" citStr="Choi and Palmer, 2012" startWordPosition="915" endWordPosition="918">, and coreference (Weischedel and others, 2011; Pradhan and others, 2013). We chose this corpus rather than the Penn Treebank used in most previous work because it is larger (2.9M vs. 1M tokens) and more diverse (7 vs. 1 genres). We used the standard data split used in CoNLL’12 3, but removed sentences containing only one token so as not to artificially inflate accuracy. Table 1 shows the distribution across genres of training, development, and test data. For the most strict and realistic comparison, we trained all ten parsers using automatically assigned POS tags from the tagger in ClearNLP (Choi and Palmer, 2012a), which achieved accuracies of 97.34 and 97.52 on the development and test data, respectively. We also excluded any “morphological” fea1The SANCL shared task used OntoNotes and the Web Treebanks instead for better generalization. 2A detailed error analysis of constituency parsing was performed by (Kummerfeld and others, 2012). 3conll.cemantix.org/2012/download/ids/ ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into</context>
</contexts>
<marker>Choi, Palmer, 2012</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2012b. Guidelines for the Clear Style Constituent to Dependency Conversion. Technical Report 01-12, Institute of Cognitive Science, University of Colorado Boulder, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert et al. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLING workshop on Cross-Framework and Cross-Domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Proceedings of the COLING workshop on Cross-Framework and Cross-Domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parser with non-deterministic oracles.</title>
<date>2013</date>
<booktitle>In Proceedings of TACL.</booktitle>
<contexts>
<context position="7897" citStr="Goldberg and Nivre, 2013" startWordPosition="1205" endWordPosition="1208"> which can be used for further analysis. We used the ClearNLP converter to produce dependencies for our experiments. LTH Stanford ClearNLP Long-distance ✓ ✓ Secondary 1 2 4 Function tags ✓ ✓ New TB format ✓ Table 2: Dependency converters. The “secondary” row shows how many types of secondary dependencies that can be produced by each converter. 4http://nlp.cs.lth.se/software 5http://nlp.stanford.edu/software 6http://www.clearnlp.com 388 Parser Approach Language License ClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transit</context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parser with non-deterministic oracles. In Proceedings of TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Marzieh Razavi</author>
<author>Anoop Sarkar</author>
</authors>
<title>An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="22380" citStr="Haffari et al., 2011" startWordPosition="3531" endWordPosition="3534"> differences among the top four parsers are generally small. Accuracy is highest for PT (pivot text, the Bible) and lowest for TC (telephone conversation) and WB (web data). The web data is itself multi-genre and includes translations from Arabic and Chinese, while telephone conversation data includes disfluencies and informal language. 6.4 Oracle Ensemble Performance One popular method for achieving higher accuracy on a classification task is to use system combination (Bj¨orkelund and others, 2014; Le Roux and others, 2012; Le Roux et al., 2013; Sagae and Lavie, 2006; Sagae and Tsujii, 2010; Haffari et al., 2011). DEPENDABLE reports ensemble upper bound performance assuming that the best tree can be identified by an oracle (macro), or that the best arc can be identified by an oracle (micro). Table 8 provides an upper bound on ensemble performance for future work. LAS UAS LS Macro 94.66 96.00 97.82 Micro 96.52 97.61 98.40 Table 8: Oracle ensemble performance. The highest match was achieved between the RBG and Mate parser (62.22 UAS). ClearNLP, GN13 and LTDP all matched with Redshift the best, and RBG, Redshift and Turbo matched with Mate the best. SNN, spaCy and Turbo did not match well with other pars</context>
</contexts>
<marker>Haffari, Razavi, Sarkar, 2011</marker>
<rawString>Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar. 2011. An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Hajiˇc, 2009</marker>
<rawString>Jan Hajiˇc et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Yoav Goldberg</author>
<author>Mark Johnson</author>
</authors>
<title>A non-monotonic arc-eager transition system for dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="8230" citStr="Honnibal et al., 2013" startWordPosition="1251" endWordPosition="1254">onverter. 4http://nlp.cs.lth.se/software 5http://nlp.stanford.edu/software 6http://www.clearnlp.com 388 Parser Approach Language License ClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache Table 3: Dependency parsers used in our experiments. 4 Parsers We compared ten state of the art parsers representing a wide range of contemporary approaches to statistical dependency parsing (Table 3). We trained each parser using the training data fro</context>
</contexts>
<marker>Honnibal, Goldberg, Johnson, 2013</marker>
<rawString>Matthew Honnibal, Yoav Goldberg, and Mark Johnson. 2013. A non-monotonic arc-eager transition system for dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL.</booktitle>
<contexts>
<context position="7991" citStr="Huang et al., 2012" startWordPosition="1219" endWordPosition="1222">our experiments. LTH Stanford ClearNLP Long-distance ✓ ✓ Secondary 1 2 4 Function tags ✓ ✓ New TB format ✓ Table 2: Dependency converters. The “secondary” row shows how many types of secondary dependencies that can be produced by each converter. 4http://nlp.cs.lth.se/software 5http://nlp.stanford.edu/software 6http://www.clearnlp.com 388 Parser Approach Language License ClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache Table 3: Depe</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA.</booktitle>
<contexts>
<context position="6549" citStr="Johansson and Nugues, 2007" startWordPosition="1019" endWordPosition="1022">y. We also excluded any “morphological” fea1The SANCL shared task used OntoNotes and the Web Treebanks instead for better generalization. 2A detailed error analysis of constituency parsing was performed by (Kummerfeld and others, 2012). 3conll.cemantix.org/2012/download/ids/ ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into dependency trees. Table 2 shows a comparison between three of the most widely used: the LTH (Johansson and Nugues, 2007),4, Stanford (de Marneffe and Manning, 2008),5 and ClearNLP (Choi and Palmer, 2012b)6 dependency converters. Compared to the Stanford converter, the ClearNLP converter produces a similar set of dependency labels but generates fewer unclassified dependencies (0.23% vs. 3.62%), which makes the training data less noisy. Both the LTH and ClearNLP converters produce long-distance dependencies and use function tags for the generation of dependency relations, which allows one to generate rich dependency structures including non-projective dependencies. However, only the ClearNLP converter adapted the</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
</authors>
<title>Parser showdown at the wall street corral: An empirical investigation of error types in parser output.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Kummerfeld, 2012</marker>
<rawString>Jonathan K. Kummerfeld et al. 2012. Parser showdown at the wall street corral: An empirical investigation of error types in parser output. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
</authors>
<title>DCU-Paris13 systems for the SANCL 2012 shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<marker>Le Roux, 2012</marker>
<rawString>Joseph Le Roux et al. 2012. DCU-Paris13 systems for the SANCL 2012 shared task. In Proceedings of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
<author>Antoine Rozenknop</author>
<author>Jennifer Foster</author>
</authors>
<title>Combining PCFG-LA models with dual decomposition: A case study with function labels and binarization.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Le Roux, Rozenknop, Foster, 2013</marker>
<rawString>Joseph Le Roux, Antoine Rozenknop, and Jennifer Foster. 2013. Combining PCFG-LA models with dual decomposition: A case study with function labels and binarization. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1381--1391</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="8154" citStr="Lei et al., 2014" startWordPosition="1242" endWordPosition="1245">how many types of secondary dependencies that can be produced by each converter. 4http://nlp.cs.lth.se/software 5http://nlp.stanford.edu/software 6http://www.clearnlp.com 388 Parser Approach Language License ClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache GN138 Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache Table 3: Dependency parsers used in our experiments. 4 Parsers We compared ten state of the art parsers representing a wide range of contemporary approaches to statistical depe</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1381–1391, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3346" citStr="Marcus et al., 1993" startWordPosition="531" endWordPosition="534">rted in these shared tasks are: labeled attachment score (LAS) – the percentage of predicted dependencies where the arc and the label are assigned correctly; unlabeled attachment score (UAS) – where the arc is assigned correctly; label accuracy score (LS) – where the label is assigned correctly; and exact match (EM) – the percentage of sentences whose predicted trees are entirely correct. Although shared tasks have been tremendously useful for advancing the state of the art in dependency parsing, most English evaluation has employed a single-genre corpus, the WSJ portion of the Penn Treebank (Marcus et al., 1993), so it is not immediately clear how these results gen387 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 387–396, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics BC BN MZ NW PT TC WB ALL Training 171,120 206,057 163,627 876,399 296,437 85,466 284,975 2,084,081 Development 29,962 25,274 15,422 147,958 25,206 11,467 36,351 291,640 Test 35,952 26,424 17,875 60,757 25,883 10,976 38,490 216,357 Training 10,826 10,349 6,672 34,492 21,419 8,969 1</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Miguel B Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order non-projective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="8471" citStr="Martins et al., 2013" startWordPosition="1283" endWordPosition="1286">irst, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2 LTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache Table 3: Dependency parsers used in our experiments. 4 Parsers We compared ten state of the art parsers representing a wide range of contemporary approaches to statistical dependency parsing (Table 3). We trained each parser using the training data from OntoNotes. For all parsers we trained using the automatic POS tags generated during data preprocessing, as described above. Training settings For most parsers, we used the default settings for training. For the SNN parser, following the re</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andr´e F. T. Martins, Miguel B. Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="4560" citStr="McDonald and Nivre (2007" startWordPosition="712" endWordPosition="715">19 8,969 12,452 105,179 Development 2,117 1,295 642 5,896 1,780 1,634 1,797 15,161 Test 2,211 1,357 780 2,327 1,869 1,366 1,787 11,697 Table 1: Distribution of data used for our experiments. The first three/last three rows show the number of tokens/trees in each genre. BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine, NW: newswire, PT: pivot text, TC: telephone conversation, WB: web text, ALL: all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 Data 3.1 OntoNotes 5 We used the English portion of the OntoNotes 5 corpus, a large multi-lingual, multi-genre corpus annotated with syntactic structure, predicate</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing and integrating dependency parsers.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>McDonald, Nivre, 2011</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2011. Analyzing and integrating dependency parsers. Computational Linguistics, 37(1):197–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
</authors>
<title>A comparative study of syntactic parsers for event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of BioNLP.</booktitle>
<marker>Miwa, 2010</marker>
<rawString>Makoto Miwa et al. 2010. A comparative study of syntactic parsers for event extraction. In Proceedings of BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
</authors>
<title>MaltEval: An evaluation and visualization tool for dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="11225" citStr="Nilsson and Nivre, 2008" startWordPosition="1694" endWordPosition="1697">e use the ”Standard” setting and for Yara, we use the default (”not basic”) feature setting. Output All the parsers other than LTDP output labeled dependencies. The ClearNLP, Mate, RBG, and Turbo parsers can generate non-projective dependencies. 5 DEPENDABLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, DEPENDABLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of DEPENDABLE: 18ilk.uvt.nl/conll/software.html 19www.tsarfaty.com/unipar/ 20www.maltparser.org/malteval.html 21whatswrong.googlecode.com 389 Figure 1: Screenshot of our evaluation tool. Figure</context>
</contexts>
<marker>Nilsson, Nivre, 2008</marker>
<rawString>Jens Nilsson and Joakim Nivre. 2008. MaltEval: An evaluation and visualization tool for dependency parsing. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="4560" citStr="Nivre (2007" startWordPosition="714" endWordPosition="715">52 105,179 Development 2,117 1,295 642 5,896 1,780 1,634 1,797 15,161 Test 2,211 1,357 780 2,327 1,869 1,366 1,787 11,697 Table 1: Distribution of data used for our experiments. The first three/last three rows show the number of tokens/trees in each genre. BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine, NW: newswire, PT: pivot text, TC: telephone conversation, WB: web text, ALL: all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 Data 3.1 OntoNotes 5 We used the English portion of the OntoNotes 5 corpus, a large multi-lingual, multi-genre corpus annotated with syntactic structure, predicate</context>
</contexts>
<marker>Nivre, 2007</marker>
<rawString>Joakim Nivre et al. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos G´omez Rodr´ıguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>833--841</pages>
<location>Beijing, China,</location>
<marker>Nivre, Rimell, McDonald, Rodr´ıguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos G´omez Rodr´ıguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833–841, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
</authors>
<title>SemEval</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>63--72</pages>
<marker>Oepen, 2014</marker>
<rawString>Stephan Oepen et al. 2014. SemEval 2014 Task 8: Broad-Coverage Semantic Dependency Parsing. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL) Shared Task.</booktitle>
<contexts>
<context position="2546" citStr="Petrov and McDonald, 2012" startWordPosition="403" endWordPosition="406">glish. • A new web-based tool, DEPENDABLE, for side-by-side comparison and visualization of the output from multiple dependency parsers. • A detailed error analysis for these parsers using DEPENDABLE, with recommendations for parser choice for different factors. • The release of the set of dependencies used in our experiments, the test outputs from all parsers, and the parser-specific models. 2 Related Work There have been several shared tasks on dependency parsing conducted by CoNLL (Buchholz and Marsi, 2006; Nivre and others, 2007; Surdeanu and others, 2008; Hajiˇc and others, 2009), SANCL (Petrov and McDonald, 2012), SPMRL (Seddah and others, 2013), and SemEval (Oepen and others, 2014). These shared tasks have led to the public release of numerous statistical parsers. The primary metrics reported in these shared tasks are: labeled attachment score (LAS) – the percentage of predicted dependencies where the arc and the label are assigned correctly; unlabeled attachment score (UAS) – where the arc is assigned correctly; label accuracy score (LS) – where the label is assigned correctly; and exact match (EM) – the percentage of sentences whose predicted trees are entirely correct. Although shared tasks have b</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL) Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>705--713</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="4971" citStr="Petrov et al., 2010" startWordPosition="774" endWordPosition="777">LL: all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 Data 3.1 OntoNotes 5 We used the English portion of the OntoNotes 5 corpus, a large multi-lingual, multi-genre corpus annotated with syntactic structure, predicateargument structure, word senses, named entities, and coreference (Weischedel and others, 2011; Pradhan and others, 2013). We chose this corpus rather than the Penn Treebank used in most previous work because it is larger (2.9M vs. 1M tokens) and more diverse (7 vs. 1 genres). We used the standard data split used in CoNLL’12 3, but removed sentences containing only one token so as not to artificially inflate </context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705–713, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
</authors>
<title>Towards robust linguistic analysis using OntoNotes.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Pradhan, 2013</marker>
<rawString>Sameer Pradhan et al. 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Sadegh Rasooli</author>
<author>Joel R Tetreault</author>
</authors>
<title>Yara parser: A fast and accurate dependency parser.</title>
<date>2015</date>
<location>CoRR, abs/1503.06733.</location>
<contexts>
<context position="8565" citStr="Rasooli and Tetreault, 2015" startWordPosition="1295" endWordPosition="1298">ased, beam-search + dynamic prog. (Huang et al., 2012) Python n/a Mate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2 RBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT Redshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS spaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual SNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2 Turbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2 Yara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache Table 3: Dependency parsers used in our experiments. 4 Parsers We compared ten state of the art parsers representing a wide range of contemporary approaches to statistical dependency parsing (Table 3). We trained each parser using the training data from OntoNotes. For all parsers we trained using the automatic POS tags generated during data preprocessing, as described above. Training settings For most parsers, we used the default settings for training. For the SNN parser, following the recommendation of the developers, we used the word embeddings from (Collobert and others, 2011).</context>
</contexts>
<marker>Rasooli, Tetreault, 2015</marker>
<rawString>Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015. Yara parser: A fast and accurate dependency parser. CoRR, abs/1503.06733.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings HLT-NAACL.</booktitle>
<contexts>
<context position="22333" citStr="Sagae and Lavie, 2006" startWordPosition="3523" endWordPosition="3526">est accuracy for some genres, although accuracy differences among the top four parsers are generally small. Accuracy is highest for PT (pivot text, the Bible) and lowest for TC (telephone conversation) and WB (web data). The web data is itself multi-genre and includes translations from Arabic and Chinese, while telephone conversation data includes disfluencies and informal language. 6.4 Oracle Ensemble Performance One popular method for achieving higher accuracy on a classification task is to use system combination (Bj¨orkelund and others, 2014; Le Roux and others, 2012; Le Roux et al., 2013; Sagae and Lavie, 2006; Sagae and Tsujii, 2010; Haffari et al., 2011). DEPENDABLE reports ensemble upper bound performance assuming that the best tree can be identified by an oracle (macro), or that the best arc can be identified by an oracle (micro). Table 8 provides an upper bound on ensemble performance for future work. LAS UAS LS Macro 94.66 96.00 97.82 Micro 96.52 97.61 98.40 Table 8: Oracle ensemble performance. The highest match was achieved between the RBG and Mate parser (62.22 UAS). ClearNLP, GN13 and LTDP all matched with Redshift the best, and RBG, Redshift and Turbo matched with Mate the best. SNN, spa</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with data-driven LR models and parser ensembles.</title>
<date>2010</date>
<booktitle>In Trends in Parsing Technology: Dependency Parsing, Domain Adaptation, and Deep Parsing,</booktitle>
<pages>57--68</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="22357" citStr="Sagae and Tsujii, 2010" startWordPosition="3527" endWordPosition="3530">enres, although accuracy differences among the top four parsers are generally small. Accuracy is highest for PT (pivot text, the Bible) and lowest for TC (telephone conversation) and WB (web data). The web data is itself multi-genre and includes translations from Arabic and Chinese, while telephone conversation data includes disfluencies and informal language. 6.4 Oracle Ensemble Performance One popular method for achieving higher accuracy on a classification task is to use system combination (Bj¨orkelund and others, 2014; Le Roux and others, 2012; Le Roux et al., 2013; Sagae and Lavie, 2006; Sagae and Tsujii, 2010; Haffari et al., 2011). DEPENDABLE reports ensemble upper bound performance assuming that the best tree can be identified by an oracle (macro), or that the best arc can be identified by an oracle (micro). Table 8 provides an upper bound on ensemble performance for future work. LAS UAS LS Macro 94.66 96.00 97.82 Micro 96.52 97.61 98.40 Table 8: Oracle ensemble performance. The highest match was achieved between the RBG and Mate parser (62.22 UAS). ClearNLP, GN13 and LTDP all matched with Redshift the best, and RBG, Redshift and Turbo matched with Mate the best. SNN, spaCy and Turbo did not mat</context>
</contexts>
<marker>Sagae, Tsujii, 2010</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2010. Dependency parsing and domain adaptation with data-driven LR models and parser ensembles. In Trends in Parsing Technology: Dependency Parsing, Domain Adaptation, and Deep Parsing, pages 57–68. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
</authors>
<title>Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Parsing of Morphologically Rich Languages.</booktitle>
<marker>Seddah, 2013</marker>
<rawString>Djam´e Seddah et al. 2013. Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the 4th Workshop on Statistical Parsing of Morphologically Rich Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
</authors>
<title>BRAT: A web-based tool for NLP-assisted text annotation.</title>
<date>2012</date>
<booktitle>In Proceedings of the EACL.</booktitle>
<marker>Stenetorp, 2012</marker>
<rawString>Pontus Stenetorp et al. 2012. BRAT: A web-based tool for NLP-assisted text annotation. In Proceedings of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Surdeanu, 2008</marker>
<rawString>Mihai Surdeanu et al. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Evaluating dependency parsing: Robust and heuristics-free cross-annotation evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="11187" citStr="Tsarfaty et al., 2011" startWordPosition="1688" endWordPosition="1691">pense of speed. For RBG and Turbo, we use the ”Standard” setting and for Yara, we use the default (”not basic”) feature setting. Output All the parsers other than LTDP output labeled dependencies. The ClearNLP, Mate, RBG, and Turbo parsers can generate non-projective dependencies. 5 DEPENDABLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, DEPENDABLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of DEPENDABLE: 18ilk.uvt.nl/conll/software.html 19www.tsarfaty.com/unipar/ 20www.maltparser.org/malteval.html 21whatswrong.googlecode.com 389 Figure 1: Scr</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2011</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2011. Evaluating dependency parsing: Robust and heuristics-free cross-annotation evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: A large training corpus for enhanced processing.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation.</booktitle>
<editor>In Joseph Olive, Caitlin Christianson, and John McCary, editors,</editor>
<publisher>Springer.</publisher>
<marker>Weischedel, 2011</marker>
<rawString>Ralph Weischedel et al. 2011. OntoNotes: A large training corpus for enhanced processing. In Joseph Olive, Caitlin Christianson, and John McCary, editors, Handbook of Natural Language Processing and Machine Translation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Laura Rimell</author>
<author>Aydin Han</author>
</authors>
<title>Parser evaluation using textual entailments.</title>
<date>2013</date>
<journal>Language Resources and Evaluation,</journal>
<volume>47</volume>
<issue>3</issue>
<contexts>
<context position="4992" citStr="Yuret et al., 2013" startWordPosition="778" endWordPosition="781">ed. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 Data 3.1 OntoNotes 5 We used the English portion of the OntoNotes 5 corpus, a large multi-lingual, multi-genre corpus annotated with syntactic structure, predicateargument structure, word senses, named entities, and coreference (Weischedel and others, 2011; Pradhan and others, 2013). We chose this corpus rather than the Penn Treebank used in most previous work because it is larger (2.9M vs. 1M tokens) and more diverse (7 vs. 1 genres). We used the standard data split used in CoNLL’12 3, but removed sentences containing only one token so as not to artificially inflate accuracy. Table 1 sho</context>
</contexts>
<marker>Yuret, Rimell, Han, 2013</marker>
<rawString>Deniz Yuret, Laura Rimell, and Aydin Han. 2013. Parser evaluation using textual entailments. Language Resources and Evaluation, 47(3).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>