<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000449">
<title confidence="0.972553">
Chinese Word Segmentation by Classification of Characters
</title>
<author confidence="0.987992">
Chooi-Ling GOH Masayuki ASAHARA Yuji MATSUMOTO
</author>
<affiliation confidence="0.997179">
Graduate School of Information Science,
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.826636666666667">
8916-5 Takayama, Ikoma,
Nara 630-0192,
Japan.
</address>
<email confidence="0.835406">
fling-g,masayu-a,matsufAis.naist.jp
</email>
<sectionHeader confidence="0.966041" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996055">
During the process of Chinese word segmen-
tation, two main problems occur: segmen-
tation ambiguities and unknown word occur-
rences. This paper describes a method to solve
the segmentation problem. First, we use a
dictionary-based approach to segment the text.
We simply apply maximum matching algorithm
to segment the text forwardly (FMM) and back-
wardly (BMM). Based on the difference be-
tween FMM and BMM, and the context, we
apply a classification method based on Support
Vector Machines to re-assign the word bound-
aries. By this way, we are using the output of
a dictionary-based approach, and then applying
a statistics-based approach to solve the segmen-
tation problem. The experimental results show
that our model achieves as high as 99.0 point of
F-measure for overall segmentation, given the
condition that no unknown word in the text,
and 95.1 if unknown words exist.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991823255814">
The first step in Chinese information process-
ing systems is word segmentation. It is because
in written Chinese, all characters are joined
together, and there is no separator to mark
word boundaries. A similar problem also oc-
curs in languages like Japanese, but at least
in Japanese, there exist three types of charac-
ters (hiragana, katakana and kanji), and this
could be a clue for finding word boundaries.
For Chinese, as there is only one type of char-
acters (hanzi), more segmentation ambiguities
may happen in a text. During the process of
segmentation, two main problems occur: seg-
mentation ambiguities and unknown word oc-
currences. This paper focuses on solving the
segmentation ambiguity problem, and proposes
a sub-model to solve the unknown word prob-
lem. There are basically two types of segmenta-
tion ambiguities: covering ambiguity and over-
lapping ambiguity. The definitions are given be-
low.
Let x, y, z be some strings in Chinese which
could consist of one or more characters. Sub-
sequently, covering ambiguity is defined as fol-
lows: For string w = xy, x E W, y E W
and also w E W, where W is a dictionary.
As almost any single character in Chinese can
be considered as a word, the definition reflexes
only those cases where both word boundaries
.../xy/... and .../x/y/... can be realized in some
sentences. On the other hand, overlapping am-
biguity is defined as follows: For string w = xyz,
wi = xy E W and also w2 = yz E W. Al-
though most of the time, the segmentation of
one form is more preferred than the other form,
but still we need to know where to use the other
form. Both ambiguities depend heavily on the
contexts to decide which is the correct segmen-
tation given that particular occurrences in the
texts.
(la) and (lb) show examples of covering am-
biguity. Given the string &amp;quot;— &amp;quot;, it is treated as
a word in (la), but as two words in (lb).
</bodyText>
<equation confidence="0.514734">
(la) / / /
</equation>
<bodyText confidence="0.994513090909091">
Hu/ Shiqing/ whole family/ three/ member
(The whole three members of Hu Shiqing fam-
ily)
(lb) yl&apos;/ EV-/ —/*// T./
in/ Paris/ one/ company/ magazine/ at/
(At one of the magazine company in Paris)
On the other hand, (2a) and (21)) give exam-
ples of overlapping ambiguity. The string &amp;quot;T,&apos;
N&amp;quot; is segmented as &amp;quot;T,&apos; / iN&amp;quot; in (2a) and &amp;quot;T,&apos;
/ &amp;quot;in (2b), according to the context of the
sentence.
</bodyText>
<equation confidence="0.932617">
(2a) / / / /
</equation>
<bodyText confidence="0.992505233333333">
not/ can/ forget/ far away/ hometown/ DE/
parents/
(Cannot forget the parents who are far way
at hometown)
cannot/ by/ profit/ be/ intention/
(Cannot have the intention to make profit)
We intend to solve the ambiguity problems
by combining a dictionary-based approach with
a statistical model. By this way, we make use
of the information in a dictionary to a statisti-
cal approach. Maximum Matching (MM) algo-
rithm, a very early and simple dictionary-based
approach, is used to initially segment the text
by referring to a dictionary. It tries to match the
longest possible words found in the dictionary.
We can either parse the sentence forwardly or
backwardly. Normally, the difference between
forward and backward parsing will indicate the
location where overlapping ambiguities occur.
Then, we use a Support Vector Machine-based
(SVM) classifier to decide which output should
be the correct answer. For covering ambiguity,
most of the cases, forward and backward MM
will give the same outputs, in this case, we will
just make use of the contexts to decide whether
or not to split a word into two words and etc.
Our results showed that the proposed method
could produce the correct answers for overlap-
ping ambiguities up to 92%, and 52% correctly
split the words for covering ambiguities.
</bodyText>
<sectionHeader confidence="0.979164" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999966250000001">
Solving the ambiguity problems is a funda-
mental task in Chinese segmentation process.
Although many previous researches have been
done for segmentation, only a few have re-
ported on the accuracy to solve ambiguity prob-
lems. (Li et al., 2003) suggest an unsuper-
vised method for training the Naïve Bayes clas-
sifiers to resolve overlapping ambiguities. They
achieved 94.13% accuracy with 5,759 cases of
ambiguity. A variation form of TF.IDF weight-
ing is proposed for solving covering ambiguity
problem in (Luo et al., 2002). They focus on
90 ambiguous words and achieve an accuracy of
96.58%.
Most of the previous methods reported on
the accuracy for overall segmentation. Recently,
many researches are done by combining multi-
ple models. Furthermore, most people have re-
alized that working on character-based is more
efficient than word-based for Chinese word seg-
mentation. In (Xue and Converse, 2002), two
classifiers are combined for Chinese word seg-
mentation. First, a Maximum Entropy model
is used to segment the text, then an error
driven transformation model is used to correct
the word boundaries. Similarly, they also use
character-based tagging on the position of char-
acters in words. They achieved an F-measure
of 95.17. Another recent report is by (Fu and
Luke, 2003), where hybrid models for integrated
segmentation is proposed. Modified word junc-
ture models and word-formation patterns are
used to find the word boundaries and at the
same time to identify the unknown words. They
achieved 96.1 F-measure. As both methods use
different corpora for the experiments, it is diffi-
cult to tell which method has done better than
the other.
Solving unknown word problem is also an im-
portant process in word segmentation. An un-
known word is defined as a word not found in
a dictionary. Therefore, they cannot be seg-
mented correctly by simply referring to the dic-
tionary. Many approaches have been reported
for unknown word detection such as (Chen and
Bai, 1997; Chen and Ma, 2002; Fu and Wang,
1999; Lai and Wu, 1999; Ma and Chen, 2003;
Nie et al., 1995; Shen et al., 1998; Zhang et
al., 2002; Zhou and Lua, 1997). There are rule-
based, statistics-based or even hybrid models.
In other words, we cannot ignore the unknown
word problem, as there always exist some un-
known words (such as person names, numbers
and etc) in the text even if we can get a very
large dictionary. The creation of new words in
Chinese is unlimited and is a continuous pro-
cess. For example, the name for new diseases,
technical terms, new expressions and etc. The
accuracy is better if one focuses only on certain
types of unknown words such as person names,
place names or transliteration names, with over
80%. However, for general unknown words such
as common nouns, verbs etc, the accuracy rang-
ing from 50% to 70% only.
</bodyText>
<sectionHeader confidence="0.982488" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.999975647058824">
The underlying concept of our proposed method
is as following. We regard the problem as a
character classification problem. We believe
that each character in Chinese holds its char-
acteristics to appear in a certain position in a
word. In other words, it can be either used at
the beginning of a word, in the middle of a word,
at the end of a word, or as a single-character
word. By looking at the usage of the charac-
ters, we will decide the position tag of the char-
acters using a machine learning based model,
which is the Support Vector Machines (Vapnik,
1995). This method serves as a model to solve
ambiguity problem, and at the same time, em-
beds a model to detect unknown words. We will
now describe the method in more details in the
following section.
</bodyText>
<subsectionHeader confidence="0.999777">
3.1 Maximum Matching Algorithm
</subsectionHeader>
<bodyText confidence="0.999970657894737">
We intend to solve the ambiguity problem by
combining a dictionary-based approach with a
statistical model. Maximum Matching (MM)
algorithm is regarded as the simplest dictionary-
based word segmentation approach. It starts
from one end of a sentence, and tries to match
the first longest word wherever possible. It is
a greedy algorithm, but it has been empirically
proved to achieve over 90% accuracy if the dic-
tionary used is large. However, it cannot solve
ambiguity problems and impossible to detect
unknown words because only words exist in the
dictionary can be segmented correctly. If we
look at the outputs produced by segmenting the
sentence forwardly (FMM), from the beginning
of the sentence, and backwardly (BMM), from
the end of the sentence, we will realize the places
where overlapping ambiguities occur. As an ex-
ample, FMM will segment the string &amp;quot;up
pr (when the time comes) into &amp;quot;Rp / i /
/&amp;quot; (immediately/ come/ when), but BMM
will segment it into &amp;quot;RP / / ii n /&amp;quot; (that/
future/ temporary).
Let Of and Ob be the outputs of FMM
and BMM respectively. According to (Huang,
1997), for overlapping cases: If Of = Ob, then
99% that both the MMs have the correct an-
swer. If Of Ob, then 99% that either Of or
01) has the correct answer. However, for cover-
ing ambiguity cases, even Of = Ob, but both
Of and 01) could be correct or both could be
wrong. If there exist unknown words, normally
they will be segmented as single characters by
both FMM and BMM. Based on the differences
and context created by FMM and BMM, we will
apply a machine learning based model to reas-
sign the position tags which indicate the char-
acter position in the word.
</bodyText>
<subsectionHeader confidence="0.99999">
3.2 Re-classification of Characters
</subsectionHeader>
<bodyText confidence="0.997602428571429">
We plan to re-classify the outputs of FMM and
BMM character by character. First, we will
convert the output of the MMs into character-
based, where each character will be assigned
with a position tag such as described in Table
1. The BIES tags are as in (Uchimoto et al.,
2000) and (Sang and Veenstra, 1999) for named
entity extraction. These tags show the possible
character position in a word. For example, if we
look at the character &amp;quot;7K &amp;quot;, it is used as a single
character in &amp;quot;— / / /&amp;quot; (a book), at the end
of a word in &amp;quot;Mil 7K&amp;quot; (script), at the beginning of
a word in &amp;quot;7K *&amp;quot; (originally), and at the middle
of a word in &amp;quot;a &amp;quot; (basically).
</bodyText>
<tableCaption confidence="0.994433">
Table 1: Position tags in a word
</tableCaption>
<bodyText confidence="0.999827666666667">
Then, based on these features, we will re-
classify the tags by using a Support Vector
Machine-based (SVM) chunker (Kudo and Mat-
sumoto, 2001). The solid box in Figure 1 shows
the features used to determine the class of the
character &amp;quot;v&amp;quot; at location i. Based on the out-
put position tags, finally, we will get the seg-
mentation as &amp;quot;irii / / /&amp;quot; (wel-
come/ new year/ get-together party/ at/).
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9914605">
We have run our experiments with two datasets,
PKU Corpus and SIGHAN Bakeoff data. The
evaluation is done by using the tool provided in
SIGHAN Bakeoff (Sproat and Emerson, 2003).
</bodyText>
<subsectionHeader confidence="0.921027666666667">
4.1 Experiment with PKU Corpus
4.1.1 Accuracy on Solving Ambiguity
Problem
</subsectionHeader>
<bodyText confidence="0.7369418">
The corpus used for this experiment is from
Peking University (PKU) 1, consisting of about
1 million words. It is a segmented and POS
&apos;Institute of Computational Linguistics, Peking Uni-
versity, http://www.icl.pku.edu.cn/
</bodyText>
<figure confidence="0.989968272727273">
Tag
Description
one-character word
first character in a multi-character
word
intermediate character in a multi-
character word (for words longer
than two characters)
last character in a multi-character
word
Char. FMM BMM Output
</figure>
<figureCaption confidence="0.998024">
Figure 1: An illustration of classification pro-
cess - &apos;At the New Year gathering party&apos;
</figureCaption>
<bodyText confidence="0.926836176470588">
tagged corpus, but we only use the segmenta-
tion information for our experiments. We di-
vide the corpus randomly into 80% and 20%,
for training and testing respectively. Since our
purpose in this experiment is only for solving
ambiguity problem, not the unknown word de-
tection, we assume that all words could be found
in the dictionary. We create a dictionary with
all words from the corpus, which has 62,030 en-
tries (referred to as Experiment 1). This exper-
iment intends to show the performance of the
method for solving ambiguity problem.
It is sometimes very difficult to determine
how many cases of ambiguities appearing in a
sentence. For example, in the sentence in Fig-
ure 1, &amp;quot;irii &amp;quot; (welcome the new year), &amp;quot;M &amp;quot;
(new year), &amp;quot;W. &amp;quot; (a red paper that pasted
on the door, written with some greeting words
for celebrating new year in China), &amp;quot;IR &amp;quot;(get-
together), &amp;quot;IR &amp;quot; (get-together party), &amp;quot;
(at the meeting) and &amp;quot;±&amp;quot; (at) are all possible
words. How many overlapping cases and cov-
ering cases are there? It is quite impossible to
answer. A word candidate may cause more than
one ambiguities with the alternative word can-
didates. Therefore, we try to represent the am-
biguities by character units since our method is
character based. We group each character into
one of these six categories.
Let,
Of = Output of FMM
Ob = Output of BMM
Ans = Correct answer
Out = Output from our system
</bodyText>
<sectionHeader confidence="0.169264" genericHeader="method">
Category Conditions
</sectionHeader>
<construct confidence="0.555753222222222">
Allcorrect Of = Ob = Ans = Ot
Correct Of Ob and Ans = Out
Wrong Of Ob and Ans Out
Match Of = Ob and Of Ans and
Ans = Out
Mismatch Of = Ob and Of Ans and
Ans Ot
Allwrong Of = Ob = Ans and Ans
Out
</construct>
<tableCaption confidence="0.851191">
Table 2: Categories for Characters
</tableCaption>
<bodyText confidence="0.977850928571429">
Table 2 shows the conditions for each cate-
gory. Category Allcorrect, Correct and Match
have correct answers, whereas category Wrong,
Mismatch and Allwrong have wrong answers.
We could roughly say that category Correct and
Wrong belong to overlapping ambiguities and
category Match, Mismatch, and Allwrong be-
long to covering ambiguities. We could also say
that Match and Mismatch are cases where we
need to split the words, and Allwrong are cases
where we should not split the words but have
been split by the system. Table 3 shows the re-
sults of the method for solving ambiguity prob-
lem.
</bodyText>
<table confidence="0.99164925">
Category No. of Char. Percentage
A/Icorrect 330220 96.35%
Correct 7663 2.23%
Wrong 658 0.19%
Match 1876 0.55%
Mismatch 1738 0.51%
Allwrong 571 0.17%
Total 342726 100.00%
</table>
<tableCaption confidence="0.999921">
Table 3: Results on Disambiguation
</tableCaption>
<bodyText confidence="0.99998952173913">
In total, we could obtain about 99.13% that
the characters are correctly tagged. If we
only consider the overlapping cases (Correct
and Wrong), 92.09% characters are correctly
tagged. For covering cases, if we look at only
those cases where we need to split the words
(Match and Mismatch), 51.91% have been suc-
cessfully split.
Table 4 shows the results of word segmen-
tation. We also compare our method with a
Hidden Markov Model-based (HMM) morpho-
logical analyzer, where word bi-gram is used to
calculate the probability. The size of the dic-
tionary used for HMM is the same as previous
experiment, but with real POS tags. The HMM
does segmentation and POS tagging simultane-
ously, but we only take the results of segmenta-
tion for comparison. The results show that our
proposed method can achieve higher accuracy
with over 99.0%. It means that our method is
able to solve ambiguity problem given the infor-
mation where the ambiguous locations occurred
by looking at the output of FMM and BMM.
</bodyText>
<subsectionHeader confidence="0.9384595">
4.1.2 Accuracy on Solving Unknown
Word Problem
</subsectionHeader>
<bodyText confidence="0.999821">
The corpus used is the same as in Section 4.1.1,
but the setting is different. In this round we
divide the corpus into three sets, referring to as
Set 1, Set 2 and Set 3. Set 1 plus Set 2 (80%)
are used for training and Set 3 (20%) is used
for testing, same as the previous experiment.
The difference is the preparation of dictionary.
There are two ways of preparation here. In the
first case, all the words from Set 1 and Set 2 are
</bodyText>
<figure confidence="0.980742344827586">
100
99
98
97
100
90
80
70
Recall
60
40
50
93
92
30
20
10
0
Se gme ntation (F-measure)
OO9 (recall)
IV (recall)
80/0 60/20 40/40 20/60 0/80
Division of Corpus
96
F-measure
95
94
91
90
</figure>
<table confidence="0.960159538461538">
Experiment 1 Experiment 2 Experiment 3 Experiment 4
Set1(%)/Set2(%) 80/0 60/20 40/40 20/60 0/80
# of words in 62,030 49,433 41,582 33,355 22,363 0
Dict.
# of unk-words 0 0 10,927 25,297 53,353 All
in Set 2
# of unk-words 0 8,346 9,768 11,924 17,115 All
in Test
Recall 98.9 95.3 95.8 95.7 95.2 94.0
Precision 99.1 90.7 93.5 94.5 94.7 94.3
F-measure 99.0 92.9 94.7 95.1 94.9 94.1
00V (recall) 8.0 41.2 54.9 63.3 69.3
IV (recall) 98.9 98.9 98.1 97.4 96.5 95.0
</table>
<tableCaption confidence="0.997956">
Table 5: Different Settings and Segmentation Results with Unknown Words
</tableCaption>
<bodyText confidence="0.997738545454546">
Treebank, CHTB 2). Since our system works
only on two-byte codings, some ascii codes in
the data have been converted to GB codes be-
fore processing, especially numbers and alpha-
bets. The distribution of the data is as shown
in Table 6. The original dictionaries consist of
55,226 and 19,730 words respectively. Accord-
ing to these dictionaries, there are 1,189 and
7,216 unknown words in the testing data. Af-
ter converting to GB codes, it left only 781 and
7,171 unknown words. It also means that about
34.3% and 0.6% of the unknown words automat-
ically become known words after the conversion.
The conversion reduced the number of unknown
words because for example, if a numeral word
&amp;quot;1 9 9 8 &amp;quot; written in GB code exists in train-
ing data, but it is written in ascii code &amp;quot;1998&amp;quot;
in testing data, it is treated as unknown word
at the first place. After the conversion, it will
become known word.
We have set up the experiments similar to Ex-
periment 2 and Experiment 3 above. For Ex-
periment 2, all the words in the training data
are used for creating the dictionary. For Ex-
periment 3, it is based on our previous experi-
ments where the division of half of the training
corpus generated the best result by F-measure.
Therefore, only 50% of the training corpora are
used while creating the dictionaries. As a result,
the new dictionaries contain 36,830 and 12,274
words respectively. Table 6 shows the details
for the setting.
For PKU corpus, the best result in the bakeoff
</bodyText>
<footnote confidence="0.88349275">
2We work only on GB code, the standard coding used
for simplified Chinese characters. However, it can be
modified easily to suit Big5 coding for traditional Chi-
nese characters.
</footnote>
<bodyText confidence="0.999781486486486">
achieved 95.1 in F-measure (Zhang et al., 2003).
They use hierarchical Hidden Markov Models to
segment and POS tag the text. Although it is
a closed test, they have used extra information
such as class-based segmentation and role-based
tagging models (Zhang et al., 2002), which
give better result for unknown word recognition.
Our method has done only slightly worse than
theirs, with 94.7. The recall for unknown words
is comparable, with 71.0% while the best one
has 72.4%. Unfortunately, the recall for known
words drops a bit, with 97.3%, while the best
one is slightly better, with 97.9%, as shown in
Table 7. We also compare with (Asahara et
al., 2003), where similar method is used for re-
assigning the word boundaries, except that the
words are first categorized into 5 or 10 classes
(which is assumed equivalent to POS tags) us-
ing Baum-Welch algorithm, then the sentence
is segmented into word sequence by a Hidden
Markov Model-based segmenter. Finally, the
same Support Vector Machine-based chunker is
trained to correct the errors made by the seg-
menter. Our method which is simply a forward
and backward maximum matching algorithm,
has done a lot better then theirs, where com-
plicated statistical based models are involved.
They have achieved only 92.4 F-measure while
we have 94.7.
On the other hand, our results for CHTB cor-
pus are not as comparable as the best result in
the bakeoff. We could only get 84.7 point of F-
measure, while the best one has 88.1 (Zhang et
al., 2003) and 82.9 by (Asahara et al., 2003). It
may be due to the reason that the training cor-
pus is a lot smaller than the PKU corpus and
the testing data contains more unknown words.
</bodyText>
<table confidence="0.999801444444444">
PKU Data CHTB Data
# of # of unk- unk-word # of # of unk- unk-word
words words rate words words rate
Original Training 1,121,017 0 0% 250,841 0 0%
Original Testing 17,194 1,189 6.9% 39,922 7,216 18.1%
(In GB code) (781) (4.5%) (7,171) (18.0%)
Set 1 560,649 0 0% 125,405 0 0%
Set 2 560,368 29,303 5.2% 125,436 13,976 11.1%
Testing Data 17,194 1,121 6.5% 39,922 9,769 24.5%
</table>
<tableCaption confidence="0.996367">
Table 6: Bakeoff Data
</tableCaption>
<bodyText confidence="0.999979277777778">
However, we still could get quite good recall for
unknown words, with 57.7%, while the others
have 70.5% and 41.2% respectively.
As a conclusion, our results cannot transcend
the best results in the bakeoff for both corpora.
However, our method is simpler. We only need
a dictionary that created from a segmented cor-
pus, FMM and BMM modules, and a classifier,
without the intervention of human knowledge.
We get quite comparable results for both known
words and also unknown words. The result is
worse when the training corpus is small and
there exist a lot of unknown words such as in
CHTB testing data. Therefore, we still need
to investigate on the relationship between the
size of training corpus and the division of corpus
for training of ambiguity problem and unknown
word detection.
</bodyText>
<sectionHeader confidence="0.99651" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999939086956522">
Apparently, our proposed method has generated
better result than the baseline models, FMM
and BMM. We get nearly 99% accuracy if un-
known words do not exist. However, in the real
world, it is impossible that there is no unknown
word at all even we could get a very large dic-
tionary. Therefore, we also embedded a model
to detect the unknown words. Unfortunately,
while the accuracy for unknown word detection
increased, the performance on solving known
word ambiguity drops. As shown in the experi-
ments with bakeoff data, our model works well
only when the training corpus is large enough.
As a conclusion, while our model is suited for
solving segmentation ambiguity problem, it can
also be used for unknown word detection. How-
ever we still need to find a balance point for solv-
ing these two problems simulteneously. We also
need to research on the relationship between
training corpus size and the best proportion to
divide the corpus for training optimally on solv-
ing ambiguity problem and unknown word de-
tection.
</bodyText>
<sectionHeader confidence="0.998605" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997438">
Thanks to Mr. Kudo for his tool on Support
Vector Machine-based chunker, Yamcha. We
also thank the reviewers for their invaluable
comments, Peking University and SIGHAN for
providing the corpora in our experiments.
</bodyText>
<sectionHeader confidence="0.996726" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998610034482758">
Masayuki Asahara, Chooi Ling Goh, Xiaojie
Wang, and Yuji Matsumoto. 2003. Com-
bining Segmenter and Chunker for Chinese
Word Segmentation. In Proceedings of Sec-
ond SIGHAN Workshop OTb Chinese Lan-
guage Processing, pages 144-147.
Keh-Jiann Chen and Ming-Hong Bai. 1997.
Unknown Word Detection for Chinese By a
Corpus-based Learning Method. In Proceed-
ings of ROCLING X, pages 159-174.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Un-
known Word Extraction for Chinese Docu-
ments. In Proceedings of COLING 2002, vol-
ume 1, pages 169-175.
Guohong Fu and K.K. Luke. 2003. An Inte-
grated Approach for Chinese Word Segmen-
tation. In Proceedings of PA CLIC 17.
Guohong Fu and Xiaolong Wang. 1999. Un-
supervised Chinese Word Segmentation and
Unknown Word Identification. In Proceedings
of NLPRS.
Changning Huang. 1997. Segmentation prob-
lem in chinese processing. Applied Linguis-
tics, 1:72-78.
Taku Kudo and Yuji Matsumoto. 2001. Chunk-
ing with Support Vector Machines. In Pro-
ceedings of NAACL, pages 192-199.
Yu-Sheng Lai and Chung-Hsien Wu. 1999. Un-
known Word and Phrase Extraction Using a
</reference>
<table confidence="0.9964930625">
Without Un- With Unknown Baum-Welch + Best Result in
known Word Word Processing HMM + SVM Bakeoff (Zhang
Processing (=Experiment 3) (Asahara et al., et al., 2003)
(=Experiment 2) 2003)
PKU Corpus
Recall 94.6 95.5 93.3 96.2
Precision 89.4 94.1 91.6 94.0
F-measure 92.0 94.7 92.4 95.1
00V (recall) 39.7 71.0 35.7 72.4
IV (recall) 98.7 97.3 97.5 97.9
CHTB Corpus
Recall 82.2 86.0 85.2 88.6
Precision 67.4 83.5 80.7 87.5
F-measure 74.1 84.7 82.9 88.1
00V (recall) 10.1 57.7 41.2 70.5
IV (recall) 98.2 92.2 94.9 92.7
</table>
<tableCaption confidence="0.992998">
Table 7: Segmentation Results with Bakeoff Data
</tableCaption>
<reference confidence="0.999875682539683">
Phrase-Like-Unit-Based Likelihood Ratio. In
Proceeding of ICCPOL &apos;99, pages 5-9.
Mu Li, Jianfeng Gao, Changning Huang, and
Jianfeng Li. 2003. Unsupervised Training for
Overlapping Ambiguity Resolution in Chi-
nese Word Segmentation. In Proceedings of
Second SIGHAN Workshop OTb Chinese Lan-
guage Processing, pages 1-7.
Xiao Luo, Maosong Sun, and Benjamin K.
Tsou. 2002. Covering Ambiguity Resolu-
tion in Chinese Word Segmentation Based
on Contextual Information. In Proceedings of
COLING 2002, pages 598-604.
Wei-Yun Ma and Keh-Jiann Chen. 2003. A
Bottom-up Merging Algorithm for Chinese
Unknown Word Extraction. In Proceedings of
Second SIGHAN Workshop OTb Chinese Lan-
guage Processing, pages 31-38.
Jian-Yun Nie, Marie-Louise Hannan, and
Wanying Jin. 1995. Unknown Word Detec-
tion and Segmentation of Chinese Using Sta-
tistical and Heuristic Knowledge. Communi-
cations of COUPS, Vol.5:47-57.
Erik F. Tjong Kim Sang and John Veenstra.
1999. Representing Text Chunks. In Proceed-
ings of EACL &apos;99, pages 173-179.
Dayang Shen, Maosong Sun, and Changning
Huang. 1998. The application &amp; imple-
mentation of local statistics in Chinese un-
known word identification. Communications
of COUPS, Vol.8.
Richard Sproat and Thomas Emerson. 2003.
The first international chinese word segmen-
tation bakeoff. In Proceedings of Second
SIGHAN Workshop on Chinese Language
Processing, pages 133-143.
Kiyotaka Uchimoto, Qing Ma, Masaki Murata,
Hiromi Ozaku, and Hitoshi Isahara. 2000.
Named Entity Extraction Based on A Max-
imum Entropy Model and Transformational
Rules. In Processing of the ACL 2000.
Vladimir N. Vapnik. 1995. The Nature of Sta-
tistical Learning Theory. Springer.
Nianwen Xue and Susan P. Converse. 2002.
Combining Classifiers for Chinese Word Seg-
mentation. In Proceedings of 1st SIGHAN
Workshop on Chinese Language Processing,
pages 57-63.
Hua-Ping Zhang, Qun Liu, Hao Zhang, and
Xue-Qi Cheng. 2002. Automatic Regcog-
nition of Chinese Unknown Words Based
on Roles Tagging. In Proceedings of 1st
SIGHAN Workshop on Chinese Language
Processing.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong,
and Qun Liu. 2003. HHMM-based Chinese
Lexical Analyzer ICTCLAS. In Proceedings
of Second SIGHAN Workshop on Chinese
Language Processing, pages 184-187.
Guo-Dong Zhou and Kim-Teng Lua. 1997. De-
tection of Unknown Chinese Words Using a
Hybrid Approach. Computer Processing of
Oriental Language, 11(1):63-75.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.830598">
<title confidence="0.996708">Chinese Word Segmentation by Classification of Characters</title>
<author confidence="0.960932">Chooi-Ling GOH Masayuki ASAHARA Yuji</author>
<affiliation confidence="0.999243">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.971062">8916-5 Takayama, Nara</address>
<email confidence="0.993648">fling-g,masayu-a,matsufAis.naist.jp</email>
<abstract confidence="0.996273571428571">During the process of Chinese word segmentation, two main problems occur: segmentation ambiguities and unknown word occurrences. This paper describes a method to solve the segmentation problem. First, we use a dictionary-based approach to segment the text. We simply apply maximum matching algorithm to segment the text forwardly (FMM) and backwardly (BMM). Based on the difference between FMM and BMM, and the context, we apply a classification method based on Support Vector Machines to re-assign the word boundaries. By this way, we are using the output of a dictionary-based approach, and then applying a statistics-based approach to solve the segmentation problem. The experimental results show that our model achieves as high as 99.0 point of F-measure for overall segmentation, given the condition that no unknown word in the text, and 95.1 if unknown words exist.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Chooi Ling Goh</author>
<author>Xiaojie Wang</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Combining Segmenter and Chunker for Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of Second SIGHAN Workshop OTb Chinese Language Processing,</booktitle>
<pages>144--147</pages>
<contexts>
<context position="18810" citStr="Asahara et al., 2003" startWordPosition="3273" endWordPosition="3276">l., 2003). They use hierarchical Hidden Markov Models to segment and POS tag the text. Although it is a closed test, they have used extra information such as class-based segmentation and role-based tagging models (Zhang et al., 2002), which give better result for unknown word recognition. Our method has done only slightly worse than theirs, with 94.7. The recall for unknown words is comparable, with 71.0% while the best one has 72.4%. Unfortunately, the recall for known words drops a bit, with 97.3%, while the best one is slightly better, with 97.9%, as shown in Table 7. We also compare with (Asahara et al., 2003), where similar method is used for reassigning the word boundaries, except that the words are first categorized into 5 or 10 classes (which is assumed equivalent to POS tags) using Baum-Welch algorithm, then the sentence is segmented into word sequence by a Hidden Markov Model-based segmenter. Finally, the same Support Vector Machine-based chunker is trained to correct the errors made by the segmenter. Our method which is simply a forward and backward maximum matching algorithm, has done a lot better then theirs, where complicated statistical based models are involved. They have achieved only </context>
</contexts>
<marker>Asahara, Goh, Wang, Matsumoto, 2003</marker>
<rawString>Masayuki Asahara, Chooi Ling Goh, Xiaojie Wang, and Yuji Matsumoto. 2003. Combining Segmenter and Chunker for Chinese Word Segmentation. In Proceedings of Second SIGHAN Workshop OTb Chinese Language Processing, pages 144-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Ming-Hong Bai</author>
</authors>
<title>Unknown Word Detection for Chinese By a Corpus-based Learning Method.</title>
<date>1997</date>
<booktitle>In Proceedings of ROCLING X,</booktitle>
<pages>159--174</pages>
<contexts>
<context position="6698" citStr="Chen and Bai, 1997" startWordPosition="1120" endWordPosition="1123">ied word juncture models and word-formation patterns are used to find the word boundaries and at the same time to identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if o</context>
</contexts>
<marker>Chen, Bai, 1997</marker>
<rawString>Keh-Jiann Chen and Ming-Hong Bai. 1997. Unknown Word Detection for Chinese By a Corpus-based Learning Method. In Proceedings of ROCLING X, pages 159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Wei-Yun Ma</author>
</authors>
<title>Unknown Word Extraction for Chinese Documents.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING 2002,</booktitle>
<volume>1</volume>
<pages>169--175</pages>
<contexts>
<context position="6717" citStr="Chen and Ma, 2002" startWordPosition="1124" endWordPosition="1127">dels and word-formation patterns are used to find the word boundaries and at the same time to identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on </context>
</contexts>
<marker>Chen, Ma, 2002</marker>
<rawString>Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown Word Extraction for Chinese Documents. In Proceedings of COLING 2002, volume 1, pages 169-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guohong Fu</author>
<author>K K Luke</author>
</authors>
<title>An Integrated Approach for Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of PA CLIC 17.</booktitle>
<contexts>
<context position="6012" citStr="Fu and Luke, 2003" startWordPosition="1004" endWordPosition="1007">egmentation. Recently, many researches are done by combining multiple models. Furthermore, most people have realized that working on character-based is more efficient than word-based for Chinese word segmentation. In (Xue and Converse, 2002), two classifiers are combined for Chinese word segmentation. First, a Maximum Entropy model is used to segment the text, then an error driven transformation model is used to correct the word boundaries. Similarly, they also use character-based tagging on the position of characters in words. They achieved an F-measure of 95.17. Another recent report is by (Fu and Luke, 2003), where hybrid models for integrated segmentation is proposed. Modified word juncture models and word-formation patterns are used to find the word boundaries and at the same time to identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Man</context>
</contexts>
<marker>Fu, Luke, 2003</marker>
<rawString>Guohong Fu and K.K. Luke. 2003. An Integrated Approach for Chinese Word Segmentation. In Proceedings of PA CLIC 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guohong Fu</author>
<author>Xiaolong Wang</author>
</authors>
<title>Unsupervised Chinese Word Segmentation and Unknown Word Identification.</title>
<date>1999</date>
<booktitle>In Proceedings of NLPRS.</booktitle>
<contexts>
<context position="6736" citStr="Fu and Wang, 1999" startWordPosition="1128" endWordPosition="1131">tion patterns are used to find the word boundaries and at the same time to identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on certain types of un</context>
</contexts>
<marker>Fu, Wang, 1999</marker>
<rawString>Guohong Fu and Xiaolong Wang. 1999. Unsupervised Chinese Word Segmentation and Unknown Word Identification. In Proceedings of NLPRS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changning Huang</author>
</authors>
<title>Segmentation problem in chinese processing.</title>
<date>1997</date>
<journal>Applied Linguistics,</journal>
<pages>1--72</pages>
<contexts>
<context position="9432" citStr="Huang, 1997" startWordPosition="1606" endWordPosition="1607">d impossible to detect unknown words because only words exist in the dictionary can be segmented correctly. If we look at the outputs produced by segmenting the sentence forwardly (FMM), from the beginning of the sentence, and backwardly (BMM), from the end of the sentence, we will realize the places where overlapping ambiguities occur. As an example, FMM will segment the string &amp;quot;up pr (when the time comes) into &amp;quot;Rp / i / /&amp;quot; (immediately/ come/ when), but BMM will segment it into &amp;quot;RP / / ii n /&amp;quot; (that/ future/ temporary). Let Of and Ob be the outputs of FMM and BMM respectively. According to (Huang, 1997), for overlapping cases: If Of = Ob, then 99% that both the MMs have the correct answer. If Of Ob, then 99% that either Of or 01) has the correct answer. However, for covering ambiguity cases, even Of = Ob, but both Of and 01) could be correct or both could be wrong. If there exist unknown words, normally they will be segmented as single characters by both FMM and BMM. Based on the differences and context created by FMM and BMM, we will apply a machine learning based model to reassign the position tags which indicate the character position in the word. 3.2 Re-classification of Characters We pl</context>
</contexts>
<marker>Huang, 1997</marker>
<rawString>Changning Huang. 1997. Segmentation problem in chinese processing. Applied Linguistics, 1:72-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with Support Vector Machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="10840" citStr="Kudo and Matsumoto, 2001" startWordPosition="1870" endWordPosition="1874">ition tag such as described in Table 1. The BIES tags are as in (Uchimoto et al., 2000) and (Sang and Veenstra, 1999) for named entity extraction. These tags show the possible character position in a word. For example, if we look at the character &amp;quot;7K &amp;quot;, it is used as a single character in &amp;quot;— / / /&amp;quot; (a book), at the end of a word in &amp;quot;Mil 7K&amp;quot; (script), at the beginning of a word in &amp;quot;7K *&amp;quot; (originally), and at the middle of a word in &amp;quot;a &amp;quot; (basically). Table 1: Position tags in a word Then, based on these features, we will reclassify the tags by using a Support Vector Machine-based (SVM) chunker (Kudo and Matsumoto, 2001). The solid box in Figure 1 shows the features used to determine the class of the character &amp;quot;v&amp;quot; at location i. Based on the output position tags, finally, we will get the segmentation as &amp;quot;irii / / /&amp;quot; (welcome/ new year/ get-together party/ at/). 4 Experiments and Results We have run our experiments with two datasets, PKU Corpus and SIGHAN Bakeoff data. The evaluation is done by using the tool provided in SIGHAN Bakeoff (Sproat and Emerson, 2003). 4.1 Experiment with PKU Corpus 4.1.1 Accuracy on Solving Ambiguity Problem The corpus used for this experiment is from Peking University (PKU) 1, con</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with Support Vector Machines. In Proceedings of NAACL, pages 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Sheng Lai</author>
<author>Chung-Hsien Wu</author>
</authors>
<title>Unknown Word and Phrase Extraction Using a Phrase-Like-Unit-Based Likelihood Ratio.</title>
<date>1999</date>
<booktitle>In Proceeding of ICCPOL &apos;99,</booktitle>
<pages>5--9</pages>
<contexts>
<context position="6754" citStr="Lai and Wu, 1999" startWordPosition="1132" endWordPosition="1135">sed to find the word boundaries and at the same time to identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on certain types of unknown words such a</context>
</contexts>
<marker>Lai, Wu, 1999</marker>
<rawString>Yu-Sheng Lai and Chung-Hsien Wu. 1999. Unknown Word and Phrase Extraction Using a Phrase-Like-Unit-Based Likelihood Ratio. In Proceeding of ICCPOL &apos;99, pages 5-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Jianfeng Gao</author>
<author>Changning Huang</author>
<author>Jianfeng Li</author>
</authors>
<title>Unsupervised Training for Overlapping Ambiguity Resolution in Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of Second SIGHAN Workshop OTb Chinese Language Processing,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="4978" citStr="Li et al., 2003" startWordPosition="839" endWordPosition="842">of the cases, forward and backward MM will give the same outputs, in this case, we will just make use of the contexts to decide whether or not to split a word into two words and etc. Our results showed that the proposed method could produce the correct answers for overlapping ambiguities up to 92%, and 52% correctly split the words for covering ambiguities. 2 Previous Work Solving the ambiguity problems is a fundamental task in Chinese segmentation process. Although many previous researches have been done for segmentation, only a few have reported on the accuracy to solve ambiguity problems. (Li et al., 2003) suggest an unsupervised method for training the Naïve Bayes classifiers to resolve overlapping ambiguities. They achieved 94.13% accuracy with 5,759 cases of ambiguity. A variation form of TF.IDF weighting is proposed for solving covering ambiguity problem in (Luo et al., 2002). They focus on 90 ambiguous words and achieve an accuracy of 96.58%. Most of the previous methods reported on the accuracy for overall segmentation. Recently, many researches are done by combining multiple models. Furthermore, most people have realized that working on character-based is more efficient than word-based f</context>
</contexts>
<marker>Li, Gao, Huang, Li, 2003</marker>
<rawString>Mu Li, Jianfeng Gao, Changning Huang, and Jianfeng Li. 2003. Unsupervised Training for Overlapping Ambiguity Resolution in Chinese Word Segmentation. In Proceedings of Second SIGHAN Workshop OTb Chinese Language Processing, pages 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Luo</author>
<author>Maosong Sun</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Covering Ambiguity Resolution in Chinese Word Segmentation Based on Contextual Information.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>598--604</pages>
<contexts>
<context position="5257" citStr="Luo et al., 2002" startWordPosition="883" endWordPosition="886">mbiguities up to 92%, and 52% correctly split the words for covering ambiguities. 2 Previous Work Solving the ambiguity problems is a fundamental task in Chinese segmentation process. Although many previous researches have been done for segmentation, only a few have reported on the accuracy to solve ambiguity problems. (Li et al., 2003) suggest an unsupervised method for training the Naïve Bayes classifiers to resolve overlapping ambiguities. They achieved 94.13% accuracy with 5,759 cases of ambiguity. A variation form of TF.IDF weighting is proposed for solving covering ambiguity problem in (Luo et al., 2002). They focus on 90 ambiguous words and achieve an accuracy of 96.58%. Most of the previous methods reported on the accuracy for overall segmentation. Recently, many researches are done by combining multiple models. Furthermore, most people have realized that working on character-based is more efficient than word-based for Chinese word segmentation. In (Xue and Converse, 2002), two classifiers are combined for Chinese word segmentation. First, a Maximum Entropy model is used to segment the text, then an error driven transformation model is used to correct the word boundaries. Similarly, they al</context>
</contexts>
<marker>Luo, Sun, Tsou, 2002</marker>
<rawString>Xiao Luo, Maosong Sun, and Benjamin K. Tsou. 2002. Covering Ambiguity Resolution in Chinese Word Segmentation Based on Contextual Information. In Proceedings of COLING 2002, pages 598-604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Yun Ma</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>A Bottom-up Merging Algorithm for Chinese Unknown Word Extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of Second SIGHAN Workshop OTb Chinese Language Processing,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="6773" citStr="Ma and Chen, 2003" startWordPosition="1136" endWordPosition="1139">rd boundaries and at the same time to identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on certain types of unknown words such as person names, pla</context>
</contexts>
<marker>Ma, Chen, 2003</marker>
<rawString>Wei-Yun Ma and Keh-Jiann Chen. 2003. A Bottom-up Merging Algorithm for Chinese Unknown Word Extraction. In Proceedings of Second SIGHAN Workshop OTb Chinese Language Processing, pages 31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
<author>Marie-Louise Hannan</author>
<author>Wanying Jin</author>
</authors>
<title>Unknown Word Detection and Segmentation of Chinese Using Statistical and Heuristic Knowledge.</title>
<date>1995</date>
<journal>Communications of COUPS,</journal>
<pages>5--47</pages>
<contexts>
<context position="6791" citStr="Nie et al., 1995" startWordPosition="1140" endWordPosition="1143">t the same time to identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on certain types of unknown words such as person names, place names or transl</context>
</contexts>
<marker>Nie, Hannan, Jin, 1995</marker>
<rawString>Jian-Yun Nie, Marie-Louise Hannan, and Wanying Jin. 1995. Unknown Word Detection and Segmentation of Chinese Using Statistical and Heuristic Knowledge. Communications of COUPS, Vol.5:47-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>John Veenstra</author>
</authors>
<title>Representing Text Chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL &apos;99,</booktitle>
<pages>173--179</pages>
<contexts>
<context position="10332" citStr="Sang and Veenstra, 1999" startWordPosition="1771" endWordPosition="1774"> exist unknown words, normally they will be segmented as single characters by both FMM and BMM. Based on the differences and context created by FMM and BMM, we will apply a machine learning based model to reassign the position tags which indicate the character position in the word. 3.2 Re-classification of Characters We plan to re-classify the outputs of FMM and BMM character by character. First, we will convert the output of the MMs into characterbased, where each character will be assigned with a position tag such as described in Table 1. The BIES tags are as in (Uchimoto et al., 2000) and (Sang and Veenstra, 1999) for named entity extraction. These tags show the possible character position in a word. For example, if we look at the character &amp;quot;7K &amp;quot;, it is used as a single character in &amp;quot;— / / /&amp;quot; (a book), at the end of a word in &amp;quot;Mil 7K&amp;quot; (script), at the beginning of a word in &amp;quot;7K *&amp;quot; (originally), and at the middle of a word in &amp;quot;a &amp;quot; (basically). Table 1: Position tags in a word Then, based on these features, we will reclassify the tags by using a Support Vector Machine-based (SVM) chunker (Kudo and Matsumoto, 2001). The solid box in Figure 1 shows the features used to determine the class of the character </context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>Erik F. Tjong Kim Sang and John Veenstra. 1999. Representing Text Chunks. In Proceedings of EACL &apos;99, pages 173-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayang Shen</author>
<author>Maosong Sun</author>
<author>Changning Huang</author>
</authors>
<title>The application &amp; implementation of local statistics in Chinese unknown word identification.</title>
<date>1998</date>
<journal>Communications of COUPS,</journal>
<volume>8</volume>
<contexts>
<context position="6810" citStr="Shen et al., 1998" startWordPosition="1144" endWordPosition="1147"> identify the unknown words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on certain types of unknown words such as person names, place names or transliteration names, wi</context>
</contexts>
<marker>Shen, Sun, Huang, 1998</marker>
<rawString>Dayang Shen, Maosong Sun, and Changning Huang. 1998. The application &amp; implementation of local statistics in Chinese unknown word identification. Communications of COUPS, Vol.8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<contexts>
<context position="11289" citStr="Sproat and Emerson, 2003" startWordPosition="1952" endWordPosition="1955">y). Table 1: Position tags in a word Then, based on these features, we will reclassify the tags by using a Support Vector Machine-based (SVM) chunker (Kudo and Matsumoto, 2001). The solid box in Figure 1 shows the features used to determine the class of the character &amp;quot;v&amp;quot; at location i. Based on the output position tags, finally, we will get the segmentation as &amp;quot;irii / / /&amp;quot; (welcome/ new year/ get-together party/ at/). 4 Experiments and Results We have run our experiments with two datasets, PKU Corpus and SIGHAN Bakeoff data. The evaluation is done by using the tool provided in SIGHAN Bakeoff (Sproat and Emerson, 2003). 4.1 Experiment with PKU Corpus 4.1.1 Accuracy on Solving Ambiguity Problem The corpus used for this experiment is from Peking University (PKU) 1, consisting of about 1 million words. It is a segmented and POS &apos;Institute of Computational Linguistics, Peking University, http://www.icl.pku.edu.cn/ Tag Description one-character word first character in a multi-character word intermediate character in a multicharacter word (for words longer than two characters) last character in a multi-character word Char. FMM BMM Output Figure 1: An illustration of classification process - &apos;At the New Year gathe</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international chinese word segmentation bakeoff. In Proceedings of Second SIGHAN Workshop on Chinese Language Processing, pages 133-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Masaki Murata</author>
<author>Hiromi Ozaku</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Named Entity Extraction Based on A Maximum Entropy Model and Transformational Rules.</title>
<date>2000</date>
<booktitle>In Processing of the ACL</booktitle>
<contexts>
<context position="10302" citStr="Uchimoto et al., 2000" startWordPosition="1766" endWordPosition="1769">oth could be wrong. If there exist unknown words, normally they will be segmented as single characters by both FMM and BMM. Based on the differences and context created by FMM and BMM, we will apply a machine learning based model to reassign the position tags which indicate the character position in the word. 3.2 Re-classification of Characters We plan to re-classify the outputs of FMM and BMM character by character. First, we will convert the output of the MMs into characterbased, where each character will be assigned with a position tag such as described in Table 1. The BIES tags are as in (Uchimoto et al., 2000) and (Sang and Veenstra, 1999) for named entity extraction. These tags show the possible character position in a word. For example, if we look at the character &amp;quot;7K &amp;quot;, it is used as a single character in &amp;quot;— / / /&amp;quot; (a book), at the end of a word in &amp;quot;Mil 7K&amp;quot; (script), at the beginning of a word in &amp;quot;7K *&amp;quot; (originally), and at the middle of a word in &amp;quot;a &amp;quot; (basically). Table 1: Position tags in a word Then, based on these features, we will reclassify the tags by using a Support Vector Machine-based (SVM) chunker (Kudo and Matsumoto, 2001). The solid box in Figure 1 shows the features used to determi</context>
</contexts>
<marker>Uchimoto, Ma, Murata, Ozaku, Isahara, 2000</marker>
<rawString>Kiyotaka Uchimoto, Qing Ma, Masaki Murata, Hiromi Ozaku, and Hitoshi Isahara. 2000. Named Entity Extraction Based on A Maximum Entropy Model and Transformational Rules. In Processing of the ACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="8113" citStr="Vapnik, 1995" startWordPosition="1380" endWordPosition="1381">acy ranging from 50% to 70% only. 3 Proposed Method The underlying concept of our proposed method is as following. We regard the problem as a character classification problem. We believe that each character in Chinese holds its characteristics to appear in a certain position in a word. In other words, it can be either used at the beginning of a word, in the middle of a word, at the end of a word, or as a single-character word. By looking at the usage of the characters, we will decide the position tag of the characters using a machine learning based model, which is the Support Vector Machines (Vapnik, 1995). This method serves as a model to solve ambiguity problem, and at the same time, embeds a model to detect unknown words. We will now describe the method in more details in the following section. 3.1 Maximum Matching Algorithm We intend to solve the ambiguity problem by combining a dictionary-based approach with a statistical model. Maximum Matching (MM) algorithm is regarded as the simplest dictionarybased word segmentation approach. It starts from one end of a sentence, and tries to match the first longest word wherever possible. It is a greedy algorithm, but it has been empirically proved t</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Susan P Converse</author>
</authors>
<title>Combining Classifiers for Chinese Word Segmentation.</title>
<date>2002</date>
<booktitle>In Proceedings of 1st SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>57--63</pages>
<contexts>
<context position="5635" citStr="Xue and Converse, 2002" startWordPosition="942" endWordPosition="945">r training the Naïve Bayes classifiers to resolve overlapping ambiguities. They achieved 94.13% accuracy with 5,759 cases of ambiguity. A variation form of TF.IDF weighting is proposed for solving covering ambiguity problem in (Luo et al., 2002). They focus on 90 ambiguous words and achieve an accuracy of 96.58%. Most of the previous methods reported on the accuracy for overall segmentation. Recently, many researches are done by combining multiple models. Furthermore, most people have realized that working on character-based is more efficient than word-based for Chinese word segmentation. In (Xue and Converse, 2002), two classifiers are combined for Chinese word segmentation. First, a Maximum Entropy model is used to segment the text, then an error driven transformation model is used to correct the word boundaries. Similarly, they also use character-based tagging on the position of characters in words. They achieved an F-measure of 95.17. Another recent report is by (Fu and Luke, 2003), where hybrid models for integrated segmentation is proposed. Modified word juncture models and word-formation patterns are used to find the word boundaries and at the same time to identify the unknown words. They achieved</context>
</contexts>
<marker>Xue, Converse, 2002</marker>
<rawString>Nianwen Xue and Susan P. Converse. 2002. Combining Classifiers for Chinese Word Segmentation. In Proceedings of 1st SIGHAN Workshop on Chinese Language Processing, pages 57-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Qun Liu</author>
<author>Hao Zhang</author>
<author>Xue-Qi Cheng</author>
</authors>
<title>Automatic Regcognition of Chinese Unknown Words Based on Roles Tagging.</title>
<date>2002</date>
<booktitle>In Proceedings of 1st SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="6830" citStr="Zhang et al., 2002" startWordPosition="1148" endWordPosition="1151">wn words. They achieved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on certain types of unknown words such as person names, place names or transliteration names, with over 80%. However</context>
<context position="18422" citStr="Zhang et al., 2002" startWordPosition="3206" endWordPosition="3209"> As a result, the new dictionaries contain 36,830 and 12,274 words respectively. Table 6 shows the details for the setting. For PKU corpus, the best result in the bakeoff 2We work only on GB code, the standard coding used for simplified Chinese characters. However, it can be modified easily to suit Big5 coding for traditional Chinese characters. achieved 95.1 in F-measure (Zhang et al., 2003). They use hierarchical Hidden Markov Models to segment and POS tag the text. Although it is a closed test, they have used extra information such as class-based segmentation and role-based tagging models (Zhang et al., 2002), which give better result for unknown word recognition. Our method has done only slightly worse than theirs, with 94.7. The recall for unknown words is comparable, with 71.0% while the best one has 72.4%. Unfortunately, the recall for known words drops a bit, with 97.3%, while the best one is slightly better, with 97.9%, as shown in Table 7. We also compare with (Asahara et al., 2003), where similar method is used for reassigning the word boundaries, except that the words are first categorized into 5 or 10 classes (which is assumed equivalent to POS tags) using Baum-Welch algorithm, then the </context>
</contexts>
<marker>Zhang, Liu, Zhang, Cheng, 2002</marker>
<rawString>Hua-Ping Zhang, Qun Liu, Hao Zhang, and Xue-Qi Cheng. 2002. Automatic Regcognition of Chinese Unknown Words Based on Roles Tagging. In Proceedings of 1st SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Hong-Kui Yu</author>
<author>De-Yi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese Lexical Analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proceedings of Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>184--187</pages>
<contexts>
<context position="18198" citStr="Zhang et al., 2003" startWordPosition="3170" endWordPosition="3173">iment 3, it is based on our previous experiments where the division of half of the training corpus generated the best result by F-measure. Therefore, only 50% of the training corpora are used while creating the dictionaries. As a result, the new dictionaries contain 36,830 and 12,274 words respectively. Table 6 shows the details for the setting. For PKU corpus, the best result in the bakeoff 2We work only on GB code, the standard coding used for simplified Chinese characters. However, it can be modified easily to suit Big5 coding for traditional Chinese characters. achieved 95.1 in F-measure (Zhang et al., 2003). They use hierarchical Hidden Markov Models to segment and POS tag the text. Although it is a closed test, they have used extra information such as class-based segmentation and role-based tagging models (Zhang et al., 2002), which give better result for unknown word recognition. Our method has done only slightly worse than theirs, with 94.7. The recall for unknown words is comparable, with 71.0% while the best one has 72.4%. Unfortunately, the recall for known words drops a bit, with 97.3%, while the best one is slightly better, with 97.9%, as shown in Table 7. We also compare with (Asahara e</context>
<context position="19639" citStr="Zhang et al., 2003" startWordPosition="3416" endWordPosition="3419">e sentence is segmented into word sequence by a Hidden Markov Model-based segmenter. Finally, the same Support Vector Machine-based chunker is trained to correct the errors made by the segmenter. Our method which is simply a forward and backward maximum matching algorithm, has done a lot better then theirs, where complicated statistical based models are involved. They have achieved only 92.4 F-measure while we have 94.7. On the other hand, our results for CHTB corpus are not as comparable as the best result in the bakeoff. We could only get 84.7 point of Fmeasure, while the best one has 88.1 (Zhang et al., 2003) and 82.9 by (Asahara et al., 2003). It may be due to the reason that the training corpus is a lot smaller than the PKU corpus and the testing data contains more unknown words. PKU Data CHTB Data # of # of unk- unk-word # of # of unk- unk-word words words rate words words rate Original Training 1,121,017 0 0% 250,841 0 0% Original Testing 17,194 1,189 6.9% 39,922 7,216 18.1% (In GB code) (781) (4.5%) (7,171) (18.0%) Set 1 560,649 0 0% 125,405 0 0% Set 2 560,368 29,303 5.2% 125,436 13,976 11.1% Testing Data 17,194 1,121 6.5% 39,922 9,769 24.5% Table 6: Bakeoff Data However, we still could get q</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. HHMM-based Chinese Lexical Analyzer ICTCLAS. In Proceedings of Second SIGHAN Workshop on Chinese Language Processing, pages 184-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guo-Dong Zhou</author>
<author>Kim-Teng Lua</author>
</authors>
<title>Detection of Unknown Chinese Words Using a Hybrid Approach.</title>
<date>1997</date>
<booktitle>Computer Processing of Oriental Language,</booktitle>
<pages>11--1</pages>
<contexts>
<context position="6851" citStr="Zhou and Lua, 1997" startWordPosition="1152" endWordPosition="1155">ved 96.1 F-measure. As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other. Solving unknown word problem is also an important process in word segmentation. An unknown word is defined as a word not found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary. The creation of new words in Chinese is unlimited and is a continuous process. For example, the name for new diseases, technical terms, new expressions and etc. The accuracy is better if one focuses only on certain types of unknown words such as person names, place names or transliteration names, with over 80%. However, for general unknown</context>
</contexts>
<marker>Zhou, Lua, 1997</marker>
<rawString>Guo-Dong Zhou and Kim-Teng Lua. 1997. Detection of Unknown Chinese Words Using a Hybrid Approach. Computer Processing of Oriental Language, 11(1):63-75.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>