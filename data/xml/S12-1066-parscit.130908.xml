<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020028">
<title confidence="0.996417">
UOW-SHEF: SimpLex – Lexical Simplicity Ranking based on Contextual
and Psycholinguistic Features
</title>
<author confidence="0.976512">
Sujay Kumar Jauhar
</author>
<affiliation confidence="0.984872">
Research Group in Computational Linguistics
University of Wolverhampton
</affiliation>
<address confidence="0.928017">
Stafford Street, Wolverhampton
WV1 1SB, UK
</address>
<email confidence="0.994109">
Sujay.KumarJauhar@wlv.ac.uk
</email>
<author confidence="0.991512">
Lucia Specia
</author>
<affiliation confidence="0.9978765">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.961284">
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
</address>
<email confidence="0.998691">
L.Specia@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999772375">
This paper describes SimpLex,1 a Lexical
Simplification system that participated in the
English Lexical Simplification shared task at
SemEval-2012. It operates on the basis of
a linear weighted ranking function composed
of context sensitive and psycholinguistic fea-
tures. The system outperforms a very strong
baseline, and ranked first on the shared task.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997597">
Lexical Simplification revolves around replacing
words by their simplest synonym in a context aware
fashion. It is similar in many respects to the task of
Lexical Substitution (McCarthy and Navigli, 2007)
in that it involves elements of selectional preference
on the basis of a central predefined criterion (sim-
plicity in the current case), as well as sensitivity to
context.
Lexical Simplification envisages principally a hu-
man target audience, and can greatly benefit chil-
dren, second language learners, people with low lit-
eracy levels or cognitive disabilities, and in general
facilitate the dissemination of knowledge to wider
audiences.
We experimented with a number of features that
we posited might be inherently linked with tex-
tual simplicity and selected the three that seemed
the most promising on an evaluation with the trial
dataset. These include contextual and psycholin-
guistic components. When combined using an SVM
</bodyText>
<footnote confidence="0.885059">
1Developed by co-organizers of the shared task
</footnote>
<bodyText confidence="0.9990546">
ranker to build a model, such a model provides re-
sults that offer a statistically significant improve-
ment over a very strong context-independent base-
line. The system ranked first overall on the Lexical
Simplification task.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999767565217391">
Lexical Simplification has received considerably
less interest in the NLP community as compared
with Syntactic Simplification. However, there are
a number of notable works related to the topic.
In particular Yatskar et al. (2010) leverage the
relations between Simple Wikipedia and English
Wikipedia to extract simplification pairs. Biran et al.
(2011) extend this base methodology to apply lexi-
cal simplification to input sentences. De Belder and
Moens (2010), in contrast, provide a more general
architecture for the task, with scope for possible ex-
tension to other languages.
These studies and others have envisaged a range
of different target user groups including children
(De Belder and Moens, 2010), people with low liter-
acy levels (Aluisio et al., 2008) and aphasic readers
(Carroll et al., 1998).
The current work differs from previous research
in that it envisages a stand-alone lexical simpli-
fication system based on linguistically motivated
and cognitive principles within the framework of a
shared task. Its core methodology remains open to
integration into a larger Text Simplification system.
</bodyText>
<sectionHeader confidence="0.993042" genericHeader="method">
3 Task Setup
</sectionHeader>
<bodyText confidence="0.9022085">
The English Lexical Simplification shared task at
SemEval-2012 (Specia et al., 2012) required sys-
</bodyText>
<page confidence="0.974475">
477
</page>
<note confidence="0.528474">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 477–481,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.963681">
tems to rank a number of candidate substitutes
(which were provided beforehand) based on their
simplicity of usage in a given context. For example,
given the following context with an empty place-
holder, and its candidate substitutes:
Context: During the siege , George
Robertson had appointed Shuja-ul-Mulk,
who was a boy only 12 years old and
the youngest surviving son of Aman-ul-
Mulk, as the ruler of Chitral.
Candidates: {clever} {smart}
{intelligent} {bright}
a system is required to produce a ranking, e.g.:
System: {intelligent} {bright} {clever,
smart}
Note that ties were permitted and that all candi-
dates needed to be included in the system rankings.
</bodyText>
<sectionHeader confidence="0.923852" genericHeader="method">
4 The SimpLex Lexical Simplification
System
</sectionHeader>
<bodyText confidence="0.999961">
In an approach similar to what Hassan et al. (2007)
used for Lexical Substitution, SimpLex ranks can-
didates based on a weighted linear scoring function,
which has the generalized form:
</bodyText>
<equation confidence="0.973042">
�s (cn,i) �
mEM
</equation>
<bodyText confidence="0.999974210526316">
where cn,i is the candidate substitute to be scored,
and each rm is a standalone ranking function that
attributes to each candidate its rank based on its
uniquely associated features. Based on this scoring,
candidates for context are ranked in descending or-
der of scores.
In the development of the system we experi-
mented with a number of these features including
ranking based on word length, number of syllables,
scoring with a 2-step cluster and rank architecture,
latent semantic analysis, and average point-wise mu-
tual information between the candidate and neigh-
boring words in the context.
However, the features which were intuitively the
simplest proved, in the end, to give the best results.
They were selected based on their superior perfor-
mance on the trial dataset and their competitiveness
with the strong Simple Frequency baseline. These
stand-alone features are described in what follows.
</bodyText>
<subsectionHeader confidence="0.993485">
4.1 Adapted N-Gram Model
</subsectionHeader>
<bodyText confidence="0.999904">
The motivation behind an n-gram model for Lexical
Simplification is that the task involves an inherent
WSD problem. This is because the same word may
be used with different senses (and consequently dif-
ferent levels of complexity) in different contexts.
A blind application of n-gram frequency search-
ing on the shared task’s dataset, however, gives sub-
optimal results because of two main factors:
</bodyText>
<listItem confidence="0.95919625">
1. Inconsistently lemmatized candidates.
2. Blind replacement of even correctly lemma-
tized forms in context producing ungrammat-
ical results.
</listItem>
<bodyText confidence="0.984023838709677">
We infer the correct inflection of all candidates for
a given context based on the appearance of the orig-
inal target word (which is also one of the candidate
substitutes) in context. To do this we run a part-of-
speech (POS) tagger on the source text and note the
POS of the target word. Then handcrafted rules are
used to correctly inflect the other candidates based
on this POS tag.
To resolve the issue of ungrammatical textual out-
put, we further use a simple approach of popping
words in close proximity to the placeholder and per-
forming n-gram searches on all possible query com-
binations. Take for instance the following example:
Context: He was away.
Candidates: {going} {leaving}
where “going” is evidently the original word in con-
text, but “leaving” has also been suggested as a sub-
stitute (there are many such cases in the datasets).
One of the possible outcomes of popping context
words leads to the correct sequence for the latter
substitute, i.e. “He was leaving” with the word
“away” having been popped.
The rationale behind this approach is that if one of
the combinations is grammatically correct, the num-
ber of n-gram hits it returns will far exceed those
returned by ungrammatical ones.
The n-gram (2 &lt; n &lt; 5) searches are performed
on the Google Web 1T corpus (Brants and Franz,
2006), and the number of hits is weighted by the
length of the n-gram search (such that longer se-
quences obtain higher weight). This may seem like
</bodyText>
<equation confidence="0.70098">
1
rm (cn,i)
</equation>
<page confidence="0.986985">
478
</page>
<bodyText confidence="0.998472857142857">
a simplistic approach, especially when the candidate
words appear in long-distance dependency relations
to other parts of the sentence. However, it should be
noted that since the Web 1T corpus only consists of
n-grams with n &lt; 5, structures that contain longer
dependencies than this are in any case not consid-
ered, and hence do not interfere with local context.
</bodyText>
<subsectionHeader confidence="0.637768">
4.2 Bag-of-Words Model
</subsectionHeader>
<bodyText confidence="0.999987842105263">
The limitations of performing queries on the Google
Web 1T are that n-grams hits must be in strict lin-
ear order of appearance. To overcome this diffi-
culty, we further mimic the functioning of a bag-
of-words model by taking all possible ordering of
words of a given n-gram sequence. This approach,
to some extent, gives the possibility of observing co-
occurrences of candidate and context words in vari-
ous orderings of appearance. This results in a num-
ber of inadequate query strings, but possibly a few
(as opposed to one in a linear n-gram search) good
word orderings with high hits as well.
As with the previous model, only n-grams with
2 &lt; n &lt; 5 are taken. For a given substitute the total
number of hits for all possible queries involving that
substitute are summed (with each hit being weighted
by the length of its corresponding query in words).
To obtain the final score, this sum is normalized by
the actual number of queries.
</bodyText>
<subsectionHeader confidence="0.993076">
4.3 Psycholinguistic Feature Model
</subsectionHeader>
<bodyText confidence="0.999756333333333">
The MRC Psycholinguistic Database (Wilson, 1988)
and the Bristol Norms (Stadthagen-Gonzalez and
Davis, 2006) are knowledge repositories that asso-
ciate scores to words based on a number of psy-
cholinguistic features. The ones that we felt were
most pertinent to our study are:
</bodyText>
<listItem confidence="0.9987475">
1. Concreteness - the level of abstraction associ-
ated with the concept a word describes.
2. Imageability - the ability of a given word to
arouse mental images.
3. Familiarity - the frequency of exposure to a
word.
4. Age of Acquisition - the age at which a given
word is appropriated by a speaker.
</listItem>
<bodyText confidence="0.999993310344828">
We combined both databases and compiled a sin-
gle resource consisting of all the words from both
sources that list at least one of these features. It may
be noted that these attributes were compiled in simi-
lar fashion in both databases and were normalized to
the same scale of scores falling in the range of 100
to 700.
In spite of a combined compilation, the coverage
of the resource was poor, with more than half the
candidate substitutes on both trial and test sets sim-
ply not being listed in the databases. To overcome
this difficulty we introduced a fifth frequency feature
that essentially simulates the “Simple Frequency”
baseline, 2 but with scores that were normalized to
the same scale of the other psycholinguistic features.
This composite of features was used in a linear
weighted function with weights tuned to best perfor-
mance values on the trial dataset. This function sums
the weighted scores for each candidate, and normal-
izes this sum by the number of non-zero features (in
the worst-case scenario, – when no psycholinguistic
features are found – the scorer is equivalent to the
“Simple Frequency” baseline). It is interesting to
note that the frequency feature did not dominate the
linear combination; rather there was a nice interplay
of features with Concreteness, Imageability, Famil-
iarity, Age of Acquisition and Simple Frequency be-
ing weighted (on a scale of -1 to +1) as 0.72, -0.22,
0.87, 0.36 and 0.36, respectively.
</bodyText>
<subsectionHeader confidence="0.990258">
4.4 Feature Combination
</subsectionHeader>
<bodyText confidence="0.999618285714286">
We combined the three standalone models using
the ranking function of the SVM-light package
(Joachims, 2006) for building SVM rankers. The pa-
rameters of the SVM were tuned on the trial dataset,
which consisted of only 300 example contexts. To
avoid overfitting, instead of taking the single best
parameters, we took parameter values that were the
average of the top 10 distinct runs.
It may be noted that the resulting model makes no
attempt to tie candidates, although actual ties may be
produced by chance. But since ties are rarely used
in the gold standard for the trial dataset, we reasoned
that this should not affect the system performance in
any significant way.
</bodyText>
<footnote confidence="0.992961">
2The “Simple Frequency” baseline scores each substitute
based on the number of hits it produces in the Google Web 1T
</footnote>
<page confidence="0.994165">
479
</page>
<table confidence="0.999120666666667">
bline-SFreq w-ln n-syll psycho a-n-gram b-o-w pmi lsa SimpLex
Trial 0.398 0.176 0.118 0.388 0.397 0.395 0.340 0.089 –
Test 0.471 0.236 0.163 0.432 0.460 0.460 0.404 0.054 0.496
</table>
<tableCaption confidence="0.999961">
Table 1: Comparison of Models’ Scores
</tableCaption>
<sectionHeader confidence="0.998669" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999951565217391">
The results of the SimpLex system trained and tuned
on the trial set, in comparison with the Simple Fre-
quency baseline and the other stand-alone features
we experimented with are presented in Table 1. The
scores are computed through a version of the Kappa
index over pairwise rankings, and therefore repre-
sent the average agreement between the system and
the gold-standard annotation in the ranking of pairs
of candidate substitutes.
Table 1 shows that while in isolation the features
are unable to beat the Simple Frequency model, to-
gether they form a combination which outperforms
the baseline. The improvement of SimpLex over
the other models is statistically significant (statisti-
cal significance was established using a randomiza-
tion test with 1000 iterations and p-value &lt; 0.05).
We believe that the reason why the context aware
features were still unable to score better than the
context-independent baseline is the isolated focus
on simplifying a single target word. People tend
to produce language that contains words of roughly
equal levels of complexity. Hence in some cases
the surrounding context, instead of helping to dis-
ambiguate the target word, introduces further noise
to queries, especially when its individual component
words have skewed complexity factors. A simul-
taneous simplification of all the content words in a
context could be a possible solution to this problem.
As an additional experiment to assess the impor-
tance of the size of the training data in our simplifi-
cation system, we pooled together the trial and test
datasets, and ran several iterations of the combina-
tion algorithm with a regular increment of number of
training examples and noted the effects it produced
on eventual score. Three hundred examples were ap-
portioned consistently to a test set to maintain com-
parability between experiments. Note that this time,
no optimization of the SVM parameters was made.
The results were inconclusive, and contrary to ex-
pectation, revealed that there is no general improve-
ment with additional training data. This could be
because of the difficulty of the learning problem, for
which the scope of the combined dataset is still very
limited. A more detailed study with a corpus that is
orders of magnitude larger than the current one may
be necessary to establish conclusive evidence.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999772888888889">
This paper presented our system SimpLex which
participated in the English Lexical Simplification
shared-task at SemEval-2012 and ranked first out of
9 participating systems.
Our findings showed that while a context agnostic
frequency approach to lexical simplification seems
to effectively model the problem of assessing word
complexity to a relatively decent level of accuracy,
as evidenced by the strong baseline of the shared
task, other elements, such as interplay of context
awareness with humanly perceived psycholinguistic
features can produce better results, in spite of very
limited training data.
Finally, a more global approach to lexical sim-
plification that concurrently addresses all the words
in a context to normalize simplicity levels, may be
a more realistic proposition for target applications,
and also help context aware features perform better.
</bodyText>
<sectionHeader confidence="0.943668" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99677775">
This work was supported by the European Com-
mission, Education &amp; Training, Erasmus Mundus:
EMMC 2008-0083, Erasmus Mundus Masters in
NLP &amp; HLT program.
</bodyText>
<sectionHeader confidence="0.993094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.613652">
Sandra M. Aluisio, Lucia Specia, Thiago A.S. Pardo, Er-
ick G. Maziero, and Renata P.M. Fortes. 2008. To-
wards brazilian portuguese automatic text simplifica-
tion systems. In Proceeding of the eighth ACM sym-
</reference>
<page confidence="0.992854">
480
</page>
<reference confidence="0.99973049122807">
posium on Document engineering, DocEng ’08, pages
240–248, Sao Paulo, Brazil. ACM.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 496–501,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Thorsten Brants and Alex Franz. 2006. The google web
1t 5-gram corpus version 1.1 ldc2006t13.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
Proceedings of AAAI - 98 Workshop on Integrating
Artificial Intelligence and Assistive Technology, Madi-
son, Wisconsin, July.
Jan De Belder and Marie-Francine Moens. 2010. Text
simplification for children. In Proceedings of the SI-
GIR workshop on Accessible Search Systems, pages
19–26. ACM, July.
S. Hassan, A. Csomai, C. Banea, R. Sinha, and R. Mi-
halcea. 2007. Unt: Subfinder: Combining knowledge
sources for automatic lexical substitution. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 410 – 413. Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ’06, pages 217–226, New York,
NY, USA. ACM.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), Prague, Czech Re-
public, pages 48–53.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.
2012. Semeval-2012 task 1: English lexical simplifi-
cation. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Hans Stadthagen-Gonzalez and Colin Davis. 2006. The
bristol norms for age of acquisition, imageability, and
familiarity. Behavior Research Methods, 38:598–605.
Michael Wilson. 1988. Mrc psycholinguistic database:
Machine-usable dictionary, version 2.00. Behavior
Research Methods, 20:6–10.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ’10, pages 365–368, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.998706">
481
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358115">
<title confidence="0.998173">UOW-SHEF: SimpLex – Lexical Simplicity Ranking based on and Psycholinguistic Features</title>
<author confidence="0.999618">Sujay Kumar</author>
<affiliation confidence="0.988477333333333">Research Group in Computational University of Stafford Street,</affiliation>
<address confidence="0.786784">WV1 1SB,</address>
<email confidence="0.552508">Sujay.KumarJauhar@wlv.ac.uk</email>
<author confidence="0.942387">Lucia</author>
<affiliation confidence="0.9986945">Department of Computer University of</affiliation>
<address confidence="0.910657">Regent Court, 211 Sheffield, S1 4DP,</address>
<email confidence="0.998991">L.Specia@dcs.shef.ac.uk</email>
<abstract confidence="0.998262555555556">paper describes a Lexical Simplification system that participated in the English Lexical Simplification shared task at SemEval-2012. It operates on the basis of a linear weighted ranking function composed of context sensitive and psycholinguistic features. The system outperforms a very strong baseline, and ranked first on the shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sandra M Aluisio</author>
<author>Lucia Specia</author>
<author>Thiago A S Pardo</author>
<author>Erick G Maziero</author>
<author>Renata P M Fortes</author>
</authors>
<title>Towards brazilian portuguese automatic text simplification systems.</title>
<date>2008</date>
<booktitle>In Proceeding of the eighth ACM symposium on Document engineering, DocEng ’08,</booktitle>
<pages>240--248</pages>
<publisher>ACM.</publisher>
<location>Sao Paulo, Brazil.</location>
<contexts>
<context position="2759" citStr="Aluisio et al., 2008" startWordPosition="404" endWordPosition="407">e are a number of notable works related to the topic. In particular Yatskar et al. (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Biran et al. (2011) extend this base methodology to apply lexical simplification to input sentences. De Belder and Moens (2010), in contrast, provide a more general architecture for the task, with scope for possible extension to other languages. These studies and others have envisaged a range of different target user groups including children (De Belder and Moens, 2010), people with low literacy levels (Aluisio et al., 2008) and aphasic readers (Carroll et al., 1998). The current work differs from previous research in that it envisages a stand-alone lexical simplification system based on linguistically motivated and cognitive principles within the framework of a shared task. Its core methodology remains open to integration into a larger Text Simplification system. 3 Task Setup The English Lexical Simplification shared task at SemEval-2012 (Specia et al., 2012) required sys477 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 477–481, Montr´eal, Canada, June 7-8, 2012. c�2012 Association </context>
</contexts>
<marker>Aluisio, Specia, Pardo, Maziero, Fortes, 2008</marker>
<rawString>Sandra M. Aluisio, Lucia Specia, Thiago A.S. Pardo, Erick G. Maziero, and Renata P.M. Fortes. 2008. Towards brazilian portuguese automatic text simplification systems. In Proceeding of the eighth ACM symposium on Document engineering, DocEng ’08, pages 240–248, Sao Paulo, Brazil. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>Putting it simply: a context-aware approach to lexical simplification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>496--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2350" citStr="Biran et al. (2011)" startWordPosition="338" endWordPosition="341">rganizers of the shared task ranker to build a model, such a model provides results that offer a statistically significant improvement over a very strong context-independent baseline. The system ranked first overall on the Lexical Simplification task. 2 Related Work Lexical Simplification has received considerably less interest in the NLP community as compared with Syntactic Simplification. However, there are a number of notable works related to the topic. In particular Yatskar et al. (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Biran et al. (2011) extend this base methodology to apply lexical simplification to input sentences. De Belder and Moens (2010), in contrast, provide a more general architecture for the task, with scope for possible extension to other languages. These studies and others have envisaged a range of different target user groups including children (De Belder and Moens, 2010), people with low literacy levels (Aluisio et al., 2008) and aphasic readers (Carroll et al., 1998). The current work differs from previous research in that it envisages a stand-alone lexical simplification system based on linguistically motivated</context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>Or Biran, Samuel Brody, and Noemie Elhadad. 2011. Putting it simply: a context-aware approach to lexical simplification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 496–501, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>The google web 1t 5-gram corpus version</title>
<date>2006</date>
<volume>1</volume>
<pages>2006--13</pages>
<contexts>
<context position="7058" citStr="Brants and Franz, 2006" startWordPosition="1094" endWordPosition="1097">eaving} where “going” is evidently the original word in context, but “leaving” has also been suggested as a substitute (there are many such cases in the datasets). One of the possible outcomes of popping context words leads to the correct sequence for the latter substitute, i.e. “He was leaving” with the word “away” having been popped. The rationale behind this approach is that if one of the combinations is grammatically correct, the number of n-gram hits it returns will far exceed those returned by ungrammatical ones. The n-gram (2 &lt; n &lt; 5) searches are performed on the Google Web 1T corpus (Brants and Franz, 2006), and the number of hits is weighted by the length of the n-gram search (such that longer sequences obtain higher weight). This may seem like 1 rm (cn,i) 478 a simplistic approach, especially when the candidate words appear in long-distance dependency relations to other parts of the sentence. However, it should be noted that since the Web 1T corpus only consists of n-grams with n &lt; 5, structures that contain longer dependencies than this are in any case not considered, and hence do not interfere with local context. 4.2 Bag-of-Words Model The limitations of performing queries on the Google Web </context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The google web 1t 5-gram corpus version 1.1 ldc2006t13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Practical simplification of english newspaper text to assist aphasic readers.</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI - 98 Workshop on Integrating Artificial Intelligence and Assistive Technology,</booktitle>
<location>Madison, Wisconsin,</location>
<contexts>
<context position="2802" citStr="Carroll et al., 1998" startWordPosition="411" endWordPosition="414">the topic. In particular Yatskar et al. (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Biran et al. (2011) extend this base methodology to apply lexical simplification to input sentences. De Belder and Moens (2010), in contrast, provide a more general architecture for the task, with scope for possible extension to other languages. These studies and others have envisaged a range of different target user groups including children (De Belder and Moens, 2010), people with low literacy levels (Aluisio et al., 2008) and aphasic readers (Carroll et al., 1998). The current work differs from previous research in that it envisages a stand-alone lexical simplification system based on linguistically motivated and cognitive principles within the framework of a shared task. Its core methodology remains open to integration into a larger Text Simplification system. 3 Task Setup The English Lexical Simplification shared task at SemEval-2012 (Specia et al., 2012) required sys477 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 477–481, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tems to rank </context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>John Carroll, Guido Minnen, Yvonne Canning, Siobhan Devlin, and John Tait. 1998. Practical simplification of english newspaper text to assist aphasic readers. In Proceedings of AAAI - 98 Workshop on Integrating Artificial Intelligence and Assistive Technology, Madison, Wisconsin, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan De Belder</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Text simplification for children.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGIR workshop on Accessible Search Systems,</booktitle>
<pages>pages</pages>
<publisher>ACM,</publisher>
<marker>De Belder, Moens, 2010</marker>
<rawString>Jan De Belder and Marie-Francine Moens. 2010. Text simplification for children. In Proceedings of the SIGIR workshop on Accessible Search Systems, pages 19–26. ACM, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hassan</author>
<author>A Csomai</author>
<author>C Banea</author>
<author>R Sinha</author>
<author>R Mihalcea</author>
</authors>
<title>Unt: Subfinder: Combining knowledge sources for automatic lexical substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 410 – 413. Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4144" citStr="Hassan et al. (2007)" startWordPosition="615" endWordPosition="618">For example, given the following context with an empty placeholder, and its candidate substitutes: Context: During the siege , George Robertson had appointed Shuja-ul-Mulk, who was a boy only 12 years old and the youngest surviving son of Aman-ulMulk, as the ruler of Chitral. Candidates: {clever} {smart} {intelligent} {bright} a system is required to produce a ranking, e.g.: System: {intelligent} {bright} {clever, smart} Note that ties were permitted and that all candidates needed to be included in the system rankings. 4 The SimpLex Lexical Simplification System In an approach similar to what Hassan et al. (2007) used for Lexical Substitution, SimpLex ranks candidates based on a weighted linear scoring function, which has the generalized form: �s (cn,i) � mEM where cn,i is the candidate substitute to be scored, and each rm is a standalone ranking function that attributes to each candidate its rank based on its uniquely associated features. Based on this scoring, candidates for context are ranked in descending order of scores. In the development of the system we experimented with a number of these features including ranking based on word length, number of syllables, scoring with a 2-step cluster and ra</context>
</contexts>
<marker>Hassan, Csomai, Banea, Sinha, Mihalcea, 2007</marker>
<rawString>S. Hassan, A. Csomai, C. Banea, R. Sinha, and R. Mihalcea. 2007. Unt: Subfinder: Combining knowledge sources for automatic lexical substitution. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 410 – 413. Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06,</booktitle>
<pages>217--226</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10718" citStr="Joachims, 2006" startWordPosition="1718" endWordPosition="1719">umber of non-zero features (in the worst-case scenario, – when no psycholinguistic features are found – the scorer is equivalent to the “Simple Frequency” baseline). It is interesting to note that the frequency feature did not dominate the linear combination; rather there was a nice interplay of features with Concreteness, Imageability, Familiarity, Age of Acquisition and Simple Frequency being weighted (on a scale of -1 to +1) as 0.72, -0.22, 0.87, 0.36 and 0.36, respectively. 4.4 Feature Combination We combined the three standalone models using the ranking function of the SVM-light package (Joachims, 2006) for building SVM rankers. The parameters of the SVM were tuned on the trial dataset, which consisted of only 300 example contexts. To avoid overfitting, instead of taking the single best parameters, we took parameter values that were the average of the top 10 distinct runs. It may be noted that the resulting model makes no attempt to tie candidates, although actual ties may be produced by chance. But since ties are rarely used in the gold standard for the trial dataset, we reasoned that this should not affect the system performance in any significant way. 2The “Simple Frequency” baseline scor</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 217–226, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="986" citStr="McCarthy and Navigli, 2007" startWordPosition="130" endWordPosition="133">ffield, S1 4DP, UK L.Specia@dcs.shef.ac.uk Abstract This paper describes SimpLex,1 a Lexical Simplification system that participated in the English Lexical Simplification shared task at SemEval-2012. It operates on the basis of a linear weighted ranking function composed of context sensitive and psycholinguistic features. The system outperforms a very strong baseline, and ranked first on the shared task. 1 Introduction Lexical Simplification revolves around replacing words by their simplest synonym in a context aware fashion. It is similar in many respects to the task of Lexical Substitution (McCarthy and Navigli, 2007) in that it involves elements of selectional preference on the basis of a central predefined criterion (simplicity in the current case), as well as sensitivity to context. Lexical Simplification envisages principally a human target audience, and can greatly benefit children, second language learners, people with low literacy levels or cognitive disabilities, and in general facilitate the dissemination of knowledge to wider audiences. We experimented with a number of features that we posited might be inherently linked with textual simplicity and selected the three that seemed the most promising</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Sujay Kumar Jauhar</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semeval-2012 task 1: English lexical simplification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3203" citStr="Specia et al., 2012" startWordPosition="470" endWordPosition="473">studies and others have envisaged a range of different target user groups including children (De Belder and Moens, 2010), people with low literacy levels (Aluisio et al., 2008) and aphasic readers (Carroll et al., 1998). The current work differs from previous research in that it envisages a stand-alone lexical simplification system based on linguistically motivated and cognitive principles within the framework of a shared task. Its core methodology remains open to integration into a larger Text Simplification system. 3 Task Setup The English Lexical Simplification shared task at SemEval-2012 (Specia et al., 2012) required sys477 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 477–481, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tems to rank a number of candidate substitutes (which were provided beforehand) based on their simplicity of usage in a given context. For example, given the following context with an empty placeholder, and its candidate substitutes: Context: During the siege , George Robertson had appointed Shuja-ul-Mulk, who was a boy only 12 years old and the youngest surviving son of Aman-ulMulk, as the ruler of Chitral. Ca</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Stadthagen-Gonzalez</author>
<author>Colin Davis</author>
</authors>
<title>The bristol norms for age of acquisition, imageability, and familiarity. Behavior Research Methods,</title>
<date>2006</date>
<pages>38--598</pages>
<contexts>
<context position="8678" citStr="Stadthagen-Gonzalez and Davis, 2006" startWordPosition="1371" endWordPosition="1374"> in a number of inadequate query strings, but possibly a few (as opposed to one in a linear n-gram search) good word orderings with high hits as well. As with the previous model, only n-grams with 2 &lt; n &lt; 5 are taken. For a given substitute the total number of hits for all possible queries involving that substitute are summed (with each hit being weighted by the length of its corresponding query in words). To obtain the final score, this sum is normalized by the actual number of queries. 4.3 Psycholinguistic Feature Model The MRC Psycholinguistic Database (Wilson, 1988) and the Bristol Norms (Stadthagen-Gonzalez and Davis, 2006) are knowledge repositories that associate scores to words based on a number of psycholinguistic features. The ones that we felt were most pertinent to our study are: 1. Concreteness - the level of abstraction associated with the concept a word describes. 2. Imageability - the ability of a given word to arouse mental images. 3. Familiarity - the frequency of exposure to a word. 4. Age of Acquisition - the age at which a given word is appropriated by a speaker. We combined both databases and compiled a single resource consisting of all the words from both sources that list at least one of these</context>
</contexts>
<marker>Stadthagen-Gonzalez, Davis, 2006</marker>
<rawString>Hans Stadthagen-Gonzalez and Colin Davis. 2006. The bristol norms for age of acquisition, imageability, and familiarity. Behavior Research Methods, 38:598–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wilson</author>
</authors>
<title>Mrc psycholinguistic database: Machine-usable dictionary, version 2.00. Behavior Research Methods,</title>
<date>1988</date>
<pages>20--6</pages>
<contexts>
<context position="8618" citStr="Wilson, 1988" startWordPosition="1365" endWordPosition="1366">orderings of appearance. This results in a number of inadequate query strings, but possibly a few (as opposed to one in a linear n-gram search) good word orderings with high hits as well. As with the previous model, only n-grams with 2 &lt; n &lt; 5 are taken. For a given substitute the total number of hits for all possible queries involving that substitute are summed (with each hit being weighted by the length of its corresponding query in words). To obtain the final score, this sum is normalized by the actual number of queries. 4.3 Psycholinguistic Feature Model The MRC Psycholinguistic Database (Wilson, 1988) and the Bristol Norms (Stadthagen-Gonzalez and Davis, 2006) are knowledge repositories that associate scores to words based on a number of psycholinguistic features. The ones that we felt were most pertinent to our study are: 1. Concreteness - the level of abstraction associated with the concept a word describes. 2. Imageability - the ability of a given word to arouse mental images. 3. Familiarity - the frequency of exposure to a word. 4. Age of Acquisition - the age at which a given word is appropriated by a speaker. We combined both databases and compiled a single resource consisting of all</context>
</contexts>
<marker>Wilson, 1988</marker>
<rawString>Michael Wilson. 1988. Mrc psycholinguistic database: Machine-usable dictionary, version 2.00. Behavior Research Methods, 20:6–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: unsupervised extraction of lexical simplifications from wikipedia. In Human Language Technologies:</title>
<date>2010</date>
<contexts>
<context position="2227" citStr="Yatskar et al. (2010)" startWordPosition="321" endWordPosition="324">th the trial dataset. These include contextual and psycholinguistic components. When combined using an SVM 1Developed by co-organizers of the shared task ranker to build a model, such a model provides results that offer a statistically significant improvement over a very strong context-independent baseline. The system ranked first overall on the Lexical Simplification task. 2 Related Work Lexical Simplification has received considerably less interest in the NLP community as compared with Syntactic Simplification. However, there are a number of notable works related to the topic. In particular Yatskar et al. (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Biran et al. (2011) extend this base methodology to apply lexical simplification to input sentences. De Belder and Moens (2010), in contrast, provide a more general architecture for the task, with scope for possible extension to other languages. These studies and others have envisaged a range of different target user groups including children (De Belder and Moens, 2010), people with low literacy levels (Aluisio et al., 2008) and aphasic readers (Carroll et al., 1998). The current work differ</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: unsupervised extraction of lexical simplifications from wikipedia. In Human Language Technologies:</rawString>
</citation>
<citation valid="false">
<booktitle>The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>365--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 365–368, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>