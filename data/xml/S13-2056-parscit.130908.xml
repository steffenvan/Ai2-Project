<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036157">
<title confidence="0.98682">
SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from
Biomedical Texts (DDIExtraction 2013)
</title>
<author confidence="0.97915">
Isabel Segura-Bedmar, Paloma Martinez, Maria Herrero-Zazo
</author>
<affiliation confidence="0.870053">
Universidad Carlos III de Madrid
</affiliation>
<address confidence="0.825264">
Av. Universidad, 30, Legan´es 28911, Spain
</address>
<email confidence="0.996402">
{isegura,pmf}@inf.uc3m.es, mhzazo@pa.uc3m.es
</email>
<sectionHeader confidence="0.995439" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.982624538461538">
The DDIExtraction 2013 task concerns the
recognition of drugs and extraction of drug-
drug interactions that appear in biomedical
literature. We propose two subtasks for the
DDIExtraction 2013 Shared Task challenge:
1) the recognition and classification of drug
names and 2) the extraction and classification
of their interactions. Both subtasks have been
very successful in participation and results.
There were 14 teams who submitted a total of
38 runs. The best result reported for the first
subtask was F1 of 71.5% and 65.1% for the
second one.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924185185185">
The definition of drug-drug interaction (DDI) is
broadly described as a change in the effects of one
drug by the presence of another drug (Baxter and
Stockely, 2010). The detection of DDIs is an im-
portant research area in patient safety since these in-
teractions can become very dangerous and increase
health care costs. Drug interactions are frequently
reported in journals, making medical literature the
most effective source for their detection (Aronson,
2007). Therefore, Information Extraction (IE) can
be of great benefit in the pharmaceutical industry al-
lowing identification and extraction of relevant in-
formation on DDIs and providing an interesting way
of reducing the time spent by health care profession-
als on reviewing the literature.
The DDIExtraction 2013 follows up on a
first event organized in 2011, DDIExtraction
2011 (Segura-Bedmar et al., 2011b) whose main
goal was the detection of drug-drug interactions
from biomedical texts. The new edition includes in
addition to DDI extraction also a supporting task,
the recognition and classification of pharmacologi-
cal substances. DDIExtraction 2013 is designed to
address the extraction of DDIs as a whole, but di-
vided into two subtasks to allow separate evaluation
of the performance for different aspects of the prob-
lem. The shared task includes two challenges:
</bodyText>
<listItem confidence="0.999894666666667">
• Task 9.1: Recognition and classification of
pharmacological substances.
• Task 9.2: Extraction of drug-drug interactions.
</listItem>
<bodyText confidence="0.999979294117647">
Additionally, while the datasets used for
the DDIExtraction 2011 task were composed
by texts describing DDIs from the DrugBank
database(Wishart et al., 2006), the new datasets for
DDIExtraction 2013 also include MedLine abstracts
in order to deal with different types of texts and
language styles.
This shared task has been conceived with a dual
objective: advancing the state-of-the-art of text-
mining techniques applied to the pharmacological
domain, and providing a common framework for
evaluation of the participating systems and other re-
searchers interested in the task.
In the next section we describe the DDI corpus
used in this task. Sections 3 and 4 focus on the de-
scription of the task 9.1 and 9.2 respectively. Finally,
Section 5 draws the conclusions and future work.
</bodyText>
<sectionHeader confidence="0.991411" genericHeader="method">
2 The DDI Corpus
</sectionHeader>
<bodyText confidence="0.9992685">
The DDIExtraction 2013 task relies on the DDI cor-
pus, which is a semantically annotated corpus of
</bodyText>
<page confidence="0.97128">
341
</page>
<bodyText confidence="0.972991857142857">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 341–350, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
documents describing drug-drug interactions from
the DrugBank database and MedLine abstracts on
the subject of drug-drug interactions.
The DDI corpus consists of 1,017 texts (784
DrugBank texts and 233 MedLine abstracts) and
was manually annotated with a total of 18,491 phar-
macological substances and 5,021 drug-drug inter-
actions (see Table 1). A detailed description of the
method used to collect and process documents can
be found in (Segura-Bedmar et al., 2011a). The cor-
pus is distributed in XML documents following the
unified format for PPI corpora proposed by Pyysalo
et al., (2008) (see Figure 1). A detailed description
and analysis of the DDI corpus and its methodology
are included in an article currently under review by
BioInformatics journal.1
The corpus was split in order to build the datasets
for the training and evaluation of the different par-
ticipating systems. Approximately 77% of the DDI
corpus documents were randomly selected for the
training dataset and the remaining (142 DrugBank
texts and 91 MedLine abstracts) was used for the test
dataset. The training dataset is the same for both
subtasks since it contains entity and DDI annota-
tions. The test dataset for the task 9.1 was formed by
discarding documents which contained DDI annota-
tions. Entity annotations were removed from this
dataset to be used by participants. The remaining
documents (that is, those containing some interac-
tion) were used to create the test dataset for task 9.2.
Since entity annotations are not removed from these
documents, the test dataset for the task 9.2 can also
be used as additional training data for the task 9.1.
</bodyText>
<sectionHeader confidence="0.947977" genericHeader="method">
3 Task 9.1: Recognition and classification
</sectionHeader>
<subsectionHeader confidence="0.835936">
of pharmacological substances.
</subsectionHeader>
<bodyText confidence="0.882406083333333">
This task concerns the named entity extraction of
pharmacological substances in text. This named en-
tity task is a crucial first step for information ex-
traction of drug-drug interactions. In this task, four
types of pharmacological substances are defined:
drug (generic drug names), brand (branded drug
names), group (drug group names) and drug-n (ac-
tive substances not approved for human use). For a
1M. Herrero-Zazo, I. Segura-Bedmar, P. Mart´ınez. 2013.
The DDI Corpus: an annotated corpus with pharmacological
substances and drug-drug interactions, submitted to BioInfor-
matics
</bodyText>
<table confidence="0.999203333333333">
Training Test for task 9.1 Test for task 9.2
DDI-DrugBank documents 572 54 158
sentences 5675 145 973
drug 8197 180 1518
group 3206 65 626
brand 1423 53 347
drug n 103 5 21
mechanism 1260 0 279
effect 1548 0 301
advice 819 0 215
int 178 0 94
DDI-MedLine documents 142 58 33
sentences 1301 520 326
drug 1228 171 346
group 193 90 41
brand 14 6 22
drug n 401 115 119
mechanism 62 0 24
effect 152 0 62
advice 8 0 7
int 10 0 2
</table>
<tableCaption confidence="0.999946">
Table 1: Basic statistics on the DDI corpus.
</tableCaption>
<bodyText confidence="0.9998986">
more detailed description, the reader is directed to
our annotation guidelines.2
For evaluation, a part of the DDI corpus consist-
ing of 52 documents from DrugBank and 58 Med-
Line abstracts, is provided with the gold annota-
tion hidden. The goal for participating systems is to
recreate the gold annotation. Each participant sys-
tem must output an ASCII list of reported entities,
one per line, and formatted as:
IdSentence|startOffset-endOffset|text|type
Thus, for each recognized entity, each line must
contain the id of the sentence where this entity ap-
pears, the position of the first character and the one
of the last character of the entity in the sentence, the
text of the entity, and its type. When the entity is a
discontinuous name (eg. aluminum and magnesium
hydroxide), this second field must contain the start
and end positions of all parts of the entity separated
by semicolon. Multiple mentions from the same sen-
tence should appear on separate lines.
</bodyText>
<subsectionHeader confidence="0.998958">
3.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999867111111111">
This section describes the methodology that is used
to evaluate the performance of the participating sys-
tems in task 9.1.
The major forums of the Named Entity Recogni-
tion and Classification (NERC) research community
(such as MUC-7 (Chinchor and Robinson, 1997),
CoNLL 2003 (Tjong Kim Sang and De Meulder,
2003) or ACE07 have proposed several techniques
to assess the performance of NERC systems. While
</bodyText>
<footnote confidence="0.980986">
2http://www.cs.york.ac.uk/semeval-2013/task9/
</footnote>
<page confidence="0.99599">
342
</page>
<figureCaption confidence="0.999203">
Figure 1: Example of an annotated document of the DDI corpus.
</figureCaption>
<table confidence="0.954395266666667">
Team Affiliation Description
Task 9.1 LASIGE(Grego et al., 2013) University of Lisbon, Portugal Conditional random fields
NLM LHC National Library of Medicine, USA Dictionary-based approach
UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach
UMCC DLSI(Collazo et al., 2013) Matanzas University, Cuba j48 classifier
UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system)
WBI NER(Rockt¨aschel et al., 2013) Humboldt University of Berlin, Germany Conditional random fields
Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles
NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO)
SCAI(Bobi´c et al., 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR)
UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel
UCOLORADO SOM(Hailu et al., 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM)
UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system)
UWM-TRIADS(Rastegar-Mojarad et al., 2013) University of Wisconsin-Milwaukee, USA Two-stage SVM
WBI DDI(Thomas et al., 2013) Humboldt University of Berlin, Germany Ensemble of SVMs
</table>
<tableCaption confidence="0.999079">
Table 2: Short description of the teams.
</tableCaption>
<bodyText confidence="0.99449172">
ACE evaluation is very complex because its scores
are not intuitive, MUC and CoNLL 2003 used the
standard precision/recall/f-score metrics to compare
their participating systems. The main shared tasks in
the biomedical domain have continued using these
metrics to evaluate the outputs of their participant
teams.
System performance should be scored automat-
ically by how well the generated pharmacological
substance list corresponds to the gold-standard an-
notations. In our task, we evaluate the results of
the participating systems according to several evalu-
ation criteria. Firstly, we propose a strict evaluation,
which does not only demand exact boundary match,
but also requires that both mentions have the same
entity type. We are aware that this strict criterion
may be too restrictive for our overall goal (extrac-
tion of drug interactions) because it misses partial
matches, which can provide useful information for
a DDI extraction system. Our evaluation metrics
should score if a system is able to identify the ex-
act span of an entity (regardless of the type) and if
it is able to assign the correct entity type (regardless
of the boundaries). Thus, our evaluation script will
output four sets of scores according to:
</bodyText>
<listItem confidence="0.99828325">
1. Strict evaluation (exact-boundary and type
matching).
2. Exact boundary matching (regardless to the
type).
3. Partial boundary matching (regardless to the
type).
4. Type matching (some overlap between the
tagged entity and the gold entitity is required).
</listItem>
<bodyText confidence="0.9998612">
Evaluation results are reported using the standard
precision/recall/f-score metrics. We refer the reader
to (Chinchor and Sundheim, 1993) for a more de-
tailed description of these metrics.
These metrics are calculated over all entities and
on both axes (type and span) in order to evaluate
the performance of each axe separately. The final
score is the micro-averaged F-measure, which is cal-
culated over all entity types without distinction. The
main advantage of the micro-average F1 is that it
</bodyText>
<page confidence="0.995776">
343
</page>
<bodyText confidence="0.9993742">
takes into account all possible types of errors made
by a NERC system.
Additionally, we calculate precision, recall and f-
measure for each entity type and then their macro-
average measures are provided. Calculating these
metrics for each entity type allows us to evalu-
ate the level of difficulty of recognizing each en-
tity type. In addition to this, since not all entity
types have the same frequency, we can better as-
sess the performance of the algorithms proposed by
the participating systems. This is mainly because
the results achieved on the most frequent entity type
have a much greater impact on overall performance
than those obtained on the entity types with few in-
stances.
</bodyText>
<subsectionHeader confidence="0.981412">
3.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999900214285714">
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of two weeks to upload the results. A
total of 6 teams participated, submitting 16 system
runs. Table 2 lists the teams, their affiliations and
a brief description of their approaches. Due to the
lack of space we cannot describe them in this paper.
Tables 3, 4 and 5 show the F1 scores for each run in
alphabetic order. The reader can find the full ranking
information on the SemEval-2013 Task 9 website3.
The best results were achieved by the WBI
team with a conditional random field. They em-
ployed a domain-independent feature set along
with features generated from the output of
ChemSpot (Rockt¨aschel et al., 2012), an existing
chemical named entity recognition tool, as well as
a collection of domain-specific resources. Its model
was trained on the training dataset as well as on en-
tities of the test dataset for task 9.2. The second
top best performing team developed a dictionary-
based approach combining biomedical resources
such as DrugBank, the ATC classification system,4
or MeSH,5 among others. Regarding the classifi-
cation of each entity type, we observed that brand
drugs were easier to recognize than the other types.
This could be due to the fact that when a drug is mar-
keted by a pharmaceutical company, its brand name
is carefully selected to be short, unique and easy to
</bodyText>
<footnote confidence="0.9998">
3http://www.cs.york.ac.uk/semeval-2013/task9/
4http://www.whocc.no/atc ddd index/
5http://www.ncbi.nlm.nih.gov/mesh
</footnote>
<bodyText confidence="0.999893527777778">
remember (Boring, 1997). On the other hand, sub-
stances not approved for human use (drug-n) were
more difficult, due to the greater variation and com-
plexity in their naming. In fact, the UEM UC3M
team was the only team who obtained an F1 measure
greater than 0 on the DDI-DrugBank dataset. Also,
this may indicate that this type is less clearly defined
than the others in the annotation guidelines. Another
possible reason is that the presence of such sub-
stances in this dataset is very scarce (less than 1%).
It is interesting that almost every participating sys-
tem was better in detecting and classifying entities of
a particular class compared to all other systems. For
instance, on the whole dataset the dictionary-based
system from NLM LHC had it strengths at drug en-
tities, UEM UC3M at drug N entities, UTurku at
brand entities and WBI NER at group entities.
Finally, the results on the DDI-DrugBank dataset
are much better than those obtained on the DDI-
MedLine dataset. While DDI-DrugBank texts focus
on the description of drugs and their interactions, the
main topic of DDI-MedLine texts would not neces-
sarily be on DDIs. Coupled with this, it is not al-
ways trivial to distinguish between substances that
should be classified as pharmacological substances
and those who should not. This is due to the ambi-
guity of some pharmacological terms. For example,
insulin is a hormone produced by the pancreas, but
can also be synthesized in the laboratory and used
as drug to treat insulin-dependent diabetes mellitus.
The participating systems should be able to deter-
mine if the text is describing a substance originated
within the organism or, on the contrary, it describes a
process in which the substance is used for a specific
purpose and thus should be identified as pharmaco-
logical substance.
</bodyText>
<sectionHeader confidence="0.9461625" genericHeader="method">
4 Task 9.2: Extraction of drug-drug
interactions.
</sectionHeader>
<bodyText confidence="0.9999125">
The goal of this subtask is the extraction of drug-
drug interactions from biomedical texts. However,
while the previous DDIExtraction 2011 task focused
on the identification of all possible pairs of inter-
acting drugs, DDIExtraction 2013 also pursues the
classification of each drug-drug interaction accord-
ing to one of the following four types: advice, ef-
fect, mechanism, int. A detailed description of these
</bodyText>
<page confidence="0.989103">
344
</page>
<table confidence="0.999915266666667">
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
1 6 0,656 0,781 0,808 0,69 0,741 0,581 0,712 0,171 0,577
LASIGE 2 9 0,639 0,775 0,801 0,672 0,716 0,541 0,696 0,182 0,571
3 10 0,612 0,715 0,741 0,647 0,728 0,354 0,647 0,16 0,498
NLM LHC 1 4 0,698 0,784 0,801 0,722 0,803 0,809 0,646 0 0,57
2 3 0,704 0,792 0,807 0,726 0,81 0,846 0,643 0 0,581
UMCC DLSI 1,2,3 14,15,16 0,275 0,3049 0,367 0,334 0,297 0,313 0,257 0,124 0,311
UEM UC3M 1 13 0,458 0,528 0,585 0,51 0,718 0,075 0,291 0,185 0,351
2 12 0,529 0,609 0,669 0,589 0,752 0,094 0,291 0,264 0,38
1 11 0,579 0,639 0,719 0,701 0,721 0,603 0,478 0,016 0,468
UTurku 2 8 0,641 0,659 0,731 0,766 0,784 0,901 0,495 0,015 0,557
3 7 0,648 0,666 0,743 0,777 0,783 0,912 0,485 0,076 0,604
1 5 0,692 0,772 0,807 0,729 0,768 0,787 0,761 0,071 0,615
WBI 2 2 0,708 0,831 0,855 0,741 0,786 0,803 0,757 0,134 0,643
3 1 0,715 0,833 0,856 0,748 0,79 0,836 0,776 0,141 0,652
</table>
<tableCaption confidence="0.9914005">
Table 3: F1 scores for task 9.1 on the whole test dataset (DDI-MedLine + DDI-DrugBank). (MAVG for macro-
average). Each run is ranked by STRICT performance.
</tableCaption>
<table confidence="0.9999468">
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
1 8 0,771 0,834 0,855 0,799 0,817 0,571 0,833 0 0,563
LASIGE 2 9 0,771 0,831 0,852 0,799 0,823 0,553 0,824 0 0,568
3 11 0,682 0,744 0,764 0,713 0,757 0,314 0,756 0 0,47
NLM LHC 1 2 0,869 0,902 0,922 0,902 0,909 0,907 0,766 0 0,646
2 3 0,869 0,903 0,919 0,896 0,911 0,907 0,754 0 0,644
UMCC DLSI 1,2,3 14,15,16 0,424 0,4447 0,504 0,487 0,456 0,429 0,371 0 0,351
UEM UC3M 1 13 0,561 0,632 0,69 0,632 0,827 0,056 0,362 0,022 0,354
2 12 0,595 0,667 0,721 0,667 0,842 0,063 0,366 0,028 0,37
1 10 0,739 0,753 0,827 0,864 0,829 0,735 0,553 0 0,531
UTurku 2 6 0,785 0,795 0,863 0,908 0,858 0,898 0,559 0 0,581
3 7 0,781 0,787 0,858 0,905 0,847 0,911 0,551 0 0,578
1 5 0,86 0,877 0,9 0,89 0,905 0,857 0,782 0 0,636
WBI 2 4 0,868 0,894 0,914 0,897 0,909 0,865 0,794 0 0,642
3 1 0,878 0,901 0,917 0,908 0,912 0,904 0,806 0 0,656
</table>
<tableCaption confidence="0.977461">
Table 4: F1 scores for task 9.1 on the DDI-DrugBank test data. (MAVG for macro-average). Each run is ranked by
STRICT performance.
</tableCaption>
<table confidence="0.999956866666667">
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
1 4 0,567 0,74 0,772 0,605 0,678 0,667 0,612 0,183 0,577
LASIGE 2 8 0,54 0,733 0,763 0,576 0,631 0,444 0,595 0,196 0,512
3 6 0,557 0,693 0,723 0,596 0,702 0,667 0,56 0,171 0,554
NLM LHC 1 5 0,559 0,688 0,702 0,575 0,717 0,429 0,548 0 0,462
2 3 0,569 0,702 0,715 0,586 0,726 0,545 0,555 0 0,486
UMCC DLSI 1,2,3 14,15,16 0,187 0,2228 0,287 0,245 0,2 0,091 0,191 0,13 0,23
1 13 0,39 0,461 0,516 0,431 0,618 0,111 0,238 0,222 0,341
UEM UC3M 2 11 0,479 0,564 0,628 0,529 0,665 0,182 0,233 0,329 0,387
1 12 0,435 0,538 0,623 0,556 0,614 0,143 0,413 0,016 0,328
UTurku 2 10 0,502 0,528 0,604 0,628 0,703 0,923 0,436 0,016 0,533
3 9 0,522 0,551 0,634 0,656 0,716 0,923 0,426 0,08 0,582
1 7 0,545 0,681 0,726 0,589 0,634 0,353 0,744 0,074 0,479
WBI 2 2 0,576 0,779 0,807 0,612 0,673 0,444 0,729 0,14 0,534
3 1 0,581 0,778 0,805 0,617 0,678 0,444 0,753 0,147 0,537
</table>
<tableCaption confidence="0.996034">
Table 5: F1 scores for task 9.1 on the DDI-MedLine test data. (MAVG for macro-average). Each run is ranked by
STRICT performance.
</tableCaption>
<page confidence="0.997598">
345
</page>
<bodyText confidence="0.996525615384615">
types can be found in our annotation guidelines6.
Gold standard annotations (correct, human-
created annotations) of pharmacological substances
are provided to participants both for training and test
data. The test data for this subtask consists of 158
DrugBank documents and 33 MedLine abstracts.
Each participant system must output an ASCII list
including all pairs of drugs in each sentence, one per
line (multiple DDIs from the same sentence should
appear on separate lines), its prediction (1 if the pair
is a DDI and 0 otherwise) and its type (label null
when the prediction value is 0), and formatted as:
IdSentence|IdDrug1|IdDrug2|prediction|type
</bodyText>
<subsectionHeader confidence="0.980867">
4.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999833">
Evaluation is relation-oriented and based on the
standard precision, recall and F-score metrics. A
DDI is correctly detected only if the system is able
to assign the correct prediction label and the correct
type to it. In other words, a pair is correct only if
both prediction and type are correct. The perfor-
mance of systems to identify those pairs of drugs
interacting (regardless of the type) is also evaluated.
This allows us to assess the progress made with re-
gard to the previous edition, which only dealt with
the detection of DDIs.
Additionally, we are interested in assessing which
drug interaction types are most difficult to detect.
Thus, we calculate precision, recall and F1 for each
DDI type and then their macro-average measures are
provided. While micro-averaged F1 is calculated
by constructing a global contingency table and then
calculating precision and recall, macro-averaged F-
score is calculated by first calculating precision and
recall for each type and then taking the average of
these results.
Evaluating each DDI type separately allows us to
assess the level of difficulty of detecting and classi-
fying each type of interaction. Additionally, it is im-
portant to note that the scores achieved on the most
frequent DDI type have a much greater impact on
overall performance than those achieved on the DDI
types with few instances. Therefore, by calculating
scores for each type of DDI, we can better assess
the performance of the algorithms proposed by the
</bodyText>
<footnote confidence="0.856026">
6http://www.cs.york.ac.uk/semeval-2013/task9/
</footnote>
<bodyText confidence="0.817686">
participating systems.
</bodyText>
<subsectionHeader confidence="0.83823">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999982190476191">
The task of extracting drug-drug interactions from
biomedical texts has attracted the participation of 8
teams (see Table 2) who submitted 22 runs. Tables 6,
7 and 8 show the results for each run in alphabetic
order. Due to the lack of space, the performance
information is only shown in terms of F1 score. The
reader can find the full ranking information on the
SemEval-2013 Task 9 website7.
Most of the participating systems were built on
support vector machines. In general, approaches
based on non-linear kernels methods achieved better
results than linear SVMs. As in the previous edition
of DDIExtraction, most systems have used primarily
syntactic information. However, semantic informa-
tion has been poorly used.
The best results were submitted by the team from
FBK-irst. They applied a novel hybrid kernel based
RE approach described in Chowdhury (2013a).
They also exploited the scope of negations and
semantic roles for negative instance filtering as
proposed in (Chowdhury and Lavelli, 2013b) and
(Chowdhury and Lavelli, 2012). The second best
results were obtained by the WBI team from the
Humboldt University of Berlin. Its system com-
bines several kernel methods (APG (Airola et al.,
2008) and Shallow Linguistic Kernel (SL) (Giuliano
et al., 2006) among others), the Turku Event Ex-
traction system (TEES) (Bj¨orne et al., 2011)8 and
the Moara system (Neves et al., 2009). These two
teams were also the top two ranked teams in DDIEx-
traction 2011. For a more detailed description, the
reader is encouraged to read the papers of the partic-
ipants in the proceedings book.
While the DDIExtraction 2011 shared task con-
centrated efforts on the detection of DDIs, this new
DDIExtraction 2013 task involved not only the de-
tection of DDIs, but also their classification. Al-
though the results of DDIExtraction 2011 are not di-
rectly comparable with the ones reported in DDIEx-
traction 2013 due to the use of different training and
test datasets in each edition, it should be noted that
there has been a significant improvement in the de-
</bodyText>
<footnote confidence="0.9999145">
7http://www.cs.york.ac.uk/semeval-2013/task9/
8http://jbjorne.github.io/TEES/
</footnote>
<page confidence="0.995136">
346
</page>
<table confidence="0.999868260869565">
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
1 3 0.638 0.8 0.679 0.662 0.692 0.363 0.602
FBK-irst 2 1 0.651 0.8 0.679 0.628 0.692 0.547 0.648
3 2 0.648 0.8 0.627 0.662 0.692 0.547 0.644
NIL UCM 1 12 0.517 0.588 0.515 0.489 0.613 0.427 0.535
2 10 0.548 0.656 0.531 0.556 0.61 0.393 0.526
1 14 0.46 0.69 0.446 0.459 0.562 0.02 0.423
SCAI 2 16 0.452 0.683 0.441 0.44 0.559 0.021 0.448
3 15 0.458 0.704 0.45 0.462 0.54 0.02 0.411
UC3M 1 11 0.529 0.676 0.48 0.547 0.575 0.5 0.534
2 21 0.294 0.537 0.268 0.286 0.325 0.402 0.335
1 22 0.214 0.492 0.109 0.25 0.219 0.097 0.215
UCOLORADO SOM 2 20 0.334 0.504 0.361 0.311 0.381 0.333 0.407
3 19 0.336 0.491 0.335 0.313 0.42 0.329 0.38
1 9 0.581 0.684 0.578 0.585 0.606 0.503 0.572
UTurku 2 7 0.594 0.696 0.582 0.6 0.63 0.507 0.587
3 8 0.582 0.699 0.569 0.593 0.608 0.511 0.577
1 17 0.449 0.581 0.413 0.446 0.502 0.397 0.451
UWM-TRIADS 2 13 0.47 0.599 0.446 0.449 0.532 0.421 0.472
3 18 0.432 0.564 0.442 0.383 0.537 0.292 0.444
1 6 0.599 0.736 0.602 0.604 0.618 0.516 0.588
WBI 2 5 0.601 0.745 0.616 0.595 0.637 0.49 0.588
3 4 0.609 0.759 0.618 0.61 0.632 0.51 0.597
</table>
<tableCaption confidence="0.997506666666667">
Table 6: F1 scores for Task 9.2 on the whole test dataset (DDI-MedLine + DDI-DrugBank). DEC for Detection, CLA
for detection and classification, MEC for mechanism type, EFF for effect type, ADV for advice type, INT for int type
and MAVG for macro-average. Each run is ranked by CLA performance.
</tableCaption>
<table confidence="0.999924043478261">
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
1 3 0.663 0.827 0.705 0.699 0.705 0.376 0.624
FBK-irst 2 1 0.676 0.827 0.705 0.664 0.705 0.545 0.672
3 2 0.673 0.827 0.655 0.699 0.705 0.545 0.667
NIL UCM 1 12 0.54 0.615 0.527 0.525 0.625 0.444 0.565
2 10 0.573 0.68 0.552 0.597 0.619 0.408 0.55
1 15 0.464 0.711 0.449 0.459 0.57 0.021 0.461
SCAI 2 16 0.463 0.71 0.445 0.458 0.569 0.021 0.46
3 14 0.473 0.734 0.468 0.482 0.551 0.021 0.439
UC3M 1 11 0.555 0.703 0.493 0.593 0.59 0.51 0.561
2 21 0.306 0.549 0.274 0.302 0.334 0.426 0.352
1 22 0.218 0.508 0.115 0.251 0.24 0.098 0.228
UCOLORADO SOM 2 20 0.341 0.518 0.373 0.313 0.398 0.344 0.425
3 19 0.349 0.511 0.353 0.324 0.429 0.327 0.394
1 8 0.608 0.712 0.6 0.63 0.617 0.522 0.6
UTurku 2 7 0.62 0.724 0.605 0.644 0.638 0.522 0.614
3 9 0.608 0.726 0.591 0.635 0.617 0.522 0.601
1 17 0.462 0.596 0.43 0.459 0.509 0.405 0.463
UWM-TRIADS 2 13 0.485 0.616 0.467 0.466 0.536 0.425 0.486
3 18 0.445 0.573 0.469 0.39 0.544 0.29 0.46
1 6 0.624 0.762 0.621 0.645 0.634 0.52 0.61
WBI 2 5 0.627 0.775 0.636 0.636 0.652 0.5 0.611
3 4 0.632 0.783 0.629 0.652 0.65 0.513 0.617
</table>
<tableCaption confidence="0.971076">
Table 7: F1 scores for task 9.2 on the DDI-DrugBank test dataset. Each run is ranked by CLA performance.
</tableCaption>
<table confidence="0.999939260869565">
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
1 4 0.387 0.53 0.383 0.436 0.286 0.211 0.406
FBK-irst 2 3 0.398 0.53 0.383 0.407 0.286 0.571 0.436
3 2 0.398 0.53 0.339 0.436 0.286 0.571 0.44
NIL UCM 1 20 0.19 0.206 0.286 0.186 0 0 0.121
2 19 0.219 0.336 0.143 0.271 0 0 0.11
1 1 0.42 0.462 0.412 0.458 0.2 0 0.269
SCAI 2 8 0.323 0.369 0.389 0.333 0 0 0.182
3 6 0.341 0.474 0.31 0.379 0.222 0 0.229
1 15 0.274 0.406 0.333 0.267 0 0.364 0.268
UC3M 2 22 0.186 0.421 0.222 0.171 0.143 0 0.149
1 21 0.188 0.37 0.042 0.241 0 0 0.073
UCOLORADO SOM 2 14 0.275 0.394 0.258 0.302 0.138 0 0.177
3 17 0.244 0.356 0.194 0.255 0.222 0.4 0.272
1 18 0.242 0.339 0.258 0.256 0.2 0 0.18
UTurku 2 16 0.262 0.344 0.214 0.278 0.364 0 0.224
3 13 0.286 0.376 0.286 0.289 0.333 0 0.232
1 10 0.312 0.419 0.233 0.36 0.267 0 0.219
UWM-TRIADS 2 9 0.319 0.436 0.233 0.34 0.421 0.333 0.345
3 11 0.306 0.479 0.247 0.326 0.381 0.333 0.33
1 7 0.336 0.456 0.368 0.344 0.154 0.4 0.334
WBI 2 12 0.304 0.406 0.343 0.318 0.167 0 0.209
3 5 0.365 0.503 0.476 0.347 0.143 0.4 0.353
</table>
<tableCaption confidence="0.999338">
Table 8: F1 scores for task 9.2 on the DDI-MedLine test dataset. Each run is ranked by CLA performance.
</tableCaption>
<page confidence="0.997583">
347
</page>
<bodyText confidence="0.999885027777778">
tection of DDIs: F1 has a remarkable increase from
65.74% (the best F1-score in DDIExtraction 2011)
to 80% (see DEC column of Table 6). The increase
of the size of the corpus made for DDIExtraction
2013 and of the quality of their annotations may
have contributed significantly to this improvement.
However, the results for the detection and classifi-
cation for DDIs did not exceed an F1 of 65.1%. Ta-
ble 6 suggests that some type of DDIs are more diffi-
cult to classify than others. The best F1 ranges from
69.2% for advice to 54.7% for int. One possible ex-
planation for this could be that recommendations or
advice regarding a drug interaction are typically de-
scribed by very similar text patterns such as DRUG
should not be used in combination with DRUG or
Caution should be observed when DRUG is admin-
istered with DRUG.
Regarding results for the int relationship, it should
be noted that the proportion of instances of this re-
lationship (5.6%) in the DDI corpus is much smaller
than those of the rest of the relations (41.1% for ef-
fect, 32.3% for mechanism and 20.9% for advice).
As stated earlier, one of the differences from
the previous edition is that the corpus developed
for DDIExtraction 2013 is made up of texts from
two different sources: MedLine and the DrugBank
database. Thus, the different approaches can be
evaluated on two different styles of biomedical texts.
While MedLine abstracts are usually written in ex-
tremely scientific language, texts from DrugBank
are written in a less technical form of the language
(similar to the language used in package inserts). In-
deed, this may be the reason why the results on the
DDI-DrugBank dataset are much better than those
obtained on the DDI-MedLine dataset (see Tables 7
and 8).
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999986102564102">
The DDIExtraction 2011 task concentrated efforts
on the novel aspects of the DDI extraction task, the
drug recognition was assumed and the annotations
for drugs were provided to the participants. This
new DDIExtraction 2013 task pursues the detec-
tion and classification of drug interactions as well
as the recognition and classification of pharmaco-
logical substances. The task attracted broad interest
from the community. A total of 14 teams from 7 dif-
ferent countries participated, submitted a total of 38
runs, exceeding the participation of DDIExtraction
2011 (10 teams). The participating systems demon-
strated substantial progress at the established DDI
extraction task on DrugBank texts and showed that
their methods also obtain good results for MedLine
abstracts.
The results that the participating systems have re-
ported show successful approaches to this difficult
task, and the advantages of non-linear kernel-based
methods over linear SVMs for extraction of DDIs.
In the named entity task, the participating systems
perform well in recognizing generic drugs, brand
drugs and groups of drugs, but they fail in recogniz-
ing active substances not approved for human use.
Although the results are positive, there is still much
room to improve in both subtasks. We have ac-
complished our goal of providing a framework and
a benchmark data set to allow for comparisons of
methods for the recognition of pharmacological sub-
stances and detection and classification of drug-drug
interactions from biomedical texts.
We would like that our test dataset can still serve
as the basis for fair and stable evaluation after the
task. Thus, we have decided that the full gold an-
notations for the test data are not available for the
moment. We plan to make available a web service
where researchers can test their methods on the test
dataset and compare their results with the DDIEx-
traction 2013 task participants.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999942625">
This research work has been supported by the Re-
gional Government of Madrid under the Research
Network MA2VICMR (S2009/TIC-1542), by the
Spanish Ministry of Education under the project
MULTIMEDICA (TIN2010-20644-C03-01). Addi-
tionally, we would like to thank all participants for
their efforts and to congratulate them to their inter-
esting work.
</bodyText>
<sectionHeader confidence="0.992591" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.6647634">
A. Airola, S. Pyysalo, J. Bjorne, T. Pahikkala, F. Gin-
ter, and T. Salakoski. 2008. All-paths graph kernel
for protein-protein interaction extraction with evalu-
ation of cross-corpus learning. BMC bioinformatics,
9(Suppl 11):S2.
</reference>
<page confidence="0.993843">
348
</page>
<reference confidence="0.999463076190476">
JK. Aronson. 2007. Communicating information about
drug interactions. British Journal of Clinical Pharma-
cology, 63(6):637–639, June.
K. Baxter and I.H. Stockely. 2010. Stockley’s drug inter-
actions.8th ed. London:Pharmaceutical Press.
J. Bj¨orne, J. Heimonen, F. Ginter, A. Airola, T. Pahikkala,
and T. Salakoski. 2011. Extracting contextualized
complex biological events with graph-based feature
sets. Computational Intelligence, 27(4):541–557.
J. Bj¨orne, S. Kaewphan, and T. Salakoski. 2013.
UTurku: Drug Named Entity Detection and Drug-drug
Interaction Extraction Using SVM Classification and
Domain Knowledge. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
T. Bobi´c, J. Fluck, and M. Hofmann-Apitius. 2013.
SCAI: Extracting drug-drug interactions using a rich
feature vector. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
A. Bokharaeian, B.and D´ıaz. 2013. NIL UCM: Extract-
ing Drug-Drug interactions from text through combi-
nation of sequence and tree kernels. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
D. Boring. 1997. The development and adop-
tion of nonproprietary, established, and proprietary
names for pharmaceuticals. Drug information journal,
31(3):621–634.
N. Chinchor and P. Robinson. 1997. Muc-7 named entity
task definition. In Proceedings of the 7th Conference
on Message Understanding.
N. Chinchor and B. Sundheim. 1993. Muc-5 evalua-
tion metrics. In Proceedings of the 5th conference on
Message understanding, pages 69–78. Association for
Computational Linguistics.
MFM. Chowdhury and A. Lavelli. 2012. Impact of
less skewed distributions on efficiency and effective-
ness of biomedical relation extraction. In Proceedings
of COLING 2012.
MFM. Chowdhury and A. Lavelli. 2013b. Exploiting
the scope of negations and heterogeneous features for
relation extraction: Case study drug-drug interaction
extraction. In Proceedings of NAACL 2013.
M.F.M. Chowdhury and A. Lavelli. 2013c. FBK-irst
: A Multi-Phase Kernel Based Approach for Drug-
Drug Interaction Detection and Classification that Ex-
ploits Linguistic Information. In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013).
MFM. Chowdhury. 2013a. Improving the Effectiveness
of Information Extraction from Biomedical Text. Ph.d.
dissertation, University of Trento.
A. Collazo, A. Ceballo, D Puig, Y. Guti´errez, J. Abreu,
J P´erez, A. Fern´andez-Orqu´ın, A. Montoyo, R. Mu˜noz,
and F. Camara. 2013. UMCC DLSI-(DDI): Seman-
tic and Lexical features for detection and classification
Drugs in biomedical texts. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
C. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting shallow linguistic information for relation ex-
traction from biomedical literature. In Proceedings of
the Eleventh Conference of the European Chapter of
the Association for Computational Linguistics (EACL-
2006), pages 401–408.
T. Grego, F. Pinto, and F.M. Couto. 2013. LASIGE: us-
ing Conditional Random Fields and ChEBI ontology.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
N.D. Hailu, L.E. Hunter, and K.B. Cohen. 2013.
UColorado SOM: Extraction of Drug-Drug Interac-
tions from Biomedical Text using Knowledge-rich and
Knowledge-poor Features. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
ML. Neves, JM. Carazo, and A. Pascual-Montano. 2009.
Extraction of biomedical events using case-based rea-
soning. In Proceedings of the Workshop on BioNLP:
Shared Task, pages 68–76. Association for Computa-
tional Linguistics.
S. Pyysalo, A. Airola, J. Heimonen, J. Bjorne, F. Gin-
ter, and T. Salakoski. 2008. Comparative analysis of
five protein-protein interaction corpora. BMC bioin-
formatics, 9(Suppl 3):S6.
M. Rastegar-Mojarad, R. D. Boyce, and R. Prasad. 2013.
UWM-TRIADS: Classifying Drug-Drug Interactions
with Two-Stage SVM and Post-Processing. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013).
T. Rockt¨aschel, M. Weidlich, and U. Leser. 2012.
Chemspot: a hybrid system for chemical named entity
recognition. Bioinformatics, 28(12):1633–1640.
T. Rockt¨aschel, T. Huber, M. Weidlich, and U. Leser.
2013. WBI-NER: The impact of domain-specific fea-
tures on the performance of identifying and classifying
mentions of drugs. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
D. Sanchez-Cisneros and F. Aparicio. 2013. UEM-
UC3M: An Ontology-based named entity recognition
system for biomedical texts. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
D. Sanchez-Cisneros. 2013. UC3M: A kernel-based ap-
proach for identify and classify DDIs in biomedical
</reference>
<page confidence="0.990737">
349
</page>
<reference confidence="0.999556555555556">
texts. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
I. Segura-Bedmar, P. Martinez, and C. de Pablo-S´anchez.
2011a. Using a shallow linguistic kernel for drug-drug
interaction extraction. Journal of Biomedical Infor-
matics, 44(5):789 – 804.
I. Segura-Bedmar, P. Martınez, and D. S´anchez-Cisneros.
2011b. The 1st ddiextraction-2011 challenge task:
Extraction of drug-drug interactions from biomedical
texts. In Proceedings of DDIExtraction-2011 chal-
lenge task, pages 1–9.
P. Thomas, M. Neves, T. Rockt¨aschel, and U. Leser.
2013. WBI-DDI: Drug-Drug Interaction Extraction
using Majority Voting. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
E.F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003-Volume 4, pages 142–147.
Association for Computational Linguistics.
D.S. Wishart, C. Knox, A.C. Guo, S. Shrivastava,
M. Hassanali, P. Stothard, Z. Chang, and J. Woolsey.
2006. Drugbank: a comprehensive resource for in sil-
ico drug discovery and exploration. Nucleic acids re-
search, 34(suppl 1):D668–D672.
</reference>
<page confidence="0.997671">
350
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579697">
<note confidence="0.8513675">SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013) Isabel Segura-Bedmar, Paloma Martinez, Maria Universidad Carlos III de Av. Universidad, 30, Legan´es 28911, mhzazo@pa.uc3m.es</note>
<abstract confidence="0.997460285714286">The DDIExtraction 2013 task concerns the recognition of drugs and extraction of drugdrug interactions that appear in biomedical literature. We propose two subtasks for the DDIExtraction 2013 Shared Task challenge: 1) the recognition and classification of drug names and 2) the extraction and classification of their interactions. Both subtasks have been very successful in participation and results. There were 14 teams who submitted a total of 38 runs. The best result reported for the first subtask was F1 of 71.5% and 65.1% for the second one.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Airola</author>
<author>S Pyysalo</author>
<author>J Bjorne</author>
<author>T Pahikkala</author>
<author>F Ginter</author>
<author>T Salakoski</author>
</authors>
<title>All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning. BMC bioinformatics, 9(Suppl 11):S2.</title>
<date>2008</date>
<contexts>
<context position="22164" citStr="Airola et al., 2008" startWordPosition="3589" endWordPosition="3592">ous edition of DDIExtraction, most systems have used primarily syntactic information. However, semantic information has been poorly used. The best results were submitted by the team from FBK-irst. They applied a novel hybrid kernel based RE approach described in Chowdhury (2013a). They also exploited the scope of negations and semantic roles for negative instance filtering as proposed in (Chowdhury and Lavelli, 2013b) and (Chowdhury and Lavelli, 2012). The second best results were obtained by the WBI team from the Humboldt University of Berlin. Its system combines several kernel methods (APG (Airola et al., 2008) and Shallow Linguistic Kernel (SL) (Giuliano et al., 2006) among others), the Turku Event Extraction system (TEES) (Bj¨orne et al., 2011)8 and the Moara system (Neves et al., 2009). These two teams were also the top two ranked teams in DDIExtraction 2011. For a more detailed description, the reader is encouraged to read the papers of the participants in the proceedings book. While the DDIExtraction 2011 shared task concentrated efforts on the detection of DDIs, this new DDIExtraction 2013 task involved not only the detection of DDIs, but also their classification. Although the results of DDIE</context>
</contexts>
<marker>Airola, Pyysalo, Bjorne, Pahikkala, Ginter, Salakoski, 2008</marker>
<rawString>A. Airola, S. Pyysalo, J. Bjorne, T. Pahikkala, F. Ginter, and T. Salakoski. 2008. All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning. BMC bioinformatics, 9(Suppl 11):S2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aronson</author>
</authors>
<title>Communicating information about drug interactions.</title>
<date>2007</date>
<journal>British Journal of Clinical Pharmacology,</journal>
<volume>63</volume>
<issue>6</issue>
<contexts>
<context position="1314" citStr="Aronson, 2007" startWordPosition="193" endWordPosition="194"> 14 teams who submitted a total of 38 runs. The best result reported for the first subtask was F1 of 71.5% and 65.1% for the second one. 1 Introduction The definition of drug-drug interaction (DDI) is broadly described as a change in the effects of one drug by the presence of another drug (Baxter and Stockely, 2010). The detection of DDIs is an important research area in patient safety since these interactions can become very dangerous and increase health care costs. Drug interactions are frequently reported in journals, making medical literature the most effective source for their detection (Aronson, 2007). Therefore, Information Extraction (IE) can be of great benefit in the pharmaceutical industry allowing identification and extraction of relevant information on DDIs and providing an interesting way of reducing the time spent by health care professionals on reviewing the literature. The DDIExtraction 2013 follows up on a first event organized in 2011, DDIExtraction 2011 (Segura-Bedmar et al., 2011b) whose main goal was the detection of drug-drug interactions from biomedical texts. The new edition includes in addition to DDI extraction also a supporting task, the recognition and classification</context>
</contexts>
<marker>Aronson, 2007</marker>
<rawString>JK. Aronson. 2007. Communicating information about drug interactions. British Journal of Clinical Pharmacology, 63(6):637–639, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Baxter</author>
<author>I H Stockely</author>
</authors>
<date>2010</date>
<booktitle>Stockley’s drug interactions.8th</booktitle>
<editor>ed.</editor>
<publisher>London:Pharmaceutical Press.</publisher>
<contexts>
<context position="1017" citStr="Baxter and Stockely, 2010" startWordPosition="146" endWordPosition="149"> that appear in biomedical literature. We propose two subtasks for the DDIExtraction 2013 Shared Task challenge: 1) the recognition and classification of drug names and 2) the extraction and classification of their interactions. Both subtasks have been very successful in participation and results. There were 14 teams who submitted a total of 38 runs. The best result reported for the first subtask was F1 of 71.5% and 65.1% for the second one. 1 Introduction The definition of drug-drug interaction (DDI) is broadly described as a change in the effects of one drug by the presence of another drug (Baxter and Stockely, 2010). The detection of DDIs is an important research area in patient safety since these interactions can become very dangerous and increase health care costs. Drug interactions are frequently reported in journals, making medical literature the most effective source for their detection (Aronson, 2007). Therefore, Information Extraction (IE) can be of great benefit in the pharmaceutical industry allowing identification and extraction of relevant information on DDIs and providing an interesting way of reducing the time spent by health care professionals on reviewing the literature. The DDIExtraction </context>
</contexts>
<marker>Baxter, Stockely, 2010</marker>
<rawString>K. Baxter and I.H. Stockely. 2010. Stockley’s drug interactions.8th ed. London:Pharmaceutical Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bj¨orne</author>
<author>J Heimonen</author>
<author>F Ginter</author>
<author>A Airola</author>
<author>T Pahikkala</author>
<author>T Salakoski</author>
</authors>
<title>Extracting contextualized complex biological events with graph-based feature sets.</title>
<date>2011</date>
<journal>Computational Intelligence,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Bj¨orne, Heimonen, Ginter, Airola, Pahikkala, Salakoski, 2011</marker>
<rawString>J. Bj¨orne, J. Heimonen, F. Ginter, A. Airola, T. Pahikkala, and T. Salakoski. 2011. Extracting contextualized complex biological events with graph-based feature sets. Computational Intelligence, 27(4):541–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bj¨orne</author>
<author>S Kaewphan</author>
<author>T Salakoski</author>
</authors>
<title>UTurku: Drug Named Entity Detection and Drug-drug Interaction Extraction Using SVM Classification and Domain Knowledge.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Bj¨orne, Kaewphan, Salakoski, 2013</marker>
<rawString>J. Bj¨orne, S. Kaewphan, and T. Salakoski. 2013. UTurku: Drug Named Entity Detection and Drug-drug Interaction Extraction Using SVM Classification and Domain Knowledge. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bobi´c</author>
<author>J Fluck</author>
<author>M Hofmann-Apitius</author>
</authors>
<title>SCAI: Extracting drug-drug interactions using a rich feature vector.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Bobi´c, Fluck, Hofmann-Apitius, 2013</marker>
<rawString>T. Bobi´c, J. Fluck, and M. Hofmann-Apitius. 2013. SCAI: Extracting drug-drug interactions using a rich feature vector. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bokharaeian</author>
<author>B and D´ıaz</author>
</authors>
<title>NIL UCM: Extracting Drug-Drug interactions from text through combination of sequence and tree kernels.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Bokharaeian, D´ıaz, 2013</marker>
<rawString>A. Bokharaeian, B.and D´ıaz. 2013. NIL UCM: Extracting Drug-Drug interactions from text through combination of sequence and tree kernels. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Boring</author>
</authors>
<title>The development and adoption of nonproprietary, established, and proprietary names for pharmaceuticals. Drug information journal,</title>
<date>1997</date>
<pages>31--3</pages>
<contexts>
<context position="13313" citStr="Boring, 1997" startWordPosition="2077" endWordPosition="2078">or task 9.2. The second top best performing team developed a dictionarybased approach combining biomedical resources such as DrugBank, the ATC classification system,4 or MeSH,5 among others. Regarding the classification of each entity type, we observed that brand drugs were easier to recognize than the other types. This could be due to the fact that when a drug is marketed by a pharmaceutical company, its brand name is carefully selected to be short, unique and easy to 3http://www.cs.york.ac.uk/semeval-2013/task9/ 4http://www.whocc.no/atc ddd index/ 5http://www.ncbi.nlm.nih.gov/mesh remember (Boring, 1997). On the other hand, substances not approved for human use (drug-n) were more difficult, due to the greater variation and complexity in their naming. In fact, the UEM UC3M team was the only team who obtained an F1 measure greater than 0 on the DDI-DrugBank dataset. Also, this may indicate that this type is less clearly defined than the others in the annotation guidelines. Another possible reason is that the presence of such substances in this dataset is very scarce (less than 1%). It is interesting that almost every participating system was better in detecting and classifying entities of a par</context>
</contexts>
<marker>Boring, 1997</marker>
<rawString>D. Boring. 1997. The development and adoption of nonproprietary, established, and proprietary names for pharmaceuticals. Drug information journal, 31(3):621–634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
<author>P Robinson</author>
</authors>
<title>Muc-7 named entity task definition.</title>
<date>1997</date>
<booktitle>In Proceedings of the 7th Conference on Message Understanding.</booktitle>
<contexts>
<context position="7448" citStr="Chinchor and Robinson, 1997" startWordPosition="1178" endWordPosition="1181">t character of the entity in the sentence, the text of the entity, and its type. When the entity is a discontinuous name (eg. aluminum and magnesium hydroxide), this second field must contain the start and end positions of all parts of the entity separated by semicolon. Multiple mentions from the same sentence should appear on separate lines. 3.1 Evaluation Metrics This section describes the methodology that is used to evaluate the performance of the participating systems in task 9.1. The major forums of the Named Entity Recognition and Classification (NERC) research community (such as MUC-7 (Chinchor and Robinson, 1997), CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) or ACE07 have proposed several techniques to assess the performance of NERC systems. While 2http://www.cs.york.ac.uk/semeval-2013/task9/ 342 Figure 1: Example of an annotated document of the DDI corpus. Team Affiliation Description Task 9.1 LASIGE(Grego et al., 2013) University of Lisbon, Portugal Conditional random fields NLM LHC National Library of Medicine, USA Dictionary-based approach UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach UMCC DLSI(Collazo et al.</context>
</contexts>
<marker>Chinchor, Robinson, 1997</marker>
<rawString>N. Chinchor and P. Robinson. 1997. Muc-7 named entity task definition. In Proceedings of the 7th Conference on Message Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
<author>B Sundheim</author>
</authors>
<title>Muc-5 evaluation metrics.</title>
<date>1993</date>
<booktitle>In Proceedings of the 5th conference on Message understanding,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10702" citStr="Chinchor and Sundheim, 1993" startWordPosition="1649" endWordPosition="1652">m is able to identify the exact span of an entity (regardless of the type) and if it is able to assign the correct entity type (regardless of the boundaries). Thus, our evaluation script will output four sets of scores according to: 1. Strict evaluation (exact-boundary and type matching). 2. Exact boundary matching (regardless to the type). 3. Partial boundary matching (regardless to the type). 4. Type matching (some overlap between the tagged entity and the gold entitity is required). Evaluation results are reported using the standard precision/recall/f-score metrics. We refer the reader to (Chinchor and Sundheim, 1993) for a more detailed description of these metrics. These metrics are calculated over all entities and on both axes (type and span) in order to evaluate the performance of each axe separately. The final score is the micro-averaged F-measure, which is calculated over all entity types without distinction. The main advantage of the micro-average F1 is that it 343 takes into account all possible types of errors made by a NERC system. Additionally, we calculate precision, recall and fmeasure for each entity type and then their macroaverage measures are provided. Calculating these metrics for each en</context>
</contexts>
<marker>Chinchor, Sundheim, 1993</marker>
<rawString>N. Chinchor and B. Sundheim. 1993. Muc-5 evaluation metrics. In Proceedings of the 5th conference on Message understanding, pages 69–78. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chowdhury</author>
<author>A Lavelli</author>
</authors>
<title>Impact of less skewed distributions on efficiency and effectiveness of biomedical relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="21999" citStr="Chowdhury and Lavelli, 2012" startWordPosition="3561" endWordPosition="3564">icipating systems were built on support vector machines. In general, approaches based on non-linear kernels methods achieved better results than linear SVMs. As in the previous edition of DDIExtraction, most systems have used primarily syntactic information. However, semantic information has been poorly used. The best results were submitted by the team from FBK-irst. They applied a novel hybrid kernel based RE approach described in Chowdhury (2013a). They also exploited the scope of negations and semantic roles for negative instance filtering as proposed in (Chowdhury and Lavelli, 2013b) and (Chowdhury and Lavelli, 2012). The second best results were obtained by the WBI team from the Humboldt University of Berlin. Its system combines several kernel methods (APG (Airola et al., 2008) and Shallow Linguistic Kernel (SL) (Giuliano et al., 2006) among others), the Turku Event Extraction system (TEES) (Bj¨orne et al., 2011)8 and the Moara system (Neves et al., 2009). These two teams were also the top two ranked teams in DDIExtraction 2011. For a more detailed description, the reader is encouraged to read the papers of the participants in the proceedings book. While the DDIExtraction 2011 shared task concentrated ef</context>
</contexts>
<marker>Chowdhury, Lavelli, 2012</marker>
<rawString>MFM. Chowdhury and A. Lavelli. 2012. Impact of less skewed distributions on efficiency and effectiveness of biomedical relation extraction. In Proceedings of COLING 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chowdhury</author>
<author>A Lavelli</author>
</authors>
<title>Exploiting the scope of negations and heterogeneous features for relation extraction: Case study drug-drug interaction extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="8330" citStr="Chowdhury and Lavelli, 2013" startWordPosition="1298" endWordPosition="1301">eam Affiliation Description Task 9.1 LASIGE(Grego et al., 2013) University of Lisbon, Portugal Conditional random fields NLM LHC National Library of Medicine, USA Dictionary-based approach UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach UMCC DLSI(Collazo et al., 2013) Matanzas University, Cuba j48 classifier UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) WBI NER(Rockt¨aschel et al., 2013) Humboldt University of Berlin, Germany Conditional random fields Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO) SCAI(Bobi´c et al., 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR) UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel UCOLORADO SOM(Hailu et al., 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM) UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) UWM-TRIADS(Rastegar-Mojarad et al., 2013) University of Wisconsin-Milw</context>
<context position="21963" citStr="Chowdhury and Lavelli, 2013" startWordPosition="3556" endWordPosition="3559">3 Task 9 website7. Most of the participating systems were built on support vector machines. In general, approaches based on non-linear kernels methods achieved better results than linear SVMs. As in the previous edition of DDIExtraction, most systems have used primarily syntactic information. However, semantic information has been poorly used. The best results were submitted by the team from FBK-irst. They applied a novel hybrid kernel based RE approach described in Chowdhury (2013a). They also exploited the scope of negations and semantic roles for negative instance filtering as proposed in (Chowdhury and Lavelli, 2013b) and (Chowdhury and Lavelli, 2012). The second best results were obtained by the WBI team from the Humboldt University of Berlin. Its system combines several kernel methods (APG (Airola et al., 2008) and Shallow Linguistic Kernel (SL) (Giuliano et al., 2006) among others), the Turku Event Extraction system (TEES) (Bj¨orne et al., 2011)8 and the Moara system (Neves et al., 2009). These two teams were also the top two ranked teams in DDIExtraction 2011. For a more detailed description, the reader is encouraged to read the papers of the participants in the proceedings book. While the DDIExtract</context>
</contexts>
<marker>Chowdhury, Lavelli, 2013</marker>
<rawString>MFM. Chowdhury and A. Lavelli. 2013b. Exploiting the scope of negations and heterogeneous features for relation extraction: Case study drug-drug interaction extraction. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F M Chowdhury</author>
<author>A Lavelli</author>
</authors>
<title>FBK-irst : A Multi-Phase Kernel Based Approach for DrugDrug Interaction Detection and Classification that Exploits Linguistic Information.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="8330" citStr="Chowdhury and Lavelli, 2013" startWordPosition="1298" endWordPosition="1301">eam Affiliation Description Task 9.1 LASIGE(Grego et al., 2013) University of Lisbon, Portugal Conditional random fields NLM LHC National Library of Medicine, USA Dictionary-based approach UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach UMCC DLSI(Collazo et al., 2013) Matanzas University, Cuba j48 classifier UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) WBI NER(Rockt¨aschel et al., 2013) Humboldt University of Berlin, Germany Conditional random fields Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO) SCAI(Bobi´c et al., 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR) UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel UCOLORADO SOM(Hailu et al., 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM) UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) UWM-TRIADS(Rastegar-Mojarad et al., 2013) University of Wisconsin-Milw</context>
<context position="21963" citStr="Chowdhury and Lavelli, 2013" startWordPosition="3556" endWordPosition="3559">3 Task 9 website7. Most of the participating systems were built on support vector machines. In general, approaches based on non-linear kernels methods achieved better results than linear SVMs. As in the previous edition of DDIExtraction, most systems have used primarily syntactic information. However, semantic information has been poorly used. The best results were submitted by the team from FBK-irst. They applied a novel hybrid kernel based RE approach described in Chowdhury (2013a). They also exploited the scope of negations and semantic roles for negative instance filtering as proposed in (Chowdhury and Lavelli, 2013b) and (Chowdhury and Lavelli, 2012). The second best results were obtained by the WBI team from the Humboldt University of Berlin. Its system combines several kernel methods (APG (Airola et al., 2008) and Shallow Linguistic Kernel (SL) (Giuliano et al., 2006) among others), the Turku Event Extraction system (TEES) (Bj¨orne et al., 2011)8 and the Moara system (Neves et al., 2009). These two teams were also the top two ranked teams in DDIExtraction 2011. For a more detailed description, the reader is encouraged to read the papers of the participants in the proceedings book. While the DDIExtract</context>
</contexts>
<marker>Chowdhury, Lavelli, 2013</marker>
<rawString>M.F.M. Chowdhury and A. Lavelli. 2013c. FBK-irst : A Multi-Phase Kernel Based Approach for DrugDrug Interaction Detection and Classification that Exploits Linguistic Information. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chowdhury</author>
</authors>
<title>Improving the Effectiveness of Information Extraction from Biomedical Text.</title>
<date>2013</date>
<institution>University of Trento.</institution>
<note>Ph.d. dissertation,</note>
<contexts>
<context position="21822" citStr="Chowdhury (2013" startWordPosition="3537" endWordPosition="3538">e performance information is only shown in terms of F1 score. The reader can find the full ranking information on the SemEval-2013 Task 9 website7. Most of the participating systems were built on support vector machines. In general, approaches based on non-linear kernels methods achieved better results than linear SVMs. As in the previous edition of DDIExtraction, most systems have used primarily syntactic information. However, semantic information has been poorly used. The best results were submitted by the team from FBK-irst. They applied a novel hybrid kernel based RE approach described in Chowdhury (2013a). They also exploited the scope of negations and semantic roles for negative instance filtering as proposed in (Chowdhury and Lavelli, 2013b) and (Chowdhury and Lavelli, 2012). The second best results were obtained by the WBI team from the Humboldt University of Berlin. Its system combines several kernel methods (APG (Airola et al., 2008) and Shallow Linguistic Kernel (SL) (Giuliano et al., 2006) among others), the Turku Event Extraction system (TEES) (Bj¨orne et al., 2011)8 and the Moara system (Neves et al., 2009). These two teams were also the top two ranked teams in DDIExtraction 2011. F</context>
</contexts>
<marker>Chowdhury, 2013</marker>
<rawString>MFM. Chowdhury. 2013a. Improving the Effectiveness of Information Extraction from Biomedical Text. Ph.d. dissertation, University of Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Collazo</author>
<author>A Ceballo</author>
<author>D Puig</author>
<author>Y Guti´errez</author>
<author>J Abreu</author>
<author>J P´erez</author>
<author>A Fern´andez-Orqu´ın</author>
<author>A Montoyo</author>
<author>R Mu˜noz</author>
<author>F Camara</author>
</authors>
<title>UMCC DLSI-(DDI): Semantic and Lexical features for detection and classification Drugs in biomedical texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Collazo, Ceballo, Puig, Guti´errez, Abreu, P´erez, Fern´andez-Orqu´ın, Montoyo, Mu˜noz, Camara, 2013</marker>
<rawString>A. Collazo, A. Ceballo, D Puig, Y. Guti´errez, J. Abreu, J P´erez, A. Fern´andez-Orqu´ın, A. Montoyo, R. Mu˜noz, and F. Camara. 2013. UMCC DLSI-(DDI): Semantic and Lexical features for detection and classification Drugs in biomedical texts. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Giuliano</author>
<author>A Lavelli</author>
<author>L Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL2006),</booktitle>
<pages>401--408</pages>
<contexts>
<context position="22223" citStr="Giuliano et al., 2006" startWordPosition="3598" endWordPosition="3601">arily syntactic information. However, semantic information has been poorly used. The best results were submitted by the team from FBK-irst. They applied a novel hybrid kernel based RE approach described in Chowdhury (2013a). They also exploited the scope of negations and semantic roles for negative instance filtering as proposed in (Chowdhury and Lavelli, 2013b) and (Chowdhury and Lavelli, 2012). The second best results were obtained by the WBI team from the Humboldt University of Berlin. Its system combines several kernel methods (APG (Airola et al., 2008) and Shallow Linguistic Kernel (SL) (Giuliano et al., 2006) among others), the Turku Event Extraction system (TEES) (Bj¨orne et al., 2011)8 and the Moara system (Neves et al., 2009). These two teams were also the top two ranked teams in DDIExtraction 2011. For a more detailed description, the reader is encouraged to read the papers of the participants in the proceedings book. While the DDIExtraction 2011 shared task concentrated efforts on the detection of DDIs, this new DDIExtraction 2013 task involved not only the detection of DDIs, but also their classification. Although the results of DDIExtraction 2011 are not directly comparable with the ones re</context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL2006), pages 401–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Grego</author>
<author>F Pinto</author>
<author>F M Couto</author>
</authors>
<title>LASIGE: using Conditional Random Fields and ChEBI ontology.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="7766" citStr="Grego et al., 2013" startWordPosition="1223" endWordPosition="1226">n separate lines. 3.1 Evaluation Metrics This section describes the methodology that is used to evaluate the performance of the participating systems in task 9.1. The major forums of the Named Entity Recognition and Classification (NERC) research community (such as MUC-7 (Chinchor and Robinson, 1997), CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) or ACE07 have proposed several techniques to assess the performance of NERC systems. While 2http://www.cs.york.ac.uk/semeval-2013/task9/ 342 Figure 1: Example of an annotated document of the DDI corpus. Team Affiliation Description Task 9.1 LASIGE(Grego et al., 2013) University of Lisbon, Portugal Conditional random fields NLM LHC National Library of Medicine, USA Dictionary-based approach UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach UMCC DLSI(Collazo et al., 2013) Matanzas University, Cuba j48 classifier UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) WBI NER(Rockt¨aschel et al., 2013) Humboldt University of Berlin, Germany Conditional random fields Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + s</context>
</contexts>
<marker>Grego, Pinto, Couto, 2013</marker>
<rawString>T. Grego, F. Pinto, and F.M. Couto. 2013. LASIGE: using Conditional Random Fields and ChEBI ontology. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N D Hailu</author>
<author>L E Hunter</author>
<author>K B Cohen</author>
</authors>
<title>UColorado SOM: Extraction of Drug-Drug Interactions from Biomedical Text using Knowledge-rich and Knowledge-poor Features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="8701" citStr="Hailu et al., 2013" startWordPosition="1347" endWordPosition="1350">j48 classifier UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) WBI NER(Rockt¨aschel et al., 2013) Humboldt University of Berlin, Germany Conditional random fields Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO) SCAI(Bobi´c et al., 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR) UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel UCOLORADO SOM(Hailu et al., 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM) UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) UWM-TRIADS(Rastegar-Mojarad et al., 2013) University of Wisconsin-Milwaukee, USA Two-stage SVM WBI DDI(Thomas et al., 2013) Humboldt University of Berlin, Germany Ensemble of SVMs Table 2: Short description of the teams. ACE evaluation is very complex because its scores are not intuitive, MUC and CoNLL 2003 used the standard precision/recall/f-score metrics to compare their participating systems. The main shared tasks in the biomedical d</context>
</contexts>
<marker>Hailu, Hunter, Cohen, 2013</marker>
<rawString>N.D. Hailu, L.E. Hunter, and K.B. Cohen. 2013. UColorado SOM: Extraction of Drug-Drug Interactions from Biomedical Text using Knowledge-rich and Knowledge-poor Features. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>JM Carazo Neves</author>
<author>A Pascual-Montano</author>
</authors>
<title>Extraction of biomedical events using case-based reasoning.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on BioNLP: Shared Task,</booktitle>
<pages>68--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Neves, Pascual-Montano, 2009</marker>
<rawString>ML. Neves, JM. Carazo, and A. Pascual-Montano. 2009. Extraction of biomedical events using case-based reasoning. In Proceedings of the Workshop on BioNLP: Shared Task, pages 68–76. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pyysalo</author>
<author>A Airola</author>
<author>J Heimonen</author>
<author>J Bjorne</author>
<author>F Ginter</author>
<author>T Salakoski</author>
</authors>
<title>Comparative analysis of five protein-protein interaction corpora. BMC bioinformatics, 9(Suppl 3):S6.</title>
<date>2008</date>
<contexts>
<context position="4043" citStr="Pyysalo et al., (2008)" startWordPosition="609" endWordPosition="612">n for Computational Linguistics documents describing drug-drug interactions from the DrugBank database and MedLine abstracts on the subject of drug-drug interactions. The DDI corpus consists of 1,017 texts (784 DrugBank texts and 233 MedLine abstracts) and was manually annotated with a total of 18,491 pharmacological substances and 5,021 drug-drug interactions (see Table 1). A detailed description of the method used to collect and process documents can be found in (Segura-Bedmar et al., 2011a). The corpus is distributed in XML documents following the unified format for PPI corpora proposed by Pyysalo et al., (2008) (see Figure 1). A detailed description and analysis of the DDI corpus and its methodology are included in an article currently under review by BioInformatics journal.1 The corpus was split in order to build the datasets for the training and evaluation of the different participating systems. Approximately 77% of the DDI corpus documents were randomly selected for the training dataset and the remaining (142 DrugBank texts and 91 MedLine abstracts) was used for the test dataset. The training dataset is the same for both subtasks since it contains entity and DDI annotations. The test dataset for </context>
</contexts>
<marker>Pyysalo, Airola, Heimonen, Bjorne, Ginter, Salakoski, 2008</marker>
<rawString>S. Pyysalo, A. Airola, J. Heimonen, J. Bjorne, F. Ginter, and T. Salakoski. 2008. Comparative analysis of five protein-protein interaction corpora. BMC bioinformatics, 9(Suppl 3):S6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rastegar-Mojarad</author>
<author>R D Boyce</author>
<author>R Prasad</author>
</authors>
<title>UWM-TRIADS: Classifying Drug-Drug Interactions with Two-Stage SVM and Post-Processing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="8901" citStr="Rastegar-Mojarad et al., 2013" startWordPosition="1373" endWordPosition="1376">andom fields Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO) SCAI(Bobi´c et al., 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR) UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel UCOLORADO SOM(Hailu et al., 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM) UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) UWM-TRIADS(Rastegar-Mojarad et al., 2013) University of Wisconsin-Milwaukee, USA Two-stage SVM WBI DDI(Thomas et al., 2013) Humboldt University of Berlin, Germany Ensemble of SVMs Table 2: Short description of the teams. ACE evaluation is very complex because its scores are not intuitive, MUC and CoNLL 2003 used the standard precision/recall/f-score metrics to compare their participating systems. The main shared tasks in the biomedical domain have continued using these metrics to evaluate the outputs of their participant teams. System performance should be scored automatically by how well the generated pharmacological substance list </context>
</contexts>
<marker>Rastegar-Mojarad, Boyce, Prasad, 2013</marker>
<rawString>M. Rastegar-Mojarad, R. D. Boyce, and R. Prasad. 2013. UWM-TRIADS: Classifying Drug-Drug Interactions with Two-Stage SVM and Post-Processing. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rockt¨aschel</author>
<author>M Weidlich</author>
<author>U Leser</author>
</authors>
<title>Chemspot: a hybrid system for chemical named entity recognition.</title>
<date>2012</date>
<journal>Bioinformatics,</journal>
<volume>28</volume>
<issue>12</issue>
<marker>Rockt¨aschel, Weidlich, Leser, 2012</marker>
<rawString>T. Rockt¨aschel, M. Weidlich, and U. Leser. 2012. Chemspot: a hybrid system for chemical named entity recognition. Bioinformatics, 28(12):1633–1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rockt¨aschel</author>
<author>T Huber</author>
<author>M Weidlich</author>
<author>U Leser</author>
</authors>
<title>WBI-NER: The impact of domain-specific features on the performance of identifying and classifying mentions of drugs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Rockt¨aschel, Huber, Weidlich, Leser, 2013</marker>
<rawString>T. Rockt¨aschel, T. Huber, M. Weidlich, and U. Leser. 2013. WBI-NER: The impact of domain-specific features on the performance of identifying and classifying mentions of drugs. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sanchez-Cisneros</author>
<author>F Aparicio</author>
</authors>
<title>UEMUC3M: An Ontology-based named entity recognition system for biomedical texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="7937" citStr="Sanchez-Cisneros and Aparicio, 2013" startWordPosition="1244" endWordPosition="1247">ask 9.1. The major forums of the Named Entity Recognition and Classification (NERC) research community (such as MUC-7 (Chinchor and Robinson, 1997), CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) or ACE07 have proposed several techniques to assess the performance of NERC systems. While 2http://www.cs.york.ac.uk/semeval-2013/task9/ 342 Figure 1: Example of an annotated document of the DDI corpus. Team Affiliation Description Task 9.1 LASIGE(Grego et al., 2013) University of Lisbon, Portugal Conditional random fields NLM LHC National Library of Medicine, USA Dictionary-based approach UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach UMCC DLSI(Collazo et al., 2013) Matanzas University, Cuba j48 classifier UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) WBI NER(Rockt¨aschel et al., 2013) Humboldt University of Berlin, Germany Conditional random fields Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO) SCAI(Bobi´c et al., 2013) Fraunhofer SCAI</context>
</contexts>
<marker>Sanchez-Cisneros, Aparicio, 2013</marker>
<rawString>D. Sanchez-Cisneros and F. Aparicio. 2013. UEMUC3M: An Ontology-based named entity recognition system for biomedical texts. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sanchez-Cisneros</author>
</authors>
<title>UC3M: A kernel-based approach for identify and classify DDIs in biomedical texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="8602" citStr="Sanchez-Cisneros, 2013" startWordPosition="1335" endWordPosition="1336">ity of Madrid, Spain Ontology-based approach UMCC DLSI(Collazo et al., 2013) Matanzas University, Cuba j48 classifier UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) WBI NER(Rockt¨aschel et al., 2013) Humboldt University of Berlin, Germany Conditional random fields Task 9.2 FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO) SCAI(Bobi´c et al., 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR) UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel UCOLORADO SOM(Hailu et al., 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM) UTurku(Bj¨orne et al., 2013) University of Turku, Finland SVM classifier (TEES system) UWM-TRIADS(Rastegar-Mojarad et al., 2013) University of Wisconsin-Milwaukee, USA Two-stage SVM WBI DDI(Thomas et al., 2013) Humboldt University of Berlin, Germany Ensemble of SVMs Table 2: Short description of the teams. ACE evaluation is very complex because its scores are not intuitive, MUC and CoNLL 2003 used the standard precision/recal</context>
</contexts>
<marker>Sanchez-Cisneros, 2013</marker>
<rawString>D. Sanchez-Cisneros. 2013. UC3M: A kernel-based approach for identify and classify DDIs in biomedical texts. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Segura-Bedmar</author>
<author>P Martinez</author>
<author>C de Pablo-S´anchez</author>
</authors>
<title>Using a shallow linguistic kernel for drug-drug interaction extraction.</title>
<date>2011</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>44</volume>
<issue>5</issue>
<pages>804</pages>
<marker>Segura-Bedmar, Martinez, de Pablo-S´anchez, 2011</marker>
<rawString>I. Segura-Bedmar, P. Martinez, and C. de Pablo-S´anchez. 2011a. Using a shallow linguistic kernel for drug-drug interaction extraction. Journal of Biomedical Informatics, 44(5):789 – 804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Segura-Bedmar</author>
<author>P Martınez</author>
<author>D S´anchez-Cisneros</author>
</authors>
<title>The 1st ddiextraction-2011 challenge task: Extraction of drug-drug interactions from biomedical texts.</title>
<date>2011</date>
<booktitle>In Proceedings of DDIExtraction-2011 challenge task,</booktitle>
<pages>1--9</pages>
<marker>Segura-Bedmar, Martınez, S´anchez-Cisneros, 2011</marker>
<rawString>I. Segura-Bedmar, P. Martınez, and D. S´anchez-Cisneros. 2011b. The 1st ddiextraction-2011 challenge task: Extraction of drug-drug interactions from biomedical texts. In Proceedings of DDIExtraction-2011 challenge task, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Thomas</author>
<author>M Neves</author>
<author>T Rockt¨aschel</author>
<author>U Leser</author>
</authors>
<title>WBI-DDI: Drug-Drug Interaction Extraction using Majority Voting.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Thomas, Neves, Rockt¨aschel, Leser, 2013</marker>
<rawString>P. Thomas, M. Neves, T. Rockt¨aschel, and U. Leser. 2013. WBI-DDI: Drug-Drug Interaction Extraction using Majority Voting. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,</booktitle>
<pages>142--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E.F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142–147. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Wishart</author>
<author>C Knox</author>
<author>A C Guo</author>
<author>S Shrivastava</author>
<author>M Hassanali</author>
<author>P Stothard</author>
<author>Z Chang</author>
<author>J Woolsey</author>
</authors>
<title>Drugbank: a comprehensive resource for in silico drug discovery and exploration. Nucleic acids research, 34(suppl 1):D668–D672.</title>
<date>2006</date>
<contexts>
<context position="2461" citStr="Wishart et al., 2006" startWordPosition="363" endWordPosition="366"> DDI extraction also a supporting task, the recognition and classification of pharmacological substances. DDIExtraction 2013 is designed to address the extraction of DDIs as a whole, but divided into two subtasks to allow separate evaluation of the performance for different aspects of the problem. The shared task includes two challenges: • Task 9.1: Recognition and classification of pharmacological substances. • Task 9.2: Extraction of drug-drug interactions. Additionally, while the datasets used for the DDIExtraction 2011 task were composed by texts describing DDIs from the DrugBank database(Wishart et al., 2006), the new datasets for DDIExtraction 2013 also include MedLine abstracts in order to deal with different types of texts and language styles. This shared task has been conceived with a dual objective: advancing the state-of-the-art of textmining techniques applied to the pharmacological domain, and providing a common framework for evaluation of the participating systems and other researchers interested in the task. In the next section we describe the DDI corpus used in this task. Sections 3 and 4 focus on the description of the task 9.1 and 9.2 respectively. Finally, Section 5 draws the conclus</context>
</contexts>
<marker>Wishart, Knox, Guo, Shrivastava, Hassanali, Stothard, Chang, Woolsey, 2006</marker>
<rawString>D.S. Wishart, C. Knox, A.C. Guo, S. Shrivastava, M. Hassanali, P. Stothard, Z. Chang, and J. Woolsey. 2006. Drugbank: a comprehensive resource for in silico drug discovery and exploration. Nucleic acids research, 34(suppl 1):D668–D672.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>