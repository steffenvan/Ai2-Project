<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9583745">
S-MART: Novel Tree-based Structured Learning Algorithms
Applied to Tweet Entity Linking
</title>
<author confidence="0.998935">
Yi Yang
</author>
<affiliation confidence="0.999115">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<email confidence="0.998566">
yiyang@gatech.edu
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99958888">
Non-linear models recently receive a lot
of attention as people are starting to dis-
cover the power of statistical and em-
bedding features. However, tree-based
models are seldom studied in the con-
text of structured learning despite their re-
cent success on various classification and
ranking tasks. In this paper, we propose
S-MART, a tree-based structured learning
framework based on multiple additive re-
gression trees. S-MART is especially suit-
able for handling tasks with dense fea-
tures, and can be used to learn many dif-
ferent structures under various loss func-
tions.
We apply S-MART to the task of tweet
entity linking — a core component of
tweet information extraction, which aims
to identify and link name mentions to en-
tities in a knowledge base. A novel infer-
ence algorithm is proposed to handle the
special structure of the task. The exper-
imental results show that S-MART signif-
icantly outperforms state-of-the-art tweet
entity linking systems.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999414538461538">
Many natural language processing (NLP) prob-
lems can be formalized as structured prediction
tasks. Standard algorithms for structured learning
include Conditional Random Field (CRF) (Laf-
ferty et al., 2001) and Structured Supported Vec-
tor Machine (SSVM) (Tsochantaridis et al., 2004).
These algorithms, usually equipped with a linear
model and sparse lexical features, achieve state-
of-the-art performances in many NLP applica-
tions such as part-of-speech tagging, named entity
recognition and dependency parsing.
This classical combination of linear models and
sparse features is challenged by the recent emerg-
</bodyText>
<subsectionHeader confidence="0.3870855">
Ming-Wei Chang
Microsoft Research
</subsectionHeader>
<email confidence="0.745705">
minchang@microsoft.com
</email>
<bodyText confidence="0.925266525">
ing usage of dense features such as statistical and
embedding features. Tasks with these low dimen-
sional dense features require models to be more
sophisticated to capture the relationships between
features. Therefore, non-linear models start to re-
ceive more attention as they are often more expres-
sive than linear models.
Tree-based models such as boosted trees (Fried-
man, 2001) are flexible non-linear models. They
can handle categorical features and count data bet-
ter than other non-linear models like Neural Net-
works. Unfortunately, to the best of our knowl-
edge, little work has utilized tree-based methods
for structured prediction, with the exception of
TreeCRF (Dietterich et al., 2004).
In this paper, we propose a novel structured
learning framework called S-MART (Structured
Multiple Additive Regression Trees). Unlike
TreeCRF, S-MART is very versatile, as it can be
applied to tasks beyond sequence tagging and can
be trained under various objective functions. S-
MART is also powerful, as the high order relation-
ships between features can be captured by non-
linear regression trees.
We further demonstrate how S-MART can be
applied to tweet entity linking, an important and
challenging task underlying many applications in-
cluding product feedback (Asur and Huberman,
2010) and topic detection and tracking (Math-
ioudakis and Koudas, 2010). We apply S-MART to
entity linking using a simple logistic function as
the loss function and propose a novel inference al-
gorithm to prevent overlaps between entities.
Our contributions are summarized as follows:
• We propose a novel structured learning
framework called S-MART. S-MART com-
bines non-linearity and efficiency of tree-
based models with structured prediction,
leading to a family of new algorithms. (Sec-
tion 2)
</bodyText>
<page confidence="0.968181">
504
</page>
<note confidence="0.976490666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 504–513,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.557016857142857">
• We apply S-MART to tweet entity link-
ing. Building on top of S-MART, we pro-
pose a novel inference algorithm for non-
overlapping structure with the goal of pre-
venting conflicting entity assignments. (Sec-
tion 3)
• We provide a systematic study of evaluation
criteria in tweet entity linking by conduct-
ing extensive experiments over major data
sets. The results show that S-MART sig-
nificantly outperforms state-of-the-art entity
linking systems, including the system that is
used to win the NEEL 2014 challenge (Cano
and others, 2014). (Section 4)
</bodyText>
<sectionHeader confidence="0.9995505" genericHeader="method">
2 Structured Multiple Additive
Regression Trees
</sectionHeader>
<bodyText confidence="0.99936875">
The goal of a structured learning algorithm is to
learn a joint scoring function S between an input
x and an output structure y, S : (x, y) → R. The
structured output y often contains many interde-
pendent variables, and the number of the possible
structures can be exponentially large with respect
to the size of x. At test time, the prediction y for
x is obtained by
</bodyText>
<equation confidence="0.5743485">
arg max S(x, y),
yEGen(x)
</equation>
<bodyText confidence="0.9998755">
where Gen(x) represents the set of all valid output
structures for x.
Standard learning algorithms often directly op-
timize the model parameters. For example, as-
sume that the joint scoring function S is param-
eterized by θ. Then, gradient descent algorithms
can be used to optimize the model parameters θ
iteratively. More specifically,
</bodyText>
<equation confidence="0.9960565">
∂L(y*, S(x,y; θ)) θm = θm−1 − ηm , (1)
∂θm−1
</equation>
<bodyText confidence="0.9972588125">
where y* is the gold structure, L(y*, S(x, y; θ))
is a loss function and ηm is the learning rate of the
m-th iteration.
In this paper, we propose a framework called
Structured Multiple Additive Regression Trees
(S-MART), which generalizes Multiple Additive
Regression Trees (MART) for structured learn-
ing problems. Different from Equation (1), S-
MART does not directly optimize the model pa-
rameters; instead, it approximates the optimal
scoring function that minimize the loss by adding
(weighted) regression tree models iteratively.
Due to the fact that there are exponentially
many input-output pairs in the training data,
S-MART assumes that the joint scoring function
can be decomposed as
</bodyText>
<equation confidence="0.9985485">
S(x, y) = � F(x, yk),
kEQ(x)
</equation>
<bodyText confidence="0.9996939">
where Q(x) contains the set of the all factors for
input x and yk is the sub-structure of y that cor-
responds to the k-th factor in Q(x). For instance,
in the task of word alignment, each factor can be
defined as a pair of words from source and target
languages respectively. Note that we can recover
y from the union of {yk}K 1 .
The factor scoring function F(x, yk) can be
optimized by performing gradient descent in the
function space in the following manner:
</bodyText>
<equation confidence="0.999822">
Fm(x,yk) = Fm−1(x,yk) − ηmgm(x,yk) (2)
</equation>
<bodyText confidence="0.999817428571429">
where function gm(x, yk) is the functional gradi-
ent.
Note that gm is a function rather than a vector.
Therefore, modeling gm theoretically requires an
infinite number of data points. We can address this
difficulty by approximating gm with a finite num-
ber of point-wise functional gradients
</bodyText>
<equation confidence="0.997649">
gm(x,yk = uk) = (3)
raL(y*, S(x, yk = uk))1
L ∂F(x,yk = uk) J F(xryk)=Fm−1(xryk)
</equation>
<bodyText confidence="0.9999775">
where uk index a valid sub-structure for the k-th
factor of x.
The key point of S-MART is that it approximates
−gm by modeling the point-wise negative func-
tional gradients using a regression tree hm. Then
the factor scoring function can be obtained by
</bodyText>
<equation confidence="0.998533">
M
F(x, yk) = ηmhm(x, yk),
m=1
</equation>
<bodyText confidence="0.9998035">
where hm(x, yk) is also called a basis function
and ηm can be simply set to 1 (Murphy, 2012).
The detailed S-MART algorithm is presented in
Algorithm 1. The factor scoring function F(x, yk)
is simply initialized to zero at first (line 1). After
this, we iteratively update the function by adding
regression trees. Note that the scoring function is
shared by all the factors. Specifically, given the
current decision function Fm−1, we can consider
line 3 to line 9 a process of generating the pseudo
</bodyText>
<page confidence="0.996837">
505
</page>
<bodyText confidence="0.977391333333333">
Algorithm 1 S-MART: A family of structured
learning algorithms with multiple additive regres-
sion trees
</bodyText>
<listItem confidence="0.894916833333333">
1: F0(x, yk) = 0
2: form = 1 to M do: &gt; going over all trees
3: D +— 0
4: for all examples do: &gt; going over all examples
5: for yk E Ω(x) do: &gt; going over all factors
6: For all uk, obtain gku by Equation (3)
7: D +— D U {(Φ(x, yk = uk), −gku)}
8: end for
9: end for
10: hm(x,yk) +— TrainRegressionTree(D)
11: Fm(x, yk) = Fm−1(x, yk) + hm(x, yk)
12: end for
</listItem>
<bodyText confidence="0.999895632653061">
training data D for modeling the regression tree.
For each training example, S-MART first computes
the point-wise functional gradients according to
Equation (3) (line 6). Here we use gku as the ab-
breviation for gm(x, yk = uk). In line 7, for each
sub-structure uk, we create a new training exam-
ple for the regression problem by the feature vec-
tor Φ(x, yk = uk) and the negative gradient −gku.
In line 10, a regression tree is constructed by min-
imizing differences between the prediction values
and the point-wise negative gradients. Then a new
basis function (modeled by a regression tree) will
be added into the overall F (line 11).
It is crucial to note that S-MART is a fam-
ily of algorithms rather than a single algorithm.
S-MART is flexible in the choice of the loss
functions. For example, we can use either lo-
gistic loss or hinge loss, which means that S-
MART can train probabilistic models as well as
non-probabilistic ones. Depending on the choice
of factors, S-MART can handle various structures
such as linear chains, trees, and even the semi-
Markov chain (Sarawagi and Cohen, 2004).
S-MART versus MART There are two key
differences between S-MART and MART. First,
S-MART decomposes the joint scoring function
S(x, y) into factors to address the problem of the
exploding number of input-output pairs for struc-
tured learning problems. Second, S-MART mod-
els a single scoring function F(x, yk) over inputs
and output variables directly rather than O differ-
ent functions F°(x), each of which corresponds to
a label class.
S-MART versus TreeCRF TreeCRF can be
viewed as a special case of S-MART, and there
are two points where S-MART improves upon
TreeCRF. First, the model designed in (Dietterich
et al., 2004) is tailored for sequence tagging prob-
lems. Similar to MART, for a tagging task with O
tags, they choose to model O functions F°(x, o&apos;)
instead of directly modeling the joint score of the
factor. This imposes limitations on the feature
functions, and TreeCRF is consequently unsuit-
able for many tasks such as entity linking.1Second,
S-MART is more general in terms of the objective
functions and applicable structures. In the next
section, we will see how S-MART can be applied to
a non-linear-chain structure and various loss func-
tions.
</bodyText>
<sectionHeader confidence="0.999615" genericHeader="method">
3 S-MART for Tweet Entity Linking
</sectionHeader>
<bodyText confidence="0.999992">
We first formally define the task of tweet entity
linking. As input, we are given a tweet, an entity
database (e.g., Wikipedia where each article is an
entity), and a lexicon2 which maps a surface form
into a set of entity candidates. For each incoming
tweet, all n-grams of this tweet will be used to find
matches in the lexicon, and each match will form a
mention candidate. As output, we map every men-
tion candidate (e.g., “new york giants”) in the mes-
sage to an entity (e.g., NEW YORK GIANTS) or to
Nil (i.e., a non-entity). A mention candidate can
often potentially link to multiple entities, which
we call possible entity assignments.
This task is a structured learning problem, as the
final entity assignments of a tweet should not over-
lap with each other.3 We decompose this learn-
ing problem as follows: we make each mention
candidate a factor, and the score of the entity as-
signments of a tweet is the sum of the score of
each entity and mention candidate pair. Although
all mention candidates are decomposed, the non-
overlapping constraint requires the system to per-
form global inference.
Consider the example tweet in Figure 1, where
we show the tweet with the mention candidates
in brackets. To link the mention candidate “new
york giants” to a non-Nil entity, the system has to
link previous overlapping mention candidates to
Nil. It is important to note that this is not a lin-
ear chain problem because of the non-overlapping
constraint, and the inference algorithm needs to be
</bodyText>
<footnote confidence="0.99658325">
1For example, entity linking systems need to model the
similarity between an entity and the document. The TreeCRF
formulation does not support such features.
2We use the standard techniques to construct the lexicon
from anchor texts, redirect pages and other information re-
sources.
3We follow the common practice and do not allow embed-
ded entities.
</footnote>
<page confidence="0.994631">
506
</page>
<figureCaption confidence="0.990543">
Figure 1: Example tweet and its mention candidates. Each mention candidate is marked as a pair of brackets in the original
</figureCaption>
<bodyText confidence="0.96424125">
tweet and forms a column in the graph. The graph demonstrates the non-overlapping constraint. To link the mention candidate
“new york giants” to a non-Nil entity, the system has to link previous four overlapping mention candidates to Nil. The mention
candidate “eli manning” is not affected by “new york giants”. Note that this is not a standard linear chain problem.
carefully designed.
</bodyText>
<subsectionHeader confidence="0.992427">
3.1 Applying S-MART
</subsectionHeader>
<bodyText confidence="0.999643727272727">
We derive specific model for tweet entity linking
task with S-MART and use logistic loss as our run-
ning example. The hinge loss version of the model
can be derived in a similar way.
Note that the tweet and the mention candidates
are given. Let x be the tweet, uk be the entity as-
signment of the k-th mention candidate. We use
function F (x, yk = uk) to model the score of the
k-th mention candidate choosing entity uk.4 The
overall scoring function can be decomposed as fol-
lows:
</bodyText>
<equation confidence="0.997099">
K
S(x, y = {uk}Kk=1) = F(x, yk = uk)
k=1
</equation>
<bodyText confidence="0.9987218">
S-MART utilizes regression trees to model the
scoring function F(x, yk = uk), which requires
point-wise functional gradient for each entity of
every mention candidate. Let’s first write down
the logistic loss function as
</bodyText>
<equation confidence="0.9695985">
L(y∗, S(x, y)) = − log P(y∗|x)
=log Z(x) − S(x, y∗)
</equation>
<bodyText confidence="0.999500333333333">
where Z(x) = Ey exp(S(x, y)) is the potential
function. Then the point-wise gradients can be
computed as
</bodyText>
<equation confidence="0.985062333333333">
∂L
∂F(x, yk = uk)
= P(yk = uk|x) − 1[y∗k = uk],
</equation>
<bodyText confidence="0.855932833333333">
where 1[·] represents an indicator function. The
conditional probability P (yk = uk|x) can be com-
puted by a variant of the forward-backward algo-
rithm, which we will detail in the next subsection.
4Note that each mention candidate has different own en-
tity sets.
</bodyText>
<subsectionHeader confidence="0.972997">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.999903083333333">
The non-overlapping structure is distinct from lin-
ear chain and semi-Markov chain (Sarawagi and
Cohen, 2004) structures. Hence, we propose a
carefully designed forward-backward algorithm to
calculate P(yk = uk|x) based on current scor-
ing function F(x, yk = uk) given by the re-
gression trees. The non-overlapping constraint
distinguishes our inference algorithm from other
forward-backward variants.
To compute the forward probability, we sort5
the mention candidates by their end indices and
define forward recursion by
</bodyText>
<equation confidence="0.996982833333333">
α(u1,1) = exp(F(x, y1 = u1))
α(uk, k) = exp(F(x, yk = uk))
· P�− 1 exp(F(x, yk−p = Nil))
p=1
1: · α(uk−P, k − P) (4)
uk−P
</equation>
<bodyText confidence="0.9250576">
where k − P is the index of the previous non-
overlapping mention candidate. Intuitively, for
the k-th mention candidate, we need to identify
its nearest non-overlapping fellow and recursively
compute the probability. The overlapping mention
candidates can only take the Nil entity.
Similarly, we can sort the mention candidates
by their start indices and define backward recur-
5Sorting helps the algorithms find non-overlapping candi-
dates.
</bodyText>
<equation confidence="0.9369138">
gku =
507
sion by
β(uK, K) =1
β(uk, k) = X exp(F(x, yk+Q = uk+Q))
uk+Q
Q−1
Y· exp(F (x, yk+q = Nil))
q=1
· β(uk+Q, k + Q) (5)
</equation>
<bodyText confidence="0.96309025">
where k + Q is the index of the next non-
overlapping mention candidate. Note that the third
terms of equation (4) or (5) will vanish if there are
no corresponding non-overlapping mention candi-
dates.
Given the potential function can be computed
by Z(x) = Puk α(uk, k)β(uk, k), for entities
that are not Nil,
</bodyText>
<equation confidence="0.997988857142857">
exp(F (x, yk = uk)) · β(uk, k)
P(yk = uk|x) =Z(x)
·
P Y− 1 exp(F(x, yk−p = Nil))
p=1
X· α(uk−P, k − P) (6)
uk−P
</equation>
<bodyText confidence="0.912692">
The probability for the special token Nil can be
obtained by
</bodyText>
<equation confidence="0.9422825">
P(yk = Nil|x) = 1 − X P(yk = uk|x) (7)
uk6=Nil
</equation>
<bodyText confidence="0.996771571428572">
In the worst case, the total cost of the forward-
backward algorithm is O(max{TK, K2}), where
T is the number of entities of a mention candi-
date.6
Finally, at test time, the decoding problem
arg maxy S(x, y) can be solved by a variant of
the Viterbi algorithm.
</bodyText>
<subsectionHeader confidence="0.886763">
3.3 Beyond S-MART: Modeling entity-entity
relationships
</subsectionHeader>
<bodyText confidence="0.999944875">
It is important for entity linking systems to take
advantage of the entity-to-entity information while
making local decisions. For instance, the identi-
fication of entity “eli manning” leads to a strong
clue for linking “new york giants” to the NFL
team.
Instead of defining a more complicated struc-
ture and learning everything jointly, we employ a
</bodyText>
<footnote confidence="0.997976666666667">
6The cost is O(K2) only if every mention candidate of
the tweet overlaps other mention candidates. In practice, the
algorithm is nearly linear w.r.t K.
</footnote>
<bodyText confidence="0.995688375">
two-stage approach as the solution for modeling
entity-entity relationships after we found that S-
MART achieves high precision and reasonable re-
call. Specifically, in the first stage, the system
identifies all possible entities with basic features,
which enables the extraction of entity-entity fea-
tures. In the second stage, we re-train S-MART on
a union of basic features and entity-entity features.
We define entity-entity features based on the Jac-
card distance introduced by Guo et al. (2013).
Let Γ(ei) denotes the set of Wikipedia pages
that contain a hyperlink to an entity ei and Γ(t−i)
denotes the set of pages that contain a hyperlink
to any identified entity ej of the tweet t in the first
stage excluding ei. The Jaccard distance between
ei and t is
</bodyText>
<equation confidence="0.994123">
|Γ(ei) n Γ(t−i)|
Jac(ei, t) = |Γ(ei) U Γ(t−i)|.
</equation>
<bodyText confidence="0.9999635">
In addition to the Jaccard distance, we add one ad-
ditional binary feature to indicate if the current en-
tity has the highest Jaccard distance among all en-
tities for this mention candidate.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997448333333333">
Our experiments are designed to answer the fol-
lowing three research questions in the context of
tweet entity linking:
</bodyText>
<listItem confidence="0.999193666666667">
• Do non-linear learning algorithms perform
better than linear learning algorithms?
• Do structured entity linking models perform
better than non-structured ones?
• How can we best capture the relationships be-
tween entities?
</listItem>
<subsectionHeader confidence="0.997966">
4.1 Evaluation Methodology and Data
</subsectionHeader>
<bodyText confidence="0.999958285714286">
We evaluate each entity linking system using two
evaluation policies: Information Extraction (IE)
driven evaluation and Information Retrieval (IR)
driven evaluation. For both evaluation settings,
precision, recall and F1 scores are reported. Our
data is constructed from two publicly available
sources: Named Entity Extraction &amp; Linking
(NEEL) Challenge (Cano et al., 2014) datasets,
and the datasets released by Fang and Chang
(2014). Note that we gather two datasets from
Fang and Chang (2014) and they are used in two
different evaluation settings. We refer to these two
datasets as TACL-IE and TACL-IR, respectively.
We perform some data cleaning and unification on
</bodyText>
<page confidence="0.988839">
508
</page>
<bodyText confidence="0.999757583333333">
these sets.7 The statistics of the datasets are pre-
sented in Table 1.
IE-driven evaluation The IE-driven evaluation
is the standard evaluation for an end-to-end entity
linking system. We follow Carmel et al. (2014)
and relax the definition of the correct mention
boundaries, as they are often ambiguous. A men-
tion boundary is considered to be correct if it over-
laps (instead of being the same) with the gold men-
tion boundary. Please see (Carmel et al., 2014) for
more details on the procedure of calculating the
precision, recall and F1 score.
The NEEL and TACL-IE datasets have differ-
ent annotation guidelines and different choices of
knowledge bases, so we perform the following
procedure to clean the data and unify the annota-
tions. We first filter out the annotations that link to
entities excluded by our knowledge base. We use
the same knowledge base as the ERD 2014 com-
petition (Carmel et al., 2014), which includes the
union of entities in Wikipedia and Freebase. Sec-
ond, we follow NEEL annotation guideline and
re-annotate TACL-IE dataset. For instance, in or-
der to be consistent with NEEL, all the user tags
(e.g. @BarackObama) are re-labeled as entities in
TACL-IE.
We train all the models with NEEL Train
dataset and evaluate different systems on NEEL
Test and TACL-IE datasets. In addition, we sam-
ple 800 tweets from NEEL Train dataset as our
development set to perform parameter tuning.
IR-driven evaluation The IR-driven evaluation
is proposed by Fang and Chang (2014). It is
motivated by a key application of entity linking
— retrieval of relevant tweets for target entities,
which is crucial for downstream applications such
as product research and sentiment analysis. In
particular, given a query entity we can search for
tweets based on the match with some potential sur-
face forms of the query entity. Then, an entity
linking system is evaluated by its ability to cor-
rectly identify the presence or absence of the query
entity in every tweet. Our IR-driven evaluation
is based on the TACL-IR set, which includes 980
tweets sampled for ten query entities of five entity
types (roughly 100 tweets per entity). About 37%
of the sampled tweets did not mention the query
entity due to the anchor ambiguity.
</bodyText>
<footnote confidence="0.78306">
7We plan to release the cleaned data and evaluation code
if license permitted.
</footnote>
<table confidence="0.9994878">
Data #Tweet #Entity Date
NEEL Train 2340 2202 Jul. ˜Aug. 11
NEEL Test 1164 687 Jul. ˜Aug. 11
TACL-IE 500 300 Dec. 12
TACL-IR 980 NA Dec. 12
</table>
<tableCaption confidence="0.999864">
Table 1: Statistics of data sets.
</tableCaption>
<subsectionHeader confidence="0.97881">
4.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999506486486486">
Features We employ a total number of 37 dense
features as our basic feature set. Most of the fea-
tures are adopted from (Guo et al., 2013)8, includ-
ing various statistical features such as the proba-
bility of the surface to be used as anchor text in
Wikipedia. We also add additional Entity Type
features correspond to the following entity types:
Character, Event, Product and Brand. Finally,
we include several NER features to indicate each
mention candidate belongs to one the following
NER types: Twitter user, Twitter hashtag, Person,
Location, Organization, Product, Event and Date.
Algorithms Table 2 summarizes all the algo-
rithms that are compared in our experiments. First,
we consider two linear structured learning algo-
rithms: Structured Perceptron (Collins, 2002) and
Linear Structured SVM (SSVM) (Tsochantaridis
et al., 2004).
For non-linear models, we consider polynomial
SSVM, which employs polynomial kernel inside
the structured SVM algorithm. We also include
LambdaRank (Quoc and Le, 2007), a neural-
based learning to rank algorithm, which is widely
used in the information retrieval literature. We
further compare with MART, which is designed
for performing multiclass classification using log
loss without considering the structured informa-
tion. Finally, we have our proposed log-loss S-
MART algorithm, as described in Section 3. 9
Note that our baseline systems are quite strong.
Linear SSVM has been used in one of the state-
of-the-art tweet entity linking systems (Guo et al.,
2013), and the system based on MART is the win-
ning system of the 2014 NEEL Challenge (Cano
and others, 2014)10.
Table 2 summarizes several properties of the al-
gorithms. For example, most algorithms are struc-
</bodyText>
<footnote confidence="0.996629125">
8We consider features of Base, Capitalization Rate, Pop-
ularity, Context Capitalization and Entity Type categories.
9Our pilot experiments show that the log-loss S-
MART consistently outperforms the hinge-loss S-MART.
10Note that the numbers we reported here are different
from the results in NEEL challenge due to the fact that
we have cleaned the datasets and the evaluation metrics are
slightly different in this paper.
</footnote>
<page confidence="0.99131">
509
</page>
<table confidence="0.999168714285714">
Model Structured Non-linear Tree-based
Structured Perceptron ✓
Linear SSVM ✓
Polynomial SSVM ✓ ✓
LambdaRank ✓
MART ✓ ✓
S-MART ✓ ✓ ✓
</table>
<tableCaption confidence="0.999527">
Table 2: Included algorithms and their properties.
</tableCaption>
<bodyText confidence="0.980265966666667">
tured (e.g. they perform dynamic programming
at test time) except for MART and LambdaRank,
which treat mention candidates independently.
Parameter tuning All the hyper-parameters are
tuned on the development set. Then, we re-train
our models on full training data (including the
dev set) with the best parameters. We choose the
soft margin parameter C from 10.5,1,5, 10} for
two structured SVM methods. After a prelimi-
nary parameter search, we fixed the number of
trees to 300 and the minimum number of docu-
ments in a leaf to 30 for all tree-based models.
For LambdaRank, we use a two layer feed for-
ward network. We select the number of hidden
units from 110, 20, 30, 40} and learning rate from
10.1, 0.01, 0.001}.
It is widely known that F1 score can be affected
by the trade-off between precision and recall. In
order to make the comparisons between all algo-
rithms fairer in terms of F1 score, we include a
post-processing step to balance precision and re-
call for all the systems. Note the tuning is only
conducted for the purpose of robust evaluation. In
particular, we adopt a simple tuning strategy that
works well for all the algorithms, in which we add
a bias term b to the scoring function value of Nil:
F(x, yk = Nil) ← F(x, yk = Nil) + b.
We choose the bias term b from values between
−3.0 to 3.0 on the dev set and apply the same bias
term at test time.
</bodyText>
<subsectionHeader confidence="0.84234">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.967180333333333">
Table 3 presents the empirical findings for S-
MART and competitive methods on tweet entity
linking task in both IE and IR settings. In the fol-
lowing, we analyze the empirical results in details.
Linear models vs. non-linear models Table 3
clearly shows that linear models perform worse
than non-linear models when they are restricted
to the IE setting of the tweet entity linking task.
The story is similar in IR-driven evaluation, with
</bodyText>
<equation confidence="0.727021">
−2 −1 0 1 2
Bias
</equation>
<figureCaption confidence="0.9985555">
Figure 2: Balance precisions and recalls. X-axis corresponds
to values of the bias terms for the special token Nil. Note that
S-MART is still the overall winning system without tuning the
threshold.
</figureCaption>
<bodyText confidence="0.9971036">
the exception of LambdaRank. Among the lin-
ear models, linear SSVM demonstrates its supe-
riority over Structured Perceptron on all datasets,
which aligns with the results of (Tsochantaridis et
al., 2005) on the named entity recognition task.
We have many interesting observations on the
non-linear models side. First, by adopting a
polynomial kernel, the non-linear SSVM further
improves the entity linking performances on the
NEEL datasets and TACL-IR dataset. Second,
LambdaRank, a neural network based model,
achieves better results than linear models in IE-
driven evaluation, but the results in IR-driven eval-
uation are worse than all the other methods. We
believe the reason for this dismal performance is
that the neural-based method tends to overfit the
IR setting given the small number of training ex-
amples. Third, both MART and S-MART signifi-
cantly outperform alternative linear and non-linear
methods in IE-driven evaluation and performs bet-
ter or similar to other methods in IR-driven eval-
uation. This suggests that tree-based non-linear
models are suitable for tweet entity linking task.
Finally, S-MART outperforms previous state-of-
the-art method Structured SVM by a surprisingly
large margin. In the NEEL Test dataset, the dif-
ference is more than 10% F1. Overall, the results
show that the shallow linear models are not ex-
pressive enough to capture the complex patterns
in the data, which are represented by a few dense
features.
Structured learning models To showcase
structured learning technique is crucial for entity
linking with non-linear models, we compare
S-MART against MART directly. As shown in
</bodyText>
<figure confidence="0.985860818181818">
80
70
60
50
SP
Linear SSVM
Poly. SSVM
MART
NN
S-MART
F1 score
</figure>
<page confidence="0.971569">
510
</page>
<table confidence="0.981944888888889">
Model NEEL Dev NEEL Test TACL-IE TACL-IR
P R F1 P R F1 P R F1 P R F1
Structured Perceptron 75.8 62.8 68.7 79.1 64.3 70.9 74.4 63.0 68.2 86.2 43.8 58.0
Linear SSVM 78.0 66.1 71.5 80.5 67.1 73.2 78.2 64.7 70.8 86.7 48.5 62.2
Polynomial SSVM 77.7 70.7 74.0 81.3 69.0 74.6 76.8 64.0 69.8 91.1 48.8 63.6
LambdaRank 75.0 69.0 71.9 80.3 71.2 75.5 77.8 66.7 71.8 85.8 42.4 56.8
MART 76.2 74.3 75.2 76.8 78.0 77.4 73.4 71.0 72.2 98.1 46.4 63.0
S-MART 79.1 75.8 77.4 83.2 79.2 81.1 76.8 73.0 74.9 95.1 52.2 67.4
+ entity-entity 79.2 75.8 77.5 81.5 76.4 78.9 77.3 73.7 75.4 95.5 56.7 71.1
</table>
<tableCaption confidence="0.908336333333333">
Table 3: IE-driven and IR-driven evaluation results for different models. The best results with basic features are in bold. The
results are underlined if adding entity-entity features gives the overall best results.
Table 3, S-MART can achieve higher precision and
</tableCaption>
<bodyText confidence="0.98670247826087">
recall points compared to MART on all datasets
in terms of IE-driven evaluation, and can improve
F1 by 4 points on NEEL Test and TACL-IR
datasets. The task of entity linking is to produce
non-overlapping entity assignments that match the
gold mentions. By adopting structured learning
technique, S-MART is able to automatically
take into account the non-overlapping constraint
during learning and inference, and produce global
optimal entity assignments for mention candidates
of a tweet. One effect is that S-MART can easily
eliminate some common errors caused by popular
entities (e.g. new york in Figure 1).
Modeling entity-entity relationships Entity-
entity relationships provide strong clues for entity
disambiguation. In this paper, we use the sim-
ple two-stage approach described in Section 3.3
to capture the relationships between entities. As
shown in Table 3, the significant improvement in
IR-driven evaluation indicates the importance of
incorporating entity-entity information.
Interestingly, while IR-driven results are signif-
icantly improved, IE-driven results are similar or
even worse given entity-entity features. We be-
lieve the reason is that IE-driven and IR-driven
evaluations focus on different aspects of tweet en-
tity linking task. As Guo et al. (2013) shows
that most mentions in tweets should be linked to
the most popular entities, IE setting actually pays
more attention on mention detection sub-problem.
In contrast to IE setting, IR setting focuses on en-
tity disambiguation, since we only need to decide
whether the tweet is relevant to the query entity.
Therefore, we believe that both evaluation policies
are needed for tweet entity linking.
Balance Precision and Recall Figure 2 shows
the results of tuning the bias term for balancing
precision and recall on the dev set. The results
show that S-MART outperforms competitive ap-
proaches without any tuning, with similar margins
to the results after tuning. Balancing precision
and recall improves F1 scores for all the systems,
which suggests that the simple tuning method per-
forms quite well. Finally, we have an interest-
ing observation that different methods have vari-
ous scales of model scores.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999972857142857">
Linear structured learning methods have been pro-
posed and widely used in the literature. Popu-
lar models include Structured Perceptron (Collins,
2002), Conditional Random Field (Lafferty et al.,
2001) and Structured SVM (Taskar et al., 2004;
Tsochantaridis et al., 2005). Recently, many struc-
tured learning models based on neural networks
have been proposed and are widely used in lan-
guage modeling (Bengio et al., 2006; Mikolov
et al., 2010), sentiment classification (Socher et
al., 2013), as well as parsing (Socher et al.,
2011). Cortes et al. (2014) recently proposed a
boosting framework which treats different struc-
tured learning algorithms as base learners to en-
semble structured prediction results.
Tree-based models have been shown to pro-
vide more robust and accurate performances than
neural networks in some tasks of computer vi-
sion (Roe et al., 2005; Babenko et al., 2011)
and information retrieval (Li et al., 2007; Wu et
al., 2010), suggesting that it is worth to investi-
gate tree-based non-linear models for structured
learning problems. To the best of our knowl-
edge, TreeCRF (Dietterich et al., 2004) is the only
work that explores tree-based methods for struc-
tured learning problems. The relationships be-
tween TreeCRF and our work have been discussed
in Section 2.
</bodyText>
<page confidence="0.990566">
511
</page>
<bodyText confidence="0.999933735294118">
Early research on entity linking has focused
on well written documents (Bunescu and Pasca,
2006; Cucerzan, 2007; Milne and Witten, 2008).
Due to the raise of social media, many techniques
have been proposed or tailored to short texts in-
cluding tweets, for the problem of entity linking
(Ferragina and Scaiella, 2010; Meij et al., 2012;
Guo et al., 2013) as well as the related problem
of named entity recognition (NER) (Ritter et al.,
2011). Recently, non-textual information such as
spatial and temporal signals have also been used to
improve entity linking systems (Fang and Chang,
2014). The task of entity linking has attracted a
lot of attention, and many shared tasks have been
hosted to promote entity linking research (Ji et al.,
2010; Ji and Grishman, 2011; Cano and others,
2014; Carmel et al., 2014).
Building an end-to-end entity linking system in-
volves in solving two interrelated sub-problems:
mention detection and entity disambiguation. Ear-
lier research on entity linking has been largely fo-
cused on the entity disambiguation problem, in-
cluding most work on entity linking for well-
written documents such as news and encyclope-
dia articles (Cucerzan, 2007) and also few for
tweets (Liu et al., 2013). Recently, people have
focused on building systems that consider mention
detection and entity disambiguation jointly. For
example, Cucerzan (2012) delays the mention de-
tection decision and consider the mention detec-
tion and entity linking problem jointly. Similarly,
Sil and Yates (2013) proposed to use a reranking
approach to obtain overall better results on men-
tion detection and entity disambiguation.
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999010933333333">
In this paper, we propose S-MART, a family of
structured learning algorithms which is flexible on
the choices of the loss functions and structures.
We demonstrate the power of S-MART by applying
it to tweet entity linking, and it significantly out-
performs the current state-of-the-art entity linking
systems. In the future, we would like to investigate
the advantages and disadvantages between tree-
based models and other non-linear models such
as deep neural networks or recurrent neural net-
works.
Acknowledgments We thank the reviewers for
their insightful feedback. We also thank Yin Li
and Ana Smith for their valuable comments on
earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.994299" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995760615384615">
S. Asur and B.A. Huberman. 2010. Predict-
ing the future with social media. arXiv preprint
arXiv:1003.5699.
Boris Babenko, Ming-Hsuan Yang, and Serge Be-
longie. 2011. Robust object tracking with online
multiple instance learning. Pattern Analysis and
Machine Intelligence, IEEE Transactions on, pages
1619–1632.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137–186.
R. C Bunescu and M. Pasca. 2006. Using encyclo-
pedic knowledge for named entity disambiguation.
In Proceedings of the European Chapter of the ACL
(EACL), pages 9–16.
AE Cano et al. 2014. Microposts2014 neel challenge.
In Microposts2014 NEEL Challenge.
Amparo E Cano, Giuseppe Rizzo, Andrea Varga,
Matthew Rowe, Milan Stankovic, and Aba-Sah
Dadzie. 2014. Making sense of microposts (#
microposts2014) named entity extraction &amp; linking
challenge. Making Sense of Microposts (# Microp-
osts2014).
David Carmel, Ming-Wei Chang, Evgeniy Gabrilovich,
Bo-June Paul Hsu, and Kuansan Wang. 2014.
Erd’14: entity recognition and disambiguation chal-
lenge. In ACM SIGIR Forum, pages 63–77.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the conference on Empirical methods in natural
language processing (EMNLP), pages 1–8.
Corinna Cortes, Vitaly Kuznetsov, and Mehryar Mohri.
2014. Learning ensembles of structured prediction
rules. In Proceedings ofACL.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 708–
716.
Silviu Cucerzan. 2012. The msr system for entity link-
ing at tac 2012. In Text Analysis Conference.
Thomas G Dietterich, Adam Ashenfelter, and Yaroslav
Bulatov. 2004. Training conditional random fields
via gradient tree boosting. In Proceedings of the
twenty-first international conference on Machine
learning (ICML), pages 28–35.
Yuan Fang and Ming-Wei Chang. 2014. Entity link-
ing on microblogs with spatial and temporal signals.
Transactions of the Association for Computational
Linguistics (ACL), pages 259–272.
</reference>
<page confidence="0.969258">
512
</page>
<reference confidence="0.999810091743119">
P. Ferragina and U. Scaiella. 2010. TAGME: on-the-
fly annotation of short text fragments (by Wikipedia
entities). In Proceedings of ACM Conference on
Information and Knowledge Management (CIKM),
pages 1625–1628.
Jerome H Friedman. 2001. Greedy function approx-
imation: a gradient boosting machine. Annals of
Statistics, pages 1189–1232.
Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013. To link or not to link? a study on end-to-end
tweet entity linking. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 1020–1030.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, pages
1148–1158.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Third Text
Analysis Conference (TAC).
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th inter-
national conference on Machine learning (ICML),
pages 282–289.
Ping Li, Qiang Wu, and Christopher J Burges. 2007.
Mcrank: Learning to rank using multiple classifica-
tion and gradient boosting. In Advances in neural
information processing systems (NIPS), pages 897–
904.
Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou,
Furu Wei, and Yi Lu. 2013. Entity linking for
tweets. In Proceedings of the Association for Com-
putational Linguistics (ACL), pages 1304–1311.
Michael Mathioudakis and Nick Koudas. 2010. Twit-
termonitor: trend detection over the twitter stream.
In Proceedings of the 2010 ACM SIGMOD Inter-
national Conference on Management of data (SIG-
MOD), pages 1155–1158.
E. Meij, W. Weerkamp, and M. de Rijke. 2012.
Adding semantics to microblog posts. In Proceed-
ings of International Conference on Web Search and
Web Data Mining (WSDM), pages 563–572.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
D. Milne and I. H. Witten. 2008. Learning to link with
Wikipedia. In Proceedings of ACM Conference on
Information and Knowledge Management (CIKM),
pages 509–518.
Kevin P Murphy. 2012. Machine learning: a proba-
bilistic perspective. MIT press.
C Quoc and Viet Le. 2007. Learning to rank with
nonsmooth cost functions. pages 193–200.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: an experimen-
tal study. In Proceedings of the Conference on Em-
pirical Methods for Natural Language Processing
(EMNLP), pages 1524–1534.
Byron P Roe, Hai-Jun Yang, Ji Zhu, Yong Liu, Ion
Stancu, and Gordon McGregor. 2005. Boosted de-
cision trees as an alternative to artificial neural net-
works for particle identification. Nuclear Instru-
ments and Methods in Physics Research Section A:
Accelerators, Spectrometers, Detectors and Associ-
ated Equipment, pages 577–584.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 1185–1192.
Avirup Sil and Alexander Yates. 2013. Re-ranking
for joint named-entity recognition and linking. In
Proceedings of ACM Conference on Information
and Knowledge Management (CIKM), pages 2369–
2374.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML), pages 129–136.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.
Ben Taskar, Carlos Guestrin, and Daphne Roller. 2004.
Max-margin markov networks. Advances in neural
information processing systems, 16:25.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. In Proceedings of the twenty-
first international conference on Machine learning
(ICML), page 104.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning Re-
search, pages 1453–1484.
Qiang Wu, Christopher JC Burges, Krysta M Svore,
and Jianfeng Gao. 2010. Adapting boosting for
information retrieval measures. Information Re-
trieval, pages 254–270.
</reference>
<page confidence="0.998859">
513
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989428">
<title confidence="0.998302">Novel Tree-based Structured Learning Applied to Tweet Entity Linking</title>
<author confidence="0.997644">Yi</author>
<affiliation confidence="0.9996465">School of Interactive Georgia Institute of</affiliation>
<email confidence="0.998763">yiyang@gatech.edu</email>
<abstract confidence="0.999871115384615">Non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features. However, tree-based models are seldom studied in the context of structured learning despite their recent success on various classification and ranking tasks. In this paper, we propose a tree-based structured learning framework based on multiple additive retrees. especially suitable for handling tasks with dense features, and can be used to learn many different structures under various loss functions. apply the task of tweet entity linking — a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base. A novel inference algorithm is proposed to handle the special structure of the task. The experresults show that significantly outperforms state-of-the-art tweet entity linking systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Asur</author>
<author>B A Huberman</author>
</authors>
<title>Predicting the future with social media. arXiv preprint arXiv:1003.5699.</title>
<date>2010</date>
<contexts>
<context position="3098" citStr="Asur and Huberman, 2010" startWordPosition="464" endWordPosition="467">of TreeCRF (Dietterich et al., 2004). In this paper, we propose a novel structured learning framework called S-MART (Structured Multiple Additive Regression Trees). Unlike TreeCRF, S-MART is very versatile, as it can be applied to tasks beyond sequence tagging and can be trained under various objective functions. SMART is also powerful, as the high order relationships between features can be captured by nonlinear regression trees. We further demonstrate how S-MART can be applied to tweet entity linking, an important and challenging task underlying many applications including product feedback (Asur and Huberman, 2010) and topic detection and tracking (Mathioudakis and Koudas, 2010). We apply S-MART to entity linking using a simple logistic function as the loss function and propose a novel inference algorithm to prevent overlaps between entities. Our contributions are summarized as follows: • We propose a novel structured learning framework called S-MART. S-MART combines non-linearity and efficiency of treebased models with structured prediction, leading to a family of new algorithms. (Section 2) 504 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Internat</context>
</contexts>
<marker>Asur, Huberman, 2010</marker>
<rawString>S. Asur and B.A. Huberman. 2010. Predicting the future with social media. arXiv preprint arXiv:1003.5699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Babenko</author>
<author>Ming-Hsuan Yang</author>
<author>Serge Belongie</author>
</authors>
<title>Robust object tracking with online multiple instance learning.</title>
<date>2011</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<pages>1619--1632</pages>
<contexts>
<context position="30987" citStr="Babenko et al., 2011" startWordPosition="5142" endWordPosition="5145"> 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been </context>
</contexts>
<marker>Babenko, Yang, Belongie, 2011</marker>
<rawString>Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. 2011. Robust object tracking with online multiple instance learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, pages 1619–1632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the ACL (EACL),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="31484" citStr="Bunescu and Pasca, 2006" startWordPosition="5223" endWordPosition="5226">obust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to p</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. C Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the European Chapter of the ACL (EACL), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AE Cano</author>
</authors>
<date>2014</date>
<booktitle>Microposts2014 neel challenge. In Microposts2014 NEEL Challenge.</booktitle>
<marker>Cano, 2014</marker>
<rawString>AE Cano et al. 2014. Microposts2014 neel challenge. In Microposts2014 NEEL Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amparo E Cano</author>
<author>Giuseppe Rizzo</author>
<author>Andrea Varga</author>
<author>Matthew Rowe</author>
<author>Milan Stankovic</author>
<author>Aba-Sah Dadzie</author>
</authors>
<date>2014</date>
<booktitle>Making sense of microposts (# microposts2014) named entity extraction &amp; linking challenge. Making Sense of Microposts (# Microposts2014).</booktitle>
<contexts>
<context position="18247" citStr="Cano et al., 2014" startWordPosition="3048" endWordPosition="3051">learning algorithms perform better than linear learning algorithms? • Do structured entity linking models perform better than non-structured ones? • How can we best capture the relationships between entities? 4.1 Evaluation Methodology and Data We evaluate each entity linking system using two evaluation policies: Information Extraction (IE) driven evaluation and Information Retrieval (IR) driven evaluation. For both evaluation settings, precision, recall and F1 scores are reported. Our data is constructed from two publicly available sources: Named Entity Extraction &amp; Linking (NEEL) Challenge (Cano et al., 2014) datasets, and the datasets released by Fang and Chang (2014). Note that we gather two datasets from Fang and Chang (2014) and they are used in two different evaluation settings. We refer to these two datasets as TACL-IE and TACL-IR, respectively. We perform some data cleaning and unification on 508 these sets.7 The statistics of the datasets are presented in Table 1. IE-driven evaluation The IE-driven evaluation is the standard evaluation for an end-to-end entity linking system. We follow Carmel et al. (2014) and relax the definition of the correct mention boundaries, as they are often ambigu</context>
</contexts>
<marker>Cano, Rizzo, Varga, Rowe, Stankovic, Dadzie, 2014</marker>
<rawString>Amparo E Cano, Giuseppe Rizzo, Andrea Varga, Matthew Rowe, Milan Stankovic, and Aba-Sah Dadzie. 2014. Making sense of microposts (# microposts2014) named entity extraction &amp; linking challenge. Making Sense of Microposts (# Microposts2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Carmel</author>
<author>Ming-Wei Chang</author>
<author>Evgeniy Gabrilovich</author>
<author>Bo-June Paul Hsu</author>
<author>Kuansan Wang</author>
</authors>
<title>Erd’14: entity recognition and disambiguation challenge.</title>
<date>2014</date>
<booktitle>In ACM SIGIR Forum,</booktitle>
<pages>63--77</pages>
<contexts>
<context position="18762" citStr="Carmel et al. (2014)" startWordPosition="3132" endWordPosition="3135">om two publicly available sources: Named Entity Extraction &amp; Linking (NEEL) Challenge (Cano et al., 2014) datasets, and the datasets released by Fang and Chang (2014). Note that we gather two datasets from Fang and Chang (2014) and they are used in two different evaluation settings. We refer to these two datasets as TACL-IE and TACL-IR, respectively. We perform some data cleaning and unification on 508 these sets.7 The statistics of the datasets are presented in Table 1. IE-driven evaluation The IE-driven evaluation is the standard evaluation for an end-to-end entity linking system. We follow Carmel et al. (2014) and relax the definition of the correct mention boundaries, as they are often ambiguous. A mention boundary is considered to be correct if it overlaps (instead of being the same) with the gold mention boundary. Please see (Carmel et al., 2014) for more details on the procedure of calculating the precision, recall and F1 score. The NEEL and TACL-IE datasets have different annotation guidelines and different choices of knowledge bases, so we perform the following procedure to clean the data and unify the annotations. We first filter out the annotations that link to entities excluded by our know</context>
<context position="32199" citStr="Carmel et al., 2014" startWordPosition="5343" endWordPosition="5346">ve been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) delays the mention detection decision and conside</context>
</contexts>
<marker>Carmel, Chang, Gabrilovich, Hsu, Wang, 2014</marker>
<rawString>David Carmel, Ming-Wei Chang, Evgeniy Gabrilovich, Bo-June Paul Hsu, and Kuansan Wang. 2014. Erd’14: entity recognition and disambiguation challenge. In ACM SIGIR Forum, pages 63–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the conference on Empirical methods in natural language processing (EMNLP),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21819" citStr="Collins, 2002" startWordPosition="3644" endWordPosition="3645"> statistical features such as the probability of the surface to be used as anchor text in Wikipedia. We also add additional Entity Type features correspond to the following entity types: Character, Event, Product and Brand. Finally, we include several NER features to indicate each mention candidate belongs to one the following NER types: Twitter user, Twitter hashtag, Person, Location, Organization, Product, Event and Date. Algorithms Table 2 summarizes all the algorithms that are compared in our experiments. First, we consider two linear structured learning algorithms: Structured Perceptron (Collins, 2002) and Linear Structured SVM (SSVM) (Tsochantaridis et al., 2004). For non-linear models, we consider polynomial SSVM, which employs polynomial kernel inside the structured SVM algorithm. We also include LambdaRank (Quoc and Le, 2007), a neuralbased learning to rank algorithm, which is widely used in the information retrieval literature. We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured information. Finally, we have our proposed log-loss SMART algorithm, as described in Section 3. 9 Note that our baseline sy</context>
<context position="30252" citStr="Collins, 2002" startWordPosition="5027" endWordPosition="5028">of tuning the bias term for balancing precision and recall on the dev set. The results show that S-MART outperforms competitive approaches without any tuning, with similar margins to the results after tuning. Balancing precision and recall improves F1 scores for all the systems, which suggests that the simple tuning method performs quite well. Finally, we have an interesting observation that different methods have various scales of model scores. 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the conference on Empirical methods in natural language processing (EMNLP), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vitaly Kuznetsov</author>
<author>Mehryar Mohri</author>
</authors>
<title>Learning ensembles of structured prediction rules.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="30656" citStr="Cortes et al. (2014)" startWordPosition="5090" endWordPosition="5093">ferent methods have various scales of model scores. 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-</context>
</contexts>
<marker>Cortes, Kuznetsov, Mohri, 2014</marker>
<rawString>Corinna Cortes, Vitaly Kuznetsov, and Mehryar Mohri. 2014. Learning ensembles of structured prediction rules. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>708--716</pages>
<contexts>
<context position="31500" citStr="Cucerzan, 2007" startWordPosition="5227" endWordPosition="5228">mances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity li</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708– 716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>The msr system for entity linking at tac 2012.</title>
<date>2012</date>
<booktitle>In Text Analysis Conference.</booktitle>
<contexts>
<context position="32749" citStr="Cucerzan (2012)" startWordPosition="5427" endWordPosition="5428">i and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) delays the mention detection decision and consider the mention detection and entity linking problem jointly. Similarly, Sil and Yates (2013) proposed to use a reranking approach to obtain overall better results on mention detection and entity disambiguation. 6 Conclusion and Future Work In this paper, we propose S-MART, a family of structured learning algorithms which is flexible on the choices of the loss functions and structures. We demonstrate the power of S-MART by applying it to tweet entity linking, and it significantly outperforms the current state-of-the-art entity linking systems. In</context>
</contexts>
<marker>Cucerzan, 2012</marker>
<rawString>Silviu Cucerzan. 2012. The msr system for entity linking at tac 2012. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
<author>Adam Ashenfelter</author>
<author>Yaroslav Bulatov</author>
</authors>
<title>Training conditional random fields via gradient tree boosting.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning (ICML),</booktitle>
<pages>28--35</pages>
<contexts>
<context position="2510" citStr="Dietterich et al., 2004" startWordPosition="374" endWordPosition="377">es. Tasks with these low dimensional dense features require models to be more sophisticated to capture the relationships between features. Therefore, non-linear models start to receive more attention as they are often more expressive than linear models. Tree-based models such as boosted trees (Friedman, 2001) are flexible non-linear models. They can handle categorical features and count data better than other non-linear models like Neural Networks. Unfortunately, to the best of our knowledge, little work has utilized tree-based methods for structured prediction, with the exception of TreeCRF (Dietterich et al., 2004). In this paper, we propose a novel structured learning framework called S-MART (Structured Multiple Additive Regression Trees). Unlike TreeCRF, S-MART is very versatile, as it can be applied to tasks beyond sequence tagging and can be trained under various objective functions. SMART is also powerful, as the high order relationships between features can be captured by nonlinear regression trees. We further demonstrate how S-MART can be applied to tweet entity linking, an important and challenging task underlying many applications including product feedback (Asur and Huberman, 2010) and topic d</context>
<context position="9770" citStr="Dietterich et al., 2004" startWordPosition="1606" endWordPosition="1609">. S-MART versus MART There are two key differences between S-MART and MART. First, S-MART decomposes the joint scoring function S(x, y) into factors to address the problem of the exploding number of input-output pairs for structured learning problems. Second, S-MART models a single scoring function F(x, yk) over inputs and output variables directly rather than O different functions F°(x), each of which corresponds to a label class. S-MART versus TreeCRF TreeCRF can be viewed as a special case of S-MART, and there are two points where S-MART improves upon TreeCRF. First, the model designed in (Dietterich et al., 2004) is tailored for sequence tagging problems. Similar to MART, for a tagging task with O tags, they choose to model O functions F°(x, o&apos;) instead of directly modeling the joint score of the factor. This imposes limitations on the feature functions, and TreeCRF is consequently unsuitable for many tasks such as entity linking.1Second, S-MART is more general in terms of the objective functions and applicable structures. In the next section, we will see how S-MART can be applied to a non-linear-chain structure and various loss functions. 3 S-MART for Tweet Entity Linking We first formally define the</context>
<context position="31219" citStr="Dietterich et al., 2004" startWordPosition="5180" endWordPosition="5183">s well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et </context>
</contexts>
<marker>Dietterich, Ashenfelter, Bulatov, 2004</marker>
<rawString>Thomas G Dietterich, Adam Ashenfelter, and Yaroslav Bulatov. 2004. Training conditional random fields via gradient tree boosting. In Proceedings of the twenty-first international conference on Machine learning (ICML), pages 28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Fang</author>
<author>Ming-Wei Chang</author>
</authors>
<title>Entity linking on microblogs with spatial and temporal signals.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (ACL),</journal>
<pages>259--272</pages>
<contexts>
<context position="18308" citStr="Fang and Chang (2014)" startWordPosition="3058" endWordPosition="3061">gorithms? • Do structured entity linking models perform better than non-structured ones? • How can we best capture the relationships between entities? 4.1 Evaluation Methodology and Data We evaluate each entity linking system using two evaluation policies: Information Extraction (IE) driven evaluation and Information Retrieval (IR) driven evaluation. For both evaluation settings, precision, recall and F1 scores are reported. Our data is constructed from two publicly available sources: Named Entity Extraction &amp; Linking (NEEL) Challenge (Cano et al., 2014) datasets, and the datasets released by Fang and Chang (2014). Note that we gather two datasets from Fang and Chang (2014) and they are used in two different evaluation settings. We refer to these two datasets as TACL-IE and TACL-IR, respectively. We perform some data cleaning and unification on 508 these sets.7 The statistics of the datasets are presented in Table 1. IE-driven evaluation The IE-driven evaluation is the standard evaluation for an end-to-end entity linking system. We follow Carmel et al. (2014) and relax the definition of the correct mention boundaries, as they are often ambiguous. A mention boundary is considered to be correct if it ove</context>
<context position="20031" citStr="Fang and Chang (2014)" startWordPosition="3345" endWordPosition="3348">e ERD 2014 competition (Carmel et al., 2014), which includes the union of entities in Wikipedia and Freebase. Second, we follow NEEL annotation guideline and re-annotate TACL-IE dataset. For instance, in order to be consistent with NEEL, all the user tags (e.g. @BarackObama) are re-labeled as entities in TACL-IE. We train all the models with NEEL Train dataset and evaluate different systems on NEEL Test and TACL-IE datasets. In addition, we sample 800 tweets from NEEL Train dataset as our development set to perform parameter tuning. IR-driven evaluation The IR-driven evaluation is proposed by Fang and Chang (2014). It is motivated by a key application of entity linking — retrieval of relevant tweets for target entities, which is crucial for downstream applications such as product research and sentiment analysis. In particular, given a query entity we can search for tweets based on the match with some potential surface forms of the query entity. Then, an entity linking system is evaluated by its ability to correctly identify the presence or absence of the query entity in every tweet. Our IR-driven evaluation is based on the TACL-IR set, which includes 980 tweets sampled for ten query entities of five en</context>
<context position="31978" citStr="Fang and Chang, 2014" startWordPosition="5304" endWordPosition="5307">een discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for t</context>
</contexts>
<marker>Fang, Chang, 2014</marker>
<rawString>Yuan Fang and Ming-Wei Chang. 2014. Entity linking on microblogs with spatial and temporal signals. Transactions of the Association for Computational Linguistics (ACL), pages 259–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ferragina</author>
<author>U Scaiella</author>
</authors>
<title>TAGME: on-thefly annotation of short text fragments (by Wikipedia entities).</title>
<date>2010</date>
<booktitle>In Proceedings of ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>1625--1628</pages>
<contexts>
<context position="31704" citStr="Ferragina and Scaiella, 2010" startWordPosition="5259" endWordPosition="5262">h to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>P. Ferragina and U. Scaiella. 2010. TAGME: on-thefly annotation of short text fragments (by Wikipedia entities). In Proceedings of ACM Conference on Information and Knowledge Management (CIKM), pages 1625–1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Greedy function approximation: a gradient boosting machine. Annals of Statistics,</title>
<date>2001</date>
<pages>1189--1232</pages>
<contexts>
<context position="2196" citStr="Friedman, 2001" startWordPosition="327" endWordPosition="329">s such as part-of-speech tagging, named entity recognition and dependency parsing. This classical combination of linear models and sparse features is challenged by the recent emergMing-Wei Chang Microsoft Research minchang@microsoft.com ing usage of dense features such as statistical and embedding features. Tasks with these low dimensional dense features require models to be more sophisticated to capture the relationships between features. Therefore, non-linear models start to receive more attention as they are often more expressive than linear models. Tree-based models such as boosted trees (Friedman, 2001) are flexible non-linear models. They can handle categorical features and count data better than other non-linear models like Neural Networks. Unfortunately, to the best of our knowledge, little work has utilized tree-based methods for structured prediction, with the exception of TreeCRF (Dietterich et al., 2004). In this paper, we propose a novel structured learning framework called S-MART (Structured Multiple Additive Regression Trees). Unlike TreeCRF, S-MART is very versatile, as it can be applied to tasks beyond sequence tagging and can be trained under various objective functions. SMART i</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages 1189–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Guo</author>
<author>Ming-Wei Chang</author>
<author>Emre Kiciman</author>
</authors>
<title>To link or not to link? a study on end-to-end tweet entity linking.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>1020--1030</pages>
<contexts>
<context position="16979" citStr="Guo et al. (2013)" startWordPosition="2845" endWordPosition="2848">ry mention candidate of the tweet overlaps other mention candidates. In practice, the algorithm is nearly linear w.r.t K. two-stage approach as the solution for modeling entity-entity relationships after we found that SMART achieves high precision and reasonable recall. Specifically, in the first stage, the system identifies all possible entities with basic features, which enables the extraction of entity-entity features. In the second stage, we re-train S-MART on a union of basic features and entity-entity features. We define entity-entity features based on the Jaccard distance introduced by Guo et al. (2013). Let Γ(ei) denotes the set of Wikipedia pages that contain a hyperlink to an entity ei and Γ(t−i) denotes the set of pages that contain a hyperlink to any identified entity ej of the tweet t in the first stage excluding ei. The Jaccard distance between ei and t is |Γ(ei) n Γ(t−i)| Jac(ei, t) = |Γ(ei) U Γ(t−i)|. In addition to the Jaccard distance, we add one additional binary feature to indicate if the current entity has the highest Jaccard distance among all entities for this mention candidate. 4 Experiments Our experiments are designed to answer the following three research questions in the</context>
<context position="21185" citStr="Guo et al., 2013" startWordPosition="3547" endWordPosition="3550">h includes 980 tweets sampled for ten query entities of five entity types (roughly 100 tweets per entity). About 37% of the sampled tweets did not mention the query entity due to the anchor ambiguity. 7We plan to release the cleaned data and evaluation code if license permitted. Data #Tweet #Entity Date NEEL Train 2340 2202 Jul. ˜Aug. 11 NEEL Test 1164 687 Jul. ˜Aug. 11 TACL-IE 500 300 Dec. 12 TACL-IR 980 NA Dec. 12 Table 1: Statistics of data sets. 4.2 Experimental Settings Features We employ a total number of 37 dense features as our basic feature set. Most of the features are adopted from (Guo et al., 2013)8, including various statistical features such as the probability of the surface to be used as anchor text in Wikipedia. We also add additional Entity Type features correspond to the following entity types: Character, Event, Product and Brand. Finally, we include several NER features to indicate each mention candidate belongs to one the following NER types: Twitter user, Twitter hashtag, Person, Location, Organization, Product, Event and Date. Algorithms Table 2 summarizes all the algorithms that are compared in our experiments. First, we consider two linear structured learning algorithms: Str</context>
<context position="22546" citStr="Guo et al., 2013" startWordPosition="3754" endWordPosition="3757"> SSVM, which employs polynomial kernel inside the structured SVM algorithm. We also include LambdaRank (Quoc and Le, 2007), a neuralbased learning to rank algorithm, which is widely used in the information retrieval literature. We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured information. Finally, we have our proposed log-loss SMART algorithm, as described in Section 3. 9 Note that our baseline systems are quite strong. Linear SSVM has been used in one of the stateof-the-art tweet entity linking systems (Guo et al., 2013), and the system based on MART is the winning system of the 2014 NEEL Challenge (Cano and others, 2014)10. Table 2 summarizes several properties of the algorithms. For example, most algorithms are struc8We consider features of Base, Capitalization Rate, Popularity, Context Capitalization and Entity Type categories. 9Our pilot experiments show that the log-loss SMART consistently outperforms the hinge-loss S-MART. 10Note that the numbers we reported here are different from the results in NEEL challenge due to the fact that we have cleaned the datasets and the evaluation metrics are slightly dif</context>
<context position="29184" citStr="Guo et al. (2013)" startWordPosition="4854" endWordPosition="4857">tity relationships provide strong clues for entity disambiguation. In this paper, we use the simple two-stage approach described in Section 3.3 to capture the relationships between entities. As shown in Table 3, the significant improvement in IR-driven evaluation indicates the importance of incorporating entity-entity information. Interestingly, while IR-driven results are significantly improved, IE-driven results are similar or even worse given entity-entity features. We believe the reason is that IE-driven and IR-driven evaluations focus on different aspects of tweet entity linking task. As Guo et al. (2013) shows that most mentions in tweets should be linked to the most popular entities, IE setting actually pays more attention on mention detection sub-problem. In contrast to IE setting, IR setting focuses on entity disambiguation, since we only need to decide whether the tweet is relevant to the query entity. Therefore, we believe that both evaluation policies are needed for tweet entity linking. Balance Precision and Recall Figure 2 shows the results of tuning the bias term for balancing precision and recall on the dev set. The results show that S-MART outperforms competitive approaches without</context>
<context position="31742" citStr="Guo et al., 2013" startWordPosition="5267" endWordPosition="5270"> structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. </context>
</contexts>
<marker>Guo, Chang, Kiciman, 2013</marker>
<rawString>Stephen Guo, Ming-Wei Chang, and Emre Kiciman. 2013. To link or not to link? a study on end-to-end tweet entity linking. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 1020–1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1148--1158</pages>
<contexts>
<context position="32154" citStr="Ji and Grishman, 2011" startWordPosition="5335" endWordPosition="5338"> the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) dela</context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>Heng Ji and Ralph Grishman. 2011. Knowledge base population: Successful approaches and challenges. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1148–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa Trang Dang</author>
<author>Kira Griffitt</author>
<author>Joe Ellis</author>
</authors>
<title>Overview of the tac 2010 knowledge base population track.</title>
<date>2010</date>
<booktitle>In Third Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="32131" citStr="Ji et al., 2010" startWordPosition="5331" endWordPosition="5334">en, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For exampl</context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2010</marker>
<rawString>Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis. 2010. Overview of the tac 2010 knowledge base population track. In Third Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th international conference on Machine learning (ICML),</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1358" citStr="Lafferty et al., 2001" startWordPosition="204" endWordPosition="208">ous loss functions. We apply S-MART to the task of tweet entity linking — a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base. A novel inference algorithm is proposed to handle the special structure of the task. The experimental results show that S-MART significantly outperforms state-of-the-art tweet entity linking systems. 1 Introduction Many natural language processing (NLP) problems can be formalized as structured prediction tasks. Standard algorithms for structured learning include Conditional Random Field (CRF) (Lafferty et al., 2001) and Structured Supported Vector Machine (SSVM) (Tsochantaridis et al., 2004). These algorithms, usually equipped with a linear model and sparse lexical features, achieve stateof-the-art performances in many NLP applications such as part-of-speech tagging, named entity recognition and dependency parsing. This classical combination of linear models and sparse features is challenged by the recent emergMing-Wei Chang Microsoft Research minchang@microsoft.com ing usage of dense features such as statistical and embedding features. Tasks with these low dimensional dense features require models to be</context>
<context position="30302" citStr="Lafferty et al., 2001" startWordPosition="5032" endWordPosition="5035">ision and recall on the dev set. The results show that S-MART outperforms competitive approaches without any tuning, with similar margins to the results after tuning. Balancing precision and recall improves F1 scores for all the systems, which suggests that the simple tuning method performs quite well. Finally, we have an interesting observation that different methods have various scales of model scores. 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neur</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th international conference on Machine learning (ICML), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Qiang Wu</author>
<author>Christopher J Burges</author>
</authors>
<title>Mcrank: Learning to rank using multiple classification and gradient boosting.</title>
<date>2007</date>
<booktitle>In Advances in neural information processing systems (NIPS),</booktitle>
<pages>897--904</pages>
<contexts>
<context position="31030" citStr="Li et al., 2007" startWordPosition="5149" endWordPosition="5152"> based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts includi</context>
</contexts>
<marker>Li, Wu, Burges, 2007</marker>
<rawString>Ping Li, Qiang Wu, and Christopher J Burges. 2007. Mcrank: Learning to rank using multiple classification and gradient boosting. In Advances in neural information processing systems (NIPS), pages 897– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Yitong Li</author>
<author>Haocheng Wu</author>
<author>Ming Zhou</author>
<author>Furu Wei</author>
<author>Yi Lu</author>
</authors>
<title>Entity linking for tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1304--1311</pages>
<contexts>
<context position="32602" citStr="Liu et al., 2013" startWordPosition="5406" endWordPosition="5409">ask of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) delays the mention detection decision and consider the mention detection and entity linking problem jointly. Similarly, Sil and Yates (2013) proposed to use a reranking approach to obtain overall better results on mention detection and entity disambiguation. 6 Conclusion and Future Work In this paper, we propose S-MART, a family of structured learning algorithms which is flexible on the choices of the loss functions and structures. We demonstrate t</context>
</contexts>
<marker>Liu, Li, Wu, Zhou, Wei, Lu, 2013</marker>
<rawString>Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, Furu Wei, and Yi Lu. 2013. Entity linking for tweets. In Proceedings of the Association for Computational Linguistics (ACL), pages 1304–1311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mathioudakis</author>
<author>Nick Koudas</author>
</authors>
<title>Twittermonitor: trend detection over the twitter stream.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data (SIGMOD),</booktitle>
<pages>1155--1158</pages>
<contexts>
<context position="3163" citStr="Mathioudakis and Koudas, 2010" startWordPosition="473" endWordPosition="477">pose a novel structured learning framework called S-MART (Structured Multiple Additive Regression Trees). Unlike TreeCRF, S-MART is very versatile, as it can be applied to tasks beyond sequence tagging and can be trained under various objective functions. SMART is also powerful, as the high order relationships between features can be captured by nonlinear regression trees. We further demonstrate how S-MART can be applied to tweet entity linking, an important and challenging task underlying many applications including product feedback (Asur and Huberman, 2010) and topic detection and tracking (Mathioudakis and Koudas, 2010). We apply S-MART to entity linking using a simple logistic function as the loss function and propose a novel inference algorithm to prevent overlaps between entities. Our contributions are summarized as follows: • We propose a novel structured learning framework called S-MART. S-MART combines non-linearity and efficiency of treebased models with structured prediction, leading to a family of new algorithms. (Section 2) 504 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 504–</context>
</contexts>
<marker>Mathioudakis, Koudas, 2010</marker>
<rawString>Michael Mathioudakis and Nick Koudas. 2010. Twittermonitor: trend detection over the twitter stream. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data (SIGMOD), pages 1155–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Meij</author>
<author>W Weerkamp</author>
<author>M de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>563--572</pages>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>E. Meij, W. Weerkamp, and M. de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of International Conference on Web Search and Web Data Mining (WSDM), pages 563–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
<author>I H Witten</author>
</authors>
<title>Learning to link with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>509--518</pages>
<contexts>
<context position="31525" citStr="Milne and Witten, 2008" startWordPosition="5229" endWordPosition="5232">al networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al.</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>D. Milne and I. H. Witten. 2008. Learning to link with Wikipedia. In Proceedings of ACM Conference on Information and Knowledge Management (CIKM), pages 509–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
</authors>
<title>Machine learning: a probabilistic perspective.</title>
<date>2012</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="7184" citStr="Murphy, 2012" startWordPosition="1150" endWordPosition="1151">ically requires an infinite number of data points. We can address this difficulty by approximating gm with a finite number of point-wise functional gradients gm(x,yk = uk) = (3) raL(y*, S(x, yk = uk))1 L ∂F(x,yk = uk) J F(xryk)=Fm−1(xryk) where uk index a valid sub-structure for the k-th factor of x. The key point of S-MART is that it approximates −gm by modeling the point-wise negative functional gradients using a regression tree hm. Then the factor scoring function can be obtained by M F(x, yk) = ηmhm(x, yk), m=1 where hm(x, yk) is also called a basis function and ηm can be simply set to 1 (Murphy, 2012). The detailed S-MART algorithm is presented in Algorithm 1. The factor scoring function F(x, yk) is simply initialized to zero at first (line 1). After this, we iteratively update the function by adding regression trees. Note that the scoring function is shared by all the factors. Specifically, given the current decision function Fm−1, we can consider line 3 to line 9 a process of generating the pseudo 505 Algorithm 1 S-MART: A family of structured learning algorithms with multiple additive regression trees 1: F0(x, yk) = 0 2: form = 1 to M do: &gt; going over all trees 3: D +— 0 4: for all exam</context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>Kevin P Murphy. 2012. Machine learning: a probabilistic perspective. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quoc</author>
<author>Viet Le</author>
</authors>
<title>Learning to rank with nonsmooth cost functions.</title>
<date>2007</date>
<pages>193--200</pages>
<contexts>
<context position="22051" citStr="Quoc and Le, 2007" startWordPosition="3675" endWordPosition="3678">lly, we include several NER features to indicate each mention candidate belongs to one the following NER types: Twitter user, Twitter hashtag, Person, Location, Organization, Product, Event and Date. Algorithms Table 2 summarizes all the algorithms that are compared in our experiments. First, we consider two linear structured learning algorithms: Structured Perceptron (Collins, 2002) and Linear Structured SVM (SSVM) (Tsochantaridis et al., 2004). For non-linear models, we consider polynomial SSVM, which employs polynomial kernel inside the structured SVM algorithm. We also include LambdaRank (Quoc and Le, 2007), a neuralbased learning to rank algorithm, which is widely used in the information retrieval literature. We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured information. Finally, we have our proposed log-loss SMART algorithm, as described in Section 3. 9 Note that our baseline systems are quite strong. Linear SSVM has been used in one of the stateof-the-art tweet entity linking systems (Guo et al., 2013), and the system based on MART is the winning system of the 2014 NEEL Challenge (Cano and others, 2014)10</context>
</contexts>
<marker>Quoc, Le, 2007</marker>
<rawString>C Quoc and Viet Le. 2007. Learning to rank with nonsmooth cost functions. pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>1524--1534</pages>
<contexts>
<context position="31829" citStr="Ritter et al., 2011" startWordPosition="5282" endWordPosition="5285">al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguatio</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 1524–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byron P Roe</author>
<author>Hai-Jun Yang</author>
<author>Ji Zhu</author>
<author>Yong Liu</author>
<author>Ion Stancu</author>
<author>Gordon McGregor</author>
</authors>
<title>Boosted decision trees as an alternative to artificial neural networks for particle identification. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment,</title>
<date>2005</date>
<pages>577--584</pages>
<contexts>
<context position="30964" citStr="Roe et al., 2005" startWordPosition="5138" endWordPosition="5141">hantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, man</context>
</contexts>
<marker>Roe, Yang, Zhu, Liu, Stancu, McGregor, 2005</marker>
<rawString>Byron P Roe, Hai-Jun Yang, Ji Zhu, Yong Liu, Ion Stancu, and Gordon McGregor. 2005. Boosted decision trees as an alternative to artificial neural networks for particle identification. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>1185--1192</pages>
<contexts>
<context position="9146" citStr="Sarawagi and Cohen, 2004" startWordPosition="1504" endWordPosition="1507">etween the prediction values and the point-wise negative gradients. Then a new basis function (modeled by a regression tree) will be added into the overall F (line 11). It is crucial to note that S-MART is a family of algorithms rather than a single algorithm. S-MART is flexible in the choice of the loss functions. For example, we can use either logistic loss or hinge loss, which means that SMART can train probabilistic models as well as non-probabilistic ones. Depending on the choice of factors, S-MART can handle various structures such as linear chains, trees, and even the semiMarkov chain (Sarawagi and Cohen, 2004). S-MART versus MART There are two key differences between S-MART and MART. First, S-MART decomposes the joint scoring function S(x, y) into factors to address the problem of the exploding number of input-output pairs for structured learning problems. Second, S-MART models a single scoring function F(x, yk) over inputs and output variables directly rather than O different functions F°(x), each of which corresponds to a label class. S-MART versus TreeCRF TreeCRF can be viewed as a special case of S-MART, and there are two points where S-MART improves upon TreeCRF. First, the model designed in (</context>
<context position="14045" citStr="Sarawagi and Cohen, 2004" startWordPosition="2342" endWordPosition="2345">e down the logistic loss function as L(y∗, S(x, y)) = − log P(y∗|x) =log Z(x) − S(x, y∗) where Z(x) = Ey exp(S(x, y)) is the potential function. Then the point-wise gradients can be computed as ∂L ∂F(x, yk = uk) = P(yk = uk|x) − 1[y∗k = uk], where 1[·] represents an indicator function. The conditional probability P (yk = uk|x) can be computed by a variant of the forward-backward algorithm, which we will detail in the next subsection. 4Note that each mention candidate has different own entity sets. 3.2 Inference The non-overlapping structure is distinct from linear chain and semi-Markov chain (Sarawagi and Cohen, 2004) structures. Hence, we propose a carefully designed forward-backward algorithm to calculate P(yk = uk|x) based on current scoring function F(x, yk = uk) given by the regression trees. The non-overlapping constraint distinguishes our inference algorithm from other forward-backward variants. To compute the forward probability, we sort5 the mention candidates by their end indices and define forward recursion by α(u1,1) = exp(F(x, y1 = u1)) α(uk, k) = exp(F(x, yk = uk)) · P�− 1 exp(F(x, yk−p = Nil)) p=1 1: · α(uk−P, k − P) (4) uk−P where k − P is the index of the previous nonoverlapping mention ca</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W Cohen. 2004. Semimarkov conditional random fields for information extraction. In Advances in Neural Information Processing Systems (NIPS), pages 1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avirup Sil</author>
<author>Alexander Yates</author>
</authors>
<title>Re-ranking for joint named-entity recognition and linking.</title>
<date>2013</date>
<booktitle>In Proceedings of ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>2369--2374</pages>
<contexts>
<context position="32890" citStr="Sil and Yates (2013)" startWordPosition="5447" endWordPosition="5450">interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) delays the mention detection decision and consider the mention detection and entity linking problem jointly. Similarly, Sil and Yates (2013) proposed to use a reranking approach to obtain overall better results on mention detection and entity disambiguation. 6 Conclusion and Future Work In this paper, we propose S-MART, a family of structured learning algorithms which is flexible on the choices of the loss functions and structures. We demonstrate the power of S-MART by applying it to tweet entity linking, and it significantly outperforms the current state-of-the-art entity linking systems. In the future, we would like to investigate the advantages and disadvantages between treebased models and other non-linear models such as deep </context>
</contexts>
<marker>Sil, Yates, 2013</marker>
<rawString>Avirup Sil and Alexander Yates. 2013. Re-ranking for joint named-entity recognition and linking. In Proceedings of ACM Conference on Information and Knowledge Management (CIKM), pages 2369– 2374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="30634" citStr="Socher et al., 2011" startWordPosition="5086" endWordPosition="5089">g observation that different methods have various scales of model scores. 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only wo</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="30592" citStr="Socher et al., 2013" startWordPosition="5078" endWordPosition="5081">quite well. Finally, we have an interesting observation that different methods have various scales of model scores. 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCR</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Roller</author>
</authors>
<date>2004</date>
<booktitle>Max-margin markov networks. Advances in neural information processing systems,</booktitle>
<pages>16--25</pages>
<contexts>
<context position="30342" citStr="Taskar et al., 2004" startWordPosition="5039" endWordPosition="5042">s show that S-MART outperforms competitive approaches without any tuning, with similar margins to the results after tuning. Balancing precision and recall improves F1 scores for all the systems, which suggests that the simple tuning method performs quite well. Finally, we have an interesting observation that different methods have various scales of model scores. 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vi</context>
</contexts>
<marker>Taskar, Guestrin, Roller, 2004</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Roller. 2004. Max-margin markov networks. Advances in neural information processing systems, 16:25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the twentyfirst international conference on Machine learning (ICML),</booktitle>
<pages>104</pages>
<contexts>
<context position="1435" citStr="Tsochantaridis et al., 2004" startWordPosition="216" endWordPosition="219"> — a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base. A novel inference algorithm is proposed to handle the special structure of the task. The experimental results show that S-MART significantly outperforms state-of-the-art tweet entity linking systems. 1 Introduction Many natural language processing (NLP) problems can be formalized as structured prediction tasks. Standard algorithms for structured learning include Conditional Random Field (CRF) (Lafferty et al., 2001) and Structured Supported Vector Machine (SSVM) (Tsochantaridis et al., 2004). These algorithms, usually equipped with a linear model and sparse lexical features, achieve stateof-the-art performances in many NLP applications such as part-of-speech tagging, named entity recognition and dependency parsing. This classical combination of linear models and sparse features is challenged by the recent emergMing-Wei Chang Microsoft Research minchang@microsoft.com ing usage of dense features such as statistical and embedding features. Tasks with these low dimensional dense features require models to be more sophisticated to capture the relationships between features. Therefore,</context>
<context position="21882" citStr="Tsochantaridis et al., 2004" startWordPosition="3651" endWordPosition="3654">the surface to be used as anchor text in Wikipedia. We also add additional Entity Type features correspond to the following entity types: Character, Event, Product and Brand. Finally, we include several NER features to indicate each mention candidate belongs to one the following NER types: Twitter user, Twitter hashtag, Person, Location, Organization, Product, Event and Date. Algorithms Table 2 summarizes all the algorithms that are compared in our experiments. First, we consider two linear structured learning algorithms: Structured Perceptron (Collins, 2002) and Linear Structured SVM (SSVM) (Tsochantaridis et al., 2004). For non-linear models, we consider polynomial SSVM, which employs polynomial kernel inside the structured SVM algorithm. We also include LambdaRank (Quoc and Le, 2007), a neuralbased learning to rank algorithm, which is widely used in the information retrieval literature. We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured information. Finally, we have our proposed log-loss SMART algorithm, as described in Section 3. 9 Note that our baseline systems are quite strong. Linear SSVM has been used in one of the</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the twentyfirst international conference on Machine learning (ICML), page 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>In Journal of Machine Learning Research,</journal>
<pages>1453--1484</pages>
<contexts>
<context position="25585" citStr="Tsochantaridis et al., 2005" startWordPosition="4276" endWordPosition="4279">dels Table 3 clearly shows that linear models perform worse than non-linear models when they are restricted to the IE setting of the tweet entity linking task. The story is similar in IR-driven evaluation, with −2 −1 0 1 2 Bias Figure 2: Balance precisions and recalls. X-axis corresponds to values of the bias terms for the special token Nil. Note that S-MART is still the overall winning system without tuning the threshold. the exception of LambdaRank. Among the linear models, linear SSVM demonstrates its superiority over Structured Perceptron on all datasets, which aligns with the results of (Tsochantaridis et al., 2005) on the named entity recognition task. We have many interesting observations on the non-linear models side. First, by adopting a polynomial kernel, the non-linear SSVM further improves the entity linking performances on the NEEL datasets and TACL-IR dataset. Second, LambdaRank, a neural network based model, achieves better results than linear models in IEdriven evaluation, but the results in IR-driven evaluation are worse than all the other methods. We believe the reason for this dismal performance is that the neural-based method tends to overfit the IR setting given the small number of traini</context>
<context position="30372" citStr="Tsochantaridis et al., 2005" startWordPosition="5043" endWordPosition="5046">tperforms competitive approaches without any tuning, with similar margins to the results after tuning. Balancing precision and recall improves F1 scores for all the systems, which suggests that the simple tuning method performs quite well. Finally, we have an interesting observation that different methods have various scales of model scores. 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenk</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. In Journal of Machine Learning Research, pages 1453–1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Wu</author>
<author>Christopher JC Burges</author>
<author>Krysta M Svore</author>
<author>Jianfeng Gao</author>
</authors>
<title>Adapting boosting for information retrieval measures. Information Retrieval,</title>
<date>2010</date>
<pages>254--270</pages>
<contexts>
<context position="31048" citStr="Wu et al., 2010" startWordPosition="5153" endWordPosition="5156">networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-based methods for structured learning problems. The relationships between TreeCRF and our work have been discussed in Section 2. 511 Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the</context>
</contexts>
<marker>Wu, Burges, Svore, Gao, 2010</marker>
<rawString>Qiang Wu, Christopher JC Burges, Krysta M Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Information Retrieval, pages 254–270.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>