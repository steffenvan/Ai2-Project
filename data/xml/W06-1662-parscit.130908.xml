<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.994378">
Sentence Ordering with Manifold-based Classification in
Multi-Document Summarization
</title>
<author confidence="0.981821">
Paul D Ji
</author>
<affiliation confidence="0.9935765">
Centre for Linguistics and Philology
University of Oxford
</affiliation>
<email confidence="0.99188">
paul_dji@yahoo.co.uk
</email>
<sectionHeader confidence="0.993756" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999663">
In this paper, we propose a sentence
ordering algorithm using a semi-supervised
sentence classification and historical
ordering strategy. The classification is based
on the manifold structure underlying
sentences, addressing the problem of limited
labeled data. The historical ordering helps to
ensure topic continuity and avoid topic bias.
Experiments demonstrate that the method is
effective.
</bodyText>
<sectionHeader confidence="0.997812" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.966579666666667">
Sentence ordering has been a concern in text
planning and concept-to-text generation (Reiter et
al., 2000). Recently, it has also drawn attention in
multi-document summarization (Barzilay et al.,
2002; Lapata, 2003; Bollegala et al., 2005). Since
summary sentences generally come from
different sources in multi-document
summarization, an optimal ordering is crucial to
make summaries coherent and readable.
In general, the strategies for sentence ordering
in multi-document summarization fall in two
categories. One is chronological ordering
(Barzilay et al., 2002; Bollegala et al., 2005),
which is based on time-related features of the
documents. However, such temporal features may
be not available in all cases. Furthermore,
temporal inference in texts is still a problem, in
spite of some progress in automatic
disambiguation of temporal information (Filatova
Stephen Pulman
Centre for Linguistics and Philology
University of Oxford
sgp@clg.ox.ac.uk
et al., 2001).
Another strategy is majority ordering (MO)
(McKeown et al., 2001; Barzilay et al., 2002), in
which each summary sentence is mapped to a
theme, i.e., a set of similar sentences in the
documents, and the order of these sentences
determines that for summary sentences. To do
that, a directed theme graph is built, in which if a
theme A occurs behind another theme B in a
document, B is linked to A no matter how far
away they are located. However, this may lead to
wrong theme correlations, since B’s occurrence
may rely on a third theme C and have nothing to
do with A. In addition, when outputting theme
orders, MO uses a kind of heuristic that chooses a
theme based on its in-out edge difference in the
directed theme graph. This may cause topic
disruption, since the next choice may have no
link with previous choices.
Lapata (2003) proposed a probabilistic
ordering (PO) method for text structuring, which
can be adapted to majority ordering if the training
texts are those documents to be summarized. The
primary evidence for the ordering are informative
features of sentences, including words and their
grammatical dependence relations, which needs
reliable parsing of the text. Unlike in MO,
selection of the next sentence here is based on the
most recent one. However, this may lead to topic
bias: i.e. too many sentences on the same topic.
In this paper, we propose a historical ordering
(HO) strategy, in which the selection of the next
sentence is based on the whole history of
selection, not just the most recent choice. This
</bodyText>
<page confidence="0.990599">
526
</page>
<note confidence="0.8543295">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 526–533,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999869846153846">
strategy helps to ensure continuity of topics but to
avoid topic bias at the same time.
To do that, we need to map summary sentences
to those in documents. We formalize this as a
kind of classification problem, with summary
sentences as class labels. Since there are very few
(only one) labeled examples for each class, we
adopt a kind of semi-supervised classification
method, which makes use of the manifold
structure underlying the sentences to do the
classification. A common assumption behind this
learning paradigm is that the manifold structure
among the data, revealed by higher density,
provides a global comparison between data points
(Szummer et al., 2001; Zhu et al., 2003; Zhou et
al., 2003). Under such an assumption, even one
labeled example is enough for classification, if
only the structure is determined.
The remainder of the paper is organized as
follows. In section 2, we give an overview of the
proposed method. In section 3~5, we talk about
the method including sentence networks,
classification and ordering. In section 6, we
present experiments and evaluations. Finally in
section 7, we give some conclusions and future
work.
</bodyText>
<sectionHeader confidence="0.993597" genericHeader="introduction">
2. Overview
</sectionHeader>
<bodyText confidence="0.9729255">
Fig. 1 gives the overall structure of the proposed
method, which includes three modules:
construction of sentence networks, sentence
classification and sentence ordering.
</bodyText>
<subsectionHeader confidence="0.918968">
Sentence network construction
Sentence classification
Summary sentence ordering
</subsectionHeader>
<bodyText confidence="0.999955210526316">
network with weights on edges, which can serve
as the basis for a Markov random walk (Tishby et
al., 2000). The neighborhood is based on
similarity between sentences, and weights on
edges can be seen as transition probabilities for
the random walk. From this network, we can
derive new representations for sentences.
The second step is to make a classification of
sentences, with each summary sentence as a class
label. Since only one labeled example exists for
each class, we use a semi-supervised method
based on a Markov random walk to reveal the
manifold structure for the classification.
The third step is to order summary sentences
according to the original positions of their
partners in the same class. During this process,
the next selection of a sentence is based on the
whole history of selection, i.e., the association of
the sentence with all those already selected.
</bodyText>
<sectionHeader confidence="0.960456" genericHeader="method">
3. Sentence Network Construction
</sectionHeader>
<bodyText confidence="0.999423846153846">
Suppose S is the set of all sentences in the
documents and a summary (a summary sentence
may be not a document sentence), let S={s1,
s2, ..., sN} with a distance metric d(si,sj), the
distance between two sentences si and sj, which is
based on the Jensen-Shannon divergence (Lin,
1991). We construct a graph with sentences as
points by sorting the distances among the points
in an ascending order and repeatedly connecting
two points according to the order until a
connected graph is obtained. Then, we assign a
weight wi,j, as in (1), to each edge based on the
distance.
</bodyText>
<equation confidence="0.998654">
1)
w =exp(—d(s, ss
i,j Z j) /
</equation>
<bodyText confidence="0.99985325">
The weights are symmetric, wi,i=1 and wi,j=0 for
all non-neighbors ( is set as 0.6 in this work). 2)
is the one-step transition probability p(si, sj) from
si to sj based on weights of neighbors.
</bodyText>
<equation confidence="0.36863">
)
</equation>
<figureCaption confidence="0.907903">
Fig. 1. Algorithm Overview
</figureCaption>
<bodyText confidence="0.6364575">
The first step is to build a sentence neighborhood
j
</bodyText>
<equation confidence="0.9425443">
wi,
2
(s i
) p
,s
j)
k
wi,
∑
k
</equation>
<page confidence="0.989198">
527
</page>
<bodyText confidence="0.9999854375">
Let M be the NN matrix and Mi,j= p(si, sj), then
Mt is the tth Markov random walk matrix, whose i,
j-th entry is the probability pt(si, sj) of the
transition from si to sj after t steps. In this way,
each sentence sj is associated with a vector of
conditional probabilities pt(si, sj), i=1, ..., N,
which form a new manifold-based representation
for sj. With such representations, sentences are
close whenever they have a similar distribution
over the starting points. Notice that the
representations depend on the step parameter t
(Tishby et al., 2000). With smaller values of t,
unlabeled points may be not connected with
labeled ones; with bigger values of t, the points
may be indistinguishable. So, an appropriate t
should be estimated.
</bodyText>
<sectionHeader confidence="0.966677" genericHeader="method">
4. Sentence Classification
</sectionHeader>
<bodyText confidence="0.996459888888889">
Suppose s1, s2, ..., sL are summary sentences and
their labels are c1, c2, ..., cL respectively. In our
case, each summary sentence is assigned with a
unique class label ci, 1iL. This also means that
for each class ci, there is only one labeled
example, i.e., the summary sentence, si.
i
Let S={(s1, c1), (s2, c2), ..., (sL, cL), sL+1, ... , sNI,
then the task of sentence classification is to infer
the labels for unlabeled sentences, sL+1,..., sN.
Through the classification, we can get similar
sentences for each summary sentence. To do that,
we assume that each sentence has a distribution
p(ck|si), 1k≤L, 1iN, and these probabilities are
to be estimated from the data.
Seeing a sentence as a sample from the t step
Markov random walk in the sentence graph, we
have the following interpretation of p(ck|si).
</bodyText>
<equation confidence="0.5684825">
3) p(ck  |si) = ∑ p(ck  |s j )pt (j, i)
j
</equation>
<bodyText confidence="0.999343111111111">
This means that the probability of si belonging
to ck is dependent on the probabilities of those
sentences belonging to ck which will transit to si
after t steps and their transition probabilities.
With the conditional log-likelihood of labeled
sentences 4) as the estimation criterion, we can
use the EM algorithm to estimate p(ck|si), in
which the E-step and M-step are 5) and 6)
respectively.
</bodyText>
<equation confidence="0.952280625">
L L N
4) ∑ log p(ck  |sk) = ∑ log ∑ p(ck  |s j )pt (j, k)
k=1 k= 1 j=1
L
5) p(si  |sk, ck) = p(ck  |si )pt (i, k) / ∑ p(ck  |si )pt (i, k)
k=1
6) p
(  |) (  |, ) / (  |, )
c s p s s c
k i i k k i k k
= ∑ p s s c
1≤k
≤L
The final class ci for si is given in 7).
7) ci = argmax ( |
c k p ck si )
</equation>
<bodyText confidence="0.9933885">
p(ci|si) is called the membership probability of si.
After classification, each sentence is assigned a
label according to 7).
One key problem in this setting is to estimate
the parameter t. A possible strategy for that is by
cross validation, but it needs a large amount of
labeled data. Here, following Szummer et al.,
2001, we use marginal difference of probabilities
of sentences falling in different classes as the
estimation criterion, which is given in 8).
</bodyText>
<equation confidence="0.978302666666667">
8) m( S) ∑ (L max p(ck  |s) −
s∈ S 1≤k≤L 1≤ ≤
k L p(ck  |s))
</equation>
<bodyText confidence="0.999830272727273">
To maximize 8), we can get an appropriate value
for the parameter t, which means that a better t
should make sentences belong to some classes
more prominently. Notice that the classes
represented by summary sentences may be
incomplete for all the sentences occurring in the
documents, so some sentences will belong to the
classes without obviously different probabilities.
To avoid such sentences in the estimation of t, we
only choose the top (40%) sentences in a class
based on their membership probabilities.
</bodyText>
<sectionHeader confidence="0.902342" genericHeader="method">
5. Sentence Ordering
</sectionHeader>
<bodyText confidence="0.9999195">
After sentence classification, we get a class of
similar sentences for each summary sentence,
which is also a member of the class. With these
sentence classes, we create a directed class graph
based on the order of their member sentences in
documents. In the graph, each sentence class is a
</bodyText>
<page confidence="0.98751">
528
</page>
<bodyText confidence="0.9999375">
node, and there exists a directed edge ei,j from
one node ci to another cj if and only if there is si
in ci immediately appearing before sj in cj in the
documents (the sentences not in classes are
neglected). The weight of ei,j, Fi,j, captures the
frequency of such occurrence. We add one
additional node denoting an initial class c0, and it
links to each class with a directed edge e0,j, the
weight F0,j of which is the frequency of the
member sentences of the class appearing at the
beginning of the documents.
Suppose the input is the class graph G=&lt;C, E&gt;,
where C = {c1, c2, ..., cL} is the set of the classes,
E={ei,j|1i, j≤L} is the set of the directed edges,
and o is the ordering of the classes. Fig. 2 gives
the ordering algorithm.
</bodyText>
<equation confidence="0.8313834">
i) ck ← max 0 ,
F i
c C
i∈
ii) oo ck
</equation>
<listItem confidence="0.70664975">
iii) For all ci in C, F0,i ← F0,i + Fk,i
iv) Remove ck from C and ek,j and ei,k from E;
v) Repeat i)-iv) while C{ c0}
vi) Return the order o.
</listItem>
<figureCaption confidence="0.976097">
Fig. 2 Ordering algorithm
</figureCaption>
<bodyText confidence="0.999383555555556">
In the algorithm, there are two main steps. Step i)
selects the class whose member sentences occur
most frequently immediately after those in c0.
Step iii) updates the weights of the edges e0,i. In
fact, it can be seen as merge of the original c0 and
ck, and in this sense the updated c0 represents the
history of selections.
In contrast to the MO algorithm, the ordering
algorithm here (HO) uses immediate back-front
co-occurrence, while the MO algorithm uses
relative back-front locations. On the other hand,
the selection of a class is dependent on previous
selections in HO, while in MO, the selection of a
class is mainly dependent on its in-out edge
difference.
In contrast to the PO algorithm, the selection of
a class in HO is dependent on all previous
selections, while in PO, the selection is only
related to the most recent one.
As an example, Fig. 3 gives an initial class
graph. The output orderings by PO and HO are
[c1, c3, c4, c2] and [c1, c3, c2, c4] respectively. The
difference lies in whether to select c4 or c2 after
selection of c3. PO selects c4 since it only
considers the most recent selection, while HO
selects c2 because it considers all previous
selections including c1.
</bodyText>
<figure confidence="0.783630666666667">
c0
9
c1
5 6
c2 c3
Fig. 3 Initial graph for PO and HO
As another example, Fig. 4 gives the order of the
classes in individual documents.
1) c2 c3 c1
2) c2 c3 c1
3) c3 c2 c1
4) c3 c2 c1
5) c3 c2
6) c2 c3
7) c1 c2 c3 c2 c3 c2 c3 c2
</figure>
<figureCaption confidence="0.980728">
Fig. 4. Class orders in documents
</figureCaption>
<bodyText confidence="0.999842375">
From 1)-6), we can see some regularity among
the order of the classes: c2 and c3 are
interchangeable, while c1 always appears behind
c2 or c3. From 7), we can see that c2 and c3 still
co-occur, while c1 happens to occur at the
beginning of the document. Thus, the appropriate
ordering should be [c2, c3, c1] or [c3, c2, c1]. Fig. 5
is the graph built by MO.
</bodyText>
<figure confidence="0.78059275">
c2
4 6 6 2
c3
3 2
c1
Fig. 5 Graph by MO
1 2
1 c4
</figure>
<page confidence="0.98967">
529
</page>
<bodyText confidence="0.999570571428572">
According to MO, the first node to be selected
will be c1, since the difference of its in-out edges
(+3) is bigger than that (-2, -1) of other two nodes.
Then the in-out edge differences for c2 or c3 are
both 0 after removing edges associated with c1,
and either c2 or c3 will be selected. Thus, the
output ordering should be [c1, c2, c3] or [c1, c3, c2].
</bodyText>
<equation confidence="0.552605">
c2
1 6 6 2
c3 3
0 2 3
c1 1 c0
</equation>
<figureCaption confidence="0.9157948">
Fig. 6 Graph by HO
Fig. 6 is the class graph built by HO. According
to HO, the first node to be selected will be c2 or c3,
since e0,1=e0,2=3&gt;e0,1=1. Suppose c2 is firstly
selected, then e0,3e0,3+e2,3=3+6=9, while e0,1
</figureCaption>
<bodyText confidence="0.98718125">
e0,1+e2,1=1+2=3, so c3 will be selected then.
Finally the output ordering will be [c2, c3, c1].
Similarly, if c3 is firstly selected, the output
ordering will be [c3, c2, c1].
</bodyText>
<sectionHeader confidence="0.995642" genericHeader="evaluation">
6 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.539865">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.9998154">
We used the DUC04 document dataset. The
dataset contains 50 document clusters and each
cluster includes 20 content-related documents.
For each cluster, 4 manual summaries are
provided.
</bodyText>
<subsectionHeader confidence="0.998883">
6.2 Evaluation Measure
</subsectionHeader>
<bodyText confidence="0.99985225">
The proposed method in this paper consists of
two main steps: sentence classification and
sentence ordering. For classification, we used
pointwise entropy (Dash et al., 2000) to measure
the quality of the classification result due to lack
of enough labeled data. For a n×m matrix M,
whose row vectors are normalized as 1, its
pointwise entropy is defined in 9).
</bodyText>
<equation confidence="0.999781666666667">
9) ( )
E M =−∑ ∑(M.- logMi,,. +(1−M,.,j)log(1−Mij
,
1 1
≤ ≤ ≤≤
i n j m
</equation>
<bodyText confidence="0.998257333333333">
Intuitively, if Mi,j is close to 0 or 1, E(M) tends
towards 0, which corresponds to clearer
distinctions between classes; otherwise E(M)
tends towards 1, which means there are no clear
boundaries between classes. For comparison
between different matrices, E(M) needs to be
averaged over n×m.
For sentence ordering, we used Kendall’s
coefficient (Zapata, 2003), as defined in 10),
</bodyText>
<equation confidence="0.997492">
1 0) 1
τ =
</equation>
<bodyText confidence="0.999881875">
where, NI is number of inversions of consecutive
sentences needed to transform output of the
algorithm to manual summaries. The measure
ranges from -1 for inverse ranks to +1 for
identical ranks, and can also be seen as a kind of
edit similarity between two ranks: smaller values
for lower similarity, and bigger values for higher
similarity.
</bodyText>
<subsectionHeader confidence="0.99921">
6.3 Evaluation of Classification
</subsectionHeader>
<bodyText confidence="0.698109666666667">
For sentence classification, we need to estimate
the parameter t. We randomly chose 5 document
clusters and one manual summary from the four.
Fig. 7 shows the change of the average margin
over all the top 40% sentences in a cluster with t
varying from 3 to 25.
</bodyText>
<figure confidence="0.647387">
0.4
</figure>
<figureCaption confidence="0.999574">
Fig. 7. Average margin and t
Fig. 7 indicates that the average margin changes
</figureCaption>
<bodyText confidence="0.98269025">
with t for each cluster and the values of t
maximizing the margin are different for different
clusters. For the 5 clusters, the estimated t is 16, 8,
14, 12 and 21 respectively. So we need to
</bodyText>
<figure confidence="0.966739210526316">
3 5 10 15 20 25
t values
0.8
0.6
0.2
0
average margin
cluster 1
cluster 2
cluster 3
cluster 4
cluster 5
))
2 ( )
NI
−
−
N(N
1) / 2
</figure>
<page confidence="0.98962">
530
</page>
<bodyText confidence="0.9992615">
estimate the best t for each cluster.
After estimation of t, EM was used to estimate
the membership probabilities. Table 1 gives the
average pointwise entropy for top 10% to top
100% sentences in each cluster, where sentences
were ordered by their membership probabilities.
The values were averaged over 20 runs, and for
each run, 10 document clusters and one summary
were randomly selected, and the entropy was
averaged over the summaries.
</bodyText>
<table confidence="0.999647909090909">
Sentences E_Semi E_SVM Significance
10% 0.23 0.22 ~
20% 0.26 0.27 ~
30% 0.32 0.43 *
40% 0.35 0.49 **
50% 0.42 0.51 *
60% 0.46 0.55 *
70% 0.48 0.57 *
80% 0.59 0.62 ~
90% 0.65 0.69 ~
100% 0.70 0.73 ~
</table>
<tableCaption confidence="0.999864">
Table 1. Entropy of classification result
</tableCaption>
<bodyText confidence="0.999929452830189">
In Table 1, the column E_Semi shows entropies
of the semi-supervised classification. It indicates
that the entropy increases as more sentences are
considered. This is not surprising since the
sentences are ordered by their membership
probabilities in a cluster, which can be seen as a
kind of measure for closeness between sentences
and cluster centroids, and the boundaries between
clusters become dim with more sentences
considered.
To compare the performance between this
semi-supervised classification and a standard
supervised method like Support Vector Machines
(SVM), Table 1 also lists the average entropy of a
SVM (E_SVM) over the runs. Similarly, we
found that the entropy also increases as sentences
increase. Table 2 also gives the significance sign
over the runs, where *, ** and ~ represent
p-values &lt;=0.01, (0.01, 0.05] and &gt;0.05, and
indicate that the entropy of the semi-supervised
classification is lower, significantly lower, or
almost the same as that of SVM respectively.
Table 1 demonstrates that when the top 10% or
20% sentences are considered, the performance
between the two algorithms shows no difference.
The reason may be that these top sentences are
closer to cluster centroids in both cases, and the
cluster boundaries in both algorithms are clear in
terms of these sentences.
For the top 30% sentences, the entropy for
semi-supervised classification is lower than that
for a SVM, and for the top 40%, the difference
becomes significantly lower. The reason may go
to the substantial assumptions behind the two
algorithms. SVM, based on local comparison, is
successful only when more labeled data is
available. With only one sentence labeled as in
our case, the semi-supervised method, based on
global distribution, makes use of a large amount
of unlabeled data to reveal the underlying
manifold structure. Thus, the performance is
much better than that of a SVM when more
sentences are considered.
For the top 50% to 70% sentences, E_Semi is
still lower, but not by much. The reason may be
that some noisy documents are starting to be
included. For the top 80% to 100% sentences, the
performance shows no difference again. The
reason may be that the lower ranking sentences
may belong to other classes than those
represented by summary sentences, and with
these sentences included, the cluster boundaries
become unclear in both cases.
</bodyText>
<subsectionHeader confidence="0.999738">
6.4 Evaluation of Ordering
</subsectionHeader>
<bodyText confidence="0.99980675">
We used the same classification results to test the
performance of our ordering algorithm HO as
well as MO and PO. Table 2 lists the Kendall’s
coefficient values for the three algorithms (_1).
The value was averaged over 20 runs, and for
each run, 10 summaries were randomly selected
and the score was averaged over summaries.
Since a summary sentence tends to generalize
</bodyText>
<page confidence="0.9921">
531
</page>
<bodyText confidence="0.99797">
some sentences in the documents, we also tried to
combine two or three consecutive sentences into
one, and tested their ordering performance (_2
and _3) respectively.
</bodyText>
<table confidence="0.99721875">
HO MO PO
_1 0.42 0.31 0.33
_2 0.33 0.26 0.29
_3 0.27 0.21 0.25
</table>
<tableCaption confidence="0.820956">
Table 2. scores for HO, MO and PO
Table 2 indicates that the combination of
</tableCaption>
<bodyText confidence="0.994847722222222">
sentences harms the performance. To see why, we
checked the classification results, and found that
the pointwise entropies for two and three
sentence combinations (for the top 40% sentence
in each cluster) increase 12.4% and 18.2%
respectively. This means that the cluster structure
becomes less clear with two or three sentence
combinations, which would lead to less similar
sentences being clustered with summary
sentences. This result also suggests that if the
summary sentence subsumes multiple sentences
in the documents, they tend to be not consecutive.
Fig. 8 shows change of scores with different
number of sentences used for ordering, where x
axis denotes top (1-x)*100% sentences in each
cluster. The score was averaged over 20 runs, and
for each run, 10 summaries were randomly
selected and evaluated.
</bodyText>
<figure confidence="0.996165375">
0.5
0.4
0.3
0.2
0.1
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
least probability
</figure>
<figureCaption confidence="0.999759">
Fig. 8. scores and number of sentences
</figureCaption>
<bodyText confidence="0.954042444444444">
Fig. 8 indicates that with fewer sentences (x
&gt;=0.7) used for ordering, the performance
decreases. The reason may be that with fewer and
fewer sentences used, the result is deficient
training data for the ordering. On the other hand,
with more sentences used (x &lt;0.6), the
performance also decreases. The reason may be
that as more sentences are used, the noisy
sentences could dominate the ordering. That’s
why we considered only the top 40% sentences in
each cluster as training data for sentence
reordering here.
As an example, the following is a summary for
a cluster of documents about Central American
storms, in which the ordering is given manually.
1) A category 5 storm, Hurricane Mitch roared across the northwest
Caribbean with 180 mph winds across a 350-mile front that
devastated the mainland and islands of Central America.
</bodyText>
<listItem confidence="0.8330833">
2) Although the force of the storm diminished, at least 8,000 people
died from wind, waves and flood damage.
3) The greatest losses were in Honduras where some 6,076 people
perished.
4) Around 2,000 people were killed in Nicaragua, 239 in El Salvador,
194 in Guatemala, seven in Costa Rica and six in Mexico.
5) At least 569,000 people were homeless across Central America.
6) Aid was sent from many sources (European Union, the UN, US
and Mexico).
7) Relief efforts are hampered by extensive damage.
</listItem>
<bodyText confidence="0.999723722222222">
Compared with the manual ordering, our
algorithm HO outputs the ordering [1, 3, 4, 2, 5, 6,
7]. In contrast, PO and MO created the orderings
[1, 3, 4, 5, 6, 7, 2] and [1, 3, 2, 6, 4, 5, 7]
respectively. In HO’s output, sentence 2 was put
in the wrong position. To check why this was so,
we found that sentences in cluster 2 and cluster 3
(clusters containing sentence 2 or sentence 3)
were very similar, and the size of cluster 3 was
bigger than that of cluster 2. Also we found that
sentences in cluster 4 mostly followed those in
cluster 3. This may explain why the ordering [1, 3,
4] occurred. Due to the link between cluster 2 and
cluster 1 or 3, sentence 2 followed sentence 4 in
the ordering. In PO, sentence 2 was put at the end
of the ordering, since it only considered the most
recent selection when determining next, so cluster
1 would not be considered when determining the
</bodyText>
<page confidence="0.988785">
532
</page>
<bodyText confidence="0.999725375">
4th position. This suggests that consideration of
selection history does in fact help to group those
related sentences more closely, although sentence
2 was ranked lower than expected in the example.
In MO, we found sentence 2 was put
immediately behind sentence 3. The reason was
that, after sentence 1 and 3 were selected, the
in-edges of the node representing cluster 2
became 0 in the cluster directed graph, and its
in-out edge difference became the biggest among
all nodes in the graph, so it was chosen. For
similar reasons, sentence 6 was put behind
sentence 2. This suggests that it may be difficult
to consider the selection history in MO, since its
selection is mainly based on the current status of
clusters.
</bodyText>
<sectionHeader confidence="0.99696" genericHeader="conclusions">
6. Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999635">
In this paper, we propose a sentence ordering
method for multi-document summarization based
on semi-supervised classification and historical
ordering. For sentence classification, the
semi-supervised classification groups sentences
based on their global distribution, rather than on
local comparisons. Thus, even with a small
amount of labeled data (just 1 labeled example in
our case) we nevertheless ensure good
performance for sentence classification.
For sentence ordering, we propose a kind of
history-based ordering strategy, which determines
the next selection based on the whole selection
history, rather than the most recent single
selection in probabilistic ordering, which could
result in topic bias, or in-out difference in MO,
which could result in topic disruption.
In this work, we mainly use sentence-level
information, including sentence similarity and
sentence order, etc. In future, we may explore the
role of term-level or word-level features, e.g.,
proper nouns, in the ordering of summary
sentences. To make summaries more coherent and
readable, we may also need to discover how to
detect and control topic movement automatic
summaries. One specific task is how to generate
co-reference among sentences in summaries. In
addition, we will also try other semi-supervised
classification methods, and other evaluation
metrics, etc.
</bodyText>
<sectionHeader confidence="0.943913" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999626184210526">
Barzilay, R N. Elhadad, and K. McKeown. 2002.
Inferring strategies for sentence ordering in
multidocument news summarization. Journal of
Artificial Intelligence Research, 17:35–55.
Bollegala D. Okazaki, N. Ishizuka, M. 2005. A
Machine Learning Approach to Sentence Ordering
for Multidocument Summarization, in Proceedings
of IJCNLP.
Dash M. and H. Liu, (2000) Unsupervised feature
selection, proceedings of PAKDD.
Filatova, E. &amp; Hovy, E. (2001) Assigning time-stamps
to event-clauses. In Proceedings of AACL/EACL
workshop on Temporal and Spatial Information
Processing.
Lapata, M. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceedings
of the annual meeting of ACL 545–552.
Lin, J. 1991. Divergence Measures Based on the
Shannon Entropy. IEEE Transactions on
Information Theory, 37:1, 145–150.
McKeown K., Barzilay R. Evans D., Hatzivassiloglou
V., Kan M., Schiffman B., &amp;Teufel, S. (2001).
Columbia multi-document summarization:
Approach and evaluation. In Proceedings of DUC.
Reiter, Ehud and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge
University Press, Cambridge.
Szummer M. and T. Jaakkola. (2001) Partially labeled
classification with markov random walks. NIPS14.
Tishby, N, Slonim, N. (2000) Data clustering by
Markovian relaxation and the Information
Bottleneck Method. NIPS 13.
Zhu, X., Ghahramani, Z., &amp; Lafferty, J. (2003)
Semi-Supervised Learning Using Gaussian Fields
and Harmonic Functions. ICML-2003.
Zhou D., Bousquet, O., Lal, T.N., Weston J. &amp;
Schokopf B. (2003). Learning with local and Global
Consistency. NIPS 16. pp: 321-328.
</reference>
<page confidence="0.998964">
533
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.849018">
<title confidence="0.9993985">Sentence Ordering with Manifold-based Classification in Multi-Document Summarization</title>
<author confidence="0.99958">D Paul</author>
<affiliation confidence="0.9987315">Centre for Linguistics and University of</affiliation>
<email confidence="0.977398">paul_dji@yahoo.co.uk</email>
<abstract confidence="0.985561272727273">In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy. The classification is based on the manifold structure underlying sentences, addressing the problem of limited labeled data. The historical ordering helps to ensure topic continuity and avoid topic bias. Experiments demonstrate that the method is effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R N Elhadad Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>17--35</pages>
<marker>Barzilay, McKeown, 2002</marker>
<rawString>Barzilay, R N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal of Artificial Intelligence Research, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bollegala D Okazaki</author>
<author>N Ishizuka</author>
<author>M</author>
</authors>
<title>A Machine Learning Approach to Sentence Ordering for Multidocument Summarization,</title>
<date>2005</date>
<booktitle>in Proceedings of IJCNLP.</booktitle>
<marker>Okazaki, Ishizuka, M, 2005</marker>
<rawString>Bollegala D. Okazaki, N. Ishizuka, M. 2005. A Machine Learning Approach to Sentence Ordering for Multidocument Summarization, in Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dash</author>
<author>H Liu</author>
</authors>
<title>Unsupervised feature selection, proceedings of PAKDD.</title>
<date>2000</date>
<marker>Dash, Liu, 2000</marker>
<rawString>Dash M. and H. Liu, (2000) Unsupervised feature selection, proceedings of PAKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>E Hovy</author>
</authors>
<title>Assigning time-stamps to event-clauses.</title>
<date>2001</date>
<booktitle>In Proceedings of AACL/EACL workshop on Temporal and Spatial Information Processing.</booktitle>
<marker>Filatova, Hovy, 2001</marker>
<rawString>Filatova, E. &amp; Hovy, E. (2001) Assigning time-stamps to event-clauses. In Proceedings of AACL/EACL workshop on Temporal and Spatial Information Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the annual meeting of ACL</booktitle>
<pages>545--552</pages>
<contexts>
<context position="808" citStr="Lapata, 2003" startWordPosition="106" endWordPosition="107">, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy. The classification is based on the manifold structure underlying sentences, addressing the problem of limited labeled data. The historical ordering helps to ensure topic continuity and avoid topic bias. Experiments demonstrate that the method is effective. 1. Introduction Sentence ordering has been a concern in text planning and concept-to-text generation (Reiter et al., 2000). Recently, it has also drawn attention in multi-document summarization (Barzilay et al., 2002; Lapata, 2003; Bollegala et al., 2005). Since summary sentences generally come from different sources in multi-document summarization, an optimal ordering is crucial to make summaries coherent and readable. In general, the strategies for sentence ordering in multi-document summarization fall in two categories. One is chronological ordering (Barzilay et al., 2002; Bollegala et al., 2005), which is based on time-related features of the documents. However, such temporal features may be not available in all cases. Furthermore, temporal inference in texts is still a problem, in spite of some progress in automat</context>
<context position="2398" citStr="Lapata (2003)" startWordPosition="363" endWordPosition="364">tences determines that for summary sentences. To do that, a directed theme graph is built, in which if a theme A occurs behind another theme B in a document, B is linked to A no matter how far away they are located. However, this may lead to wrong theme correlations, since B’s occurrence may rely on a third theme C and have nothing to do with A. In addition, when outputting theme orders, MO uses a kind of heuristic that chooses a theme based on its in-out edge difference in the directed theme graph. This may cause topic disruption, since the next choice may have no link with previous choices. Lapata (2003) proposed a probabilistic ordering (PO) method for text structuring, which can be adapted to majority ordering if the training texts are those documents to be summarized. The primary evidence for the ordering are informative features of sentences, including words and their grammatical dependence relations, which needs reliable parsing of the text. Unlike in MO, selection of the next sentence here is based on the most recent one. However, this may lead to topic bias: i.e. too many sentences on the same topic. In this paper, we propose a historical ordering (HO) strategy, in which the selection </context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Lapata, M. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the annual meeting of ACL 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Divergence Measures Based on the Shannon Entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<pages>145--150</pages>
<contexts>
<context position="5900" citStr="Lin, 1991" startWordPosition="923" endWordPosition="924">tion. The third step is to order summary sentences according to the original positions of their partners in the same class. During this process, the next selection of a sentence is based on the whole history of selection, i.e., the association of the sentence with all those already selected. 3. Sentence Network Construction Suppose S is the set of all sentences in the documents and a summary (a summary sentence may be not a document sentence), let S={s1, s2, ..., sN} with a distance metric d(si,sj), the distance between two sentences si and sj, which is based on the Jensen-Shannon divergence (Lin, 1991). We construct a graph with sentences as points by sorting the distances among the points in an ascending order and repeatedly connecting two points according to the order until a connected graph is obtained. Then, we assign a weight wi,j, as in (1), to each edge based on the distance. 1) w =exp(—d(s, ss i,j Z j) / The weights are symmetric, wi,i=1 and wi,j=0 for all non-neighbors ( is set as 0.6 in this work). 2) is the one-step transition probability p(si, sj) from si to sj based on weights of neighbors. ) Fig. 1. Algorithm Overview The first step is to build a sentence neighborhood j wi, 2 </context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Lin, J. 1991. Divergence Measures Based on the Shannon Entropy. IEEE Transactions on Information Theory, 37:1, 145–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>Barzilay R Evans D</author>
<author>V Hatzivassiloglou</author>
<author>M Kan</author>
<author>B Schiffman</author>
<author>S &amp;Teufel</author>
</authors>
<date>2001</date>
<marker>McKeown, D, Hatzivassiloglou, Kan, Schiffman, &amp;Teufel, 2001</marker>
<rawString>McKeown K., Barzilay R. Evans D., Hatzivassiloglou V., Kan M., Schiffman B., &amp;Teufel, S. (2001).</rawString>
</citation>
<citation valid="false">
<title>Columbia multi-document summarization: Approach and evaluation.</title>
<booktitle>In Proceedings of DUC.</booktitle>
<marker></marker>
<rawString>Columbia multi-document summarization: Approach and evaluation. In Proceedings of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Reiter, Dale, 2000</marker>
<rawString>Reiter, Ehud and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szummer</author>
<author>T Jaakkola</author>
</authors>
<title>Partially labeled classification with markov random walks.</title>
<date>2001</date>
<pages>14</pages>
<marker>Szummer, Jaakkola, 2001</marker>
<rawString>Szummer M. and T. Jaakkola. (2001) Partially labeled classification with markov random walks. NIPS14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tishby</author>
<author>N Slonim</author>
</authors>
<title>Data clustering by Markovian relaxation and the Information Bottleneck Method.</title>
<date>2000</date>
<journal>NIPS</journal>
<volume>13</volume>
<marker>Tishby, Slonim, 2000</marker>
<rawString>Tishby, N, Slonim, N. (2000) Data clustering by Markovian relaxation and the Information Bottleneck Method. NIPS 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions.</title>
<date>2003</date>
<tech>ICML-2003.</tech>
<contexts>
<context position="3973" citStr="Zhu et al., 2003" startWordPosition="613" endWordPosition="616">s at the same time. To do that, we need to map summary sentences to those in documents. We formalize this as a kind of classification problem, with summary sentences as class labels. Since there are very few (only one) labeled examples for each class, we adopt a kind of semi-supervised classification method, which makes use of the manifold structure underlying the sentences to do the classification. A common assumption behind this learning paradigm is that the manifold structure among the data, revealed by higher density, provides a global comparison between data points (Szummer et al., 2001; Zhu et al., 2003; Zhou et al., 2003). Under such an assumption, even one labeled example is enough for classification, if only the structure is determined. The remainder of the paper is organized as follows. In section 2, we give an overview of the proposed method. In section 3~5, we talk about the method including sentence networks, classification and ordering. In section 6, we present experiments and evaluations. Finally in section 7, we give some conclusions and future work. 2. Overview Fig. 1 gives the overall structure of the proposed method, which includes three modules: construction of sentence network</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu, X., Ghahramani, Z., &amp; Lafferty, J. (2003) Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions. ICML-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhou</author>
<author>O Bousquet</author>
<author>T N Lal</author>
<author>J Weston</author>
<author>B Schokopf</author>
</authors>
<title>Learning with local and Global Consistency.</title>
<date>2003</date>
<journal>NIPS</journal>
<volume>16</volume>
<pages>321--328</pages>
<contexts>
<context position="3993" citStr="Zhou et al., 2003" startWordPosition="617" endWordPosition="620">. To do that, we need to map summary sentences to those in documents. We formalize this as a kind of classification problem, with summary sentences as class labels. Since there are very few (only one) labeled examples for each class, we adopt a kind of semi-supervised classification method, which makes use of the manifold structure underlying the sentences to do the classification. A common assumption behind this learning paradigm is that the manifold structure among the data, revealed by higher density, provides a global comparison between data points (Szummer et al., 2001; Zhu et al., 2003; Zhou et al., 2003). Under such an assumption, even one labeled example is enough for classification, if only the structure is determined. The remainder of the paper is organized as follows. In section 2, we give an overview of the proposed method. In section 3~5, we talk about the method including sentence networks, classification and ordering. In section 6, we present experiments and evaluations. Finally in section 7, we give some conclusions and future work. 2. Overview Fig. 1 gives the overall structure of the proposed method, which includes three modules: construction of sentence networks, sentence classifi</context>
</contexts>
<marker>Zhou, Bousquet, Lal, Weston, Schokopf, 2003</marker>
<rawString>Zhou D., Bousquet, O., Lal, T.N., Weston J. &amp; Schokopf B. (2003). Learning with local and Global Consistency. NIPS 16. pp: 321-328.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>