<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000532">
<title confidence="0.9496815">
Chinese Zero Pronoun Resolution: A Joint Unsupervised
Discourse-Aware Model Rivaling State-of-the-Art Resolvers
</title>
<author confidence="0.991559">
Chen Chen and Vincent Ng
</author>
<affiliation confidence="0.9938465">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.903208">
Richardson, TX 75083-0688
</address>
<email confidence="0.999527">
{yzcchen,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999517692307692">
We propose an unsupervised probabilistic
model for zero pronoun resolution. To our
knowledge, this is the first such model that
(1) is trained on zero pronouns in an unsu-
pervised manner; (2) jointly identifies and
resolves anaphoric zero pronouns; and (3)
exploits discourse information provided by
a salience model. Experiments demon-
strate that our unsupervised model signif-
icantly outperforms its state-of-the-art un-
supervised counterpart when resolving the
Chinese zero pronouns in the OntoNotes
corpus.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950571428571">
A zero pronoun (ZP) is a gap in a sentence that
is found when a phonetically null form is used to
refer to a real-world entity. An anaphoric zero pro-
noun (AZP) is a ZP that corefers with one or more
preceding mentions in the associated text. Below
is an example taken from the Chinese TreeBank
(CTB), where the ZP (denoted as *pro*) refers to
</bodyText>
<sectionHeader confidence="0.917737" genericHeader="introduction">
4 VWTr (Russia).
</sectionHeader>
<bodyText confidence="0.995488660714286">
[4 VWTr] 4&apos; )�)K Ofd—AMjC44,
*pro* a 1KA��X3/1 C tl&amp;-
([Russia] is a consistent supporter of Milošević,
*pro* has proposed to mediate the political crisis.)
As we can see, ZPs lack grammatical attributes
that are useful for overt pronoun resolution such as
NUMBER and GENDER. This makes ZP resolution
more challenging than overt pronoun resolution.
Automatic ZP resolution is typically composed
of two steps. The first step, AZP identification, in-
volves extracting ZPs that are anaphoric. The sec-
ond step, AZP resolution, aims to identify an an-
tecedent of an AZP. State-of-the-art ZP resolvers
have tackled both of these steps in a supervised
manner, training one classifier for AZP identifica-
tion and another for AZP resolution (e.g., Zhao and
Ng (2007), Kong and Zhou (2010)).
More recently, we have proposed an unsuper-
vised AZP resolution model (henceforth the CN14
model) that rivals its supervised counterparts in
performance (Chen and Ng, 2014). The idea is to
resolve AZPs by using a probabilistic pronoun res-
olution model trained on overt pronouns in an un-
supervised manner. This is an appealing approach,
as its language-independent generative process en-
ables it to be applied to languages where data an-
notated with ZP links are not available.
In light of the advantages of unsupervised mod-
els, we examine in this paper the possibility of ad-
vancing the state of the art in unsupervised AZP
resolution. The design of our unsupervised model
is motivated by a key question: can we resolve
AZPs by using a probabilistic model trained on
zero pronouns in an unsupervised manner? As
mentioned above, the CN14 model was trained on
overt pronouns, but it is not clear how much this
helped its resolution performance. In particular,
the contexts in which overt and zero pronouns oc-
cur may not statistically resemble each other. For
example, a ZP is likely to be closer to its antecedent
than its overt counterpart. As another example,
the verbs governing a ZP and its antecedent are
more likely to be identical than the verbs govern-
ing an overt pronoun and its antecedent. Given
such differences, it is not clear whether the knowl-
edge learned from overt pronouns is always appli-
cable to the resolution of AZPs. For this reason,
we propose to train an unsupervised AZP resolu-
tion model directly on zero pronouns. Moreover,
while we previously employed a pipeline architec-
ture where we (1) used a set of heuristic rules for
AZP identification, and then (2) applied their prob-
abilistic model to all and only those ZPs that were
determined to be anaphoric (Chen and Ng, 2014),
in this work we identify and resolve AZPs in a
joint fashion. To our knowledge, the model we are
</bodyText>
<page confidence="0.915814">
320
</page>
<bodyText confidence="0.938093952380953">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 320–326,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
proposing here is the first unsupervised model for
joint AZP identification and resolution.1
In addition, motivated by work on overt pronoun
resolution, we hypothesize that AZP resolution can
be improved by exploiting discourse information.
Specifically, we design a model of salience and in-
corporate salience information into our model as a
feature. Inspired by traditional work on discourse-
based anaphora resolution (e.g., Lappin and Leass
(1994)), we compute salience based on the corefer-
ence clusters constructed so far using a rule-based
coreference resolver. While ZPs have been ex-
ploited to improve coreference resolution (Kong
and Ng, 2013), we are the first to improve AZP
resolution using coreference information.
When evaluated on the Chinese portion of the
OntoNotes corpus, our AZP resolver outperforms
the CN14 model, achieving state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999964">
Early approaches to AZP resolution employed
heuristic rules to resolve AZPs in Chinese (e.g.,
Converse (2006), Yeh and Chen (2007)) and Span-
ish (e.g., Ferrández and Peral (2000)). More re-
cently, supervised approaches have been exten-
sively employed to resolve AZPs in Chinese (e.g.,
Zhao and Ng (2007), Kong and Zhou (2010),
Chen and Ng (2013)), Korean (e.g., Han (2006)),
Japanese (e.g., Seki et al. (2002), Isozaki and Hi-
rao (2003), Iida et al. (2003; 2006; 2007), Ima-
mura et al. (2009), Iida and Poesio (2011), Sasano
and Kurohashi (2011)), and Italian (e.g., Iida and
Poesio (2011)). As mentioned before, in order
to reduce reliance on annotated data, we recently
proposed an unsupervised probabilistic model for
Chinese AZP resolution that rivaled its supervised
counterparts in performance (Chen and Ng, 2014).
</bodyText>
<sectionHeader confidence="0.992345" genericHeader="method">
3 The Generative Model
</sectionHeader>
<bodyText confidence="0.99993">
Next, we present our model for jointly identifying
and resolving AZPs in an unsupervised manner.
</bodyText>
<subsectionHeader confidence="0.988514">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.9999548">
Let z be a ZP. C, the set of candidate antecedents
of z, contains (1) the maximal or modifier NPs that
precede z in the associated text that are at most
two sentences away from it; and (2) a dummy can-
didate antecedent d (to which z will be resolved
</bodyText>
<footnote confidence="0.978579">
1Note that Iida and Poesio (2011) perform joint infer-
ence over an AZP identification model and an AZP resolution
model trained separately, not joint learning of the two tasks.
</footnote>
<bodyText confidence="0.5858278">
if it is non-anaphoric). k is the context surround-
ing z as well as every candidate antecedent c in
C; kc is the context surrounding z and candidate
antecedent c; and l is a binary variable indicating
whether c is the correct antecedent of z.
</bodyText>
<subsectionHeader confidence="0.999068">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.99995545">
Our model estimates P(z, k, c, l), the probability
of seeing (1) the ZP z; (2) the context k surround-
ing z and its candidate antecedents; (3) a candidate
antecedent c of z; and (4) whether c is the correct
antecedent of z. Since we estimate this probability
from a raw, unannotated corpus, we are treating z,
k, and c as observed data2 and l as hidden data.
Motivated in part by previous work on En-
glish overt pronoun resolution (e.g., Cherry and
Bergsma (2005) and Charniak and Elsner (2009)),
we estimate the model parameters using the
Expectation-Maximization algorithm (Dempster
et al., 1977). Specifically, we use EM to iteratively
(1) estimate the model parameters from data in
which each ZP is labeled with the probability that
it corefers with each of its candidate antecedents,
and (2) apply the resulting model to re-label each
ZP with the probability that it corefers with each of
its candidate antecedents. Below we describe the
details of the E-step and the M-step.
</bodyText>
<subsectionHeader confidence="0.780574">
3.2.1 E-Step
</subsectionHeader>
<bodyText confidence="0.999983142857143">
The goal of the E-step is to compute
P(l=1|z, k, c), the probability that a candi-
date antecedent c is the correct antecedent of
z given context k. Applying the definition of
conditional probability and the Theorem of Total
Probability, we can rewrite P(l=1|z, k, c) as
follows:
</bodyText>
<equation confidence="0.909378333333333">
P(z, k, c, l=1)
P(l=1|z, k, c) =
P(z, k, c, l=1) + P(z, k, c, l=0)
(1)
Assuming that exactly one of z&apos;s candidate an-
tecedents is its correct antecedent, we can rewrite
P(z, k, c, l=0) as follows:
�P(z, k, c, l=0) = P(z, k, c′, l=1) (2)
c′∈C,c′̸�c
</equation>
<bodyText confidence="0.9862212">
Given Equation (2), we can rewrite
2Here, we treat z as observed data because we assume that
the set of ZPs has been identified by a separate process. We
adopt the heuristics for ZP identification that we introduced
in Chen and Ng (2014).
</bodyText>
<page confidence="0.860821">
321
</page>
<equation confidence="0.987426285714286">
P(l=1|z, k, c) as follows:
P (z, k,c, l=1)
P(l=1|z, k, c) = ∑c′∈C P(z, k, c′, l=1) (3)
Applying the Chain Rule, we can rewrite
P(z, k, c, l=1) as follows:
P(z, k, c, l=1) = P(z|k, c, l=1) ∗ P(l=1|k, c)
∗ P(c|k) ∗ P(k)
</equation>
<bodyText confidence="0.986005">
Next, since z is a phonetically null form (and
therefore is not represented by any linguistic at-
tributes), we assume that each of its candidate an-
tecedents and the associated context has the same
probability of generating it. So we can rewrite
P(z|k, c, l=1) as follows:
</bodyText>
<equation confidence="0.898324">
P(z|k, c, l=1) = P(z|k, c′, l=1) ∀ c, c′ ∈ C
</equation>
<bodyText confidence="0.9996238">
Moreover, we assume that (1) given z and c&apos;s
context, the probability of c being the antecedent
of z is not affected by the context of the other can-
didate antecedents; and (2) kc is sufficient for de-
termining whether c is the antecedent of z. So,
</bodyText>
<equation confidence="0.8463506">
P(l=1|k, c) ≈ P(l=1|kc, c) ≈ P(l=1|kc) (6)
Next, applying Bayes Rule to P(l=1|kc), we
get:
P(kc|l=1)P(l=1) (7)
P(kc|l=1)P(l=1) + P(kc|l=0)P(l=0)
</equation>
<bodyText confidence="0.96495875">
Representing kc as a set of n features f1c , ... fnc
and assuming that each fic is conditionally inde-
pendent given l, we can approximate Expression
(7) as:
</bodyText>
<equation confidence="0.990775333333333">
∏i P(fic|l=1)P(l=1)
∏i P(fic|l=1)P(l=1) + ∏i P(fic|l=0)P(l=0)
(8)
</equation>
<bodyText confidence="0.999687333333333">
Furthermore, we assume that given context k,
each candidate antecedent of z is generated with
equal probability. In other words,
</bodyText>
<equation confidence="0.894467142857143">
P(c|k) = P(c′|k) ∀ c, c′ ∈ C (9)
Given Equations (4), (5), (8) and (9), we can
rewrite P(l=1|z, k, c) as:
P(z, k, c, l=1)
P(l=1|z, k, c) =
∑c′∈C P(z, k, c′,l=1)
P(z|k, c, l=1)∗P(l=1|k, c)∗P(c|k)
</equation>
<bodyText confidence="0.545272">
where
</bodyText>
<equation confidence="0.974280666666667">
∏Zx= P (fi x|l=1)P (l=1)+ ∏ P(fix|l=0)P(l=0)
i i
(11)
</equation>
<bodyText confidence="0.999938230769231">
As we can see from Equation (10), our model
has one group of parameters, namely P(fic|l=1).
Using Equation (10) and the current parameter es-
timates, we can compute P(l=1|z, k, c).
A point deserves mention before we describe
the M-step. By including d as a dummy candidate
antecedent for each z, we effectively model AZP
identification and resolution in a joint fashion. If
the model resolves z to d, it means that the model
posits z as non-anaphoric; on the other hand, if
the model resolves z to a non-dummy candidate
antecedent c, it means that the model posits z as
anaphoric and c as z&apos;s correct antecedent.
</bodyText>
<subsectionHeader confidence="0.656235">
3.2.2 M-Step
</subsectionHeader>
<bodyText confidence="0.85831525">
Given P(l=1|z, k, c), the goal of the M-step is to
(re)estimate the model parameters, P(l=1|kc), us-
ing maximum likelihood estimation. Specifically,
P(l=1|kc) is estimated as follows:
</bodyText>
<equation confidence="0.999842333333333">
P 1=1 k= Count(kc, l=1) + 0 12
( I c)
Count (kc) + 0 ∗ 2 ( )
</equation>
<bodyText confidence="0.999943428571428">
where Count(kc) is the number of times kc ap-
pears in the training data, Count(kc, l=1) is the
expected number of times kc is the context sur-
rounding an AZP and its antecedent c, and 0 is the
Laplace smoothing parameter, which we set to 1.
Given context k′c, we compute Count(k′c, l=1) as
follows:
</bodyText>
<equation confidence="0.902435">
Count(k′c, l=1) = ∑ P(l=1|z, k, c) (13)
k:kc=k′c
</equation>
<bodyText confidence="0.999735625">
To start the induction process, we initialize all
parameters with uniform values. Specifically,
P(l=1|kc) is set to 0.5. Then we iteratively run
the E-step and the M-step until convergence.
There is an important question we have not ad-
dressed: what features should we use to represent
context kc, which we need to estimate P(l=1|kc)?
We answer this question in Section 4.
</bodyText>
<table confidence="0.995663666666667">
= 3.3 Inference
≈P(l=1 |kc) ∏i P(fic|l=1) ∑c′∈C P(z|k, c′, l=1)∗P(l=1|k, c′)∗P(c′|k)
∑c′∈C P(l=1|kc′) ≈ Zc After training, we can apply the resulting model to
∑c′∈C ∏i P(Zc/ resolve ZPs. Given a test document, we process its
(10) ZPs in a left-to-right manner. For each ZP z enoun-
tered, we determine its antecedent as follows:
</table>
<page confidence="0.962908">
322
</page>
<equation confidence="0.987086">
c� = arg max P(l=1|z, k, c) (14)
cEC
</equation>
<bodyText confidence="0.9999701">
where C is the set of candidate antecedents of z. If
we resolve a ZP to a preceding NP c, we fill its gap
with c. Hence, when we process a ZP z, all of its
preceding AZPs in the associated text have already
been resolved, having had their gaps filled with
their associated NPs. To resolve z, we create test
instances between z and its candidate antecedents
in the same way we described before. The only dif-
ference is that z&apos;s candidate antecedents may now
include the NPs to which previous AZPs were re-
solved. In other words, this incremental resolution
procedure may increase the number of candidate
antecedents of each ZP z. Some of these addi-
tional candidate antecedents are closer to z than
were their parent NPs, thus facilitating the resolu-
tion of z to the NPs in the following way: If the
model resolves z to the additional candidate an-
tecedent that fills the gap left behind by, say, AZP
z′, we postprocess the output by resolving z to the
NP that z′ is resolved to.3
</bodyText>
<sectionHeader confidence="0.990605" genericHeader="method">
4 Context Features
</sectionHeader>
<bodyText confidence="0.87343125">
To fully specify our model, we need to describe
how to represent kc, which is needed to compute
P(l=1|kc). Recall that kc encodes the context sur-
rounding candidate antecedent c and the associated
ZP z. As described below, we represent kc using
eight features. Note that (1) all but feature 1 are
computed based on syntactic parse trees, and (2)
features 2, 3, and 6 are ternary-valued features.
</bodyText>
<listItem confidence="0.994040933333333">
1. the sentence distance between c and z;
2. whether the node spanning c has an ancestor
NP node; if so, whether this NP node is a de-
scendant of c&apos;s lowest ancestor IP node;
3. whether the node spanning c has an ancestor
VP node; if so, whether this VP node is a de-
scendant of c&apos;s lowest ancestor IP node;
4. whether vp has an ancestor NP node, where
vp is the VP node spanning the VP that fol-
lows z;
5. whether vp has an ancestor VP node;
6. whether z is the first word of a sentence; if
not, whether z is the first word of an IP clause;
7. whether c is a subject whose governing verb
is lexically identical to the verb governing z;
</listItem>
<footnote confidence="0.6520615">
3This postprocessing step is needed because the additional
candidate antecedents are only gap fillers.
</footnote>
<table confidence="0.9995808">
Training Test
Documents 1,391 172
Sentences 36,487 6,083
Words 756,063 110,034
AZPs − 1,713
</table>
<tableCaption confidence="0.8075605">
Table 1: Statistics on the training and test sets.
8. c&apos;s salience rank (see Section 5).
</tableCaption>
<bodyText confidence="0.9998583">
Note that features 1, 2, 3 and 7 are not directly
applicable to the dummy candidate. To compute
the feature values of the dummy candidate, we first
find the highest ranking non-dummy entity E in
the salience list, and then set the values of these
four features of the dummy candidate to the corre-
sponding feature values of the rightmost mention
of E. The motivation is that we want the dummy
candidate to compete with the most salient non-
dummy candidate.
</bodyText>
<sectionHeader confidence="0.962997" genericHeader="method">
5 Adding Salience
</sectionHeader>
<bodyText confidence="0.999816666666667">
Recall from Section 4 that feature 8 requires the
computation of salience. Intuitively, salient enti-
ties are more likely to contain the antecedent of an
AZP.
We model salience as follows. For each ZP z,
we compute the salience score for each (partial)
entity preceding z.4 To reduce the size of the list
of preceding entities, we only consider a partial
entity active if one of its mentions appears within
two sentences of the active ZP z. We compute the
salience score of each active entity w.r.t. z using
the following equation:
</bodyText>
<equation confidence="0.969329">
� g(m) * decay(m) (15)
MEE
</equation>
<bodyText confidence="0.99879475">
where m is a mention belonging to active entity
E, g(m) is a grammatical score which is set to
4, 2, or 1 depending on whether m&apos;s grammati-
cal role is SUBJECT, OBJECT, or OTHER, respec-
tively, and decay(m) is decay factor that is set to
0.5dZ3 (where dis is the sentence distance between
m and z). After computing the scores, we first sort
the list of the active entities in descending order of
salience. Then, within each active entity, we sort
the mentions in increasing order of distance from
z. Finally, we set the salience rank of each mention
m to its position in the sorted list, but cap the rank
</bodyText>
<footnote confidence="0.98197525">
4We compute the list of preceding entities automatically
using SinoCoreferencer, a publicly available Chinese en-
tity coreference resolver. See http://www.hlt.utdallas.
edu/~yzcchen/coreference/.
</footnote>
<page confidence="0.994254">
323
</page>
<table confidence="0.9982654">
Setting 1: Gold Parses, Gold AZPs Setting 2: Gold Parses, System AZPs Setting 3: System Parses, System AZPs
Baseline Our Model Baseline Our Model Baseline Our Model
Source R P F R P F R P F R P F R P F R P F
Overall 47.5 47.9 47.7 50.0 50.4 50.2 35.4 21.0 26.4 35.7 26.2 30.3 19.9 12.9 15.7 19.6 15.5 17.3
NW 41.7 41.7 41.7 46.4 46.4 46.4 29.8 24.8 27.0 32.1 28.1 30.0 11.9 13.0 12.4 11.9 14.3 13.0
MZ 34.0 34.2 34.1 38.9 39.1 39.0 24.1 14.5 18.1 29.6 19.6 23.6 6.2 5.2 5.7 4.9 4.7 4.8
WB 47.9 47.9 47.9 51.8 51.8 51.8 37.3 18.7 24.9 39.1 22.9 28.9 19.0 11.3 14.2 20.1 14.3 16.7
BN 52.8 52.8 52.8 53.8 53.8 53.8 31.5 28.1 29.7 30.8 30.7 30.7 18.2 19.5 18.8 18.2 22.3 20.0
BC 49.8 50.3 50.0 49.2 49.6 49.4 38.0 21.0 27.0 35.9 26.6 30.6 20.6 12.4 15.5 19.4 14.6 16.7
TC 45.2 46.7 46.0 51.9 53.5 52.7 42.4 20.3 27.4 43.5 28.7 34.6 32.2 13.3 18.8 31.8 17.0 22.2
</table>
<tableCaption confidence="0.998073">
Table 2: AZP resolution results of the baseline and our model on the test set.
</tableCaption>
<bodyText confidence="0.997580125">
at 5 in order to reduce sparseness during parameter
estimation.
Note that the above list contains only non-
dummy entities. We model the salience of a
dummy entity D, which contains only the dummy
candidate for z, as follows. Intuitively, if z is non-
anaphoric, D should be the most salient entity.
Hence, we put D at the top of the list if z satisfies
any of the following three conditions, all of which
are strong indicators of non-anaphoricity: (1) z ap-
pears at the beginning of a document; (2) the verb
following z is 有 (there is) or 没有 (there is not)
with part of speech VE; or (3) the VP node in the
syntactic parse tree following z does not span any
verb. If none of these conditions is satisfied, we
put D at the bottom of the list.
</bodyText>
<sectionHeader confidence="0.999502" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999328">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999981066666667">
Datasets. We employ the Chinese portion of the
OntoNotes 5.0 corpus that was used in the official
CoNLL-2012 shared task (Pradhan et al., 2012).
In the CoNLL-2012 data, the training set and de-
velopment set contain ZP coreference annotations,
but the test set does not. Therefore, we train our
models on the training set and perform evaluation
on the development set. Statistics on the datasets
are shown in Table 1. The documents in these
datasets come from six sources, namely Broadcast
News (BN), Newswires (NW), Broadcast Conver-
sations (BC), Telephone Conversations (TC), Web
Blogs (WB), and Magazines (MZ).
Evaluation measures. We express results in
terms of recall (R), precision (P), and F-score (F)
on resolving AZPs, considering an AZP z correctly
resolved if it is resolved to any NP in the same
coreference chain as z.
Evaluation settings. Following Chen and Ng
(2014), we evaluate our model in three settings. In
Setting 1, we assume the availability of gold syn-
tactic parse trees and gold AZPs.5 In Setting 2, we
employ gold syntactic parse trees and system (i.e.,
automatically identified) AZPs. Finally, in Set-
ting 3 (the end-to-end setting), we employ system
syntactic parse trees and system AZPs. The gold
and system syntactic parse trees, as well as the gold
AZPs, are obtained from the CoNLL-2012 shared
task dataset, while the system AZPs are identified
by our generative model.
</bodyText>
<subsectionHeader confidence="0.888585">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999974875">
As our baseline, we employ the CN14 system,
which has achieved the best result to date on our
test set. Table 2 shows results obtained using both
the baseline system and our model on the entire
test set as well as on each of the six sources. As
we can see, our model significantly6 outperforms
the baseline under all three settings by 2.5%, 3.9%
and 1.6% respectively in terms of overall F-score.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999816714285714">
We proposed a novel unsupervised model for Chi-
nese zero pronoun resolution by (1) training on
zero pronouns; (2) jointly identifying and resolv-
ing anaphoric zero pronouns; and (3) exploit-
ing salience information. Experiments on the
OntoNotes 5.0 corpus showed that our unsuper-
vised model achieved state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999535">
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of this paper. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
</bodyText>
<footnote confidence="0.998126">
5When gold AZPs are used (i.e., Setting 1), we simply
remove the dummy candidate antecedent from the list of can-
didate antecedents during inference.
6All significance tests are paired t-test, with p &lt; 0.05.
</footnote>
<page confidence="0.997283">
324
</page>
<sectionHeader confidence="0.995348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994019532110092">
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 148--156.
Chen Chen and Vincent Ng. 2013. Chinese zero pro-
noun resolution: Some recent advances. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1360--1365.
Chen Chen and Vincent Ng. 2014. Chinese zero
pronoun resolution: An unsupervised probabilistic
model rivaling supervised resolvers. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing, pages 763--774.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution.
In Proceedings of the Ninth Conference on Natural
Language Learning, pages 88--95.
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39:1--38.
Antonio Ferrández and Jesús Peral. 2000. A compu-
tational approach to zero-pronouns in Spanish. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 166--172.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203--226.
Na-Rae Han. 2006. Korean zero pronouns: analysis
and resolution. Ph.D. thesis, University of Pennsyl-
vania.
Jerry Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311--338.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 804--813.
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating contextual cues in
trainable models for coreference resolution. In Pro-
ceedings of the EACL Workshop on The Computa-
tional Treatment ofAnaphora, pages 23--30.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 625--632.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing, 6(4).
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85--88.
Hideki Isozaki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the 2003 Con-
ference on Empirical methods in natural language
processing, pages 184--191.
Fang Kong and Hwee Tou Ng. 2013. Exploiting zero
pronouns to improve chinese coreference resolution.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
278--288.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 882--891.
Shalom Lappin and Herbert Leass. 1994. An algorithm
for pronominal anaphora resolution. Computational
Linguistics, 20(4):535--562.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings of
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning: Shared Task, pages 1--40.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of the 5th International Joint Confer-
ence on Natural Language Processing, pages 758--
766.
Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and res-
olution. In Proceedings of the 19th International
Conference on Computational linguistics.
Ching-Long Yeh and Yi-Chun Chen. 2007. Zero
anaphora resolution in Chinese with shallow pars-
ing. Journal of Chinese Language and Computing,
17(1):41--56.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007
Joint Conference on Empirical Methods on Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 541--550.
</reference>
<page confidence="0.987641">
325
</page>
<reference confidence="0.9958284">
GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008.
Context-sensitive convolution tree kernel for pro-
noun resolution. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 25--31.
</reference>
<page confidence="0.999114">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801034">
<title confidence="0.9974235">Chinese Zero Pronoun Resolution: A Joint Discourse-Aware Model Rivaling State-of-the-Art Resolvers</title>
<author confidence="0.999915">Chen Ng</author>
<affiliation confidence="0.994559">Human Language Technology Research Institute University of Texas at</affiliation>
<address confidence="0.874323">Richardson, TX 75083-0688</address>
<email confidence="0.999774">yzcchen@hlt.utdallas.edu</email>
<email confidence="0.999774">vince@hlt.utdallas.edu</email>
<abstract confidence="0.994083214285714">We propose an unsupervised probabilistic model for zero pronoun resolution. To our knowledge, this is the first such model that (1) is trained on zero pronouns in an unsupervised manner; (2) jointly identifies and resolves anaphoric zero pronouns; and (3) exploits discourse information provided by a salience model. Experiments demonstrate that our unsupervised model significantly outperforms its state-of-the-art unsupervised counterpart when resolving the Chinese zero pronouns in the OntoNotes corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="7116" citStr="Charniak and Elsner (2009)" startWordPosition="1156" endWordPosition="1159">urrounding z and candidate antecedent c; and l is a binary variable indicating whether c is the correct antecedent of z. 3.2 Training Our model estimates P(z, k, c, l), the probability of seeing (1) the ZP z; (2) the context k surrounding z and its candidate antecedents; (3) a candidate antecedent c of z; and (4) whether c is the correct antecedent of z. Since we estimate this probability from a raw, unannotated corpus, we are treating z, k, and c as observed data2 and l as hidden data. Motivated in part by previous work on English overt pronoun resolution (e.g., Cherry and Bergsma (2005) and Charniak and Elsner (2009)), we estimate the model parameters using the Expectation-Maximization algorithm (Dempster et al., 1977). Specifically, we use EM to iteratively (1) estimate the model parameters from data in which each ZP is labeled with the probability that it corefers with each of its candidate antecedents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. 3.2.1 E-Step The goal of the E-step is to compute P(l=1|z, k, c), the probability that a candidate antecedent c i</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 148--156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Chinese zero pronoun resolution: Some recent advances.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1360--1365</pages>
<contexts>
<context position="5337" citStr="Chen and Ng (2013)" startWordPosition="845" endWordPosition="848">oreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 No</context>
</contexts>
<marker>Chen, Ng, 2013</marker>
<rawString>Chen Chen and Vincent Ng. 2013. Chinese zero pronoun resolution: Some recent advances. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1360--1365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Chinese zero pronoun resolution: An unsupervised probabilistic model rivaling supervised resolvers.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>763--774</pages>
<contexts>
<context position="2112" citStr="Chen and Ng, 2014" startWordPosition="323" endWordPosition="326">noun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, we have proposed an unsupervised AZP resolution model (henceforth the CN14 model) that rivals its supervised counterparts in performance (Chen and Ng, 2014). The idea is to resolve AZPs by using a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner. This is an appealing approach, as its language-independent generative process enables it to be applied to languages where data annotated with ZP links are not available. In light of the advantages of unsupervised models, we examine in this paper the possibility of advancing the state of the art in unsupervised AZP resolution. The design of our unsupervised model is motivated by a key question: can we resolve AZPs by using a probabilistic model trained on zero pro</context>
<context position="3733" citStr="Chen and Ng, 2014" startWordPosition="602" endWordPosition="605">g a ZP and its antecedent are more likely to be identical than the verbs governing an overt pronoun and its antecedent. Given such differences, it is not clear whether the knowledge learned from overt pronouns is always applicable to the resolution of AZPs. For this reason, we propose to train an unsupervised AZP resolution model directly on zero pronouns. Moreover, while we previously employed a pipeline architecture where we (1) used a set of heuristic rules for AZP identification, and then (2) applied their probabilistic model to all and only those ZPs that were determined to be anaphoric (Chen and Ng, 2014), in this work we identify and resolve AZPs in a joint fashion. To our knowledge, the model we are 320 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 320–326, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics proposing here is the first unsupervised model for joint AZP identification and resolution.1 In addition, motivated by work on overt pronoun resolution, we hypothesize that AZP resolution can be improved by exploiting dis</context>
<context position="5809" citStr="Chen and Ng, 2014" startWordPosition="919" endWordPosition="922">upervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, the set of candidate antecedents of z, contains (1) the maximal or modifier NPs that precede z in the associated text that are at most two sentences away from it; and (2) a dummy candidate antecedent d (to which z will be resolved 1Note that Iida and Poesio (2011) perform joint inference over an AZP identification model and an AZP resolution model trained separately, not joint learning of the two tasks. if it is non-anaphoric). k is the contex</context>
<context position="8378" citStr="Chen and Ng (2014)" startWordPosition="1377" endWordPosition="1380">k. Applying the definition of conditional probability and the Theorem of Total Probability, we can rewrite P(l=1|z, k, c) as follows: P(z, k, c, l=1) P(l=1|z, k, c) = P(z, k, c, l=1) + P(z, k, c, l=0) (1) Assuming that exactly one of z&apos;s candidate antecedents is its correct antecedent, we can rewrite P(z, k, c, l=0) as follows: �P(z, k, c, l=0) = P(z, k, c′, l=1) (2) c′∈C,c′̸�c Given Equation (2), we can rewrite 2Here, we treat z as observed data because we assume that the set of ZPs has been identified by a separate process. We adopt the heuristics for ZP identification that we introduced in Chen and Ng (2014). 321 P(l=1|z, k, c) as follows: P (z, k,c, l=1) P(l=1|z, k, c) = ∑c′∈C P(z, k, c′, l=1) (3) Applying the Chain Rule, we can rewrite P(z, k, c, l=1) as follows: P(z, k, c, l=1) = P(z|k, c, l=1) ∗ P(l=1|k, c) ∗ P(c|k) ∗ P(k) Next, since z is a phonetically null form (and therefore is not represented by any linguistic attributes), we assume that each of its candidate antecedents and the associated context has the same probability of generating it. So we can rewrite P(z|k, c, l=1) as follows: P(z|k, c, l=1) = P(z|k, c′, l=1) ∀ c, c′ ∈ C Moreover, we assume that (1) given z and c&apos;s context, the pr</context>
<context position="18601" citStr="Chen and Ng (2014)" startWordPosition="3229" endWordPosition="3232">oes not. Therefore, we train our models on the training set and perform evaluation on the development set. Statistics on the datasets are shown in Table 1. The documents in these datasets come from six sources, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Conversations (TC), Web Blogs (WB), and Magazines (MZ). Evaluation measures. We express results in terms of recall (R), precision (P), and F-score (F) on resolving AZPs, considering an AZP z correctly resolved if it is resolved to any NP in the same coreference chain as z. Evaluation settings. Following Chen and Ng (2014), we evaluate our model in three settings. In Setting 1, we assume the availability of gold syntactic parse trees and gold AZPs.5 In Setting 2, we employ gold syntactic parse trees and system (i.e., automatically identified) AZPs. Finally, in Setting 3 (the end-to-end setting), we employ system syntactic parse trees and system AZPs. The gold and system syntactic parse trees, as well as the gold AZPs, are obtained from the CoNLL-2012 shared task dataset, while the system AZPs are identified by our generative model. 6.2 Results As our baseline, we employ the CN14 system, which has achieved the b</context>
</contexts>
<marker>Chen, Ng, 2014</marker>
<rawString>Chen Chen and Vincent Ng. 2014. Chinese zero pronoun resolution: An unsupervised probabilistic model rivaling supervised resolvers. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 763--774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Shane Bergsma</author>
</authors>
<title>An expectation maximization approach to pronoun resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Natural Language Learning,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="7085" citStr="Cherry and Bergsma (2005)" startWordPosition="1151" endWordPosition="1154">nt c in C; kc is the context surrounding z and candidate antecedent c; and l is a binary variable indicating whether c is the correct antecedent of z. 3.2 Training Our model estimates P(z, k, c, l), the probability of seeing (1) the ZP z; (2) the context k surrounding z and its candidate antecedents; (3) a candidate antecedent c of z; and (4) whether c is the correct antecedent of z. Since we estimate this probability from a raw, unannotated corpus, we are treating z, k, and c as observed data2 and l as hidden data. Motivated in part by previous work on English overt pronoun resolution (e.g., Cherry and Bergsma (2005) and Charniak and Elsner (2009)), we estimate the model parameters using the Expectation-Maximization algorithm (Dempster et al., 1977). Specifically, we use EM to iteratively (1) estimate the model parameters from data in which each ZP is labeled with the probability that it corefers with each of its candidate antecedents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. 3.2.1 E-Step The goal of the E-step is to compute P(l=1|z, k, c), the probability </context>
</contexts>
<marker>Cherry, Bergsma, 2005</marker>
<rawString>Colin Cherry and Shane Bergsma. 2005. An expectation maximization approach to pronoun resolution. In Proceedings of the Ninth Conference on Natural Language Learning, pages 88--95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Converse</author>
</authors>
<title>Pronominal Anaphora Resolution in Chinese.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5104" citStr="Converse (2006)" startWordPosition="807" endWordPosition="808">rk on discoursebased anaphora resolution (e.g., Lappin and Leass (1994)), we compute salience based on the coreference clusters constructed so far using a rule-based coreference resolver. While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic mod</context>
</contexts>
<marker>Converse, 2006</marker>
<rawString>Susan Converse. 2006. Pronominal Anaphora Resolution in Chinese. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>39--1</pages>
<contexts>
<context position="7220" citStr="Dempster et al., 1977" startWordPosition="1169" endWordPosition="1172">cedent of z. 3.2 Training Our model estimates P(z, k, c, l), the probability of seeing (1) the ZP z; (2) the context k surrounding z and its candidate antecedents; (3) a candidate antecedent c of z; and (4) whether c is the correct antecedent of z. Since we estimate this probability from a raw, unannotated corpus, we are treating z, k, and c as observed data2 and l as hidden data. Motivated in part by previous work on English overt pronoun resolution (e.g., Cherry and Bergsma (2005) and Charniak and Elsner (2009)), we estimate the model parameters using the Expectation-Maximization algorithm (Dempster et al., 1977). Specifically, we use EM to iteratively (1) estimate the model parameters from data in which each ZP is labeled with the probability that it corefers with each of its candidate antecedents, and (2) apply the resulting model to re-label each ZP with the probability that it corefers with each of its candidate antecedents. Below we describe the details of the E-step and the M-step. 3.2.1 E-Step The goal of the E-step is to compute P(l=1|z, k, c), the probability that a candidate antecedent c is the correct antecedent of z given context k. Applying the definition of conditional probability and th</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39:1--38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Ferrández</author>
<author>Jesús Peral</author>
</authors>
<title>A computational approach to zero-pronouns in Spanish.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>166--172</pages>
<contexts>
<context position="5172" citStr="Ferrández and Peral (2000)" startWordPosition="817" endWordPosition="820">d Leass (1994)), we compute salience based on the coreference clusters constructed so far using a rule-based coreference resolver. While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterpar</context>
</contexts>
<marker>Ferrández, Peral, 2000</marker>
<rawString>Antonio Ferrández and Jesús Peral. 2000. A computational approach to zero-pronouns in Spanish. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 166--172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203--226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
</authors>
<title>Korean zero pronouns: analysis and resolution.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5364" citStr="Han (2006)" startWordPosition="851" endWordPosition="852"> 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, th</context>
</contexts>
<marker>Han, 2006</marker>
<rawString>Na-Rae Han. 2006. Korean zero pronouns: analysis and resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<journal>Lingua,</journal>
<pages>44--311</pages>
<marker>Hobbs, 1978</marker>
<rawString>Jerry Hobbs. 1978. Resolving pronoun references. Lingua, 44:311--338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Massimo Poesio</author>
</authors>
<title>A cross-lingual ILP solution to zero anaphora resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume</booktitle>
<volume>1</volume>
<pages>804--813</pages>
<contexts>
<context position="5506" citStr="Iida and Poesio (2011)" startWordPosition="875" endWordPosition="878">oNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, the set of candidate antecedents of z, contains (1) the maximal or modifier NPs that precede z in the associated text that are at most two sente</context>
</contexts>
<marker>Iida, Poesio, 2011</marker>
<rawString>Ryu Iida and Massimo Poesio. 2011. A cross-lingual ILP solution to zero anaphora resolution. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 804--813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Hiroya Takamura</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Incorporating contextual cues in trainable models for coreference resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on The Computational Treatment ofAnaphora,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="5446" citStr="Iida et al. (2003" startWordPosition="864" endWordPosition="867">ation. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, the set of candidate antecedents of z, contains (1) the maximal or modifier NPs that</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Matsumoto. 2003. Incorporating contextual cues in trainable models for coreference resolution. In Proceedings of the EACL Workshop on The Computational Treatment ofAnaphora, pages 23--30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Exploting syntactic patterns as clues in zero-anaphora resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>625--632</pages>
<marker>Iida, Inui, Matsumoto, 2006</marker>
<rawString>Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Exploting syntactic patterns as clues in zero-anaphora resolution. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 625--632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Zero-anaphora resolution by learning rich syntactic pattern features.</title>
<date>2007</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>6</volume>
<issue>4</issue>
<marker>Iida, Inui, Matsumoto, 2007</marker>
<rawString>Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Zero-anaphora resolution by learning rich syntactic pattern features. ACM Transactions on Asian Language Information Processing, 6(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Kuniko Saito</author>
<author>Tomoko Izumi</author>
</authors>
<title>Discriminative approach to predicateargument structure analysis with zero-anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>85--88</pages>
<contexts>
<context position="5482" citStr="Imamura et al. (2009)" startWordPosition="870" endWordPosition="874">nese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, the set of candidate antecedents of z, contains (1) the maximal or modifier NPs that precede z in the associated text th</context>
</contexts>
<marker>Imamura, Saito, Izumi, 2009</marker>
<rawString>Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicateargument structure analysis with zero-anaphora resolution. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 85--88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
</authors>
<title>Japanese zero pronoun resolution based on ranking rules and machine learning.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical methods in natural language processing,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="5427" citStr="Isozaki and Hirao (2003)" startWordPosition="859" endWordPosition="863">n using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, the set of candidate antecedents of z, contains (1) the maximal o</context>
</contexts>
<marker>Isozaki, Hirao, 2003</marker>
<rawString>Hideki Isozaki and Tsutomu Hirao. 2003. Japanese zero pronoun resolution based on ranking rules and machine learning. In Proceedings of the 2003 Conference on Empirical methods in natural language processing, pages 184--191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Exploiting zero pronouns to improve chinese coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>278--288</pages>
<contexts>
<context position="4760" citStr="Kong and Ng, 2013" startWordPosition="755" endWordPosition="758">supervised model for joint AZP identification and resolution.1 In addition, motivated by work on overt pronoun resolution, we hypothesize that AZP resolution can be improved by exploiting discourse information. Specifically, we design a model of salience and incorporate salience information into our model as a feature. Inspired by traditional work on discoursebased anaphora resolution (e.g., Lappin and Leass (1994)), we compute salience based on the coreference clusters constructed so far using a rule-based coreference resolver. While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2</context>
</contexts>
<marker>Kong, Ng, 2013</marker>
<rawString>Fang Kong and Hwee Tou Ng. 2013. Exploiting zero pronouns to improve chinese coreference resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 278--288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Guodong Zhou</author>
</authors>
<title>A tree kernelbased unified framework for Chinese zero anaphora resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>882--891</pages>
<contexts>
<context position="1938" citStr="Kong and Zhou (2010)" startWordPosition="297" endWordPosition="300"> As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as NUMBER and GENDER. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, we have proposed an unsupervised AZP resolution model (henceforth the CN14 model) that rivals its supervised counterparts in performance (Chen and Ng, 2014). The idea is to resolve AZPs by using a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner. This is an appealing approach, as its language-independent generative process enables it to be applied to languages where data annotated with ZP links are not available. In light of the advantages of unsupervised models, we examine in this paper the possibility of advancing the state of the a</context>
<context position="5317" citStr="Kong and Zhou (2010)" startWordPosition="841" endWordPosition="844">exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsuper</context>
</contexts>
<marker>Kong, Zhou, 2010</marker>
<rawString>Fang Kong and Guodong Zhou. 2010. A tree kernelbased unified framework for Chinese zero anaphora resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882--891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="4560" citStr="Lappin and Leass (1994)" startWordPosition="724" endWordPosition="727">nternational Joint Conference on Natural Language Processing (Short Papers), pages 320–326, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics proposing here is the first unsupervised model for joint AZP identification and resolution.1 In addition, motivated by work on overt pronoun resolution, we hypothesize that AZP resolution can be improved by exploiting discourse information. Specifically, we design a model of salience and incorporate salience information into our model as a feature. Inspired by traditional work on discoursebased anaphora resolution (e.g., Lappin and Leass (1994)), we compute salience based on the coreference clusters constructed so far using a rule-based coreference resolver. While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and </context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535--562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--40</pages>
<contexts>
<context position="17866" citStr="Pradhan et al., 2012" startWordPosition="3110" endWordPosition="3113">Hence, we put D at the top of the list if z satisfies any of the following three conditions, all of which are strong indicators of non-anaphoricity: (1) z appears at the beginning of a document; (2) the verb following z is 有 (there is) or 没有 (there is not) with part of speech VE; or (3) the VP node in the syntactic parse tree following z does not span any verb. If none of these conditions is satisfied, we put D at the bottom of the list. 6 Evaluation 6.1 Experimental Setup Datasets. We employ the Chinese portion of the OntoNotes 5.0 corpus that was used in the official CoNLL-2012 shared task (Pradhan et al., 2012). In the CoNLL-2012 data, the training set and development set contain ZP coreference annotations, but the test set does not. Therefore, we train our models on the training set and perform evaluation on the development set. Statistics on the datasets are shown in Table 1. The documents in these datasets come from six sources, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Conversations (TC), Web Blogs (WB), and Magazines (MZ). Evaluation measures. We express results in terms of recall (R), precision (P), and F-score (F) on resolving AZPs, considering an AZP</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task, pages 1--40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>758--766</pages>
<contexts>
<context position="5535" citStr="Sasano and Kurohashi (2011)" startWordPosition="879" endWordPosition="882">esolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, the set of candidate antecedents of z, contains (1) the maximal or modifier NPs that precede z in the associated text that are at most two sentences away from it; and (2) a </context>
</contexts>
<marker>Sasano, Kurohashi, 2011</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2011. A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 758--766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Seki</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics.</booktitle>
<contexts>
<context position="5401" citStr="Seki et al. (2002)" startWordPosition="855" endWordPosition="858">mprove AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolving AZPs in an unsupervised manner. 3.1 Notation Let z be a ZP. C, the set of candidate antecedents of z, </context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa. 2002. A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution. In Proceedings of the 19th International Conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Yi-Chun Chen</author>
</authors>
<title>Zero anaphora resolution in Chinese with shallow parsing.</title>
<date>2007</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="5125" citStr="Yeh and Chen (2007)" startWordPosition="809" endWordPosition="812">sed anaphora resolution (e.g., Lappin and Leass (1994)), we compute salience based on the coreference clusters constructed so far using a rule-based coreference resolver. While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP re</context>
</contexts>
<marker>Yeh, Chen, 2007</marker>
<rawString>Ching-Long Yeh and Yi-Chun Chen. 2007. Zero anaphora resolution in Chinese with shallow parsing. Journal of Chinese Language and Computing, 17(1):41--56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Identification and resolution of Chinese zero pronouns: A machine learning approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods on Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>541--550</pages>
<contexts>
<context position="1916" citStr="Zhao and Ng (2007)" startWordPosition="293" endWordPosition="296">e political crisis.) As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as NUMBER and GENDER. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g., Zhao and Ng (2007), Kong and Zhou (2010)). More recently, we have proposed an unsupervised AZP resolution model (henceforth the CN14 model) that rivals its supervised counterparts in performance (Chen and Ng, 2014). The idea is to resolve AZPs by using a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner. This is an appealing approach, as its language-independent generative process enables it to be applied to languages where data annotated with ZP links are not available. In light of the advantages of unsupervised models, we examine in this paper the possibility of advanc</context>
<context position="5295" citStr="Zhao and Ng (2007)" startWordPosition="837" endWordPosition="840">While ZPs have been exploited to improve coreference resolution (Kong and Ng, 2013), we are the first to improve AZP resolution using coreference information. When evaluated on the Chinese portion of the OntoNotes corpus, our AZP resolver outperforms the CN14 model, achieving state-of-the-art results. 2 Related Work Early approaches to AZP resolution employed heuristic rules to resolve AZPs in Chinese (e.g., Converse (2006), Yeh and Chen (2007)) and Spanish (e.g., Ferrández and Peral (2000)). More recently, supervised approaches have been extensively employed to resolve AZPs in Chinese (e.g., Zhao and Ng (2007), Kong and Zhou (2010), Chen and Ng (2013)), Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2003; 2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). As mentioned before, in order to reduce reliance on annotated data, we recently proposed an unsupervised probabilistic model for Chinese AZP resolution that rivaled its supervised counterparts in performance (Chen and Ng, 2014). 3 The Generative Model Next, we present our model for jointly identifying and resolv</context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2007. Identification and resolution of Chinese zero pronouns: A machine learning approach. In Proceedings of the 2007 Joint Conference on Empirical Methods on Natural Language Processing and Computational Natural Language Learning, pages 541--550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Context-sensitive convolution tree kernel for pronoun resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>25--31</pages>
<marker>Zhou, Kong, Zhu, 2008</marker>
<rawString>GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008. Context-sensitive convolution tree kernel for pronoun resolution. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 25--31.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>