<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9981535">
General-to-Specific Model Selection
for Subcategorization Preference*
</title>
<author confidence="0.699421">
Takehito Utsuro and Takashi Miyata and Yuji Matsumoto
</author>
<affiliation confidence="0.60531">
Graduate School of Information Science, Nara Institute of Science and Technology
</affiliation>
<address confidence="0.593514">
8916-5, Takayama-cho, Ikoma-shi, Nara, 630-0101, JAPAN
</address>
<email confidence="0.7734">
E-mail: utsurais.aist-nara.ac. jp, URL: http://cl.aist-nara.ac.jp/utsuro/
</email>
<sectionHeader confidence="0.995466" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938583333333">
This paper proposes a novel method for learning
probability models of subcategorization preference of
verbs. We consider the issues of case dependencies
and noun class generalization in a uniform way by em-
ploying the maximum entropy modeling method. We
also propose a new model selection algorithm which
starts from the most general model and gradually ex-
amines more specific models. In the experimental
evaluation, it is shown that both of the case depen-
dencies and specific sense restriction selected by the
proposed method contribute to improving the perfor-
mance in subcategorization preference resolution.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999873892857143">
In empirical approaches to parsing, lexi-
cal/semantic collocation extracted from corpus
has been proved to be quite useful for ranking
parses in syntactic analysis. For example, Mager-
man (1995), Collins (1996), and Charniak (1997)
proposed statistical parsing models which incor-
porated lexical/semantic information. In their
models, syntactic and lexical/semantic features
are dependent on each other and are combined
together. This paper also proposes a method
of utilizing lexical/semantic features for the pur-
pose of applying them to ranking parses in syn-
tactic analysis. However, unlike the models of
Magerman (1995), Collins (1996), and Char-
niak (1997), we assume that syntactic and lex-
ical/semantic features are independent. Then,
we focus on extracting lexical/semantic colloca-
tional knowledge of verbs which is useful in syn-
tactic analysis.
More specifically, we propose a novel method
for learning a probability model of subcategoriza-
tion preference of verbs. In general, when learn-
ing lexical/semantic collocational knowledge of
verbs from corpus, it is necessary to consider
the two issues of 1) case dependencies, and 2)
noun class generalization. When considering 1),
we have to decide which cases are dependent on
each other and which cases are optional and in-
</bodyText>
<footnote confidence="0.9581456">
This research was partially supported by the Ministry
of Education, Science, Sports and Culture, Japan, Grant-
in-Aid for Encouragement of Young Scientists, 09780338,
1998. An extended version of this paper is available from
the above URL.
</footnote>
<bodyText confidence="0.999729125">
dependent of other cases. When considering 2),
we have to decide which superordinate class gen-
erates each observed leaf class in the verb-noun
collocation. So far, there exist several works
which worked on these two issues in learning col-
locational knowledge of verbs and also evaluated
the results in terms of syntactic disambiguation.
Resnik (1993) and Li and Abe (1995) studied how
to find an optimal abstraction level of an argu-
ment noun in a tree-structured thesaurus. Their
works are limited to only one argument. Li and
Abe (1996) also studied a method for learning de-
pendencies between case slots and reported that
dependencies were discovered only at the slot-
level and not at the class-level.
Compared with these previous works, this pa-
per proposes to consider the above two issues
in a uniform way. First, we introduce a model
of generating a collocation of a verb and argu-
ment/adjunct nouns (section 2) and then view
the model as a probability model (section 3). As
a model learning method, we adopt the max-
imum entropy model learning method (Della
Pietra et al., 1997; Berger et al., 1996). Case
dependencies and noun class generalization are
represented as features in the maximum entropy
approach. Features are allowed to have overlap
and this is quite advantageous when we consider
case dependencies and noun class generalization
in parameter estimation. An optimal model is se-
lected by searching for an optimal set of features,
i.e, optimal case dependencies and optimal noun
class generalization levels. As the feature selec-
tion process, this paper proposes a new feature
selection algorithm which starts from the most
general model and gradually examines more spe-
cific models (section 4). As the model evalua-
tion criterion during the model search from gen-
eral to specific ones, we employ the description
length of the model and guide the search process
so as to minimize the description length (Ris-
sanen, 1984). Then, after obtaining a sequence
of subcategorization preference models which are
totally ordered from general to specific, we se-
lect an approximately optimal subcategorization
preference model according to the accuracy of
subcategorization preference test. In the exper-
imental evaluation of performance of subcatego-
</bodyText>
<page confidence="0.990539">
1314
</page>
<bodyText confidence="0.999458">
rization preference, it is shown that both of the
case dependencies and specific sense restriction
selected by the proposed method contribute to
improving the performance in subcategorization
preference resolution (section 5).
</bodyText>
<sectionHeader confidence="0.707056" genericHeader="introduction">
2 A Model of Generating a Verb-Noun
Collocation from Subcategorization
</sectionHeader>
<subsectionHeader confidence="0.351346">
Frame (s)
</subsectionHeader>
<bodyText confidence="0.697354666666667">
This section introduces a model of generating
a verb-noun collocation from subcategorization
frame(s).
</bodyText>
<subsectionHeader confidence="0.983377">
2.1 Data Structure
</subsectionHeader>
<bodyText confidence="0.962223714285714">
Verb-Noun Collocation Verb-noun colloca-
tion is a data structure for the collocation of a
verb and all of its argument/adjunct nouns. A
verb-noun collocation e is represented by a fea-
ture structure which consists of the verb v and
all the pairs of co-occurring case-markers p and
thesaurus classes c of case-marked nouns:
</bodyText>
<equation confidence="0.99373425">
e = pred : v 1
Pi : ci (1)
Pk Ck
• •
</equation>
<bodyText confidence="0.979198375">
We assume that a thesaurus is a tree-structured
type hierarchy in which each node represents
a semantic class, and each thesaurus class
, ck in a verb-noun collocation is a leaf class
in the thesaurus. We also introduce -c as the
superordinate-subordinate relation of classes in
a thesaurus: c1 c2 means that c1 is subordi-
nate to c2. 1
</bodyText>
<listItem confidence="0.37943525">
Subcategorization Frame A subcategoriza-
tion frame s is represented by a feature structure
which consists of a verb v and the pairs of case-
markers p and sense restriction c of case-marked
argument/adjunct nouns:
Fed : v
: ci
•
</listItem>
<bodyText confidence="0.9126643">
:
cation e and a subcategorization frame s:
e &lt;Sf s if. for each case-marker p, in s and its
noun class est, there exists the same
case-marker pi in e and its noun
class ce, is subordinate to cs„ i.e.
ce, --&lt;e Cs,
The subsumption relation -&lt;,/ is applicable also
as a subsumption relation of two subcategoriza-
tion frames.
</bodyText>
<subsectionHeader confidence="0.952557">
2.2 Generating a Verb-Noun Collocation
from Subcategorization Frame(s)
</subsectionHeader>
<bodyText confidence="0.432948">
Suppose a verb-noun collocation e is given as:
</bodyText>
<equation confidence="0.460593">
e Fed : v
Pk Cek
•
•
: cei (3)
Then, let us consider a tuple (Si, ,s,,) of
</equation>
<bodyText confidence="0.9893766">
partial subcategorization frames which satisfies
the following requirements: 0 the unification
Si A • • • A sn of all the partial subcategorization
frames has exactly the same case-markers as e
has as in (4), ii) each semantic class c81 of a
case-marked noun of the partial subcategoriza-
tion frames is superordinate to the correspond-
ing leaf semantic class cei of e as in (5), and iii)
any pair si and (i i&apos;) do not have common
case-markers as in (6):
</bodyText>
<equation confidence="0.9929955">
si A • • • A sn =
c
VjVj&apos; pi; 0
&apos;=1, ,n, i i&apos;) (6)
</equation>
<bodyText confidence="0.957492">
When a tuple (si, , s„) satisfies the above
three requirements, we assume that the tuple (si,
, sn) can generate the verb-noun collocation e
and denote as below:
</bodyText>
<equation confidence="0.9996247">
S =
(2)
cei
pred : v
Si = : cii
FPked : v
csi
Csk•
Csi (i=1,.k)
e (7)
</equation>
<bodyText confidence="0.9650044375">
Sense restriction , Ci of case-marked argu-
ment/adjunct nouns are represented by classes
at arbitrary levels of the thesaurus.
Subsumption Relation We introduce the
subsumption relation sf of a verb-noun collo-
&apos;Although we ignore sense ambiguities of case-marked
nouns in the definitions of this section, in the current
implementation, we deal with sense ambiguities of case-
marked nouns by deciding that a class c is superordinate
to an ambiguous leaf class Ci if c is superordinate to at
least one of the possible unambiguous classes of
As we will describe in section 3.2, we assume that
the partial subcategorization frames .51, . • • , sn
are regarded as events occurring independently
of each other and each of them is assigned an
independent parameter.
</bodyText>
<subsectionHeader confidence="0.990294">
2.3 Example
</subsectionHeader>
<bodyText confidence="0.99863625">
This section shows how we can incorporate case
dependencies and noun class generalization into
the model of generating a verb-noun collocation
from a tuple of partial subcategorization frames.
</bodyText>
<page confidence="0.95185">
1315
</page>
<bodyText confidence="0.841155615384615">
The Ambiguity of Case Dependencies
The problem of the ambiguity of case dependen-
cies is caused by the fact that, only by observing
each verb-noun collocation in corpus, it is not de-
cidable which cases are dependent on each other
and which cases are optional and independent of
other cases. Consider the following example:
Example 1
Kodomo-ga kouen-de juusu-wo nomu.
child-NOM park-at juice-ACC drink
(A child drinks juice at the park.)
The verb-noun collocation is represented as a
feature structure e below:
</bodyText>
<equation confidence="0.9973385">
pred : nomu
ga : cc
wo:c3
de : cp
</equation>
<bodyText confidence="0.947464166666667">
where cc, and ci represent the leaf classes
(in the thesaurus) of the nouns &amp;quot;kodomo(child)&amp;quot;,
&amp;quot;kouen(park)&amp;quot;, and &amp;quot;juusu(juice)&amp;quot;.
Next, we assume that the concepts &amp;quot;hu-
man&amp;quot;, &amp;quot;place&amp;quot;, and &amp;quot;beverage&amp;quot; are superordi-
nate to &amp;quot;kodomo(child)&amp;quot;, &amp;quot;kouen(park)&amp;quot;, and
&amp;quot;juusu(juice)&amp;quot;, respectively, and introduce the
corresponding classes chum, epic, and cheu as sense
restriction in subcategorization frames. Then,
according to the dependencies of cases, we can
consider several patterns of subcategorization
frames each of which can generate the verb-noun
collocation e.
If the three cases &amp;quot;ga(NOM)&amp;quot;, &amp;quot;wo(ACC)&amp;quot;,
and &amp;quot;de(at)&amp;quot; are dependent on each other and
it is not possible to find any division into several
independent subcategorization frames, e can be
regarded as generated from a subcategorization
frame containing all of the three cases:
predcum
:hnomu
ga:
wo:cbca
de : epic
Otherwise, if only the two cases &amp;quot;ga(NOM)&amp;quot;
and &amp;quot;wo(ACC)&amp;quot; are dependent on each other and
the &amp;quot;de(at)&amp;quot; case is independent of those two
cases, e can be regarded as generated from the
following two subcategorization frames indepen-
dently:
</bodyText>
<construct confidence="0.484624">
(1 prgaed: c: hun :mu
Cbev de : epic
pred : nomu ])
</construct>
<subsectionHeader confidence="0.833969">
The Ambiguity of Noun Class Generaliza-
</subsectionHeader>
<bodyText confidence="0.999847291666667">
tion The problem of the ambiguity of noun
class generalization is caused by the fact that,
only by observing each verb-noun collocation in
corpus, it is not decidable which superordinate
class generates each observed leaf class in the
verb-noun collocation. Let us again consider Ex-
ample 1. We assume that the concepts &amp;quot;mam-
mal&amp;quot; and &amp;quot;liquid&amp;quot; are superordinate to &amp;quot;human&amp;quot;
and &amp;quot;beverage&amp;quot;, respectively, and introduce the
corresponding classes cmum and gig. If we addi-
tionally allow these superordinate classes as sense
restriction in subcategorization frames, we can
consider several additional patterns of subcate-
gorization frames which can generate the verb-
noun collocation e.
Suppose that only the two cases &amp;quot;ga(NOM)&amp;quot;
and &amp;quot;wo(ACC)&amp;quot; are dependent on each other
and the &amp;quot;de(at)&amp;quot; case is independent of those two
cases as in the formula (10). Since the leaf class
cc (&amp;quot;child&amp;quot;) can be generated from either chum
or cmum, and also the leaf class ci (&amp;quot;juice&amp;quot;) can
be generated from either ch„ or cup e can be
regarded as generated according to either of the
four formulas (10) and (11)-413):
</bodyText>
<equation confidence="0.960877272727273">
pred:nmnu
ga: emam
wo:ebea
pred : nomu 1
[pred : nomu
de : epic I e(12)
ga : chum
pred:namu
ga : cmc„, Fred: nomu ])
de : epic --+ e (13)
wo:chg
</equation>
<sectionHeader confidence="0.984717" genericHeader="method">
3 Maximum Entropy Modeling of
</sectionHeader>
<subsectionHeader confidence="0.546614">
Subcategorization Preference
</subsectionHeader>
<bodyText confidence="0.99996275">
This section describes how we apply the maxi-
mum entropy modeling approach of Della Pietra
et al. (1997) and Berger et al. (1996) to model
learning of subcategorization preference.
</bodyText>
<subsectionHeader confidence="0.99552">
3.1 Maximum Entropy Modeling
</subsectionHeader>
<bodyText confidence="0.999118">
Given the training sample e of the events (x, y),
our task is to estimate the conditional probabil-
ity p(y I x) that, given a context x, the process
will output y. In order to express certain features
of the whole event (x, y), a binary-valued indica-
tor function is introduced and called a feature
function. Usually, we suppose that there exists a
large collection F of candidate features, and in-
clude in the model only a subset S of the full set
of candidate features F. We call S the set of ac-
tive features. Now, we assume that S contains n
feature functions. For each feature fi(E S), the
sets Vxi and Vy, indicate the sets of the values
of x and y for that feature. According to those
sets, each feature function fi will be defined as
follows:
</bodyText>
<equation confidence="0.764997">
1 if x E Vzi and y E Vyi
0 otherwise
</equation>
<bodyText confidence="0.999821428571429">
Then, in the maximum entropy modeling ap-
proach, the model with the maximum entropy
is selected among the possible models. With this
constraint, the conditional probability of the out-
put y given the context x can be estimated as the
following pA(y I x) of the form of the exponen-
tial family, where a parameter Ai is introduced
</bodyText>
<equation confidence="0.993076083333333">
e =
(8)
e (9)
e (10)
[
pred : nomu
de : epic ])
e(11)
fi(x,Y) =
1316
for each feature fi. exp(E Aifi(x, y))
px(Y I = E exp (E)fi(x,y))
</equation>
<bodyText confidence="0.9971381">
In the maximum entropy modeling approach,
each feature is assigned an independent param-
(14) eter, i.e., each (partial) subcategorization frame
is assigned an independent parameter.
Parameter Estimation Suppose that the set
S(C .F) of active features is found by the pro-
cedure of the next section. Then, the param-
eters of subcategorization frames are estimated
according to ITS Algorithm and the conditional
probability distribution ps(ep I v) is given as:
</bodyText>
<equation confidence="0.9882924">
exp E ft(v, ep))
LES
PS(ep I v) = (15)
E exp( E A, ft(v, ep))
f. ES
</equation>
<bodyText confidence="0.9995">
The parameter values A*i are estimated by an
algorithm called Improved Iterative Scaling (IIS)
algorithm.
Feature Selection by One-by-one Feature
Adding The feature selection process pre-
sented in Della Pietra et al. (1997) and Berger
et al. (1996) is an incremental procedure that
builds up S by successively adding features one-
by-one. It starts with S as empty, and, at each
step, selects the candidate feature which, when
adjoined to the set of active features S, pro-
duces the greatest increase in log-likelihood of
the training sample.
</bodyText>
<listItem confidence="0.4558115">
3.2 Modeling Subcategorization Prefer-
ence
</listItem>
<bodyText confidence="0.9454389">
Events In our task of model learning of sub-
categorization preference, each event (x, y) in
the training sample is a verb-noun collocation e,
which is defined in the formula (1). A verb-noun
collocation e can be divided into two parts: one
is the verbal part ev containing the verb v while
the other is the nominal part ep containing all the
pairs of case-markers p and thesaurus leaf classes
c of case-marked nouns:
: C11e = et, A ep = pred : v A
</bodyText>
<equation confidence="0.328391333333333">
•
Pk
: ek
</equation>
<bodyText confidence="0.9996798125">
Then, we define the context x of an event (x, y)
as the verb v and the output y as the nominal part
e of e, and each event in the training sample is
denoted as (v, ep):
x v , y e p
Features We represent each partial subcatego-
rization frame as a feature in the maximum en-
tropy modeling. According to the possible vari-
ations of case dependencies and noun class gen-
eralization, we consider every possible patterns
of subcategorization frames which can generate
a verb-noun collocation, and then construct the
full set .F of candidate features. Next, for the
given verb-noun collocation e, tuples of partial
subcategorization frames which can generate e
are collected into the set SF(e) as below:
</bodyText>
<equation confidence="0.979394">
SF(e) = f(si, • • • , sn)1(si, • • . , sn) el
</equation>
<bodyText confidence="0.98918175">
Then, for each partial subcategorization frame
s, a binary-valued feature function f s(v, ep) is de-
fined to be true if and only if at least one element
of the set S F (e) is a tuple (s1, , s,. . . , sn) that
</bodyText>
<equation confidence="0.9484695">
contains s:
1 if 3(si,...,s,...,sn)
fs(v,ep) = E S F(e = ([Fred : v] A ep))
0 otherwise
</equation>
<sectionHeader confidence="0.8471735" genericHeader="method">
4 General-to-Specific Feature Selec-
tion
</sectionHeader>
<bodyText confidence="0.999971615384615">
This section describes the new feature selection
algorithm which utilizes the subsumption rela-
tion of subcategorization frames. It starts from
the most general model, i.e., a model with no
case dependency as well as the most general
sense restrictions which correspond to the high-
est classes in the thesaurus. This starting model
has high coverage of the test data. Then, the al-
gorithm gradually examines more specific mod-
els with case dependencies as well as more spe-
cific sense restrictions which correspond to lower
classes in the thesaurus. The model search pro-
cess is guided by a model evaluation criterion.
</bodyText>
<subsectionHeader confidence="0.931107">
4.1 Partially-Ordered Feature Space
</subsectionHeader>
<bodyText confidence="0.979354478260869">
In section 2.1, we introduced subsumption rela-
tion --&lt;sf of two subcategorization frames. All the
subcategorization frames are partially ordered
according to this subsumption relation, and el-
ements of the set F of candidate features consti-
tute a partially ordered feature space.
Constraint on Active Feature Set
Throughout the feature selection process,
we put the following constraint on the active
feature set S:
Case Covering Constraint: for each verb-noun
collocation in the training set E, each case p (and
the leaf class marked by p) of e has to be covered
by at least one feature in S.
Initial Active Feature Set Initial set So of
active features is constructed by collecting fea-
tures which are not subsumed by any other can-
didate features in
So = fs Vs, (0 E F, s Zs/ s&apos; (16)
This constraint on the initial active feature set
means that each feature in So has only one case
and the sense restriction of the case is (one of)
the most general class(es).
</bodyText>
<page confidence="0.990486">
1317
</page>
<table confidence="0.430707">
Candidate Non-active Features for Re-
</table>
<bodyText confidence="0.995937625">
placement At each step of feature selection,
one of the active features is replaced with sev-
eral non-active features. Let g be a set of non-
active features which have never been active until
that step. Then, for each active feature ME S),
the set D1 (C g) of candidate non-active features
with which h is replaced has to satisfy the fol-
lowing two requirements2 3.
</bodyText>
<listItem confidence="0.979572333333333">
1. Subsumption with s: for each element fe of Dh,
s&apos; has to be subsumed by s.
2. Upper Bound of g: for each element fe of Df,,
</listItem>
<bodyText confidence="0.933187833333333">
and for each element f t of g, t does not subsume
i.e., D1 is a subset of the upper bound of g
with respect to the subsumption relation
Among all the possible replacements, the most
appropriate one is selected according to a model
evaluation criterion.
</bodyText>
<subsectionHeader confidence="0.995638">
4.2 Model Evaluation Criterion
</subsectionHeader>
<bodyText confidence="0.999965">
As the model evaluation criterion during feature
selection, we consider the following two types.
</bodyText>
<subsubsectionHeader confidence="0.856966">
4.2.1 MDL Principle
</subsubsectionHeader>
<bodyText confidence="0.9596475">
The MDL (Minimum Description Length) prin-
ciple (Rissanen, 1984) is a model selection crite-
rion. It is designed so as to &amp;quot;select the model that
has as much fit to a given data as possible and
that is as simple as possible.&amp;quot; The MDL princi-
ple selects the model that minimizes the follow-
ing description length l(111, D) of the probability
model M for the data D: 1
</bodyText>
<equation confidence="0.991356">
l(M, D) = — log L m (D) + —2Nm log PI (17)
</equation>
<bodyText confidence="0.9999205">
where log LM(D) is the log-likelihood of the
model M to the data D, NM is the number of
the parameters in the model M, and ID 1 is the
size of the data D.
</bodyText>
<subsectionHeader confidence="0.937025">
Description Length of Subcategorization
</subsectionHeader>
<bodyText confidence="0.831379666666667">
Preference Model The description length
l(ps,E) of the probability model ps (of (15)) for
the training data set E is given as1below:4
</bodyText>
<equation confidence="0.973782">
l(ps,£) = —&gt;.2 iogps(ep v) + —21Silog lE1 (18)
(v,e7,)Ee
</equation>
<bodyText confidence="0.684180375">
2The general-to-specific feature selection considers only
a small portion of the non-active features as the next can-
didate for the active feature, while the feature selection by
one-by-one feature adding considers all the non-active fea-
tures as the next candidate. Thus, in terms of efficiency,
the general-to-specific feature selection has an advantage
over the one-by-one feature adding algorithm, especially
when the number of the candidate features is large.
</bodyText>
<footnote confidence="0.918093909090909">
3As long as the case covering constraint is satisfied, the
set Df. of candidate non-active features with which f„ is
replaced could be an empty set 0.
4More precisely, we slightly modify the probability
model ps by multiplying the probability of generating the
verb-noun collocation e from the (partial) subcategoriza-
tion frames that correspond to active features evaluating
to true for e, and then apply the MDL principle to this
modified model. The probability of generating a verb-
noun collocation from (partial) subcategorization frames
is simply estimated as the product of the probabilities
</footnote>
<note confidence="0.634643">
4.2.2 Subcategorization Preference Test
</note>
<subsectionHeader confidence="0.524413">
using Positive/Negative Examples
</subsectionHeader>
<bodyText confidence="0.999021636363636">
The other type of the model evaluation criterion
is the performance in the subcategorization pref-
erence test presented in Utsuro and Matsumoto
(1997), in which the goodness of the model is
measured according to how many of the posi-
tive examples can be judged as more appropriate
than the negative examples. This subcategoriza-
tion preference test can be regarded as modeling
the subcategorization ambiguity of an argument
noun in a Japanese sentence with more than one
verbs like the one in Example 2.
</bodyText>
<subsectionHeader confidence="0.864306">
Example 2
</subsectionHeader>
<bodyText confidence="0.981592741935484">
TV-de mouketa shounin-wo mita
TV-by/on earn money merchant-ACC see
(If the phrase &amp;quot;TV-de&amp;quot; (by/on TV) modifies the verb
&amp;quot;mouketa&amp;quot;(earn money), the sentence means that
&amp;quot;(Somebody) saw a merchant who earned money by
(selling) TV.&amp;quot; On the other hand, if the phrase &amp;quot;TV-
de&amp;quot;(by/on TV) modifies the verb &amp;quot;mita&amp;quot;(see), the
sentence means that &amp;quot;On TV, (somebody) saw a mer-
chant who earned money.&amp;quot;)
Negative examples are artificially generated from
the positive examples by choosing a case element
in a positive example of one verb at random and
moving it to a positive example of another verb.
Compared with the calculation of the descrip-
tion length l(ps,E) in (18), the calculation of the
accuracy of subcategorization preference test re-
quires comparison of probability values for suffi-
cient number of positive and negative data and
its computational cost is much higher than that
of calculating the description length. There-
fore, at present, we employ the description length
l(ps,E) in (18) as the model evaluation crite-
rion during the general-to-specific feature selec-
tion procedure, which we will describe in the next
section in detail. After obtaining a sequence of
active feature sets (i.e., subcategorization pref-
erence models) which are totally ordered from
general to specific, we select an optimal subcate-
gorization preference model according to the ac-
curacy of subcategorization preference test, as we
will describe in section 4.4.
</bodyText>
<subsectionHeader confidence="0.999797">
4.3 Feature Selection Algorithm
</subsectionHeader>
<bodyText confidence="0.999556818181818">
The following gives the details of the general-to-
specific feature selection algorithm, where the de-
of generating each leaf-class in the verb-noun collocation
from the corresponding superordinate class in the subcat-
egorization frame. With this generation probability, the
more general the sense restriction of the subcategoriza-
tion frames is, the less fit the model has to the data, and
the greater the data description length (the first term of
(18)) of the model is. Thus, this modification causes the
feature selection process to be more sensitive to the sense
restriction of the model.
</bodyText>
<page confidence="0.985045">
1318
</page>
<table confidence="0.630662428571429">
scription length l(ps,E) in (18) is employed as
the model evaluation criterion:5
General-to-Specific Feature Selection
Input: Training data set E;
collection .F of candidate features
Output: Set S of active features;
model Ps incorporating these features
</table>
<listItem confidence="0.77669925">
1. Start with S = So of the definition (16) and with
g = F - So
2. Do for each active feature f E S and every pos-
sible replacement D1 C g:
</listItem>
<bodyText confidence="0.59862375">
Compute the model psupf-{f} using
ITS Algorithm.
Compute the decrease in the descrip-
tion length of (18).
</bodyText>
<listItem confidence="0.9988615">
3. Check the termination condition6
4. Select the feature f and its replacement Df with
maximum decrease in the description length
5. S4—SUDi-{j}, g g- Di
6. Compute ps using ITS Algorithm
7. Go to step 2
</listItem>
<subsectionHeader confidence="0.999440666666667">
4.4 Selecting a Model with Approx-
imately Optimal Subcategorization
Preference Accuracy
</subsectionHeader>
<bodyText confidence="0.94479925">
Suppose that we are constructing subcategoriza-
tion preference models for the verbs v1, ,
By the general-to-specific feature selection algo-
rithm in the previous section, for each verb vi,
a totally ordered sequence of ni active feature
sets So, • • , Sini (i.e., subcategorization prefer-
ence models) are obtained from the training sam-
ple E. Then, using another training sample E.&apos;
which is different from E and consists of positive
as well as negative data, a model with optimal
subcategorization preference accuracy is approx-
imately selected by the following procedure. Let
, 7,7, denote the current sets of active fea-
tures for verbs vi, , vni, respectively:
1. Initially, for each verb v2, set 7; as the most gen-
eral one S20 of the sequence Seo, • • , S., •
2. For each verb v2, from the sequence Si, • • Sm, ,
search for an active feature set which gives a
maximum subcategorization preference accuracy
for r , then set Ti as it.
</bodyText>
<listItem confidence="0.947013666666667">
3. Repeat the same procedure as 2.
4. Return the current sets , , 7,7., as the approx-
imately optimal active feature sets Si,.., S,„
</listItem>
<bodyText confidence="0.312905">
for verbs v1, ,Vin, respectively.
</bodyText>
<footnote confidence="0.972034571428571">
5Note that this feature selection algorithm is a hill-
climbing one and the model selected here may have a de-
scription length greater than the global minimum.
6In the present implementation, the feature selection
process is terminated after the description length of the
model stops decreasing and then certain number of active
features are replaced.
</footnote>
<sectionHeader confidence="0.969565" genericHeader="evaluation">
5 Experiment and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.978841">
5.1 Corpus and Thesaurus
</subsectionHeader>
<bodyText confidence="0.999946444444444">
As the training and test corpus, we used the
EDR Japanese bracketed corpus (EDR, 1995),
which contains about 210,000 sentences collected
from newspaper and magazine articles. We
used `Bunrui Goi Hyou&apos;(BGH) (NLRI, 1993)
as the Japanese thesaurus. BGH has a seven-
layered abstraction hierarchy and more than
60,000 words are assigned at the leaves and its
nominal part contains about 45,000 words.
</bodyText>
<subsectionHeader confidence="0.999539">
5.2 Training/Test Events and Features
</subsectionHeader>
<bodyText confidence="0.999756275862069">
We conduct the model learning experiment under
the following conditions: i) the noun class gener-
alization level of each feature is limited to above
the level 5 from the root node in the thesaurus,
ii) since verbs are independent of each other in
our model learning framework, we collect verb-
noun collocations of one verb into a training data
set and conduct the model learning procedure for
each verb separately.
For the experiment, seven Japanese verbs7 are
selected so that the difficulty of the subcatego-
rization preference test is balanced among verb
pairs. The number of training events for each
verb varies from about 300 to 400, while the
number of candidate features for each verb varies
from 200 to 1,350. From this data, we construct
the following three types of data set, each pair
of which has no common element: i) the training
data E which consists of positive data only, and
is used for selecting a sequence of active feature
sets by the general-to-specific feature selection
algorithm in section 4.3, ii) the training data E.&apos;
which consists of positive and negative data and
is used in the procedure of section 4.4, and iii) the
test data ets which consists of positive and neg-
ative data and is used for evaluating the selected
models in terms of the performance of subcate-
gorization preference test. The sizes of the data
sets E, , and Ets are 2,333, 2,100, and 2,100.
</bodyText>
<sectionHeader confidence="0.54668" genericHeader="evaluation">
5.3 Results
</sectionHeader>
<bodyText confidence="0.9968752">
Table 1 shows the performance of subcategoriza-
tion preference test described in section 4.2.2, for
the approximately optimal models selected by the
procedure in section 4.4 (the &amp;quot;Optimal&amp;quot; model
of &amp;quot;General-to-Specific&amp;quot; method), as well as for
several other models including baseline models.
Coverage is the rate of test instances which sat-
isfy the case covering constraint of section 4.1.
Accuracy is measured with the following heuris-
tics: i) verb-noun collocations which satisfy the
</bodyText>
<footnote confidence="0.922664666666667">
7 &amp;quot;Agaru (rise)&amp;quot;, &amp;quot;kau (buy)&amp;quot;, &amp;quot;motoduku (base)&amp;quot;,
&amp;quot;oujiru (respond)&amp;quot;, &amp;quot;sumu (live)&amp;quot;, &amp;quot;tigau (differ)&amp;quot;, and
&amp;quot;tsunagaru (connect)&amp;quot;.
</footnote>
<page confidence="0.997168">
1319
</page>
<tableCaption confidence="0.9703045">
Table 1: Comparison of Coverage and Accuracy
of Optimal and Other Models (%)
</tableCaption>
<table confidence="0.998153333333333">
Coverage Accuracy
General-to-Specific
(Initial) 84.8 81.3
(Independent Cases) 84.8 82.2
(General Classes) 77.5 79.5
(Optimal) 75.4 87.1
(MDL) 15.9 70.5
One-by-one Feature Adding 60.8 79.0
(Optimal)
</table>
<bodyText confidence="0.999804509803922">
case covering constraint are preferred, ii) even
those verb-noun collocations which do not satisfy
the case covering constraint are assigned the con-
ditional probabilities in (15) by neglecting cases
which are not covered by the model. With these
heuristics, subcategorization preference can be
judged for all the test instances, and test set cov-
erage becomes 100%.
In Table 1, the &amp;quot;Initial&amp;quot; model is the one
constructed according to the description in sec-
tion 4.1, in which cases are independent of each
other and the sense restriction of each case is
(one of) the most general class(es). The &amp;quot;Inde-
pendent Cases&amp;quot; model is the one obtained by re-
moving all the case dependencies from the &amp;quot;Op-
timal&amp;quot; model, while the &amp;quot;General Classes&amp;quot; model
is the one obtained by generalizing all the sense
restriction of the &amp;quot;Optimal&amp;quot; model to the most
general classes. The &amp;quot;MDL&amp;quot; model is the one
with the minimum description length. This is
for evaluating the effect of the MDL principle in
the task of subcategorization preference model
learning. The &amp;quot;Optimal&amp;quot; model of &amp;quot;One-by-one
Feature Adding&amp;quot; method is the one selected from
the sequence of one-by-one feature adding in sec-
tion 3.1 by the procedure in section 4.4.
The &amp;quot;Optimal&amp;quot; model of &apos;General-to-Specific&amp;quot;
method performs best among all the models in
Table 1. Especially, it outperforms the &amp;quot;Op-
timal&amp;quot; model of &amp;quot;One-by-one Feature Adding&amp;quot;
method both in coverage and accuracy. As for
the size of the optimal model, the average num-
ber of the active feature set is 126 for &amp;quot;General-
to-Specific&amp;quot; method and 800 for &amp;quot;One-by-one
Feature Adding&amp;quot; method. Therefore, general-to-
specific feature selection algorithm achieves sig-
nificant improvements over the one-by-one fea-
ture adding algorithm with much smaller num-
ber of active features. The &amp;quot;Optimal&amp;quot; model of
&amp;quot;General-to-Specific&amp;quot; method outperforms both
the &amp;quot;Independent Cases&amp;quot; and &amp;quot;General Classes&amp;quot;
models, and thus both of the case dependencies
and specific sense restriction selected by the pro-
posed method have much contribution to improv-
ing the performance in subcategorization prefer-
ence test. The &amp;quot;MDL&amp;quot; model performs worse
than the &amp;quot;Optimal&amp;quot; model, because the features
of the &amp;quot;MDL&amp;quot; model have much more specific
sense restriction than those of the &amp;quot;Optimal&amp;quot;
model, and the coverage of the &amp;quot;MDL&amp;quot; model
is much lower than that of the &amp;quot;Optimal&amp;quot; model.
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974615384615">
This paper proposed a novel method for learn-
ing probability models of subcategorization pref-
erence of verbs. Especially, we proposed a new
model selection algorithm which starts from the
most general model and gradually examines more
specific models. In the experimental evaluation,
it is shown that both of the case dependencies
and specific sense restriction selected by the pro-
posed method contribute to improving the per-
formance in subcategorization preference resolu-
tion. As for future works, it is important to eval-
uate the performance of the learned subcatego-
rization preference model in the real parsing task.
</bodyText>
<sectionHeader confidence="0.999319" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999957175">
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguistics,
22(1):39-71.
E. Charniak. 1997. Statistical Parsing with a Context-
free Grammar and Word Statistics. In Proceedings of
the 14th AAAI, pages 598-603.
M. Collins. 1996. A New Statistical Parser Based on Bi-
gram Lexical Dependencies. In Proceedings of the 34th
Annual Meeting of ACL, pages 184-191.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing Features of Random Fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380-393.
EDR (Japan Electronic Dictionary Research Institute,
Ltd.). 1995. EDR Electronic Dictionary Technical
Guide.
H. Li and N. Abe. 1995. Generalizing Case Frames Using
a Thesaurus and the MDL Principle. In Proceedings of
International Conference on Recent Advances in Natu-
ral Language Processing, pages 239-248.
H. Li and N. Abe. 1996. Learning Dependencies between
Case Frame Slots. In Proceedings of the 16th COLING,
pages 10-15.
D. M. Magerman. 1995. Statistical Decision-Tree Models
for Parsing. In Proceedings of the 33rd Annual Meeting
of ACL, pages 276-283.
NLRI (National Language Research Institute). 1993.
Word List by Semantic Principles. Syuei Syuppan. (in
Japanese).
P. Resnik. 1993. Semantic Classes and Syntactic Ambigu-
ity. In Proceedings of the Human Language Technology
Workshop, pages 278-283.
J. Rissanen. 1984. Universal Coding, Information, Pre-
diction, and Estimation. IEEE Transactions on Infor-
mation Theory, IT-30(4):629-636.
T. Utsuro and Y. Matsumoto. 1997. Learning Probabilis-
tic Subcategorization Preference by Identifying Case
Dependencies and Optimal Noun Class Generalization
Level. In Proceedings of the 5th ANLP, pages 364-371.
</reference>
<page confidence="0.990103">
1320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.617060">
<title confidence="0.982305">General-to-Specific Model Selection for Subcategorization Preference*</title>
<author confidence="0.879912">Utsuro Miyata Matsumoto</author>
<affiliation confidence="0.996682">Graduate School of Information Science, Nara Institute of Science and Technology</affiliation>
<address confidence="0.999245">8916-5, Takayama-cho, Ikoma-shi, Nara, 630-0101, JAPAN</address>
<web confidence="0.72236">jp, URL: http://cl.aist-nara.ac.jp/utsuro/</web>
<abstract confidence="0.998365769230769">This paper proposes a novel method for learning probability models of subcategorization preference of We consider the issues of dependencies noun generalization a uniform way by employing the maximum entropy modeling method. We also propose a new model selection algorithm which from the most and gradually exmore In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="3580" citStr="Berger et al., 1996" startWordPosition="543" endWordPosition="546">orks are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and noun class generalization are represented as features in the maximum entropy approach. Features are allowed to have overlap and this is quite advantageous when we consider case dependencies and noun class generalization in parameter estimation. An optimal model is selected by searching for an optimal set of features, i.e, optimal case dependencies and optimal noun class generalization levels. As the feature selection process, this paper proposes a new feature selection algorithm which starts from the most general model and gradually examines more specific models (sectio</context>
<context position="11450" citStr="Berger et al. (1996)" startWordPosition="1857" endWordPosition="1860">f those two cases as in the formula (10). Since the leaf class cc (&amp;quot;child&amp;quot;) can be generated from either chum or cmum, and also the leaf class ci (&amp;quot;juice&amp;quot;) can be generated from either ch„ or cup e can be regarded as generated according to either of the four formulas (10) and (11)-413): pred:nmnu ga: emam wo:ebea pred : nomu 1 [pred : nomu de : epic I e(12) ga : chum pred:namu ga : cmc„, Fred: nomu ]) de : epic --+ e (13) wo:chg 3 Maximum Entropy Modeling of Subcategorization Preference This section describes how we apply the maximum entropy modeling approach of Della Pietra et al. (1997) and Berger et al. (1996) to model learning of subcategorization preference. 3.1 Maximum Entropy Modeling Given the training sample e of the events (x, y), our task is to estimate the conditional probability p(y I x) that, given a context x, the process will output y. In order to express certain features of the whole event (x, y), a binary-valued indicator function is introduced and called a feature function. Usually, we suppose that there exists a large collection F of candidate features, and include in the model only a subset S of the full set of candidate features F. We call S the set of active features. Now, we as</context>
<context position="13547" citStr="Berger et al. (1996)" startWordPosition="2235" endWordPosition="2238">ned an independent parameter. Parameter Estimation Suppose that the set S(C .F) of active features is found by the procedure of the next section. Then, the parameters of subcategorization frames are estimated according to ITS Algorithm and the conditional probability distribution ps(ep I v) is given as: exp E ft(v, ep)) LES PS(ep I v) = (15) E exp( E A, ft(v, ep)) f. ES The parameter values A*i are estimated by an algorithm called Improved Iterative Scaling (IIS) algorithm. Feature Selection by One-by-one Feature Adding The feature selection process presented in Della Pietra et al. (1997) and Berger et al. (1996) is an incremental procedure that builds up S by successively adding features oneby-one. It starts with S as empty, and, at each step, selects the candidate feature which, when adjoined to the set of active features S, produces the greatest increase in log-likelihood of the training sample. 3.2 Modeling Subcategorization Preference Events In our task of model learning of subcategorization preference, each event (x, y) in the training sample is a verb-noun collocation e, which is defined in the formula (1). A verb-noun collocation e can be divided into two parts: one is the verbal part ev conta</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Parsing with a Contextfree Grammar and Word Statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th AAAI,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="1197" citStr="Charniak (1997)" startWordPosition="164" endWordPosition="165">ng method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in s</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical Parsing with a Contextfree Grammar and Word Statistics. In Proceedings of the 14th AAAI, pages 598-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of ACL,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="1176" citStr="Collins (1996)" startWordPosition="161" endWordPosition="162">ximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lexical/semantic collocational knowledge of verbs</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. Collins. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies. In Proceedings of the 34th Annual Meeting of ACL, pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing Features of Random Fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--4</pages>
<contexts>
<context position="3558" citStr="Pietra et al., 1997" startWordPosition="539" endWordPosition="542">ed thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and noun class generalization are represented as features in the maximum entropy approach. Features are allowed to have overlap and this is quite advantageous when we consider case dependencies and noun class generalization in parameter estimation. An optimal model is selected by searching for an optimal set of features, i.e, optimal case dependencies and optimal noun class generalization levels. As the feature selection process, this paper proposes a new feature selection algorithm which starts from the most general model and gradually examines more s</context>
<context position="11425" citStr="Pietra et al. (1997)" startWordPosition="1852" endWordPosition="1855">t)&amp;quot; case is independent of those two cases as in the formula (10). Since the leaf class cc (&amp;quot;child&amp;quot;) can be generated from either chum or cmum, and also the leaf class ci (&amp;quot;juice&amp;quot;) can be generated from either ch„ or cup e can be regarded as generated according to either of the four formulas (10) and (11)-413): pred:nmnu ga: emam wo:ebea pred : nomu 1 [pred : nomu de : epic I e(12) ga : chum pred:namu ga : cmc„, Fred: nomu ]) de : epic --+ e (13) wo:chg 3 Maximum Entropy Modeling of Subcategorization Preference This section describes how we apply the maximum entropy modeling approach of Della Pietra et al. (1997) and Berger et al. (1996) to model learning of subcategorization preference. 3.1 Maximum Entropy Modeling Given the training sample e of the events (x, y), our task is to estimate the conditional probability p(y I x) that, given a context x, the process will output y. In order to express certain features of the whole event (x, y), a binary-valued indicator function is introduced and called a feature function. Usually, we suppose that there exists a large collection F of candidate features, and include in the model only a subset S of the full set of candidate features F. We call S the set of ac</context>
<context position="13522" citStr="Pietra et al. (1997)" startWordPosition="2230" endWordPosition="2233">gorization frame is assigned an independent parameter. Parameter Estimation Suppose that the set S(C .F) of active features is found by the procedure of the next section. Then, the parameters of subcategorization frames are estimated according to ITS Algorithm and the conditional probability distribution ps(ep I v) is given as: exp E ft(v, ep)) LES PS(ep I v) = (15) E exp( E A, ft(v, ep)) f. ES The parameter values A*i are estimated by an algorithm called Improved Iterative Scaling (IIS) algorithm. Feature Selection by One-by-one Feature Adding The feature selection process presented in Della Pietra et al. (1997) and Berger et al. (1996) is an incremental procedure that builds up S by successively adding features oneby-one. It starts with S as empty, and, at each step, selects the candidate feature which, when adjoined to the set of active features S, produces the greatest increase in log-likelihood of the training sample. 3.2 Modeling Subcategorization Preference Events In our task of model learning of subcategorization preference, each event (x, y) in the training sample is a verb-noun collocation e, which is defined in the formula (1). A verb-noun collocation e can be divided into two parts: one is</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Inducing Features of Random Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380-393.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>EDR Electronic Dictionary Technical Guide.</booktitle>
<institution>EDR (Japan Electronic Dictionary Research Institute, Ltd.).</institution>
<contexts>
<context position="1160" citStr="(1995)" startWordPosition="160" endWordPosition="160">g the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lexical/semantic collocational kn</context>
<context position="2851" citStr="(1995)" startWordPosition="419" endWordPosition="419"> research was partially supported by the Ministry of Education, Science, Sports and Culture, Japan, Grantin-Aid for Encouragement of Young Scientists, 09780338, 1998. An extended version of this paper is available from the above URL. dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probability model (section 3)</context>
</contexts>
<marker>1995</marker>
<rawString>EDR (Japan Electronic Dictionary Research Institute, Ltd.). 1995. EDR Electronic Dictionary Technical Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing Case Frames Using a Thesaurus and the MDL Principle.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="2851" citStr="Li and Abe (1995)" startWordPosition="416" endWordPosition="419"> and inThis research was partially supported by the Ministry of Education, Science, Sports and Culture, Japan, Grantin-Aid for Encouragement of Young Scientists, 09780338, 1998. An extended version of this paper is available from the above URL. dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probability model (section 3)</context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>H. Li and N. Abe. 1995. Generalizing Case Frames Using a Thesaurus and the MDL Principle. In Proceedings of International Conference on Recent Advances in Natural Language Processing, pages 239-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Learning Dependencies between Case Frame Slots.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th COLING,</booktitle>
<pages>10--15</pages>
<contexts>
<context position="3016" citStr="Li and Abe (1996)" startWordPosition="445" endWordPosition="448">80338, 1998. An extended version of this paper is available from the above URL. dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and noun class g</context>
</contexts>
<marker>Li, Abe, 1996</marker>
<rawString>H. Li and N. Abe. 1996. Learning Dependencies between Case Frame Slots. In Proceedings of the 16th COLING, pages 10-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of ACL,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="1160" citStr="Magerman (1995)" startWordPosition="158" endWordPosition="160"> employing the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lexical/semantic collocational kn</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. M. Magerman. 1995. Statistical Decision-Tree Models for Parsing. In Proceedings of the 33rd Annual Meeting of ACL, pages 276-283.</rawString>
</citation>
<citation valid="true">
<title>Word List by Semantic Principles. Syuei Syuppan.</title>
<date>1993</date>
<institution>NLRI (National Language Research Institute).</institution>
<note>(in Japanese).</note>
<contexts>
<context position="2829" citStr="(1993)" startWordPosition="414" endWordPosition="414">re optional and inThis research was partially supported by the Ministry of Education, Science, Sports and Culture, Japan, Grantin-Aid for Encouragement of Young Scientists, 09780338, 1998. An extended version of this paper is available from the above URL. dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probabi</context>
</contexts>
<marker>1993</marker>
<rawString>NLRI (National Language Research Institute). 1993. Word List by Semantic Principles. Syuei Syuppan. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Semantic Classes and Syntactic Ambiguity.</title>
<date>1993</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>278--283</pages>
<contexts>
<context position="2829" citStr="Resnik (1993)" startWordPosition="413" endWordPosition="414">cases are optional and inThis research was partially supported by the Ministry of Education, Science, Sports and Culture, Japan, Grantin-Aid for Encouragement of Young Scientists, 09780338, 1998. An extended version of this paper is available from the above URL. dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probabi</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. 1993. Semantic Classes and Syntactic Ambiguity. In Proceedings of the Human Language Technology Workshop, pages 278-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Universal Coding, Information, Prediction, and Estimation.</title>
<date>1984</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>30--4</pages>
<contexts>
<context position="4407" citStr="Rissanen, 1984" startWordPosition="675" endWordPosition="677">s and noun class generalization in parameter estimation. An optimal model is selected by searching for an optimal set of features, i.e, optimal case dependencies and optimal noun class generalization levels. As the feature selection process, this paper proposes a new feature selection algorithm which starts from the most general model and gradually examines more specific models (section 4). As the model evaluation criterion during the model search from general to specific ones, we employ the description length of the model and guide the search process so as to minimize the description length (Rissanen, 1984). Then, after obtaining a sequence of subcategorization preference models which are totally ordered from general to specific, we select an approximately optimal subcategorization preference model according to the accuracy of subcategorization preference test. In the experimental evaluation of performance of subcatego1314 rization preference, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution (section 5). 2 A Model of Generating a Verb-Noun Collocation fr</context>
<context position="18017" citStr="Rissanen, 1984" startWordPosition="3033" endWordPosition="3034">ng two requirements2 3. 1. Subsumption with s: for each element fe of Dh, s&apos; has to be subsumed by s. 2. Upper Bound of g: for each element fe of Df,, and for each element f t of g, t does not subsume i.e., D1 is a subset of the upper bound of g with respect to the subsumption relation Among all the possible replacements, the most appropriate one is selected according to a model evaluation criterion. 4.2 Model Evaluation Criterion As the model evaluation criterion during feature selection, we consider the following two types. 4.2.1 MDL Principle The MDL (Minimum Description Length) principle (Rissanen, 1984) is a model selection criterion. It is designed so as to &amp;quot;select the model that has as much fit to a given data as possible and that is as simple as possible.&amp;quot; The MDL principle selects the model that minimizes the following description length l(111, D) of the probability model M for the data D: 1 l(M, D) = — log L m (D) + —2Nm log PI (17) where log LM(D) is the log-likelihood of the model M to the data D, NM is the number of the parameters in the model M, and ID 1 is the size of the data D. Description Length of Subcategorization Preference Model The description length l(ps,E) of the probabil</context>
</contexts>
<marker>Rissanen, 1984</marker>
<rawString>J. Rissanen. 1984. Universal Coding, Information, Prediction, and Estimation. IEEE Transactions on Information Theory, IT-30(4):629-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
<author>Y Matsumoto</author>
</authors>
<title>Learning Probabilistic Subcategorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ANLP,</booktitle>
<pages>364--371</pages>
<contexts>
<context position="20026" citStr="Utsuro and Matsumoto (1997)" startWordPosition="3365" endWordPosition="3368">probability model ps by multiplying the probability of generating the verb-noun collocation e from the (partial) subcategorization frames that correspond to active features evaluating to true for e, and then apply the MDL principle to this modified model. The probability of generating a verbnoun collocation from (partial) subcategorization frames is simply estimated as the product of the probabilities 4.2.2 Subcategorization Preference Test using Positive/Negative Examples The other type of the model evaluation criterion is the performance in the subcategorization preference test presented in Utsuro and Matsumoto (1997), in which the goodness of the model is measured according to how many of the positive examples can be judged as more appropriate than the negative examples. This subcategorization preference test can be regarded as modeling the subcategorization ambiguity of an argument noun in a Japanese sentence with more than one verbs like the one in Example 2. Example 2 TV-de mouketa shounin-wo mita TV-by/on earn money merchant-ACC see (If the phrase &amp;quot;TV-de&amp;quot; (by/on TV) modifies the verb &amp;quot;mouketa&amp;quot;(earn money), the sentence means that &amp;quot;(Somebody) saw a merchant who earned money by (selling) TV.&amp;quot; On the oth</context>
</contexts>
<marker>Utsuro, Matsumoto, 1997</marker>
<rawString>T. Utsuro and Y. Matsumoto. 1997. Learning Probabilistic Subcategorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level. In Proceedings of the 5th ANLP, pages 364-371.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>