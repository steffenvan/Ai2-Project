<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.6961978">
A Developmental Model of Syntax Acquisition in the Construction Grammar
Framework with Cross-Linguistic Validation in English and Japanese
Peter Ford Dominey
Sequential Cognition and Language Group
Institut des Sciences Cognitives, CNRS
</title>
<address confidence="0.466946">
69675 Bron CEDES, France
</address>
<email confidence="0.809482">
dominey@isc.cnrs.fr
</email>
<note confidence="0.690007666666667">
Toshio Inui
Graduate School of Informatics,
Kyoto University,
</note>
<keyword confidence="0.674589">
Yoshida-honmachi, Sakyo-ku, 606-8501,
Kyoto, Japan
inui@kyoto-u.ac.jp
</keyword>
<sectionHeader confidence="0.979766" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957357142857">
The current research demonstrates a system
inspired by cognitive neuroscience and
developmental psychology that learns to
construct mappings between the grammatical
structure of sentences and the structure of their
meaning representations. Sentence to meaning
mappings are learned and stored as
grammatical constructions. These are stored
and retrieved from a construction inventory
based on the constellation of closed class
items uniquely identifying each construction.
These learned mappings allow the system to
processes natural language sentences in order
to reconstruct complex internal representations
of the meanings these sentences describe. The
system demonstrates error free performance
and systematic generalization for a rich subset
of English constructions that includes complex
hierarchical grammatical structure, and
generalizes systematically to new sentences of
the learned construction categories. Further
testing demonstrates (1) the capability to
accommodate a significantly extended set of
constructions, and (2) extension to Japanese, a
free word order language that is structurally
quite different from English, thus
demonstrating the extensibility of the structure
mapping model.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999517735849057">
The nativist perspective on the problem of
language acquisition holds that the &lt;sentence,
meaning&gt; data to which the child is exposed is
highly indeterminate, and underspecifies the
mapping to be learned. This “poverty of the
stimulus” is a central argument for the existence of
a genetically specified universal grammar, such
that language acquisition consists of configuring
the UG for the appropriate target language
(Chomsky 1995). In this framework, once a given
parameter is set, its use should apply to new
constructions in a generalized, generative manner.
An alternative functionalist perspective holds
that learning plays a much more central role in
language acquisition. The infant develops an
inventory of grammatical constructions as
mappings from form to meaning (Goldberg 1995).
These constructions are initially rather fixed and
specific, and later become generalized into a more
abstract compositional form employed by the adult
(Tomasello 1999, 2003). In this context,
construction of the relation between perceptual and
cognitive representations and grammatical form
plays a central role in learning language (e.g.
Feldman et al. 1990, 1996; Langacker 1991;
Mandler 1999; Talmy 1998).
These issues of learnability and innateness have
provided a rich motivation for simulation studies
that have taken a number of different forms.
Elman (1990) demonstrated that recurrent
networks are sensitive to predictable structure in
grammatical sequences. Subsequent studies of
grammar induction demonstrate how syntactic
structure can be recovered from sentences (e.g.
Stolcke &amp; Omohundro 1994). From the
“grounding of language in meaning” perspective
(e.g. Feldman et al. 1990, 1996; Langacker 1991;
Goldberg 1995) Chang &amp; Maia (2001) exploited
the relations between action representation and
simple verb frames in a construction grammar
approach. In effort to consider more complex
grammatical forms, Miikkulainen (1996)
demonstrated a system that learned the mapping
between relative phrase constructions and multiple
event representations, based on the use of a stack
for maintaining state information during the
processing of the next embedded clause in a
recursive manner.
In a more generalized approach, Dominey
(2000) exploited the regularity that sentence to
meaning mapping is encoded in all languages by
word order and grammatical marking (bound or
free) (Bates et al. 1982). That model was based on
</bodyText>
<page confidence="0.992406">
33
</page>
<bodyText confidence="0.71574">
“gugle” to the object of push.
</bodyText>
<equation confidence="0.999402666666667">
WordToReferent(i,j) = WordToReferent(i,j) +
OCA(k,i) * SEA(m,j) *
max(a, SentenceToScene(m,k)) (1)
</equation>
<bodyText confidence="0.966211869565217">
the functional neurophysiology of cognitive
sequence and language processing and an
associated neural network model that has been
demonstrated to simulate interesting aspects of
infant (Dominey &amp; Ramus 2000) and adult
language processing (Dominey et al. 2003).
2 Structure mapping for language learning
The mapping of sentence form onto meaning
(Goldberg 1995) takes place at two distinct levels
in the current model: Words are associated with
individual components of event descriptions, and
grammatical structure is associated with functional
roles within scene events. The first level has been
addressed by Siskind (1996), Roy &amp; Pentland
(2002) and Steels (2001) and we treat it here in a
relatively simple but effective manner. Our
principle interest lies more in the second level of
mapping between scene and sentence structure.
Equations 1-7 implement the model depicted in
Figure 1, and are derived from a
neurophysiologically motivated model of
sensorimotor sequence learning (Dominey et al.
2003).
</bodyText>
<subsectionHeader confidence="0.994188">
2.1 Word Meaning
</subsectionHeader>
<bodyText confidence="0.984863892857143">
Equation (1) describes the associative memory,
WordToReferent, that links word vectors in the
OpenClassArray (OCA) with their referent vectors
in the SceneEventArray (SEA)1. In the initial
learning phases there is no influence of syntactic
knowledge and the word-referent associations are
stored in the WordToReferent matrix (Eqn 1) by
associating every word with every referent in the
current scene (a = 1), exploiting the cross-
situational regularity (Siskind 1996) that a given
word will have a higher coincidence with referent
to which it refers than with other referents. This
initial word learning contributes to learning the
mapping between sentence and scene structure
(Eqn. 4, 5 &amp; 6 below). Then, knowledge of the
syntactic structure, encoded in SentenceToScene
can be used to identify the appropriate referent (in
the SEA) for a given word (in the OCA),
corresponding to a zero value of a in Eqn. 1. In
this “syntactic bootstrapping” for the new word
“gugle,” for example, syntactic knowledge of
Agent-Event-Object structure of the sentence
“John pushed the gugle” can be used to assign
1 In Eqn 1, the index k = 1 to 6, corresponding to the maximum
number of words in the open class array (OCA). Index m = 1 to 6,
corresponding to the maximum number of elements in the scene event
array (SEA). Indices i and j = 1 to 25, corresponding to the word and
scene item vector sizes, respectively.
</bodyText>
<subsectionHeader confidence="0.997596">
2.2 Open vs Closed Class Word Categories
</subsectionHeader>
<bodyText confidence="0.999953705882353">
Our approach is based on the cross-linguistic
observation that open class words (e.g. nouns,
verbs, adjectives and adverbs) are assigned to their
thematic roles based on word order and/or
grammatical function words or morphemes (Bates
et al. 1982). Newborn infants are sensitive to the
perceptual properties that distinguish these two
categories (Shi et al. 1999), and in adults, these
categories are processed by dissociable
neurophysiological systems (Brown et al. 1999).
Similarly, artificial neural networks can also learn
to make this function/content distinction (Morgan
et al. 1996). Thus, for the speech input that is
provided to the learning model open and closed
class words are directed to separate processing
streams that preserve their order and identity, as
indicated in Figure 2.
</bodyText>
<figureCaption confidence="0.7853985">
Figure 1. Structure-Mapping Architecture. 1. Lexical categorization.
2. Open class words in Open Class Array are translated to Predicted
</figureCaption>
<bodyText confidence="0.827377833333333">
Referents in the PRA via the WordtoReferent mapping. 3. PRA
elements are mapped onto their roles in the SceneEventArray by the
SentenceToScene mapping, specific to each sentence type. 4. This
mapping is retrieved from Construction Inventory, via the
ConstructionIndex that encodes the closed class words that
characterize each grammatical construction type.
</bodyText>
<subsectionHeader confidence="0.999677">
2.3 Mapping Sentence to Meaning
</subsectionHeader>
<bodyText confidence="0.9990642">
Meanings are encoded in an event predicate,
argument representation corresponding to the
SceneEventArray in Figure 1 (e.g. push(Block,
triangle) for “The triangle pushed the block”).
There, the sentence to meaning mapping can be
</bodyText>
<page confidence="0.998377">
34
</page>
<bodyText confidence="0.999154285714286">
characterized in the following successive steps.
First, words in the Open Class Array are decoded
into their corresponding scene referents (via the
WordToReferent mapping) to yield the Predicted
Referents Array that contains the translated words
while preserving their original order from the OCA
(Eqn 2) 2.
</bodyText>
<equation confidence="0.9438155">
n
PRA(k,j) =OCA(k,i) * WordToReferent(i,j)
� (2)
i =1
</equation>
<bodyText confidence="0.999520857142857">
Next, each sentence type will correspond to a
specific form to meaning mapping between the
PRA and the SEA. encoded in the
SentenceToScene array. The problem will be to
retrieve for each sentence type, the appropriate
corresponding SentenceToScene mapping. To
solve this problem, we recall that each sentence
type will have a unique constellation of closed
class words and/or bound morphemes (Bates et al.
1982) that can be coded in a ConstructionIndex
(Eqn.3) that forms a unique identifier for each
sentence type.
The ConstructionIndex is a 25 element vector.
Each function word is encoded as a single bit in a
25 element FunctionWord vector. When a
function word is encountered during sentence
processing, the current contents of
ConstructionIndex are shifted (with wrap-around)
by n + m bits where n corresponds to the bit that is
on in the FunctionWord, and m corresponds to the
number of open class words that have been
encountered since the previous function word (or
the beginning of the sentence). Finally, a vector
addition is performed on this result and the
FunctionWord vector. Thus, the appropriate
SentenceToScene mapping for each sentence type
can be indexed in ConstructionInventory by its
corresponding ConstructionIndex.
</bodyText>
<equation confidence="0.9239015">
ConstructionIndex = fcircularShift(ConstructionIndex,
FunctionWord) (3)
</equation>
<bodyText confidence="0.998937434782609">
The link between the ConstructionIndex and the
corresponding SentenceToScene mapping is
established as follows. As each new sentence is
processed, we first reconstruct the specific
SentenceToScene mapping for that sentence (Eqn
4)3, by mapping words to referents (in PRA) and
2 Index k = 1 to 6, corresponding to the maximum number of scene
items in the predicted references array (PRA). Indices i and j = 1 to
25, corresponding to the word and scene item vector sizes,
respectively.
3 Index m = 1 to 6, corresponding to the maximum number of
elements in the scene event array (SEA). Index k = 1 to 6,
corresponding to the maximum number of words in the predicted
referents to scene elements (in SEA). The
resulting, SentenceToSceneCurrent encodes the
correspondence between word order (that is
preserved in the PRA Eqn 2) and thematic roles in
the SEA. Note that the quality of
SentenceToSceneCurrent will depend on the
quality of acquired word meanings in
WordToReferent. Thus, syntactic learning
requires a minimum baseline of semantic
knowledge.
</bodyText>
<equation confidence="0.9970355">
SentenceToSceneCurrent(m,k) =
E PRA(k,i)*SEA(m,i)
n (4)
i=1
</equation>
<bodyText confidence="0.9999588">
Given the SentenceToSceneCurrent mapping
for the current sentence, we can now associate it in
the ConstructionInventory with the corresponding
function word configuration or ConstructionIndex
for that sentence, expressed in (Eqn 5)4.
</bodyText>
<equation confidence="0.976896">
ConstructionInventory(i,j) = ConstructionInventory(i,j)
+ ConstructionIndex(i)
* SentenceToScene-Current(j) (5)
</equation>
<bodyText confidence="0.999735">
Finally, once this learning has occurred, for
new sentences we can now extract the
SentenceToScene mapping from the learned
ConstructionInventory by using the
ConstructionIndex as an index into this associative
memory, illustrated in Eqn. 65.
</bodyText>
<equation confidence="0.99243225">
SentenceToScene(i) =
n (6)
� ConstructionInventory(i,j) * ConstructinIndex(j)
i=1
</equation>
<bodyText confidence="0.990261777777778">
To accommodate the dual scenes for complex
events Eqns. 4-7 are instantiated twice each, to
represent the two components of the dual scene. In
the case of simple scenes, the second component of
the dual scene representation is null.
We evaluate performance by using the
WordToReferent and SentenceToScene knowledge
to construct for a given input sentence the
“predicted scene”. That is, the model will
</bodyText>
<tableCaption confidence="0.8623297">
references array (PRA). Index i = 1 to 25, corresponding to the word
and scene item vector sizes.
4 Note that we have linearized SentenceToSceneCurrent from 2 to
1 dimensions to make the matrix multiplication more transparent.
Thus index j varies from 1 to 36 corresponding to the 6x6 dimensions
of SentenceToSceneCurrent.
5 Again to simplify the matrix multiplication, SentenceToScene
has been linearized to one dimension, based on the original 6x6
matrix. Thus, index i = 1 to 36, and index j = 1 to 25 corresponding to
the dimension of the ConstructionIndex.
</tableCaption>
<page confidence="0.99799">
35
</page>
<bodyText confidence="0.998981">
construct an internal representation of the scene
that should correspond to the input sentence. This
is achieved by first converting the Open-Class-
Array into its corresponding scene items in the
Predicted-Referents-Array as specified in Eqn. 2.
The referents are then re-ordered into the proper
scene representation via application of the
SentenceToScene transformation as described in
Eqn. 76.
</bodyText>
<equation confidence="0.998781">
PSA(m,i) = PRA(k,i) * SentenceToScene(m,k) (7)
</equation>
<bodyText confidence="0.999965833333333">
When learning has proceeded correctly, the
predicted scene array (PSA) contents should match
those of the scene event array (SEA) that is
directly derived from input to the model. We then
quantify performance error in terms of the number
of mismatches between PSA and SEA.
</bodyText>
<sectionHeader confidence="0.991428" genericHeader="introduction">
3 Learning Experiments
</sectionHeader>
<bodyText confidence="0.9997183">
Three sets of results will be presented. First the
demonstration of the model sentence to meaning
mapping for a reduced set of constructions is
presented as a proof of concept. This will be
followed by a test of generalization to a new
extended set of grammatical constructions.
Finally, in order to validate the cross-linguistic
validity of the underlying principals, the model is
tested with Japanese, a free word-order language
that is qualitatively quite distinct from English.
</bodyText>
<subsectionHeader confidence="0.941219333333333">
3.1 Proof of Concept with Two Constructions
3.1.1 Initial Learning of Active Forms for
Simple Event Meanings
</subsectionHeader>
<bodyText confidence="0.99993675">
The first experiment examined learning with
sentence, meaning pairs with sentences only in the
active voice, corresponding to the grammatical
forms 1 and 2.
</bodyText>
<listItem confidence="0.981073333333333">
1. Active: The block pushed the triangle.
2. Dative: The block gave the triangle to the
moon.
</listItem>
<bodyText confidence="0.962863892857143">
For this experiment, the model was trained on
544 &lt;sentence, meaning&gt; pairs. Again, meaning is
coded in a predicate-argument format, e.g.
push(block, triangle) for sentence 1. During the
first 200 trials (scene/sentence pairs), value a in
Eqn. 1 was 1 and thereafter it was 0. This was
necessary in order to avoid the effect of erroneous
6 In Eqn 7, index i = 1 to 25 corresponding to the size of the scene
and word vectors. Indices m and k = 1 to 6, corresponding to the
dimension of the predicted scene array, and the predicted references
array, respectively.
(random) syntactic knowledge on semantic
learning in the initial learning stages. Evaluation
of the performance of the model after this training
indicated that for all sentences, there was error-free
performance. That is, the PredictedScene
generated from each sentence corresponded to the
actual scene paired with that sentence. An
important test of language learning is the ability to
generalize to new sentences that have not
previously been tested. Generalization in this form
also yielded error free performance. In this
experiment, only 2 grammatical constructions were
learned, and the lexical mapping of words to their
scene referents was learned. Word meaning
provides the basis for extracting more complex
syntactic structure. Thus, these word meanings are
fixed and used for the subsequent experiments.
</bodyText>
<subsectionHeader confidence="0.470513">
3.1.2 Passive forms
</subsectionHeader>
<bodyText confidence="0.999901">
The second experiment examined learning with
the introduction of passive grammatical forms,
thus employing grammatical forms 1-4.
</bodyText>
<listItem confidence="0.969579666666667">
3. Passive: The triangle was pushed by the block.
4. Dative Passive: The moon was given to the
triangle by the block.
</listItem>
<bodyText confidence="0.999930857142857">
A new set of &lt;sentence, scene&gt; pairs was
generated that employed grammatical
constructions, with two- and three- arguments, and
active and passive grammatical forms for the
narration. Word meanings learned in Experiment 1
were used, so only the structural mapping from
grammatical to scene structure was learned. With
exposure to less than 100 &lt;sentence, scene&gt;, error
free performance was achieved. Note that only the
WordToReferent mappings were retained from
Experiment 1. Thus, the 4 grammatical forms
were learned from the initial naive state. This
means that the ConstructionIndex and
ConstructionInventory mechanism correctly
discriminates and learns the mappings for the
different grammatical constructions. In the
generalization test, the learned values were fixed,
and the model demonstrated error-free
performance on new sentences for all four
grammatical forms that had not been used during
the training.
</bodyText>
<subsectionHeader confidence="0.355944">
3.1.3 Relative forms for Complex Events
</subsectionHeader>
<bodyText confidence="0.9995368">
The complexity of the scenes/meanings and
corresponding grammatical forms in the previous
experiments were quite limited. Here we consider
complex &lt;sentence, scene&gt; mappings that involve
relativised sentences and dual event scenes. A
</bodyText>
<page confidence="0.996288">
36
</page>
<bodyText confidence="0.845766739130435">
small corpus of complex &lt;sentence, scene&gt; pairs
were generated corresponding to the grammatical
construction types 5-10
5. The block that pushed the triangle touched the
moon.
6. The block pushed the triangle that touched the
moon.
7. The block that pushed the triangle was touched by
the moon.
8. The block pushed the triangle that was touched the
moon.
9. The block that was pushed by the triangle touched
the moon.
10. The block was pushed by the triangle that touched
the moon.
After exposure to less than 100 sentences
generated from these relativised constructions, the
model performed without error for these 6
construction types. In the generalization test, the
learned values were fixed, and the model
demonstrated error-free performance on new
sentences for all six grammatical forms that had
not been used during the training.
</bodyText>
<subsectionHeader confidence="0.699078">
3.1.4 Combined Test
</subsectionHeader>
<bodyText confidence="0.999964769230769">
The objective of the final experiment was to
verify that the model was capable of learning the
10 grammatical forms together in a single learning
session. Training material from the previous
experiments were employed that exercised the
ensemble of 10 grammatical forms. After
exposure to less than 150 &lt;sentence, scene&gt; pairs,
the model performed without error. Likewise, in
the generalization test the learned values were
fixed, and the model demonstrated error-free
performance on new sentences for all ten
grammatical forms that had not been used during
the training.
This set of experiments in ideal conditions
demonstrates a proof of concept for the system,
though several open questions can be posed based
on these results. First, while the demonstration
with 10 grammatical constructions is interesting,
we can ask if the model will generalize to an
extended set of constructions. Second, we know
that the English language is quite restricted with
respect to its word order, and thus we can ask
whether the theoretical framework of the model
will generalize to free word order languages such
as Japanese. These questions are addressed in the
following three sections.
</bodyText>
<subsectionHeader confidence="0.9908395">
3.2 Generalization to Extended Construction
Set
</subsectionHeader>
<bodyText confidence="0.999979072727273">
As illustrated above the model can accommodate
10 distinct form-meaning mappings or
grammatical constructions, including constructions
involving &amp;quot;dual&amp;quot; events in the meaning
representation that correspond to relative clauses.
Still, this is a relatively limited size for the
construction inventory. The current experiment
demonstrates how the model generalizes to a
number of new and different relative phrases, as
well as additional sentence types including:
conjoined (John took the key and opened the door),
reflexive (The boy said that the dog was chased by
the cat), and reflexive pronoun (The block said that
it pushed the cylinder) sentence types, for a total of
38 distinct abstract grammatical constructions. The
consideration of these sentence types requires us to
address how their meanings are represented.
Conjoined sentences are represented by the two
corresponding events, e.g. took(John, key),
open(John, door) for the conjoined example above.
Reflexives are represented, for example, as
said(boy), chased(cat, dog). This assumes indeed,
for reflexive verbs (e.g. said, saw), that the
meaning representation includes the second event
as an argument to the first. Finally, for the
reflexive pronoun types, in the meaning
representation the pronoun&apos;s referent is explicit, as
in said(block), push(block, cylinder) for &amp;quot;The
block said that it pushed the cylinder.&amp;quot;
For this testing, the ConstructionInventory is
implemented as a lookup table in which the
ConstructionIndex is paired with the corresponding
SentenceToScene mapping during a single learning
trial. Based on the tenets of the construction
grammar framework (Goldberg 1995), if a
sentence is encountered that has a form (i.e.
ConstructionIndex) that does not have a
corresponding entry in the ConstructionInventory,
then a new construction is defined. Thus, one
exposure to a sentence of a new construction type
allows the model to generalize to any new sentence
of that type. In this sense, developing the capacity
to handle a simple initial set of constructions leads
to a highly extensible system. Using the training
procedures as described above, with a pre-learned
lexicon (WordToReferent), the model successfully
learned all of the constructions, and demonstrated
generalization to new sentences that it was not
trained on.
That the model can accommodate these 38
different grammatical constructions with no
modifications indicates its capability to generalize.
This translates to a (partial) validation of the
hypothesis that across languages, thematic role
assignment is encoded by a limited set of
</bodyText>
<page confidence="0.998605">
37
</page>
<bodyText confidence="0.999114333333333">
parameters including word order and grammatical
marking, and that distinct grammatical
constructions will have distinct and identifying
ensembles of these parameters. However, these
results have been obtained with English that is a
relatively fixed word-order language, and a more
rigorous test of this hypothesis would involve
testing with a free word-order language such as
Japanese.
</bodyText>
<subsectionHeader confidence="0.999682">
3.3 Generalization to Japanese
</subsectionHeader>
<bodyText confidence="0.9997751">
The current experiment will test the model with
sentences in Japanese. Unlike English, Japanese
allows extensive liberty in the ordering of words,
with grammatical roles explicitly marked by
postpositional function words -ga, -ni, -wo, -yotte.
This word-order flexibility of Japanese with
respect to English is illustrated here with the
English active and passive di-transitive forms that
each can be expressed in 4 different common
manners in Japanese:
</bodyText>
<listItem confidence="0.992605285714286">
1. The block gave the circle to the triangle.
1.1 Block-ga triangle-ni circle-wo watashita .
1.2 Block-ga circle-wo triangle-ni watashita .
1.3 Triangle-ni block-ga circle-wo watashita .
1.4 Circle-wo block-ga triangle-ni watashita .
2. The circle was given to the triangle by the
block.
</listItem>
<subsectionHeader confidence="0.9966245">
2.1 Circle-ga block-ni-yotte triangle-ni watasareta.
2.2 Block-ni-yotte circle-ga triangle-ni watasareta .
2.3 Block-ni-yotte triangle-ni circle-ga watasareta .
2.4 Triangle-ni circle-ga block-ni-yotte watasareta
</subsectionHeader>
<bodyText confidence="0.999479631578947">
.
In the “active” Japanese sentences, the
postpositional function words -ga, -ni and –wo
explicitly mark agent, recipient and, object
whereas in the passive, these are marked
respectively by –ni-yotte, -ga, and –ni. For both
the active and passive forms, there are four
different legal word-order permutations that
preserve and rely on this marking. Japanese thus
provides an interesting test of the model’s ability to
accommodate such freedom in word order.
Employing the same method as described in the
previous experiment, we thus expose the model to
&lt;sentence, meaning&gt; pairs generated from 26
Japanese constructions that employ the equivalent
of active, passive, relative forms and their
permutations. We predicted that by processing the
-ga, -ni, -yotte and –wo markers as closed class
elements, the model would be able to discriminate
and identify the distinct grammatical constructions
and learn the corresponding mappings. Indeed, the
model successfully discriminates between all of the
construction types based on the ConstructionIndex
unique to each construction type, and associates
the correct SentenceToScene mapping with each of
them. As for the English constructions, once
learned, a given construction could generalize to
new untrained sentences.
This demonstration with Japanese is an
important validation that at least for this subset of
constructions, the construction-based model is
applicable both to fixed word order languages such
as English, as well as free word order languages
such as Japanese. This also provides further
validation for the proposal of Bates and
MacWhinney (et al. 1982) that thematic roles are
indicated by a constellation of cues including
grammatical markers and word order.
</bodyText>
<subsectionHeader confidence="0.992643">
3.4 Effects of Noise
</subsectionHeader>
<bodyText confidence="0.999876578947368">
The model relies on lexical categorization of
open vs. closed class words both for learning
lexical semantics, and for building the
ConstructionIndex for phrasal semantics. While we
can cite strong evidence that this capability is
expressed early in development (Shi et al. 1999) it
is still likely that there will be errors in lexical
categorization. The performance of the model for
learning lexical and phrasal semantics for active
transitive and ditransitive structures is thus
examined under different conditions of lexical
categorization errors. A lexical categorization error
consists of a given word being assigned to the
wrong category and processed as such (e.g. an
open class word being processed as a closed class
word, or vice-versa). Figure 2 illustrates the
performance of the model with random errors of
this type introduced at levels of 0 to 20 percent
errors.
</bodyText>
<figureCaption confidence="0.808376">
Figure 2. The effects of Lexical Categorization Errors (mis-
</figureCaption>
<bodyText confidence="0.919867714285714">
categorization of an open-class word as a closed-class word or vice-
versa) on performance (Scene Interpretation Errors) over Training
Epochs. The 0% trace indicates performance in the absences of noise,
with a rapid elimination of errors . The successive introduction of
categorization errors yields a corresponding progressive impairment in
learning. While sensitive to the errors, the system demonstrates a
desired graceful degradation
</bodyText>
<page confidence="0.997002">
38
</page>
<bodyText confidence="0.998997592592593">
We can observe that there is a graceful 39 The obvious weakness is that it does not go
degradation, with interpretation errors further. That is, it cannot accommodate new
progressively increasing as categorization errors construction types without first being exposed to a
rise to 20 percent. In order to further asses the training example of a well formed &lt;sentence,
learning that was able to occur in the presence of meaning&gt; pair. Interestingly, however, this
noise, after training with noise, we then tested appears to reflect a characteristic stage of human
performance on noise-free input. The interpretation development, in which the infant relies on the use
error values in these conditions were 0.0, 0.4, 2.3, of constructions that she has previously heard (see
20.7 and 33.6 out of a maximum of 44 for training Tomasello 2003). Further on in development,
with 0, 5, 10, 15 and 20 percent lexical however, as pattern finding mechanisms operate
categorization errors, respectively. This indicates on statistically relevant samples of this data, the
that up to 10 percent input lexical categorization child begins to recognize structural patterns,
errors allows almost error free learning. At 15 corresponding for example to noun phrases (rather
percent input errors the model has still than solitary nouns) in relative clauses. When this
significantly improved with respect to the random is achieved, these phrasal units can then be inserted
behavior (~45 interpretation errors per epoch). into existing constructions, thus providing the basis
Other than reducing the lexical and phrasal for “on the fly” processing of novel relativised
learning rates, no efforts were made to optimize constructions. This suggests how the abstract
the performance for these degraded conditions, construction model can be extended to a more
thus there remains a certain degree of freedom for generalized compositional capability. We are
improvement. The main point is that the model currently addressing this issue in an extension of
does not demonstrate a catastrophic failure in the the proposed model, in which recognition of
presence of lexical categorization errors. linguistic markers (e.g. “that”, and directly
4 Discussion successive NPs) are learned to signal embedded
The research demonstrates an implementation of relative phrases (see Miikkulainen 1996).
a model of sentence-to-meaning mapping in the Future work will address the impact of
developmental and neuropsychologically inspired ambiguous input. The classical example “John
construction grammar framework. The strength of saw the girl with the telescope” implies that a
the model is that with relatively simple “innate” given grammatical form can map onto multiple
learning mechanisms, it can acquire a variety of meaning structures. In order to avoid this violation
grammatical constructions in English and Japanese of the one to one mapping, we must concede that
based on exposure to &lt;sentence, meaning&gt; pairs, form is influenced by context. Thus, the model
with only the lexical categories of open vs. closed will fail in the same way that humans do, and
class being prespecified. This lexical should be able to succeed in the same way that
categorization can be provided by frequency humans do. That is, when context is available to
analysis, and/or acoustic properties specific to the disambiguate then ambiguity can be resolved. This
two classes (Blanc et al. 2003; Shi et al. 1999). The will require maintenance of the recent discourse
model learns grammatical constructions, and context, and the influence of this on grammatical
generalizes in a systematic manner to new construction selection to reduce ambiguity.
sentences within the class of learned constructions. 5 Acknowledgements
This demonstrates the cross-linguistic validity of This work was supported by the ACI
our implementation of the construction grammar Computational Neuroscience Project, The
approach (Goldberg 1995, Tomasello 2003) and of Eurocores OMLL project and the HFSP
the “cue competition” model for coding of Organization.
grammatical structure (Bates et al. 1982). The References
point of the Japanese study was to demonstrate this Bates E, McNew S, MacWhinney B, Devescovi A,
cross-linguistic validity – i.e. that nothing extra Smith S (1982) Functional constraints on
was needed, just the identification of constructions sentence processing: A cross linguistic study,
based on lexical category information. Of course a Cognition (11) 245-299.
better model for Japanese and Hungarian etc. that Blanc JM, Dodane C, Dominey P (2003)
exploits the explicit marking of grammatical roles Temporal processing for syntax acquisition.
of NPs would have been interesting – but it Proc. 25th Ann. Mtg. Cog. Science Soc. Boston
wouldn’t have worked for English! Brown CM, Hagoort P, ter Keurs M (1999)
Electrophysiological signatures of visual lexical
</bodyText>
<reference confidence="0.996379978723404">
processing : Open- and closed-class words.
Journal of Cognitive Neuroscience. 11 :3, 261-
281
Chang NC, Maia TV (2001) Grounded learning of
grammatical constructions, AAAI Spring Symp.
On Learning Grounded Representations,
Stanford CA.
Chomsky N. (1995) The Minimalist Program.
MIT
Crangle C. &amp; Suppes P. (1994) Language and
Learning for Robots, CSLI lecture notes: no. 41,
Stanford.
Dominey PF, Ramus F (2000) Neural network
processing of natural language: I. Sensitivity to
serial, temporal and abstract structure of
language in the infant. Lang. and Cognitive
Processes, 15(1) 87-127
Dominey PF (2000) Conceptual Grounding in
Simulation Studies of Language Acquisition,
Evolution of Communication, 4(1), 57-85.
Dominey PF, Hoen M, Lelekov T, Blanc JM
(2003) Neurological basis of language and
sequential cognition: Evidence from simulation,
aphasia and ERP studies, Brain and Language,
86, 207-225
Elman J (1990) Finding structure in time.
Cognitive Science, 14:179-211.
Feldman JA, Lakoff G, Stolcke A, Weber SH
(1990) Miniature language acquisition: A
touchstone for cognitive science. In Proceedings
of the 12th Ann Conf. Cog. Sci. Soc. 686-693,
MIT, Cambridge MA
Feldman J., G. Lakoff, D. Bailey, S. Narayanan, T.
Regier, A. Stolcke (1996). L0: The First Five
Years. Artificial Intelligence Review, v10 103-
129.
Goldberg A (1995) Constructions. U Chicago
Press, Chicago and London.
Hirsh-Pasek K, Golinkof RM (1996) The origins of
grammar: evidence from early language
comprehension. MIT Press, Boston.
Kotovsky L, Baillargeon R, The development of
calibration-based reasoning about collision
events in young infants. 1998, Cognition, 67,
311-351
Langacker, R. (1991). Foundations of Cognitive
Grammar. Practical Applications, Volume 2.
Stanford University Press, Stanford.
Mandler J (1999) Preverbal representations and
language, in P. Bloom, MA Peterson, L Nadel
and MF Garrett (Eds) Language and Space, MIT
Press, 365-384
Miikkulainen R (1996) Subsymbolic case-role
analysis of sentences with embedded clauses.
Cognitive Science, 20:47-73.
Morgan JL, Shi R, Allopenna P (1996) Perceptual
bases of rudimentary grammatical categories:
Toward a broader conceptualization of
bootstrapping, pp 263-286, in Morgan JL,
Demuth K (Eds) Signal to syntax, Lawrence
Erlbaum, Mahwah NJ, USA.
Pollack JB (1990) Recursive distributed
representations. Artificial Intelligence, 46:77-
105.
Roy D, Pentland A (2002). Learning Words from
Sights and Sounds: A Computational Model.
Cognitive Science, 26(1), 113-146.
Shi R., Werker J.F., Morgan J.L. (1999) Newborn
infants&apos; sensitivity to perceptual cues to lexical
and grammatical words, Cognition, Volume 72,
Issue 2, B11-B21.
Siskind JM (1996) A computational study of cross-
situational techniques for learning word-to-
meaning mappings, Cognition (61) 39-91.
Siskind JM (2001) Grounding the lexical semantics
of verbs in visual perception using force
dynamics and event logic. Journal of AI
Research (15) 31-90
Steels, L. (2001) Language Games for
Autonomous Robots. IEEE Intelligent Systems,
vol. 16, nr. 5, pp. 16-22, New York: IEEE Press.
Stolcke A, Omohundro SM (1994) Inducing
probabilistic grammars by Bayesian model
merging/ In Grammatical Inference and
Applications: Proc. 2nd Intl. Colloq. On
Grammatical Inference, Springer Verlag.
Talmy L (1988) Force dynamics in language and
cognition. Cognitive Science, 10(2) 117-149.
Tomasello M (1999) The item-based nature of
children&apos;s early syntactic development, Trends in
Cognitive Science, 4(4):156-163
Tomasello, M. (2003) Constructing a language: A
usage-based theory of language acquisition.
Harvard University Press, Cambridge.
</reference>
<page confidence="0.998649">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.586941">
<title confidence="0.9978865">A Developmental Model of Syntax Acquisition in the Construction Grammar Framework with Cross-Linguistic Validation in English and Japanese</title>
<author confidence="0.997883">Peter Ford</author>
<affiliation confidence="0.9558265">Sequential Cognition and Language Institut des Sciences Cognitives,</affiliation>
<address confidence="0.999714">69675 Bron CEDES,</address>
<email confidence="0.993019">dominey@isc.cnrs.fr</email>
<author confidence="0.744543">Toshio Inui</author>
<affiliation confidence="0.9895675">Graduate School of Kyoto University,</affiliation>
<address confidence="0.957901">Yoshida-honmachi, Sakyo-ku, 606-8501, Kyoto, Japan</address>
<email confidence="0.979446">inui@kyoto-u.ac.jp</email>
<abstract confidence="0.999624689655172">The current research demonstrates a system inspired by cognitive neuroscience and developmental psychology that learns to construct mappings between the grammatical structure of sentences and the structure of their meaning representations. Sentence to meaning mappings are learned and stored as grammatical constructions. These are stored and retrieved from a construction inventory based on the constellation of closed class items uniquely identifying each construction. These learned mappings allow the system to processes natural language sentences in order to reconstruct complex internal representations of the meanings these sentences describe. The system demonstrates error free performance and systematic generalization for a rich subset of English constructions that includes complex hierarchical grammatical structure, and generalizes systematically to new sentences of the learned construction categories. Further testing demonstrates (1) the capability to accommodate a significantly extended set of constructions, and (2) extension to Japanese, a free word order language that is structurally quite different from English, thus demonstrating the extensibility of the structure mapping model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>processing</author>
</authors>
<title>Open- and closed-class words.</title>
<journal>Journal of Cognitive Neuroscience.</journal>
<volume>11</volume>
<pages>261--281</pages>
<marker>processing, </marker>
<rawString>processing : Open- and closed-class words. Journal of Cognitive Neuroscience. 11 :3, 261-281</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang NC</author>
<author>Maia TV</author>
</authors>
<title>Grounded learning of grammatical constructions, AAAI Spring Symp. On Learning Grounded Representations,</title>
<date>2001</date>
<location>Stanford CA.</location>
<marker>NC, TV, 2001</marker>
<rawString>Chang NC, Maia TV (2001) Grounded learning of grammatical constructions, AAAI Spring Symp. On Learning Grounded Representations, Stanford CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>The Minimalist Program.</title>
<date>1995</date>
<publisher>MIT</publisher>
<contexts>
<context position="2077" citStr="Chomsky 1995" startWordPosition="276" endWordPosition="277">ion to Japanese, a free word order language that is structurally quite different from English, thus demonstrating the extensibility of the structure mapping model. 1 Introduction The nativist perspective on the problem of language acquisition holds that the &lt;sentence, meaning&gt; data to which the child is exposed is highly indeterminate, and underspecifies the mapping to be learned. This “poverty of the stimulus” is a central argument for the existence of a genetically specified universal grammar, such that language acquisition consists of configuring the UG for the appropriate target language (Chomsky 1995). In this framework, once a given parameter is set, its use should apply to new constructions in a generalized, generative manner. An alternative functionalist perspective holds that learning plays a much more central role in language acquisition. The infant develops an inventory of grammatical constructions as mappings from form to meaning (Goldberg 1995). These constructions are initially rather fixed and specific, and later become generalized into a more abstract compositional form employed by the adult (Tomasello 1999, 2003). In this context, construction of the relation between perceptual</context>
</contexts>
<marker>Chomsky, 1995</marker>
<rawString>Chomsky N. (1995) The Minimalist Program. MIT</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Crangle</author>
<author>P Suppes</author>
</authors>
<title>Language and Learning for Robots,</title>
<date>1994</date>
<booktitle>CSLI lecture notes: no. 41,</booktitle>
<location>Stanford.</location>
<marker>Crangle, Suppes, 1994</marker>
<rawString>Crangle C. &amp; Suppes P. (1994) Language and Learning for Robots, CSLI lecture notes: no. 41, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominey PF</author>
<author>F Ramus</author>
</authors>
<title>Neural network processing of natural language: I. Sensitivity to serial, temporal and abstract structure of language</title>
<date>2000</date>
<booktitle>in the infant. Lang. and Cognitive Processes,</booktitle>
<volume>15</volume>
<issue>1</issue>
<pages>87--127</pages>
<marker>PF, Ramus, 2000</marker>
<rawString>Dominey PF, Ramus F (2000) Neural network processing of natural language: I. Sensitivity to serial, temporal and abstract structure of language in the infant. Lang. and Cognitive Processes, 15(1) 87-127</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominey PF</author>
</authors>
<title>Conceptual Grounding in Simulation Studies of Language Acquisition,</title>
<date>2000</date>
<journal>Evolution of Communication,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>57--85</pages>
<marker>PF, 2000</marker>
<rawString>Dominey PF (2000) Conceptual Grounding in Simulation Studies of Language Acquisition, Evolution of Communication, 4(1), 57-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominey PF</author>
<author>M Hoen</author>
<author>T Lelekov</author>
<author>Blanc JM</author>
</authors>
<title>Neurological basis of language and sequential cognition: Evidence from simulation, aphasia and ERP studies,</title>
<date>2003</date>
<journal>Brain and Language,</journal>
<volume>86</volume>
<pages>207--225</pages>
<marker>PF, Hoen, Lelekov, JM, 2003</marker>
<rawString>Dominey PF, Hoen M, Lelekov T, Blanc JM (2003) Neurological basis of language and sequential cognition: Evidence from simulation, aphasia and ERP studies, Brain and Language, 86, 207-225</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<pages>14--179</pages>
<contexts>
<context position="3003" citStr="Elman (1990)" startWordPosition="412" endWordPosition="413">appings from form to meaning (Goldberg 1995). These constructions are initially rather fixed and specific, and later become generalized into a more abstract compositional form employed by the adult (Tomasello 1999, 2003). In this context, construction of the relation between perceptual and cognitive representations and grammatical form plays a central role in learning language (e.g. Feldman et al. 1990, 1996; Langacker 1991; Mandler 1999; Talmy 1998). These issues of learnability and innateness have provided a rich motivation for simulation studies that have taken a number of different forms. Elman (1990) demonstrated that recurrent networks are sensitive to predictable structure in grammatical sequences. Subsequent studies of grammar induction demonstrate how syntactic structure can be recovered from sentences (e.g. Stolcke &amp; Omohundro 1994). From the “grounding of language in meaning” perspective (e.g. Feldman et al. 1990, 1996; Langacker 1991; Goldberg 1995) Chang &amp; Maia (2001) exploited the relations between action representation and simple verb frames in a construction grammar approach. In effort to consider more complex grammatical forms, Miikkulainen (1996) demonstrated a system that le</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Elman J (1990) Finding structure in time. Cognitive Science, 14:179-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feldman JA</author>
<author>G Lakoff</author>
<author>A Stolcke</author>
<author>Weber SH</author>
</authors>
<title>Miniature language acquisition: A touchstone for cognitive science.</title>
<date>1990</date>
<booktitle>In Proceedings of the 12th Ann Conf. Cog. Sci. Soc. 686-693, MIT,</booktitle>
<location>Cambridge MA</location>
<marker>JA, Lakoff, Stolcke, SH, 1990</marker>
<rawString>Feldman JA, Lakoff G, Stolcke A, Weber SH (1990) Miniature language acquisition: A touchstone for cognitive science. In Proceedings of the 12th Ann Conf. Cog. Sci. Soc. 686-693, MIT, Cambridge MA</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Feldman</author>
<author>G Lakoff</author>
<author>D Bailey</author>
<author>S Narayanan</author>
<author>T Regier</author>
<author>A Stolcke</author>
</authors>
<title>L0: The First Five Years.</title>
<date>1996</date>
<journal>Artificial Intelligence Review,</journal>
<volume>10</volume>
<pages>103--129</pages>
<marker>Feldman, Lakoff, Bailey, Narayanan, Regier, Stolcke, 1996</marker>
<rawString>Feldman J., G. Lakoff, D. Bailey, S. Narayanan, T. Regier, A. Stolcke (1996). L0: The First Five Years. Artificial Intelligence Review, v10 103-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goldberg</author>
</authors>
<title>Constructions.</title>
<date>1995</date>
<publisher>U Chicago Press,</publisher>
<location>Chicago and London.</location>
<contexts>
<context position="2435" citStr="Goldberg 1995" startWordPosition="329" endWordPosition="330">the mapping to be learned. This “poverty of the stimulus” is a central argument for the existence of a genetically specified universal grammar, such that language acquisition consists of configuring the UG for the appropriate target language (Chomsky 1995). In this framework, once a given parameter is set, its use should apply to new constructions in a generalized, generative manner. An alternative functionalist perspective holds that learning plays a much more central role in language acquisition. The infant develops an inventory of grammatical constructions as mappings from form to meaning (Goldberg 1995). These constructions are initially rather fixed and specific, and later become generalized into a more abstract compositional form employed by the adult (Tomasello 1999, 2003). In this context, construction of the relation between perceptual and cognitive representations and grammatical form plays a central role in learning language (e.g. Feldman et al. 1990, 1996; Langacker 1991; Mandler 1999; Talmy 1998). These issues of learnability and innateness have provided a rich motivation for simulation studies that have taken a number of different forms. Elman (1990) demonstrated that recurrent net</context>
<context position="4555" citStr="Goldberg 1995" startWordPosition="636" endWordPosition="637">oded in all languages by word order and grammatical marking (bound or free) (Bates et al. 1982). That model was based on 33 “gugle” to the object of push. WordToReferent(i,j) = WordToReferent(i,j) + OCA(k,i) * SEA(m,j) * max(a, SentenceToScene(m,k)) (1) the functional neurophysiology of cognitive sequence and language processing and an associated neural network model that has been demonstrated to simulate interesting aspects of infant (Dominey &amp; Ramus 2000) and adult language processing (Dominey et al. 2003). 2 Structure mapping for language learning The mapping of sentence form onto meaning (Goldberg 1995) takes place at two distinct levels in the current model: Words are associated with individual components of event descriptions, and grammatical structure is associated with functional roles within scene events. The first level has been addressed by Siskind (1996), Roy &amp; Pentland (2002) and Steels (2001) and we treat it here in a relatively simple but effective manner. Our principle interest lies more in the second level of mapping between scene and sentence structure. Equations 1-7 implement the model depicted in Figure 1, and are derived from a neurophysiologically motivated model of sensori</context>
<context position="20783" citStr="Goldberg 1995" startWordPosition="3128" endWordPosition="3129">his assumes indeed, for reflexive verbs (e.g. said, saw), that the meaning representation includes the second event as an argument to the first. Finally, for the reflexive pronoun types, in the meaning representation the pronoun&apos;s referent is explicit, as in said(block), push(block, cylinder) for &amp;quot;The block said that it pushed the cylinder.&amp;quot; For this testing, the ConstructionInventory is implemented as a lookup table in which the ConstructionIndex is paired with the corresponding SentenceToScene mapping during a single learning trial. Based on the tenets of the construction grammar framework (Goldberg 1995), if a sentence is encountered that has a form (i.e. ConstructionIndex) that does not have a corresponding entry in the ConstructionInventory, then a new construction is defined. Thus, one exposure to a sentence of a new construction type allows the model to generalize to any new sentence of that type. In this sense, developing the capacity to handle a simple initial set of constructions leads to a highly extensible system. Using the training procedures as described above, with a pre-learned lexicon (WordToReferent), the model successfully learned all of the constructions, and demonstrated gen</context>
<context position="30165" citStr="Goldberg 1995" startWordPosition="4536" endWordPosition="4537">ific to the disambiguate then ambiguity can be resolved. This two classes (Blanc et al. 2003; Shi et al. 1999). The will require maintenance of the recent discourse model learns grammatical constructions, and context, and the influence of this on grammatical generalizes in a systematic manner to new construction selection to reduce ambiguity. sentences within the class of learned constructions. 5 Acknowledgements This demonstrates the cross-linguistic validity of This work was supported by the ACI our implementation of the construction grammar Computational Neuroscience Project, The approach (Goldberg 1995, Tomasello 2003) and of Eurocores OMLL project and the HFSP the “cue competition” model for coding of Organization. grammatical structure (Bates et al. 1982). The References point of the Japanese study was to demonstrate this Bates E, McNew S, MacWhinney B, Devescovi A, cross-linguistic validity – i.e. that nothing extra Smith S (1982) Functional constraints on was needed, just the identification of constructions sentence processing: A cross linguistic study, based on lexical category information. Of course a Cognition (11) 245-299. better model for Japanese and Hungarian etc. that Blanc JM, </context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Goldberg A (1995) Constructions. U Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hirsh-Pasek</author>
<author>Golinkof RM</author>
</authors>
<title>The origins of grammar: evidence from early language comprehension.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Boston.</location>
<marker>Hirsh-Pasek, RM, 1996</marker>
<rawString>Hirsh-Pasek K, Golinkof RM (1996) The origins of grammar: evidence from early language comprehension. MIT Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kotovsky</author>
<author>R Baillargeon</author>
</authors>
<title>The development of calibration-based reasoning about collision events in young infants.</title>
<date>1998</date>
<journal>Cognition,</journal>
<volume>67</volume>
<pages>311--351</pages>
<marker>Kotovsky, Baillargeon, 1998</marker>
<rawString>Kotovsky L, Baillargeon R, The development of calibration-based reasoning about collision events in young infants. 1998, Cognition, 67, 311-351</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Langacker</author>
</authors>
<title>Foundations of Cognitive Grammar. Practical Applications, Volume 2.</title>
<date>1991</date>
<publisher>Stanford University Press, Stanford.</publisher>
<contexts>
<context position="2818" citStr="Langacker 1991" startWordPosition="384" endWordPosition="385">er. An alternative functionalist perspective holds that learning plays a much more central role in language acquisition. The infant develops an inventory of grammatical constructions as mappings from form to meaning (Goldberg 1995). These constructions are initially rather fixed and specific, and later become generalized into a more abstract compositional form employed by the adult (Tomasello 1999, 2003). In this context, construction of the relation between perceptual and cognitive representations and grammatical form plays a central role in learning language (e.g. Feldman et al. 1990, 1996; Langacker 1991; Mandler 1999; Talmy 1998). These issues of learnability and innateness have provided a rich motivation for simulation studies that have taken a number of different forms. Elman (1990) demonstrated that recurrent networks are sensitive to predictable structure in grammatical sequences. Subsequent studies of grammar induction demonstrate how syntactic structure can be recovered from sentences (e.g. Stolcke &amp; Omohundro 1994). From the “grounding of language in meaning” perspective (e.g. Feldman et al. 1990, 1996; Langacker 1991; Goldberg 1995) Chang &amp; Maia (2001) exploited the relations between</context>
</contexts>
<marker>Langacker, 1991</marker>
<rawString>Langacker, R. (1991). Foundations of Cognitive Grammar. Practical Applications, Volume 2. Stanford University Press, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mandler</author>
</authors>
<title>Preverbal representations and language,</title>
<date>1999</date>
<pages>365--384</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="2832" citStr="Mandler 1999" startWordPosition="386" endWordPosition="387">ve functionalist perspective holds that learning plays a much more central role in language acquisition. The infant develops an inventory of grammatical constructions as mappings from form to meaning (Goldberg 1995). These constructions are initially rather fixed and specific, and later become generalized into a more abstract compositional form employed by the adult (Tomasello 1999, 2003). In this context, construction of the relation between perceptual and cognitive representations and grammatical form plays a central role in learning language (e.g. Feldman et al. 1990, 1996; Langacker 1991; Mandler 1999; Talmy 1998). These issues of learnability and innateness have provided a rich motivation for simulation studies that have taken a number of different forms. Elman (1990) demonstrated that recurrent networks are sensitive to predictable structure in grammatical sequences. Subsequent studies of grammar induction demonstrate how syntactic structure can be recovered from sentences (e.g. Stolcke &amp; Omohundro 1994). From the “grounding of language in meaning” perspective (e.g. Feldman et al. 1990, 1996; Langacker 1991; Goldberg 1995) Chang &amp; Maia (2001) exploited the relations between action repres</context>
</contexts>
<marker>Mandler, 1999</marker>
<rawString>Mandler J (1999) Preverbal representations and language, in P. Bloom, MA Peterson, L Nadel and MF Garrett (Eds) Language and Space, MIT Press, 365-384</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Miikkulainen</author>
</authors>
<title>Subsymbolic case-role analysis of sentences with embedded clauses.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<pages>20--47</pages>
<contexts>
<context position="3573" citStr="Miikkulainen (1996)" startWordPosition="490" endWordPosition="491">e taken a number of different forms. Elman (1990) demonstrated that recurrent networks are sensitive to predictable structure in grammatical sequences. Subsequent studies of grammar induction demonstrate how syntactic structure can be recovered from sentences (e.g. Stolcke &amp; Omohundro 1994). From the “grounding of language in meaning” perspective (e.g. Feldman et al. 1990, 1996; Langacker 1991; Goldberg 1995) Chang &amp; Maia (2001) exploited the relations between action representation and simple verb frames in a construction grammar approach. In effort to consider more complex grammatical forms, Miikkulainen (1996) demonstrated a system that learned the mapping between relative phrase constructions and multiple event representations, based on the use of a stack for maintaining state information during the processing of the next embedded clause in a recursive manner. In a more generalized approach, Dominey (2000) exploited the regularity that sentence to meaning mapping is encoded in all languages by word order and grammatical marking (bound or free) (Bates et al. 1982). That model was based on 33 “gugle” to the object of push. WordToReferent(i,j) = WordToReferent(i,j) + OCA(k,i) * SEA(m,j) * max(a, Sent</context>
<context position="28568" citStr="Miikkulainen 1996" startWordPosition="4294" endWordPosition="4295">t the performance for these degraded conditions, construction model can be extended to a more thus there remains a certain degree of freedom for generalized compositional capability. We are improvement. The main point is that the model currently addressing this issue in an extension of does not demonstrate a catastrophic failure in the the proposed model, in which recognition of presence of lexical categorization errors. linguistic markers (e.g. “that”, and directly 4 Discussion successive NPs) are learned to signal embedded The research demonstrates an implementation of relative phrases (see Miikkulainen 1996). a model of sentence-to-meaning mapping in the Future work will address the impact of developmental and neuropsychologically inspired ambiguous input. The classical example “John construction grammar framework. The strength of saw the girl with the telescope” implies that a the model is that with relatively simple “innate” given grammatical form can map onto multiple learning mechanisms, it can acquire a variety of meaning structures. In order to avoid this violation grammatical constructions in English and Japanese of the one to one mapping, we must concede that based on exposure to &lt;sentenc</context>
</contexts>
<marker>Miikkulainen, 1996</marker>
<rawString>Miikkulainen R (1996) Subsymbolic case-role analysis of sentences with embedded clauses. Cognitive Science, 20:47-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgan JL</author>
<author>R Shi</author>
<author>P Allopenna</author>
</authors>
<title>Perceptual bases of rudimentary grammatical categories: Toward a broader conceptualization of bootstrapping,</title>
<date>1996</date>
<pages>263--286</pages>
<location>Mahwah NJ, USA.</location>
<marker>JL, Shi, Allopenna, 1996</marker>
<rawString>Morgan JL, Shi R, Allopenna P (1996) Perceptual bases of rudimentary grammatical categories: Toward a broader conceptualization of bootstrapping, pp 263-286, in Morgan JL, Demuth K (Eds) Signal to syntax, Lawrence Erlbaum, Mahwah NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pollack JB</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--77</pages>
<marker>JB, 1990</marker>
<rawString>Pollack JB (1990) Recursive distributed representations. Artificial Intelligence, 46:77-105.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Roy</author>
</authors>
<title>Pentland A (2002). Learning Words from Sights and Sounds:</title>
<journal>A Computational Model. Cognitive Science,</journal>
<volume>26</volume>
<issue>1</issue>
<pages>113--146</pages>
<marker>Roy, </marker>
<rawString>Roy D, Pentland A (2002). Learning Words from Sights and Sounds: A Computational Model. Cognitive Science, 26(1), 113-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Shi</author>
<author>J F Werker</author>
<author>J L Morgan</author>
</authors>
<title>Newborn infants&apos; sensitivity to perceptual cues to lexical and grammatical words,</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>72</volume>
<pages>11--21</pages>
<contexts>
<context position="7023" citStr="Shi et al. 1999" startWordPosition="1031" endWordPosition="1034">lass array (OCA). Index m = 1 to 6, corresponding to the maximum number of elements in the scene event array (SEA). Indices i and j = 1 to 25, corresponding to the word and scene item vector sizes, respectively. 2.2 Open vs Closed Class Word Categories Our approach is based on the cross-linguistic observation that open class words (e.g. nouns, verbs, adjectives and adverbs) are assigned to their thematic roles based on word order and/or grammatical function words or morphemes (Bates et al. 1982). Newborn infants are sensitive to the perceptual properties that distinguish these two categories (Shi et al. 1999), and in adults, these categories are processed by dissociable neurophysiological systems (Brown et al. 1999). Similarly, artificial neural networks can also learn to make this function/content distinction (Morgan et al. 1996). Thus, for the speech input that is provided to the learning model open and closed class words are directed to separate processing streams that preserve their order and identity, as indicated in Figure 2. Figure 1. Structure-Mapping Architecture. 1. Lexical categorization. 2. Open class words in Open Class Array are translated to Predicted Referents in the PRA via the Wo</context>
<context position="25114" citStr="Shi et al. 1999" startWordPosition="3766" endWordPosition="3769">plicable both to fixed word order languages such as English, as well as free word order languages such as Japanese. This also provides further validation for the proposal of Bates and MacWhinney (et al. 1982) that thematic roles are indicated by a constellation of cues including grammatical markers and word order. 3.4 Effects of Noise The model relies on lexical categorization of open vs. closed class words both for learning lexical semantics, and for building the ConstructionIndex for phrasal semantics. While we can cite strong evidence that this capability is expressed early in development (Shi et al. 1999) it is still likely that there will be errors in lexical categorization. The performance of the model for learning lexical and phrasal semantics for active transitive and ditransitive structures is thus examined under different conditions of lexical categorization errors. A lexical categorization error consists of a given word being assigned to the wrong category and processed as such (e.g. an open class word being processed as a closed class word, or vice-versa). Figure 2 illustrates the performance of the model with random errors of this type introduced at levels of 0 to 20 percent errors. F</context>
<context position="29662" citStr="Shi et al. 1999" startWordPosition="4466" endWordPosition="4469">atical constructions in English and Japanese of the one to one mapping, we must concede that based on exposure to &lt;sentence, meaning&gt; pairs, form is influenced by context. Thus, the model with only the lexical categories of open vs. closed will fail in the same way that humans do, and class being prespecified. This lexical should be able to succeed in the same way that categorization can be provided by frequency humans do. That is, when context is available to analysis, and/or acoustic properties specific to the disambiguate then ambiguity can be resolved. This two classes (Blanc et al. 2003; Shi et al. 1999). The will require maintenance of the recent discourse model learns grammatical constructions, and context, and the influence of this on grammatical generalizes in a systematic manner to new construction selection to reduce ambiguity. sentences within the class of learned constructions. 5 Acknowledgements This demonstrates the cross-linguistic validity of This work was supported by the ACI our implementation of the construction grammar Computational Neuroscience Project, The approach (Goldberg 1995, Tomasello 2003) and of Eurocores OMLL project and the HFSP the “cue competition” model for codi</context>
</contexts>
<marker>Shi, Werker, Morgan, 1999</marker>
<rawString>Shi R., Werker J.F., Morgan J.L. (1999) Newborn infants&apos; sensitivity to perceptual cues to lexical and grammatical words, Cognition, Volume 72, Issue 2, B11-B21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siskind JM</author>
</authors>
<title>A computational study of crosssituational techniques for learning word-tomeaning mappings,</title>
<date>1996</date>
<journal>Cognition</journal>
<volume>61</volume>
<pages>39--91</pages>
<marker>JM, 1996</marker>
<rawString>Siskind JM (1996) A computational study of crosssituational techniques for learning word-tomeaning mappings, Cognition (61) 39-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siskind JM</author>
</authors>
<title>Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic.</title>
<date>2001</date>
<journal>Journal of AI Research</journal>
<volume>15</volume>
<pages>31--90</pages>
<marker>JM, 2001</marker>
<rawString>Siskind JM (2001) Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic. Journal of AI Research (15) 31-90</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Steels</author>
</authors>
<title>Language Games for Autonomous Robots.</title>
<date>2001</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>16</volume>
<pages>16--22</pages>
<publisher>IEEE Press.</publisher>
<location>New York:</location>
<contexts>
<context position="4860" citStr="Steels (2001)" startWordPosition="682" endWordPosition="683"> and language processing and an associated neural network model that has been demonstrated to simulate interesting aspects of infant (Dominey &amp; Ramus 2000) and adult language processing (Dominey et al. 2003). 2 Structure mapping for language learning The mapping of sentence form onto meaning (Goldberg 1995) takes place at two distinct levels in the current model: Words are associated with individual components of event descriptions, and grammatical structure is associated with functional roles within scene events. The first level has been addressed by Siskind (1996), Roy &amp; Pentland (2002) and Steels (2001) and we treat it here in a relatively simple but effective manner. Our principle interest lies more in the second level of mapping between scene and sentence structure. Equations 1-7 implement the model depicted in Figure 1, and are derived from a neurophysiologically motivated model of sensorimotor sequence learning (Dominey et al. 2003). 2.1 Word Meaning Equation (1) describes the associative memory, WordToReferent, that links word vectors in the OpenClassArray (OCA) with their referent vectors in the SceneEventArray (SEA)1. In the initial learning phases there is no influence of syntactic k</context>
</contexts>
<marker>Steels, 2001</marker>
<rawString>Steels, L. (2001) Language Games for Autonomous Robots. IEEE Intelligent Systems, vol. 16, nr. 5, pp. 16-22, New York: IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>Omohundro SM</author>
</authors>
<title>Inducing probabilistic grammars by Bayesian model merging/</title>
<date>1994</date>
<booktitle>In Grammatical Inference and Applications: Proc. 2nd Intl. Colloq. On Grammatical Inference,</booktitle>
<publisher>Springer Verlag.</publisher>
<marker>Stolcke, SM, 1994</marker>
<rawString>Stolcke A, Omohundro SM (1994) Inducing probabilistic grammars by Bayesian model merging/ In Grammatical Inference and Applications: Proc. 2nd Intl. Colloq. On Grammatical Inference, Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Talmy</author>
</authors>
<title>Force dynamics in language and cognition.</title>
<date>1988</date>
<journal>Cognitive Science,</journal>
<volume>10</volume>
<issue>2</issue>
<pages>117--149</pages>
<marker>Talmy, 1988</marker>
<rawString>Talmy L (1988) Force dynamics in language and cognition. Cognitive Science, 10(2) 117-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomasello</author>
</authors>
<title>The item-based nature of children&apos;s early syntactic development,</title>
<date>1999</date>
<journal>Trends in Cognitive Science,</journal>
<pages>4--4</pages>
<contexts>
<context position="2604" citStr="Tomasello 1999" startWordPosition="353" endWordPosition="354">ition consists of configuring the UG for the appropriate target language (Chomsky 1995). In this framework, once a given parameter is set, its use should apply to new constructions in a generalized, generative manner. An alternative functionalist perspective holds that learning plays a much more central role in language acquisition. The infant develops an inventory of grammatical constructions as mappings from form to meaning (Goldberg 1995). These constructions are initially rather fixed and specific, and later become generalized into a more abstract compositional form employed by the adult (Tomasello 1999, 2003). In this context, construction of the relation between perceptual and cognitive representations and grammatical form plays a central role in learning language (e.g. Feldman et al. 1990, 1996; Langacker 1991; Mandler 1999; Talmy 1998). These issues of learnability and innateness have provided a rich motivation for simulation studies that have taken a number of different forms. Elman (1990) demonstrated that recurrent networks are sensitive to predictable structure in grammatical sequences. Subsequent studies of grammar induction demonstrate how syntactic structure can be recovered from </context>
</contexts>
<marker>Tomasello, 1999</marker>
<rawString>Tomasello M (1999) The item-based nature of children&apos;s early syntactic development, Trends in Cognitive Science, 4(4):156-163</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomasello</author>
</authors>
<title>Constructing a language: A usage-based theory of language acquisition.</title>
<date>2003</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="27048" citStr="Tomasello 2003" startWordPosition="4070" endWordPosition="4071">construction types without first being exposed to a rise to 20 percent. In order to further asses the training example of a well formed &lt;sentence, learning that was able to occur in the presence of meaning&gt; pair. Interestingly, however, this noise, after training with noise, we then tested appears to reflect a characteristic stage of human performance on noise-free input. The interpretation development, in which the infant relies on the use error values in these conditions were 0.0, 0.4, 2.3, of constructions that she has previously heard (see 20.7 and 33.6 out of a maximum of 44 for training Tomasello 2003). Further on in development, with 0, 5, 10, 15 and 20 percent lexical however, as pattern finding mechanisms operate categorization errors, respectively. This indicates on statistically relevant samples of this data, the that up to 10 percent input lexical categorization child begins to recognize structural patterns, errors allows almost error free learning. At 15 corresponding for example to noun phrases (rather percent input errors the model has still than solitary nouns) in relative clauses. When this significantly improved with respect to the random is achieved, these phrasal units can the</context>
<context position="30182" citStr="Tomasello 2003" startWordPosition="4538" endWordPosition="4539">ambiguate then ambiguity can be resolved. This two classes (Blanc et al. 2003; Shi et al. 1999). The will require maintenance of the recent discourse model learns grammatical constructions, and context, and the influence of this on grammatical generalizes in a systematic manner to new construction selection to reduce ambiguity. sentences within the class of learned constructions. 5 Acknowledgements This demonstrates the cross-linguistic validity of This work was supported by the ACI our implementation of the construction grammar Computational Neuroscience Project, The approach (Goldberg 1995, Tomasello 2003) and of Eurocores OMLL project and the HFSP the “cue competition” model for coding of Organization. grammatical structure (Bates et al. 1982). The References point of the Japanese study was to demonstrate this Bates E, McNew S, MacWhinney B, Devescovi A, cross-linguistic validity – i.e. that nothing extra Smith S (1982) Functional constraints on was needed, just the identification of constructions sentence processing: A cross linguistic study, based on lexical category information. Of course a Cognition (11) 245-299. better model for Japanese and Hungarian etc. that Blanc JM, Dodane C, Dominey</context>
</contexts>
<marker>Tomasello, 2003</marker>
<rawString>Tomasello, M. (2003) Constructing a language: A usage-based theory of language acquisition. Harvard University Press, Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>