<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.9983925">
Learning Constraint Grammar-style disambiguation rules using
Inductive Logic Programming
</title>
<author confidence="0.996134">
Nikolaj Lindberg
</author>
<affiliation confidence="0.966926">
Centre for Speech Technology
Royal Institute of Technology
</affiliation>
<address confidence="0.585737">
SE-100 44 Stockholm, Sweden
</address>
<email confidence="0.595092">
nikolajOspeech.kth.se
</email>
<author confidence="0.666221">
Martin Eineborg
</author>
<affiliation confidence="0.4598005">
Telia Research AB
Spoken Language Processing
</affiliation>
<address confidence="0.375494">
SE-136 80 Haninge, Sweden
</address>
<email confidence="0.600487">
Martin. E.Eineborg@telia. se
</email>
<sectionHeader confidence="0.984345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999710357142857">
This paper reports a pilot study, in which
Constraint Grammar inspired rules were learnt
using the Progol machine-learning system.
Rules discarding faulty readings of ambiguously
tagged words were learnt for the part of speech
tags of the Stockholm-UmeS, Corpus. Several
thousand disambiguation rules were induced.
When tested on unseen data, 98% of the words
retained the correct reading after tagging. How-
ever, there were ambiguities pending after tag-
ging, on an average 1.13 tags per word. The
results suggest that the Progol system can be
useful for learning tagging rules of good qual-
ity.
</bodyText>
<sectionHeader confidence="0.998802" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937444444444">
The success of the Constraint Grammar (CG)
(Karlsson et al., 1995) approach to part of
speech tagging and surface syntactic depen-
dency parsing is due to the minutely hand-
crafted grammar and two-level morphology lex-
icon, developed over several years.
In the study reported here, the Progol
machine-learning system was used to induce
CG-style tag eliminating rules from a one mil-
lion word part of speech tagged corpus of
Swedish. Some 7 000 rules were induced. When
tested on unseen data, 98% of the words re-
tained the correct tag. There were still ambi-
guities left in the output, on an average 1.13
readings per word.
In the following sections, the CG framework
and the Progol machine learning system will be
presented very briefly.
</bodyText>
<subsectionHeader confidence="0.999045">
1.1 Constraint Grammar POS tagging
</subsectionHeader>
<bodyText confidence="0.9989788">
Constraint Grammar is a system for part of
speech tagging and (shallow) syntactic depen-
dency analysis of unrestricted text. In the fol-
lowing, only the part of speech tagging step will
be discussed.
The following as a typical `reductionistic&apos; ex-
ample of a CG rule which discards a verbal read-
ing of a word following a word unambiguously
tagged as determiner (Tapanainen, 1996, page
12):
</bodyText>
<sectionHeader confidence="0.68662" genericHeader="introduction">
REMOVE (V) IF (-1C DET) ;
</sectionHeader>
<bodyText confidence="0.999963">
where V is the target tag to be discarded and -1C
DET denotes the word immediately to the left
(-1), unambiguously (C) tagged as determiner
(DET). There are several types of rules, not only
`reductionistic&apos; ones, making the CG formalism
quite powerful. A full-scale CG has hundreds of
rules. The developers of English CG report that
99.7% of the words retain their correct reading,
and that 93-97% of the words are unambiguous
after tagging (Karlsson et al., 1995, page 186).
A parser applying the constraints is described
in Tapanainen (1996).
</bodyText>
<subsectionHeader confidence="0.980692">
1.2 Inductive Logic Programming
</subsectionHeader>
<bodyText confidence="0.999741142857143">
Inductive Logic Programming (ILP) is a combi-
nation of machine learning and logic program-
ming, where the goal is to find a hypothesis,
H, given examples, E, and background knowl-
edge, B, such that the hypothesis along with
the background knowledge logically implies the
examples (Muggleton, 1995, page 2):
</bodyText>
<sectionHeader confidence="0.367699" genericHeader="method">
BAH =E
</sectionHeader>
<bodyText confidence="0.999797">
The examples are usually split into a positive,
E+, and a negative, E-, subset.
The ILP system used in this paper, CPro-
gol Version 4.2, uses Horn clauses as the repre-
sentational language. Progol creates, for each
E+, a most specific clause _Li and then searches
through the lattice of hypotheses, from specific
</bodyText>
<page confidence="0.996584">
775
</page>
<bodyText confidence="0.961504575757576">
to more general, bounded by
H
to find the clause that maximally compresses
the data where -&lt; (0-subsumption) is defined as
ci c2 -1=&gt; 30 : c10 C c2
and 0 is the empty clause. As an example, con-
sider the two clauses:
: p(X,Y) ,Y)
C2: p(a,b) q(a,b) , r(Z)
where Cl c2 under the substitution 0 =
{X/a, Y/b}.
When Progol has found the clause that com-
presses the data the most, it is added to the
background knowledge and all examples that
are redundant with respect to this new back-
ground knowledge are removed.
More informally, Progol builds the most spe-
cific clause for each positive example. It then
tries to find a more general version of the clause
(with respect to the background knowledge and
mode declarations, see below) that explains as
many positive and as few negative examples as
possible.
Mode declarations specifying the properties
of the rules have to be given by the user. A
modeh declaration specifies the head of the rules,
while modeb declarations specify what the bod-
ies of the rules to induce might contain. The
user also declares the types of arguments, and
whether they are input or output arguments, or
if an argument should be instantiated by Pro-
gol. Progol is freely available and documented
in Muggleton (1995) and Roberts (1997).
</bodyText>
<subsectionHeader confidence="0.998855">
1.3 The Stockholm-Umea Corpus
</subsectionHeader>
<bodyText confidence="0.999856111111111">
The training material in the experiments re-
ported here is sampled from a pre-release of
the Stockholm-Umei Corpus (SUC). SUC cov-
ers just over one million words of part of speech
tagged Swedish text, sampled from different
text genres (largely following the Brown corpus
text categories). The first official release is now
available on CD-ROM.
The SUC tagset has 146 different tags, and
the tags consist of a part of speech tag, e.g. VB
(the verb) followed by a (possibly empty) set of
morphological features, such as PRS (the present
tense) and AKT (the active voice), etc. There are
25 different part of speech tags. Thus, many of
the 146 tags represent different inflected forms.
Examples of the tags are found in &apos;able 1. The
SUC tagging scheme is presented in Ejerhed et
al. (1992).
</bodyText>
<sectionHeader confidence="0.987944" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999991257142857">
Two previous studies on the induction of rules
for part of speech tagging are presented in this
section.
Samuelsson et al. (1996) describe experi-
ments of inducing English CG rules, intended
more as a help for the grammarian, rather than
as an attempt to induce a full-scale CG. The
training corpus consisted of some 55 000 words
of English text, morphologically and syntacti-
cally tagged according to the EngCG tagset.
Constraints of the form presented in Sec-
tion 1.1 were induced based on bigram statistics.
Also lexical rules, discarding unlikely readings
for certain word forms, were induced. In addi-
tion to these, &apos;barrier&apos; rules were learnt. While
the induced &apos;remove&apos; rules were based on bi-
grams, the barrier rules utilized longer contexts.
When tested on a 10 000 word test corpus, the
recall of the induced grammar was 98.2% with
a precision of 87.3%, which means that some of
the ambiguities were left pending after tagging
(1.12 readings per word).
Cussens (1997) describes a project in which
CG inspired rules for tagging English text were
induced using the Progol machine-learning sys-
tem. To its help the Progol system had a small
hand-crafted syntactic grammar. The grammar
was used as background knowledge to the Pro-
gol system only, and was not used for producing
any syntactic structure in the final output. The
examples consisted of the tags of all of the words
on each side of the word to be disambiguated
(the target word). Given no unknown words
and a tag set of 43 different tags, the system
tagged 96.4% of the words correctly.
</bodyText>
<sectionHeader confidence="0.998534" genericHeader="method">
3 Present work
</sectionHeader>
<bodyText confidence="0.9999676">
The current work was inspired by Cussens
(1997) as well as Samuelsson et al. (1996), but
departs from both in several respects. It also
follows up an initial experiment conducted by
the current authors (Eineborg and Lindberg,
</bodyText>
<page confidence="0.99188">
776
</page>
<bodyText confidence="0.999400029411765">
1998).
Following Samuelsson et al. (1996) local-
context and lexical rules were induced. In the
present work, no barrier rules were induced. In
contrast to their study, a TWOL lexicon and an
annotated training text using the same tagset
were not available. Instead, a lexicon was cre-
ated from the training corpus.
Just as in Cussens work, Progol was used
to induce tag elimination rules from an anno-
tated corpus. In contrast to his study, no gram-
matical background knowledge is given to the
learner and also word tokens, and not only part
of speech tags, are in the training data.
In order to induce the new rules, the context
has been limited to a window of maximally five
words, with the target word to disambiguate in
the middle. A motivation for using a rather
small window size can be found in Karlsson et
al. (1995, page 59) where it is pointed out that
sensible constraints referring to a position rel-
ative to the target word utilize close context,
typically 1-3 words.
Some further restrictions on how the learn-
ing system may use the information in the win-
dow have been applied in order to reduce the
complexity of the problem. This is described in
Section 3.2.
A pre-release of the Stockholm-Umea Corpus
was used. Some 10% of the corpus was put aside
to be used as test data, and the rest of the cor-
pus made up the training data. The test data
files were evenly distributed over the different
text genres.
</bodyText>
<subsectionHeader confidence="0.998722">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999983363636363">
Before starting the learning of constraints, the
training data was preprocessed in different
ways. Following Cussens (1997), a lexicon was
produced from the training corpus. All different
word forms in the corpus were represented in the
lexicon by one look-up word and an ambiguity
class, the set of different tags which occurred
in the corpus for the word form. The lexicon
ended up just over 86 000 entries big.
Similar to Karlsson et al. (1995), the first
step of the tagging process was to identify &apos;id-
ioms&apos;, although the term is used somewhat dif-
ferently in this study; bi- and trigrams which
were always tagged with one specific tag se-
quence (unambiguously tagged, i.e.) were ex-
tracted from the training text. Example &apos;id-
ioms&apos; are given in Table 1. 1 530 such bi- and
trigrams were used.
Following Samuelsson et al. (1996), a list of
very unlikely readings for certain words was pro-
duced (&apos;lexical rules&apos;). For a word form plus tag
to qualify as a lexical rule, the word form should
have a frequency of at least 100 occurrences in
the training data, and the word should occur
with the tag to discard in no more than 1% of
the cases. 355 lexical rules were produced this
way. The role of lexical rules and &apos;idioms&apos; is to
remove the simple cases of ambiguities, making
it possible for the induced rules to fire, since
these rules are all &apos;careful&apos;, meaning that they
can refer to unambiguous contexts only (if they
refer to tag features, and not word forms only,
i.e.).
</bodyText>
<subsectionHeader confidence="0.997829">
3.2 Rule induction
</subsectionHeader>
<bodyText confidence="0.999780121212121">
Rules were induced for all part of speech cat-
egories. Allowing the rules to refer to spe-
cific morphological features (and not necessar-
ily a complete specification) has increased the
expressive power of the rules, compared to
the initial experiments (Eineborg and Lindberg,
1998). The rules can look at word form, part of
speech, morphological features, and whether a
word has an upper or lower case initial charac-
ter. Although we used a window of size 5, the
rules can look at maximally four positions at
the same time within the window. Another re-
striction has been put on which combination of
features the system may select from a context
word. The closer a context word is to the target
the more features it may use. This is done in
order to reduce the search space. Each context
word is represented as a prolog term with argu-
ments for word form, upper/lower case charac-
ter and part of speech tag along with a set of
morphological features (if any).
A different set of training data was produced
for each of the 24 part speech categories. The
training data was pre-processed by applying the
bi- and trigrams and the lexical rules, described
above (Section 3.1). This step was taken in or-
der to reduce the amount of training data —
rules should not be learnt for ambiguities which
would be taken care of anyway.
Progol is able to induce a hypothesis using
only positive examples, or using both positive
and negative examples. Since we are inducing
tag eliminating rules, an example is considered
</bodyText>
<page confidence="0.986657">
777
</page>
<figure confidence="0.559769142857143">
BI- AND TRIGRAMS
ett par
det ix
i samband med
pi grund av
• • •
POS READINGS (UNAMBIGUOUS TAG SEQUENCE)
</figure>
<note confidence="0.8278996">
ett/DT NEU SIN IND par/NN NEU SIN IND NOM
det/PN NEU SIN DEF SUB/OBJ är/VB PRS AKT
i/PP samband/NN NEU SIN IND NOM med/PP
pi/PP grund/NN 11Th SIN IND NOM av/PP
• • •
</note>
<tableCaption confidence="0.99558">
Table 1: &apos;Idioms&apos;. Unambiguous word sequences found in the training data.
</tableCaption>
<bodyText confidence="0.999410615384615">
positive when a word is incorrectly tagged and
the reading should be discarded. A negative
example is a correctly tagged word where the
reading should be retained. The training data
for each part of speech tag consisted of between
4000 and 6000 positive examples with an equiv-
alent number of negative examples. The exam-
ples for each part of speech category were ran-
domly drawn from all examples available in the
training data.
A noise level of 1% was tolerated to make sure
that Progol could find important rules despite
the fact that some examples could be incorrect.
</bodyText>
<subsectionHeader confidence="0.998895">
3.3 Rule format
</subsectionHeader>
<bodyText confidence="0.999918333333333">
The induced rules code two types of informa-
tion: Firstly, the rules state the number and
positions of the context words relative to the
target word (the word to disambiguate). Sec-
ondly, for each context word referred to by a
rule, and possibly also for the target word, the
rule states under what conditions the rule is
applicable. These conditions can be the word
form, morphological features or whether a word
is spellt with an initial capital letter or not, and
combinations of these things. Examples of in-
duced rules are
</bodyText>
<equation confidence="0.619976666666667">
remove(vb,A) :-
constr(A,left(feats([dt]))).
remove(ie,A) :-
</equation>
<bodyText confidence="0.9216639">
constr(A,right_right(featsadef]),
feats([vb]))).
remove(vb, A) :-
context(A,left_target(word(att),
featlist([imp,akt]))).
where the first rule eliminates all verbal (vb)
readings of a word immediately preceded by a
word tagged as determiner (dt). The second
rule deletes the infinitive marker (ie) reading
of a word followed by any word which has the
feature &apos;definite&apos; (clef), followed by a verb (vb).
The third rule deletes verb tags which have the
features &apos;imperative&apos; (imp) and &apos;active voice&apos;
(akt) if the preceding word is att (vord(att)).
As alredy been mentioned, the scope of the
rules has been limited to a window of five words,
the target word included. In an earlier attempt,
the window was seven words, but these rules
were less expressive in other respects (Eineborg
and Lindberg, 1998).
</bodyText>
<sectionHeader confidence="0.999891" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99996629032258">
Just under 7 000 rules were induced. The tagger
was tested on a subset of the unseen data. Only
sentences in which all words were in the lexicon
were allowed. Sentences including words tagged
as UO were discarded. The U0 tag is a peculiarity
of the SUC tagset, and conveys no grammatical
information; it stands for &apos;foreign word&apos; and is
used e.g. for the words in passages quoting text
which is not in Swedish.
The test data consisted of 42 925 words, in-
cluding punctuation marks. After lexicon look-
up the words were assigned 93 810 readings,
i.e., on average 2.19 readings per word. 41 926
words retained the correct reading after disam-
biguation, which means that the correct tag sur-
vived for 97.7% of the words. After tagging,
there were 48 691 readings left, 1.13 readings
per word.
As a comparison to these results, a prelim-
inary test of the Brill tagger also trained on
the Stockholm-Umea Corpus, tagged 96.9% of
the words correctly, and Oliver Mason&apos;s QTag
got 96.3% on the same data (Ridings, 1998).
Neither of these two taggers leave ambigui-
ties pending and both handles unknown words,
which makes a direct comparison of the figures
given above hard.
The processing times were quite long for most
of the rule sets — few of them were actually
allowed to continue until all examples were ex-
hausted.
</bodyText>
<sectionHeader confidence="0.991448" genericHeader="discussions">
5 Discussion and future work
</sectionHeader>
<bodyText confidence="0.9790695">
The figures of the experimental tagger are not
optimal, but promising, considering that the
</bodyText>
<page confidence="0.992728">
778
</page>
<bodyText confidence="0.999774863636364">
rules induced is a limited subset of possible rule
types.
Part of the explanation for the figure of am-
biguities pending after tagging is that there are
some ambiguity classes which are very hard to
deal with. For example, there is a tag for the ad-
verb, AB, and one tag for the verbal particle, PL.
In the lexicon built from the corpus, there are 83
word forms which can have at least both these
readings. Thus, turning a corpus into a lexicon
might lead to the introduction of ambiguities
hard to solve. A lexicon better tailored to the
task would be of much use. Another important
issue is that of handling unknown words.
To reduce the error rate, the bad rules should
be identified by testing all rules against the
training data. To tackle the residual ambigu-
ities, the next step will be to learn also different
kinds of rules, for example &apos;select&apos; rules which
retain a given reading, but discard all others.
Also rules scoping longer contexts than a win-
dow of 5-7 words must be considered.
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999823583333333">
Using the Progol ILP system, some 7 000
tag eliminating rules were induced from the
Stockholm-Umea Corpus. A lexicon was built
from the corpus, and after lexicon look-up, test
data (including only known words) was disam-
biguated with the help of the induced rules. Of
42 925 known words, 41 926 (98%) retained the
correct reading after disambiguation. Some am-
biguities remained in output: on an average 1.13
readings per word. Considering the experimen-
tal status of the tagger, we find the results en-
couraging.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8507164">
Britt Hartmann (Stockholm University) an-
swered many corpus related questions. Henrik
Bostrom (Stockholm University/Royal Institute
of Technology) helped us untangle a few ILP
mysteries.
</bodyText>
<sectionHeader confidence="0.996231" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998956238095238">
Eric Brill. 1994. Some advances in
transformation-based part of speech tagging.
In Proceedings of the Twelfth National Con-
ference on Artificial Intelligence (AAAI-94).
James Cussens. 1997. Part of speech tagging
using Progol. In Proceedings of the 7th Inter-
national Workshop on Inductive Logic Pro-
gramming (ILP-97), pages 93-108.
Martin Eineborg and Nikolaj Lindberg. 1998.
Induction of Constraint Grammar-rules using
Progol. In Proceedings of The Eighth Inter-
national Conference on Inductive Logic Pro-
gramming (ILP &apos;98), Madison, Wisconsin.
Eva Ejerhed, Gunnel Kallgren, Wennstedt Ola,
and Magnus Astrom. 1992. The Linguistic
Annotation System of the Stockholm-Umed
Project. Department of General Linguistics,
University of Umea.
Fred Karlsson, Atro Voutilainen, Juha Heikkila,
and Arto Anttila, editors. 1995. Constraint
Grammar: A language-independent system
for parsing unrestricted text. Mouton de
Gruyter, Berlin and New York.
Oliver Manson, 1997. QTAG—A portable prob-
abilistic tagger. Corpus Research, The Uni-
versity of Birmingham, U.K.
Stephen Muggleton. 1995. Inverse entailment
and Progol. New Generation Computing
Journal, 13:245-286.
Daniel Ridings. 1998. SUC and the Brill tagger.
GU-ISS-98-1 (Research Reports from the De-
partment of Swedish, Goteborg University).
Sam Roberts, 1997. An introduction to Progol.
Christer Samuelsson, Pasi Tapanainen, and
Atro Voutilainen. 1996. Inducing Con-
straint Grammars. In Miclet Laurent and
de la Higuera Colin, editors, Grammatical
Inference: Learning Syntax from Sentences,
pages 146-155. Springer Verlag.
Pasi Tapanainen. 1996. The Constraint Gram-
mar Parser CG-2. Department of General
Linguistics, University of Helsinki.
</reference>
<page confidence="0.998661">
779
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605914">
<title confidence="0.9995455">Learning Constraint Grammar-style disambiguation rules using Inductive Logic Programming</title>
<author confidence="0.997532">Nikolaj Lindberg</author>
<affiliation confidence="0.9998285">Centre for Speech Technology Royal Institute of Technology</affiliation>
<address confidence="0.998122">SE-100 44 Stockholm, Sweden</address>
<email confidence="0.916456">nikolajOspeech.kth.se</email>
<author confidence="0.998019">Martin Eineborg</author>
<affiliation confidence="0.908428">Telia Research AB Spoken Language Processing</affiliation>
<address confidence="0.966465">SE-136 80 Haninge, Sweden</address>
<abstract confidence="0.987573">This paper reports a pilot study, in which Constraint Grammar inspired rules were learnt using the Progol machine-learning system. Rules discarding faulty readings of ambiguously tagged words were learnt for the part of speech tags of the Stockholm-UmeS, Corpus. Several thousand disambiguation rules were induced. When tested on unseen data, 98% of the words retained the correct reading after tagging. However, there were ambiguities pending after tagging, on an average 1.13 tags per word. The results suggest that the Progol system can be useful for learning tagging rules of good quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in transformation-based part of speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94).</booktitle>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some advances in transformation-based part of speech tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Cussens</author>
</authors>
<title>Part of speech tagging using Progol.</title>
<date>1997</date>
<booktitle>In Proceedings of the 7th International Workshop on Inductive Logic Programming (ILP-97),</booktitle>
<pages>93--108</pages>
<contexts>
<context position="6391" citStr="Cussens (1997)" startWordPosition="1059" endWordPosition="1060">lly and syntactically tagged according to the EngCG tagset. Constraints of the form presented in Section 1.1 were induced based on bigram statistics. Also lexical rules, discarding unlikely readings for certain word forms, were induced. In addition to these, &apos;barrier&apos; rules were learnt. While the induced &apos;remove&apos; rules were based on bigrams, the barrier rules utilized longer contexts. When tested on a 10 000 word test corpus, the recall of the induced grammar was 98.2% with a precision of 87.3%, which means that some of the ambiguities were left pending after tagging (1.12 readings per word). Cussens (1997) describes a project in which CG inspired rules for tagging English text were induced using the Progol machine-learning system. To its help the Progol system had a small hand-crafted syntactic grammar. The grammar was used as background knowledge to the Progol system only, and was not used for producing any syntactic structure in the final output. The examples consisted of the tags of all of the words on each side of the word to be disambiguated (the target word). Given no unknown words and a tag set of 43 different tags, the system tagged 96.4% of the words correctly. 3 Present work The curre</context>
<context position="8766" citStr="Cussens (1997)" startWordPosition="1473" endWordPosition="1474">lose context, typically 1-3 words. Some further restrictions on how the learning system may use the information in the window have been applied in order to reduce the complexity of the problem. This is described in Section 3.2. A pre-release of the Stockholm-Umea Corpus was used. Some 10% of the corpus was put aside to be used as test data, and the rest of the corpus made up the training data. The test data files were evenly distributed over the different text genres. 3.1 Preprocessing Before starting the learning of constraints, the training data was preprocessed in different ways. Following Cussens (1997), a lexicon was produced from the training corpus. All different word forms in the corpus were represented in the lexicon by one look-up word and an ambiguity class, the set of different tags which occurred in the corpus for the word form. The lexicon ended up just over 86 000 entries big. Similar to Karlsson et al. (1995), the first step of the tagging process was to identify &apos;idioms&apos;, although the term is used somewhat differently in this study; bi- and trigrams which were always tagged with one specific tag sequence (unambiguously tagged, i.e.) were extracted from the training text. Example</context>
</contexts>
<marker>Cussens, 1997</marker>
<rawString>James Cussens. 1997. Part of speech tagging using Progol. In Proceedings of the 7th International Workshop on Inductive Logic Programming (ILP-97), pages 93-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Eineborg</author>
<author>Nikolaj Lindberg</author>
</authors>
<title>Induction of Constraint Grammar-rules using Progol.</title>
<date>1998</date>
<booktitle>In Proceedings of The Eighth International Conference on Inductive Logic Programming (ILP &apos;98),</booktitle>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="10414" citStr="Eineborg and Lindberg, 1998" startWordPosition="1761" endWordPosition="1764">lexical rules were produced this way. The role of lexical rules and &apos;idioms&apos; is to remove the simple cases of ambiguities, making it possible for the induced rules to fire, since these rules are all &apos;careful&apos;, meaning that they can refer to unambiguous contexts only (if they refer to tag features, and not word forms only, i.e.). 3.2 Rule induction Rules were induced for all part of speech categories. Allowing the rules to refer to specific morphological features (and not necessarily a complete specification) has increased the expressive power of the rules, compared to the initial experiments (Eineborg and Lindberg, 1998). The rules can look at word form, part of speech, morphological features, and whether a word has an upper or lower case initial character. Although we used a window of size 5, the rules can look at maximally four positions at the same time within the window. Another restriction has been put on which combination of features the system may select from a context word. The closer a context word is to the target the more features it may use. This is done in order to reduce the search space. Each context word is represented as a prolog term with arguments for word form, upper/lower case character a</context>
<context position="13953" citStr="Eineborg and Lindberg, 1998" startWordPosition="2365" endWordPosition="2368">adings of a word immediately preceded by a word tagged as determiner (dt). The second rule deletes the infinitive marker (ie) reading of a word followed by any word which has the feature &apos;definite&apos; (clef), followed by a verb (vb). The third rule deletes verb tags which have the features &apos;imperative&apos; (imp) and &apos;active voice&apos; (akt) if the preceding word is att (vord(att)). As alredy been mentioned, the scope of the rules has been limited to a window of five words, the target word included. In an earlier attempt, the window was seven words, but these rules were less expressive in other respects (Eineborg and Lindberg, 1998). 4 Results Just under 7 000 rules were induced. The tagger was tested on a subset of the unseen data. Only sentences in which all words were in the lexicon were allowed. Sentences including words tagged as UO were discarded. The U0 tag is a peculiarity of the SUC tagset, and conveys no grammatical information; it stands for &apos;foreign word&apos; and is used e.g. for the words in passages quoting text which is not in Swedish. The test data consisted of 42 925 words, including punctuation marks. After lexicon lookup the words were assigned 93 810 readings, i.e., on average 2.19 readings per word. 41 9</context>
</contexts>
<marker>Eineborg, Lindberg, 1998</marker>
<rawString>Martin Eineborg and Nikolaj Lindberg. 1998. Induction of Constraint Grammar-rules using Progol. In Proceedings of The Eighth International Conference on Inductive Logic Programming (ILP &apos;98), Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Ejerhed</author>
<author>Gunnel Kallgren</author>
<author>Wennstedt Ola</author>
<author>Magnus Astrom</author>
</authors>
<date>1992</date>
<booktitle>The Linguistic Annotation System of the Stockholm-Umed Project.</booktitle>
<institution>Department of General Linguistics, University of Umea.</institution>
<contexts>
<context position="5401" citStr="Ejerhed et al. (1992)" startWordPosition="892" endWordPosition="895">art of speech tagged Swedish text, sampled from different text genres (largely following the Brown corpus text categories). The first official release is now available on CD-ROM. The SUC tagset has 146 different tags, and the tags consist of a part of speech tag, e.g. VB (the verb) followed by a (possibly empty) set of morphological features, such as PRS (the present tense) and AKT (the active voice), etc. There are 25 different part of speech tags. Thus, many of the 146 tags represent different inflected forms. Examples of the tags are found in &apos;able 1. The SUC tagging scheme is presented in Ejerhed et al. (1992). 2 Previous work Two previous studies on the induction of rules for part of speech tagging are presented in this section. Samuelsson et al. (1996) describe experiments of inducing English CG rules, intended more as a help for the grammarian, rather than as an attempt to induce a full-scale CG. The training corpus consisted of some 55 000 words of English text, morphologically and syntactically tagged according to the EngCG tagset. Constraints of the form presented in Section 1.1 were induced based on bigram statistics. Also lexical rules, discarding unlikely readings for certain word forms, w</context>
</contexts>
<marker>Ejerhed, Kallgren, Ola, Astrom, 1992</marker>
<rawString>Eva Ejerhed, Gunnel Kallgren, Wennstedt Ola, and Magnus Astrom. 1992. The Linguistic Annotation System of the Stockholm-Umed Project. Department of General Linguistics, University of Umea.</rawString>
</citation>
<citation valid="true">
<title>Constraint Grammar: A language-independent system for parsing unrestricted text. Mouton de Gruyter,</title>
<date>1995</date>
<editor>Fred Karlsson, Atro Voutilainen, Juha Heikkila, and Arto Anttila, editors.</editor>
<location>Berlin and New York.</location>
<contexts>
<context position="4565" citStr="(1995)" startWordPosition="753" endWordPosition="753">version of the clause (with respect to the background knowledge and mode declarations, see below) that explains as many positive and as few negative examples as possible. Mode declarations specifying the properties of the rules have to be given by the user. A modeh declaration specifies the head of the rules, while modeb declarations specify what the bodies of the rules to induce might contain. The user also declares the types of arguments, and whether they are input or output arguments, or if an argument should be instantiated by Progol. Progol is freely available and documented in Muggleton (1995) and Roberts (1997). 1.3 The Stockholm-Umea Corpus The training material in the experiments reported here is sampled from a pre-release of the Stockholm-Umei Corpus (SUC). SUC covers just over one million words of part of speech tagged Swedish text, sampled from different text genres (largely following the Brown corpus text categories). The first official release is now available on CD-ROM. The SUC tagset has 146 different tags, and the tags consist of a part of speech tag, e.g. VB (the verb) followed by a (possibly empty) set of morphological features, such as PRS (the present tense) and AKT </context>
<context position="9090" citStr="(1995)" startWordPosition="1532" endWordPosition="1532">test data, and the rest of the corpus made up the training data. The test data files were evenly distributed over the different text genres. 3.1 Preprocessing Before starting the learning of constraints, the training data was preprocessed in different ways. Following Cussens (1997), a lexicon was produced from the training corpus. All different word forms in the corpus were represented in the lexicon by one look-up word and an ambiguity class, the set of different tags which occurred in the corpus for the word form. The lexicon ended up just over 86 000 entries big. Similar to Karlsson et al. (1995), the first step of the tagging process was to identify &apos;idioms&apos;, although the term is used somewhat differently in this study; bi- and trigrams which were always tagged with one specific tag sequence (unambiguously tagged, i.e.) were extracted from the training text. Example &apos;idioms&apos; are given in Table 1. 1 530 such bi- and trigrams were used. Following Samuelsson et al. (1996), a list of very unlikely readings for certain words was produced (&apos;lexical rules&apos;). For a word form plus tag to qualify as a lexical rule, the word form should have a frequency of at least 100 occurrences in the traini</context>
</contexts>
<marker>1995</marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila, and Arto Anttila, editors. 1995. Constraint Grammar: A language-independent system for parsing unrestricted text. Mouton de Gruyter, Berlin and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Manson</author>
</authors>
<title>QTAG—A portable probabilistic tagger.</title>
<date>1997</date>
<institution>Corpus Research, The University of Birmingham, U.K.</institution>
<marker>Manson, 1997</marker>
<rawString>Oliver Manson, 1997. QTAG—A portable probabilistic tagger. Corpus Research, The University of Birmingham, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Muggleton</author>
</authors>
<title>Inverse entailment and Progol.</title>
<date>1995</date>
<journal>New Generation Computing Journal,</journal>
<pages>13--245</pages>
<contexts>
<context position="2998" citStr="Muggleton, 1995" startWordPosition="475" endWordPosition="476">full-scale CG has hundreds of rules. The developers of English CG report that 99.7% of the words retain their correct reading, and that 93-97% of the words are unambiguous after tagging (Karlsson et al., 1995, page 186). A parser applying the constraints is described in Tapanainen (1996). 1.2 Inductive Logic Programming Inductive Logic Programming (ILP) is a combination of machine learning and logic programming, where the goal is to find a hypothesis, H, given examples, E, and background knowledge, B, such that the hypothesis along with the background knowledge logically implies the examples (Muggleton, 1995, page 2): BAH =E The examples are usually split into a positive, E+, and a negative, E-, subset. The ILP system used in this paper, CProgol Version 4.2, uses Horn clauses as the representational language. Progol creates, for each E+, a most specific clause _Li and then searches through the lattice of hypotheses, from specific 775 to more general, bounded by H to find the clause that maximally compresses the data where -&lt; (0-subsumption) is defined as ci c2 -1=&gt; 30 : c10 C c2 and 0 is the empty clause. As an example, consider the two clauses: : p(X,Y) ,Y) C2: p(a,b) q(a,b) , r(Z) where Cl c2 u</context>
<context position="4565" citStr="Muggleton (1995)" startWordPosition="752" endWordPosition="753">e general version of the clause (with respect to the background knowledge and mode declarations, see below) that explains as many positive and as few negative examples as possible. Mode declarations specifying the properties of the rules have to be given by the user. A modeh declaration specifies the head of the rules, while modeb declarations specify what the bodies of the rules to induce might contain. The user also declares the types of arguments, and whether they are input or output arguments, or if an argument should be instantiated by Progol. Progol is freely available and documented in Muggleton (1995) and Roberts (1997). 1.3 The Stockholm-Umea Corpus The training material in the experiments reported here is sampled from a pre-release of the Stockholm-Umei Corpus (SUC). SUC covers just over one million words of part of speech tagged Swedish text, sampled from different text genres (largely following the Brown corpus text categories). The first official release is now available on CD-ROM. The SUC tagset has 146 different tags, and the tags consist of a part of speech tag, e.g. VB (the verb) followed by a (possibly empty) set of morphological features, such as PRS (the present tense) and AKT </context>
</contexts>
<marker>Muggleton, 1995</marker>
<rawString>Stephen Muggleton. 1995. Inverse entailment and Progol. New Generation Computing Journal, 13:245-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ridings</author>
</authors>
<title>SUC and the Brill tagger. GU-ISS-98-1 (Research Reports from the</title>
<date>1998</date>
<institution>Department of Swedish, Goteborg University). Sam Roberts,</institution>
<contexts>
<context position="14970" citStr="Ridings, 1998" startWordPosition="2546" endWordPosition="2547"> is not in Swedish. The test data consisted of 42 925 words, including punctuation marks. After lexicon lookup the words were assigned 93 810 readings, i.e., on average 2.19 readings per word. 41 926 words retained the correct reading after disambiguation, which means that the correct tag survived for 97.7% of the words. After tagging, there were 48 691 readings left, 1.13 readings per word. As a comparison to these results, a preliminary test of the Brill tagger also trained on the Stockholm-Umea Corpus, tagged 96.9% of the words correctly, and Oliver Mason&apos;s QTag got 96.3% on the same data (Ridings, 1998). Neither of these two taggers leave ambiguities pending and both handles unknown words, which makes a direct comparison of the figures given above hard. The processing times were quite long for most of the rule sets — few of them were actually allowed to continue until all examples were exhausted. 5 Discussion and future work The figures of the experimental tagger are not optimal, but promising, considering that the 778 rules induced is a limited subset of possible rule types. Part of the explanation for the figure of ambiguities pending after tagging is that there are some ambiguity classes </context>
</contexts>
<marker>Ridings, 1998</marker>
<rawString>Daniel Ridings. 1998. SUC and the Brill tagger. GU-ISS-98-1 (Research Reports from the Department of Swedish, Goteborg University). Sam Roberts, 1997. An introduction to Progol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Pasi Tapanainen, and Atro Voutilainen.</title>
<date>1996</date>
<booktitle>In Miclet Laurent and de la Higuera Colin, editors, Grammatical Inference: Learning Syntax from Sentences,</booktitle>
<pages>146--155</pages>
<publisher>Springer Verlag.</publisher>
<marker>Samuelsson, 1996</marker>
<rawString>Christer Samuelsson, Pasi Tapanainen, and Atro Voutilainen. 1996. Inducing Constraint Grammars. In Miclet Laurent and de la Higuera Colin, editors, Grammatical Inference: Learning Syntax from Sentences, pages 146-155. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
</authors>
<title>The Constraint Grammar Parser CG-2.</title>
<date>1996</date>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<contexts>
<context position="2090" citStr="Tapanainen, 1996" startWordPosition="326" endWordPosition="327">tag. There were still ambiguities left in the output, on an average 1.13 readings per word. In the following sections, the CG framework and the Progol machine learning system will be presented very briefly. 1.1 Constraint Grammar POS tagging Constraint Grammar is a system for part of speech tagging and (shallow) syntactic dependency analysis of unrestricted text. In the following, only the part of speech tagging step will be discussed. The following as a typical `reductionistic&apos; example of a CG rule which discards a verbal reading of a word following a word unambiguously tagged as determiner (Tapanainen, 1996, page 12): REMOVE (V) IF (-1C DET) ; where V is the target tag to be discarded and -1C DET denotes the word immediately to the left (-1), unambiguously (C) tagged as determiner (DET). There are several types of rules, not only `reductionistic&apos; ones, making the CG formalism quite powerful. A full-scale CG has hundreds of rules. The developers of English CG report that 99.7% of the words retain their correct reading, and that 93-97% of the words are unambiguous after tagging (Karlsson et al., 1995, page 186). A parser applying the constraints is described in Tapanainen (1996). 1.2 Inductive Log</context>
</contexts>
<marker>Tapanainen, 1996</marker>
<rawString>Pasi Tapanainen. 1996. The Constraint Grammar Parser CG-2. Department of General Linguistics, University of Helsinki.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>