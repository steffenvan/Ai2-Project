<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000831">
<title confidence="0.999455">
A Discriminative Approach for Dependency Based
Statistical Machine Translation
</title>
<author confidence="0.534402">
Sriram Venkatapathy Rajeev Sangal
</author>
<affiliation confidence="0.314092">
LTRC, IIIT-Hyderabad LTRC, IIIT-Hyderabad
</affiliation>
<email confidence="0.952954">
sriram@research.iiit.ac.in sangal@mail.iiit.ac.in
</email>
<author confidence="0.997409">
Aravind Joshi Karthik Gali1
</author>
<affiliation confidence="0.998387">
University of Pennsylvania Talentica
</affiliation>
<email confidence="0.999299">
joshi@seas.upenn.edu karthik.gali@gmail.com
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970615384615">
In this paper, we propose a dependency
based statistical system that uses discrim-
inative techniques to train its parameters.
We conducted experiments on an English-
Hindi parallel corpora. The use of syntax
(dependency tree) allows us to address the
large word-reorderings between English
and Hindi. And, discriminative training
allows us to use rich feature sets, includ-
ing linguistic features that are useful in the
machine translation task. We present re-
sults of the experimental implementation
of the system in this paper.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999013642857143">
Syntax based approaches for Machine Translation
(MT) have gained popularity in recent times be-
cause of their ability to handle long distance re-
orderings (Wu, 1997; Yamada and Knight, 2002;
Quirk et al., 2005; Chiang, 2005), especially for
divergent language pairs such as English-Hindi
(or English-Urdu). Languages such as Hindi are
also known for their rich morphology and long
distance agreement of features of syntactically re-
lated units. The morphological richness can be
handled by employing techniques that factor the
lexical items into morphological factors. This
strategy is also useful in the context of English-
Hindi MT (Bharati et al., 1997; Bharati et al.,
</bodyText>
<footnote confidence="0.545172">
1This work was done at LTRC, IIIT-Hyderabad, when he
was a masters student, till July 2008
</footnote>
<bodyText confidence="0.998635151515151">
2002; Ananthakrishnan et al., 2008; Ramanathan
et al., 2009) where there is very limited paral-
lel corpora available, and breaking words into
smaller units helps in reducing sparsity. In or-
der to handle phenomenon such as long-distance
word agreement to achieve accurate generation of
target language words, the inter-dependence be-
tween the factors of syntactically related words
need to be modelled effectively.
Some of the limitations with the syntax based
approaches such as (Yamada and Knight, 2002;
Quirk et al., 2005; Chiang, 2005) are, (1) They
do not offer flexibility for adding linguistically
motivated features, and (2) It is not possible to
use morphological factors in the syntax based ap-
proaches. In a recent work (Shen et al., 2009), lin-
guistic and contextual information was effectively
used in the framework of a hierarchical machine
translation system. In their work, four linguistic
and contextual features are used for accurate se-
lection of translation rules. In our approach in
contrast, linguistically motivated features can be
defined that directly effect the prediction of var-
ious elements in the target during the translation
process. This features use syntactic labels and col-
location statistics in order to allow effective train-
ing of the model.
Some of the other approaches related to our
model are the Direct Translation Model 2 (DTM2)
(Ittycheriah and Roukos, 2007), End-to-End Dis-
criminative Approach to MT (Liang et al., 2006)
and Factored Translation Models (Koehn and
Hoang, 2007). In DTM2, a discriminative trans-
</bodyText>
<page confidence="0.918546">
66
</page>
<note confidence="0.9817535">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66–74,
COLING 2010, Beijing, August 2010.
</note>
<bodyText confidence="0.9997756">
lation model is defined in the setting of a phrase
based translation system. In their approach, the
features are optimized globally. In contrast to
their approach, we define a discriminative model
for translation in the setting of a syntax based ma-
chine translation system. This allows us to use
both the power of a syntax based approach, as
well as, the power of a large feature space during
translation. In our approach, the weights are op-
timized in order to achieve an accurate prediction
of the individual target nodes, and their relative
positions.
We propose an approach for syntax based sta-
tistical machine translation which models the fol-
lowing aspects of language divergence effectively.
</bodyText>
<listItem confidence="0.9941966">
• Word-order variation including long-
distance reordering which is prevalent
between language pairs such as English-
Hindi and English-Japanese.
• Generation of word-forms in the target lan-
guage by predicting the word and its factors.
During prediction, the inter-dependence of
factors of the target word form with the fac-
tors of syntactically related words is consid-
ered.
</listItem>
<bodyText confidence="0.998667277777778">
To accomplish this goal, we visualize the prob-
lem of MT as transformation from a morpho-
logically analyzed source syntactic structure to a
target syntactic structure1 (See Figure 1). The
transformation is factorized into a series of mini-
transformations, which we address as features of
the transformation. The features denote the vari-
ous linguistic modifications in the source structure
to obtain the target syntactic structure. Some of
the examples of features are lexical translation of
a particular source node, the ordering at a particu-
lar source node etc. These features can be entirely
local to a particular node in the syntactic structure
or can span across syntactically related entities.
More about the features (or mini-transformations)
is explained in section 3. The transformation of
a source syntactic structure is scored by taking a
weighted sum of its features 2. Let T represent
</bodyText>
<footnote confidence="0.996608">
1Note that target structure contains only the target fac-
tors. An accurate and deterministic morphological generator
combines these factors to produce the target word form.
2The features can be either binary-values or real-valued
</footnote>
<bodyText confidence="0.922078666666667">
the transformation of source syntactic structure s,
the score of transformation is computed as repre-
sented in Equation 1.
</bodyText>
<equation confidence="0.9963385">
�score(T|s) = wi ∗ fi(T, s) (1)
i
</equation>
<bodyText confidence="0.99993041025641">
In Equation 1, f�is are the various features of
transformation and w�is are the weights of the fea-
tures. The strength of our approach lies in the flex-
ibility it offers in incorporating linguistic features
that are useful in the task of machine translation.
These features are also known as prediction fea-
tures as they map from source language informa-
tion to information in the target language that is
being predicted.
During decoding a source sentence, the goal
is to choose a transformation that has the high-
est score. The source syntactic structure is tra-
versed in a bottom-up fashion and the target syn-
tactic structure is simultaneously built. We used
a bottom-up traversal while decoding because it
builds a contiguous sequence of nodes for the sub-
trees during traversal enabling the application of a
wide variety of language models.
In the training phase, the task is to learn the
weights of features. We use an online large-
margin training algorithm, MIRA (Crammer et
al., 2005), for learning the weights. The weights
are locally updated at every source node during
the bottom-up traversal of the source structure.
For training the translation model, automatically
obtained word-aligned parallel corpus is used. We
used GIZA++ (Och and Ney, 2003) along with the
growing heuristics to word-align the training cor-
pus.
The basic factors of the word used in our exper-
iments are root, part-of-speech, gender, number
and person. In Hindi, common nouns and verbs
have gender information whereas, English doesn’t
contain that information. Apart from the basic
factors, we also consider the role information pro-
vided by labelled dependency parsers. For com-
puting the dependency tree on the source side, We
used stanford parser (Klein and Manning, 2003)
in the experiments presented in this chapter3.
</bodyText>
<footnote confidence="0.991215">
3Stanford parser gives both the phrase-structure tree as
well as dependency relations for a sentence.
</footnote>
<page confidence="0.998265">
67
</page>
<figure confidence="0.9997395">
Ram/NNP
visit/NN
to/TO
a/DT
root=a, gnp=x3sg
role=nmod
Shyam/NNP
root=Shyam, gnp=x1sg
role=pmod
root=shyaam
gnp=m1sg
paid/VBD
root=pay, tense=PAST
gnp=x3sg, role=X
root=mila, tense=PAST
gnp=m3sg
root=Ram, gnp=x1sg
role=subj
root=visit, gnp=x3sg
role=obj
root=to, gnp=x3sg
role=vmod
root=raam
gnp=m1sg
root=se
gnp=x3sg
</figure>
<figureCaption confidence="0.999984">
Figure 1: Transformation from source structure to target language
</figureCaption>
<bodyText confidence="0.999987461538462">
The function words such as prepositions and
auxiliary verbs largely express the grammatical
roles/functions of the content words in the sen-
tence. In fact, in many agglutinative languages,
these words are commonly attached to the con-
tent word to form one word form. In this pa-
per, we also conduct experiments where we begin
by grouping the function words with their corre-
sponding function words. These groups of words
are called local-word groups. In these cases, the
function words are considered as factors of the
content words. Section 2 explains more about the
local word groups in English and Hindi.
</bodyText>
<sectionHeader confidence="0.963808" genericHeader="method">
2 Local Word Groups
</sectionHeader>
<bodyText confidence="0.999966761904762">
Local word groups (LWGs) (Bharati et al., 1998;
Vaidya et al., 2009) consist of a content word and
its associated function words. Local word group-
ing reduces a sentence to a sequence of content
words with the case-markers and tense-markers
acting as their factors. For example, consider
an English sentence ‘People of these island have
adopted Hindi as a means of communication’.
‘have adopted’ is a LWG with root ‘adopt’ and
tense markers being ‘have ed’. Another example
for the LWG will be ‘of communication’ where
‘communication’ is the root, and ‘of’ is the case-
marker. It is to be noted that Local word grouping
is different from chunking, where more than one
content word can be part of a chunk. We obtain lo-
cal word groups in English by processing the out-
put of the stanford parser. In Hindi, the function
words always appear immediately after the con-
tent word4, and it requires simple pattern
matching to obtain the LWGs. The rules ap-
plied are, (1) VM (RB VAUX)+, and (2) N.* IN.
</bodyText>
<sectionHeader confidence="0.99962" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999932666666667">
There are three types of transformation features
explored by us, (1) Local Features, (2) Syntactic
Features and, (3) Contextual Features. In this sec-
tion, we describe each of these categories of fea-
tures representing different aspects of transforma-
tion with examples.
</bodyText>
<subsectionHeader confidence="0.999379">
3.1 Local Features
</subsectionHeader>
<bodyText confidence="0.971118210526316">
The local features capture aspects of local trans-
formation of an atomic treelet in the source
structure to an atomic treelet in the target lan-
guage. Atomic treelet is a semantically non-
decomposible group of one or more nodes in the
syntactic structure. It usually contains only one
node, except for the case of multi-word expres-
sions (MWEs). Figure 2 presents the examples of
local transformation.
Some of the local features used by us in our ex-
periments are (1) dice coefficient, (2) dice coeffi-
cient of roots, (3) dice coefficient of null transla-
tions, (4) treelet translation probability, (5) gnp-
gnp pair, (5) preposition-postposition pair, (6)
tense-tense pair, (7) part-of-speech fertility etc.
Dice coefficients and treelet translation probabil-
ities are measures that express the statistical co-
occurrence of the atomic treelets.
4case-markers are called postpositions
</bodyText>
<page confidence="0.998211">
68
</page>
<figureCaption confidence="0.99795">
Figure 2: Local transformations
</figureCaption>
<subsectionHeader confidence="0.999763">
3.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999973461538462">
The syntactic features are used to model the differ-
ence in the word orders of the two languages. At
every node of the source syntactic structure, these
features define the changes in the relative order
of children during the process of transformation.
They heavily use source information such as part-
of-speech tags and syntactic roles of the source
nodes. One of the features used is reorderPostags.
This feature captures the change in relative po-
sitions of children with respect to their parents
during the tree transformation. An example fea-
ture for the transformation given in Figure 1 is
shown in Figure 3.
</bodyText>
<figureCaption confidence="0.989443">
Figure 3: Syntactic feature - reorder postags
</figureCaption>
<bodyText confidence="0.999940214285714">
The feature reorderPostags is in the form of a
complete transfer rule. To handle cases, where the
left-hand side of ‘reorderPostags’ does not match
the syntactic structure of the source tree, the sim-
pler feature functions are used to qualify various
reorderings. Instead of using POS tags, feature
functions can be defined that use syntactic roles.
Apart from the above feature functions, we can
also have features that compute the score of a par-
ticular order of children using syntactic language
models (Gali and Venkatapathy, 2009; Guo et al.,
2008). Different features can be defined that use
different levels of information pertaining to the
atomic treelet and its children.
</bodyText>
<subsectionHeader confidence="0.999556">
3.3 Contextual Features
</subsectionHeader>
<bodyText confidence="0.99992625">
Contextual features model the inter-dependence
of factors of nodes connected by dependency arcs.
These features are used to enable access to global
information for prediction of target nodes (words
and its factors).
One of the features diceCoeffParent, relates the
parent of a source node to the corresponding target
node (see figure 4.
</bodyText>
<figureCaption confidence="0.9265535">
Figure 4: Use of Contextual (parent) information
of x2 for generation of y
</figureCaption>
<bodyText confidence="0.99997805">
The use of this feature is expected to address of
the limitations of using ‘atomic treelets’ as the ba-
sic units in contrast to phrase based systems which
consider arbitrary sequences of words as units to
encode the local contextual information. In my
case, We relate the target treelet with the contex-
tual information of the source treelet using feature
functions rather than using larger units. Similar
features are used to connect the context of a source
node to the target node.
Various feature functions are defined to han-
dle interaction between the factors of syntacti-
cally related treelets. The gender-number-person
agreement is a factor that is dependent of gender-
number-person factors of the syntactically related
treelets in Hindi. The rules being learnt here
are simple. However, more complex interac-
tions can also be handled though features such as
prep Tense where, the case-marker in the target is
linked to the tense of parent verb.
</bodyText>
<sectionHeader confidence="0.99776" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999909428571429">
The goal is to compute the most probable target
sentence given a source sentence. First, the source
sentence is analyzed using a morphological ana-
lyzer5, local word grouper (see section 2) and a
dependency parser. Given the source structure,
the task of the decoding algorithm is to choose the
transformation that has the maximum score.
</bodyText>
<footnote confidence="0.66671">
5http://www.cis.upenn.edu/∼xtag/
</footnote>
<figure confidence="0.996177956521739">
root=mila, tense=PAST
gnp=m3sg
root=raam
gnp=m1sg
Ram/NNP
root=Ram, gnp=x1sg
role=subj
paid/VBD
root=pay, tense=PAST
gnp=x3sg, role=X
visit/NN
root=visit, gnp=x3sg
role=obj
VB
NNP
VB
NNP IN
TO
x1
x2
dice
y
x3 x4
</figure>
<page confidence="0.990263">
69
</page>
<bodyText confidence="0.999982176470588">
The dependency tree of the source language
sentence is traversed in a bottom-up fashion for
building the target language structure. At every
source node during the traversal, the local trans-
formation is first computed. Then, the relative or-
der of its children is then computed using the syn-
tactic features. This results in a target structure
associated with the subtree rooted at the particular
node. The target structure associated with the root
node of the source structure is the result of the best
transformation of the entire source structure.
Hence, the task of computing the best transfor-
mation of the entire source structure is factorized
into the tasks of computing the best transforma-
tions of the source treelets. The equation for com-
puting the score of a transformation, Equation 1,
can be modified as Equation 2 given below.
</bodyText>
<equation confidence="0.999617">
�score(τ|s) = �|r |∗ wZ ∗ fZ(τr, r) (2)
r Z
</equation>
<bodyText confidence="0.999171">
where, τj is the local transformation of the
source treelet r. The best transformation τˆ of
source sentence s is,
</bodyText>
<equation confidence="0.997925">
τˆ = argmaxτ score(τ|s) (3)
</equation>
<sectionHeader confidence="0.991307" genericHeader="method">
5 Training Algorithm
</sectionHeader>
<bodyText confidence="0.999812388888889">
The goal of the training algorithm is to learn the
feature weights from the word aligned corpus. For
word-alignment, we used the IBM Model 5 imple-
mented in GIZA++ along with the growing heuris-
tics (Koehn et al., 2003). The gold atomic treelets
in the source and their transformation is obtained
by mapping the source node to the target using the
word-alignment information. This information is
stored in the form of transformation tables that is
used for the prediction of target atomic treelets,
prepositions and other factors. The transformation
tables are pruned in order to limit the search and
eliminate redundant information. For each source
element, only the top few entries are retained in
the table. This limit ranges from 3 to 20.
We used an online-large margin algorithm,
MIRA (McDonald and Pereira, 2006; Crammer
et al., 2005), for updating the weights. During
parameter optimization, it is sometimes impossi-
ble to achieve the gold transformation for a node
because the pruned transformation tables may not
lead to the target gold prediction for the source
node. In such cases where the gold transforma-
tion is unreachable, the weights are not updated
at all for the source node as it might cause erro-
neous weight updates. We conducted our exper-
iments by considering both the cases, (1) Identi-
fying source nodes with unreachable transforma-
tions, and (2) Updating weights for all the source
nodes (till a maximum iteration limit). The num-
ber of iterations on the entire corpus can also be
fixed. Typically, two iterations have been found to
be sufficient to train the model.
The dependency tree is traversed in a bottom-up
fashion and the weights are updated at each source
node.
</bodyText>
<sectionHeader confidence="0.998395" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999994352941177">
The important aspects of the translation model
proposed in this paper have been implemented.
Some of the components that handle word in-
sertions and non-projective transformations have
not yet been implemented in the decoder, and
should be considered beyond the scope of this
paper. The focus of this work has been to
build a working syntax based statistical machine
translation system, which can act as a plat-
form for further experiments on similar lines.
The system would be available for download
at http://shakti.iiit.ac.in/∼sriram/vaanee.html. To
evaluate this experimental system, a restricted
set of experiments are conducted. The experi-
ments are conducted on the English-Hindi lan-
guage pair using a corpus in tourism domain con-
taining 11300 sentence pairs6.
</bodyText>
<subsectionHeader confidence="0.968663">
6.1 Training
6.1.1 Configuration
</subsectionHeader>
<bodyText confidence="0.999962">
For training, we used DIT-TOURISM-ALIGN-
TRAIN dataset which is the word-aligned dataset
of 11300 sentence pairs. The word-alignment is
done using GIZA++ (Och and Ney, 2003) toolkit
and then growing heuristics are applied. For
our experiments, we use two growing heuristics,
GROW-DIAG-FINAL-AND and GROW-DIAG-
FINAL as they cover most number of words in
both the sides of the parallel corpora.
</bodyText>
<footnote confidence="0.874136">
6DIT-TOURISM corpus
</footnote>
<page confidence="0.987772">
70
</page>
<table confidence="0.999507">
Number of Training Sentences 500
Iterations on Corpus 1-2
Parameter optimization algorithm MIRA
Beam Size 1-20
Maximum update attempts at source node 1-4
Unreachable updates False
Size of transformation tables 3
</table>
<tableCaption confidence="0.999747">
Table 1: Training Configuration
</tableCaption>
<bodyText confidence="0.999933">
The training of the model can be performed un-
der different configurations. The configurations
that we used for the training experiments are given
in Table 6.1.1.
</bodyText>
<subsectionHeader confidence="0.763993">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999729871794872">
For the complete training, the number of sen-
tences that should be used for the best perfor-
mance of the decoder should be the complete set.
In the paper, we have conducted experiments by
considering 500 training sentences to observe the
best training configuration.
At a source node, the weight vector is itera-
tively updated till the system predicts the gold
transformation. We conducted experiments by fix-
ing the maximum number of update attempts. A
source node, where the gold transformation is not
achieved even after the maximum updates limit,
the update at this source node is termed a update
failure. The source nodes, where the gold trans-
formation is achieved even without making any
updates is known as the correct prediction.
At some of the source nodes, it is not possible
to arrive at the gold target transformation because
of limited size of the training corpus. At such
nodes, we have avoided doing any weight update.
As the desired transformation is unachievable, any
attempt to update the weight vector would cause
noisy weight updates.
We observe various parameters to check the ef-
fectiveness of the training configuration. One of
the parameters (which we refer to as ‘updateHits’)
computes the number of successful updates (S)
performed at the source nodes in contrast to num-
ber of failed updates (F). Successful updates re-
sult in the prediction of the transformation that is
same as the reference transformation. A failed up-
date doesn’t result in the achievement of the cor-
rect prediction even after the maximum iteration
limit (see section 6.1.1) is reached. At some of the
source nodes, the reference transformations are
unreachable (U). The goal is to choose the con-
figuration that has least number of average failed
updates (F) because it implies that the model has
been learnt effectively.
</bodyText>
<table confidence="0.929537166666667">
UpdateHit
K m P S F U
1 4 1680 2692 84 4081
5 4 1595 2786 75 4081
10 4 1608 2799 49 4081
20 4 1610 2799 47 4081
</table>
<tableCaption confidence="0.997814">
Table 2: Training Statistics - Effect of Beam Size
</tableCaption>
<bodyText confidence="0.9965156">
From Table 2, we can see that the bigger beam
size leads to a better training of the model. The
beam size was varied between 1 and 20, and the
number of update failures (F) was observed to be
least at K=20.
</bodyText>
<table confidence="0.9179446">
UpdateHit
K m P S F U
1. 20 1 1574 2724 158 4081
2. 20 2 1598 2767 91 4081
3. 20 4 1610 2799 47 4081
</table>
<tableCaption confidence="0.948029">
Table 3: Training Statistics - Effect of maximum
update attempts
</tableCaption>
<bodyText confidence="0.999814">
In Table 3, we can see that an higher limit on
the maximum number of update attempts results
in less number of update attempts as expected. A
much higher value of m is not preferable because
the training updates makes noisy updates in case
of difficult nodes i.e., the nodes where target trans-
formation is reachable in theory, but is unreach-
able given the set of features.
</bodyText>
<table confidence="0.985256">
UpdateHit
K i P S F U
1. 1 1 1680 2692 84 4081
2. 1 2 1679 2694 83 4081
</table>
<tableCaption confidence="0.975472">
Table 4: Training Statistics - Effect of number of
iterations
</tableCaption>
<bodyText confidence="0.939153">
Now, we examine the effect of number of it-
</bodyText>
<page confidence="0.996984">
71
</page>
<bodyText confidence="0.999066166666667">
erations on the quality of the model. In table 4,
we can observe that the number of iterations on
the data has no effect on the quality of the model.
This implies, that the model is adequately learnt
after one pass through the data. This is possible
because of the multiple number of update attempts
allowed at every node. Hence, the weights are up-
dated at a node till the model prediction is consis-
tent with the gold transformation.
Based on the above observations, we consider
the configuration 4 in Table 2 for the decoding ex-
periments.
Now, we present some of the top features
weights leant by the best configuration. The
weights convey that important properties of trans-
formation are being learnt well. Table 5 presents
the weights of the features ‘diceRoot’, ‘dice-
RootChildren’ and ‘diceRootParent’.
</bodyText>
<table confidence="0.9903745">
Feature Weight
dice 75.67
diceChildren 540.31
diceParent 595.94
treelet translation probability (ttp) 1 0.77
treelet translation probability (ttp) 2 389.62
</table>
<tableCaption confidence="0.998974">
Table 5: Weights of dice coefficient based features
</tableCaption>
<bodyText confidence="0.9872059">
We see that the dice coefficient based local and
contextual features have a positive impact on the
selection of correct transformations. A feature
that uses a syntactic language model to compute
the perplexity per word has a negative weight of
-1.115.
Table 6 presents the top-5 entries of contex-
tual features that describe the translation of source
argument ’nsubj’ using contextual information
(‘tense’ of its parent).
</bodyText>
<table confidence="0.998520333333333">
Feature Weight
roleTenseVib:nsubj+NULL NULL 44.194196513246
roleTenseVib:nsubj+has VBN ne 14.4541356715382
roleTenseVib:nsubj+VBD ne 10.9241093097953
roleTenseVib:nsubj+VBP meM 6.14149937079584
roleTenseVib:nsubj+VBP NULL 4.76795730621754
</table>
<tableCaption confidence="0.880898666666667">
Table 6: Top weights of a contextual feature :
preposition+Tense-postposition
Table 7 presents the top-10 ordering relative po-
</tableCaption>
<bodyText confidence="0.999007333333333">
sition feature where the head word is a verb. In
this feature, the relative position (left or right) of
the head and the child is captured. For example, a
feature ‘relPos:amod-NN’, if active, conveys that
an argument with the role ‘amod’ is at the left of
a head word with POS tag ‘NN’.
</bodyText>
<table confidence="0.9982745">
Feature Weight
relPos:amod-NN 6.70
relPos:NN-appos 1.62
relPos:lrb-NN 1.62
</table>
<tableCaption confidence="0.999525">
Table 7: Top weights of relPos feature
</tableCaption>
<subsectionHeader confidence="0.997793">
6.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999832571428572">
We computed the translation accuracies using two
metrics, (1) BLEU score (Papineni et al., 2002),
and (2) Lexical Accuracy (or F-Score) on a test
set of 30 sentences. We compared the accuracy
of the experimental system (Vaanee) presented in
this paper, with Moses (state-of-the-art translation
system) and Shakti (rule-based translation system
7) under similar conditions (with using a develop-
ment set to tune the models). The rule-based sys-
tem considered is a general domain system tuned
to the tourism domain. The best BLEU score for
Moses on the test set is 0.118, and the best lexi-
cal accuracy is 0.512. The best BLEU score for
Shakti is 0.054, and the best lexical accuracy is
0.369.
In comparison, the best BLEU score of Vaanee
is 0.067, while the best lexical accuracy is 0.445.
As observed, the decoding results of the experi-
mental system mentioned here are not yet compa-
rable to the state-of-art. The main reasons for the
low translation accuracies are,
</bodyText>
<listItem confidence="0.904467">
1. Poor Quality of the dataset
</listItem>
<bodyText confidence="0.999782">
The dataset currently available for English-
Hindi language pair is noisy. This is an
extremely large limiting factor for a model
which uses rich linguistic information within
the statistical framework.
</bodyText>
<listItem confidence="0.468022">
2. Low Parser accuracy
</listItem>
<footnote confidence="0.961952">
7http://shakti.iiit.ac.in/
</footnote>
<page confidence="0.997678">
72
</page>
<bodyText confidence="0.99348275">
The parser accuracy on the English-Hindi
dataset is low, the reasons being, (1) Noise,
(2) Length of sentences, and (3) Wide scope
of the tourism domain.
</bodyText>
<listItem confidence="0.989147">
3. Word insertions not implemented yet
4. Non-projectivity not yet handled
5. BLEU is not an appropriate metric
</listItem>
<bodyText confidence="0.907738896551724">
BLEU is not an appropriate metric (Anan-
thakrishnan et al., ) for measuring the trans-
lation accuracy into Indian languages.
6. Model is context free as far as targets words
are concerned. Selection depends on chil-
dren but not parents and siblings
This point concerns the decoding algorithm.
The current algorithm is greedy while chos-
ing the best translation at every source node.
It first explores the K-best local transforma-
tions at a source node. It then makes a greedy
selection of the predicted subtree based on
it’s overall score after considering the predic-
tions at the child nodes, and the relative posi-
tion of the local transformation with respect
the predictions at the child nodes.
The problem in this approach is that, an er-
ror once made at a lower level of the tree
is propogated to the top, causing more mis-
takes. A computationally reasonable solution
to this problem is to maintain a K-best list
of predicted subtrees corresponding to every
source node. This allows rectification of a
mistake made at any stage.
The system, however, performs better than the
rule based system. As observed earlier, the right
type of information is being learnt by the model,
and the approach looks promising. The limitations
expressed here shall be addressed in the future.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99996584">
In this work, we presented a syntax based de-
pendency model to effectively handle problems in
translation from English to Indian languages such
as, (1) Large word order variation, and (2) Ac-
curate generation of word-forms in the target lan-
guage by predicted the word and its factors. The
model that we have proposed, has the flexibility of
adding rich linguistic features.
An experimental version of the system has been
implemented, which is available for download at
http://shakti.iiit.ac.in/∼sriram/vaanee.html. This
can facilitate as a platform for future research in
syntax based statistical machine translation from
English to Indian languages. We also plan to per-
form experiments using this system between Eu-
ropean languages in future.
The performance of the implemented transla-
tion system, is not yet comparable to the state-
of-art results primarily for two reasons, (1) Poor
quality of available data, because of which our
model which uses rich linguistic information
doesn’t perform as expected, and (2) Components
for word insertion and non-projectivity handling
are yet to be implemented in this version of the
system.
</bodyText>
<sectionHeader confidence="0.997882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996123">
Ananthakrishnan, R, B Pushpak, M Sasikumar, and
Ritesh Shah. Some issues in automatic evaluation
of english-hindi mt: more blues for bleu. ICON-
2007.
Ananthakrishnan, R., Jayprasad Hegde, Pushpak Bhat-
tacharyya, and M. Sasikumar. 2008. Simple syntac-
tic and morphological processing can help english-
hindi statistical machine translation. In Proceedings
of IJCNLP-2008. IJCNLP.
Bharati, Akshar, Vineet Chaitanya, Amba P Kulkarni,
and Rajeev Sangal. 1997. Anusaaraka: Machine
translation in stages. A Quarterly in Artificial Intel-
ligence, NCST, Bombay (renamed as CDAC, Mum-
bai).
Bharati, Akshar, Medhavi Bhatia, Vineet Chaitanya,
and Rajeev Sangal. 1998. Paninian grammar
framework applied to english. South Asian Lan-
guage Review, (3).
Bharati, Akshar, Rajeev Sangal, Dipti M Sharma, and
Amba P Kulkarni. 2002. Machine translation activ-
ities in india: A survey. In Proceedings of workshop
on survey on Research and Development of Machine
Translation in Asian Countries.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
263–270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.983401">
73
</page>
<reference confidence="0.99989167032967">
Crammer, K., R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for structured
classification. Technical report, University of Penn-
sylvania.
Gali, Karthik and Sriram Venkatapathy. 2009. Sen-
tence realisation from bag of words with depen-
dency constraints. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Student Research Workshop and Doctoral Consor-
tium, pages 19–24, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Guo, Yuqing, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for gen-
eral purpose sentence realisation. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 297–304,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Ittycheriah, Abraham and Salim Roukos. 2007. Di-
rect translation model 2. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
pages 57–64, Rochester, New York, April. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423–430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Koehn, Philipp and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868–876.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-
NAACL 2003), Edmonton, Canada, May.
Liang, P., A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In International Conference
on Computational Linguistics and Association for
Computational Linguistics (COLING/ACL).
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL.
Och, F.J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
Papineni, Kishore, Salim Roukos, Todd Ward, and W.J.
Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association of Compu-
tational Linguistics, pages 313–318, Philadelphia,
PA, July.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’05), pages 271–279, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Ramanathan, Ananthakrishnan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: Addressing the crux
of the fluency problem in english-hindi smt. In Pro-
ceedings of ACL-IJCNLP 2009. ACL-IJCNLP.
Shen, Libin, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 72–80, Singapore, August. Asso-
ciation for Computational Linguistics.
Vaidya, Ashwini, Samar Husain, Prashanth Reddy, and
Dipti M Sharma. 2009. A karaka based annotation
scheme for english. In Proceedings of CICLing ,
2009.
Wu, Dekai. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377–404.
Yamada, Kenji and Kevin Knight. 2002. A decoder
for syntax-based statistical mt. In Proceedings of
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 303–310, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.999133">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.360959">
<title confidence="0.6915448">A Discriminative Approach for Dependency Based Statistical Machine Translation Sriram Venkatapathy Rajeev Sangal LTRC, IIIT-Hyderabad LTRC, IIIT-Hyderabad sriram@research.iiit.ac.in sangal@mail.iiit.ac.in</title>
<author confidence="0.999132">Joshi Karthik</author>
<affiliation confidence="0.998661">University of Pennsylvania Talentica</affiliation>
<email confidence="0.998283">joshi@seas.upenn.edukarthik.gali@gmail.com</email>
<abstract confidence="0.997952428571429">In this paper, we propose a dependency based statistical system that uses discriminative techniques to train its parameters. We conducted experiments on an English- Hindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R Ananthakrishnan</author>
<author>B Pushpak</author>
<author>M Sasikumar</author>
<author>Ritesh Shah</author>
</authors>
<title>Some issues in automatic evaluation of english-hindi mt: more blues for bleu.</title>
<pages>2007</pages>
<marker>Ananthakrishnan, Pushpak, Sasikumar, Shah, </marker>
<rawString>Ananthakrishnan, R, B Pushpak, M Sasikumar, and Ritesh Shah. Some issues in automatic evaluation of english-hindi mt: more blues for bleu. ICON2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ananthakrishnan</author>
<author>Jayprasad Hegde</author>
<author>Pushpak Bhattacharyya</author>
<author>M Sasikumar</author>
</authors>
<title>Simple syntactic and morphological processing can help englishhindi statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP-2008. IJCNLP.</booktitle>
<contexts>
<context position="1656" citStr="Ananthakrishnan et al., 2008" startWordPosition="234" endWordPosition="237">amada and Knight, 2002; Quirk et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., 1This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 2002; Ananthakrishnan et al., 2008; Ramanathan et al., 2009) where there is very limited parallel corpora available, and breaking words into smaller units helps in reducing sparsity. In order to handle phenomenon such as long-distance word agreement to achieve accurate generation of target language words, the inter-dependence between the factors of syntactically related words need to be modelled effectively. Some of the limitations with the syntax based approaches such as (Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005) are, (1) They do not offer flexibility for adding linguistically motivated features, and (2) It i</context>
</contexts>
<marker>Ananthakrishnan, Hegde, Bhattacharyya, Sasikumar, 2008</marker>
<rawString>Ananthakrishnan, R., Jayprasad Hegde, Pushpak Bhattacharyya, and M. Sasikumar. 2008. Simple syntactic and morphological processing can help englishhindi statistical machine translation. In Proceedings of IJCNLP-2008. IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Vineet Chaitanya</author>
<author>Amba P Kulkarni</author>
<author>Rajeev Sangal</author>
</authors>
<title>Anusaaraka: Machine translation in stages. A Quarterly in Artificial Intelligence, NCST, Bombay (renamed as CDAC,</title>
<date>1997</date>
<location>Mumbai).</location>
<contexts>
<context position="1513" citStr="Bharati et al., 1997" startWordPosition="210" endWordPosition="213">chine Translation (MT) have gained popularity in recent times because of their ability to handle long distance reorderings (Wu, 1997; Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., 1This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 2002; Ananthakrishnan et al., 2008; Ramanathan et al., 2009) where there is very limited parallel corpora available, and breaking words into smaller units helps in reducing sparsity. In order to handle phenomenon such as long-distance word agreement to achieve accurate generation of target language words, the inter-dependence between the factors of syntactically related words need to be modelled effectively. Some of the limitations with the syntax based approaches such as (Yamada and Kn</context>
</contexts>
<marker>Bharati, Chaitanya, Kulkarni, Sangal, 1997</marker>
<rawString>Bharati, Akshar, Vineet Chaitanya, Amba P Kulkarni, and Rajeev Sangal. 1997. Anusaaraka: Machine translation in stages. A Quarterly in Artificial Intelligence, NCST, Bombay (renamed as CDAC, Mumbai).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Medhavi Bhatia</author>
<author>Vineet Chaitanya</author>
<author>Rajeev Sangal</author>
</authors>
<title>Paninian grammar framework applied to english.</title>
<date>1998</date>
<journal>South Asian Language Review,</journal>
<volume>3</volume>
<contexts>
<context position="8611" citStr="Bharati et al., 1998" startWordPosition="1326" endWordPosition="1329">ary verbs largely express the grammatical roles/functions of the content words in the sentence. In fact, in many agglutinative languages, these words are commonly attached to the content word to form one word form. In this paper, we also conduct experiments where we begin by grouping the function words with their corresponding function words. These groups of words are called local-word groups. In these cases, the function words are considered as factors of the content words. Section 2 explains more about the local word groups in English and Hindi. 2 Local Word Groups Local word groups (LWGs) (Bharati et al., 1998; Vaidya et al., 2009) consist of a content word and its associated function words. Local word grouping reduces a sentence to a sequence of content words with the case-markers and tense-markers acting as their factors. For example, consider an English sentence ‘People of these island have adopted Hindi as a means of communication’. ‘have adopted’ is a LWG with root ‘adopt’ and tense markers being ‘have ed’. Another example for the LWG will be ‘of communication’ where ‘communication’ is the root, and ‘of’ is the casemarker. It is to be noted that Local word grouping is different from chunking, </context>
</contexts>
<marker>Bharati, Bhatia, Chaitanya, Sangal, 1998</marker>
<rawString>Bharati, Akshar, Medhavi Bhatia, Vineet Chaitanya, and Rajeev Sangal. 1998. Paninian grammar framework applied to english. South Asian Language Review, (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Rajeev Sangal</author>
<author>Dipti M Sharma</author>
<author>Amba P Kulkarni</author>
</authors>
<title>Machine translation activities in india: A survey.</title>
<date>2002</date>
<booktitle>In Proceedings of workshop on survey on Research and Development of Machine Translation in Asian Countries.</booktitle>
<marker>Bharati, Sangal, Sharma, Kulkarni, 2002</marker>
<rawString>Bharati, Akshar, Rajeev Sangal, Dipti M Sharma, and Amba P Kulkarni. 2002. Machine translation activities in india: A survey. In Proceedings of workshop on survey on Research and Development of Machine Translation in Asian Countries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1085" citStr="Chiang, 2005" startWordPosition="147" endWordPosition="148"> on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 1 Introduction Syntax based approaches for Machine Translation (MT) have gained popularity in recent times because of their ability to handle long distance reorderings (Wu, 1997; Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., 1This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 2002; Ananthakrishnan et al., 2008; Ramanathan et al., 2009) wh</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Scalable large-margin online learning for structured classification.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6639" citStr="Crammer et al., 2005" startWordPosition="1029" endWordPosition="1032">rmation in the target language that is being predicted. During decoding a source sentence, the goal is to choose a transformation that has the highest score. The source syntactic structure is traversed in a bottom-up fashion and the target syntactic structure is simultaneously built. We used a bottom-up traversal while decoding because it builds a contiguous sequence of nodes for the subtrees during traversal enabling the application of a wide variety of language models. In the training phase, the task is to learn the weights of features. We use an online largemargin training algorithm, MIRA (Crammer et al., 2005), for learning the weights. The weights are locally updated at every source node during the bottom-up traversal of the source structure. For training the translation model, automatically obtained word-aligned parallel corpus is used. We used GIZA++ (Och and Ney, 2003) along with the growing heuristics to word-align the training corpus. The basic factors of the word used in our experiments are root, part-of-speech, gender, number and person. In Hindi, common nouns and verbs have gender information whereas, English doesn’t contain that information. Apart from the basic factors, we also consider </context>
<context position="15982" citStr="Crammer et al., 2005" startWordPosition="2527" endWordPosition="2530">. The gold atomic treelets in the source and their transformation is obtained by mapping the source node to the target using the word-alignment information. This information is stored in the form of transformation tables that is used for the prediction of target atomic treelets, prepositions and other factors. The transformation tables are pruned in order to limit the search and eliminate redundant information. For each source element, only the top few entries are retained in the table. This limit ranges from 3 to 20. We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; Crammer et al., 2005), for updating the weights. During parameter optimization, it is sometimes impossible to achieve the gold transformation for a node because the pruned transformation tables may not lead to the target gold prediction for the source node. In such cases where the gold transformation is unreachable, the weights are not updated at all for the source node as it might cause erroneous weight updates. We conducted our experiments by considering both the cases, (1) Identifying source nodes with unreachable transformations, and (2) Updating weights for all the source nodes (till a maximum iteration limit</context>
</contexts>
<marker>Crammer, McDonald, Pereira, 2005</marker>
<rawString>Crammer, K., R. McDonald, and F. Pereira. 2005. Scalable large-margin online learning for structured classification. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Karthik Gali</author>
<author>Sriram Venkatapathy</author>
</authors>
<title>Sentence realisation from bag of words with dependency constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="11973" citStr="Gali and Venkatapathy, 2009" startWordPosition="1877" endWordPosition="1880">ormation given in Figure 1 is shown in Figure 3. Figure 3: Syntactic feature - reorder postags The feature reorderPostags is in the form of a complete transfer rule. To handle cases, where the left-hand side of ‘reorderPostags’ does not match the syntactic structure of the source tree, the simpler feature functions are used to qualify various reorderings. Instead of using POS tags, feature functions can be defined that use syntactic roles. Apart from the above feature functions, we can also have features that compute the score of a particular order of children using syntactic language models (Gali and Venkatapathy, 2009; Guo et al., 2008). Different features can be defined that use different levels of information pertaining to the atomic treelet and its children. 3.3 Contextual Features Contextual features model the inter-dependence of factors of nodes connected by dependency arcs. These features are used to enable access to global information for prediction of target nodes (words and its factors). One of the features diceCoeffParent, relates the parent of a source node to the corresponding target node (see figure 4. Figure 4: Use of Contextual (parent) information of x2 for generation of y The use of this f</context>
</contexts>
<marker>Gali, Venkatapathy, 2009</marker>
<rawString>Gali, Karthik and Sriram Venkatapathy. 2009. Sentence realisation from bag of words with dependency constraints. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium, pages 19–24, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Josef van Genabith</author>
<author>Haifeng Wang</author>
</authors>
<title>Dependency-based n-gram models for general purpose sentence realisation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>297--304</pages>
<location>Manchester, UK,</location>
<marker>Guo, van Genabith, Wang, 2008</marker>
<rawString>Guo, Yuqing, Josef van Genabith, and Haifeng Wang. 2008. Dependency-based n-gram models for general purpose sentence realisation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297–304, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="3016" citStr="Ittycheriah and Roukos, 2007" startWordPosition="448" endWordPosition="451">ntextual information was effectively used in the framework of a hierarchical machine translation system. In their work, four linguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the target during the translation process. This features use syntactic labels and collocation statistics in order to allow effective training of the model. Some of the other approaches related to our model are the Direct Translation Model 2 (DTM2) (Ittycheriah and Roukos, 2007), End-to-End Discriminative Approach to MT (Liang et al., 2006) and Factored Translation Models (Koehn and Hoang, 2007). In DTM2, a discriminative trans66 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66–74, COLING 2010, Beijing, August 2010. lation model is defined in the setting of a phrase based translation system. In their approach, the features are optimized globally. In contrast to their approach, we define a discriminative model for translation in the setting of a syntax based machine translation system. This allows us to use both the p</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Ittycheriah, Abraham and Salim Roukos. 2007. Direct translation model 2. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 57–64, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="7404" citStr="Klein and Manning, 2003" startWordPosition="1148" endWordPosition="1151">training the translation model, automatically obtained word-aligned parallel corpus is used. We used GIZA++ (Och and Ney, 2003) along with the growing heuristics to word-align the training corpus. The basic factors of the word used in our experiments are root, part-of-speech, gender, number and person. In Hindi, common nouns and verbs have gender information whereas, English doesn’t contain that information. Apart from the basic factors, we also consider the role information provided by labelled dependency parsers. For computing the dependency tree on the source side, We used stanford parser (Klein and Manning, 2003) in the experiments presented in this chapter3. 3Stanford parser gives both the phrase-structure tree as well as dependency relations for a sentence. 67 Ram/NNP visit/NN to/TO a/DT root=a, gnp=x3sg role=nmod Shyam/NNP root=Shyam, gnp=x1sg role=pmod root=shyaam gnp=m1sg paid/VBD root=pay, tense=PAST gnp=x3sg, role=X root=mila, tense=PAST gnp=m3sg root=Ram, gnp=x1sg role=subj root=visit, gnp=x3sg role=obj root=to, gnp=x3sg role=vmod root=raam gnp=m1sg root=se gnp=x3sg Figure 1: Transformation from source structure to target language The function words such as prepositions and auxiliary verbs lar</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<contexts>
<context position="3135" citStr="Koehn and Hoang, 2007" startWordPosition="466" endWordPosition="469">inguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the target during the translation process. This features use syntactic labels and collocation statistics in order to allow effective training of the model. Some of the other approaches related to our model are the Direct Translation Model 2 (DTM2) (Ittycheriah and Roukos, 2007), End-to-End Discriminative Approach to MT (Liang et al., 2006) and Factored Translation Models (Koehn and Hoang, 2007). In DTM2, a discriminative trans66 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66–74, COLING 2010, Beijing, August 2010. lation model is defined in the setting of a phrase based translation system. In their approach, the features are optimized globally. In contrast to their approach, we define a discriminative model for translation in the setting of a syntax based machine translation system. This allows us to use both the power of a syntax based approach, as well as, the power of a large feature space during translation. In our approach, th</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, Philipp and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="15361" citStr="Koehn et al., 2003" startWordPosition="2428" endWordPosition="2431">zed into the tasks of computing the best transformations of the source treelets. The equation for computing the score of a transformation, Equation 1, can be modified as Equation 2 given below. �score(τ|s) = �|r |∗ wZ ∗ fZ(τr, r) (2) r Z where, τj is the local transformation of the source treelet r. The best transformation τˆ of source sentence s is, τˆ = argmaxτ score(τ|s) (3) 5 Training Algorithm The goal of the training algorithm is to learn the feature weights from the word aligned corpus. For word-alignment, we used the IBM Model 5 implemented in GIZA++ along with the growing heuristics (Koehn et al., 2003). The gold atomic treelets in the source and their transformation is obtained by mapping the source node to the target using the word-alignment information. This information is stored in the form of transformation tables that is used for the prediction of target atomic treelets, prepositions and other factors. The transformation tables are pruned in order to limit the search and eliminate redundant information. For each source element, only the top few entries are retained in the table. This limit ranges from 3 to 20. We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference 2003 (HLTNAACL 2003), Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cote</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL).</booktitle>
<contexts>
<context position="3079" citStr="Liang et al., 2006" startWordPosition="458" endWordPosition="461">cal machine translation system. In their work, four linguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the target during the translation process. This features use syntactic labels and collocation statistics in order to allow effective training of the model. Some of the other approaches related to our model are the Direct Translation Model 2 (DTM2) (Ittycheriah and Roukos, 2007), End-to-End Discriminative Approach to MT (Liang et al., 2006) and Factored Translation Models (Koehn and Hoang, 2007). In DTM2, a discriminative trans66 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66–74, COLING 2010, Beijing, August 2010. lation model is defined in the setting of a phrase based translation system. In their approach, the features are optimized globally. In contrast to their approach, we define a discriminative model for translation in the setting of a syntax based machine translation system. This allows us to use both the power of a syntax based approach, as well as, the power of a lar</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>Liang, P., A. Bouchard-Cote, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="15959" citStr="McDonald and Pereira, 2006" startWordPosition="2523" endWordPosition="2526">ristics (Koehn et al., 2003). The gold atomic treelets in the source and their transformation is obtained by mapping the source node to the target using the word-alignment information. This information is stored in the form of transformation tables that is used for the prediction of target atomic treelets, prepositions and other factors. The transformation tables are pruned in order to limit the search and eliminate redundant information. For each source element, only the top few entries are retained in the table. This limit ranges from 3 to 20. We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; Crammer et al., 2005), for updating the weights. During parameter optimization, it is sometimes impossible to achieve the gold transformation for a node because the pruned transformation tables may not lead to the target gold prediction for the source node. In such cases where the gold transformation is unreachable, the weights are not updated at all for the source node as it might cause erroneous weight updates. We conducted our experiments by considering both the cases, (1) Identifying source nodes with unreachable transformations, and (2) Updating weights for all the source nodes (till a </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>McDonald, R. and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6907" citStr="Och and Ney, 2003" startWordPosition="1069" endWordPosition="1072">eously built. We used a bottom-up traversal while decoding because it builds a contiguous sequence of nodes for the subtrees during traversal enabling the application of a wide variety of language models. In the training phase, the task is to learn the weights of features. We use an online largemargin training algorithm, MIRA (Crammer et al., 2005), for learning the weights. The weights are locally updated at every source node during the bottom-up traversal of the source structure. For training the translation model, automatically obtained word-aligned parallel corpus is used. We used GIZA++ (Och and Ney, 2003) along with the growing heuristics to word-align the training corpus. The basic factors of the word used in our experiments are root, part-of-speech, gender, number and person. In Hindi, common nouns and verbs have gender information whereas, English doesn’t contain that information. Apart from the basic factors, we also consider the role information provided by labelled dependency parsers. For computing the dependency tree on the source side, We used stanford parser (Klein and Manning, 2003) in the experiments presented in this chapter3. 3Stanford parser gives both the phrase-structure tree a</context>
<context position="17829" citStr="Och and Ney, 2003" startWordPosition="2821" endWordPosition="2824">stical machine translation system, which can act as a platform for further experiments on similar lines. The system would be available for download at http://shakti.iiit.ac.in/∼sriram/vaanee.html. To evaluate this experimental system, a restricted set of experiments are conducted. The experiments are conducted on the English-Hindi language pair using a corpus in tourism domain containing 11300 sentence pairs6. 6.1 Training 6.1.1 Configuration For training, we used DIT-TOURISM-ALIGNTRAIN dataset which is the word-aligned dataset of 11300 sentence pairs. The word-alignment is done using GIZA++ (Och and Ney, 2003) toolkit and then growing heuristics are applied. For our experiments, we use two growing heuristics, GROW-DIAG-FINAL-AND and GROW-DIAGFINAL as they cover most number of words in both the sides of the parallel corpora. 6DIT-TOURISM corpus 70 Number of Training Sentences 500 Iterations on Corpus 1-2 Parameter optimization algorithm MIRA Beam Size 1-20 Maximum update attempts at source node 1-4 Unreachable updates False Size of transformation tables 3 Table 1: Training Configuration The training of the model can be performed under different configurations. The configurations that we used for the</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F.J. and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>313--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="23699" citStr="Papineni et al., 2002" startWordPosition="3801" endWordPosition="3804">eights of a contextual feature : preposition+Tense-postposition Table 7 presents the top-10 ordering relative position feature where the head word is a verb. In this feature, the relative position (left or right) of the head and the child is captured. For example, a feature ‘relPos:amod-NN’, if active, conveys that an argument with the role ‘amod’ is at the left of a head word with POS tag ‘NN’. Feature Weight relPos:amod-NN 6.70 relPos:NN-appos 1.62 relPos:lrb-NN 1.62 Table 7: Top weights of relPos feature 6.3 Decoding We computed the translation accuracies using two metrics, (1) BLEU score (Papineni et al., 2002), and (2) Lexical Accuracy (or F-Score) on a test set of 30 sentences. We compared the accuracy of the experimental system (Vaanee) presented in this paper, with Moses (state-of-the-art translation system) and Shakti (rule-based translation system 7) under similar conditions (with using a development set to tune the models). The rule-based system considered is a general domain system tuned to the tourism domain. The best BLEU score for Moses on the test set is 0.118, and the best lexical accuracy is 0.512. The best BLEU score for Shakti is 0.054, and the best lexical accuracy is 0.369. In comp</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and W.J. Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association of Computational Linguistics, pages 313–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>271--279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1070" citStr="Quirk et al., 2005" startWordPosition="143" endWordPosition="146">onducted experiments on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 1 Introduction Syntax based approaches for Machine Translation (MT) have gained popularity in recent times because of their ability to handle long distance reorderings (Wu, 1997; Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., 1This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 2002; Ananthakrishnan et al., 2008; Ramanathan e</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 271–279, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Hansraj Choudhary</author>
<author>Avishek Ghosh</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Case markers and morphology: Addressing the crux of the fluency problem in english-hindi smt.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<publisher>ACL-IJCNLP.</publisher>
<contexts>
<context position="1682" citStr="Ramanathan et al., 2009" startWordPosition="238" endWordPosition="241">et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., 1This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 2002; Ananthakrishnan et al., 2008; Ramanathan et al., 2009) where there is very limited parallel corpora available, and breaking words into smaller units helps in reducing sparsity. In order to handle phenomenon such as long-distance word agreement to achieve accurate generation of target language words, the inter-dependence between the factors of syntactically related words need to be modelled effectively. Some of the limitations with the syntax based approaches such as (Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005) are, (1) They do not offer flexibility for adding linguistically motivated features, and (2) It is not possible to use morp</context>
</contexts>
<marker>Ramanathan, Choudhary, Ghosh, Bhattacharyya, 2009</marker>
<rawString>Ramanathan, Ananthakrishnan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and morphology: Addressing the crux of the fluency problem in english-hindi smt. In Proceedings of ACL-IJCNLP 2009. ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>72--80</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="2368" citStr="Shen et al., 2009" startWordPosition="348" endWordPosition="351">g words into smaller units helps in reducing sparsity. In order to handle phenomenon such as long-distance word agreement to achieve accurate generation of target language words, the inter-dependence between the factors of syntactically related words need to be modelled effectively. Some of the limitations with the syntax based approaches such as (Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005) are, (1) They do not offer flexibility for adding linguistically motivated features, and (2) It is not possible to use morphological factors in the syntax based approaches. In a recent work (Shen et al., 2009), linguistic and contextual information was effectively used in the framework of a hierarchical machine translation system. In their work, four linguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the target during the translation process. This features use syntactic labels and collocation statistics in order to allow effective training of the model. Some of the other approaches related to our model are the Direct Translati</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Shen, Libin, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashwini Vaidya</author>
<author>Samar Husain</author>
<author>Prashanth Reddy</author>
<author>Dipti M Sharma</author>
</authors>
<title>A karaka based annotation scheme for english.</title>
<date>2009</date>
<booktitle>In Proceedings of CICLing ,</booktitle>
<contexts>
<context position="8633" citStr="Vaidya et al., 2009" startWordPosition="1330" endWordPosition="1333">ess the grammatical roles/functions of the content words in the sentence. In fact, in many agglutinative languages, these words are commonly attached to the content word to form one word form. In this paper, we also conduct experiments where we begin by grouping the function words with their corresponding function words. These groups of words are called local-word groups. In these cases, the function words are considered as factors of the content words. Section 2 explains more about the local word groups in English and Hindi. 2 Local Word Groups Local word groups (LWGs) (Bharati et al., 1998; Vaidya et al., 2009) consist of a content word and its associated function words. Local word grouping reduces a sentence to a sequence of content words with the case-markers and tense-markers acting as their factors. For example, consider an English sentence ‘People of these island have adopted Hindi as a means of communication’. ‘have adopted’ is a LWG with root ‘adopt’ and tense markers being ‘have ed’. Another example for the LWG will be ‘of communication’ where ‘communication’ is the root, and ‘of’ is the casemarker. It is to be noted that Local word grouping is different from chunking, where more than one co</context>
</contexts>
<marker>Vaidya, Husain, Reddy, Sharma, 2009</marker>
<rawString>Vaidya, Ashwini, Samar Husain, Prashanth Reddy, and Dipti M Sharma. 2009. A karaka based annotation scheme for english. In Proceedings of CICLing , 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1025" citStr="Wu, 1997" startWordPosition="137" endWordPosition="138">iques to train its parameters. We conducted experiments on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 1 Introduction Syntax based approaches for Machine Translation (MT) have gained popularity in recent times because of their ability to handle long distance reorderings (Wu, 1997; Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., 1This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 200</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Wu, Dekai. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical mt.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>303--310</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="1050" citStr="Yamada and Knight, 2002" startWordPosition="139" endWordPosition="142">rain its parameters. We conducted experiments on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 1 Introduction Syntax based approaches for Machine Translation (MT) have gained popularity in recent times because of their ability to handle long distance reorderings (Wu, 1997; Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., 1This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 2002; Ananthakrishnan et al.</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-based statistical mt. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 303–310, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>