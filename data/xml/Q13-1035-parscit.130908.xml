<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.987904">
Measuring Machine Translation Errors in New Domains
</title>
<author confidence="0.978428">
Ann Irvine John Morgan
</author>
<affiliation confidence="0.989848">
Johns Hopkins University University of Maryland
</affiliation>
<email confidence="0.994311">
anni@jhu.edu jjm@cs.umd.edu
</email>
<author confidence="0.995761">
Marine Carpuat Hal Daum´e III Dragos Munteanu
</author>
<affiliation confidence="0.989488">
National Research Council Canada University of Maryland SDL Research
</affiliation>
<email confidence="0.994115">
marine.carpuat@nrc.gc.ca me@hal3.name dmunteanu@sdl.com
</email>
<sectionHeader confidence="0.993792" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963928571428">
We develop two techniques for analyzing the
effect of porting a machine translation system
to a new domain. One is a macro-level ana-
lysis that measures how domain shift affects
corpus-level evaluation; the second is a micro-
level analysis for word-level errors. We ap-
ply these methods to understand what happens
when a Parliament-trained phrase-based ma-
chine translation system is applied in four very
different domains: news, medical texts, scien-
tific articles and movie subtitles. We present
quantitative and qualitative experiments that
highlight opportunities for future research in
domain adaptation for machine translation.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997468032258065">
When building a statistical machine translation
(SMT) system, the expected use case is often limited
to a specific domain, genre and register (henceforth
“domain” refers to this set, in keeping with standard,
imprecise, terminology), such as a particular type of
legal or medical document. Unfortunately, it is ex-
pensive to obtain enough parallel data to reliably es-
timate translation models in a new domain. Instead,
one can hope that large amounts of data from ano-
ther, “old domain,” might be close enough to stand
as a proxy. This is the defacto standard: we train
SMT systems on Parliament proceedings, but then
use them to translate all sorts of new text. Unfortuna-
tely, this results in significantly degraded translation
quality. In this paper, we present two complemen-
tary methods for quantifiably measuring the source
of translation errors (§5.1 and §5.2) in a novel taxo-
nomy (§4). We show quantitative (§7.1) and quali-
tative (§7.2) results obtained from our methods on
Old Domain (Hansard)
Inp monsieur le pr´esident, les pˆecheurs de homard de la
Ref r´egion de l’atlantique sont dans une situation catastro-
Out phique.
mr. speaker, lobster fishers in atlantic canada are facing
a disaster.
mr. speaker, the lobster fishers in atlantic canada are in
a mess.
New Domain (Medical)
Inp mode et voie(s) d’administration
Ref method and route(s) of administration
Out fashion and voie(s) of directors
</bodyText>
<tableCaption confidence="0.483272666666667">
TABLE 1: Example inputs, references and system outputs.
There are three types of errors: unseen words (blue), in-
correct sense selection (red) and unknown sense (green).
</tableCaption>
<bodyText confidence="0.999941782608696">
four very different new domains: newswire, medical
texts, scientific abstracts, and movie subtitles.
Our basic approach is to think of translation er-
rors in the context of a novel taxonomy of error
categories, “S4.” Our taxonomy contains categories
for the errors shown in Table 1, in which an SMT
system trained on the Hansard parliamentary proce-
dings is applied to a new domain (in this case, me-
dical texts). Our categorization focuses on the follo-
wing: new French words, new French senses, and in-
correctly chosen translations. The first methodology
we develop for studying such errors is a micro-level
study of the frequency and distribution of these error
types in real translation output at the level of indivi-
dual words (§5.1), without respect to how these er-
rors affect overall translation quality. The second is
a macro-level study of how these errors affect trans-
lation performance (measured by BLEU; §5.2). One
important feature of our methodologies is that we
focus on errors that could possibly be fixed given
access to data from a new domain, rather than all
errors that might arise because the particular transla-
tion model used is inadequate to capture the required
</bodyText>
<page confidence="0.994823">
429
</page>
<bodyText confidence="0.936678666666667">
Transactions of the Association for Computational Linguistics, 1 (2013) 429–440. Action Editor: Philipp Koehn.
Submitted 3/2013; Revised 8/2013; Published 10/2013. c�2013 Association for Computational Linguistics.
translation task (formally: we measure estimation er-
ror, not approximation error).
Our goal is neither to build better SMT systems
nor to develop novel domain adaptation methods.
We take an ab initio approach and ask: given a large
unadapted, out of the box SMT system, what hap-
pens when it is applied in a new domain ? In order to
answer this question, we will use parallel data in new
domains, but only for testing purposes. The baseline
SMT system is not adapted, except for the use of
(1) a language model trained on monolingual new-
domain language data,1 and (2) a few thousand pa-
rallel sentences of tuning data in the new domain.
</bodyText>
<sectionHeader confidence="0.930783" genericHeader="introduction">
2 Summary of Results
</sectionHeader>
<bodyText confidence="0.997505714285714">
We conduct experiments across a variety of do-
mains (described in §6).2 As in any study, our re-
sults are limited by assumptions about language, do-
mains, and MT systems: these assumptions and their
consequences are discussed in §8. Our high-level
conclusions on the domains we study are summa-
rized below (details may be found in §7).
</bodyText>
<listItem confidence="0.830819653846154">
1. Adapting an SMT system from the Parliament
domain to the news domain is not a representative
adaptation task; there are a very small number of er-
rors due to unseen words, which are minor in compa-
rison to all other domains. (Despite the fact that most
previous work focuses exclusively on using news as
a “new” domain, §3).
2. For the remaining domains, unseen words have a
significant effect, both in terms of BLEU scores as
well as fine-grained translation distinctions. Howe-
ver, many of these words have multiple translations,
and a system must be able to correctly select which
one to use in a particular context.
3. Likewise, words that gain new senses account for
approximately as much error as unseen words, sug-
gesting a novel avenue for research in sense induc-
tion. Unfortunately, it appears that choosing the right
sense for these at translation time is even more diffi-
cult than in the unseen word case.
4. The story is more complicated for seen words with
known translations: if we limit ourselves to “high
1. We use old/new to refer to domains and source/target to
refer to languages, to avoid ambiguity (we stay away from in-
domain and out-of-domain, which is itself ambiguous).
2. All source data, methodological code and outputs are
available at http://hal3.name/damt.
</listItem>
<bodyText confidence="0.998298">
confidence” translations, there is a lot to be gai-
ned by improving the scores in translation models.
However, for an entire phrase table, manipulating
scores can hurt as often as it helps.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.997548">
Most related work has focused on either (a) analy-
zing errors made by machine translation systems in
a non-adaptation setting (Popovi´c and Ney, 2011),
or (b) trying to directly improve machine translation
performance. A small amount of work (discussed
next) addresses issues of analyzing MT systems in
a domain adaptation setting.
</bodyText>
<subsectionHeader confidence="0.999851">
3.1 Analysis of Domain Effects
</subsectionHeader>
<bodyText confidence="0.999990285714286">
To date, work on domain adaptation in SMT
mostly proposed methods to efficiently combine
data from multiple domains. To the best of our
knowledge, there have been only a few studies to un-
derstand how domain shifts affect translation quality
(Duh et al., 2010; Bisazza et al., 2011; Haddow and
Koehn, 2012). However, these start from different
premises than this paper, and as a result, ask related
but complementary questions. These previous ana-
lyses focus on how to improve a particular MT ar-
chitecture (trained on new domain data) by injecting
old domain data into a specific part of the pipeline in
order to improve BLEU score. In comparison to this
work, we focus on finer-grained phenomena. We dis-
tinguish between effects previously lumped together
as “missing phrase-table entries.”
Despite different starting assumptions, language
pairs and data, some of our conclusions are
consistent with previous work: in particular, we
highlight the importance of differences in coverage
in an adaptation setting. However, our fine-grained
analysis shows that correctly scoring translations for
previously unseen words and senses is a complex
issue. Finally, these other studies suggest potential
directions for refining our error categories: for ins-
tance, Haddow and Koehn (2012) show that the im-
pact of additional new or old domain data is different
for rare vs. frequent phrases.
</bodyText>
<subsectionHeader confidence="0.999531">
3.2 Domain Adaptation for MT
</subsectionHeader>
<bodyText confidence="0.990594666666667">
Prior work focuses on methods combining data
from old and new domains to learn translation and
language models.
</bodyText>
<page confidence="0.995826">
430
</page>
<bodyText confidence="0.999944868421053">
Many filtering techniques have been proposed to
select OLD data that is similar to NEW . Informa-
tion retrieval techniques have been used to improve
the language model (Zhao et al., 2004), the transla-
tion model (Hildebrand et al., 2005; Lu et al., 2007;
Gong et al., 2011; Duh et al., 2010; Banerjee et
al., 2012), or both (Lu et al., 2007); language mo-
del cross-entropy has also been used for data se-
lection (Axelrod et al., 2011; Mansour et al., 2011;
Sennrich, 2012).
Another research thread addresses corpora weigh-
ting, rather than hard filtering. Weighting has been
applied at different levels of granularity: sentence
pairs (Matsoukas et al., 2009), phrase pairs (Foster et
al., 2010), n-grams (Ananthakrishnan et al., 2011),
or sub-corpora through factored models (Niehues
and Waibel, 2010). In particular, Foster et al. (2010)
show that adapting at the phrase pair levels outper-
form earlier coarser corpus level combination ap-
proaches (Foster and Kuhn, 2007). This is consistent
with our analysis: domain shifts have a fine-grained
impact on translation quality.
Finally, strategies have been proposed to com-
bine sub-models trained independently on different
sub-corpora. Linear interpolation is widely used for
mixing language models in speech recognition, and
it has also been used for adapting translation and lan-
guage models in MT(Foster and Kuhn, 2007; Tiede-
mann, 2010; Lavergne et al., 2011). Log-linear com-
bination fits well in existing SMT architectures (Fos-
ter and Kuhn, 2007; Koehn and Schroeder, 2007).
Koehn and Schroeder (2007) consider both an inter-
section setting (where only entries occurring in all
phrase-tables combined are considered), and a union
setting (where entries which are not in the intersec-
tion are given an arbitrary null score). Razmara et al.
(2012) take this approach further and frame combi-
nation as ensemble decoding.
</bodyText>
<subsectionHeader confidence="0.999927">
3.3 Targeting Specific Error Types
</subsectionHeader>
<bodyText confidence="0.9566">
The experiments conducted in this article motiva-
ted follow-up work on identifying when a word has
gained a new sense in a new domain (Carpuat et al.,
2013), as well as learning joint word translation pro-
bability distributions from comparable new domain
corpora (Irvine et al., 2013). Earlier, Daum´e III and
Jagarlamudi (2011) showed how mining translations
for unseen words from comparable corpora can im-
prove SMT in a new domain.
</bodyText>
<sectionHeader confidence="0.852606" genericHeader="method">
4 The S4 Taxonomy
</sectionHeader>
<bodyText confidence="0.99993590625">
We begin with a simple question: when we move
an SMT system from an old domain to a new do-
main, what goes wrong? We employ a set of four
error types as our taxonomy. We refer to these er-
ror types as SEEN, SENSE, SCORE and SEARCH, and
together as the S4 taxonomy:
SEEN: an attempt to translate a source word or
phrase that has never been seen before. For example,
“voie(s)” in Table 1.
SENSE: an attempt to translate a previously seen
source word or phrase, but for which the correct
target language sense has never been observed. 3 In
Table 1, the Hansard-trained system had never seen
“mode” translated as “method.”
SCORE: an incorrect translation for which the sys-
tem could have succeeded but did not because an
incorrect alternative outweighed the correct transla-
tion. In a conventional translation system, this could
be due to errors in the language model, translation
model, or both. In Table 1, the Hansard-trained sys-
tem had seen “administration” translated as “admi-
nistration,” but “directors” had a higher probability.
SEARCH: an error due to pruning in beam search.
When limiting oneself to issues of lexical selec-
tion, this set is exhaustive and disjoint: any lexi-
cal selection error made by an MT system can be
attributed to exactly one of these error categories.
This observation is important for developing metho-
dologies for measuring the impact of each of these
sources of error. Partitions of the set of errors that
focus on categories other than lexical choice have
been investigated by Vilar et al. (2006).
</bodyText>
<sectionHeader confidence="0.998537" genericHeader="method">
5 Methodology for Analyzing MT Systems
</sectionHeader>
<bodyText confidence="0.9998946">
Given the S4 taxonomy for categorizing SMT er-
rors, it would be possible (if painstaking) to ma-
nually annotate SMT output with error types. We
prefer automated methods. In this section we des-
cribe two such methods: a micro-level analysis to
</bodyText>
<footnote confidence="0.791576">
3. We define “sense” as a particular translation into a target
language, in line with Carpuat &amp; Wu (2007) or Mihalcea et al.
(2010). This means both traditional word sense errors and other
translation errors (like morphological variants) are included.
</footnote>
<page confidence="0.99778">
431
</page>
<figureCaption confidence="0.997828">
FIGURE 1: Example of WADE visualization. Dashed
boxes around the French input mark the phrase spans
used by the decoder.
</figureCaption>
<bodyText confidence="0.999881833333333">
see what happens at the word level (regardless of
how it affects translation performance) and a macro-
level analysis to discover impact on corpus transla-
tion performance. We focus on the first three S4 ca-
tegories and separately discuss search errors (§7). In
both cases, we use exact string match to detect trans-
lation equivalences, as has been done previously in
other settings that also use word alignments to ins-
pect errors or automatically generate data for other
tasks (Blatz et al., 2004; Carpuat and Wu, 2007;
Popovi´c and Ney, 2011; Bach et al., 2011, among
others).
</bodyText>
<subsectionHeader confidence="0.875932">
5.1 Micro-analysis: WADE
</subsectionHeader>
<bodyText confidence="0.999974687500001">
We define Word Alignment Driven Evaluation, or
WADE, which is a technique for analyzing MT sys-
tem output at the word level, allowing us to (1) ma-
nually browse visualizations of MT output annota-
ted with S4 error types, and (2) aggregate counts of
errors. WADE is based on the fact that we can au-
tomatically word-align a French test sentence and
its English reference translation, and the MT deco-
der naturally produces a word alignment between a
French sentence and its machine translation. We can
then check whether the MT output has the same set
of English words aligned to each French word that
we would hope for, given the reference.
In some ways, WADE is similar to the word-based
analysis technique of Popovi´c and Ney (2011). Ho-
wever, in contrast to that work, we do not directly
align the hypothesis and reference translations but,
rather, pivot through the source text. Additionally,
we use WADE to annotate S4 errors, which are dri-
ven more by how lexical choice is made within
the SMT framework than by linguistic properties of
words in the reference and hypothesis translations.
For example, in the case of domain adaptation, we
do not expect the rate of inflectional errors to be af-
fected by domain shift.
In WADE, the unit of analysis is each word align-
ment between a French word, fi, and a reference En-
glish word, ej. To annotate the aligned pair, ai,j, we
consider the word(s), Hi, in the output English sen-
tence which are aligned (by the decoder) to fi. If ej
appears in the set Hi, then the alignment ai,j is mar-
ked correct. If not, the alignment is categorized with
one of the S4 error types. If the French word fi does
not appear in the phrase table used for translation,
then the alignment is marked as a SEEN error. If fi
does appear in the phrase table, but it is never obser-
ved translating as ej, then the alignment is marked
as a SENSE error. If fi had been observed translating
as ej, but the decoder chose an alternate translation,
then the alignment is marked as a SCORE error. Our
results in §7 show that SEARCH errors are very in-
frequent, so we mark all errors other than SEEN and
SENSE as SCORE errors. We make use of one ad-
ditional category: Freebie. Our MT system copies
unseen (aka “OOV”) French words into the English
output, and “freebies” are French words for which
this is correct.
For WADE analysis only, we use the alignments
yielded by a model trained over our train and test
datasets and the grow-diag-final heuristic. Because
WADE’s unit of analysis is each alignment link bet-
ween the source text and its reference, it ignores una-
ligned words in the input source text.
Figure 1 shows an example of a WADE-annotated
sentence. In addition to providing an easy way to vi-
sualize and browse the errors in MT output, WADE
allows us to aggregate counts over the S4 error
types. In our analysis (§7), we present results that
show not only total numbers of each error type but
also how WADE-annotations change when we intro-
duce some NEW-domain parallel training data. For
example, SEEN errors could remain SEEN errors, be-
come correct, or become SENSE or SCORE errors
when we introduce additional training data.
</bodyText>
<subsectionHeader confidence="0.977796">
5.2 Macro-analysis: TETRA
</subsectionHeader>
<bodyText confidence="0.999942333333333">
In this section, we discuss an approach to measu-
ring the effect of each potential source of error when
a translation system is considered in full. The key
</bodyText>
<page confidence="0.993411">
432
</page>
<bodyText confidence="0.999823391304348">
idea is to enhance the translation model of OLD, an
MT system trained on old domain parallel text, to
compare the impact of potential sources of improve-
ment. We use parallel new domain data to propose
enhancements to the OLD system. This provides a
realistic measure of what could be achieved if one
had access to parallel data in the new domain. The
specific system we build, called MIXED, is a linear
interpolation of a translation model trained only on
old domain data and a model trained only on new
domain data (Foster and Kuhn, 2007). The mixing
weights are selected via grid search on a tuning set,
selecting for BLEU. We call our approach TETRA:
Table Enhancement for Translation Analysis.
Below, we design experiments to tease apart the
differences in domains by adjusting the models and
enhancing OLD to be more like MIXED. We perform
different enhancements depending on the error cate-
gory we are targeting. As discussed in §6, our expe-
riments are conducted using phrase-based SMT sys-
tems, so the translation models (TM) that are enhan-
ced are the phrase table and reordering table.
Seen In order to estimate the effect of SEEN er-
rors, we enhance the TM of OLD by adding phrase
pairs that translate words found only in the new-
domain data, and we measure the BLEU improve-
ment. More precisely, we identify the set of phrase
pairs in the TM of MIXED, for which the French side
contains at least one word that does not appear in the
old-domain training data. These are the phrases res-
ponsible for the SEEN errors. We build system TE-
TRA+SEEN by adding these phrases to the TM of
OLD. When adding these phrases, we add them to-
gether with their feature value scores.
Sense Analogously, the phrases responsible for
SENSE errors are those from MIXED where the
French side exists in the phrase table of OLD, but
their English translations do not. We build TE-
TRA+SENSE by adding these phrases to OLD.
Score To isolate and measure the effect of phrase
scores, we consider the phrases that our OLD and
MIXED systems have in common: the intersection of
their translation tables. We build two systems, OLD
SCORE and NEW SCORE, with identical phrase pairs;
in OLD SCORE, the feature values are taken from the
OLD system’s tables; in NEW SCORE the feature va-
</bodyText>
<table confidence="0.999877545454545">
Domain Sentences L Tokens Types # Phrases
Hansard 8,107,356 fr 161.7m 192k 479.0m
en 144.5m 187k
News 135,838 fr 3.9m 63k 12.4m
en 3.3m 52k
EMEA fr 6.5m 35k 4.4m
472,231 en 5.9m 30k
Science fr 4.3m 118k 8.4m
139,215 en 3.6m 114k
Subs fr 155.0m 362k 364.7m
19,239,980 en 174.4m 293k
</table>
<tableCaption confidence="0.665132">
TABLE 2: Basic characteristics of the training data: Num-
ber of sentences, tokens, word types and number of
phrase pairs in the phrase tables.
</tableCaption>
<bodyText confidence="0.724058">
lues are taken from the MIXED system’s tables.
</bodyText>
<sectionHeader confidence="0.995085" genericHeader="method">
6 Experimental conditions
</sectionHeader>
<subsectionHeader confidence="0.996161">
6.1 Domains and Data
</subsectionHeader>
<bodyText confidence="0.98213484375">
We conduct our study on French-English datasets.
We consider five very different domains for which
large corpora are publicly available. The largest cor-
pus is the Hansard parliamentary proceedings. Cor-
pora in the four other domains are smaller and more
specialized, and, thus, more naturally serve as new
domains. For each new domain, we use all available
data. We do not attempt to hold the amount of new
domain data constant, as we suspect that such arti-
ficial constraints would not be sufficient to control
for the very different natures of the domains. De-
tailed statistics for the parallel corpora are given in
Table 2.
Hansard: Canadian parliamentary proceedings,
consists of manual transcriptions and translations of
meetings of Canada’s House of Commons and its
committees from 2001 to 2009. Discussions cover
a wide variety of topics, and speaking styles range
from prepared speeches by a single speaker to more
interactive discussions. It is significantly larger than
Europarl, the common source of old domain data.
EMEA: Documents from the European Medi-
cines Agency, made available with the OPUS cor-
pora collection (Tiedemann, 2009). This corpus pri-
marily consists of drug usage guidelines.
News: News commentary corpus made available
for the WMT 2009 evaluation. It has been com-
monly used in the domain adaptation literature
(Koehn and Schroeder, 2007; Foster and Kuhn,
2007; Haddow and Koehn, 2012, for instance).
Science: Parallel abstracts from scientific publi-
cations in many disciplines including physics, bio-
</bodyText>
<page confidence="0.997116">
433
</page>
<bodyText confidence="0.99997">
logy, and computer science. We collected data from
two distinct sources: (1) Canadian Science Publi-
shing made available translated abstracts from their
journals which span many research disciplines; (2)
parallel abstracts from PhD theses in Physics and
Computer Science collected from the HAL public
repository (Lambert et al., 2012).
Subs: Translated movie subtitles, available
through the OPUS corpora collection (Tiedemann,
2009). In contrast to the other domains considered,
subtitles consist of informal noisy text. 4
In this study, we use the Hansard domain as the
OLD domain, and we consider four possible NEW
domains: EMEA, News, Science and Subs. Data sets
for all domains were processed consistently. After
tokenization, we paid particular attention to norma-
lization in order to minimize artificial differences
when combining data, such as American, British and
Canadian spellings. This proved particularly impor-
tant for the news domain; the impact of SEEN redu-
ced by more than half after normalization.
</bodyText>
<subsectionHeader confidence="0.998835">
6.2 MT systems
</subsectionHeader>
<bodyText confidence="0.999809">
We build standard phrase-based SMT systems
using the Moses toolkit (Koehn et al., 2007) for all
experiments. Each system scores translation candi-
dates using standard features: 5 phrase-table fea-
tures, including phrasal translation probabilities and
lexical weights in both translation directions, and
a constant phrase penalty; 6 lexicalized reorde-
ring features, including bidirectional models built
for monotone, swap, discontinuous reorderings; 1
distance-based reordering feature; and 2 language
models, a 5-gram model learned on the OLD domain,
and a 5-gram model learned on the NEW domain.
Features are combined using a log-linear model
optimized for BLEU, using the n-best batch MIRA
algorithm (Cherry and Foster, 2012). This results in
a strong large-scale OLD system, which performs
well on the old domain and is a good starting point
for studying domain shifts. 5 The word alignments,
</bodyText>
<footnote confidence="0.920593">
4. Hansards, News and the Canadian Science Publishing are
available, respectively, at: http://www.parl.gc.ca, http:
//www.statmt.org/wmt09/translation-task.html,
and http://www.nrcresearchpress.com, preprocessed
versions and data splits used in this paper can be downloaded
from http://hal3.name/damt.
5. We use (unadapted) HMM word alignments (Vogel et
</footnote>
<bodyText confidence="0.9972414">
language models and tuning sets are kept constant
across all experiments per domain. For reference,
we built systems using NEW domain data only; these
achieved BLEU scores as follows: News = 21.70,
EMEA = 34.63, Science = 30.72, Subs = 18.51.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
7 Results
</sectionHeader>
<bodyText confidence="0.999908263157895">
Before moving on to the interesting results, we
show that SEARCH is not a major source of er-
ror. We analyzed search errors separately by com-
puting BLEU scores for each domain with varying
beam size from 10 to 1000, using the OLD sys-
tem. We find that increasing the beam from 10 to
200 yields approximately a one BLEU point ad-
vantage across all domains. Increasing it further (to
500 or 1000) does not bestow any additional advan-
tages. This suggests that for sufficiently wide beams,
search is unlikely to contribute to adaptation errors. 6
This is consistent with previous results obtained in
non-adapted settings using other measurement tech-
niques: search errors account for less than 5% of
the error in modern MT systems (Wisniewski et al.,
2010), or 0.13% for small beam settings with a “gap
constraint” (Chang and Collins, 2011). We use a
beam value of 200 for all other experiments in this
work.
</bodyText>
<subsectionHeader confidence="0.995872">
7.1 Quantitative Results
</subsectionHeader>
<bodyText confidence="0.9997559375">
Results are summarized in Tables 3 and 4. Table
3 gives an overview of our WADE analysis on test
sets in each domain translated using OLD and MIXED
models. Table 4 shows BLEU score results based on
the TETRA analysis.
We first present general observations based on
each set of results. WADE shows that for news, new
domain data helps solve only a small number of
SEEN issues, and SENSE and SCORE errors remain
essentially unchanged. TETRA agrees that SENSE
and SCORE are not issues in this domain. In general,
the OLD system performs better on news than on the
other three domains. For comparison, using the OLD
system to translate a test set in the old (Hansard) do-
main yields a BLEU score of 37.41 and, according
to our WADE analysis, 67.64% of all alignments are
</bodyText>
<footnote confidence="0.5021522">
al., 1996) in both directions, combined using grow-diag-final
(Koehn et al., 2005). We estimate alignments jointly on all data
sets. Thus, TETRA may have artificially good phrase tables.
6. This is likely dependent on language choice and the large
amount of old domain parallel data.
</footnote>
<page confidence="0.984357">
434
</page>
<table confidence="0.983724">
Domain OLD % Correct % Seen Errors %Δ % Sense Errors %Δ % Score Errors %Δ
MIXED OLD MIXED OLD MIXED OLD MIXED
News 57.42 57.77 5.73 5.38 - 6% 11.02 11.13 + 1% 25.84 25.72 + 0%
EMEA 55.97 62.60 9.28 4.01 -56% 16.16 13.76 -15% 18.59 19.63 + 6%
Science 56.20 61.50 10.22 5.63 -45% 13.58 13.11 - 3% 20.00 19.77 - 1%
Subs 55.76 59.58 5.25 1.67 -68% 13.93 9.71 -30% 25.06 29.04 +15%
TABLE 3: WADE: Percent correct, percent seen errors, percent sense errors, and percent score errors. The changes
(%A) from OLD to MIXED are also given; here, negative changes are good (error reduction).
Domain OLD +SEEN +SENSE OLD vs MIXED SCORE MIXED
News 22.81 23.87 + 0% 23.95 + 1% 23.72 23.86 + 1% 24.15
EMEA 28.69 31.02 + 8% 30.59 + 7% 28.89 30.21 + 5% 36.60
Science 26.13 27.72 + 6% 27.29 + 4% 26.09 28.68 +10% 32.23
Subs 15.10 15.96 + 6% 16.41 + 9% 14.99 16.25 + 8% 18.49
</table>
<tableCaption confidence="0.996746666666667">
TABLE 4: TETRA: Results on all new domains using OLD and MIXED models (first and last columns), OLD enhanced
with seen translations (second), sense translations (third), and scores (fourth), together with percent improvements in
terms of BLEU score. Here, positive improvements are good (higher BLEU scores).
</tableCaption>
<bodyText confidence="0.999262606060606">
correct. As in the news domain, most of the errors
are SCORE followed by SENSE and then SEEN. For
the other three domains, the two evaluation methods
agree that SEEN is a fairly substantial problem. TE-
TRA believes that SENSE is a fairly substantial is-
sue, but WADE does not show this for Science. For
SCORE, TETRA detects significant room for impro-
vement, especially for Science.
The large changes in BLEU score found with TE-
TRA are somewhat surprising given how little the
phrase tables change in each of these experimen-
tal conditions. For News, EMEA, and Science, ad-
ding unseen words results in an increase in number
of phrase pairs between 0.045% (News) and 0.3%
(Science). The sense additions were similarly small:
from 0.15% (EMEA) to 0.59% (News). For Subs the
story was different: adding unseen words amoun-
ted to a growth of 4.2% in phrase table size; sense
amounted to 25.1%. In all cases, the size of the score
phrase tables was only 0.05% smaller than that of
OLD.
At first glance, the WADE and TETRA analyses
of the SCORE error type seem to contradict each
other. The MIXED systems are worse in terms of
SCORE (positive deltas, more errors than OLD), but
have better BLEU scores. To understand this discre-
pancy, we must recognize that TETRA analyzes the
score errors in isolation: by restricting the phrase
tables to the intersection of the OLD and MIXED do-
main phrase tables, we remove all score and sense
errors. In the WADE analysis however, many errors
that “used to be” SEEN errors in the old domain be-
come SCORE errors in the new domain.
</bodyText>
<table confidence="0.995253166666667">
0.0 1.9 0.0 0.0 55.6
1.7 0.0 0.0 0.0 1.8
0.0 23.6 0.0 0.0 25.8
0.0 0.0 5.4 0.3 5.7
0.0 0.2 0.0 10.8 11.0
1.7 25.7 5.4 11.1 100
0.0 3.1 0.0 0.0 51.5
2.8 0.1 0.0 0.0 4.5
0.0 13.3 0.0 0.0 18.6
0.0 0.5 4.0 2.5 9.3
0.0 2.6 0.0 11.3 16.2
2.8 19.6 4.0 13.8 100
0.0 3.6 0.0 0.0 53.3
1.4 0.0 0.0 0.0 2.9
0.0 14.2 0.0 0.0 20.0
0.0 0.3 5.6 2.5 10.2
0.0 1.6 0.0 10.6 13.6
1.4 19.8 5.6 13.1 100
0.0 2.5 0.0 0.0 54.8
0.3 0.0 0.0 0.0 0.9
0.0 20.6 0.0 0.0 25.1
0.0 0.5 1.7 2.0 5.3
0.0 5.4 0.0 7.7 13.9
0.3 29.0 1.7 9.7 100
</table>
<figureCaption confidence="0.6658308">
TABLE 5: Percent of WADE annotation changes moving
from OLD (rows) to MIXED (columns) models, for each
domain. Non-zero off-diagonals are bolded. Seen-C in-
dicates Freebies, and Seen-I indicates unseen words that
were mistranslated.
</figureCaption>
<bodyText confidence="0.999944083333333">
To see the full picture, we must look at how the
different error categories change from the OLD sys-
tem to the MIXED system in WADE. This is shown in
Table 5. In this table, the rightmost column contains
the total percentage of errors in the OLD systems; the
rows labeled Total show the total percentage of er-
rors in the MIXED systems; the remaining cells these
errors changing from OLD to MIXED. For the news
domain, the OLD system has 25.8% SCORE errors.
Of those, 2.2% are fixed in the MIXED system.
For the three domains of interest (all except
news), addressing SEEN errors can be substantially
</bodyText>
<figure confidence="0.998500407407408">
Correct Incorrect Total
Cor Seen Score Seen Sense
News
Cor
53.7
0.1
Seen-C
Score
2.2
Seen-I
0.0
Sense
56.1
Total
48.3
EMEA
Cor
1.6
Seen-C
5.3
Score
2.3
Seen-I
2.3
Sense
59.8
Total
0.1
Science
Cor
49.8
1.4
Seen-C
5.8
Score
1.8
Seen-I
1.4
Sense
60.1
Total
52.4
Subtitles
Cor
0.6
Seen-C
4.5
Score
1.1
Seen-I
0.8
Sense
Total
59.3
</figure>
<page confidence="0.998959">
435
</page>
<bodyText confidence="0.999989763157895">
helpful, in terms of both BLEU score and the
fine-grained distinctions considered by WADE. The
more interesting conclusion, however, is that sim-
ply bringing in new words isn’t enough. Table 5
shows that in these three domains there are a sub-
stantial number of errors that transition from being
SEEN-Incorrect to SENSE-Incorrect. This indicates
that besides observing a new word, we must also ob-
serve it with all of its correct translations.
Likewise, there is a lot to be gained in BLEU by
correcting new SENSE translation errors (essentially
the same percentage as for SEEN). But this is harder
to solve. We can see in Table 5 that from the SENSE
errors of the OLD system, half become correct but
the other half become SCORE errors. So giving ap-
propriate scores to the new senses is a challenge.
This makes sense: these new sense are now “com-
peting” with old ones, and getting the interpolation
right between old and new domain tables is difficult.
For SCORE, the situation is more complicated.
Our TETRA analysis clearly indicates that there is
room for improvement. But this is based on intersec-
ted phrase tables, from which we removed seen and
sense distinctions, and in which there is no compe-
tition between phrases from the OLD and NEW sys-
tems. The WADE analysis shows a positive effect
only for Science. The data in Table 5 shows that a
lot (5.8/20) of the errors are corrected, but we also
introduce a number of additional errors (3.6% that
were correct, 0.3% that were SEEN and 1.6% that
were SENSE). Similarly, in the EMEA domain, we
fix 5% of 18% of SCORE errors but introduce 2.6%
that were new sense errors before, 0.5% that were
SEEN errors before, and make 3% additional error
on words we got right before. Subs is similar: out of
25% SCORE errors we fix 4.5%, but introduce 0.5%
from SEEN and 5.4% from SENSE, and suffer addi-
tional error on 2.5% of what we had correct before.
</bodyText>
<subsectionHeader confidence="0.997805">
7.2 Qualitative Results
</subsectionHeader>
<bodyText confidence="0.999307">
Table 7 shows examples of the French words that
WADE frequently identified as incorrectly transla-
ted by the OLD system due to SCORE or SEEN but
that were correctly translated under the MIXED sys-
tem. 7 For example, in the Science domain, ‘mesu-
res’ suffered from SCORE errors under the OLD sys-
</bodyText>
<footnote confidence="0.5802125">
7. Complete output lists are available at http://hal3.
name/damt
</footnote>
<bodyText confidence="0.999917875">
tem. While its correct translation was often ‘mea-
surements,’ the OLD system preferred its most pro-
bable translations (‘savings,’ ‘actions,’ ‘issues,’ and
‘provisions.’). Thirty of these error cases were cor-
rectly translated by the MIXED system. Similarly, in
the Science domain, the French word ‘finis,’ when
it should have been translated as ‘finite,’ was trans-
lated incorrectly due to a sense error 27 times. Its
most frequent translations under the OLD system
were ‘finish,’ ‘finished,’ and ‘more.’ The MIXED sys-
tem corrected these sense errors. We omit examples
of where seen errors made by OLD were frequently
corrected by the MIXED system because they tend to
be less interesting. Examples can be found in Daum´e
III and Jagarlamudi (2011).
We annotate the French test sentences using the
Stanford part-of-speech (POS) tagger (Toutanova
et al., 2003) and examine which POS categories
correspond to the most errors of each type. Using
the OLD system, new sense errors in the Subs do-
main are made on French nouns 40% of the time
and on verbs 35% of the time. In EMEA, 51%
are nouns and 23% are adjectives; in Science, 51%
nouns and 20% adjectives; in News, 46% nouns
and 23% verbs. Seen errors show a very similar
trend: in the Subs domain 50% are nouns and 25%
verbs; In EMEA, 48% are nouns and 37% adjec-
tives; in Science, 46% are nouns and 40% adjectives;
in News, 46% are nouns and 28% adjectives. Simi-
larly, for all domains, more score errors are made on
source nouns than any other POS category. In sum-
mary, we find that most errors correspond to source
language nouns, followed by adjectives, except for
Subs, where verbs are also commonly mistranslated
due to all error types.
Table 6 (left) shows some examples of how TE-
TRA can automatically estimate the errors due to
unseen words when moving to a new domain. For
example, the OCR error “miie” in the source sen-
tence is correctly translated as “miss” by the en-
hanced system. The enhanced phrase tables of TE-
TRA can also automatically estimate the errors due
to poor lexical choice when moving to a new do-
main, and can select a more lucid translation term.
For example, the enhanced system appropriately se-
lected “shoot” instead of “growth” in the Science
example in Table 6 (middle). When TETRA inserts
the scores from the new domain into the transla-
</bodyText>
<page confidence="0.997676">
436
</page>
<table confidence="0.992187102941177">
Medical
Inp en ce qui concerne les indications th´erapeutiques pour l’in-
Ref suffisance cardiaque, le tamm a propos´e le texte suivant:
Old &lt; traitement de l’insuffisance cardiaque congestive. &gt;
Fix regarding the therapeutic indications for heart failure, the
mah proposed the wording: “treatment of congestive heart
failure.”
for therapeutic indications for heart failure, tamm proposed
the following: treatment of congestive heart failure.
for therapeutic indications for heart failure, the mah has
suggested the following: treatment of congestive heart fai-
lure.
Science
Inp les r´esultats a` la base de cette hypoth´ese sont r´evis´es.
Ref findings that form the basis of this hypothesis are reviewed.
Old the results at the base of this assumption are reviewed.
Fix the results of this hypothesis are reviewed.
Subtitles
Inp Bonne nuit, MIIe Kenton.
Ref good night, miss kenton.
Old good night, miie kenton.
Fix good night, miss kenton.
Medical
Inp ce m´edicament est une solution lim-
pide, incolore a` jaune pˆale.
Ref this medicinal product is a clear, co-
lorless to pale yellow solution.
Old lantus is a clear, colorless to pale yel-
low.
Fix this medicine is a solution is clear, co-
lorless to pale yellow.
Science
Inp tous les traitements ont augment´e la
production de pousses.
Ref all treatments increased shoot pro-
duction.
Old all treatments have increased the pro-
duction of growth.
Fix all treatments increased shoot pro-
duction.
Subtitles
Inp Le sexe c’est naturel.
Ref sex is natural.
Old the sex is natural.
Fix sex is natural.
Medical
Inp les deux substances actives ont des
effets inverses sur la kali´emie.
Ref the two active substances have in-
verse effects on plasma potassium.
Old the two active substances are reverse
effects on the kali´emie.
Fix the two active substances have side
inverses on kali´emie.
Science
Inp par ailleurs, les constantes d’´equilibre
sont plus faibles.
Ref in contrast, the equilibrium constants
are lower.
Old furthermore, the constant balance are
lower.
Fix moreover, the equilibrium constants
are lower.
Subtitles
Inp Je bouge mieux.
Ref i move better.
Old i get better.
Fix i move better.
</table>
<tableCaption confidence="0.55904">
TABLE 6: Example MT results obtained by fixing seen errors (left), sense errors (middle) and score errors (right). In-
cludes source, a reference translation, the output of the OLD system and the output obtained via TETRA methodology.
</tableCaption>
<table confidence="0.997380095238095">
D # French Correct-E Hansard-E
Score −→ Correct
21 doit should has must needs shall requires
E 8 association combination partnership association
6 noms names names people nominee speakers
30 mesures measurements savings actions issues provisions
Sc 27 courant current knowledge knew heads-up
26 article paper standing clause order section
9 comme like as because like akin how sort
Su 5 maison house home house homes head place
4 fric money cash dough money fric bucks loot
Sense −→ Correct
9 notice leaflet informed directions notice
E 8 perfusion infusion perfusion intravenous
8 molles capsules lax limpid soft weak
27 finis finite finish finished more
Sc 18 jonctions junctions junction (only once)
10 substrats substrates corn streams area substrata
5 emmerde fuck annoying (only once)
Su 3 redites say repetitious tell covered again
3 mec man cme guy mec
</table>
<tableCaption confidence="0.781809">
TABLE 7: For score/sense errors, in (E)MEA, (Sc)ience
</tableCaption>
<bodyText confidence="0.810954666666667">
and (Su)bs, frequent French words that fall into that ca-
tegory (by WADE), as well as the corrected translations
and the most frequent OLD translations.
tion tables, the system produces translations that
take on the flavor of the new domain, yielding hi-
gher BLEU scores. This can be observed in Table 6
(right) where the TETRA-enhanced system used the
science-specific word “equilibrium” rather than the
political word “balance.”
</bodyText>
<subsectionHeader confidence="0.995489">
7.3 Results on an Adapted System
</subsectionHeader>
<bodyText confidence="0.999992217391304">
To show how WADE can be used on already adap-
ted systems, we performed a simple experiment ba-
sed on a standard adaptation technique. We used
bilingual cross-entropy difference (Axelrod et al.,
2011) to quantify the distance between each OLD do-
main sentence pair and each NEW domain. We selec-
ted the top K closest sentences for each domain. For
EMEA and Science, we set K to the size of the NEW
domain data. For Subs, this would select nearly all of
Hansard, so we arbitrarily set K = 1m. (We exclu-
ded the news domain.) We took this data, concatena-
ted it to the NEW domain data, trained full models,
and ran the WADE analysis on their outputs.
The trends across the three domains were remar-
kably similar. In all, SCORE in the adapted system
were lower by around 2% than even the MIXED ba-
seline (as much as 4% for Subs). This is likely be-
cause by excluding parts of the OLD domain most
unlike the relevant NEW domain, the correct sense is
observed more often. However, this comes at a price:
SENSE and SEEN errors go up about 1% or 2% each.
This suggests that a more fine-grained adaptation ap-
proach might achieve the best of both worlds.
</bodyText>
<sectionHeader confidence="0.844005" genericHeader="method">
8 Limiting Assumptions
</sectionHeader>
<bodyText confidence="0.999842416666667">
This paper represents a partial exploration of the
space of possible assumptions about models and
data. We cannot hope to explore the combinatorial
explosion of possibilities, and therefore have restric-
ted our analysis to the following settings:
Phrase-based models. All of our experiments are
carried out using phrase-based translation, as imple-
mented in the open-source Moses translation system
(Koehn et al., 2007) to ensure that they are repro-
ducible. Our methods are easily extended to hierar-
chical phrase-based models (Chiang, 2007). It is not
clear whether the same conclusions would hold: on
</bodyText>
<page confidence="0.995669">
437
</page>
<bodyText confidence="0.999781064516129">
the one hand, complex phrasal rules might overfit
even more badly than phrases; on the other hand,
hierarchical models might have more flexibility to
generalize structures.
Translation languages. We only translate from
French to English. This well-studied language pair
presents several advantages; large quantities of data
are publicly available in a wide variety of domains,
and standard statistical machine translation archi-
tectures yield good performance. Unlike with more
distant languages such as Chinese-English, or lan-
guages with radically different morphology or word
order such as German or Czech, we know that
the old-domain translation quality is high, and that
translation failures during domain shift can be pri-
marily attributed to domain issues rather than to pro-
blems with the SMT system.
Constant old domain. Our old domain is from
Hansards, and we only vary our new domain. It
would be interesting to consider other datasets as old
domains. We deliberately only use the Hansard data:
based on its size and scope, we assume that it yields
the most general of our SMT systems.
Monolingual new-domain data. We assume that
we always have access to monolingual English data
in the new domain for learning a domain-specific
language model. Our focus is on the effect of the
translation model; the effect of adapting language
models has been studied previously (see §3). Wi-
thout access to a new domain language model, the
effect of unseen words and words with new senses
is likely to be dramatically underestimated, because
their translations are likely to be “thrown out” by
an old-domain LM. Moreover, since SCORE er-
rors conflate language model and translation model
scores, using a new-domain language model lets us
mostly isolate the effect of the translation model.
Parallel new-domain data for tuning. We as-
sume that we always have access to a small amount
of parallel data in the new domain, essentially for the
purpose of running parameter tuning. Without this,
one would not even be able to evaluate the perfor-
mance of one’s system, typically a non-starter.
Automatic word alignments for WADE WADE
is fundamentally based upon word alignments, so
alignment errors may affect its accuracy. Such errors
are obvious in manually inspecting sentence triples
using the visualizer. When developing this tool, we
checked that alignment noise does not invalidate
conclusions drawn from WADE counts. In order to
estimate how much alignment errors affect WADE,
a French speaker manually corrected the word ali-
gnments for 955 EMEA test set sentences. The ana-
lyses based on manual experiments show fewer er-
rors overall, but the erroneous annotations appear to
be randomly distributed among all categories (de-
tails ommited for space). As a result, we believe that
WADE yields results which are informative despite
the inevitable automatic alignment errors. In particu-
lar, because alignments between a test and reference
set are held constant in a system comparison, such
errors should impact all analyses in the same way.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="conclusions">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999939206896551">
Translation performance degrades dramatically
when migrating an SMT system to a new and dif-
ferent domain. Our work demonstrates that the ma-
jority of this degradation in performance is due to
SEEN and SENSE errors: namely, unknown source-
language words and known source-language words
with unknown translations. This result holds in all
domains we studied, except for news, in which there
appears to be little adaptation influence at all (espe-
cially after spelling normalization).
Our two analysis methods: WADE (Section 5.1)
and TETRA analysis (Section 5.2), are both lenses
on the adverse affects of domain mismatch. Using
WADE, we are able to pinpoint precise translation
errors and their sources. This could be extended to
more nuanced, human-assisted, analysis of adapta-
tion effects. WADE also “labels” translations with
different error types, which could be used to train
more complex models. Using TETRA, we are able
to see how these errors affect overall translation per-
formance. In principle, this performance could be
any measure, including human assessment. We star-
ted with the BLEU metric since it is most widely
used in the community. One point of possible im-
provement would be to replace exact string match in
WADE, and BLEU in TETRA, with metrics that are
more morphologically or semantically informed.
Error analysis opens the door to building adapted
machine translation systems that directly target spe-
</bodyText>
<page confidence="0.997">
438
</page>
<bodyText confidence="0.999907666666667">
cific error categories. As we have seen, most exis-
ting domain adaptation techniques in MT aim to
improve translation quality in general, and are ac-
cordingly evaluated using corpus-level metrics such
as BLEU. Our intuitive finer-grained analysis sug-
gests that finer-grained models might be better sui-
ted to understanding and comparing the errors made
by adapted and unadapted systems. We have shown
that considering the S4 taxonomy is important: im-
proving coverage, for example, does not necessarily
improve translation quality. Translation candidates
must also be complete and must be scored correctly.
Our techniques provide an intuitive way to unders-
tand the effectiveness of new MT domain adaptation
approaches.
Acknowledgments We gratefully acknowledge
the support of the 2012 JHU Summer Workshop
and NSF Grant No 1005411, as well as the NRC
for Marine Carpuat, and DARPA CSSG Grant
D11AP00279 for Hal Daum´e III. We would like
to thank the entire DAMT team (http://hal3.name/
damt/) and Sanjeev Khudanpur for their invaluable
help and suggestions, as well as all the reviewers for
their insightful feedback.
</bodyText>
<sectionHeader confidence="0.998515" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999509628205128">
Sankaranarayanan Ananthakrishnan, Rohit Prasad, and
Prem Natarajan. 2011. On-line language model bia-
sing for statistical machine translation. In Proceedings
of the Conference of the Association for Computatio-
nal Linguistics (ACL).
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: A method for measuring machine transla-
tion confidence. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Pratyush Banerjee, Sudip Kumar Naskar, Johann Rotu-
rier, Andy Way, and Josef van Genabith. 2012. Trans-
lation quality-based supplementary data selection by
incremental update of translation models. In Procee-
dings of the International Conference on Computatio-
nal Linguistics (COLING).
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for phrase-
based SMT adaptation. International Workshop on
Spoken Language Translation (IWSLT).
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of the Interna-
tional Conference on Computational Linguistics (CO-
LING).
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Marine Carpuat, Hal Daum´e III, Katharine Henry, Ann
Irvine, Jagadeesh Jagarlamudi, and Rachel Rudinger.
2013. SenseSpotting: Never let your parallel data tie
you to an old domain. In Proceedings of the Confe-
rence of the Association for Computational Linguistics
(ACL).
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through la-
grangian relaxation. In Proceedings of the Conference
on Empirical Methods in Natural Language Proces-
sing (EMNLP).
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Hal Daum´e III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation in sta-
tistical machine translation. In Proceedings of the In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT).
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Workshop
on Statistical Machine Translation (WMT).
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on SMT systems. In
Proceedings of the Workshop on Statistical Machine
Translation (WMT).
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
</reference>
<page confidence="0.990721">
439
</page>
<reference confidence="0.999610761904762">
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the Conference
of the European Association for Computational Lin-
guistics (EACL).
Ann Irvine, Chris Quirk, and Hal Daum´e III. 2013.
Monolingual marginal matching for translation mo-
del adaptation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation (WMT).
Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris
Callison-Burch, Miles Osborne, David Talbot, and Mi-
chael White. 2005. Edinburgh system description for
the 2005 IWSLT speech translation evaluation. In Pro-
ceedings of International Workshop on Spoken Lan-
guage Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Patrik Lambert, Holger Schwenk, and Fr´ed´eric Blain.
2012. Automatic translation of scientific documents
in the HAL archive. In Proceedings of the Interna-
tional Conference on Language Resourcesand Evalua-
tion (LREC).
Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, and
Franc¸ois Yvon. 2011. LIMSI’s experiments in do-
main adaptation for IWSLT11. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT).
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by training
data selection and optimization. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Saab Mansour, Joern Wuebker, and Hermann Ney. 2011.
Combining translation and language model scoring for
domain-specific data filtering. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT).
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-Lingual Lexical Substi-
tution. In Proceedings of the 5th International Work-
shop on Semantic Evaluation.
Jan Niehues and Alex Waibel. 2010. Domain adaptation
in statistical machine translation using factored trans-
lation models. In Proceedings of the European Asso-
ciation for Machine Translation (EAMT).
Maja Popovi´c and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Computational Linguistics, 37(4).
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation mo-
dels in statistical machine translation. In Proceedings
of the Conference of the Association for Computatio-
nal Linguistics (ACL).
Rico Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical machine
translation. In Proceedings of the Conference of the
European Association for Computational Linguistics
(EACL).
J¨org Tiedemann. 2009. News from OPUS - A collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing (RANLP).
J¨org Tiedemann. 2010. To cache or not to cache? ex-
periments with adaptive models in statistical machine
translation. In Proceedings of the ACL Workshop on
Statistical Machine Translation and Metrics (MATR).
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proceedings of the Internatio-
nal Conference on Language Resourcesand Evalua-
tion (LREC).
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the International Conference
on Computational Linguistics (COLING).
Guillaume Wisniewski, Alexandre Allauzen, and
Franc¸ois Yvon. 2010. Assessing phrase-based trans-
lation models with oracle decoding. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Procee-
dings of the International Conference on Computatio-
nal Linguistics (COLING).
</reference>
<page confidence="0.998187">
440
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.611279">
<title confidence="0.999953">Measuring Machine Translation Errors in New Domains</title>
<author confidence="0.999123">Ann Irvine John Morgan</author>
<affiliation confidence="0.772451">Johns Hopkins University University of Maryland</affiliation>
<email confidence="0.997817">anni@jhu.edujjm@cs.umd.edu</email>
<author confidence="0.997818">Marine Carpuat Hal Daum´e Dragos Munteanu</author>
<affiliation confidence="0.999199">National Research Council Canada University of Maryland SDL Research</affiliation>
<email confidence="0.813448">marine.carpuat@nrc.gc.came@hal3.namedmunteanu@sdl.com</email>
<abstract confidence="0.998483866666667">We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a microlevel analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sankaranarayanan Ananthakrishnan</author>
<author>Rohit Prasad</author>
<author>Prem Natarajan</author>
</authors>
<title>On-line language model biasing for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="9036" citStr="Ananthakrishnan et al., 2011" startWordPosition="1449" endWordPosition="1452"> retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language </context>
</contexts>
<marker>Ananthakrishnan, Prasad, Natarajan, 2011</marker>
<rawString>Sankaranarayanan Ananthakrishnan, Rohit Prasad, and Prem Natarajan. 2011. On-line language model biasing for statistical machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8736" citStr="Axelrod et al., 2011" startWordPosition="1406" endWordPosition="1409">a is different for rare vs. frequent phrases. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fin</context>
<context position="38487" citStr="Axelrod et al., 2011" startWordPosition="6452" endWordPosition="6455"> fall into that category (by WADE), as well as the corrected translations and the most frequent OLD translations. tion tables, the system produces translations that take on the flavor of the new domain, yielding higher BLEU scores. This can be observed in Table 6 (right) where the TETRA-enhanced system used the science-specific word “equilibrium” rather than the political word “balance.” 7.3 Results on an Adapted System To show how WADE can be used on already adapted systems, we performed a simple experiment based on a standard adaptation technique. We used bilingual cross-entropy difference (Axelrod et al., 2011) to quantify the distance between each OLD domain sentence pair and each NEW domain. We selected the top K closest sentences for each domain. For EMEA and Science, we set K to the size of the NEW domain data. For Subs, this would select nearly all of Hansard, so we arbitrarily set K = 1m. (We excluded the news domain.) We took this data, concatenated it to the NEW domain data, trained full models, and ran the WADE analysis on their outputs. The trends across the three domains were remarkably similar. In all, SCORE in the adapted system were lower by around 2% than even the MIXED baseline (as m</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Fei Huang</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Goodness: A method for measuring machine translation confidence.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="13391" citStr="Bach et al., 2011" startWordPosition="2168" endWordPosition="2171">the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We can then check whether the MT output has the same set of English w</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: A method for measuring machine translation confidence. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pratyush Banerjee</author>
<author>Sudip Kumar Naskar</author>
<author>Johann Roturier</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Translation quality-based supplementary data selection by incremental update of translation models.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<marker>Banerjee, Naskar, Roturier, Way, van Genabith, 2012</marker>
<rawString>Pratyush Banerjee, Sudip Kumar Naskar, Johann Roturier, Andy Way, and Josef van Genabith. 2012. Translation quality-based supplementary data selection by incremental update of translation models. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus interpolation methods for phrasebased SMT adaptation.</title>
<date>2011</date>
<booktitle>International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="7064" citStr="Bisazza et al., 2011" startWordPosition="1135" endWordPosition="1138">focused on either (a) analyzing errors made by machine translation systems in a non-adaptation setting (Popovi´c and Ney, 2011), or (b) trying to directly improve machine translation performance. A small amount of work (discussed next) addresses issues of analyzing MT systems in a domain adaptation setting. 3.1 Analysis of Domain Effects To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some of our conclusions ar</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus interpolation methods for phrasebased SMT adaptation. International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="13326" citStr="Blatz et al., 2004" startWordPosition="2156" endWordPosition="2159"> 431 FIGURE 1: Example of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We c</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="13348" citStr="Carpuat and Wu, 2007" startWordPosition="2160" endWordPosition="2163">le of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We can then check whether </context>
<context position="12562" citStr="Carpuat &amp; Wu (2007)" startWordPosition="2031" endWordPosition="2034">on is important for developing methodologies for measuring the impact of each of these sources of error. Partitions of the set of errors that focus on categories other than lexical choice have been investigated by Vilar et al. (2006). 5 Methodology for Analyzing MT Systems Given the S4 taxonomy for categorizing SMT errors, it would be possible (if painstaking) to manually annotate SMT output with error types. We prefer automated methods. In this section we describe two such methods: a micro-level analysis to 3. We define “sense” as a particular translation into a target language, in line with Carpuat &amp; Wu (2007) or Mihalcea et al. (2010). This means both traditional word sense errors and other translation errors (like morphological variants) are included. 431 FIGURE 1: Example of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalenc</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Hal Daum´e Katharine Henry</author>
<author>Ann Irvine</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Rachel Rudinger</author>
</authors>
<title>SenseSpotting: Never let your parallel data tie you to an old domain.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="10358" citStr="Carpuat et al., 2013" startWordPosition="1656" endWordPosition="1659">its well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributions from comparable new domain corpora (Irvine et al., 2013). Earlier, Daum´e III and Jagarlamudi (2011) showed how mining translations for unseen words from comparable corpora can improve SMT in a new domain. 4 The S4 Taxonomy We begin with a simple question: when we move an SMT system from an old domain to a new domain, what goes wrong? We employ a set of four error types as our taxonomy. We refer to these error types as SEEN, SENSE, SCORE and SEARCH, and together as the S4 taxonomy: SEEN: an attempt to translate a source wor</context>
</contexts>
<marker>Carpuat, Henry, Irvine, Jagarlamudi, Rudinger, 2013</marker>
<rawString>Marine Carpuat, Hal Daum´e III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel Rudinger. 2013. SenseSpotting: Never let your parallel data tie you to an old domain. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yin-Wen Chang</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="24419" citStr="Chang and Collins, 2011" startWordPosition="4008" endWordPosition="4011"> 1000, using the OLD system. We find that increasing the beam from 10 to 200 yields approximately a one BLEU point advantage across all domains. Increasing it further (to 500 or 1000) does not bestow any additional advantages. This suggests that for sufficiently wide beams, search is unlikely to contribute to adaptation errors. 6 This is consistent with previous results obtained in non-adapted settings using other measurement techniques: search errors account for less than 5% of the error in modern MT systems (Wisniewski et al., 2010), or 0.13% for small beam settings with a “gap constraint” (Chang and Collins, 2011). We use a beam value of 200 for all other experiments in this work. 7.1 Quantitative Results Results are summarized in Tables 3 and 4. Table 3 gives an overview of our WADE analysis on test sets in each domain translated using OLD and MIXED models. Table 4 shows BLEU score results based on the TETRA analysis. We first present general observations based on each set of results. WADE shows that for news, new domain data helps solve only a small number of SEEN issues, and SENSE and SCORE errors remain essentially unchanged. TETRA agrees that SENSE and SCORE are not issues in this domain. In gener</context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Yin-Wen Chang and Michael Collins. 2011. Exact decoding of phrase-based translation models through lagrangian relaxation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="22811" citStr="Cherry and Foster, 2012" startWordPosition="3753" endWordPosition="3756">iments. Each system scores translation candidates using standard features: 5 phrase-table features, including phrasal translation probabilities and lexical weights in both translation directions, and a constant phrase penalty; 6 lexicalized reordering features, including bidirectional models built for monotone, swap, discontinuous reorderings; 1 distance-based reordering feature; and 2 language models, a 5-gram model learned on the OLD domain, and a 5-gram model learned on the NEW domain. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm (Cherry and Foster, 2012). This results in a strong large-scale OLD system, which performs well on the old domain and is a good starting point for studying domain shifts. 5 The word alignments, 4. Hansards, News and the Canadian Science Publishing are available, respectively, at: http://www.parl.gc.ca, http: //www.statmt.org/wmt09/translation-task.html, and http://www.nrcresearchpress.com, preprocessed versions and data splits used in this paper can be downloaded from http://hal3.name/damt. 5. We use (unadapted) HMM word alignments (Vogel et language models and tuning sets are kept constant across all experiments per </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="39987" citStr="Chiang, 2007" startWordPosition="6716" endWordPosition="6717">ion approach might achieve the best of both worlds. 8 Limiting Assumptions This paper represents a partial exploration of the space of possible assumptions about models and data. We cannot hope to explore the combinatorial explosion of possibilities, and therefore have restricted our analysis to the following settings: Phrase-based models. All of our experiments are carried out using phrase-based translation, as implemented in the open-source Moses translation system (Koehn et al., 2007) to ensure that they are reproducible. Our methods are easily extended to hierarchical phrase-based models (Chiang, 2007). It is not clear whether the same conclusions would hold: on 437 the one hand, complex phrasal rules might overfit even more badly than phrases; on the other hand, hierarchical models might have more flexibility to generalize structures. Translation languages. We only translate from French to English. This well-studied language pair presents several advantages; large quantities of data are publicly available in a wide variety of domains, and standard statistical machine translation architectures yield good performance. Unlike with more distant languages such as Chinese-English, or languages w</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>Hal Daum´e III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Analysis of translation model adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="7042" citStr="Duh et al., 2010" startWordPosition="1131" endWordPosition="1134"> related work has focused on either (a) analyzing errors made by machine translation systems in a non-adaptation setting (Popovi´c and Ney, 2011), or (b) trying to directly improve machine translation performance. A small amount of work (discussed next) addresses issues of analyzing MT systems in a domain adaptation setting. 3.1 Analysis of Domain Effects To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some</context>
<context position="8595" citStr="Duh et al., 2010" startWordPosition="1380" endWordPosition="1383">ections for refining our error categories: for instance, Haddow and Koehn (2012) show that the impact of additional new or old domain data is different for rare vs. frequent phrases. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperfor</context>
</contexts>
<marker>Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Analysis of translation model adaptation in statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT).</booktitle>
<contexts>
<context position="9272" citStr="Foster and Kuhn, 2007" startWordPosition="1485" endWordPosition="1488">nguage model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an </context>
<context position="17299" citStr="Foster and Kuhn, 2007" startWordPosition="2860" endWordPosition="2863">ntial source of error when a translation system is considered in full. The key 432 idea is to enhance the translation model of OLD, an MT system trained on old domain parallel text, to compare the impact of potential sources of improvement. We use parallel new domain data to propose enhancements to the OLD system. This provides a realistic measure of what could be achieved if one had access to parallel data in the new domain. The specific system we build, called MIXED, is a linear interpolation of a translation model trained only on old domain data and a model trained only on new domain data (Foster and Kuhn, 2007). The mixing weights are selected via grid search on a tuning set, selecting for BLEU. We call our approach TETRA: Table Enhancement for Translation Analysis. Below, we design experiments to tease apart the differences in domains by adjusting the models and enhancing OLD to be more like MIXED. We perform different enhancements depending on the error category we are targeting. As discussed in §6, our experiments are conducted using phrase-based SMT systems, so the translation models (TM) that are enhanced are the phrase table and reordering table. Seen In order to estimate the effect of SEEN er</context>
<context position="20913" citStr="Foster and Kuhn, 2007" startWordPosition="3472" endWordPosition="3475"> its committees from 2001 to 2009. Discussions cover a wide variety of topics, and speaking styles range from prepared speeches by a single speaker to more interactive discussions. It is significantly larger than Europarl, the common source of old domain data. EMEA: Documents from the European Medicines Agency, made available with the OPUS corpora collection (Tiedemann, 2009). This corpus primarily consists of drug usage guidelines. News: News commentary corpus made available for the WMT 2009 evaluation. It has been commonly used in the domain adaptation literature (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Haddow and Koehn, 2012, for instance). Science: Parallel abstracts from scientific publications in many disciplines including physics, bio433 logy, and computer science. We collected data from two distinct sources: (1) Canadian Science Publishing made available translated abstracts from their journals which span many research disciplines; (2) parallel abstracts from PhD theses in Physics and Computer Science collected from the HAL public repository (Lambert et al., 2012). Subs: Translated movie subtitles, available through the OPUS corpora collection (Tiedemann, 2009). In contrast to the oth</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8996" citStr="Foster et al., 2010" startWordPosition="1444" endWordPosition="1447">is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been use</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8577" citStr="Gong et al., 2011" startWordPosition="1376" endWordPosition="1379">ggest potential directions for refining our error categories: for instance, Haddow and Koehn (2012) show that the impact of additional new or old domain data is different for rare vs. frequent phrases. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pai</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Analysing the effect of out-of-domain data on SMT systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT).</booktitle>
<contexts>
<context position="7089" citStr="Haddow and Koehn, 2012" startWordPosition="1139" endWordPosition="1142">analyzing errors made by machine translation systems in a non-adaptation setting (Popovi´c and Ney, 2011), or (b) trying to directly improve machine translation performance. A small amount of work (discussed next) addresses issues of analyzing MT systems in a domain adaptation setting. 3.1 Analysis of Domain Effects To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some of our conclusions are consistent with previou</context>
<context position="20937" citStr="Haddow and Koehn, 2012" startWordPosition="3476" endWordPosition="3479">01 to 2009. Discussions cover a wide variety of topics, and speaking styles range from prepared speeches by a single speaker to more interactive discussions. It is significantly larger than Europarl, the common source of old domain data. EMEA: Documents from the European Medicines Agency, made available with the OPUS corpora collection (Tiedemann, 2009). This corpus primarily consists of drug usage guidelines. News: News commentary corpus made available for the WMT 2009 evaluation. It has been commonly used in the domain adaptation literature (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Haddow and Koehn, 2012, for instance). Science: Parallel abstracts from scientific publications in many disciplines including physics, bio433 logy, and computer science. We collected data from two distinct sources: (1) Canadian Science Publishing made available translated abstracts from their journals which span many research disciplines; (2) parallel abstracts from PhD theses in Physics and Computer Science collected from the HAL public repository (Lambert et al., 2012). Subs: Translated movie subtitles, available through the OPUS corpora collection (Tiedemann, 2009). In contrast to the other domains considered, s</context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on SMT systems. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="8541" citStr="Hildebrand et al., 2005" startWordPosition="1368" endWordPosition="1371">lex issue. Finally, these other studies suggest potential directions for refining our error categories: for instance, Haddow and Koehn (2012) show that the impact of additional new or old domain data is different for rare vs. frequent phrases. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) </context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Quirk</author>
<author>Hal Daum´e</author>
</authors>
<title>Monolingual marginal matching for translation model adaptation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Irvine, Quirk, Daum´e, 2013</marker>
<rawString>Ann Irvine, Chris Quirk, and Hal Daum´e III. 2013. Monolingual marginal matching for translation model adaptation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT).</booktitle>
<contexts>
<context position="9826" citStr="Koehn and Schroeder, 2007" startWordPosition="1569" endWordPosition="1572">lier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributio</context>
<context position="20890" citStr="Koehn and Schroeder, 2007" startWordPosition="3468" endWordPosition="3471">nada’s House of Commons and its committees from 2001 to 2009. Discussions cover a wide variety of topics, and speaking styles range from prepared speeches by a single speaker to more interactive discussions. It is significantly larger than Europarl, the common source of old domain data. EMEA: Documents from the European Medicines Agency, made available with the OPUS corpora collection (Tiedemann, 2009). This corpus primarily consists of drug usage guidelines. News: News commentary corpus made available for the WMT 2009 evaluation. It has been commonly used in the domain adaptation literature (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Haddow and Koehn, 2012, for instance). Science: Parallel abstracts from scientific publications in many disciplines including physics, bio433 logy, and computer science. We collected data from two distinct sources: (1) Canadian Science Publishing made available translated abstracts from their journals which span many research disciplines; (2) parallel abstracts from PhD theses in Physics and Computer Science collected from the HAL public repository (Lambert et al., 2012). Subs: Translated movie subtitles, available through the OPUS corpora collection (Tiedemann, 2009).</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
<author>Michael White</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="25361" citStr="Koehn et al., 2005" startWordPosition="4174" endWordPosition="4177">esent general observations based on each set of results. WADE shows that for news, new domain data helps solve only a small number of SEEN issues, and SENSE and SCORE errors remain essentially unchanged. TETRA agrees that SENSE and SCORE are not issues in this domain. In general, the OLD system performs better on news than on the other three domains. For comparison, using the OLD system to translate a test set in the old (Hansard) domain yields a BLEU score of 37.41 and, according to our WADE analysis, 67.64% of all alignments are al., 1996) in both directions, combined using grow-diag-final (Koehn et al., 2005). We estimate alignments jointly on all data sets. Thus, TETRA may have artificially good phrase tables. 6. This is likely dependent on language choice and the large amount of old domain parallel data. 434 Domain OLD % Correct % Seen Errors %Δ % Sense Errors %Δ % Score Errors %Δ MIXED OLD MIXED OLD MIXED OLD MIXED News 57.42 57.77 5.73 5.38 - 6% 11.02 11.13 + 1% 25.84 25.72 + 0% EMEA 55.97 62.60 9.28 4.01 -56% 16.16 13.76 -15% 18.59 19.63 + 6% Science 56.20 61.50 10.22 5.63 -45% 13.58 13.11 - 3% 20.00 19.77 - 1% Subs 55.76 59.58 5.25 1.67 -68% 13.93 9.71 -30% 25.06 29.04 +15% TABLE 3: WADE: Pe</context>
</contexts>
<marker>Koehn, Axelrod, Birch, Callison-Burch, Osborne, Talbot, White, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris Callison-Burch, Miles Osborne, David Talbot, and Michael White. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="22173" citStr="Koehn et al., 2007" startWordPosition="3662" endWordPosition="3665">of informal noisy text. 4 In this study, we use the Hansard domain as the OLD domain, and we consider four possible NEW domains: EMEA, News, Science and Subs. Data sets for all domains were processed consistently. After tokenization, we paid particular attention to normalization in order to minimize artificial differences when combining data, such as American, British and Canadian spellings. This proved particularly important for the news domain; the impact of SEEN reduced by more than half after normalization. 6.2 MT systems We build standard phrase-based SMT systems using the Moses toolkit (Koehn et al., 2007) for all experiments. Each system scores translation candidates using standard features: 5 phrase-table features, including phrasal translation probabilities and lexical weights in both translation directions, and a constant phrase penalty; 6 lexicalized reordering features, including bidirectional models built for monotone, swap, discontinuous reorderings; 1 distance-based reordering feature; and 2 language models, a 5-gram model learned on the OLD domain, and a 5-gram model learned on the NEW domain. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MI</context>
<context position="39866" citStr="Koehn et al., 2007" startWordPosition="6695" endWordPosition="6698">However, this comes at a price: SENSE and SEEN errors go up about 1% or 2% each. This suggests that a more fine-grained adaptation approach might achieve the best of both worlds. 8 Limiting Assumptions This paper represents a partial exploration of the space of possible assumptions about models and data. We cannot hope to explore the combinatorial explosion of possibilities, and therefore have restricted our analysis to the following settings: Phrase-based models. All of our experiments are carried out using phrase-based translation, as implemented in the open-source Moses translation system (Koehn et al., 2007) to ensure that they are reproducible. Our methods are easily extended to hierarchical phrase-based models (Chiang, 2007). It is not clear whether the same conclusions would hold: on 437 the one hand, complex phrasal rules might overfit even more badly than phrases; on the other hand, hierarchical models might have more flexibility to generalize structures. Translation languages. We only translate from French to English. This well-studied language pair presents several advantages; large quantities of data are publicly available in a wide variety of domains, and standard statistical machine tra</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Holger Schwenk</author>
<author>Fr´ed´eric Blain</author>
</authors>
<title>Automatic translation of scientific documents in the HAL archive.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Language Resourcesand Evaluation (LREC).</booktitle>
<contexts>
<context position="21390" citStr="Lambert et al., 2012" startWordPosition="3541" endWordPosition="3544">ble for the WMT 2009 evaluation. It has been commonly used in the domain adaptation literature (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Haddow and Koehn, 2012, for instance). Science: Parallel abstracts from scientific publications in many disciplines including physics, bio433 logy, and computer science. We collected data from two distinct sources: (1) Canadian Science Publishing made available translated abstracts from their journals which span many research disciplines; (2) parallel abstracts from PhD theses in Physics and Computer Science collected from the HAL public repository (Lambert et al., 2012). Subs: Translated movie subtitles, available through the OPUS corpora collection (Tiedemann, 2009). In contrast to the other domains considered, subtitles consist of informal noisy text. 4 In this study, we use the Hansard domain as the OLD domain, and we consider four possible NEW domains: EMEA, News, Science and Subs. Data sets for all domains were processed consistently. After tokenization, we paid particular attention to normalization in order to minimize artificial differences when combining data, such as American, British and Canadian spellings. This proved particularly important for th</context>
</contexts>
<marker>Lambert, Schwenk, Blain, 2012</marker>
<rawString>Patrik Lambert, Holger Schwenk, and Fr´ed´eric Blain. 2012. Automatic translation of scientific documents in the HAL archive. In Proceedings of the International Conference on Language Resourcesand Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Alexandre Allauzen</author>
<author>Hai-Son Le</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>LIMSI’s experiments in domain adaptation for IWSLT11.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="9711" citStr="Lavergne et al., 2011" startWordPosition="1551" endWordPosition="1554"> Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a n</context>
</contexts>
<marker>Lavergne, Allauzen, Le, Yvon, 2011</marker>
<rawString>Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, and Franc¸ois Yvon. 2011. LIMSI’s experiments in domain adaptation for IWSLT11. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="8558" citStr="Lu et al., 2007" startWordPosition="1372" endWordPosition="1375"> other studies suggest potential directions for refining our error categories: for instance, Haddow and Koehn (2012) show that the impact of additional new or old domain data is different for rare vs. frequent phrases. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adaptin</context>
</contexts>
<marker>Lu, Huang, Liu, 2007</marker>
<rawString>Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saab Mansour</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Combining translation and language model scoring for domain-specific data filtering.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="8758" citStr="Mansour et al., 2011" startWordPosition="1410" endWordPosition="1413">e vs. frequent phrases. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on tr</context>
</contexts>
<marker>Mansour, Wuebker, Ney, 2011</marker>
<rawString>Saab Mansour, Joern Wuebker, and Hermann Ney. 2011. Combining translation and language model scoring for domain-specific data filtering. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8960" citStr="Matsoukas et al., 2009" startWordPosition="1438" endWordPosition="1441"> been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech r</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ravi Sinha</author>
<author>Diana McCarthy</author>
</authors>
<title>SemEval-2010 Task 2: Cross-Lingual Lexical Substitution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="12588" citStr="Mihalcea et al. (2010)" startWordPosition="2036" endWordPosition="2039">eloping methodologies for measuring the impact of each of these sources of error. Partitions of the set of errors that focus on categories other than lexical choice have been investigated by Vilar et al. (2006). 5 Methodology for Analyzing MT Systems Given the S4 taxonomy for categorizing SMT errors, it would be possible (if painstaking) to manually annotate SMT output with error types. We prefer automated methods. In this section we describe two such methods: a micro-level analysis to 3. We define “sense” as a particular translation into a target language, in line with Carpuat &amp; Wu (2007) or Mihalcea et al. (2010). This means both traditional word sense errors and other translation errors (like morphological variants) are included. 431 FIGURE 1: Example of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previ</context>
</contexts>
<marker>Mihalcea, Sinha, McCarthy, 2010</marker>
<rawString>Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. SemEval-2010 Task 2: Cross-Lingual Lexical Substitution. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Domain adaptation in statistical machine translation using factored translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Association for Machine Translation (EAMT).</booktitle>
<contexts>
<context position="9103" citStr="Niehues and Waibel, 2010" startWordPosition="1458" endWordPosition="1461">o et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al</context>
</contexts>
<marker>Niehues, Waibel, 2010</marker>
<rawString>Jan Niehues and Alex Waibel. 2010. Domain adaptation in statistical machine translation using factored translation models. In Proceedings of the European Association for Machine Translation (EAMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Towards automatic error analysis of machine translation output.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Popovi´c, Ney, 2011</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2011. Towards automatic error analysis of machine translation output. Computational Linguistics, 37(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>George Foster</author>
<author>Baskaran Sankaran</author>
<author>Anoop Sarkar</author>
</authors>
<title>Mixing multiple translation models in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="10096" citStr="Razmara et al. (2012)" startWordPosition="1612" endWordPosition="1615">b-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributions from comparable new domain corpora (Irvine et al., 2013). Earlier, Daum´e III and Jagarlamudi (2011) showed how mining translations for unseen words from comparable corpora can improve SMT in a new domain. 4 The S4 Taxonomy We begin with a simple question: when we mo</context>
</contexts>
<marker>Razmara, Foster, Sankaran, Sarkar, 2012</marker>
<rawString>Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models in statistical machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="8775" citStr="Sennrich, 2012" startWordPosition="1414" endWordPosition="1415">. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing (RANLP).</booktitle>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<contexts>
<context position="20670" citStr="Tiedemann, 2009" startWordPosition="3435" endWordPosition="3436">ferent natures of the domains. Detailed statistics for the parallel corpora are given in Table 2. Hansard: Canadian parliamentary proceedings, consists of manual transcriptions and translations of meetings of Canada’s House of Commons and its committees from 2001 to 2009. Discussions cover a wide variety of topics, and speaking styles range from prepared speeches by a single speaker to more interactive discussions. It is significantly larger than Europarl, the common source of old domain data. EMEA: Documents from the European Medicines Agency, made available with the OPUS corpora collection (Tiedemann, 2009). This corpus primarily consists of drug usage guidelines. News: News commentary corpus made available for the WMT 2009 evaluation. It has been commonly used in the domain adaptation literature (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Haddow and Koehn, 2012, for instance). Science: Parallel abstracts from scientific publications in many disciplines including physics, bio433 logy, and computer science. We collected data from two distinct sources: (1) Canadian Science Publishing made available translated abstracts from their journals which span many research disciplines; (2) parallel a</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>To cache or not to cache? experiments with adaptive models in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation and Metrics (MATR).</booktitle>
<contexts>
<context position="9687" citStr="Tiedemann, 2010" startWordPosition="1548" endWordPosition="1550">dels (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying wh</context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010. To cache or not to cache? experiments with adaptive models in statistical machine translation. In Proceedings of the ACL Workshop on Statistical Machine Translation and Metrics (MATR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="33084" citStr="Toutanova et al., 2003" startWordPosition="5554" endWordPosition="5557">XED system. Similarly, in the Science domain, the French word ‘finis,’ when it should have been translated as ‘finite,’ was translated incorrectly due to a sense error 27 times. Its most frequent translations under the OLD system were ‘finish,’ ‘finished,’ and ‘more.’ The MIXED system corrected these sense errors. We omit examples of where seen errors made by OLD were frequently corrected by the MIXED system because they tend to be less interesting. Examples can be found in Daum´e III and Jagarlamudi (2011). We annotate the French test sentences using the Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) and examine which POS categories correspond to the most errors of each type. Using the OLD system, new sense errors in the Subs domain are made on French nouns 40% of the time and on verbs 35% of the time. In EMEA, 51% are nouns and 23% are adjectives; in Science, 51% nouns and 20% adjectives; in News, 46% nouns and 23% verbs. Seen errors show a very similar trend: in the Subs domain 50% are nouns and 25% verbs; In EMEA, 48% are nouns and 37% adjectives; in Science, 46% are nouns and 40% adjectives; in News, 46% are nouns and 28% adjectives. Similarly, for all domains, more score errors are m</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error analysis of statistical machine translation output.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resourcesand Evaluation (LREC).</booktitle>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error analysis of statistical machine translation output. In Proceedings of the International Conference on Language Resourcesand Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Assessing phrase-based translation models with oracle decoding.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="24335" citStr="Wisniewski et al., 2010" startWordPosition="3994" endWordPosition="3997">eparately by computing BLEU scores for each domain with varying beam size from 10 to 1000, using the OLD system. We find that increasing the beam from 10 to 200 yields approximately a one BLEU point advantage across all domains. Increasing it further (to 500 or 1000) does not bestow any additional advantages. This suggests that for sufficiently wide beams, search is unlikely to contribute to adaptation errors. 6 This is consistent with previous results obtained in non-adapted settings using other measurement techniques: search errors account for less than 5% of the error in modern MT systems (Wisniewski et al., 2010), or 0.13% for small beam settings with a “gap constraint” (Chang and Collins, 2011). We use a beam value of 200 for all other experiments in this work. 7.1 Quantitative Results Results are summarized in Tables 3 and 4. Table 3 gives an overview of our WADE analysis on test sets in each domain translated using OLD and MIXED models. Table 4 shows BLEU score results based on the TETRA analysis. We first present general observations based on each set of results. WADE shows that for news, new domain data helps solve only a small number of SEEN issues, and SENSE and SCORE errors remain essentially </context>
</contexts>
<marker>Wisniewski, Allauzen, Yvon, 2010</marker>
<rawString>Guillaume Wisniewski, Alexandre Allauzen, and Franc¸ois Yvon. 2010. Assessing phrase-based translation models with oracle decoding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="8493" citStr="Zhao et al., 2004" startWordPosition="1360" endWordPosition="1363">reviously unseen words and senses is a complex issue. Finally, these other studies suggest potential directions for refining our error categories: for instance, Haddow and Koehn (2012) show that the impact of additional new or old domain data is different for rare vs. frequent phrases. 3.2 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. 430 Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Wai</context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>