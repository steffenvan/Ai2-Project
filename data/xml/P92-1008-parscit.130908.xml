<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<sectionHeader confidence="0.914162333333333" genericHeader="abstract">
INTEGRATING MULTIPLE KNOWLEDGE SOURCES FOR
DETECTION AND CORRECTION OF REPAIRS IN
HUMAN-COMPUTER DIALOG*
</sectionHeader>
<author confidence="0.609625">
John Bear, John Dowding, Elizabeth Shribergt
</author>
<affiliation confidence="0.518682">
SRI International
</affiliation>
<address confidence="0.735049">
Menlo Park, California 94025
</address>
<email confidence="0.514083">
ABSTRACT
</email>
<bodyText confidence="0.999865555555556">
We have analyzed 607 sentences of sponta-
neous human-computer speech data containing re-
pairs, drawn from a total corpus of 10,718 sen-
tences. We present here criteria and techniques for
automatically detecting the presence of a repair,
its location, and making the appropriate correc-
tion. The criteria involve integration of knowledge
from several sources: pattern matching, syntactic
and semantic analysis, and acoustics.
</bodyText>
<sectionHeader confidence="0.997869" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999914222222222">
Spontaneous spoken language often includes
speech that is not intended by the speaker to be
part of the content of the utterance. This speech
must be detected and deleted in order to correctly
identify the intended meaning. The broad class
of disfluencies encompasses a number of phenom-
ena, including word fragments, interjections, filled
pauses, restarts, and repairs. We are analyzing
the repairs in a large subset (over ten thousand
sentences) of spontaneous speech data collected
for the DARPA Spoken Language Program.&apos; We
have categorized these disfluencies as to type and
frequency, and are investigating methods for their
automatic detection and correction. Here we re-
port promising results on detection and correction
of repairs by combining pattern matching, syn-
tactic and semantic analysis, and acoustics. This
paper extends work reported in an earlier paper
</bodyText>
<footnote confidence="0.9198082">
*This research was supported by the Defense Advanced
Research Projects Agency under Contract ONR N00014-
90-C-0085 with the Office of Naval Research. It was also
supported by a Grant, NSF IRI-8905249, from the National
Science Foundation. The views and conclusions contained
in this document are those of the authors and should not
be interpreted as necessarily representing the official poli-
cies, either expressed or implied, of the Defense Advanced
Research Projects Agency of the U.S. Government, or of
the National Science Foundation.
t Elizabeth Shriberg is also affiliated with the Depart-
ment of Psychology at the University of California at
Berkeley.
1DARPA is the Defense Advanced Research Projects
Agency of the United States Goverrunent
</footnote>
<bodyText confidence="0.982643605263158">
(Shriberg et al., 1992a).
The problem of disfluent speech for language
understanding systems has been noted but has
received limited attention. Hindle (1983) at-
tempts to delimit and correct repairs in sponta-
neous human-human dialog, based on transcripts
containing an &amp;quot;edit signal,&amp;quot; or external and reli-
able marker at the &amp;quot;expunction point,&amp;quot; or point of
interruption. Carbonell and Hayes (1983) briefly
describe recovery strategies for broken-off and
restarted utterances in textual input. Ward (1991)
addresses repairs in spontaneous speech, but does
not attempt to identify or correct them. Our ap-
proach is most similar to that of Hindle. It differs,
however, in that we make no assumption about
the existence of an explicit edit signal. As a reli-
able edit signal has yet to be found, we take it as
our problem to find the site of the repair automat-
ically.
It is the case, however, that cues to repair exist
over a range of syllables. Research in speech pro-
duction has shown that repairs tend to be marked
prosodically (Levelt and Cutler, 1983) and there
is perceptual evidence from work using lowpass-
filtered speech that human listeners can detect the
occurrence of a repair in the absence of segmental
information (Lickley, 1991).
In the sections that follow, we describe in de-
tail our corpus of spontaneous speech data and
present an analysis of the repair phenomena ob-
served. In addition, we describe ways in which
pattern matching, syntactic and semantic a.naly-
sis, and acoustic analysis can be helpful in detect-
ing and correcting these repairs. We use pattern
matching to determine an initial set of possible
repairs; we then apply information from syntac-
tic, semantic, and acoustic analyses to distinguish
actual repairs from false positives.
</bodyText>
<page confidence="0.995386">
56
</page>
<sectionHeader confidence="0.491367" genericHeader="method">
THE CORPUS
</sectionHeader>
<bodyText confidence="0.999699545454546">
The data we are analyzing were collected
as part of DARPA&apos;s Spoken Language Systems
project. The corpus contains digitized waveforms
and transcriptions of a large number of sessions in
which subjects made air travel plans using a com-
puter. In the majority of sessions, data were col-
lected in a Wizard of Oz setting, in which subjects
were led to believe they were talking to a com-
puter, but in which a human actually interpreted
and responded to queries. In a small portion of
the sessions, data were collected using SRI&apos;s Spo-
ken Language System (Shriberg et al., 1992b), in
which no human intervention was involved. Rel-
evant to the current paper is the fact that al-
though the speech was spontaneous, it was some-
what planned (subjects pressed a button to begin
speaking to the system) and the transcribers who
produced lexical transcriptions of the sessions were
instructed to mark words they inferred were ver-
bally deleted by the speaker with special symbols.
For further description of the corpus, see MAD-
COW (1992).
</bodyText>
<sectionHeader confidence="0.88377" genericHeader="method">
NOTATION
</sectionHeader>
<bodyText confidence="0.986478310344828">
In order to classify these repairs, and to facil-
itate communication among the authors, it was
necessary to develop a notational system that
would: (1) be relatively simple, (2) capture suf-
ficient detail, and (3) describe the vast majority
of repairs observed. Table 1 shows examples of
the notation used, which is described fully in Bear
et al. (1992).
The basic aspects of the notation include
marking the interruption point, the extent of
the repair, and relevant correspondences between
words in the region. To mark the site of a re-
pair, corresponding to Hindle&apos;s &amp;quot;edit signal&amp;quot; (un-
die, 1983), we use a vertical bar (I). To express
the notion that words on one side of the repair
correspond to words on the other, we use a com-
bination of a letter plus a numerical index. The
letter M indicates that two words match exactly.
R indicates that the second of the two words
was intended by the speaker to replace the first.
The two words must be similar—either of the same
lexical category, or morphological variants of the
same base form (including contraction pairs like
&amp;quot;I/I&apos;d&amp;quot;). Any other word within a repair is no-
tated with X. A hyphen affixed to a symbol in-
dicates a word fragment. In addition, certain cue
words, such as &amp;quot;sorry&amp;quot; or &amp;quot;oops&amp;quot; (marked with
CR) as well as filled pauses (CF) are also labeled
want fl- flights to boston.
</bodyText>
<equation confidence="0.921312">
M1- I M1
what what are the fares
M1 I M1
show me flights daily flights
M1 X MI
I want a flight one way flight
X X Mi
I want to leave depart before
Ri I R1
what are what are the fares
M1 M2 I Mi M2
... fly to boston from boston
R1 M1 Ri
... fly from boston from denver
R1 M1 R1
what are are there any flights
X X I
</equation>
<tableCaption confidence="0.968621">
Table 1: Examples of Notation
</tableCaption>
<bodyText confidence="0.9819275">
if they occur immediately before the site of a re-
pair.
</bodyText>
<sectionHeader confidence="0.965663" genericHeader="method">
DISTRIBUTION
</sectionHeader>
<bodyText confidence="0.999850066666667">
Of the 10,000 sentences in our corpus, 607 con-
tained repairs. We found that 10% of sentences
longer than nine words contained repairs. In con-
trast, Levelt (1983) reports a repair rate of 34% for
human-human dialog. While the rates in this cor-
pus are lower, they are still high enough to be sig-
nificant. And, as system developers move toward
more closely modeling human-human interaction,
the percentage is likely to rise.
Although only 607 sentences contained dele-
tions, some sentences contained more than one,
for a total of 646 deletions. Table 2 gives the
breakdown of deletions by length, where length
is defined as the number of consecutive deleted
words or word fragments. Most of the deletions
</bodyText>
<table confidence="0.974027285714286">
Deletion Length Occurrences Percentage
1 376 59%
2 154 24%
3 52 8%
4 25 4%
5 23 4%
6+ 16 3%
</table>
<tableCaption confidence="0.99601">
Table 2: Distribution of Repairs by Length
</tableCaption>
<page confidence="0.988742">
57
</page>
<figure confidence="0.827983666666667">
Length 2 Repairs
Repeats MI 11/2 I MI M2 28%
Replace 2nd MI RI MI RI 27%
Insertions M1M2 I MI XI • • Xi M2 19%
Replace 1st RI MI I RI MI 10%
Other 17%
</figure>
<tableCaption confidence="0.980636">
Table 3: Distribution of Repairs by Type
</tableCaption>
<bodyText confidence="0.99997064">
were fairly short; deletions of one or two words ac-
counted for 82% of the data. We categorized the
length 1 and length 2 repairs according to their
transcriptions. The results are summarized in Ta-
ble 3. For simplicity, in this table we have counted
fragments (which always occurred as the second
deleted word) as whole words. The overall rate of
fragments for the length 2 repairs was 34%.
A major repair type involved matching strings
of identical words. More than half (339 out of 436)
of the nontrivial repairs (more editing necessary
than deleting fragments and filled pauses) in the
corpus were of this type. Table 4 shows the distri-
butions of these repairs with respect to two param-
eters: the length in words of the matched string,
and the number of words between the two matched
strings. Numbers in parentheses indicate the num-
ber of occurrences, and probabilities represent the
likelihood that the phrase was actually a repair
and not a false positive. Two trends emerge from
these data. First, the longer the matched string,
the more likely the phrase was a repair. Second,
the more words there were intervening between the
matched strings, the less likely the phrase was a
repair.
</bodyText>
<sectionHeader confidence="0.977937" genericHeader="method">
SIMPLE PATTERN MATCHING
</sectionHeader>
<bodyText confidence="0.9996195">
We analyzed a subset of 607 sentences con-
taining repairs and concluded that certain sim-
ple pattern-matching techniques could successfully
detect a number of them. The pattern-matching
</bodyText>
<figure confidence="0.981382148148148">
Type Pattern Freq.
Length 1 Repairs
Fragments
Repeats
Insertions
Replacement
Other
, R1—, X—
Mi I Xi • • •XiMi
Ri I Ri
XIX
61%
16%
7%
9%
5%
Match 0 Fill Length 3
Length 1 2
1 .82 .74 .69 .28.
(39) (65) (43) (39)
2 1.0 .83 .73 .00
(10) (6) (11) (1)
3 1.0 .80 1.0 —
(4) (5) (2)
4 1.0 1.0 — —
(2) (1)
— indicates no observations
</figure>
<tableCaption confidence="0.963485">
Table 4: Fill Length vs. Match Length
</tableCaption>
<bodyText confidence="0.999169730769231">
component reported on here looks for identical se-
quences of words, and simple syntactic anomalies,
such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot;
Of the 406 sentences containing nontrivial re-
pairs, the program successfully found 309. Of
these it successfully corrected 177. There were 97
sentences that contained repairs which it did not
find. In addition, out of the 10,517 sentence corpus
(10,718 — 201 trivial), it incorrectly hypothesized
that an additional 191 contained repairs. Thus of
10,517 sentences of varying lengths, it pulled out
500 as possibly containing a repair and missed 97
sentences actually containing a repair. Of the 500
that it proposed as containing a repair, 62% actu-
ally did and 38% did not. Of the 62% that had re-
pairs, it made the appropriate correction for 57%.
These numbers show that although pattern
matching is useful in identifying possible repairs,
it is less successful at making appropriate correc-
tions. This problem stems largely from the over-
lap of related patterns. Many sentences contain a
subsequence of words that match not one but sev-
eral patterns. For example the phrase &amp;quot;FLIGHT
&lt;word&gt; FLIGHT&amp;quot; matches three different pat-
terns:
show the flight earliest flight
</bodyText>
<equation confidence="0.768305">
M1 j X
</equation>
<bodyText confidence="0.657249">
show the flight time flight date
All R1 I M1 R1
</bodyText>
<page confidence="0.954388">
58
</page>
<bodyText confidence="0.957036133333333">
show the delta flight united flight
R1 Mi. I Ri
Each of these sentences is a false positive for
the other two patterns. Despite these problems
of overlap, pattern matching is useful in reducing
the set of candidate sentences to be processed for
repairs. Rather than applying detailed and pos-
sibly time-intensive analysis techniques to 10,000
sentences, we can increase efficiency by limiting
ourselves to the 500 sentences selected by the pat-
tern matcher, which has (at least on one measure)
a 75% recall rate. The repair sites hypothesized
by the pattern matcher constitute useful input for
further processing based on other sources of infor-
mation.
</bodyText>
<sectionHeader confidence="0.9998205" genericHeader="method">
NATURAL LANGUAGE
CONSTRAINTS
</sectionHeader>
<bodyText confidence="0.99998080952381">
Here we describe two sets of experiments to
measure the effectiveness of a natural language
processing system in distinguishing repairs from
false positives. One approach is based on parsing
of whole sentences; the other is based on parsing
localized word sequences identified as potential re-
pairs. Both of these experiments rely on the pat-
tern matcher to suggest potential repairs.
The syntactic and semantic components of the
Gemini natural language processing system are
used for both of these experiments. Gemini is
an extensive reimplementation of the Core Lan-
guage Engine (Alshawi et al., 1988). It includes
modular syntactic and semantic components, inte-
grated into an efficient all-paths bottom-up parser
(Moore and Dowding, 1991). Gemini was trained
on a 2,200-sentence subset of the full 10,718-
sentence corpus. Since this subset excluded the
unanswerable sentences, Gemini&apos;s coverage on the
full corpus is only an estimated 70% for syntax,
and 50% for semantics.2
</bodyText>
<subsectionHeader confidence="0.952366">
Global Syntax and Semantics
</subsectionHeader>
<bodyText confidence="0.798712636363636">
In the first experiment, based on parsing com-
plete sentences, Gemini was tested on a subset
of the data that the pattern matcher returned as
likely to contain a repair. We excluded all sen-
tences that contained fragments, resulting in a
2Gemini&apos;s syntactic coverage of the 2,200-sentence
dataset it was trained on (the set of annotated and an-
swerable MADCOW queries) is approximately 91%, while
its semantic coverage is approximately 77%. On a recent
fair test, Gemini&apos;s syntactic coverage was 87% and seman-
tic coverage was 71%.
</bodyText>
<table confidence="0.939951916666667">
Syntax Only
Marked Marked
as as
Repair False Positive
Repairs 68 (96%) 56 (30%)
False Positives 3 (4%) 131 (70%)
Syntax and Semantics
Marked Marked
as as
Repair False Positive
Repairs 64 (85%) 23 (20%)
False Positives 11(15%) 90 (80%)
</table>
<tableCaption confidence="0.999485">
Table 5: Syntax and Semantics Results
</tableCaption>
<bodyText confidence="0.9999833125">
dataset of 335 sentences, of which 179 contained
repairs and 176 contained false positives. The ap-
proach was as follows: for each sentence, parsing
was attempted. If parsing succeeded, the sentence
was marked as a false positive. If parsing did not
succeed, then pattern matching was used to detect
possible repairs, and the edits associated with the
repairs were made. Parsing was then reattempted.
If parsing succeeded at this point, the sentence was
marked as a repair. Otherwise, it was marked as
no opinion.
Table 5 shows the results of these experiments.
We ran them two ways: once using syntactic con-
straints alone and again using both syntactic and
semantic constraints. As can be seen, Gemini
is quite accurate at detecting a repair, although
somewhat less accurate at detecting a false posi-
tive. Furthermore, in cases where Gemini detected
a repair, it produced the intended correction in 62
out of 68 cases for syntax alone, and in 60 out of
64 cases using combined syntax and semantics. In
both cases, a large number of sentences (29% for
syntax, 50% for semantics) received a no opinion
evaluation. The no opinion cases were evenly
split between repairs and false positives in both
tests.
The main points to be noted from Table 5 are
that with syntax alone, the system is quite ac-
curate in detecting repairs, and with syntax and
semantics working together, it is accurate at de-
tecting false positives. However, since the coverage
of syntax and semantics will always be lower than
</bodyText>
<page confidence="0.997521">
59
</page>
<bodyText confidence="0.9999095625">
the coverage of syntax alone, we cannot compare
these rates directly.
Since multiple repairs and false positives can
occur in the same sentence, the pattern matching
process is constrained to prefer fewer repairs to
more repairs, and shorter repairs to longer repairs.
This is done to favor an analysis that deletes the
fewest words from a sentence. It is often the case
that more drastic repairs would result in a syntac-
tically and semantically well-formed sentence, but
not the sentence that the speaker intended. For
instance, the sentence &amp;quot;show me &lt;flights&gt; daily
flights to boston&amp;quot; could be repaired by deleting
the words &amp;quot;flights daily,&amp;quot; and would then yield a
grammatical sentence, but in this case the speaker
intended to delete only &amp;quot;flights.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.959025">
Local Syntax and Semantics
</subsectionHeader>
<bodyText confidence="0.999341489361702">
In the second experiment we attempted to im-
prove robustness by applying the parser to small
substrings of the sentence. When analyzing long
word strings, the parser is more likely to fail due
to factors unrelated to the repair. For this ex-
periment, the parser was using both syntax and
semantics.
The phrases used for this experiment were the
phrases found by the pattern matcher to contain
matching strings of length one, with up to three
intervening words. This set was selected because,
as can be seen from Table 4, it constitutes a large
subset of the data (186 such phrases). Further-
more, pattern matching alone contains insufficient
information for reliably correcting these sentences.
The relevant substring is taken to be the
phrase constituting the matched string plus in-
tervening material plus the immediately preceding
word. So far we have used only phrases where the
grammatical category of the matched word was ei-
ther noun or name (proper noun). For this test we
specified a list of possible phrase types (NP, VP,
PP, N, Name) that count as a successful parse. We
intend to run other tests with other grammatical
categories, but expect that these other categories
could need a different heuristic for deciding which
substring to parse, as well as a different set of ac-
ceptable phrase types.
Four candidate strings were derived from the
original by making the three different possible
edits, and also including the original string un-
changed. Each of these strings was analyzed by
the parser. When the original sequence did not
parse, but one of edits resulted in a sequence that
parsed, the original sequence was very unlikely to
be a false positive (right for 34 of 35 cases). Fur-
thermore, the edit that parsed was chosen to be
the repaired string. When more than one of the
edited strings parsed, the edit was chosen by pre-
ferring them in the following order: (1) Mi IX/VI ,
(2) RIM]. , (3) M1R11M1R1. Of the 37 cases
of repairs, the correct edit was found in 27 cases,
while in 7 more an incorrect edit was found; in
3 cases no opinion was registered. While these
numbers are quite promising, they may improve
even more when information from syntax and se-
mantics is combined with that from acoustics.
</bodyText>
<sectionHeader confidence="0.994946" genericHeader="method">
ACOUSTICS
</sectionHeader>
<bodyText confidence="0.999963352941176">
A third source of information that can be help-
ful in detecting repairs is acoustics. In this sec-
tion we describe first how prosodic information can
help in distinguishing repairs from false positives
for patterns involving matched words. Second, we
report promising results from a preliminary study
of cue words such as &amp;quot;no&amp;quot; and &amp;quot;well.&amp;quot; And third,
we discuss how acoustic information can aid in
the detection of word fragments, which occur fre-
quently and which pose difficulty for automatic
speech recognition systems.
Acoustic features reported in the following
analyses were obtained by listening to the sound
files associated with each transcription, and by
inspecting waveforms, pitch tracks, and spectro-
grams produced by the Entropic Waves software
package.
</bodyText>
<subsectionHeader confidence="0.988634">
Simple Patterns
</subsectionHeader>
<bodyText confidence="0.999983764705883">
While acoustics alone cannot tackle the prob-
lem of locating repairs, since any prosodic patterns
found in repairs are likely to be found in fluent
speech, acoustic information can be quite effective
when combined with other sources of information,
in particular with pattern matching.
In studying the ways in which acoustics might
help distinguish repairs from false positives, we
began by examining two patterns conducive to
acoustic measurement and comparison. First, we
focused on patterns in which there was only one
matched word, and in which the two occurrences
of that word were either adjacent or separated by
only one word. Matched words allow for compar-
ison of word duration; proximity helps avoid vari-
ability due to global intonation contours not asso-
ciated with the patterns themselves. We present
</bodyText>
<page confidence="0.996343">
60
</page>
<bodyText confidence="0.999855142857143">
here analyses for the M1 Mi (&amp;quot;flights for &lt;one&gt;
one person&amp;quot;) and MI (&amp;quot;&lt;flight&gt; earliest
flight&amp;quot;) repairs, and their associated false positives
(&amp;quot;u s air five one one,&amp;quot; &amp;quot;a flight on flight number
five one one,&amp;quot; respectively).
In examining the Mi I M1 repair pattern, we
found that the strongest distinguishing cue be-
tween the repairs (N = 20) and the false positives
(N = 20) was the interval between the offset of
the first word and the onset of the second. False
positives had a mean gap of 42 msec (s.d. = 55.8)
as opposed to 380 msec (s.d. = 200.4) for repairs.
A second difference found between the two groups
was that, in the case of repairs, there was a statis-
tically reliable reduction in duration for the sec-
ond occurrence of M1, with a mean difference of
53.4 msec. However because false positives showed
no reliable difference for word duration, this was
a much less useful predictor than gap duration.
FO of the matched words was not helpful in sep-
arating repairs from false positives; both groups
showed a highly significant correlation for, and no
significant difference between, the mean FO of the
matched words.
A different set of features was found to be use-
ful in distinguishing repairs from false positives
for the M11XM1 pattern. A set of 12 repairs
and 24 false positives was examined; the set of
false positives for this analysis included only flu-
ent cases (i.e., it did not include other types of
repairs matching the pattern). Despite the small
data set, some suggestive trends emerge. For ex-
ample, for cases in which there was a pause (200
msec or greater) on only one side of the inserted
word, the pause was never after the insertion (X)
for the repairs, and rarely before the X in the
false positives. A second distinguishing character-
istic was the peak FO value of X. For repairs, the
inserted word was nearly always higher in FO than
the preceding MI; for false positives, this increase
in FO was rarely observed. Table 6 shows the re-
sults of combining the acoustic constraints just de-
scribed. As can be seen, such features in combina-
tion can be quite helpful in distinguishing repairs
from false positives of this pattern. Future work
will investigate the use of prosody in distinguish-
ing the M11X/V/1 repair not only from false posi-
tives, but also from other possible repairs having
this pattern, i.e., MiRi IM1R1 and RIM].
</bodyText>
<table confidence="0.99929825">
Pauses after Pauses before
X (only) X (only)
and and
FO of X less FO of X greater
than FO of 1st M1 than FO of 1st Mi
Repairs .00 .92
False .58 .00
Positives
</table>
<tableCaption confidence="0.9071075">
Table 6: Combining Acoustic Characteristics of
Mi I XMI Repairs
</tableCaption>
<subsectionHeader confidence="0.958795">
Cue Words
</subsectionHeader>
<bodyText confidence="0.999991458333333">
A second way in which acoustics can be helpful
given the output of a pattern matcher is in deter-
mining whether or not potential cue words such
as &amp;quot;no&amp;quot; are used as an editing expression (Hock-
ett, 1967) as in &amp;quot;...flights &lt;between&gt; &lt;boston&gt;
&lt;and&gt; &lt;dallas&gt; &lt;no&gt; between oakland and
boston.&amp;quot; False positives for these cases are in-
stances in which the cue word functions in some
other sense (&amp;quot;I want to leave boston no later than
one p m.&amp;quot;). Hirshberg and Litman (1987) have
shown that cue words that function differently can
be distinguished perceptually by listeners on the
basis of prosody. Thus, we sought to determine
whether acoustic analysis could help in deciding,
when such words were present, whether or not
they marked the interruption point of a repair.
In a preliminary study of the cue words &amp;quot;no&amp;quot;
and &amp;quot;well,&amp;quot; we compared 9 examples of these
words at the site of a repair to 15 examples of
the same words occurring in fluent speech. We
found that these groups were quite distinguishable
on the basis of simple prosodic features. Table 7
shows the percentage of repairs versus false pos-
itives characterized by a clear rise or fall in FO
</bodyText>
<table confidence="0.99479375">
FO FO Lexical Cont.
rise fall stress speech
Repairs .00 1.00 .00 .00
False Positives .87 .00 .87 .73
</table>
<tableCaption confidence="0.99922">
Table 7: Acoustic Characteristics of Cue Words
</tableCaption>
<page confidence="0.981642">
61
</page>
<figure confidence="0.880349">
73r-A-a,
I would like to &lt; fra—&gt; fly
</figure>
<figureCaption confidence="0.993837">
Figure 1: A glottalized fragment
</figureCaption>
<figure confidence="0.97645325">
6000
4000
2000
1.2
3.2
3
2.6 2.8
1.8 2
2.2
2.4
1.6
1.4
</figure>
<bodyText confidence="0.999183111111111">
(greater than 15 Hz), lexical stress (determined
perceptually), and continuity of the speech im-
mediately preceding and following the editing ex-
pression (&amp;quot;continuous&amp;quot; means there was no silent
pause on either side of the cue word). As can be
seen, at least for this limited data set, cue words
marking repairs were quite distinguishable from
those same words found in fluent strings on the
basis of simple prosodic features.
</bodyText>
<sectionHeader confidence="0.652006" genericHeader="method">
Fragments
</sectionHeader>
<bodyText confidence="0.990961583333334">
A third way in which acoustic knowledge can
assist in detecting and correcting repairs is in the
recognition of word fragments. As shown earlier,
fragments are exceedingly common; they occurred
in 366 of our 607 repairs. Fragments pose diffi-
culty for state-of-the-art recognition systems be-
cause most recognizers are constrained to produce
strings of actual words, rather than allowing par-
tial words as output. Because so many repairs in-
volve fragments, if fragments are not represented
in the recognizer output, then information relevant
to the processing of repairs is lost.
We found that often when a fragment had suf-
ficient acoustic energy, one of two recognition er-
rors occurred. Either the fragment was misrecog-
nized as a complete word, or it caused a recog-
nition error on a neighboring word. Therefore if
recognizers were able to flag potential word frag-
ments, this information could aid subsequent pro-
cessing by indicating the higher likelihood that
words in the region might require deletion. Frag-
ments can also be useful in the detection of repairs
requiring deletion of more than just the fragment.
In approximately 40% of the sentences containing
fragments in our data, the fragment occurred at
the right edge of a longer repair. In a portion of
these cases, for example,
&amp;quot;leaving at &lt;seven&gt; &lt;fif-&gt; eight thirty,&amp;quot;
the presence of the fragment is an especially im-
portant cue because there is nothing (e.g., no
matched words) to cause the pattern matcher to
hypothesize the presence of a repair.
We studied 50 fragments drawn at random
from our total corpus of 366. The most reliable
acoustic cue over the set was the presence of a
silence following the fragment. In 49 out of 50
cases, there was a silence of greater than 60 msec;
the average silence was 282 msec. Of the 50 frag-
ments, 25 ended in a vowel, 13 contained a vowel
and ended in a consonant, and 12 contained no
vocalic portion.
It is likely that recognition of fragments of the
first type, in which there is abrupt cessation of
speech during a vowel, can be aided by looking for
heavy glottalization at the end of the fragment.
We coded fragments as glottalized if they showed
irregular pitch pulses in their associated waveform,
spectrogram, and pitch tracks. We found glottal-
ization in 24 of the 25 vowel-final fragments in
our data. An example of a glottalized fragment is
shown in Figure 1.
Although it is true that glottalization occurs
in fluent speech as well, it normally appears on
unstressed, low FO portions of a signal. The 24
glottalized fragments we examined however, were
not at the bottom of the speaker&apos;s range, and
most had considerable energy. Thus when com-
bined with the feature of a following silence of at.
least 60 msec, glottalization on syllables with suffi-
cient. energy and not at the bottom of the speaker&apos;s
</bodyText>
<page confidence="0.997554">
62
</page>
<bodyText confidence="0.993795">
range, may prove a useful feature in recognizing
fragments.
</bodyText>
<sectionHeader confidence="0.987209" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999954434782609">
In summary, disfluencies occur at high enough
rates in human-computer dialog to merit consid-
eration. In contrast to earlier approaches, we have
made it our goal to detect and correct repairs au-
tomatically, without assuming an explicit edit sig-
nal. Without such an edit signal, however, re-
pairs are easily confused both with false positives
and with other repairs. Preliminary results show
that pattern matching is effective at detecting re-
pairs without excessive overgeneration. Our syn-
tactic/semantic approaches are quite accurate at
detecting repairs and correcting them. Acoustics
is a third source of information that can be tapped
to provide evidence about the existence of a repair.
While none of these knowledge sources by it-
self is sufficient, we propose that by combining
them, and possibly others, we can greatly enhance
our ability to detect and correct repairs. As a next
step, we intend to explore additional aspects of the
syntax and semantics of repairs, analyze further
acoustic patterns, and pursue the question of how
best to integrate information from these multiple
knowledge sources.
</bodyText>
<sectionHeader confidence="0.995777" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999954125">
We would like to thank Patti Price for her
helpful comments on earlier drafts, as well as for
her participation in the development of the nota-
tional system used. We would also like to thank
Robin Lickley for his feedback on the acoustics
section, Elizabeth Wade for assistance with the
statistics, and Mark Gawron for work on the Gem-
ini grammar.
</bodyText>
<sectionHeader confidence="0.999367" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.99993875">
1. Alshawi, H, Carter, D., van Eijck, J., Moore, R.
C., Moran, D. B., Pereira, F., Pulman, S., and
A. Smith (1988) Research Programme In Natural
Language Processing: July 1988 Annual Report,
SRI International Tech Note, Cambridge, Eng-
land.
2. Bear, J., Dowding, J., Price, P., and E. E.
Shriberg (1992) &amp;quot;Labeling Conventions for No-
tating Grammatical Repairs in Speech,&amp;quot; unpub-
lished manuscript, to appear as an SRI Tech Note.
3. Hirschberg, J. and D. Litman (1987) &amp;quot;Now Let&apos;s
Talk About Now: Identifying Cue Phrases Into-
nationally,&amp;quot; Proceedings of the ACL, pp. 163-171.
4. Carbonell, J. and P. Hayes, P., (1983) &amp;quot;Recov-
ery Strategies for Parsing Extragrammatical Lan-
guage,&amp;quot; American Journal of Computational Lin-
guistics, Vol. 9, Numbers 3-4, pp. 123-146.
5. Hindle, D. (1983) &amp;quot;Deterministic Parsing of Syn-
tactic Non-fluencies,&amp;quot; Proceedings of the ACL, pp.
123-128.
6. Hockett, C. (1967) &amp;quot;Where the Tongue Slips,
There Slip I,&amp;quot; in To Honor Roman Jakobson: Vol.
2, The Hague: Mouton.
7. Levelt, W. (1983) &amp;quot;Monitoring and self-repair in
speech,&amp;quot; Cognition, Vol. 14, pp. 41-104.
8. Levelt, W., and A. Cutler (1983) &amp;quot;Prosodic Mark-
ing in Speech Repair,&amp;quot; Journal of Semantics, Vol.
2, pp. 205-217.
9. Lickley, R., R. Shillcock, and E. Bard (1991)
&amp;quot;Processing Disfluent Speech: How and when are
disfluencies found?&amp;quot; Proceedings of the Second
European Conference on Speech Communication
and Technology, Vol. 3, pp. 1499-1502.
10. MADCOW (1992) &amp;quot;Multi-site Data Collection for
a Spoken Language Corpus,&amp;quot; Proceedings of the
DARPA Speech and Natural Language Workshop,
February 23-26, 1992.
11. Moore, R. and J. Dowding (1991) &amp;quot;Efficient
Bottom-up Parsing,&amp;quot; Proceedings of the DARPA
Speech and Natural Language Workshop, Febru-
ary 19-22, 1991, pp. 200-203.
12. Shriberg, E., Bear, J., and Dowding, J. (1992 a)
&amp;quot;Automatic Detection and Correction of Repairs
in Human-Computer Dialog&amp;quot; Proceedings of the
DARPA Speech and Natural Language Workshop,
February 23-26, 1992.
13. Shriberg, E., Wade, E., and P. Price (1992 b)
&amp;quot;Human-Machine Problem Solving Using Spoken
Language Systems (SLS): Factors Affecting Per-
formance and User Satisfaction,&amp;quot; Proceedings of
the DARPA Speech and Natural Language Work-
shop, February 23-26, 1992.
14. Ward, W. (1991) &amp;quot;Evaluation of the CMU ATIS
System,&amp;quot; Proceedings of the DARPA Speech and
Natural Language Workshop, February 19-22,
1991, pp. 101-105.
</reference>
<page confidence="0.999465">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883441">
<title confidence="0.963981">INTEGRATING MULTIPLE KNOWLEDGE SOURCES FOR DETECTION AND CORRECTION OF REPAIRS IN HUMAN-COMPUTER DIALOG*</title>
<author confidence="0.997966">John Bear</author>
<author confidence="0.997966">John Dowding</author>
<author confidence="0.997966">Elizabeth Shribergt</author>
<affiliation confidence="0.999981">SRI International</affiliation>
<address confidence="0.99985">Menlo Park, California 94025</address>
<abstract confidence="0.9982795">We have analyzed 607 sentences of spontaneous human-computer speech data containing repairs, drawn from a total corpus of 10,718 sentences. We present here criteria and techniques for automatically detecting the presence of a repair, its location, and making the appropriate correction. The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>D Carter</author>
<author>J van Eijck</author>
<author>R C Moore</author>
<author>D B Moran</author>
<author>F Pereira</author>
<author>S Pulman</author>
<author>A Smith</author>
</authors>
<date>1988</date>
<booktitle>Research Programme In Natural Language Processing:</booktitle>
<location>Cambridge, England.</location>
<contexts>
<context position="5200" citStr="(1)" startWordPosition="824" endWordPosition="824">which no human intervention was involved. Relevant to the current paper is the fact that although the speech was spontaneous, it was somewhat planned (subjects pressed a button to begin speaking to the system) and the transcribers who produced lexical transcriptions of the sessions were instructed to mark words they inferred were verbally deleted by the speaker with special symbols. For further description of the corpus, see MADCOW (1992). NOTATION In order to classify these repairs, and to facilitate communication among the authors, it was necessary to develop a notational system that would: (1) be relatively simple, (2) capture sufficient detail, and (3) describe the vast majority of repairs observed. Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992). The basic aspects of the notation include marking the interruption point, the extent of the repair, and relevant correspondences between words in the region. To mark the site of a repair, corresponding to Hindle&apos;s &amp;quot;edit signal&amp;quot; (undie, 1983), we use a vertical bar (I). To express the notion that words on one side of the repair correspond to words on the other, we use a combination of a letter p</context>
<context position="9475" citStr="(1)" startWordPosition="1608" endWordPosition="1608">likely the phrase was a repair. Second, the more words there were intervening between the matched strings, the less likely the phrase was a repair. SIMPLE PATTERN MATCHING We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them. The pattern-matching Type Pattern Freq. Length 1 Repairs Fragments Repeats Insertions Replacement Other , R1—, X— Mi I Xi • • •XiMi Ri I Ri XIX 61% 16% 7% 9% 5% Match 0 Fill Length 3 Length 1 2 1 .82 .74 .69 .28. (39) (65) (43) (39) 2 1.0 .83 .73 .00 (10) (6) (11) (1) 3 1.0 .80 1.0 — (4) (5) (2) 4 1.0 1.0 — — (2) (1) — indicates no observations Table 4: Fill Length vs. Match Length component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309. Of these it successfully corrected 177. There were 97 sentences that contained repairs which it did not find. In addition, out of the 10,517 sentence corpus (10,718 — 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs. Thus of 10,517 </context>
<context position="17477" citStr="(1)" startWordPosition="2939" endWordPosition="2939">t of acceptable phrase types. Four candidate strings were derived from the original by making the three different possible edits, and also including the original string unchanged. Each of these strings was analyzed by the parser. When the original sequence did not parse, but one of edits resulted in a sequence that parsed, the original sequence was very unlikely to be a false positive (right for 34 of 35 cases). Furthermore, the edit that parsed was chosen to be the repaired string. When more than one of the edited strings parsed, the edit was chosen by preferring them in the following order: (1) Mi IX/VI , (2) RIM]. , (3) M1R11M1R1. Of the 37 cases of repairs, the correct edit was found in 27 cases, while in 7 more an incorrect edit was found; in 3 cases no opinion was registered. While these numbers are quite promising, they may improve even more when information from syntax and semantics is combined with that from acoustics. ACOUSTICS A third source of information that can be helpful in detecting repairs is acoustics. In this section we describe first how prosodic information can help in distinguishing repairs from false positives for patterns involving matched words. Second, we re</context>
</contexts>
<marker>1.</marker>
<rawString>Alshawi, H, Carter, D., van Eijck, J., Moore, R. C., Moran, D. B., Pereira, F., Pulman, S., and A. Smith (1988) Research Programme In Natural Language Processing: July 1988 Annual Report, SRI International Tech Note, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bear</author>
<author>J Dowding</author>
<author>P Price</author>
<author>E E Shriberg</author>
</authors>
<title>Labeling Conventions for Notating Grammatical Repairs in Speech,&amp;quot;</title>
<date>1992</date>
<note>unpublished manuscript, to appear as an SRI Tech Note.</note>
<contexts>
<context position="5226" citStr="(2)" startWordPosition="828" endWordPosition="828">n was involved. Relevant to the current paper is the fact that although the speech was spontaneous, it was somewhat planned (subjects pressed a button to begin speaking to the system) and the transcribers who produced lexical transcriptions of the sessions were instructed to mark words they inferred were verbally deleted by the speaker with special symbols. For further description of the corpus, see MADCOW (1992). NOTATION In order to classify these repairs, and to facilitate communication among the authors, it was necessary to develop a notational system that would: (1) be relatively simple, (2) capture sufficient detail, and (3) describe the vast majority of repairs observed. Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992). The basic aspects of the notation include marking the interruption point, the extent of the repair, and relevant correspondences between words in the region. To mark the site of a repair, corresponding to Hindle&apos;s &amp;quot;edit signal&amp;quot; (undie, 1983), we use a vertical bar (I). To express the notion that words on one side of the repair correspond to words on the other, we use a combination of a letter plus a numerical index. The</context>
<context position="9503" citStr="(2)" startWordPosition="1616" endWordPosition="1616">ir. Second, the more words there were intervening between the matched strings, the less likely the phrase was a repair. SIMPLE PATTERN MATCHING We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them. The pattern-matching Type Pattern Freq. Length 1 Repairs Fragments Repeats Insertions Replacement Other , R1—, X— Mi I Xi • • •XiMi Ri I Ri XIX 61% 16% 7% 9% 5% Match 0 Fill Length 3 Length 1 2 1 .82 .74 .69 .28. (39) (65) (43) (39) 2 1.0 .83 .73 .00 (10) (6) (11) (1) 3 1.0 .80 1.0 — (4) (5) (2) 4 1.0 1.0 — — (2) (1) — indicates no observations Table 4: Fill Length vs. Match Length component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309. Of these it successfully corrected 177. There were 97 sentences that contained repairs which it did not find. In addition, out of the 10,517 sentence corpus (10,718 — 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs. Thus of 10,517 sentences of varying lengths</context>
<context position="17492" citStr="(2)" startWordPosition="2943" endWordPosition="2943"> phrase types. Four candidate strings were derived from the original by making the three different possible edits, and also including the original string unchanged. Each of these strings was analyzed by the parser. When the original sequence did not parse, but one of edits resulted in a sequence that parsed, the original sequence was very unlikely to be a false positive (right for 34 of 35 cases). Furthermore, the edit that parsed was chosen to be the repaired string. When more than one of the edited strings parsed, the edit was chosen by preferring them in the following order: (1) Mi IX/VI , (2) RIM]. , (3) M1R11M1R1. Of the 37 cases of repairs, the correct edit was found in 27 cases, while in 7 more an incorrect edit was found; in 3 cases no opinion was registered. While these numbers are quite promising, they may improve even more when information from syntax and semantics is combined with that from acoustics. ACOUSTICS A third source of information that can be helpful in detecting repairs is acoustics. In this section we describe first how prosodic information can help in distinguishing repairs from false positives for patterns involving matched words. Second, we report promising </context>
</contexts>
<marker>2.</marker>
<rawString>Bear, J., Dowding, J., Price, P., and E. E. Shriberg (1992) &amp;quot;Labeling Conventions for Notating Grammatical Repairs in Speech,&amp;quot; unpublished manuscript, to appear as an SRI Tech Note.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
</authors>
<title>Now Let&apos;s Talk About Now: Identifying Cue Phrases Intonationally,&amp;quot;</title>
<date>1987</date>
<booktitle>Proceedings of the ACL,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="5261" citStr="(3)" startWordPosition="834" endWordPosition="834">rent paper is the fact that although the speech was spontaneous, it was somewhat planned (subjects pressed a button to begin speaking to the system) and the transcribers who produced lexical transcriptions of the sessions were instructed to mark words they inferred were verbally deleted by the speaker with special symbols. For further description of the corpus, see MADCOW (1992). NOTATION In order to classify these repairs, and to facilitate communication among the authors, it was necessary to develop a notational system that would: (1) be relatively simple, (2) capture sufficient detail, and (3) describe the vast majority of repairs observed. Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992). The basic aspects of the notation include marking the interruption point, the extent of the repair, and relevant correspondences between words in the region. To mark the site of a repair, corresponding to Hindle&apos;s &amp;quot;edit signal&amp;quot; (undie, 1983), we use a vertical bar (I). To express the notion that words on one side of the repair correspond to words on the other, we use a combination of a letter plus a numerical index. The letter M indicates that two words </context>
<context position="17504" citStr="(3)" startWordPosition="2946" endWordPosition="2946">s. Four candidate strings were derived from the original by making the three different possible edits, and also including the original string unchanged. Each of these strings was analyzed by the parser. When the original sequence did not parse, but one of edits resulted in a sequence that parsed, the original sequence was very unlikely to be a false positive (right for 34 of 35 cases). Furthermore, the edit that parsed was chosen to be the repaired string. When more than one of the edited strings parsed, the edit was chosen by preferring them in the following order: (1) Mi IX/VI , (2) RIM]. , (3) M1R11M1R1. Of the 37 cases of repairs, the correct edit was found in 27 cases, while in 7 more an incorrect edit was found; in 3 cases no opinion was registered. While these numbers are quite promising, they may improve even more when information from syntax and semantics is combined with that from acoustics. ACOUSTICS A third source of information that can be helpful in detecting repairs is acoustics. In this section we describe first how prosodic information can help in distinguishing repairs from false positives for patterns involving matched words. Second, we report promising results from</context>
</contexts>
<marker>3.</marker>
<rawString>Hirschberg, J. and D. Litman (1987) &amp;quot;Now Let&apos;s Talk About Now: Identifying Cue Phrases Intonationally,&amp;quot; Proceedings of the ACL, pp. 163-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>P Hayes</author>
<author>P</author>
</authors>
<title>Recovery Strategies for Parsing Extragrammatical Language,&amp;quot;</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>9</volume>
<pages>3--4</pages>
<contexts>
<context position="9495" citStr="(4)" startWordPosition="1614" endWordPosition="1614">s a repair. Second, the more words there were intervening between the matched strings, the less likely the phrase was a repair. SIMPLE PATTERN MATCHING We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them. The pattern-matching Type Pattern Freq. Length 1 Repairs Fragments Repeats Insertions Replacement Other , R1—, X— Mi I Xi • • •XiMi Ri I Ri XIX 61% 16% 7% 9% 5% Match 0 Fill Length 3 Length 1 2 1 .82 .74 .69 .28. (39) (65) (43) (39) 2 1.0 .83 .73 .00 (10) (6) (11) (1) 3 1.0 .80 1.0 — (4) (5) (2) 4 1.0 1.0 — — (2) (1) — indicates no observations Table 4: Fill Length vs. Match Length component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309. Of these it successfully corrected 177. There were 97 sentences that contained repairs which it did not find. In addition, out of the 10,517 sentence corpus (10,718 — 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs. Thus of 10,517 sentences of varying</context>
</contexts>
<marker>4.</marker>
<rawString>Carbonell, J. and P. Hayes, P., (1983) &amp;quot;Recovery Strategies for Parsing Extragrammatical Language,&amp;quot; American Journal of Computational Linguistics, Vol. 9, Numbers 3-4, pp. 123-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Deterministic Parsing of Syntactic Non-fluencies,&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the ACL,</booktitle>
<pages>123--128</pages>
<contexts>
<context position="9499" citStr="(5)" startWordPosition="1615" endWordPosition="1615">repair. Second, the more words there were intervening between the matched strings, the less likely the phrase was a repair. SIMPLE PATTERN MATCHING We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them. The pattern-matching Type Pattern Freq. Length 1 Repairs Fragments Repeats Insertions Replacement Other , R1—, X— Mi I Xi • • •XiMi Ri I Ri XIX 61% 16% 7% 9% 5% Match 0 Fill Length 3 Length 1 2 1 .82 .74 .69 .28. (39) (65) (43) (39) 2 1.0 .83 .73 .00 (10) (6) (11) (1) 3 1.0 .80 1.0 — (4) (5) (2) 4 1.0 1.0 — — (2) (1) — indicates no observations Table 4: Fill Length vs. Match Length component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309. Of these it successfully corrected 177. There were 97 sentences that contained repairs which it did not find. In addition, out of the 10,517 sentence corpus (10,718 — 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs. Thus of 10,517 sentences of varying len</context>
</contexts>
<marker>5.</marker>
<rawString>Hindle, D. (1983) &amp;quot;Deterministic Parsing of Syntactic Non-fluencies,&amp;quot; Proceedings of the ACL, pp. 123-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hockett</author>
</authors>
<title>Where the Tongue Slips, There Slip I,&amp;quot;</title>
<date>1967</date>
<journal>in To Honor Roman Jakobson:</journal>
<volume>2</volume>
<location>The Hague: Mouton.</location>
<contexts>
<context position="9466" citStr="(6)" startWordPosition="1606" endWordPosition="1606">the more likely the phrase was a repair. Second, the more words there were intervening between the matched strings, the less likely the phrase was a repair. SIMPLE PATTERN MATCHING We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them. The pattern-matching Type Pattern Freq. Length 1 Repairs Fragments Repeats Insertions Replacement Other , R1—, X— Mi I Xi • • •XiMi Ri I Ri XIX 61% 16% 7% 9% 5% Match 0 Fill Length 3 Length 1 2 1 .82 .74 .69 .28. (39) (65) (43) (39) 2 1.0 .83 .73 .00 (10) (6) (11) (1) 3 1.0 .80 1.0 — (4) (5) (2) 4 1.0 1.0 — — (2) (1) — indicates no observations Table 4: Fill Length vs. Match Length component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309. Of these it successfully corrected 177. There were 97 sentences that contained repairs which it did not find. In addition, out of the 10,517 sentence corpus (10,718 — 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs. Thus o</context>
</contexts>
<marker>6.</marker>
<rawString>Hockett, C. (1967) &amp;quot;Where the Tongue Slips, There Slip I,&amp;quot; in To Honor Roman Jakobson: Vol. 2, The Hague: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Levelt</author>
</authors>
<title>Monitoring and self-repair in speech,&amp;quot;</title>
<date>1983</date>
<journal>Cognition,</journal>
<volume>14</volume>
<pages>41--104</pages>
<marker>7.</marker>
<rawString>Levelt, W. (1983) &amp;quot;Monitoring and self-repair in speech,&amp;quot; Cognition, Vol. 14, pp. 41-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Levelt</author>
<author>A Cutler</author>
</authors>
<title>Prosodic Marking in Speech Repair,&amp;quot;</title>
<date>1983</date>
<journal>Journal of Semantics,</journal>
<volume>2</volume>
<pages>205--217</pages>
<marker>8.</marker>
<rawString>Levelt, W., and A. Cutler (1983) &amp;quot;Prosodic Marking in Speech Repair,&amp;quot; Journal of Semantics, Vol. 2, pp. 205-217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lickley</author>
<author>R Shillcock</author>
<author>E Bard</author>
</authors>
<title>Processing Disfluent Speech: How and when are disfluencies found?&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings of the Second European Conference on Speech Communication and Technology,</booktitle>
<volume>3</volume>
<pages>1499--1502</pages>
<marker>9.</marker>
<rawString>Lickley, R., R. Shillcock, and E. Bard (1991) &amp;quot;Processing Disfluent Speech: How and when are disfluencies found?&amp;quot; Proceedings of the Second European Conference on Speech Communication and Technology, Vol. 3, pp. 1499-1502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MADCOW</author>
</authors>
<title>Multi-site Data Collection for a Spoken Language Corpus,&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<contexts>
<context position="9462" citStr="(10)" startWordPosition="1605" endWordPosition="1605">ing, the more likely the phrase was a repair. Second, the more words there were intervening between the matched strings, the less likely the phrase was a repair. SIMPLE PATTERN MATCHING We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them. The pattern-matching Type Pattern Freq. Length 1 Repairs Fragments Repeats Insertions Replacement Other , R1—, X— Mi I Xi • • •XiMi Ri I Ri XIX 61% 16% 7% 9% 5% Match 0 Fill Length 3 Length 1 2 1 .82 .74 .69 .28. (39) (65) (43) (39) 2 1.0 .83 .73 .00 (10) (6) (11) (1) 3 1.0 .80 1.0 — (4) (5) (2) 4 1.0 1.0 — — (2) (1) — indicates no observations Table 4: Fill Length vs. Match Length component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309. Of these it successfully corrected 177. There were 97 sentences that contained repairs which it did not find. In addition, out of the 10,517 sentence corpus (10,718 — 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs. Th</context>
</contexts>
<marker>10.</marker>
<rawString>MADCOW (1992) &amp;quot;Multi-site Data Collection for a Spoken Language Corpus,&amp;quot; Proceedings of the DARPA Speech and Natural Language Workshop, February 23-26, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
</authors>
<title>Efficient Bottom-up Parsing,&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>200--203</pages>
<contexts>
<context position="9471" citStr="(11)" startWordPosition="1607" endWordPosition="1607">more likely the phrase was a repair. Second, the more words there were intervening between the matched strings, the less likely the phrase was a repair. SIMPLE PATTERN MATCHING We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them. The pattern-matching Type Pattern Freq. Length 1 Repairs Fragments Repeats Insertions Replacement Other , R1—, X— Mi I Xi • • •XiMi Ri I Ri XIX 61% 16% 7% 9% 5% Match 0 Fill Length 3 Length 1 2 1 .82 .74 .69 .28. (39) (65) (43) (39) 2 1.0 .83 .73 .00 (10) (6) (11) (1) 3 1.0 .80 1.0 — (4) (5) (2) 4 1.0 1.0 — — (2) (1) — indicates no observations Table 4: Fill Length vs. Match Length component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &amp;quot;a the&amp;quot; or &amp;quot;to from.&amp;quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309. Of these it successfully corrected 177. There were 97 sentences that contained repairs which it did not find. In addition, out of the 10,517 sentence corpus (10,718 — 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs. Thus of 10,</context>
</contexts>
<marker>11.</marker>
<rawString>Moore, R. and J. Dowding (1991) &amp;quot;Efficient Bottom-up Parsing,&amp;quot; Proceedings of the DARPA Speech and Natural Language Workshop, February 19-22, 1991, pp. 200-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>J Bear</author>
<author>J Dowding</author>
</authors>
<title>a) &amp;quot;Automatic Detection and Correction of Repairs in Human-Computer Dialog&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<marker>12.</marker>
<rawString>Shriberg, E., Bear, J., and Dowding, J. (1992 a) &amp;quot;Automatic Detection and Correction of Repairs in Human-Computer Dialog&amp;quot; Proceedings of the DARPA Speech and Natural Language Workshop, February 23-26, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>E Wade</author>
<author>P Price</author>
</authors>
<title>b) &amp;quot;Human-Machine Problem Solving Using Spoken Language Systems (SLS): Factors Affecting Performance and User Satisfaction,&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<marker>13.</marker>
<rawString>Shriberg, E., Wade, E., and P. Price (1992 b) &amp;quot;Human-Machine Problem Solving Using Spoken Language Systems (SLS): Factors Affecting Performance and User Satisfaction,&amp;quot; Proceedings of the DARPA Speech and Natural Language Workshop, February 23-26, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ward</author>
</authors>
<title>Evaluation of the CMU ATIS System,&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>101--105</pages>
<marker>14.</marker>
<rawString>Ward, W. (1991) &amp;quot;Evaluation of the CMU ATIS System,&amp;quot; Proceedings of the DARPA Speech and Natural Language Workshop, February 19-22, 1991, pp. 101-105.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>