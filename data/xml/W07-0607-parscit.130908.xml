<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004471">
<title confidence="0.988346">
ISA meets Lara:
An incremental word space model
for cognitively plausible simulations of semantic learning
</title>
<author confidence="0.997928">
Marco Baroni
</author>
<affiliation confidence="0.747037666666667">
CIMeC (University of Trento)
C.so Bettini 31
38068 Rovereto, Italy
</affiliation>
<email confidence="0.987843">
marco.baroni@unitn.it
</email>
<author confidence="0.989884">
Alessandro Lenci
</author>
<affiliation confidence="0.9976545">
Department of Linguistics
University of Pisa
</affiliation>
<address confidence="0.640397">
Via Santa Maria 36
56126 Pisa, Italy
</address>
<author confidence="0.971499">
Luca Onnis
</author>
<affiliation confidence="0.99642">
Department of Psychology
Cornell University
</affiliation>
<address confidence="0.615955">
Ithaca, NY 14853
</address>
<email confidence="0.970962">
lo35@cornell.edu
alessandro.lenci@ilc.cnr.it
</email>
<sectionHeader confidence="0.998532" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899">
We introduce Incremental Semantic Analy-
sis, a fully incremental word space model,
and we test it on longitudinal child-directed
speech data. On this task, ISA outperforms
the related Random Indexing algorithm, as
well as a SVD-based technique. In addi-
tion, the model has interesting properties
that might also be characteristic of the se-
mantic space of children.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999718714285714">
Word space models induce a semantic space from
raw textual input by keeping track of patterns of
co-occurrence of words with other words through a
vectorial representation. Proponents of word space
models such as HAL (Burgess and Lund, 1997) and
LSA (Landauer and Dumais, 1997) have argued that
such models can capture a variety of facts about hu-
man semantic learning, processing, and representa-
tion. As such, word space methods are not only
increasingly useful as engineering applications, but
they are also potentially promising for modeling
cognitive processes of lexical semantics.
However, to the extent that current word space
models are largely non-incremental, they can hardly
accommodate how young children develop a seman-
tic space by moving from virtually no knowledge
of the language to reach an adult-like state. The
family of models based on singular value decom-
position (SVD) and similar dimensionality reduc-
tion techniques (e.g., LSA) first construct a full co-
occurrence matrix based on statistics extracted from
</bodyText>
<page confidence="0.555963">
49
</page>
<bodyText confidence="0.999971205882353">
the whole input corpus, and then build a model at
once via matrix algebra operations. Admittedly,
this is hardly a plausible simulation of how chil-
dren learn word meanings incrementally by being
exposed to short sentences containing a relatively
small number of different words. The lack of incre-
mentality of several models appears conspicuous es-
pecially given their explicit claim to solve old theo-
retical issues about the acquisition of language (e.g.,
(Landauer and Dumais, 1997)). Other extant models
display some degree if incrementality. For instance,
HAL and Random Indexing (Karlgren and Sahlgren,
2001) can generate well-formed vector representa-
tions at intermediate stages of learning. However,
they lack incrementality when they make use of stop
word lists or weigthing techniques that are based on
whole corpus statistics. For instance, consistently
with the HAL approach, Li et al. (2000) first build
a word co-occurrence matrix, and then compute the
variance of each column to reduce the vector dimen-
sions by discarding those with the least contextual
diversity.
Farkas and Li (2000) and Li et al. (2004) pro-
pose an incremental version of HAL by using a a re-
current neural network trained with Hebbian learn-
ing. The networks incrementally build distributional
vectors that are then used to induce word semantic
clusters with a Self-Organizing Map.Farkas and Li
(2000) does not contain any evaluation of the struc-
ture of the semantic categories emerged in the SOM.
A more precise evaluation is instead performed by
Li et al. (2004), revealing the model’s ability to sim-
ulate interesting aspects of early vocabulary dynam-
ics. However, this is achieved by using hybrid word
</bodyText>
<subsectionHeader confidence="0.575365">
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 49–56,
Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999945902439024">
representations, in which the distributional vectors
are enriched with semantic features derived from
WordNet.
Borovsky and Elman (2006) also model word
learning in a fairly incremental fashion, by using the
hidden layer vectors of a Simple Recurrent Network
as word representations. The network is probed at
different training epochs and its internal represen-
tations are evaluated against a gold standard ontol-
ogy of semantic categories to monitor the progress in
word learning. Borovsky and Elman (2006)’s claim
that their model simulates relevant aspects of child
word learning should probably be moderated by the
fact that they used a simplified set of artificial sen-
tences as training corpus. From their simulations it
is thus difficult to evaluate whether the model would
scale up to large naturalistic samples of language.
In this paper, we introduce Incremental Semantic
Indexing (ISA), a model that strives to be more de-
velopmentally plausible by achieving full incremen-
tality. We test the model and some of its less incre-
mental rivals on Lara, a longitudinal corpus of child-
directed speech based on samples of child-adult lin-
guistic interactions collected regularly from 1 to 3
years of age of a single English child. ISA achieves
the best performance on these data, and it learns
a semantic space that has interesting properties for
our understanding of how children learn and struc-
ture word meaning. Thus, the desirability of incre-
mentality increases as the model promises to cap-
ture specific developmental trajectories in semantic
learning.
The plan of the paper is as follows. First, we
introduce ISA together with its main predecessor,
Random Indexing. Then, we present the learning
experiments in which several versions of ISA and
other models are trained to induce and organize lexi-
cal semantic information from child-directed speech
transcripts. Lastly, we discuss further work in devel-
opmental computational modeling using word space
models.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="introduction">
2 Models
</sectionHeader>
<subsectionHeader confidence="0.998376">
2.1 Random Indexing
</subsectionHeader>
<bodyText confidence="0.999909722222222">
Since the model we are proposing can be seen as
a fully incremental variation on Random Indexing
(RI), we start by introducing the basic features of
RI (Karlgren and Sahlgren, 2001). Initially, each
context word is assigned an arbitrary vector repre-
sentation of fixed dimensionality d made of a small
number of randomly distributed +1 and -1, with all
other dimensions assigned a 0 value (d is typically
much smaller than the dimensionality of the full co-
occurrence matrix). This vector representation is
called signature. The context-dependent represen-
tation for a given target word is then obtained by
adding the signatures of the words it co-occurs with
to its history vector. Multiplying the history by a
small constant called impact typically improves RI
performance. Thus, at each encounter of target word
t with a context word c, the history of t is updated as
follows:
</bodyText>
<equation confidence="0.992966">
ht += i x s, (1)
</equation>
<bodyText confidence="0.9999336">
where i is the impact constant, ht is the history vec-
tor of t and s, is the signature vector of c. In this
way, the history of a word keeps track of the con-
texts in which it occurred. Similarity among words
is then measured by comparing their history vectors,
e.g., measuring their cosine.
RI is an extremely efficient technique, since it di-
rectly builds and updates a matrix of reduced di-
mensionality (typically, a few thousands elements),
instead of constructing a full high-dimensional co-
occurrence matrix and then reducing it through SVD
or similar procedures. The model is incremental
to the extent that at each stage of corpus process-
ing the vector representations are well-formed and
could be used to compute similarity among words.
However, RI misses the “second order” effects that
are claimed to account, at least in part, for the ef-
fectiveness of SVD-based techniques (Manning and
Sch¨utze, 1999, 15.4). Thus, for example, since dif-
ferent random signatures are assigned to the words
cat, dog and train, the model does not capture the
fact that the first two words, but not the third, should
count as similar contexts. Moreover, RI is not fully
incremental in several respects. First, on each en-
counter of two words, the same fixed random sig-
nature of one of them is added to the history of the
other, i.e., the way in which a word affects another
does not evolve with the changes in the model’s
knowledge about the words. Second, RI makes use
of filtering and weighting procedures that rely on
</bodyText>
<page confidence="0.974762">
50
</page>
<bodyText confidence="0.999969230769231">
global statistics, i.e., statistics based on whole cor-
pus counts. These procedures include: a) treating
the most frequent words as stop words; b) cutting
off the lowest frequency words as potential contexts;
and c) using mutual information or entropy mea-
sures to weight the effect of a word on the other).
In addition, although procedures b) and c) may have
some psychological grounding, procedure a) would
implausibly entail that to build semantic represen-
tations the child actively filters out high frequency
words as noise from her linguistic experience. Thus,
as it stands RI has some noticeable limitations as a
developmental model.
</bodyText>
<subsectionHeader confidence="0.994385">
2.2 Incremental Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999845366666666">
Incremental Semantic Analysis (ISA) differs from
RI in two main respects. First and most importantly,
when a word encounters another word, the history
vector of the former is updated with a weighted sum
of the signature and the history of the latter. This
corresponds to the idea that a target word is affected
not only by its context words, but also by the se-
mantic information encoded by that their distribu-
tional histories. In this way, ISA can capture SVD-
like second order effects: cat and dog might work
like similar contexts because they are likely to have
similar histories. More generally, this idea relies on
two intuitively plausible assumptions about contex-
tual effects in word learning, i.e., that the informa-
tion carried by a context word will change as our
knowledge about the word increases, and that know-
ing about the history of co-occurrence of a context
word is an important part of the information being
contributed by the word to the targets it affects.
Second, ISA does not rely on global statistics for
filtering and weighting purposes. Instead, it uses a
weighting scheme that changes as a function of the
frequency of the context word at each update. This
makes the model fully incremental and (together
with the previous innovation) sensitive not only to
the overall frequency of words in the corpus, but to
the order in which they appear.
More explicitly, at each encounter of a target word
t with a context word c, the history vector of t is
updated as follows:
</bodyText>
<equation confidence="0.95511">
ht += i x (mchc + (1 − mc)sc) (2)
</equation>
<bodyText confidence="0.9999715">
The constant i is the impact rate, as in the RI for-
mula (1) above. The value mc determines how much
the history of a word will influence the history of an-
other word. The intuition here is that frequent words
tend to co-occur with a lot of other words by chance.
Thus, the more frequently a word is seen, the less
informative its history will be, since it will reflect
uninteresting co-occurrences with all sorts of words.
ISA implements this by reducing the influence that
the history of a context word c has on the target word
t as a function of the token frequency of c (notice
that the model still keeps track of the encounter with
c, by adding its signature to the history of t; it is just
the history of c that is weighted down). More pre-
cisely, the m weight associated with a context word
c decreases as follows:
where kn is a parameter determining how fast the
decay will be.
</bodyText>
<sectionHeader confidence="0.996593" genericHeader="method">
3 Experimental setting
</sectionHeader>
<subsectionHeader confidence="0.997835">
3.1 The Lara corpus
</subsectionHeader>
<bodyText confidence="0.999918130434783">
The input for our experiments is provided by the
Child-Directed-Speech (CDS) section of the Lara
corpus (Rowland et al., 2005), a longitudinal cor-
pus of natural conversation transcripts of a single
child, Lara, between the ages of 1;9 and 3;3. Lara
was the firstborn monolingual English daughter of
two White university graduates and was born and
brought up in Nottinghamshire, England. The cor-
pus consists of transcripts from 122 separate record-
ing sessions in which the child interacted with adult
caretakers in spontaneous conversations. The total
recording time of the corpus is of about 120 hours,
representing one of the densest longitudinal corpora
available. The adult CDS section we used contains
about 400K tokens and about 6K types.
We are aware that the use of a single-child corpus
may have a negative impact on the generalizations
on semantic development that we can draw from the
experiments. On the other hand, this choice has the
important advantage of providing a fairly homoge-
neous data environment for our computational sim-
ulations. In fact, we can abstract from the intrin-
sic variability characterizing any multi-child corpus,
</bodyText>
<figure confidence="0.53664875">
1
exp (Count(c)1
kms, f
mc =
</figure>
<page confidence="0.96017">
51
</page>
<bodyText confidence="0.989927905263158">
and stemming from differences in the conversation top 30 most frequent words), as well as simulations
settings, in the adults’ grammar and lexicon, etc. with no stop word filtering. For RI and ISA, we used
Moreover, whereas we can take our experiments to signature and history vectors of 1,800 and 2,400 di-
constitute a (very rough) simulation of how a par- mensions (the first value, again, inspired by Karl-
ticular child acquires semantic representations from gren and Sahlgren’s work). Preliminary experiments
her specific linguistic input, it is not clear what simu- with 300 and 900 dimensions produced poor results,
lations based on an “averages” of different linguistic especially with RI. For SVD, we used 300 dimen-
experiences would represent. sions only. This was in part due to technical lim-
The corpus was part-of-speech-tagged and lem- itations of the implementation, but 300 dimensions
matized using the CLAN toolkit (MacWhinney, is also a fairly typical choice for SVD-based mod-
2000). The automated output was subsequently els such as LSA, and a value reported to produce
checked and disambiguated manually, resulting in excellent results in the literature. More importantly,
very accurate annotation. In our experiments, we in unrelated experiments SVD with 300 dimensions
use lemma-POS pairs as input to the word space and function word filtering achieved state-of-the-art
models (e.g., go-v rather than going, goes, etc.) performance (accuracy above 90%) in the by now
Thus, we make the unrealistic assumptions that the standard TOEFL synonym detection task (Landauer
learner already solved the problem of syntactic cate- and Dumais, 1997).
gorization and figured out the inflectional morphol- After preliminary experiments showed that both
ogy of her language. While a multi-level bootstrap- models (especially ISA) benefited from a very low
ping process in which the morphosyntactic and lex- impact rate, the impact parameter i of RI and ISA
ical properties of words are learned in parallel is was set to 0.003 and 0.009. Finally, km (the ISA pa-
probably cognitively more likely, it seems reason- rameter determining the steepness of decay of the
able at the current stage of experimentation to fix influence of history as the token frequency of the
morphosyntax and focus on semantic learning. context word increases) was set to 20 and 100 (recall
3.2 Model training that a higher km correspond to a less steep decay).
We experimented with three word space models: The parameter settings we explored were system-
ISA, RI (our implementations in both cases) and the atically crossed in a series of experiments. More-
SVD-based technique implemented by the Infomap over, for RI and ISA, given that different random
package.1 initializations will lead to (slightly) different results,
Parameter settings may considerably impact the each experiment was repeated 10 times.
performance of word space models (Sahlgren, Below, we will report results for the best perform-
2006). In a stage of preliminary investigations (not ing models of each type: ISA with 1,800 dimen-
reported here, and involving also other corpora) we sions, i set to 0.003 and km set to 100; RI with 2,400
identified a relatively small range of values for each dimensions, i set to 0.003 and no stop words; SVD
parameter of each model that produced promising with 300-dimensional vectors and function words
results, and we focused on it in the subsequent, more removed. However, it must be stressed that 6 out
systematic exploration of the parameter space. of the 8 ISA models we experimented with outper-
For all models, we used a context window of five formed the best RI model (and they all outperformed
words to the left and five words to the right of the the best SVD model) in the Noun AP task discussed
target. For both RI and ISA, we set signature initial- in section 4.1. This suggests that the results we re-
ization parameters (determining the random assign- port are not overly dependent on specific parameter
ment of 0s, +1s and -1s to signature vectors) similar choices.
to those described by Karlgren and Sahlgren (2001). 3.3 Evaluation method
For RI and SVD, we used two stop word filtering The test set was composed of 100 nouns and 70
lists (removing all function words, and removing the verbs (henceforth, Ns and Vs), selected from the
most frequent words in Lara’s CDS section (word
frequency ranges from 684 to 33 for Ns, and from
1http://infomap-nlp.sourceforge.net/
52
3501 to 89 for Vs). This asymmetry in the test
set mirrors the different number of V and N types
that occur in the input (2828 Ns vs. 944 Vs). As
a further constraint, we verified that all the words
in the test set also appeared among the child’s pro-
ductions in the corpus. The test words were un-
ambiguously assigned to semantic categories pre-
viously used to model early lexical development
and represent plausible early semantic groupings.
Semantic categories for nouns and verbs were de-
rived by combining two methods. For nouns, we
used the ontologies from the Macarthur-Bates Com-
municative Development Inventories (CDI).2 All
the Ns in the test set also appear in the Tod-
dler’s List in CDI. The noun semantic categories are
the following (in parenthesis, we report the num-
ber of words per class and an example): ANI-
MALS REAL OR TOY (19; dog), BODY PARTS (16;
nose), CLOTHING (5; hat), FOOD AND DRINK (13;
pizza), FURNITURE AND ROOMS (8; table), OUT-
SIDE THINGS AND PLACES TO GO (10; house),
PEOPLE (10; baby), SMALL HOUSEHOLD ITEMS
(13; bottle), TOYS (6; pen). Since categories for
verbs were underspecified in the CDI, we used
12 broad verb semantic categories for event types,
partly extending those in Borovsky and Elman
(2006): ACTION (11; play), ACTION BODY (6;
eat), ACTION FORCE (5; pull), ASPECTUAL (6;
start), CHANGE (12; open), COMMUNICATION (4;
talk), MOTION (5; run), PERCEPTION (6; hear),
PSYCH (7; remember), SPACE (3; stand), TRANS-
FER (6; buy).
It is worth emphasizing that this experimental set-
ting is much more challenging than those that are
usually adopted by state-of-the-art computational
simulations of word learning, as the ones reported
above. For instance, the number of words in our
test set is larger than the one in Borovsky and Elman
(2006), and so is the number of semantic categories,
both for Ns and for Vs. Conversely, the Lara corpus
is much smaller than the data-sets normally used to
train word space models. For instance, the best re-
sults reported by Li et al. (2000) are obtained with
an input corpus which is 10 times bigger than ours.
As an evaluation measure of the model perfor-
mance in the word learning task, we adopted Aver-
</bodyText>
<footnote confidence="0.802644">
2http://www.sci.sdsu.edu/cdi/
</footnote>
<bodyText confidence="0.999665428571429">
age Precision (AP), recently used by Borovsky and
Elman (2006). AP evaluates how close all members
of a certain category are to each other in the seman-
tic space built by the model.
To calculate AP, for each wi in the test set we first
extracted the corresponding distributional vector vi
produced by the model. Vectors were used to cal-
culate the pair-wise cosine between each test word,
as a measure of their distance in the semantic space.
Then, for each target word wi, we built the list ri of
the other test words ranked by their decreasing co-
sine values with respect to wi. The ranking ri was
used to calculate AP(wi), the Word Average Preci-
sion for wi, with the following formula:
</bodyText>
<equation confidence="0.996708333333333">
1 � nwj(Cwi)
AP(wi) = |Cwi |nwj
wj�Cwi
</equation>
<bodyText confidence="0.999812470588235">
where Cwi is the semantic category assigned to wi,
nwj is the set of words appearing in ri up to the rank
occupied by wj, and nwj(Cwi) is the subset of words
in nwj that belong to category Cwi.
AP(wi) calculates the proportion of words that
belong to the same category of wi at each rank in
ri, and then divides this proportion by the number
of words that appear in the category. AP ranges
from 0 to 1: AP(wi) = 1 would correspond to the
ideal case in which all the closest words to wi in ri
belonged to the same category as wi; conversely, if
all the words belonging to categories other than Cwi
were closer to wi than the words in Cwi, AP(wi)
would approach 0. We also defined the Class AP
for a certain semantic category by simply averaging
over the Word AP(wi) for each word in that cate-
gory:
</bodyText>
<equation confidence="0.9556735">
=|Ci |AP( w
AP(Ci) = �j=1 |Ci |j)
</equation>
<bodyText confidence="0.999437090909091">
We adopted AP as a measure of the purity and co-
hesiveness of the semantic representations produced
by the model. Words and categories for which the
model is able to converge on well-formed represen-
tations should therefore have higher AP values. If
we define Recall as the number of words in nwj be-
longing to Cwi divided by the total number of words
in Cwi, then all the AP scores reported in our exper-
iments correspond to 100% Recall, since the neigh-
bourhood we used to compute AP(wi) always in-
cluded all the words in Cwi. This represents a very
</bodyText>
<page confidence="0.9979">
53
</page>
<table confidence="0.999755363636364">
Nouns
Tokens ISA RI SVD
100k 0.321 0.317 0.243
200k 0.343 0.337 0.284
300k 0.374 0.367 0.292
400k 0.400 0.393 0.306
Verbs
100k 0.242 0.247 0.183
200k 0.260 0.266 0.205
300k 0.261 0.266 0.218
400k 0.270 0.272 0.224
</table>
<tableCaption confidence="0.883999666666667">
Table 1: Word AP scores for Nouns (top) and Verbs
(bottom). For ISA and RI, scores are averaged
across 10 iterations
</tableCaption>
<bodyText confidence="0.953769333333333">
stringent evaluation condition for our models, far be-
yond what is commonly used in the evaluation of
classification and clustering algorithms.
</bodyText>
<sectionHeader confidence="0.994173" genericHeader="evaluation">
4 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.99792">
4.1 Word learning
</subsectionHeader>
<bodyText confidence="0.99887784">
Since we intended to monitor the incremental path
of word learning given increasing amounts of lin-
guistic input, AP scores were computed at four
“training checkpoints” established at 100K, 200K,
300K and 400K word tokens (the final point corre-
sponding to the whole corpus).3 Scores were calcu-
lated independently for Ns and Vs. In Table 1, we
report the AP scores obtained by the best perform-
ing models of each type, as described in section 3.2.
The reported AP values refer to Word AP averaged
respectively over the number of Ns and Vs in the test
set. Moreover, for ISA and RI we report mean AP
values across 10 repetitions of the experiment.
For Ns, both ISA and RI outperformed SVD at all
learning stages. Moreover, ISA also performed sig-
nificantly better than RI in the full-size input condi-
tion (400k checkpoint), as well as at the 300k check-
point (Welch t-test; df = 17, p &lt; .05).
One of the most striking results of these experi-
ments was the strong N-V asymmetry in the Word AP
scores, with the Vs performing significantly worse
than the Ns. For Vs, RI appeared to have a small
advantage over ISA, although it was never signifi-
cant at any stage. The asymmetry is suggestive of
the widely attested N-V asymmetry in child word
</bodyText>
<footnote confidence="0.987930666666667">
3The checkpoint results for SVD were obtained by training
different models on increasing samples from the corpus, given
the non-incremental nature of this method.
</footnote>
<bodyText confidence="0.999690736842105">
learning. A consensus has gathered in the early
word learning literature that children from several
languages acquire Ns earlier and more rapidly than
Vs (Gentner, 1982). An influential account explains
this noun-bias as a product of language-external fac-
tors such as the different complexity of the world
referents for Ns and Vs. Recently, Christiansen and
Monaghan (2006) found that distributional informa-
tion in English CDS was more reliable for identi-
fying Ns than Vs. This suggests that the category-
bias may also be partly driven by how good cer-
tain language-internal cues for Ns and Vs are in a
given language. Likewise, distributional cues to se-
mantics may be stronger for English Ns than for
Vs. The noun-bias shown by ISA (and by the other
models) could be taken to complement the results
of Christiansen and Monaghan in showing that En-
glish Ns are more easily discriminable than Vs on
distributionally-grounded semantic terms.
</bodyText>
<subsectionHeader confidence="0.979032">
4.2 Category learning
</subsectionHeader>
<bodyText confidence="0.999866777777778">
In Table 2, we have reported the Class AP scores
achieved by ISA, RI and SVD (best models) under
the full-corpus training regime for the nine nominal
semantic categories. Although even in this case ISA
and RI generally perform better than SVD (with the
only exceptions of FURNITURE AND ROOMS
and SMALL HOUSEHOLD ITEMS), results
show a more complex and articulated sit-
uation. With BODY PARTS, PEOPLE, and
SMALL HOUSEHOLD ITEMS, ISA significantly
outperforms its best rival RI (Welch t-test; p &lt; .05).
For the other classes, the differences among the two
models are not significant, except for CLOTHING
in which RI performs significantly better than ISA.
For verb semantic classes (whose analytical data are
not reported here for lack of space), no significant
differences exist among the three models.
Some of the lower scores in Table 2 can be ex-
plained either by the small number of class mem-
bers (e.g. TOYS has only 6 items), or by the class
highly heterogeneous composition (e.g. in OUT-
SIDE THINGS AND PLACES TO GO we find nouns
like garden, flower and zoo). The case of PEOPLE,
for which the performance of all the three models
is far below their average Class AP score (ISA =
0.35; RI = 0.35; SVD = 0.27), is instead much more
surprising. In fact, PEOPLE is one of the classes
</bodyText>
<page confidence="0.996428">
54
</page>
<table confidence="0.9889533">
Semantic class ISA RI SVD
ANIMALS REAL OR TOY 0.616 0.619 0.438
BODY PARTS 0.671 0.640 0.406
CLOTHING 0.301 0.349 0.328
FOOD AND DRINK 0.382 0.387 0.336
FURNITURE AND ROOMS 0.213 0.207 0.242
OUTSIDE THINGS PLACES 0.199 0.208 0.198
PEOPLE 0.221 0.213 0.201
SMALL HOUSEHOLD ITEMS 0.208 0.199 0.244
TOYS 0.362 0.368 0.111
</table>
<tableCaption confidence="0.90606">
Table 2: Class AP scores for Nouns. For ISA and
RI, scores are averaged across 10 iterations
</tableCaption>
<bodyText confidence="0.999691085714285">
with the highest degree of internal coherence, be-
ing composed only of nouns unambiguously denot-
ing human beings, such as girl, man, grandma, etc.
The token frequency of the members in this class is
also fairly high, ranging between 684 and 55 occur-
rences. Last but not least, in unrelated experiments
we found that a SVD model trained on the British
National Corpus with the same parameters as those
used with Lara was able to achieve very good per-
formances with human denoting nouns, similar to
the members of our PEOPLE class.
These facts have prompted us to better investi-
gate the reasons why with Lara none of the three
models was able to converge on a satisfactory rep-
resentation for the nouns belonging to the PEO-
PLE class. We zoomed in on this semantic class
by carrying out another experiment with ISA. This
model underwent 8 cycles of evaluation, in each of
which the 10 words originally assigned to PEOPLE
have been reclassified into one of the other nom-
inal classes. For each cycle, AP scores were re-
computed for the 10 test words. The results are re-
ported in Figure 1 (where AP refers to the average
Word AP achieved by the 10 words originally be-
longing to the class PEOPLE). The highest score is
reached when the PEOPLE nouns are re-labeled as
ANIMALS REAL OR TOY (we obtained similar re-
sults in a parallel experiment with SVD). This sug-
gests that the low score for the class PEOPLE in the
original experiment was due to ISA mistaking peo-
ple names for animals. What prima facie appeared
as an error could actually turn out to be an interesting
feature of the semantic space acquired by the model.
The experiments show that ISA (as well as the other
models) groups together animals and people Ns, as
</bodyText>
<figureCaption confidence="0.928274">
Figure 1: AP scores for Ns in PEOPLE reclassified
in the other classes
</figureCaption>
<bodyText confidence="0.99998745">
it has formed a general and more underspecified se-
mantic category that we might refer to as ANIMATE.
This hypothesis is also supported by qualitative ev-
idence. A detailed inspection of the CDS in the
Lara corpus reveals that the animal nouns in the
test set are mostly used by adults to refer either to
toy-animals with which Lara plays or to characters
in stories. In the transcripts, both types of entities
display a very human-like behavior (i.e., they talk,
play, etc.), as it happens to animal characters in most
children’s stories. Therefore, the difference between
model performance and the gold standard ontology
can well be taken as an interesting clue to a genuine
peculiarity in children’s semantic space with respect
to adult-like categorization. Starting from an input
in which animal and human nouns are used in sim-
ilar contexts, ISA builds a semantic space in which
these nouns belong to a common underspecified cat-
egory, much like the world of a child in which cats
and mice behave and feel like human beings.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999966714285714">
Our main experiments show that ISA significantly
outperforms state-of-the-art word space models in
a learning task carried out under fairly challenging
training and testing conditions. Both the incremen-
tal nature and the particular shape of the semantic
representations built by ISA make it a (relatively)
realistic computational model to simulate the emer-
</bodyText>
<page confidence="0.941922">
55
</page>
<bodyText confidence="0.983682185185185">
gence of a semantic space in early childhood. early word learning. Journal of Child Language, 33:
Of course, many issues remain open. First of all, 759-790.
although the Lara corpus presents many attractive Burgess, C. and K. Lund. 1997. Modelling parsing
characteristics, it still contains data pertaining to a constraints with high-dimensional context space. Lan-
single child, whose linguistic experience may be un- guage and Cognitive Processes, 12: 1-34.
usual. The evaluation of the model should be ex- Choi, S. and A. Gopnik, A. 1995. Early acquisition of
tended to more CDS corpora. It will be especially verbs in Korean: a cross-linguistic study. Journal of
interesting to run experiments in languages such as Child Language 22: 497-529.
as Korean (Choi and Gopnik, 1995), where no noun- Christiansen, M.H. and P. Monaghan. 2006. Dis-
bias is attested. There, we would predict that the dis- covering verbs through multiple-cue integration. In
tributional information to semantics be less skewed K. Hirsh-Pasek and R.M. Golinkoff (eds.), Action
in favor of nouns. All CDS corpora we are aware of meets word: How children learn verbs. OUP, Oxford.
are rather small, compared to the amount of linguis- Farkas, I. and P. Li. 2001. A self-organizing neural net-
tic input a child hears. Thus, we also plan to test the work model of the acquisition of word meaning. Pro-
model on “artificially enlarged” corpora, composed ceedings of the 4th International Conference on Cog-
of CDS from more than one child, plus other texts nitive Modeling.
that might be plausible sources of early linguistic in- Gentner, D. 1982. Why nouns are learned before verbs:
put, such as children’s stories. Linguistic relativity versus natural partitioning. In
In addition, the target of the model’s evaluation S.A. Kuczaj (ed.), Language development, vol. 2: Lan-
should not be to produce as high a performance as guage, thought and culture. Erlbaum, Hillsdale, NJ.
possible, but rather to produce performance match- Karlgren, J. and M. Sahlgren. 2001. From words to un-
ing that of human learners.4 In this respect, the derstanding. In Uesaka, Y., P. Kanerva and H. Asoh
output of the model should be compared to what is (eds.), Foundations of real-world intelligence, CSLI,
known about human semantic knowledge at various Stanford: 294-308,
stages, either by looking at experimental results in Landauer, T.K. and S.T. Dumais. 1997. A solution to
the acquisition literature or, more directly, by com- Plato’s problem: The Latent Semantic Analysis theory
paring the output of the model to what we can in- of acquisition, induction and representation of knowl-
fer about the semantic generalizations made by the edge. Psychological Review, 104(2): 211-240.
child from her/his linguistic production recorded in Li, P., C. Burgess and K. Lund. 2000. The acquisition of
the corpus. word meaning through global lexical co-occurrences.
Finally, further studies should explore how the Proceedings of the 31st Child Language Research Fo-
space constructed by ISA depends on the order in rum: 167-178.
which sentences are presented to it. This could shed Li, P., I. Farkas and B. MacWhinney. 2004. Early lexical
some light on the issue of how different experien- acquisition in a self-organizing neural network. Neu-
tial paths might lead to different semantic general- ral Networks, 17(8-9): 1345-1362.
izations. Manning Ch. and H. Sch¨utze. 1999. Foundations of sta-
While these and many other experiments must be tistical natural language processing The MIT Press,
run to help clarifying the properties and effective- Cambridge, MASS.
ness of ISA, we believe that the data presented here MacWhinney, B. 2000. The CHILDES project: Tools for
constitute a very promising beginning for this new analyzing talk (3d edition). Erlbaum, Mahwah, NJ.
line of research. Rowland, C., J. Pine, E. Lieven and A. Theakston.
2005. The incidence of error in young children’s wh-
questions. Journal of Speech, Language and Hearing
Research, 48(2): 384-404.
Sahlgren, M. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, Depart-
ment of Linguistics, Stockholm University.
References
Borovsky, A. and J. Elman. 2006. Language input and
semantic categories: a relation between cognition and
4We thank an anonymous reviewer for this note
56
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386680">
<title confidence="0.973246">ISA meets Lara: An incremental word space for cognitively plausible simulations of semantic learning</title>
<author confidence="0.993364">Marco</author>
<affiliation confidence="0.882744">CIMeC (University of C.so Bettini</affiliation>
<address confidence="0.992268">38068 Rovereto,</address>
<email confidence="0.997995">marco.baroni@unitn.it</email>
<author confidence="0.999919">Alessandro Lenci</author>
<affiliation confidence="0.999944">Department of Linguistics University of Pisa</affiliation>
<address confidence="0.991653">Via Santa Maria 36 56126 Pisa, Italy</address>
<author confidence="0.864979">Luca</author>
<affiliation confidence="0.981513">Department of Cornell</affiliation>
<address confidence="0.687715">Ithaca, NY</address>
<email confidence="0.984678">lo35@cornell.edualessandro.lenci@ilc.cnr.it</email>
<abstract confidence="0.9978338">introduce Semantic Analya fully incremental word space model, and we test it on longitudinal child-directed speech data. On this task, ISA outperforms the related Random Indexing algorithm, as well as a SVD-based technique. In addition, the model has interesting properties that might also be characteristic of the semantic space of children.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>