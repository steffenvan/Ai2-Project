<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.100982">
<title confidence="0.9983475">
TELIDA: A Package for Manipulation and Visualization of
Timed Linguistic Data
</title>
<author confidence="0.949291">
Titus von der Malsburg, Timo Baumann, David Schlangen
</author>
<affiliation confidence="0.9747765">
Department of Linguistics
University of Potsdam, Germany
</affiliation>
<email confidence="0.997939">
{malsburg|timo|das}@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967833333333">
We present a toolkit for manipulating and
visualising time-aligned linguistic data
such as dialogue transcripts or language
processing data. The package comple-
ments existing editing tools by allowing
for conversion between their formats, in-
formation extraction from the raw files,
and by adding sophisticated, and easily ex-
tended methods for visualising the dynam-
ics of dialogue processing. To illustrate
the versatility of the package, we describe
its use in three different projects at our site.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999666566666667">
Manual inspection and visualization of raw data is
often an important first step in the analysis of lin-
guistic data, be that transcripts of conversations or
records of the performance of processing modules.
Dialogue data or speech processing data in gen-
eral are typically temporally aligned, which poses
additional challenges for handling and visualiza-
tion. A number of tools are available for work-
ing with timed data, each with different focus:
as a small selection, Praat (Boersma, 2001) and
Wavesurfer (Sj¨olander and Beskow, 2000) excel at
acoustic analysis and are helpful for transcription
work, Anvil (Kipp, 2001) helps with the analysis
of video material, Exmaralda (Schmidt, 2004) of-
fers a suite of specialized tools for discourse anal-
ysis.
We developed TELIDA (TimEd LInguistic
DAta) to complement the strengths of these tools.
TELIDA comprises (a) a suite of Perl mod-
ules that offer flexible data structures for stor-
ing timed data; tools for converting data in other
formats to and from this format; a command-
line based interface for querying such data, en-
abling for example statistical analysis outside of
the original creators of transcriptions or annota-
tions; and (b) a lightweight but powerful visual-
ization tool, TEDview, that has certain unique fea-
tures, as will be described in Section 2.3. TEL-
IDA is available for download from http://www.
ling.uni-potsdam.de/~timo/code/telida/.
</bodyText>
<sectionHeader confidence="0.822571" genericHeader="method">
2 Overview of TELIDA
</sectionHeader>
<subsectionHeader confidence="0.992886">
2.1 Data Structures
</subsectionHeader>
<bodyText confidence="0.999467653846154">
Like the tools mentioned above, we handle timed
data as discrete labels which span a certain time
and contain some data. To give an example, in a
word-aligned transcription of a recording, a single
word would correspond to one label. Sequences
of (non-overlapping) labels are collected into what
we call alignments. In our example of the word-
aligned transcription, all words from one speaker
might be collected in one alignment.
This so far is a conceptualization that is com-
mon to many tools. In Praat for example, our
alignments would be called a tier. TELIDA adds a
further, novel, abstraction, by treating alignments
as belief states that can have a time (namely that
of their formation) as well. Concretely, an incre-
mental ASR may hypothesize a certain way of an-
alyzing a stretch of sound at one point, but at a
later point might slighlty adapt this analysis; in our
conceptualization, this would be two alignments
that model the same original data, each with a time
stamp. For other applications, timed belief states
may contain other information, e.g. new states of
parse constructions or dialogue manager informa-
tion states. We also allow to store several of such
alignment sequences (= successive belief states) in
parallel, to represent n-best lists.
</bodyText>
<subsubsectionHeader confidence="0.791407">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 302–305,
</subsubsectionHeader>
<affiliation confidence="0.947785">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998303">
302
</page>
<figureCaption confidence="0.99971">
Figure 1: TEDview Showing Annotated Dialogue Data
</figureCaption>
<bodyText confidence="0.999896714285714">
A document finally can consist of collections
of such alignments that reference the same time-
line, but model different aspects of the base-data.
For example, we may want to store information
about turns, how they decompose into words, and
into phonemes; or, for dual-channel dialogue, have
separate alignments for the different speakers.
</bodyText>
<subsectionHeader confidence="0.997669">
2.2 Data Manipulation Tools
</subsectionHeader>
<bodyText confidence="0.99985825">
In order to process timed linguistic data, we im-
plemented a Perl library and command-line tools,
TGtool and INtool for non-incremental and incre-
mental data respectively. They facilitate handling
(showing, merging, editing, ...) and processing
(search-and-replace, hypothesis filtering, ...) of
data and interface to TEDview for interactive vi-
sualization.
</bodyText>
<subsectionHeader confidence="0.994244">
2.3 TEDview
</subsectionHeader>
<bodyText confidence="0.99889184">
TEDview is the visualization component of TEL-
IDA. It organizes the different sources of informa-
tion (i.e., alignments or alignment sequences) in
horizontal tracks. Similar as in many of the above-
mentioned tools, time progresses from left to right
in those tracks. The content of tracks consists of
events that are displayed as bars if they have a tem-
poral extent or as diamonds otherwise. TEDview
uses a player metaphor and therefore has a cursor
that marks the current time and a play-mode that
can be used to replay recorded sequences of events
(in real-time or sped-up / slowed-down). Unlike in
other tools, TEDview has a steady cursor (the red
line in the Figures) across which events flow, and
this cursor can be moved, e.g. to give a configura-
tion where no future events are shown.
Information encapsulated by events is displayed
in two different ways:
a) Labels are represented as bars, with the la-
bel information shown as text. (Figure 1 shows a
configuration with only labels.)
b) Events without duration are displayed as di-
amonds at the appropriate time (all other Figures).
Such events can carry a “payload”; depending on
its type, different display methods are chosen:
</bodyText>
<listItem confidence="0.772424714285714">
• If the payload is an alignment, it is displayed
on the same track, as a sequence of labels.
• In all other cases TEDview determines the
data type of the information and selects an appro-
priate plug-in for displaying it in a separate inspec-
tor window. These data types can be syntax trees,
probability distributions, etc.
</listItem>
<bodyText confidence="0.999933090909091">
To avoid visual clutter, only the information
contained in the diamonds that most recently
passed the cursor are displayed. In this way, TED-
view can elegantly visualize the dynamics of in-
formation state development.
Events can be fed to TEDview either from a file,
in a use case where pre-recorded material is re-
played for analysis, or online, via a network con-
nection, in use cases where processing compo-
nents are monitored or profiled in real-time. The
format used to encode events and their encapsu-
</bodyText>
<page confidence="0.998299">
303
</page>
<figureCaption confidence="0.861485">
Figure 2: TEDview showing different filtering
strategies for incremental ASR: Diamonds corre-
spond to edits of the hypothesis.
</figureCaption>
<bodyText confidence="0.999888923076923">
lated information is a simple and generic XML
format (which the data manipulation tools can cre-
ate out of other formats, if necessary), i.e. the for-
mat does not make any assumptions as to what the
events represent. For this reason TEDview can be
used to visualize almost any type of discrete tem-
poral data. Intervals can be adorned with display
information, for example to encode further infor-
mation via colouring. Plug-ins for special data-
types can be written in the programming language
Python with its powerful library of extension mod-
ules; this enabled us to implement an inspector for
syntax trees in only 20 lines of code.
</bodyText>
<sectionHeader confidence="0.995697" genericHeader="method">
3 Use Cases
</sectionHeader>
<bodyText confidence="0.99991425">
To illustrate the versatility of the tool, we now de-
scribe how we use it in several projects at our site.
(Technical manuals can be downloaded from the
page listed above.)
</bodyText>
<subsectionHeader confidence="0.999954">
3.1 Analysis of Dialogue Data
</subsectionHeader>
<bodyText confidence="0.99872325">
In the DEAWU project (see e.g. (Schlangen and
Fern´andez, 2007)), we used the package to main-
tain transcriptions made in Praat and annotations
made in MMAX2 (M¨uller and Strube, 2006), and
to visualize these together in a time-aligned view.
As Figure 1 shows, we made heavy use of the
possibility of encoding information via colour. In
the example, there is one track (mac, for mouse
activity) where a numerical value (how much the
mouse travels in a certain time frame) is visual-
ized through the colouring of the interval. In other
tracks other information is encoded through colour
as well. We found this to be of much use in the
“getting to know the data” phase of the analysis of
our experiment. We have also used the tool and
the data in teaching about dialogue structure.
</bodyText>
<figureCaption confidence="0.929246">
Figure 3: TEDview showing 5-best incremental
ASR hypotheses.
</figureCaption>
<subsectionHeader confidence="0.99992">
3.2 Analysis of SDS Performance
</subsectionHeader>
<bodyText confidence="0.99998884">
In another project, we use TELIDA to analyze and
visualize the incremental output of several mod-
ules of a spoken dialogue system we are currently
developing.
In incremental speech recognition, what is con-
sidered the best hypothesis frequently changes as
more speech comes in. We used TEDview to an-
alyze these changes and to develop filtering meth-
ods to reduce the jitter and to reduce edits of the
ASR’s incremental hypothesis (Baumann et al.,
2009a). Figure 2 shows incremental hypotheses
and different settings of two filtering strategies.
When evaluating the utility of using n-best ASR
hypotheses, we used TEDview to visualize the
best hypotheses (Baumann et al., 2009b). An in-
teresting result we got from this analysis is that
typically the best hypothesis seems to be more sta-
ble than lower-ranked hypotheses, as can be seen
in Figure 3.
We also evaluated the behaviour of our in-
cremental reference resolution module, which
outputs distributions over possible referents
(Schlangen et al., 2009). We implemented a TED-
view plug-in to show distributions in bar-charts, as
can be seen in Figure 4.
</bodyText>
<subsectionHeader confidence="0.999981">
3.3 Analysis of Cognitive Models
</subsectionHeader>
<bodyText confidence="0.9999654">
In another project, we use TEDview to visualize
the output of an ACT-R (Anderson et al., 2004)
simulation of human sentence parsing developed
by (Patil et al., 2009). This model produces
predictions of parsing costs based on working-
memory load which in turn are used to predict
eye tracking measures in reading. Figure 5 shows
an example where the German sentence “Den Ton
gab der K¨unstler seinem Gehilfen” (the artist gives
the clay to his assistant) is being parsed, taking
</bodyText>
<page confidence="0.998473">
304
</page>
<figureCaption confidence="0.8285275">
Figure 5: TEDview visualizing the dynamics of
an ACT-R simulation, including the current parse-
tree.
Figure 4: TEDview showing the output of our in-
cremental reference resolution module. Distribu-
tions are shown with a bar-chart plug-in.
</figureCaption>
<bodyText confidence="0.98644668">
about 3 seconds of simulated time. The items in
the channel labeled “Memory” indicate retrievals
of items from memory, the items in the channel la-
beled “Parse” indicate that the parser produced a
new hypothesis, and the inspector window on the
right shows the latest of these hypotheses accord-
ing to cursor time. The grey bars finally in the
remaining channels show the activity of the pro-
duction rules. Such visualizations help to quickly
grasp the behaviour of a model, and so greatly aid
development and debugging.
Timo Baumann, Okko Buß, Michaela Atterer, and David
Schlangen. 2009b. Evaluating the Potential Utility of
ASR N-Best Lists for Incremental Spoken Dialogue Sys-
tems. In Proceedings of Interspeech 2009, Brighton, UK.
Paul Boersma. 2001. Praat, a system for doing phonetics by
computer. Glot International, 5(9–10):341–345.
Michael Kipp. 2001. Anvil - a generic annotation tool for
multimodal dialogue. In Proceedings of the 7th Euro-
pean Conference on Speech Communication and Technol-
ogy (Eurospeech), pages 1367–1370, Aalborg, Denmark.
Christoph M¨uller and Michael Strube. 2006. Multi-level an-
notation of linguistic data with MMAX2. In Corpus Tech-
nology and Language Pedagogy: New Resources, New
Tools, New Methods, pages 197–214. Peter Lang.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.997565307692308">
We presented TELIDA, a package for the manip-
ulation and visualization of temporally aligned
(linguistic) data. The package enables convenient
handling of dynamic data, especially from incre-
mental processing, but more generally from all
kinds of belief update. We believe that it can be
of use to anyone who is interested in exploring
complex state changes over time, be that in
dialogue annotations or in system performance
profiles.
Acknowledgments This work was funded by
a grant from DFG in the Emmy Noether Pro-
gramme.
</bodyText>
<sectionHeader confidence="0.999607" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821107142857">
J.R. Anderson, D. Bothell, M.D. Byrne, S. Douglass,
C. Lebiere, and Y. Qin. 2004. An integrated theory of
the mind. Psychological Review, 111(4):1036–1060.
Timo Baumann, Michaela Atterer, and David Schlangen.
2009a. Assessing and Improving the Performance of
Speech Recognition for Incremental Systems. In Proceed-
ings of NAACL-HLT 2009, Boulder, USA.
Umesh Patil, Marisa Ferrara Boston, John T. Hale, Shravan
Vasishth, and Reinhold Kliegl. 2009. The interaction of
surprisal and working memory cost during reading. In
Proc. of the CUNYsentence processing conference, Davis,
USA.
David Schlangen and Raquel Fern´andez. 2007. Speaking
through a noisy channel - experiments on inducing clarifi-
cation behaviour in human-human dialogue. In Proceed-
ings of Interspeech 2007, Antwerp, Belgium.
David Schlangen, Timo Baumann, and Michaela Atterer.
2009. Incremental Reference Resolution: The Task, Met-
rics for Evaluation, and a Bayesian Filtering Model that is
Sensitive to Disfluencies. In Proc. of SigDial 2009, Lon-
don, UK.
Thomas Schmidt. 2004. Transcribing and annotating spoken
language with exmaralda. In Proceedings of the LREC-
Workshop on XML based richly annotated corpora, Lis-
bon 2004, Paris. ELRA. EN.
K. Sj¨olander and J. Beskow. 2000. Wavesurfer—an open
source speech tool. In Sixth International Conference on
Spoken Language Processing, Beijing, China. ISCA.
</reference>
<page confidence="0.999238">
305
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.990576">
<title confidence="0.9980505">TELIDA: A Package for Manipulation and Visualization Timed Linguistic Data</title>
<author confidence="0.997544">Titus von_der_Malsburg</author>
<author confidence="0.997544">Timo Baumann</author>
<author confidence="0.997544">David</author>
<affiliation confidence="0.999598">Department of University of Potsdam,</affiliation>
<abstract confidence="0.999809769230769">We present a toolkit for manipulating and visualising time-aligned linguistic data such as dialogue transcripts or language processing data. The package complements existing editing tools by allowing for conversion between their formats, information extraction from the raw files, and by adding sophisticated, and easily extended methods for visualising the dynamics of dialogue processing. To illustrate the versatility of the package, we describe its use in three different projects at our site.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Anderson</author>
<author>D Bothell</author>
<author>S Douglass Byrne</author>
<author>C Lebiere</author>
<author>Y Qin</author>
</authors>
<title>An integrated theory of the mind.</title>
<date>2004</date>
<journal>Psychological Review,</journal>
<volume>111</volume>
<issue>4</issue>
<contexts>
<context position="9537" citStr="Anderson et al., 2004" startWordPosition="1539" endWordPosition="1542">used TEDview to visualize the best hypotheses (Baumann et al., 2009b). An interesting result we got from this analysis is that typically the best hypothesis seems to be more stable than lower-ranked hypotheses, as can be seen in Figure 3. We also evaluated the behaviour of our incremental reference resolution module, which outputs distributions over possible referents (Schlangen et al., 2009). We implemented a TEDview plug-in to show distributions in bar-charts, as can be seen in Figure 4. 3.3 Analysis of Cognitive Models In another project, we use TEDview to visualize the output of an ACT-R (Anderson et al., 2004) simulation of human sentence parsing developed by (Patil et al., 2009). This model produces predictions of parsing costs based on workingmemory load which in turn are used to predict eye tracking measures in reading. Figure 5 shows an example where the German sentence “Den Ton gab der K¨unstler seinem Gehilfen” (the artist gives the clay to his assistant) is being parsed, taking 304 Figure 5: TEDview visualizing the dynamics of an ACT-R simulation, including the current parsetree. Figure 4: TEDview showing the output of our incremental reference resolution module. Distributions are shown with</context>
</contexts>
<marker>Anderson, Bothell, Byrne, Lebiere, Qin, 2004</marker>
<rawString>J.R. Anderson, D. Bothell, M.D. Byrne, S. Douglass, C. Lebiere, and Y. Qin. 2004. An integrated theory of the mind. Psychological Review, 111(4):1036–1060.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>Michaela Atterer</author>
<author>David Schlangen</author>
</authors>
<title>Assessing and Improving the Performance of Speech Recognition for Incremental Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT 2009,</booktitle>
<location>Boulder, USA.</location>
<contexts>
<context position="8758" citStr="Baumann et al., 2009" startWordPosition="1413" endWordPosition="1416"> We have also used the tool and the data in teaching about dialogue structure. Figure 3: TEDview showing 5-best incremental ASR hypotheses. 3.2 Analysis of SDS Performance In another project, we use TELIDA to analyze and visualize the incremental output of several modules of a spoken dialogue system we are currently developing. In incremental speech recognition, what is considered the best hypothesis frequently changes as more speech comes in. We used TEDview to analyze these changes and to develop filtering methods to reduce the jitter and to reduce edits of the ASR’s incremental hypothesis (Baumann et al., 2009a). Figure 2 shows incremental hypotheses and different settings of two filtering strategies. When evaluating the utility of using n-best ASR hypotheses, we used TEDview to visualize the best hypotheses (Baumann et al., 2009b). An interesting result we got from this analysis is that typically the best hypothesis seems to be more stable than lower-ranked hypotheses, as can be seen in Figure 3. We also evaluated the behaviour of our incremental reference resolution module, which outputs distributions over possible referents (Schlangen et al., 2009). We implemented a TEDview plug-in to show distr</context>
</contexts>
<marker>Baumann, Atterer, Schlangen, 2009</marker>
<rawString>Timo Baumann, Michaela Atterer, and David Schlangen. 2009a. Assessing and Improving the Performance of Speech Recognition for Incremental Systems. In Proceedings of NAACL-HLT 2009, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umesh Patil</author>
<author>Marisa Ferrara Boston</author>
<author>John T Hale</author>
<author>Shravan Vasishth</author>
<author>Reinhold Kliegl</author>
</authors>
<title>The interaction of surprisal and working memory cost during reading.</title>
<date>2009</date>
<booktitle>In Proc. of the CUNYsentence processing conference,</booktitle>
<location>Davis, USA.</location>
<contexts>
<context position="9608" citStr="Patil et al., 2009" startWordPosition="1550" endWordPosition="1553">interesting result we got from this analysis is that typically the best hypothesis seems to be more stable than lower-ranked hypotheses, as can be seen in Figure 3. We also evaluated the behaviour of our incremental reference resolution module, which outputs distributions over possible referents (Schlangen et al., 2009). We implemented a TEDview plug-in to show distributions in bar-charts, as can be seen in Figure 4. 3.3 Analysis of Cognitive Models In another project, we use TEDview to visualize the output of an ACT-R (Anderson et al., 2004) simulation of human sentence parsing developed by (Patil et al., 2009). This model produces predictions of parsing costs based on workingmemory load which in turn are used to predict eye tracking measures in reading. Figure 5 shows an example where the German sentence “Den Ton gab der K¨unstler seinem Gehilfen” (the artist gives the clay to his assistant) is being parsed, taking 304 Figure 5: TEDview visualizing the dynamics of an ACT-R simulation, including the current parsetree. Figure 4: TEDview showing the output of our incremental reference resolution module. Distributions are shown with a bar-chart plug-in. about 3 seconds of simulated time. The items in t</context>
</contexts>
<marker>Patil, Boston, Hale, Vasishth, Kliegl, 2009</marker>
<rawString>Umesh Patil, Marisa Ferrara Boston, John T. Hale, Shravan Vasishth, and Reinhold Kliegl. 2009. The interaction of surprisal and working memory cost during reading. In Proc. of the CUNYsentence processing conference, Davis, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Raquel Fern´andez</author>
</authors>
<title>Speaking through a noisy channel - experiments on inducing clarification behaviour in human-human dialogue.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<location>Antwerp, Belgium.</location>
<marker>Schlangen, Fern´andez, 2007</marker>
<rawString>David Schlangen and Raquel Fern´andez. 2007. Speaking through a noisy channel - experiments on inducing clarification behaviour in human-human dialogue. In Proceedings of Interspeech 2007, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Timo Baumann</author>
<author>Michaela Atterer</author>
</authors>
<title>Incremental Reference Resolution: The Task, Metrics for Evaluation, and a Bayesian Filtering Model that is Sensitive to Disfluencies.</title>
<date>2009</date>
<booktitle>In Proc. of SigDial</booktitle>
<location>London, UK.</location>
<contexts>
<context position="9310" citStr="Schlangen et al., 2009" startWordPosition="1499" endWordPosition="1502"> reduce edits of the ASR’s incremental hypothesis (Baumann et al., 2009a). Figure 2 shows incremental hypotheses and different settings of two filtering strategies. When evaluating the utility of using n-best ASR hypotheses, we used TEDview to visualize the best hypotheses (Baumann et al., 2009b). An interesting result we got from this analysis is that typically the best hypothesis seems to be more stable than lower-ranked hypotheses, as can be seen in Figure 3. We also evaluated the behaviour of our incremental reference resolution module, which outputs distributions over possible referents (Schlangen et al., 2009). We implemented a TEDview plug-in to show distributions in bar-charts, as can be seen in Figure 4. 3.3 Analysis of Cognitive Models In another project, we use TEDview to visualize the output of an ACT-R (Anderson et al., 2004) simulation of human sentence parsing developed by (Patil et al., 2009). This model produces predictions of parsing costs based on workingmemory load which in turn are used to predict eye tracking measures in reading. Figure 5 shows an example where the German sentence “Den Ton gab der K¨unstler seinem Gehilfen” (the artist gives the clay to his assistant) is being parse</context>
</contexts>
<marker>Schlangen, Baumann, Atterer, 2009</marker>
<rawString>David Schlangen, Timo Baumann, and Michaela Atterer. 2009. Incremental Reference Resolution: The Task, Metrics for Evaluation, and a Bayesian Filtering Model that is Sensitive to Disfluencies. In Proc. of SigDial 2009, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Schmidt</author>
</authors>
<title>Transcribing and annotating spoken language with exmaralda.</title>
<date>2004</date>
<booktitle>In Proceedings of the LRECWorkshop on XML based richly annotated corpora,</booktitle>
<publisher>EN.</publisher>
<location>Lisbon</location>
<contexts>
<context position="1440" citStr="Schmidt, 2004" startWordPosition="210" endWordPosition="211"> step in the analysis of linguistic data, be that transcripts of conversations or records of the performance of processing modules. Dialogue data or speech processing data in general are typically temporally aligned, which poses additional challenges for handling and visualization. A number of tools are available for working with timed data, each with different focus: as a small selection, Praat (Boersma, 2001) and Wavesurfer (Sj¨olander and Beskow, 2000) excel at acoustic analysis and are helpful for transcription work, Anvil (Kipp, 2001) helps with the analysis of video material, Exmaralda (Schmidt, 2004) offers a suite of specialized tools for discourse analysis. We developed TELIDA (TimEd LInguistic DAta) to complement the strengths of these tools. TELIDA comprises (a) a suite of Perl modules that offer flexible data structures for storing timed data; tools for converting data in other formats to and from this format; a commandline based interface for querying such data, enabling for example statistical analysis outside of the original creators of transcriptions or annotations; and (b) a lightweight but powerful visualization tool, TEDview, that has certain unique features, as will be descri</context>
</contexts>
<marker>Schmidt, 2004</marker>
<rawString>Thomas Schmidt. 2004. Transcribing and annotating spoken language with exmaralda. In Proceedings of the LRECWorkshop on XML based richly annotated corpora, Lisbon 2004, Paris. ELRA. EN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sj¨olander</author>
<author>J Beskow</author>
</authors>
<title>Wavesurfer—an open source speech tool.</title>
<date>2000</date>
<booktitle>In Sixth International Conference on Spoken Language Processing,</booktitle>
<location>Beijing, China. ISCA.</location>
<marker>Sj¨olander, Beskow, 2000</marker>
<rawString>K. Sj¨olander and J. Beskow. 2000. Wavesurfer—an open source speech tool. In Sixth International Conference on Spoken Language Processing, Beijing, China. ISCA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>