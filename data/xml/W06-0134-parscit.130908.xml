<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003599">
<title confidence="0.996952">
A Pragmatic Chinese Word Segmentation System
</title>
<author confidence="0.998556">
Wei Jiang, Yi Guan, Xiao-Long Wang
</author>
<affiliation confidence="0.999293">
School of Computer Science and Technology, Harbin Institute of Technology,
</affiliation>
<address confidence="0.924795">
Heilongjiang Province, 150001, P.R.China
</address>
<email confidence="0.997039">
jiangwei@insun.hit.edu.cn
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998671714285715">
This paper presents our work for partici-
pation in the Third International Chinese
Word Segmentation Bakeoff. We apply
several processing approaches according
to the corresponding sub-tasks, which are
exhibited in real natural language. In our
system, Trigram model with smoothing
algorithm is the core module in word
segmentation, and Maximum Entropy
model is the basic model in Named En-
tity Recognition task. The experiment
indicates that this system achieves F-
measure 96.8% in MSRA open test in the
third SIGHAN-2006 bakeoff.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998915">
Word is a logical semantic and syntactic unit in
natural language. Unlike English, there is no de-
limiter to mark word boundaries in Chinese lan-
guage, so in most Chinese NLP tasks, word seg-
mentation is a foundation task, which transforms
Chinese character string into word sequence. It is
prerequisite to POS tagger, parser or further ap-
plications, such as Information Extraction, Ques-
tion Answer system.
Our system participated in the Third Interna-
tional Chinese Word Segmentation Bakeoff,
which held in 2006. Compared with our system
in the last bakeoff (Jiang 2005A), the system in
the third bakeoff is adjusted intending to have a
better pragmatic performance. This paper mainly
focuses on describing two sub-tasks: (1) The ba-
sic Word Segmentation; (2) Named entities rec-
ognition. We apply different approaches to solve
above two tasks, and all the modules are inte-
grated into a pragmatic system (FLUS).
</bodyText>
<sectionHeader confidence="0.988781" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.942033666666667">
All the words in our system are categorized into
five types: Lexicon words (LW), Factoid words
(FT), Morphologically derived words (MDW),
</bodyText>
<figureCaption confidence="0.825515">
Named entities (NE), and New words (NW).
Figure 1 demonstrates our system structure.
Figure 1 ELUS Segmenter and NER
</figureCaption>
<bodyText confidence="0.99993">
The input character sequence is converted into
one or several sentences, which is the basic deal-
ing unit. The “Basic Segmentation” is used to
identify the LW, FT, MDW words, and “Named
Entity Recognition” is used to detect NW words.
We don’t adopt the New Word detection algo-
rithm in our system in this bakeoff. The “Disam-
biguation” module performs to classify compli-
cated ambiguous words, and all the above results
are connected into the final result, which is de-
noted by XML format.
</bodyText>
<subsectionHeader confidence="0.991375">
2.1 Trigram and Smoothing Algorithm
</subsectionHeader>
<bodyText confidence="0.99608725">
We apply the trigram model to the word segmen-
tation task (Jiang 2005A), and make use of Ab-
solute Smoothing algorithm to overcome the
sparse data problem.
Trigram model is used to convert the sentence
into a word sequence. Let w = w1 w2 ...wn be a
word sequence, then the most likely word se-
quence w* in trigram is:
</bodyText>
<equation confidence="0.993462">
n
w* = arg max ∏ P(wi  |wi_2wi_1) (1)
w1w2...wn i=1
</equation>
<bodyText confidence="0.9998992">
where let P(w0|w-2 w-1) be P(w0) and let P(w1|w-1
w0) be P(w1|w0), and wi represents LW or a type
of FT or MDW. In order to search the best seg-
mentation way, all the word candidates are filled
in the word lattice (Zhao 2005). And the Viterbi
</bodyText>
<figure confidence="0.998370285714286">
Disambiguation
Named Entity
Recognition
Basic
Segmentation
Sentence
Lexicon Words
Morphological
Word
Factoid Detect
Word
Sequence
Input
Sequence
</figure>
<page confidence="0.982974">
189
</page>
<bodyText confidence="0.98500612">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 189–192,
Sydney, July 2006. c�2006 Association for Computational Linguistics
algorithm is used to search the best word seg-
mentation path.
FT and MDW need to be detected when con-
structing word lattice (detailed in section 2.2).
The data structure of lexicon can affect the effi-
ciency of word segmentation, so we represent
lexicon words as a set of TRIEs, which is a tree-
like structure. Words starting with the same
character are represented as a TRIE, where the
root represents the first Chinese character, and
the children of the root represent the second
characters, and so on (Gao 2004).
When searching a word lattice, there is the
zero-probability phenomenon, due to the sparse
data problem. For instance, if there is no cooc-
curence pair “我们/吃/香蕉”(we eat bananas) in
the training corpus, then P(香蕉|我们,吃) = 0.
According to formula (1), the probability of the
whole candidate path, which includes “我们/吃/
香蕉” is zero, as a result of the local zero prob-
ability. In order to overcome the sparse data
problem, our system has applied Absolute Dis-
counting Smoothing algorithm (Chen, 1999).
</bodyText>
<equation confidence="0.988367">
i 1
N1+ (wi−n+1 •) = |{wi : c(wi−n+1wi) &gt; 0} |(2)
</equation>
<bodyText confidence="0.999918">
The notation N1+ is meant to evoke the number
of words that have one or more counts, and the •
is meant to evoke a free variable that is summed
over. The function c() represents the count of
one word or the cooccurence count of multi-
words. In this case, the smoothing probability
</bodyText>
<equation confidence="0.993336045454545">
max{ ( ) ,
c w i − D
1 i n
− + 1
)
=
∑− +
c w i
( i n
wi
⎛ ⎞ ⎜D ⎟
where, λ N w i −
1 1
− = ⎜ • ⎟ (4)
+ − +
( )
⎜ ∑ − +
i 1 i n 1
c w
( ) ⎟
i n 1
⎝ w i ⎠
</equation>
<bodyText confidence="0.99996625">
Because we use trigram model, so the maxi-
mum n may be 3. A fixed discount D (0≤ D≤ 1)
can be set through the deleted estimation on the
training data. They arrive at the estimate
</bodyText>
<equation confidence="0.9749865">
n
D = 1 (5)
n 2n
1 + 2
</equation>
<bodyText confidence="0.999885333333333">
where n1 and n2 are the total number of n-grams
with exactly one and two counts, respectively.
After the basic segmentation, some compli-
cated ambiguous segmentation can be further
disambiguated. In trigram model, only the previ-
ous two words are considered as context features,
while in disambiguation processing, we can use
the Maximum Entropy model fused more fea-
tures (Jiang 2005B) or rule based method.
</bodyText>
<subsectionHeader confidence="0.996033">
2.2 Factoid and Morphological words
</subsectionHeader>
<bodyText confidence="0.9976938">
All the Factoid words can be represented as regu-
lar expressions. So the detection of factoid words
can be achieved by Finite State Automaton(FSA).
In our system, the following categories of factoid
words can be detected, as shown in table 1.
</bodyText>
<tableCaption confidence="0.996638">
Table 1 Factoid word categories
</tableCaption>
<table confidence="0.9995299">
FT type Factoid word Example
Number Integer, real, 2910, 46.12%, 二十
percent etc. fit, 三千七百二十
Date Date 2005 年 5 月 12 日
Time Time 8:00, 十点二十分
English English word, How, are, you
www Website, IP http://www.hit.edu.cn
address 192.168.140.133
email Email elus@google.com
phone Phone, fax 0451-86413322
</table>
<bodyText confidence="0.9919851">
Deterministic FSA (DFA) is efficient because
a unique “next state” is determined, when given
an input symbol and the current state. While it is
common for a linguist to write rule, which can be
represented directly as a non-deterministic FSA
(NFA), i.e. which allows several “next states” to
follow a given input and state.
Since every NFA has an equivalent DFA, we
build a FT rule compiler to convert all the FT
generative rules into a DFA. e.g.
</bodyText>
<listItem confidence="0.999895333333333">
• “&lt; digit &gt; -&gt; [0..9];
• &lt; year &gt; ::= &lt; digit &gt;{&lt; digit &gt;+}年”;
• &lt; integer &gt; ::= {&lt; digit &gt;+};
</listItem>
<bodyText confidence="0.9999806">
where “-&gt;” is a temporary generative rule, and
“::=” is a real generative rule.
As for the morphological words, we erase the
dealing module, because the word segmentation
definition of our system adopts the PKU standard.
</bodyText>
<sectionHeader confidence="0.995964" genericHeader="method">
3 Named Entity Recognition
</sectionHeader>
<bodyText confidence="0.999940529411765">
We adopt Maximum Entropy model to perform
the Named Entity Recognition. The extensive
evaluation on NER systems in recent years (such
as CoNLL-2002 and CoNLL-2003) indicates the
best statistical systems are typically achieved by
using a linear (or log-linear) classification algo-
rithm, such as Maximum Entropy model, to-
gether with a vast amount of carefully designed
linguistic features. And this seems still true at
present in terms of statistics based methods.
Maximum Entropy model (ME) is defined
over H × T in segmentation disambiguation,
where H is the set of possible contexts around
target word that will be tagged, and T is the set of
allowable tags, such as B-PER, I-PER, B-LOC,
I-LOC etc. in our NER task. Then the model’s
conditional probability is defined as
</bodyText>
<equation confidence="0.978961458333333">
i
−
p(wi wi
1
+
n
1
)
}
0
(1 ) (  |1 )
λ p wi wi n (3)
i −
+ − − + 2
190
p h t
( , )
p t h
(  |) = (6)
p(h, t&apos; )
�t=T
k
α fj (h,t) (7)
j
</equation>
<bodyText confidence="0.999265666666667">
where h is the current context and t is one of the
possible tags.
The several typical kinds of features can be
used in the NER system. They usually include
the context feature, the entity feature, and the
total resource or some additional resources.
</bodyText>
<tableCaption confidence="0.685538">
Table 2 shows the context feature templates.
Table2 NER feature template1
</tableCaption>
<table confidence="0.96860075">
Type Feature Template
One order feature wi-2, wi-1, wi, wi+1, wi+2
Two order feature wi-1:i, wi:i+1
NER tag feature t i-1
</table>
<bodyText confidence="0.9993556">
While, we only point out the local feature
template, some other feature templates, such as
long distance dependency templates, are also
helpful to NER performance. These trigger fea-
tures can be collected by Average Mutual Infor-
mation or Information Gain algorithm etc.
Besides context features, entity features is an-
other important factor, such as the suffix of Lo-
cation or Organization. The following 8 kinds of
dictionaries are usually useful (Zhao 2006):
</bodyText>
<tableCaption confidence="0.962273">
Table 3 NER resource dictiona
</tableCaption>
<table confidence="0.9952483">
List Type Lexicon Example
Word list Place lexicon IL�, _$R�, ���
Chinese surname �, _T, A2�, W�m
String list Prefix of PER �, puJ, &amp;quot;J&amp;quot;
Suffix of PLA 111, ASS, �rf, -r 1 �
Suffix of ORG �, *&amp;, _41M, �
Character Character for CPER �,R1, �, -9, %
list
Character for FPER �, -0,, JW, 0, in,
Rare character �, U, 34
</table>
<bodyText confidence="0.999839357142857">
In addition, some external resources may im-
prove the NER performance too, e.g. we collect a
lot of entities for Chinese Daily Newspaper in
2000, and total some entity features.
However, our system is based on Peking Uni-
versity (PKU) word segmentation definition and
PKU NER definition, so we only used the basic
features in table 2 in this bakeoff. Another effect
is the corpus: our system is training by the Chi-
nese Peoples’ Daily Newspaper corpora in 1998,
which conforms to PKU NER definition. In the
section 4, we will give our system performance
with the basic features in Chinese Peoples’ Daily
Newspaper corpora.
</bodyText>
<footnote confidence="0.685686">
1 wi – current word, wi-1 – previous word, ti – current tag.
2 Partial translation: 4LAr BeiJing,纽约 New york;张 Zhang,
H_E Wang; -Iin Old;[l mountain,M lake;N bureau.
</footnote>
<sectionHeader confidence="0.750374" genericHeader="evaluation">
4 Performance and analysis
</sectionHeader>
<subsectionHeader confidence="0.998685">
4.1 The Evaluation in Word Segmentation
</subsectionHeader>
<bodyText confidence="0.99746525">
The performance of our system in the third bake-
off is presented in table 4 in terms of recall(R),
precision(P) and F score in percentages. The
score software is standard and open by SIGHAN.
</bodyText>
<tableCaption confidence="0.999416">
Table 4 MSRA test in SIGHAN2006 (%
</tableCaption>
<table confidence="0.997708333333333">
MSRA R P F OOV Roov Riv
Close 96.3 91.8 94.0 3.4 17.5 99.1
Open 97.7 96.0 96.8 3.4 62.4 98.9
</table>
<bodyText confidence="0.997253">
Our system has good performance in terms of
Riv measure. The Riv measure in close test and in
open test are 99.1% and 98.9% respectively. This
good performance owes to class-based trigram
with the absolute smoothing and word disam-
biguation algorithm.
In our system, it is the following reasons that
the open test has a better performance than the
close test:
</bodyText>
<listItem confidence="0.71052225">
(1) Named Entity Recognition module is
added into the open test system. And Named En-
tities, including PER, LOC, ORG, occupy the
most of the out-of-vocabulary words.
</listItem>
<bodyText confidence="0.939952277777778">
(2) The system of close test can only use the
dictionary that is collected from the given train-
ing corpus, while the system of open test can use
a better dictionary, which includes the words that
exist in MSRA training corpus in SIGHAN2005.
And we know, the dictionary is the one of impor-
tant factors that affects the performance, because
the LW candidates in the word lattice are gener-
ated from the dictionary.
As for the dictionary, we compare the two col-
lections in SIGHAN2005 and SIGHAN2006, and
evaluating in SIGHAN2005 MSRA close test.
There are less training sentence in SIGHAN2006,
as a result, there is at least 1.2% performance
decrease. So this result indicates that the diction-
ary can bring an important impact in our system.
Table 5 gives our system performance in the
second bakeoff. We’ll make brief comparison.
</bodyText>
<tableCaption confidence="0.996691">
Table 5 MSRA test in SIGHAN 2005 (%
</tableCaption>
<table confidence="0.954944333333333">
MSRA R P F OOV Roov Riv
Close 97.3 94.5 95.9 2.6 32.3 99.1
Open 98.0 96.5 97.2 2.6 59.0 99.0
</table>
<bodyText confidence="0.9681848">
Comparing table 4 with table 5, we find that
the OOV is 3.4 in third bakeoff, which is higher
than the value in the last bakeoff. Obviously, it is
one of reasons that affect our performance.
In addition, based on pragmatic consideration,
our system has been made some simplifier, for
instance, we erase the new word detection algo-
rithm and the is no morphological word detection.
where p(h,t)=;ryi I
j=1
</bodyText>
<page confidence="0.993087">
191
</page>
<subsectionHeader confidence="0.911632">
4.2 Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.9993024">
In MSRA NER open test, our NER system is
training in prior six-month corpora of Chinese
Peoples’ Daily Newspaper in 1998, which were
annotated by Peking University. Table 6 shows
the NER performance in the MSRA open test.
</bodyText>
<tableCaption confidence="0.98187">
Table 6 The NER performance in MSRA Open test
</tableCaption>
<table confidence="0.9987552">
MSRA NER Precision Recall F Score
PER 93.68% 86.37% 89.87
LOC 85.50% 59.67% 70.29
ORG 75.87% 47.48% 58.41
Overall 86.97% 65.56% 74.76
</table>
<bodyText confidence="0.999840117647059">
As a result of insufficiency in preparing bake-
off, our system is only trained in Chinese Peo-
ples’ Daily Newspaper, in which the NER is de-
fined according to PKU standard. However, the
NER definition of MSRA is different from that
of PKU, e.g, “rP*/LOC KN”, “ /PER 3�q
/PER -t-X” in MSRA, are not entities in PKU.
So the training corpus becomes a main handicap
to decrease the performance of our system, and it
also explains that there is much difference be-
tween the recall rate and the precision in table 6.
Table 7 gives the evaluation of our NER sys-
tem in Chinese Peoples’ Daily Newspaper, train-
ing in prior five-month corpora and testing in the
sixth month corpus. We also use the feature tem-
plates in table 2, in order to make comparison
with table 6.
</bodyText>
<tableCaption confidence="0.995671">
Table 7 The NER test in Chinese Peoples’ Daily
</tableCaption>
<table confidence="0.997437666666667">
MSRA NER Precision Recall F Score
CPN 93.56 90.96 92.24
FPN 90.42 86.47 88.40
LOC 91.94 90.52 91.22
ORG 88.38 84.52 86.40
Overall 91.35 88.85 90.08
</table>
<bodyText confidence="0.99495725">
This experiment indicates that our system can
have a good performance, if the test corpus and
the training corpora conform to the condition of
independent identically distributed attribution.
</bodyText>
<subsectionHeader confidence="0.999225">
4.3 Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.954451368421053">
Some points need to be further considered:
(1) The dictionary can bring a big impact to
the performance, as the LW candidates come
from the dictionary. However a big dictionary
can be easily acquired in the real application.
(2) Due to our technical and insufficiently
preparing problem, we use the PKU NER defini-
tion, however they seem not unified with the
MSRA definition.
(3) Our NER system is a word-based model,
and we have find out that the word segmentation
with two different dictionaries can bring a big
impact to the NER performance.
(4) We erase the new word recognition algo-
rithm in our system. While, we should explore
the real annotated corpora, and add new word
detection algorithm, if it has positive effect. e.g.
“荷花 奖”(lotus prize) can be recognized as one
word by the conditional random fields model.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999962461538461">
We have briefly described our word segmenta-
tion system and NER system. We use word-
based features in the whole processing. Our sys-
tem has a good performance in terms of Riv
measure, so this means that the trigram model
with the smoothing algorithm can deal with the
basic segmentation task well. However, the result
in the bakeoff indicates that detecting out-of-
vocabulary word seems to be a harder task than
dealing with the segmentation-ambiguity task.
The work in the future will concentrate on two
sides: improving the NER performance and add-
ing New Word Detection Algorithm.
</bodyText>
<sectionHeader confidence="0.999101" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998351214285714">
HuaPing Zhang, Qun Liu etc. 2003. Chinese Lexical
Analysis Using Hierarchical Hidden Markov
Model, Second SIGHAN workshop affiliated with
4th ACL, Sapporo Japan, pp.63-70.
Jianfeng Gao, Mu Li et al. 2004. Chinese Word Seg-
mentation: A Pragmatic Approach. MSR-TR-2004-
123, November 2004.
Peng Fuchun, Fangfang Feng and Andrew McCallum.
Chinese segmentation and new word detection us-
ing conditional random fields. In:COLING 2004.
Stanley F.Chen and J. Goodman. 1999. An empirical
study of smoothing techniques for language model-
ing. Computer Speech and Language. 13:369-394.
Wei Jiang, Jian Zhao et al. 2005A.Chinese Word
Segmentation based on Mixing Model. 4th
SIGHAN Workshop. pp. 180-182.
Wei Jiang, Xiao-Long Wang, Yi Guan et al. 2005B.
applying rough sets in word segmentation disam-
biguation based on maximum entropy model. Jour-
nal of Harbin Institute of Technology (New Series).
13(1): 94-98.
Zhao Jian. 2006. Research on Conditional Probabilis-
tic Model and Its Application in Chinese Named
Entity Recognition. Ph.D. Thesis. Harbin Institute
of Technology, China.
Zhao Yan. 2005. Research on Chinese Morpheme
Analysis Based on Statistic Language Model. Ph.D.
Thesis. Harbin Institute of Technology, China.
</reference>
<page confidence="0.998219">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.632618">
<title confidence="0.999841">A Pragmatic Chinese Word Segmentation System</title>
<author confidence="0.999708">Wei Jiang</author>
<author confidence="0.999708">Yi Guan</author>
<author confidence="0.999708">Xiao-Long Wang</author>
<affiliation confidence="0.98263">School of Computer Science and Technology, Harbin Institute of</affiliation>
<address confidence="0.928733">Heilongjiang Province, 150001, P.R.China</address>
<email confidence="0.866741">jiangwei@insun.hit.edu.cn</email>
<abstract confidence="0.9812328">This paper presents our work for participation in the Third International Chinese Word Segmentation Bakeoff. We apply several processing approaches according to the corresponding sub-tasks, which are exhibited in real natural language. In our system, Trigram model with smoothing algorithm is the core module in word segmentation, and Maximum Entropy model is the basic model in Named Entity Recognition task. The experiment indicates that this system achieves Fmeasure 96.8% in MSRA open test in the third SIGHAN-2006 bakeoff.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>HuaPing Zhang</author>
</authors>
<title>Qun Liu etc.</title>
<date>2003</date>
<booktitle>Second SIGHAN workshop affiliated with 4th ACL,</booktitle>
<pages>63--70</pages>
<location>Sapporo</location>
<marker>Zhang, 2003</marker>
<rawString>HuaPing Zhang, Qun Liu etc. 2003. Chinese Lexical Analysis Using Hierarchical Hidden Markov Model, Second SIGHAN workshop affiliated with 4th ACL, Sapporo Japan, pp.63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
</authors>
<title>Chinese Word Segmentation: A Pragmatic Approach.</title>
<date>2004</date>
<pages>2004--123</pages>
<marker>Gao, Li, 2004</marker>
<rawString>Jianfeng Gao, Mu Li et al. 2004. Chinese Word Segmentation: A Pragmatic Approach. MSR-TR-2004-123, November 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Fuchun</author>
</authors>
<title>Fangfang Feng and Andrew McCallum. Chinese segmentation and new word detection using conditional random fields. In:COLING</title>
<date>2004</date>
<marker>Fuchun, 2004</marker>
<rawString>Peng Fuchun, Fangfang Feng and Andrew McCallum. Chinese segmentation and new word detection using conditional random fields. In:COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. Computer Speech and Language.</title>
<date>1999</date>
<pages>13--369</pages>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F.Chen and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language. 13:369-394.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wei Jiang</author>
<author>Jian Zhao</author>
</authors>
<booktitle>2005A.Chinese Word Segmentation based on Mixing Model. 4th SIGHAN Workshop.</booktitle>
<pages>180--182</pages>
<marker>Jiang, Zhao, </marker>
<rawString>Wei Jiang, Jian Zhao et al. 2005A.Chinese Word Segmentation based on Mixing Model. 4th SIGHAN Workshop. pp. 180-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Jiang</author>
<author>Xiao-Long Wang</author>
<author>Yi Guan</author>
</authors>
<title>applying rough sets in word segmentation disambiguation based on maximum entropy model.</title>
<date>2005</date>
<journal>Journal of Harbin Institute of Technology (New Series).</journal>
<volume>13</volume>
<issue>1</issue>
<pages>94--98</pages>
<marker>Jiang, Wang, Guan, 2005</marker>
<rawString>Wei Jiang, Xiao-Long Wang, Yi Guan et al. 2005B. applying rough sets in word segmentation disambiguation based on maximum entropy model. Journal of Harbin Institute of Technology (New Series). 13(1): 94-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Jian</author>
</authors>
<title>Research on Conditional Probabilistic Model and Its Application in Chinese Named Entity Recognition.</title>
<date>2006</date>
<tech>Ph.D. Thesis.</tech>
<institution>Harbin Institute of Technology,</institution>
<marker>Jian, 2006</marker>
<rawString>Zhao Jian. 2006. Research on Conditional Probabilistic Model and Its Application in Chinese Named Entity Recognition. Ph.D. Thesis. Harbin Institute of Technology, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Yan</author>
</authors>
<title>Research on Chinese Morpheme Analysis Based on Statistic Language Model.</title>
<date>2005</date>
<tech>Ph.D. Thesis.</tech>
<institution>Harbin Institute of Technology,</institution>
<marker>Yan, 2005</marker>
<rawString>Zhao Yan. 2005. Research on Chinese Morpheme Analysis Based on Statistic Language Model. Ph.D. Thesis. Harbin Institute of Technology, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>