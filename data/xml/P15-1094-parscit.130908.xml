<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040325">
<title confidence="0.7273315">
Jointly optimizing word representations for lexical and sentential tasks
with the C-PHRASE model
</title>
<author confidence="0.885971">
Nghia The Pham Germ´an Kruszewski Angeliki Lazaridou Marco Baroni
</author>
<affiliation confidence="0.9738375">
Center for Mind/Brain Sciences
University of Trento
</affiliation>
<email confidence="0.992051">
{thenghia.pham|german.kruszewski|angeliki.lazaridou|marco.baroni}@unitn.it
</email>
<sectionHeader confidence="0.997297" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999497928571429">
We introduce C-PHRASE, a distributional
semantic model that learns word repre-
sentations by optimizing context predic-
tion for phrases at all levels in a syntactic
tree, from single words to full sentences.
C-PHRASE outperforms the state-of-the-
art C-BOW model on a variety of lexical
tasks. Moreover, since C-PHRASE word
vectors are induced through a composi-
tional learning objective (modeling the
contexts of words combined into phrases),
when they are summed, they produce sen-
tence representations that rival those gen-
erated by ad-hoc compositional models.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996409877192983">
Distributional semantic models, that induce
vector-based meaning representations from pat-
terns of co-occurrence of words in corpora, have
proven very successful at modeling many lexical
relations, such as synonymy, co-hyponomy and
analogy (Mikolov et al., 2013c; Turney and Pan-
tel, 2010). The recent evaluation of Baroni et al.
(2014b) suggests that the C-BOW model intro-
duced by Mikolov et al. (2013a) is, consistently,
the best across many tasks.1
Interestingly, C-BOW vectors are estimated
with a simple compositional approach: The
weights of adjacent words are jointly optimized so
that their sum will predict the distribution of their
contexts. This is reminiscent of how the parame-
ters of some compositional distributional seman-
1We refer here not only to the results reported in
Baroni et al. (2014b), but also to the more exten-
sive evaluation that Baroni and colleagues present in
the companion website (http://clic.cimec.unitn.
it/composes/semantic-vectors.html). The ex-
periments there suggest that only the Glove vectors of Pen-
nington et al. (2014) are competitive with C-BOW, and only
when trained on a corpus several orders of magnitude larger
than the one used for C-BOW.
tic models are estimated by optimizing the pre-
diction of the contexts in which phrases occur in
corpora (Baroni and Zamparelli, 2010; Guevara,
2010; Dinu et al., 2013). However, these compo-
sitional approaches assume that word vectors have
already been constructed, and contextual evidence
is only used to induce optimal combination rules
to derive representations of phrases and sentences.
In this paper, we follow through on this observa-
tion to propose the new C-PHRASE model. Sim-
ilarly to C-BOW, C-PHRASE learns word repre-
sentations by optimizing their joint context pre-
diction. However, unlike in flat, window-based
C-BOW, C-PHRASE groups words according to
their syntactic structure, and it simultaneously op-
timizes context-predictions at different levels of
the syntactic hierarchy. For example, given train-
ing sentence “A sad dog is howling in the park”,
C-PHRASE will optimize context prediction for
dog, sad dog, a sad dog, a sad dog is howling,
etc., but not, for example, for howling in, as these
two words do not form a syntactic constituent by
themselves.
C-PHRASE word representations outperform
C-BOW on several word-level benchmarks. In ad-
dition, because they are estimated in a composi-
tional way, C-PHRASE word vectors, when com-
bined through simple addition, produce sentence
representations that are better than those obtained
when adding other kinds of vectors, and competi-
tive against ad-hoc compositional methods on var-
ious sentence meaning benchmarks.
</bodyText>
<sectionHeader confidence="0.996855" genericHeader="method">
2 The C-PHRASE model
</sectionHeader>
<bodyText confidence="0.999774">
We start with a brief overview of the models pro-
posed by Mikolov et al. (2013a), as C-PHRASE
builds on them. The Skip-gram model derives
the vector of a target word by setting its weights
to predict the words surrounding it in the corpus.
</bodyText>
<page confidence="0.970601">
971
</page>
<note confidence="0.979468666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 971–981,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.861416">
More specifically, the objective function is:
</bodyText>
<equation confidence="0.911049">
log p(wt+j|wt) (1)
</equation>
<bodyText confidence="0.99939475">
where the word sequence w1, w2, ..., wT is the
training corpus and c is the size of the window
around the target word wt, consisting of the con-
text words wt+j that must be predicted by the in-
duced vector representation for the target.
While Skip-gram learns each word represen-
tation separately, the C-BOW model takes their
combination into account. More precisely, it tries
to predict a context word from the combination of
the previous and following words, where the com-
bination method is vector addition. The objective
function is:
</bodyText>
<equation confidence="0.974599">
log p(wt|wt−c..wt−1, wt+1..wt+c) (2)
</equation>
<bodyText confidence="0.998865565217391">
While other distributional models consider se-
quences of words jointly as context when estimat-
ing the parameters for a single word (Agirre et al.,
2009; Melamud et al., 2014), C-BOW is unique
in that it estimates the weights of a sequence of
words jointly, based on their shared context. In
this respect, C-BOW extends the distributional hy-
pothesis (Harris, 1954) that words with similar
context distributions should have similar meanings
to longer sequences. However, the word combi-
nations of C-BOW are not natural linguistic con-
stituents, but arbitrary n-grams (e.g., sequences of
5 words with a gap in the middle). Moreover, the
model does not attempt to capture the hierarchical
nature of syntactic phrasing, such that big brown
dog is a meaningful phrase, but so are its children
brown dog and dog.
C-PHRASE aims at capturing the same intu-
ition that word combinations with similar con-
text distributions will have similar meaning, but
it applies it to syntactically motivated, potentially
nested phrases. More precisely, we estimate word
vectors such that they and their summed combi-
nations are able to predict the contexts of words,
phrases and sentences. The model is formalized
as follows. We start from a parsed text corpus
T, composed of constituents C[wl, · · · , wr], where
wl, · · · , wr are the words spanned by the con-
stituent, located in positions l to r in the corpus.
We minimize an objective function analogous to
equations (1) and (2), but instead of just using in-
dividual words or bags of words to predict context,
we use summed vector representations of well-
formed constituents at all levels in the syntactic
tree to predict the context of these constituents.
There are similarities with both CBOW and Skip-
gram. At the leaf nodes, C-PHRASE acts like
Skip-gram, whereas at higher node in the parse
tree, it behaves like CBOW model. Concretely, we
try to predict the words located within a window
cC from every constituent in the parse tree.2 In or-
der to do so, we learn vector representations for
words vw by maximizing the sum of the log prob-
abilities of the words in the context window of the
well-formed constituents with stochastic gradient
descent:
</bodyText>
<equation confidence="0.997026727272727">
E E(log p(wl−j |C[wl, ··· , wr])
C[wl,··· ,wr]∈T 1≤j≤cC
�+ log p(wr+j|C[wl, ··· , wr]) (3)
with p theoretically defined as:
p(wO|C[wl,··· ,wr])
�0&gt; Er�
i=l vwi
exp v wO r−l+1
� �
W ex 2I/T Eri=l vwi
�w=1 p w r−l+1
</equation>
<bodyText confidence="0.999832388888889">
where W is the size of the vocabulary, v0 and v
denote output (context) and input vectors, respec-
tively, and we take the input vectors to represent
the words. In practice, since the normalization
constant for the above probability is expensive to
compute, we follow Mikolov et al. (2013b) and
use negative sampling.
We let the context window size cC vary as a
function of the height of the constituent in the
syntactic tree. The height h(C) of a constituent
is given by the maximum number of intermedi-
ate nodes separating it from any of the words it
dominates (such that h = 0 for words, h = 1 for
two-word phrases, etc.). Then, for a constituent
of height h(C), C-PHRASE considers cC = c1 +
h(C)c2 context words to its left and right (the non-
negative integers c1 and c2 are hyperparameters of
the model; with c2 = 0, context becomes constant
</bodyText>
<footnote confidence="0.99629075">
2Although here we only use single words as context,
the latter can be extended to encompass any sensible lin-
guistic item, e.g., frequent n-grams or, as discussed below,
syntactically-mediated expressions
</footnote>
<equation confidence="0.991168272727273">
E
−c≤j≤c,j6=0
1
T
ET
t=1
1
T
ET
t=1
=
</equation>
<page confidence="0.99559">
972
</page>
<figureCaption confidence="0.996163">
Figure 1: C-PHRASE context prediction objec-
</figureCaption>
<bodyText confidence="0.998191310344828">
tive for the phrase small cat and its children. The
phrase vector is obtained by summing the word
vectors. The predicted window is wider for the
higher constituent (the phrase).
across heights). The intuition for enlarging the
window proportionally to height is that, for shorter
phrases, narrower contexts are likely to be most in-
formative (e.g., a modifying adjective for a noun),
whereas for longer phrases and sentences it might
be better to focus on broader “topical” information
spread across larger windows (paragraphs contain-
ing sentences about weather might also contain the
words rain and sun, but without any tendency for
these words to be perfectly adjacent to the target
sentences).
Figure 1 illustrates the prediction objective for
a two-word phrase and its children. Since all
constituents (except the topmost) form parts of
larger constituents, their representations will be
learned both from the objective of predicting their
own contexts, and from error propagation from the
same objective applied higher in the tree. As a side
effect, words, being lower in the syntactic tree,
will have their vectors updated more often, and
thus might have a greater impact on the learned pa-
rameters. This is another reason for varying win-
dow size with height, so that the latter effect will
be counter-balanced by higher constituents having
larger context windows to predict.
For lexical tasks, we directly use the vectors
induced by C-PHRASE as word representations.
For sentential tasks, we simply add the vectors of
the words in a sentence to obtain its representation,
exploiting the fact that C-PHRASE was trained to
predict phrase contexts from the additive combi-
nation of their elements.
Joint optimization of word and phrase vectors
The C-PHRASE hierarchical learning objective
can capture, in parallel, generalizations about the
contexts of words and phrases at different levels
of complexity. This results, as we will see, in bet-
ter word vectors, presumably because C-PHRASE
is trained to predict how the contexts of a word
change based on its phrasal collocates (cup will
have very different contexts in world cup vs. cof-
fee cup ). At the same time, because the vectors are
optimized based on their occurrence in phrases of
different syntactic complexity, they produce good
sentence representations when they are combined.
To the best of our knowledge, C-PHRASE is the
first model that is jointly optimized for lexical and
compositional tasks. C-BOW uses shallow com-
position information to learn word vectors. Con-
versely, some compositional models –e.g., Kalch-
brenner et al. (2014), Socher et al. (2013)– in-
duce word representations, that are only optimized
for a compositional task and are not tested at the
lexical level. Somewhat relatedly to what we
do, Hill et al. (2014) evaluated representations
learned in a sentence translation task on word-
level benchmarks. Some a priori justification for
treating word and sentence learning as joint prob-
lems comes from human language acquisition, as
it is obvious that children learn word and phrase
meanings in parallel and interactively, not sequen-
tially (Tomasello, 2003).
Knowledge-leanness and simplicity For train-
ing, C-PHRASE requires a large, syntactically-
parsed corpus (more precisely, it only requires the
constituent structure assigned by the parser, as it
is blind to syntactic labels). Both large unan-
notated corpora and efficient pre-trained parsers
are available for many languages, making the C-
PHRASE knowledge demands feasible for practi-
cal purposes. There is no need to parse the sen-
tences we want to build representations for at test
time, since the component word vectors are sim-
ply added. The only parameters of the model are
the word vectors; specifically, no extra parameters
are needed for composition (composition models
such as the one presented in Socher et al. (2012)
require an extra parameter matrix for each word
in the vocabulary, and even leaner models such as
the one of Guevara (2010) must estimate a param-
eter matrix for each composition rule in the gram-
mar). This makes C-PHRASE as simple as addi-
tive and multiplicative composition (Mitchell and
</bodyText>
<page confidence="0.995068">
973
</page>
<bodyText confidence="0.995550324324325">
Lapata, 2010),3 but C-PHRASE is both more ef-
fective in compositional tasks (see evaluation be-
low), and it has the further advantage that it learns
its own word vectors, thus reducing the number of
arbitrary choices to be made in modeling.
Supervision Unlike many recent composition
models (Kalchbrenner and Blunsom, 2013; Kalch-
brenner et al., 2014; Socher et al., 2012; Socher
et al., 2013, among others), the context-prediction
objective of C-PHRASE does not require anno-
tated data, and it is meant to provide general-
purpose representations that can serve in differ-
ent tasks. C-PHRASE vectors can also be used
as initialization parameters for fully supervised,
task-specific systems. Alternatively, the current
unsupervised objective could be combined with
task-specific supervised objectives to fine-tune C-
PHRASE to specific purposes.
Sensitivity to syntactic structure During train-
ing, C-PHRASE is sensitive to syntactic structure.
To cite an extreme example, boy flowers will be
joined in a context-predicting phrase in “these are
considered [boy flowers]”, but not in “he gave
[the boy] [flowers]”. A more common case is
that of determiners, that will only occur in phrases
that also contain the following word, but not nec-
essarily the preceding one. Sentence composi-
tion at test time, on the other hand, is additive,
and thus syntax-insensitive. Still, the vectors be-
ing combined will reflect syntactic generalizations
learned in training. Even if C-PHRASE produces
the same representation for red+car and car+red,
this representation combines a red vector that, dur-
ing training, has often occurred in the modifier
position of adjective-noun phrases, whereas car
will have often occurred in the corresponding head
position. So, presumably, the red+car=car+red
vector will encode the adjective-noun asymmetry
induced in learning. While the model won’t be
able to distinguish the rare cases in which car red
is genuinely used as a phrase, in realistic scenar-
ios this won’t be a problem, because only red car
will be encountered. In this respect, the successes
and failures of C-PHRASE can tell us to what ex-
tent word order information is truly distinctive in
practice, and to what extent it can instead be re-
constructed from the typical role that words play
in sentences.
3We do not report results for component-wise multiplica-
tive in our evaluation because it performed much worse than
addition in all the tasks.
Comparison with traditional syntax-sensitive
word representations Syntax has often been
exploited in distributional semantics for a richer
characterization of context. By relying on a syn-
tactic parse of the input corpus, a distributional
model can take more informative contexts such
as subject-of-eat vs. object-of-eat into account
(Baroni and Lenci, 2010; Curran and Moens,
2002; Grefenstette, 1994; Erk and Pad´o, 2008;
Levy and Goldberg, 2014a; Pad´o and Lapata,
2007; Rothenh¨ausler and Sch¨utze, 2009). In this
approach, syntactic information serves to select
and/or enrich the contexts that are used to build
representations of target units. On the other hand,
we use syntax to determine the target units that
we build representations for (in the sense that we
jointly learn representations of their constituents).
The focus is thus on unrelated aspects of model in-
duction, and we could indeed use syntax-mediated
contexts together with our phrasing strategy. Cur-
rently, given eat (red apples), we treat eat as
window-based context of red apples, but we could
also take the context to be object-of-eat.
</bodyText>
<sectionHeader confidence="0.997109" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998676">
3.1 Data sets
</subsectionHeader>
<bodyText confidence="0.99991684">
Semantic relatedness of words In this classic
lexical task, the models are required to quantify
the degree of semantic similarity or relatedness of
pairs of words in terms of cosines between the cor-
responding vectors. These scores are then com-
pared to human gold standards. Performance is as-
sessed by computing the correlation between sys-
tem and human scores (Spearman correlation in
all tasks except rg, where it is customary to re-
port Pearson). We used, first of all, the MEN
(men) data set of Bruni et al. (2014), that is split
into 1K pairs for training/development, and 1K
pairs for testing. We used the training set to tune
the hyperparameters of our model, and report per-
formance on the test set. The C-BOW model
of Baroni et al. (2014b) achieved state-of-the art
performance on MEN test. We also evaluate on
the widely used WordSim353 set introduced by
Finkelstein et al. (2002), which consists of 353
word pairs. The WordSim353 data were split by
Agirre et al. (2009) into similarity (wss) and re-
latedness (wsr) subsets, focusing on strictly taxo-
nomic (television/radio) vs. broader topical cases
(Maradona/football), respectively. State-of-the-
art performance on both sets is reported by Baroni
</bodyText>
<page confidence="0.993246">
974
</page>
<bodyText confidence="0.99993388372093">
et al. (2014b), with the C-BOW model. We fur-
ther consider the classic data set of Rubenstein and
Goodenough (1965) (rg), consisting of 65 noun
pairs. We report the state-of-the-art from Hassan
and Mihalcea (2011), which exploited Wikipedia’s
linking structure.
Concept categorization Systems are asked to
group a set of nominal concepts into broader cate-
gories (e.g. arthritis and anthrax into illness; ba-
nana and grape into fruit). As in previous work,
we treat this as an unsupervised clustering task.
We feed the similarity matrix produced by a model
for all concepts in a test set to the CLUTO toolkit
(Karypis, 2003), that clusters them into n groups,
where n is the number of categories. We use
standard CLUTO parameters from the literature,
and quantify performance by cluster purity with
respect to the gold categories. The Almuhareb-
Poesio benchmark (Almuhareb, 2006) (ap) con-
sists of 402 concepts belonging to 21 categories.
A distributional model based on carefully chosen
syntactic relations achieved top ap performance
(Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI
2008 data set (Baroni et al., 2008) (esslli) consists
of 6 categories and 42 concepts. State of the art
was achieved by Katrenko and Adriaans (2008) by
using full-Web queries and manually crafted pat-
terns.
Semantic analogy The last lexical task we pick
is analogy (an), introduced in Mikolov et al.
(2013c). We focus on their semantic challenge,
containing about 9K questions. In each question,
the system is given a pair exemplifying a relation
(man/king) and a test word (woman); it is then
asked to find the word (queen) that instantiates the
same relation with the test word as that of the ex-
ample pair. Mikolov et al. (2013c) subtract the
vector of the first word in a pair from the sec-
ond, add the vector of the test word and look for
the nearest neighbor of the resulting vector (e.g.,
find the word whose vector is closest to king -
man + woman). We follow the method introduced
by Levy and Goldberg (2014b), which returns the
</bodyText>
<equation confidence="0.7505895">
word x maximizing cos(king,x)×cos(woman,x) .This
cos(man,x)
</equation>
<bodyText confidence="0.998832607142857">
method yields better results for all models. Per-
formance is measured by accuracy in retrieving
the correct answer (in our search space of 180K
words). The current state of the art on the seman-
tic part and on the whole data set was reached by
Pennington et al. (2014), who trained their word
representations on a huge corpus consisting of 42B
words.
Sentential semantic relatedness Similarly to
word relatedness, composed sentence representa-
tions can be evaluated against benchmarks where
humans provided relatedness/similarity scores for
sentence pairs (sentences with high scores, such as
“A person in a black jacket is doing tricks on a mo-
torbike”/“A man in a black jacket is doing tricks
on a motorbike” from the SICK data-set, tend to
be near-paraphrases). Following previous work on
these data sets, Pearson correlation is our figure
of merit, and we report it between human scores
and sentence vector cosine similarities computed
by the models. SICK (Marelli et al., 2014) (sick-r)
was created specifically for the purpose of evalu-
ating compositional models, focusing on linguistic
phenomena such as lexical variation and word or-
der. Here we report performance of the systems
on the test part of the data set, which contains 5K
sentence pairs. The top performance (from the
SICK SemEval shared task) was reached by Zhao
et al. (2014) using a heterogeneous set of features
that include WordNet and extra training corpora.
Agirre et al. (2012) and Agirre et al. (2013) cre-
ated two collections of sentential similarities con-
sisting of subsets coming from different sources.
From these, we pick the Microsoft Research video
description dataset (msrvid), where near para-
phrases are descriptions of the same short video,
and the OnWN 2012 (onwn1) and OnWN 2013
(onwn2) data sets (each of these sets contains 750
pairs). The latter are quite different from other
sentence relatedness benchmarks, since they com-
pare definitions for the same or different words
taken from WordNet and OntoNotes: these glosses
often are syntactic fragments (“cause something
to pass or lead somewhere”), rather than full sen-
tences. We report top performance on these tasks
from the respective shared challenges, as sum-
marized by Agirre et al. (2012) and Agirre et al.
(2013). Again, the top systems use feature-rich,
supervised methods relying on distributional sim-
ilarity as well as other sources, such as WordNet
and named entity recognizers.
Sentential entailment Detecting the presence
of entailment between sentences or longer pas-
sages is one of the most useful features that the
computational analysis of text could provide (Da-
gan et al., 2009). We test our model on the SICK
</bodyText>
<page confidence="0.994903">
975
</page>
<bodyText confidence="0.999652323529412">
entailment task (sick-e) (Marelli et al., 2014).
All SICK sentence pairs are labeled as ENTAIL-
ING (“Two teams are competing in a football
match”/”Two groups of people are playing foot-
ball”), CONTRADICTING (“The brown horse is
near a red barrel at the rodeo”/“The brown horse
is far from a red barrel at the rodeo”) or NEU-
TRAL (“A man in a black jacket is doing tricks on
a motorbike”/”A person is riding the bicycle on
one wheel”). For each model, we train a simple
SVM classifier based on 2 features: cosine simi-
larity between the two sentence vectors, as given
by the models, and whether the sentence pair con-
tains a negation word (the latter has been shown
to be a very informative feature for SICK entail-
ment). The current state-of-the-art is reached by
Lai and Hockenmaier (2014), using a much richer
set of features, that include WordNet, the denota-
tion graph of Young et al. (2014) and extra training
data from other resources.
Sentiment analysis Finally, as sentiment analy-
sis has emerged as a popular area of application
for compositional models, we test our methods on
the Stanford Sentiment Treebank (Socher et al.,
2013) (sst), consisting of 11,855 sentences from
movie reviews, using the coarse annotation into
2 sentiment degrees (negative/positive). We fol-
low the official split into train (8,544), develop-
ment (1,101) and test (2,210) parts. We train an
SVM classifier on the training set, using the sen-
tence vectors composed by a model as features,
and report accuracy on the test set. State of the
art is obtained by Le and Mikolov (2014) with the
Paragraph Vector approach we describe below.
</bodyText>
<subsectionHeader confidence="0.999131">
3.2 Model implementation
</subsectionHeader>
<bodyText confidence="0.999789333333334">
The source corpus we use to build the lex-
ical vectors is created by concatenating three
sources: ukWaC,4 a mid-2009 dump of the En-
glish Wikipedia5 and the British National Corpus6
(about 2.8B words in total). We build vectors for
the 180K words occurring at least 100 times in
the corpus. Since our training procedure requires
parsed trees, we parse the corpus using the Stan-
ford parser (Klein and Manning, 2003).
C-PHRASE has two hyperparameters (see Sec-
tion 2 above), namely basic window size (c1) and
height-dependent window enlargement factor (c2).
</bodyText>
<footnote confidence="0.999913">
4http://wacky.sslmit.unibo.it
5http://en.wikipedia.org
6http://www.natcorp.ox.ac.uk
</footnote>
<bodyText confidence="0.999884276595745">
Moreover, following Mikolov et al. (2013b), dur-
ing training we sub-sample less informative, very
frequent words: this option is controlled by a pa-
rameter t, resulting in aggressive subsampling of
words with relative frequency above it. We tune
on MEN-train, obtaining c1 = 5, c2 = 2 and
t = 10−5. As already mentioned, sentence vec-
tors are built by summing the vectors of the words
in them.
In lexical tasks, we compare our model to the
best C-BOW model from Baroni et al. (2014b),7
and to a Skip-gram model built using the same hy-
perparameters as C-PHRASE (that also led to the
best MEN-train results for Skip-gram).
In sentential tasks, we compare our model
against adding the best C-BOW vectors pre-
trained by Baroni and colleagues,8 and adding our
Skip-gram vectors. We compare the additive ap-
proaches to two sophisticated composition mod-
els. The first is the Practical Lexical Function
(PLF) model of Paperno et al. (2014). This is a
linguistically motivated model in the tradition of
the “functional composition” approaches of Co-
ecke et al. (2010) and Baroni et al. (2014a), and
the only model in this line of research that has
been shown to empirically scale up to real-life sen-
tence challenges. In short, in the PLF model all
words are represented by vectors. Words acting
as argument-taking functions (such as verbs or ad-
jectives) are also associated to one matrix for each
argument they take (e.g., each transitive verb has
a subject and an object matrix). Vector represen-
tations of arguments are recursively multiplied by
function matrices, following the syntactic tree up
to the top node. The final sentence representa-
tion is obtained by summing all the resulting vec-
tors. The PLF approach requires syntactic parsing
both in training and in testing and, more cumber-
somely, to train a separate matrix for each argu-
ment slot of each function word (the training ob-
jective is again a context-predicting one). Here,
we report PLF results on msrvid and onwn2 from
Paperno et al. (2014), noting that they also used
two simple but precious cues (word overlap and
sentence length) we do not adopt here. We used
their pre-trained vectors and matrices also for the
SICK challenges, while the number of new ma-
</bodyText>
<footnote confidence="0.9877608">
7For fairness, we report their results when all tasks were
evaluated with the same set of parameters, tuned on rg: this
is row 8 of their Table 2.
8http://clic.cimec.unitn.it/composes/
semantic-vectors.html
</footnote>
<page confidence="0.998305">
976
</page>
<bodyText confidence="0.999855851851852">
trices to estimate made it too time-consuming to
implement this model in the onwn1 and sst tasks.
Finally, we test the Paragraph Vector (PV) ap-
proach recently proposed by Le and Mikolov
(2014). Under PV, sentence representations are
learned by predicting the words that occur in them.
This unsupervised method has been shown by
the authors to outperform much more sophisti-
cated, supervised neural-network-based composi-
tion models on the sst task. We use our own imple-
mentation for this approach. Unlike in the original
experiments, we found the PV-DBOW variant of
PV to consistently outperform PV-DM, and so we
report results obtained with the former.
Note that PV-DBOW aims mainly at providing
representations for sentences, not words. When
we do not need to induce vectors for sentences
in the training corpus, i.e., only train to learn
single words’ representations and the softmax
weights, PV-DBOW essentially reduces to Skip-
gram. Therefore, we produce the PV-DBOW vec-
tors for the sentences in the evaluation data sets
using the softmax weights learned by Skip-gram.
However, it is not clear that, if we were to train PV-
DBOW jointly for words and sentences, we would
get word vectors as good as those that Skip-gram
induces.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9998205">
The results on the lexical tasks reported in Table 1
prove that C-PHRASE is providing excellent word
representations, (nearly) as good or better than the
C-BOW vectors of Baroni and colleagues in all
cases, except for ap. Whenever C-PHRASE is not
close to the state of the art results, the latter relied
on richer knowledge sources and/or much larger
corpora (ap, esslli, an).
Turning to the sentential tasks (Table 2), we first
remark that using high-quality word vectors (such
as C-BOW) and summing them leads to good re-
sults in all tasks, competitive with those obtained
with more sophisticated composition models. This
confirms the observation made by Blacoe and La-
pata (2012) that simple-minded composition mod-
els are not necessarily worse than advanced ap-
proaches. Still, C-PHRASE is consistently better
than C-BOW in all tasks, except sst, where the two
models reach the same performance level.
C-PHRASE is outperforming PV on all tasks
except sick-e, where the two models have the same
performance, and onwn2, where PV is slightly
better. C-PHRASE is outperforming PLF by a
large margin on the SICK sets, whereas the two
models are equal on msrvid, and PLF better on
onwn2. Recall, however, that on the latter two
benchmarks PLF used extra word overlap and sen-
tence length features, so the comparison is not en-
tirely fair.
The fact that state-of-the-art performance is
well above our models is not surprising, since the
SOA systems are invariably based on a wealth
of knowledge sources, and highly optimized for
a task. To put some of our results in a broader
perspective, C-PHRASE’s sick-r performance is
1% better than the median result of systems that
participated in the SICK SemEval challenge, and
comparable to that of Beltagy et al. (2014), who
entered the competition with a system combining
distributional semantics with a supervised proba-
bilistic soft logic system. For sick-e (the entail-
ment task), C-PHRASE’s performance is less than
one point below the median of the SemEval sys-
tems, and slightly above that of the Stanford sub-
mission, that used a recursive neural network with
a tensor layer.
Finally, the performance of all our models, in-
cluding PV, on sst is remarkably lower than the
state-of-the-art performance of PV as reported by
Le and Mikolov (2014). We believe that the
crucial difference is that these authors estimated
PV vectors specifically on the sentiment treebank
training data, thus building ad-hoc vectors encod-
ing the semantics of movie reviews. We leave it
to further research to ascertain whether we could
better fine-tune our models to sst by including the
sentiment treebank training phrases in our source
corpus.
Comparing vector lengths of C-BOW and C-
PHRASE We gather some insight into how the
C-PHRASE objective might adjust word represen-
tations for composition with respect to C-BOW by
looking at how the length of word vectors changes
across the two models.9 While this is a very coarse
measure, if a word vector is much longer/shorter
(relative to the length of other word vectors of the
same model) for C-PHRASE vs. C-BOW, it means
that, when sentences are composed by addition,
the effect of the word on the resulting sentence
representation will be stronger/weaker.
</bodyText>
<footnote confidence="0.753451333333333">
9We performed the same analysis for C-PHRASE and
Skip-gram, finding similar general trends to the ones we re-
port for C-PHRASE and C-BOW.
</footnote>
<page confidence="0.985707">
977
</page>
<table confidence="0.9972948">
men wss wsr rg ap esslli an
Skip-gram 78 77 66 80 65 82 63
C-BOW 80 78 68 83 71 77 68
C-PHRASE 79 79 70 83 65 84 69
SOA 80 80 70 86 79 91 82
</table>
<tableCaption confidence="0.8363635">
Table 1: Lexical task performance. See Section 3.1 for figures of merit (all in percentage form) and
state-of-the-art references. C-BOW results (tuned on rg) are taken from Baroni et al. 2014b.
</tableCaption>
<table confidence="0.999943285714285">
sick-r sick-e msrvid onwn1 onwn2 sst
Skip-gram 70 72 74 66 62 78
C-BOW 70 74 74 69 63 79
C-PHRASE 72 75 79 70 65 79
PLF 57 72 79 NA 67 NA
PV 67 75 77 66 66 77
SOA 83 85 88 71 75 88
</table>
<tableCaption confidence="0.8907575">
Table 2: Sentential task performance. See Section 3.1 for figures of merit (all in percentage form) and
state-of-the-art references. The PLF results on msrvid and onwn2 are taken from Paperno et al. 2014.
</tableCaption>
<bodyText confidence="0.99892126984127">
The relative-length-difference test returns the
following words as the ones that are most severely
de-emphasized by C-PHRASE compared to C-
BOW: be, that, an, not, they, he, who, when,
well, have. Clearly, C-PHRASE is weighting
down grammatical terms that tend to be context-
agnostic, and will be accompanied, in phrases, by
more context-informative content words. Indeed,
the list of terms that are instead emphasized by
C-PHRASE include such content-rich, monose-
mous words as gnawing, smackdown, demograph-
ics. This is confirmed by a POS-level analysis
that indicates that the categories that are, on av-
erage, most de-emphasized by C-PHRASE are:
determiners, modals, pronouns, prepositions and
(more surprisingly) proper nouns. The ones that
are, in relative terms, more emphasized are: -ing
verb forms, plural and singular nouns, adjectives
and their superlatives. While this reliance on con-
tent words to the detriment of grammatical terms
is not always good for sentential tasks (“not al-
ways good” means something very different from
“always good”!), the convincing comparative per-
formance of C-PHRASE in such tasks suggests
that the semantic effect of grammatical terms is
in any case beyond the scope of current corpus-
based models, and often not crucial to attain com-
petitive results on typical benchmarks (think, e.g.,
of how little modals, one of the categories that C-
PHRASE downplays the most, will matter when
detecting paraphrases that are based on picture de-
scriptions).
We also applied the length difference test to
words in specific categories, finding similar pat-
terns. For example, looking at -ly adverbs only,
those that are de-emphasized the most by C-
PHRASE are recently, eventually, originally, no-
tably and currently – all adverbs denoting tempo-
ral factors or speaker attitude. On the other hand,
the ones that C-PHRASE lengthens the most, rel-
ative to C-BOW, are clinically, divinely, ecolog-
ically, noisily and theatrically: all adverbs with
more specific, content-word-like meanings, that
are better captured by distributional methods, and
are likely to have a bigger impact on tasks such as
paraphrasing or entailment.
Effects of joint optimization at word and
phrase levels As we have argued before, C-
PHRASE is able to obtain good word representa-
tions presumably because it learns to predict how
the context of a word changes in the presence of
different collocates. To gain further insight into
this claim, we looked at the nearest neighbours
of some example terms, like neural, network and
neural network (the latter, composed by addition)
both in C-PHRASE and C-BOW. The results for
this particular example can be appreciated in Ta-
ble 3.
Interestingly, while for C-BOW we observe
some confusion between the meaning of the indi-
vidual words and the phrase, C-PHRASE seems
to provide more orthogonal representations for
the lexical items. For example, neural in C-
</bodyText>
<page confidence="0.995294">
978
</page>
<table confidence="0.986825363636364">
C-BOW C-PHRASE
neural network neural network neural network neural network
neuronal networks network neuronal networks network
neurons superjanet4 neural cortical internetwork neural
hopfield backhaul networks connectionist wans perceptron
cortical fiber-optic hopfield neurophysiological network. networks
connectionist point-to-multipoint packet-switched sensorimotor multicasting hebbian
feed-forward nsfnet small-world sensorimotor nsfnet neurons
feedforward multi-service local-area neocortex networking neocortex
neuron circuit-switched superjanet4 electrophysiological tymnet connectionist
backpropagation wide-area neuronal neurobiological x.25 neuronal
</table>
<tableCaption confidence="0.999882">
Table 3: Nearest neighbours of neural, network and neural network both for C-BOW and C-PHRASE
</tableCaption>
<bodyText confidence="0.999793636363636">
BOW contains neighbours that fit well with neu-
ral network, like hopfield, connectionist and feed-
forward. Conversely, neural network has neigh-
bours that correspond to network like local-area
and packet-switched. In contrast, C-PHRASE
neighbours for neural are mostly related to the
brain sense of the word, e.g., cortical, neurophys-
iological, etc. (with the only exception of connec-
tionist). The first neighbour of neural network, ex-
cluding its own component words, quite sensibly,
is perceptron.
</bodyText>
<sectionHeader confidence="0.995738" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999980156862745">
We introduced C-PHRASE, a distributional se-
mantic model that is trained on the task of pre-
dicting the contexts surrounding phrases at all lev-
els of a hierarchical sentence parse, from single
words to full sentences. Consequently, word vec-
tors are induced by taking into account not only
their contexts, but also how co-occurrence with
other words within a syntactic constituent is af-
fecting these contexts.
C-PHRASE vectors outperform state-of-the-art
C-BOW vectors in a wide range of lexical tasks.
Moreover, because of the way they are induced,
when C-PHRASE vectors are summed, they pro-
duce sentence representations that are as good or
better than those obtained with sophisticated com-
position methods.
C-PHRASE is a very parsimonious approach:
The only major resource required, compared
to a completely knowledge-free, unsupervised
method, is an automated parse of the training cor-
pus (but no syntactic labels are required, nor pars-
ing at test time). C-PHRASE has only 3 hyperpa-
rameters and no composition-specific parameter to
tune and store.
Having established a strong empirical baseline
with this parsimonious approach, in future re-
search we want to investigate the impact of possi-
ble extensions on both lexical and sentential tasks.
When combining the vectors, either for induction
or composition, we will try replacing plain addi-
tion with other operations, starting with something
as simple as learning scalar weights for different
words in a phrase (Mitchell and Lapata, 2010).
We also intend to explore more systematic ways
to incorporate supervised signals into learning, to
fine-tune C-PHRASE vectors to specific tasks.
On the testing side, we are fascinated by the
good performance of additive models, that (at test
time, at least) do not take word order nor syntactic
structure into account. We plan to perform a sys-
tematic analysis of both existing benchmarks and
natural corpus data, both to assess the actual im-
pact that such factors have on the aspects of mean-
ing we are interested in (take two sentences in an
entailment relation: how often does shuffling the
words in them make it impossible to detect entail-
ment?), and to construct new benchmarks that are
more challenging for additive methods.
The C-PHRASE vectors described in this paper
are made publicly available at: http://clic.
cimec.unitn.it/composes/.
</bodyText>
<sectionHeader confidence="0.99814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99730325">
We thank Gemma Boleda and the anonymous re-
viewers for useful comments. We acknowledge
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.973304428571429">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of HLT-NAACL, pages 19–27, Boulder, CO.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: a
</reference>
<page confidence="0.995185">
979
</page>
<reference confidence="0.997576113207547">
pilot on semantic textual similarity. In Proceedings
of *SEM, pages 385–393, Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings
of *SEM, pages 32–43, Atlanta, GA.
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-
itors. 2008. Bridging the Gap between Semantic
Theory and Computational Simulations: Proceed-
ings of the ESSLLI Workshop on Distributional Lex-
ical Semantic. FOLLI, Hamburg.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014a. Frege in space: A program for
compositional distributional semantics. Linguistic
Issues in Language Technology, 9(6):5–110.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014b. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
ofACL, pages 238–247, Baltimore, MD.
Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin
Erk, and Raymond Mooney. 2014. UTexas: Nat-
ural language semantics using distributional seman-
tics and probabilistic logic. In Proceedings of Se-
mEval, pages 796–801, Dublin, Ireland, August.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546–556, Jeju Island, Korea.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345–384.
James Curran and Marc Moens. 2002. Improvements
in automatic thesaurus extraction. In Proceedings of
the ACL Workshop on Unsupervised Lexical Acqui-
sition, pages 59–66, Philadelphia, PA.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: ratio-
nale, evaluation and approaches. Natural Language
Engineering, 15:459–476.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50–
58, Sofia, Bulgaria.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897–906, Honolulu,
HI.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer, Boston, MA.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the EMNLP GEMS
Workshop, pages 33–37, Uppsala, Sweden.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):1456–1162.
Samer Hassan and Rada Mihalcea. 2011. Semantic
relatedness using salient semantic analysis. In Pro-
ceedings of AAAI, pages 884–889, San Francisco,
CA.
Felix Hill, KyungHyun Cho, S´ebastien Jean, Col-
ine Devin, and Yoshua Bengio. 2014. Not
all neural embeddings are born equal. In
Proceedings of the NIPS Learning Semantics
Workshop, Montreal, Canada. Published on-
line: https://sites.google.com/site/
learningsemantics2014/.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse com-
positionality. In Proceedings of ACL Workshop on
Continuous Vector Space Models and their Compo-
sitionality, pages 119–126, Sofia, Bulgaria.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL, pages
655–665, Baltimore, MD.
George Karypis. 2003. CLUTO: A clustering toolkit.
Technical Report 02-017, University of Minnesota
Department of Computer Science.
Sophia Katrenko and Pieter Adriaans. 2008. Qualia
structures and their impact on the concrete noun
categorization task. In Proceedings of the ESS-
LLI Workshop on Distributional Lexical Semantics,
pages 17–24, Hamburg, Germany.
</reference>
<page confidence="0.973344">
980
</page>
<reference confidence="0.999866797979798">
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings ofACL, pages
423–430, Sapporo, Japan.
Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
329–334, Dublin, Ireland, August. Association for
Computational Linguistics and Dublin City Univer-
sity.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.
Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of ACL
(Volume 2: Short Papers), pages 302–308, Balti-
more, Maryland.
Omer Levy and Yoav Goldberg. 2014b. Linguistic reg-
ularities in sparse and explicit word representations.
In Proceedings of CoNLL, pages 171–180, Ann Ar-
bor, MI.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. SemEval-2014 Task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proceedings of SemEval, pages
1–8, Dublin, Ireland.
Oren Melamud, Ido Dagan, Jacob Goldberger, Idan
Szpektor, and Deniz Yuret. 2014. Probabilistic
modeling of joint-context in distributional similar-
ity. In Proceedings of CoNLL, pages 181–190, Ann
Arbor, MI.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. http://arxiv.org/
abs/1301.3781/.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119, Lake
Tahoe, NV.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746–751, Atlanta, Georgia.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
Proceedings ofACL, pages 90–99, Baltimore, MD.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP, pages
1532–1543, Doha, Qatar.
Klaus Rothenh¨ausler and Hinrich Sch¨utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of the EACL GEMS
Workshop, pages 17–24, Athens, Greece.
Herbert Rubenstein and John Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Ko-
rea.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of EMNLP, pages 1631–1642,
Seattle, WA.
Michael Tomasello. 2003. Constructing a Language:
A Usage-Based Theory of Language Acquisition.
Harvard University Press, Cambridge, MA.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics, 2:67–78.
Jiang Zhao, Tiantian Zhu, and Man Lan. 2014. Ecnu:
One stone two birds: Ensemble of heterogenous
measures for semantic relatedness and textual entail-
ment. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
271–277, Dublin, Ireland, August. Association for
Computational Linguistics and Dublin City Univer-
sity.
</reference>
<page confidence="0.998164">
981
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497438">
<title confidence="0.895286">Jointly optimizing word representations for lexical and sentential with the C-PHRASE model</title>
<affiliation confidence="0.883932666666667">Nghia The Pham Germ´an Kruszewski Angeliki Lazaridou Marco Center for Mind/Brain University of Trento</affiliation>
<abstract confidence="0.9958982">We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences. C-PHRASE outperforms the state-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those genby models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<journal>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor</journal>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>19--27</pages>
<location>Boulder, CO.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of HLT-NAACL, pages 19–27, Boulder, CO. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: a pilot on semantic textual similarity. In Proceedings of *SEM, pages 385–393, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>shared task: Semantic Textual Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>32--43</pages>
<publisher>SEM</publisher>
<location>Atlanta, GA.</location>
<contexts>
<context position="20664" citStr="Agirre et al. (2013)" startWordPosition="3320" endWordPosition="3323"> we report it between human scores and sentence vector cosine similarities computed by the models. SICK (Marelli et al., 2014) (sick-r) was created specifically for the purpose of evaluating compositional models, focusing on linguistic phenomena such as lexical variation and word order. Here we report performance of the systems on the test part of the data set, which contains 5K sentence pairs. The top performance (from the SICK SemEval shared task) was reached by Zhao et al. (2014) using a heterogeneous set of features that include WordNet and extra training corpora. Agirre et al. (2012) and Agirre et al. (2013) created two collections of sentential similarities consisting of subsets coming from different sources. From these, we pick the Microsoft Research video description dataset (msrvid), where near paraphrases are descriptions of the same short video, and the OnWN 2012 (onwn1) and OnWN 2013 (onwn2) data sets (each of these sets contains 750 pairs). The latter are quite different from other sentence relatedness benchmarks, since they compare definitions for the same or different words taken from WordNet and OntoNotes: these glosses often are syntactic fragments (“cause something to pass or lead so</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic Textual Similarity. In Proceedings of *SEM, pages 32–43, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdulrahman Almuhareb</author>
</authors>
<title>Attributes in Lexical Acquisition. Phd thesis,</title>
<date>2006</date>
<institution>University of Essex.</institution>
<contexts>
<context position="17993" citStr="Almuhareb, 2006" startWordPosition="2879" endWordPosition="2880">. Concept categorization Systems are asked to group a set of nominal concepts into broader categories (e.g. arthritis and anthrax into illness; banana and grape into fruit). As in previous work, we treat this as an unsupervised clustering task. We feed the similarity matrix produced by a model for all concepts in a test set to the CLUTO toolkit (Karypis, 2003), that clusters them into n groups, where n is the number of categories. We use standard CLUTO parameters from the literature, and quantify performance by cluster purity with respect to the gold categories. The AlmuharebPoesio benchmark (Almuhareb, 2006) (ap) consists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance (Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI 2008 data set (Baroni et al., 2008) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by Katrenko and Adriaans (2008) by using full-Web queries and manually crafted patterns. Semantic analogy The last lexical task we pick is analogy (an), introduced in Mikolov et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, t</context>
</contexts>
<marker>Almuhareb, 2006</marker>
<rawString>Abdulrahman Almuhareb. 2006. Attributes in Lexical Acquisition. Phd thesis, University of Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional Memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="15129" citStr="Baroni and Lenci, 2010" startWordPosition="2413" endWordPosition="2416">inctive in practice, and to what extent it can instead be reconstructed from the typical role that words play in sentences. 3We do not report results for component-wise multiplicative in our evaluation because it performed much worse than addition in all the tasks. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional Memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2187" citStr="Baroni and Zamparelli, 2010" startWordPosition="316" endWordPosition="319">rs of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unitn. it/composes/semantic-vectors.html). The experiments there suggest that only the Glove vectors of Pennington et al. (2014) are competitive with C-BOW, and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW. tic models are estimated by optimizing the prediction of the contexts in which phrases occur in corpora (Baroni and Zamparelli, 2010; Guevara, 2010; Dinu et al., 2013). However, these compositional approaches assume that word vectors have already been constructed, and contextual evidence is only used to induce optimal combination rules to derive representations of phrases and sentences. In this paper, we follow through on this observation to propose the new C-PHRASE model. Similarly to C-BOW, C-PHRASE learns word representations by optimizing their joint context prediction. However, unlike in flat, window-based C-BOW, C-PHRASE groups words according to their syntactic structure, and it simultaneously optimizes context-pred</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Stefan Evert</author>
<author>Alessandro Lenci</author>
<author>editors</author>
</authors>
<date>2008</date>
<booktitle>Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI,</booktitle>
<location>Hamburg.</location>
<contexts>
<context position="18232" citStr="Baroni et al., 2008" startWordPosition="2913" endWordPosition="2916">ing task. We feed the similarity matrix produced by a model for all concepts in a test set to the CLUTO toolkit (Karypis, 2003), that clusters them into n groups, where n is the number of categories. We use standard CLUTO parameters from the literature, and quantify performance by cluster purity with respect to the gold categories. The AlmuharebPoesio benchmark (Almuhareb, 2006) (ap) consists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance (Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI 2008 data set (Baroni et al., 2008) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by Katrenko and Adriaans (2008) by using full-Web queries and manually crafted patterns. Semantic analogy The last lexical task we pick is analogy (an), introduced in Mikolov et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the example pair. Mikolov et al. (2013c) subt</context>
</contexts>
<marker>Baroni, Evert, Lenci, editors, 2008</marker>
<rawString>Marco Baroni, Stefan Evert, and Alessandro Lenci, editors. 2008. Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI, Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in space: A program for compositional distributional semantics.</title>
<date>2014</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>9</volume>
<issue>6</issue>
<contexts>
<context position="1204" citStr="Baroni et al. (2014" startWordPosition="162" endWordPosition="165">s. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models. 1 Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unit</context>
<context position="16652" citStr="Baroni et al. (2014" startWordPosition="2667" endWordPosition="2670">larity or relatedness of pairs of words in terms of cosines between the corresponding vectors. These scores are then compared to human gold standards. Performance is assessed by computing the correlation between system and human scores (Spearman correlation in all tasks except rg, where it is customary to report Pearson). We used, first of all, the MEN (men) data set of Bruni et al. (2014), that is split into 1K pairs for training/development, and 1K pairs for testing. We used the training set to tune the hyperparameters of our model, and report performance on the test set. The C-BOW model of Baroni et al. (2014b) achieved state-of-the art performance on MEN test. We also evaluate on the widely used WordSim353 set introduced by Finkelstein et al. (2002), which consists of 353 word pairs. The WordSim353 data were split by Agirre et al. (2009) into similarity (wss) and relatedness (wsr) subsets, focusing on strictly taxonomic (television/radio) vs. broader topical cases (Maradona/football), respectively. State-of-theart performance on both sets is reported by Baroni 974 et al. (2014b), with the C-BOW model. We further consider the classic data set of Rubenstein and Goodenough (1965) (rg), consisting of</context>
<context position="24608" citStr="Baroni et al. (2014" startWordPosition="3966" endWordPosition="3969">c1) and height-dependent window enlargement factor (c2). 4http://wacky.sslmit.unibo.it 5http://en.wikipedia.org 6http://www.natcorp.ox.ac.uk Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5. As already mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, we compare our model against adding the best C-BOW vectors pretrained by Baroni and colleagues,8 and adding our Skip-gram vectors. We compare the additive approaches to two sophisticated composition models. The first is the Practical Lexical Function (PLF) model of Paperno et al. (2014). This is a linguistically motivated model in the tradition of the “functional composition” approaches of Coecke et al. (2010) and Baroni et al. (2014a)</context>
<context position="31482" citStr="Baroni et al. 2014" startWordPosition="5114" endWordPosition="5117">it means that, when sentences are composed by addition, the effect of the word on the resulting sentence representation will be stronger/weaker. 9We performed the same analysis for C-PHRASE and Skip-gram, finding similar general trends to the ones we report for C-PHRASE and C-BOW. 977 men wss wsr rg ap esslli an Skip-gram 78 77 66 80 65 82 63 C-BOW 80 78 68 83 71 77 68 C-PHRASE 79 79 70 83 65 84 69 SOA 80 80 70 86 79 91 82 Table 1: Lexical task performance. See Section 3.1 for figures of merit (all in percentage form) and state-of-the-art references. C-BOW results (tuned on rg) are taken from Baroni et al. 2014b. sick-r sick-e msrvid onwn1 onwn2 sst Skip-gram 70 72 74 66 62 78 C-BOW 70 74 74 69 63 79 C-PHRASE 72 75 79 70 65 79 PLF 57 72 79 NA 67 NA PV 67 75 77 66 66 77 SOA 83 85 88 71 75 88 Table 2: Sentential task performance. See Section 3.1 for figures of merit (all in percentage form) and state-of-the-art references. The PLF results on msrvid and onwn2 are taken from Paperno et al. 2014. The relative-length-difference test returns the following words as the ones that are most severely de-emphasized by C-PHRASE compared to CBOW: be, that, an, not, they, he, who, when, well, have. Clearly, C-PHRAS</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Baroni, Raffaella Bernardi, and Roberto Zamparelli. 2014a. Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology, 9(6):5–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>238--247</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="1204" citStr="Baroni et al. (2014" startWordPosition="162" endWordPosition="165">s. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models. 1 Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unit</context>
<context position="16652" citStr="Baroni et al. (2014" startWordPosition="2667" endWordPosition="2670">larity or relatedness of pairs of words in terms of cosines between the corresponding vectors. These scores are then compared to human gold standards. Performance is assessed by computing the correlation between system and human scores (Spearman correlation in all tasks except rg, where it is customary to report Pearson). We used, first of all, the MEN (men) data set of Bruni et al. (2014), that is split into 1K pairs for training/development, and 1K pairs for testing. We used the training set to tune the hyperparameters of our model, and report performance on the test set. The C-BOW model of Baroni et al. (2014b) achieved state-of-the art performance on MEN test. We also evaluate on the widely used WordSim353 set introduced by Finkelstein et al. (2002), which consists of 353 word pairs. The WordSim353 data were split by Agirre et al. (2009) into similarity (wss) and relatedness (wsr) subsets, focusing on strictly taxonomic (television/radio) vs. broader topical cases (Maradona/football), respectively. State-of-theart performance on both sets is reported by Baroni 974 et al. (2014b), with the C-BOW model. We further consider the classic data set of Rubenstein and Goodenough (1965) (rg), consisting of</context>
<context position="24608" citStr="Baroni et al. (2014" startWordPosition="3966" endWordPosition="3969">c1) and height-dependent window enlargement factor (c2). 4http://wacky.sslmit.unibo.it 5http://en.wikipedia.org 6http://www.natcorp.ox.ac.uk Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5. As already mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, we compare our model against adding the best C-BOW vectors pretrained by Baroni and colleagues,8 and adding our Skip-gram vectors. We compare the additive approaches to two sophisticated composition models. The first is the Practical Lexical Function (PLF) model of Paperno et al. (2014). This is a linguistically motivated model in the tradition of the “functional composition” approaches of Coecke et al. (2010) and Baroni et al. (2014a)</context>
<context position="31482" citStr="Baroni et al. 2014" startWordPosition="5114" endWordPosition="5117">it means that, when sentences are composed by addition, the effect of the word on the resulting sentence representation will be stronger/weaker. 9We performed the same analysis for C-PHRASE and Skip-gram, finding similar general trends to the ones we report for C-PHRASE and C-BOW. 977 men wss wsr rg ap esslli an Skip-gram 78 77 66 80 65 82 63 C-BOW 80 78 68 83 71 77 68 C-PHRASE 79 79 70 83 65 84 69 SOA 80 80 70 86 79 91 82 Table 1: Lexical task performance. See Section 3.1 for figures of merit (all in percentage form) and state-of-the-art references. C-BOW results (tuned on rg) are taken from Baroni et al. 2014b. sick-r sick-e msrvid onwn1 onwn2 sst Skip-gram 70 72 74 66 62 78 C-BOW 70 74 74 69 63 79 C-PHRASE 72 75 79 70 65 79 PLF 57 72 79 NA 67 NA PV 67 75 77 66 66 77 SOA 83 85 88 71 75 88 Table 2: Sentential task performance. See Section 3.1 for figures of merit (all in percentage form) and state-of-the-art references. The PLF results on msrvid and onwn2 are taken from Paperno et al. 2014. The relative-length-difference test returns the following words as the ones that are most severely de-emphasized by C-PHRASE compared to CBOW: be, that, an, not, they, he, who, when, well, have. Clearly, C-PHRAS</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014b. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings ofACL, pages 238–247, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Stephen Roller</author>
<author>Gemma Boleda</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>UTexas: Natural language semantics using distributional semantics and probabilistic logic.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>796--801</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="29542" citStr="Beltagy et al. (2014)" startWordPosition="4781" endWordPosition="4784">ls are equal on msrvid, and PLF better on onwn2. Recall, however, that on the latter two benchmarks PLF used extra word overlap and sentence length features, so the comparison is not entirely fair. The fact that state-of-the-art performance is well above our models is not surprising, since the SOA systems are invariably based on a wealth of knowledge sources, and highly optimized for a task. To put some of our results in a broader perspective, C-PHRASE’s sick-r performance is 1% better than the median result of systems that participated in the SICK SemEval challenge, and comparable to that of Beltagy et al. (2014), who entered the competition with a system combining distributional semantics with a supervised probabilistic soft logic system. For sick-e (the entailment task), C-PHRASE’s performance is less than one point below the median of the SemEval systems, and slightly above that of the Stanford submission, that used a recursive neural network with a tensor layer. Finally, the performance of all our models, including PV, on sst is remarkably lower than the state-of-the-art performance of PV as reported by Le and Mikolov (2014). We believe that the crucial difference is that these authors estimated P</context>
</contexts>
<marker>Beltagy, Roller, Boleda, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin Erk, and Raymond Mooney. 2014. UTexas: Natural language semantics using distributional semantics and probabilistic logic. In Proceedings of SemEval, pages 796–801, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>546--556</pages>
<location>Jeju Island,</location>
<contexts>
<context position="28468" citStr="Blacoe and Lapata (2012)" startWordPosition="4601" endWordPosition="4605">ve that C-PHRASE is providing excellent word representations, (nearly) as good or better than the C-BOW vectors of Baroni and colleagues in all cases, except for ap. Whenever C-PHRASE is not close to the state of the art results, the latter relied on richer knowledge sources and/or much larger corpora (ap, esslli, an). Turning to the sentential tasks (Table 2), we first remark that using high-quality word vectors (such as C-BOW) and summing them leads to good results in all tasks, competitive with those obtained with more sophisticated composition models. This confirms the observation made by Blacoe and Lapata (2012) that simple-minded composition models are not necessarily worse than advanced approaches. Still, C-PHRASE is consistently better than C-BOW in all tasks, except sst, where the two models reach the same performance level. C-PHRASE is outperforming PV on all tasks except sick-e, where the two models have the same performance, and onwn2, where PV is slightly better. C-PHRASE is outperforming PLF by a large margin on the SICK sets, whereas the two models are equal on msrvid, and PLF better on onwn2. Recall, however, that on the latter two benchmarks PLF used extra word overlap and sentence length</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of EMNLP, pages 546–556, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="16425" citStr="Bruni et al. (2014)" startWordPosition="2626" endWordPosition="2629">ntext of red apples, but we could also take the context to be object-of-eat. 3 Evaluation 3.1 Data sets Semantic relatedness of words In this classic lexical task, the models are required to quantify the degree of semantic similarity or relatedness of pairs of words in terms of cosines between the corresponding vectors. These scores are then compared to human gold standards. Performance is assessed by computing the correlation between system and human scores (Spearman correlation in all tasks except rg, where it is customary to report Pearson). We used, first of all, the MEN (men) data set of Bruni et al. (2014), that is split into 1K pairs for training/development, and 1K pairs for testing. We used the training set to tune the hyperparameters of our model, and report performance on the test set. The C-BOW model of Baroni et al. (2014b) achieved state-of-the art performance on MEN test. We also evaluate on the widely used WordSim353 set introduced by Finkelstein et al. (2002), which consists of 353 word pairs. The WordSim353 data were split by Agirre et al. (2009) into similarity (wss) and relatedness (wsr) subsets, focusing on strictly taxonomic (television/radio) vs. broader topical cases (Maradona</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="25182" citStr="Coecke et al. (2010)" startWordPosition="4058" endWordPosition="4062"> the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, we compare our model against adding the best C-BOW vectors pretrained by Baroni and colleagues,8 and adding our Skip-gram vectors. We compare the additive approaches to two sophisticated composition models. The first is the Practical Lexical Function (PLF) model of Paperno et al. (2014). This is a linguistically motivated model in the tradition of the “functional composition” approaches of Coecke et al. (2010) and Baroni et al. (2014a), and the only model in this line of research that has been shown to empirically scale up to real-life sentence challenges. In short, in the PLF model all words are represented by vectors. Words acting as argument-taking functions (such as verbs or adjectives) are also associated to one matrix for each argument they take (e.g., each transitive verb has a subject and an object matrix). Vector representations of arguments are recursively multiplied by function matrices, following the syntactic tree up to the top node. The final sentence representation is obtained by sum</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>59--66</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="15153" citStr="Curran and Moens, 2002" startWordPosition="2417" endWordPosition="2420"> to what extent it can instead be reconstructed from the typical role that words play in sentences. 3We do not report results for component-wise multiplicative in our evaluation because it performed much worse than addition in all the tasks. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, gi</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition, pages 59–66, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: rationale, evaluation and approaches.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--459</pages>
<contexts>
<context position="21821" citStr="Dagan et al., 2009" startWordPosition="3501" endWordPosition="3505">n are syntactic fragments (“cause something to pass or lead somewhere”), rather than full sentences. We report top performance on these tasks from the respective shared challenges, as summarized by Agirre et al. (2012) and Agirre et al. (2013). Again, the top systems use feature-rich, supervised methods relying on distributional similarity as well as other sources, such as WordNet and named entity recognizers. Sentential entailment Detecting the presence of entailment between sentences or longer passages is one of the most useful features that the computational analysis of text could provide (Dagan et al., 2009). We test our model on the SICK 975 entailment task (sick-e) (Marelli et al., 2014). All SICK sentence pairs are labeled as ENTAILING (“Two teams are competing in a football match”/”Two groups of people are playing football”), CONTRADICTING (“The brown horse is near a red barrel at the rodeo”/“The brown horse is far from a red barrel at the rodeo”) or NEUTRAL (“A man in a black jacket is doing tricks on a motorbike”/”A person is riding the bicycle on one wheel”). For each model, we train a simple SVM classifier based on 2 features: cosine similarity between the two sentence vectors, as given b</context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: rationale, evaluation and approaches. Natural Language Engineering, 15:459–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>50--58</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2222" citStr="Dinu et al., 2013" startWordPosition="322" endWordPosition="325">n1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unitn. it/composes/semantic-vectors.html). The experiments there suggest that only the Glove vectors of Pennington et al. (2014) are competitive with C-BOW, and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW. tic models are estimated by optimizing the prediction of the contexts in which phrases occur in corpora (Baroni and Zamparelli, 2010; Guevara, 2010; Dinu et al., 2013). However, these compositional approaches assume that word vectors have already been constructed, and contextual evidence is only used to induce optimal combination rules to derive representations of phrases and sentences. In this paper, we follow through on this observation to propose the new C-PHRASE model. Similarly to C-BOW, C-PHRASE learns word representations by optimizing their joint context prediction. However, unlike in flat, window-based C-BOW, C-PHRASE groups words according to their syntactic structure, and it simultaneously optimizes context-predictions at different levels of the </context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. General estimation and evaluation of compositional distributional semantic models. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, pages 50– 58, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>897--906</pages>
<location>Honolulu, HI.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP, pages 897–906, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="16796" citStr="Finkelstein et al. (2002)" startWordPosition="2689" endWordPosition="2692">d standards. Performance is assessed by computing the correlation between system and human scores (Spearman correlation in all tasks except rg, where it is customary to report Pearson). We used, first of all, the MEN (men) data set of Bruni et al. (2014), that is split into 1K pairs for training/development, and 1K pairs for testing. We used the training set to tune the hyperparameters of our model, and report performance on the test set. The C-BOW model of Baroni et al. (2014b) achieved state-of-the art performance on MEN test. We also evaluate on the widely used WordSim353 set introduced by Finkelstein et al. (2002), which consists of 353 word pairs. The WordSim353 data were split by Agirre et al. (2009) into similarity (wss) and relatedness (wsr) subsets, focusing on strictly taxonomic (television/radio) vs. broader topical cases (Maradona/football), respectively. State-of-theart performance on both sets is reported by Baroni 974 et al. (2014b), with the C-BOW model. We further consider the classic data set of Rubenstein and Goodenough (1965) (rg), consisting of 65 noun pairs. We report the state-of-the-art from Hassan and Mihalcea (2011), which exploited Wikipedia’s linking structure. Concept categoriz</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="15173" citStr="Grefenstette, 1994" startWordPosition="2421" endWordPosition="2422">nstead be reconstructed from the typical role that words play in sentences. 3We do not report results for component-wise multiplicative in our evaluation because it performed much worse than addition in all the tasks. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, given eat (red apples)</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the EMNLP GEMS Workshop,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="2202" citStr="Guevara, 2010" startWordPosition="320" endWordPosition="321">ributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unitn. it/composes/semantic-vectors.html). The experiments there suggest that only the Glove vectors of Pennington et al. (2014) are competitive with C-BOW, and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW. tic models are estimated by optimizing the prediction of the contexts in which phrases occur in corpora (Baroni and Zamparelli, 2010; Guevara, 2010; Dinu et al., 2013). However, these compositional approaches assume that word vectors have already been constructed, and contextual evidence is only used to induce optimal combination rules to derive representations of phrases and sentences. In this paper, we follow through on this observation to propose the new C-PHRASE model. Similarly to C-BOW, C-PHRASE learns word representations by optimizing their joint context prediction. However, unlike in flat, window-based C-BOW, C-PHRASE groups words according to their syntactic structure, and it simultaneously optimizes context-predictions at diff</context>
<context position="12187" citStr="Guevara (2010)" startWordPosition="1957" endWordPosition="1958">arge unannotated corpora and efficient pre-trained parsers are available for many languages, making the CPHRASE knowledge demands feasible for practical purposes. There is no need to parse the sentences we want to build representations for at test time, since the component word vectors are simply added. The only parameters of the model are the word vectors; specifically, no extra parameters are needed for composition (composition models such as the one presented in Socher et al. (2012) require an extra parameter matrix for each word in the vocabulary, and even leaner models such as the one of Guevara (2010) must estimate a parameter matrix for each composition rule in the grammar). This makes C-PHRASE as simple as additive and multiplicative composition (Mitchell and 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Supervision Unlike many recent composition models (Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Socher et al., 2012; Socher et al., 2013, among others), the context-prediction obj</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the EMNLP GEMS Workshop, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="5051" citStr="Harris, 1954" startWordPosition="770" endWordPosition="771">ir combination into account. More precisely, it tries to predict a context word from the combination of the previous and following words, where the combination method is vector addition. The objective function is: log p(wt|wt−c..wt−1, wt+1..wt+c) (2) While other distributional models consider sequences of words jointly as context when estimating the parameters for a single word (Agirre et al., 2009; Melamud et al., 2014), C-BOW is unique in that it estimates the weights of a sequence of words jointly, based on their shared context. In this respect, C-BOW extends the distributional hypothesis (Harris, 1954) that words with similar context distributions should have similar meanings to longer sequences. However, the word combinations of C-BOW are not natural linguistic constituents, but arbitrary n-grams (e.g., sequences of 5 words with a gap in the middle). Moreover, the model does not attempt to capture the hierarchical nature of syntactic phrasing, such that big brown dog is a meaningful phrase, but so are its children brown dog and dog. C-PHRASE aims at capturing the same intuition that word combinations with similar context distributions will have similar meaning, but it applies it to syntact</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(2-3):1456–1162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semantic relatedness using salient semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>884--889</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="17330" citStr="Hassan and Mihalcea (2011)" startWordPosition="2771" endWordPosition="2774">. We also evaluate on the widely used WordSim353 set introduced by Finkelstein et al. (2002), which consists of 353 word pairs. The WordSim353 data were split by Agirre et al. (2009) into similarity (wss) and relatedness (wsr) subsets, focusing on strictly taxonomic (television/radio) vs. broader topical cases (Maradona/football), respectively. State-of-theart performance on both sets is reported by Baroni 974 et al. (2014b), with the C-BOW model. We further consider the classic data set of Rubenstein and Goodenough (1965) (rg), consisting of 65 noun pairs. We report the state-of-the-art from Hassan and Mihalcea (2011), which exploited Wikipedia’s linking structure. Concept categorization Systems are asked to group a set of nominal concepts into broader categories (e.g. arthritis and anthrax into illness; banana and grape into fruit). As in previous work, we treat this as an unsupervised clustering task. We feed the similarity matrix produced by a model for all concepts in a test set to the CLUTO toolkit (Karypis, 2003), that clusters them into n groups, where n is the number of categories. We use standard CLUTO parameters from the literature, and quantify performance by cluster purity with respect to the g</context>
</contexts>
<marker>Hassan, Mihalcea, 2011</marker>
<rawString>Samer Hassan and Rada Mihalcea. 2011. Semantic relatedness using salient semantic analysis. In Proceedings of AAAI, pages 884–889, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>KyungHyun Cho</author>
<author>S´ebastien Jean</author>
<author>Coline Devin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Not all neural embeddings are born equal.</title>
<date>2014</date>
<booktitle>In Proceedings of the NIPS Learning Semantics Workshop,</booktitle>
<location>Montreal, Canada.</location>
<note>Published online: https://sites.google.com/site/ learningsemantics2014/.</note>
<contexts>
<context position="10999" citStr="Hill et al. (2014)" startWordPosition="1768" endWordPosition="1771">the vectors are optimized based on their occurrence in phrases of different syntactic complexity, they produce good sentence representations when they are combined. To the best of our knowledge, C-PHRASE is the first model that is jointly optimized for lexical and compositional tasks. C-BOW uses shallow composition information to learn word vectors. Conversely, some compositional models –e.g., Kalchbrenner et al. (2014), Socher et al. (2013)– induce word representations, that are only optimized for a compositional task and are not tested at the lexical level. Somewhat relatedly to what we do, Hill et al. (2014) evaluated representations learned in a sentence translation task on wordlevel benchmarks. Some a priori justification for treating word and sentence learning as joint problems comes from human language acquisition, as it is obvious that children learn word and phrase meanings in parallel and interactively, not sequentially (Tomasello, 2003). Knowledge-leanness and simplicity For training, C-PHRASE requires a large, syntacticallyparsed corpus (more precisely, it only requires the constituent structure assigned by the parser, as it is blind to syntactic labels). Both large unannotated corpora a</context>
</contexts>
<marker>Hill, Cho, Jean, Devin, Bengio, 2014</marker>
<rawString>Felix Hill, KyungHyun Cho, S´ebastien Jean, Coline Devin, and Yoshua Bengio. 2014. Not all neural embeddings are born equal. In Proceedings of the NIPS Learning Semantics Workshop, Montreal, Canada. Published online: https://sites.google.com/site/ learningsemantics2014/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>119--126</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="12675" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="2034" endWordPosition="2037">cher et al. (2012) require an extra parameter matrix for each word in the vocabulary, and even leaner models such as the one of Guevara (2010) must estimate a parameter matrix for each composition rule in the grammar). This makes C-PHRASE as simple as additive and multiplicative composition (Mitchell and 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Supervision Unlike many recent composition models (Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Socher et al., 2012; Socher et al., 2013, among others), the context-prediction objective of C-PHRASE does not require annotated data, and it is meant to provide generalpurpose representations that can serve in different tasks. C-PHRASE vectors can also be used as initialization parameters for fully supervised, task-specific systems. Alternatively, the current unsupervised objective could be combined with task-specific supervised objectives to fine-tune CPHRASE to specific purposes. Sensitivity to syntactic structure During training, C-PHRASE is sensitive to syntac</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural networks for discourse compositionality. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, pages 119–126, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>655--665</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="10804" citStr="Kalchbrenner et al. (2014)" startWordPosition="1733" endWordPosition="1737">y because C-PHRASE is trained to predict how the contexts of a word change based on its phrasal collocates (cup will have very different contexts in world cup vs. coffee cup ). At the same time, because the vectors are optimized based on their occurrence in phrases of different syntactic complexity, they produce good sentence representations when they are combined. To the best of our knowledge, C-PHRASE is the first model that is jointly optimized for lexical and compositional tasks. C-BOW uses shallow composition information to learn word vectors. Conversely, some compositional models –e.g., Kalchbrenner et al. (2014), Socher et al. (2013)– induce word representations, that are only optimized for a compositional task and are not tested at the lexical level. Somewhat relatedly to what we do, Hill et al. (2014) evaluated representations learned in a sentence translation task on wordlevel benchmarks. Some a priori justification for treating word and sentence learning as joint problems comes from human language acquisition, as it is obvious that children learn word and phrase meanings in parallel and interactively, not sequentially (Tomasello, 2003). Knowledge-leanness and simplicity For training, C-PHRASE req</context>
<context position="12702" citStr="Kalchbrenner et al., 2014" startWordPosition="2038" endWordPosition="2042">tra parameter matrix for each word in the vocabulary, and even leaner models such as the one of Guevara (2010) must estimate a parameter matrix for each composition rule in the grammar). This makes C-PHRASE as simple as additive and multiplicative composition (Mitchell and 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Supervision Unlike many recent composition models (Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Socher et al., 2012; Socher et al., 2013, among others), the context-prediction objective of C-PHRASE does not require annotated data, and it is meant to provide generalpurpose representations that can serve in different tasks. C-PHRASE vectors can also be used as initialization parameters for fully supervised, task-specific systems. Alternatively, the current unsupervised objective could be combined with task-specific supervised objectives to fine-tune CPHRASE to specific purposes. Sensitivity to syntactic structure During training, C-PHRASE is sensitive to syntactic structure. To cite an e</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of ACL, pages 655–665, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Karypis</author>
</authors>
<title>CLUTO: A clustering toolkit.</title>
<date>2003</date>
<tech>Technical Report 02-017,</tech>
<institution>University of Minnesota Department of Computer Science.</institution>
<contexts>
<context position="17739" citStr="Karypis, 2003" startWordPosition="2840" endWordPosition="2841">(2014b), with the C-BOW model. We further consider the classic data set of Rubenstein and Goodenough (1965) (rg), consisting of 65 noun pairs. We report the state-of-the-art from Hassan and Mihalcea (2011), which exploited Wikipedia’s linking structure. Concept categorization Systems are asked to group a set of nominal concepts into broader categories (e.g. arthritis and anthrax into illness; banana and grape into fruit). As in previous work, we treat this as an unsupervised clustering task. We feed the similarity matrix produced by a model for all concepts in a test set to the CLUTO toolkit (Karypis, 2003), that clusters them into n groups, where n is the number of categories. We use standard CLUTO parameters from the literature, and quantify performance by cluster purity with respect to the gold categories. The AlmuharebPoesio benchmark (Almuhareb, 2006) (ap) consists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance (Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI 2008 data set (Baroni et al., 2008) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by Katrenko and Adriaans </context>
</contexts>
<marker>Karypis, 2003</marker>
<rawString>George Karypis. 2003. CLUTO: A clustering toolkit. Technical Report 02-017, University of Minnesota Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophia Katrenko</author>
<author>Pieter Adriaans</author>
</authors>
<title>Qualia structures and their impact on the concrete noun categorization task.</title>
<date>2008</date>
<booktitle>In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics,</booktitle>
<pages>17--24</pages>
<location>Hamburg, Germany.</location>
<contexts>
<context position="18345" citStr="Katrenko and Adriaans (2008)" startWordPosition="2932" endWordPosition="2935">oolkit (Karypis, 2003), that clusters them into n groups, where n is the number of categories. We use standard CLUTO parameters from the literature, and quantify performance by cluster purity with respect to the gold categories. The AlmuharebPoesio benchmark (Almuhareb, 2006) (ap) consists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance (Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI 2008 data set (Baroni et al., 2008) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by Katrenko and Adriaans (2008) by using full-Web queries and manually crafted patterns. Semantic analogy The last lexical task we pick is analogy (an), introduced in Mikolov et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the example pair. Mikolov et al. (2013c) subtract the vector of the first word in a pair from the second, add the vector of the test word and look for the nea</context>
</contexts>
<marker>Katrenko, Adriaans, 2008</marker>
<rawString>Sophia Katrenko and Pieter Adriaans. 2008. Qualia structures and their impact on the concrete noun categorization task. In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics, pages 17–24, Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="23905" citStr="Klein and Manning, 2003" startWordPosition="3858" endWordPosition="3861"> vectors composed by a model as features, and report accuracy on the test set. State of the art is obtained by Le and Mikolov (2014) with the Paragraph Vector approach we describe below. 3.2 Model implementation The source corpus we use to build the lexical vectors is created by concatenating three sources: ukWaC,4 a mid-2009 dump of the English Wikipedia5 and the British National Corpus6 (about 2.8B words in total). We build vectors for the 180K words occurring at least 100 times in the corpus. Since our training procedure requires parsed trees, we parse the corpus using the Stanford parser (Klein and Manning, 2003). C-PHRASE has two hyperparameters (see Section 2 above), namely basic window size (c1) and height-dependent window enlargement factor (c2). 4http://wacky.sslmit.unibo.it 5http://en.wikipedia.org 6http://www.natcorp.ox.ac.uk Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5. As already mentioned, sentence vectors are built by summing the vectors of the</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings ofACL, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Lai</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Illinois-lh: A denotational and distributional approach to semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>329--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="22641" citStr="Lai and Hockenmaier (2014)" startWordPosition="3648" endWordPosition="3651">f people are playing football”), CONTRADICTING (“The brown horse is near a red barrel at the rodeo”/“The brown horse is far from a red barrel at the rodeo”) or NEUTRAL (“A man in a black jacket is doing tricks on a motorbike”/”A person is riding the bicycle on one wheel”). For each model, we train a simple SVM classifier based on 2 features: cosine similarity between the two sentence vectors, as given by the models, and whether the sentence pair contains a negation word (the latter has been shown to be a very informative feature for SICK entailment). The current state-of-the-art is reached by Lai and Hockenmaier (2014), using a much richer set of features, that include WordNet, the denotation graph of Young et al. (2014) and extra training data from other resources. Sentiment analysis Finally, as sentiment analysis has emerged as a popular area of application for compositional models, we test our methods on the Stanford Sentiment Treebank (Socher et al., 2013) (sst), consisting of 11,855 sentences from movie reviews, using the coarse annotation into 2 sentiment degrees (negative/positive). We follow the official split into train (8,544), development (1,101) and test (2,210) parts. We train an SVM classifier</context>
</contexts>
<marker>Lai, Hockenmaier, 2014</marker>
<rawString>Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A denotational and distributional approach to semantics. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329–334, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</title>
<date>2014</date>
<contexts>
<context position="23413" citStr="Mikolov (2014)" startWordPosition="3778" endWordPosition="3779">nt analysis Finally, as sentiment analysis has emerged as a popular area of application for compositional models, we test our methods on the Stanford Sentiment Treebank (Socher et al., 2013) (sst), consisting of 11,855 sentences from movie reviews, using the coarse annotation into 2 sentiment degrees (negative/positive). We follow the official split into train (8,544), development (1,101) and test (2,210) parts. We train an SVM classifier on the training set, using the sentence vectors composed by a model as features, and report accuracy on the test set. State of the art is obtained by Le and Mikolov (2014) with the Paragraph Vector approach we describe below. 3.2 Model implementation The source corpus we use to build the lexical vectors is created by concatenating three sources: ukWaC,4 a mid-2009 dump of the English Wikipedia5 and the British National Corpus6 (about 2.8B words in total). We build vectors for the 180K words occurring at least 100 times in the corpus. Since our training procedure requires parsed trees, we parse the corpus using the Stanford parser (Klein and Manning, 2003). C-PHRASE has two hyperparameters (see Section 2 above), namely basic window size (c1) and height-dependent</context>
<context position="26741" citStr="Mikolov (2014)" startWordPosition="4322" endWordPosition="4323">g that they also used two simple but precious cues (word overlap and sentence length) we do not adopt here. We used their pre-trained vectors and matrices also for the SICK challenges, while the number of new ma7For fairness, we report their results when all tasks were evaluated with the same set of parameters, tuned on rg: this is row 8 of their Table 2. 8http://clic.cimec.unitn.it/composes/ semantic-vectors.html 976 trices to estimate made it too time-consuming to implement this model in the onwn1 and sst tasks. Finally, we test the Paragraph Vector (PV) approach recently proposed by Le and Mikolov (2014). Under PV, sentence representations are learned by predicting the words that occur in them. This unsupervised method has been shown by the authors to outperform much more sophisticated, supervised neural-network-based composition models on the sst task. We use our own implementation for this approach. Unlike in the original experiments, we found the PV-DBOW variant of PV to consistently outperform PV-DM, and so we report results obtained with the former. Note that PV-DBOW aims mainly at providing representations for sentences, not words. When we do not need to induce vectors for sentences in </context>
<context position="30068" citStr="Mikolov (2014)" startWordPosition="4869" endWordPosition="4870">ticipated in the SICK SemEval challenge, and comparable to that of Beltagy et al. (2014), who entered the competition with a system combining distributional semantics with a supervised probabilistic soft logic system. For sick-e (the entailment task), C-PHRASE’s performance is less than one point below the median of the SemEval systems, and slightly above that of the Stanford submission, that used a recursive neural network with a tensor layer. Finally, the performance of all our models, including PV, on sst is remarkably lower than the state-of-the-art performance of PV as reported by Le and Mikolov (2014). We believe that the crucial difference is that these authors estimated PV vectors specifically on the sentiment treebank training data, thus building ad-hoc vectors encoding the semantics of movie reviews. We leave it to further research to ascertain whether we could better fine-tune our models to sst by including the sentiment treebank training phrases in our source corpus. Comparing vector lengths of C-BOW and CPHRASE We gather some insight into how the C-PHRASE objective might adjust word representations for composition with respect to C-BOW by looking at how the length of word vectors ch</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL (Volume 2: Short Papers),</booktitle>
<pages>302--308</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="15219" citStr="Levy and Goldberg, 2014" startWordPosition="2427" endWordPosition="2430">role that words play in sentences. 3We do not report results for component-wise multiplicative in our evaluation because it performed much worse than addition in all the tasks. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, given eat (red apples), we treat eat as window-based context of red </context>
<context position="19110" citStr="Levy and Goldberg (2014" startWordPosition="3070" endWordPosition="3073">v et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the example pair. Mikolov et al. (2013c) subtract the vector of the first word in a pair from the second, add the vector of the test word and look for the nearest neighbor of the resulting vector (e.g., find the word whose vector is closest to king - man + woman). We follow the method introduced by Levy and Goldberg (2014b), which returns the word x maximizing cos(king,x)×cos(woman,x) .This cos(man,x) method yields better results for all models. Performance is measured by accuracy in retrieving the correct answer (in our search space of 180K words). The current state of the art on the semantic part and on the whole data set was reached by Pennington et al. (2014), who trained their word representations on a huge corpus consisting of 42B words. Sentential semantic relatedness Similarly to word relatedness, composed sentence representations can be evaluated against benchmarks where humans provided relatedness/si</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Dependencybased word embeddings. In Proceedings of ACL (Volume 2: Short Papers), pages 302–308, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>171--180</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="15219" citStr="Levy and Goldberg, 2014" startWordPosition="2427" endWordPosition="2430">role that words play in sentences. 3We do not report results for component-wise multiplicative in our evaluation because it performed much worse than addition in all the tasks. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, given eat (red apples), we treat eat as window-based context of red </context>
<context position="19110" citStr="Levy and Goldberg (2014" startWordPosition="3070" endWordPosition="3073">v et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the example pair. Mikolov et al. (2013c) subtract the vector of the first word in a pair from the second, add the vector of the test word and look for the nearest neighbor of the resulting vector (e.g., find the word whose vector is closest to king - man + woman). We follow the method introduced by Levy and Goldberg (2014b), which returns the word x maximizing cos(king,x)×cos(woman,x) .This cos(man,x) method yields better results for all models. Performance is measured by accuracy in retrieving the correct answer (in our search space of 180K words). The current state of the art on the semantic part and on the whole data set was reached by Pennington et al. (2014), who trained their word representations on a huge corpus consisting of 42B words. Sentential semantic relatedness Similarly to word relatedness, composed sentence representations can be evaluated against benchmarks where humans provided relatedness/si</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014b. Linguistic regularities in sparse and explicit word representations. In Proceedings of CoNLL, pages 171–180, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>1--8</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="20170" citStr="Marelli et al., 2014" startWordPosition="3238" endWordPosition="3241">al semantic relatedness Similarly to word relatedness, composed sentence representations can be evaluated against benchmarks where humans provided relatedness/similarity scores for sentence pairs (sentences with high scores, such as “A person in a black jacket is doing tricks on a motorbike”/“A man in a black jacket is doing tricks on a motorbike” from the SICK data-set, tend to be near-paraphrases). Following previous work on these data sets, Pearson correlation is our figure of merit, and we report it between human scores and sentence vector cosine similarities computed by the models. SICK (Marelli et al., 2014) (sick-r) was created specifically for the purpose of evaluating compositional models, focusing on linguistic phenomena such as lexical variation and word order. Here we report performance of the systems on the test part of the data set, which contains 5K sentence pairs. The top performance (from the SICK SemEval shared task) was reached by Zhao et al. (2014) using a heterogeneous set of features that include WordNet and extra training corpora. Agirre et al. (2012) and Agirre et al. (2013) created two collections of sentential similarities consisting of subsets coming from different sources. F</context>
<context position="21904" citStr="Marelli et al., 2014" startWordPosition="3517" endWordPosition="3520">han full sentences. We report top performance on these tasks from the respective shared challenges, as summarized by Agirre et al. (2012) and Agirre et al. (2013). Again, the top systems use feature-rich, supervised methods relying on distributional similarity as well as other sources, such as WordNet and named entity recognizers. Sentential entailment Detecting the presence of entailment between sentences or longer passages is one of the most useful features that the computational analysis of text could provide (Dagan et al., 2009). We test our model on the SICK 975 entailment task (sick-e) (Marelli et al., 2014). All SICK sentence pairs are labeled as ENTAILING (“Two teams are competing in a football match”/”Two groups of people are playing football”), CONTRADICTING (“The brown horse is near a red barrel at the rodeo”/“The brown horse is far from a red barrel at the rodeo”) or NEUTRAL (“A man in a black jacket is doing tricks on a motorbike”/”A person is riding the bicycle on one wheel”). For each model, we train a simple SVM classifier based on 2 features: cosine similarity between the two sentence vectors, as given by the models, and whether the sentence pair contains a negation word (the latter ha</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of SemEval, pages 1–8, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Melamud</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
<author>Idan Szpektor</author>
<author>Deniz Yuret</author>
</authors>
<title>Probabilistic modeling of joint-context in distributional similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>181--190</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="4862" citStr="Melamud et al., 2014" startWordPosition="737" endWordPosition="740">onsisting of the context words wt+j that must be predicted by the induced vector representation for the target. While Skip-gram learns each word representation separately, the C-BOW model takes their combination into account. More precisely, it tries to predict a context word from the combination of the previous and following words, where the combination method is vector addition. The objective function is: log p(wt|wt−c..wt−1, wt+1..wt+c) (2) While other distributional models consider sequences of words jointly as context when estimating the parameters for a single word (Agirre et al., 2009; Melamud et al., 2014), C-BOW is unique in that it estimates the weights of a sequence of words jointly, based on their shared context. In this respect, C-BOW extends the distributional hypothesis (Harris, 1954) that words with similar context distributions should have similar meanings to longer sequences. However, the word combinations of C-BOW are not natural linguistic constituents, but arbitrary n-grams (e.g., sequences of 5 words with a gap in the middle). Moreover, the model does not attempt to capture the hierarchical nature of syntactic phrasing, such that big brown dog is a meaningful phrase, but so are it</context>
</contexts>
<marker>Melamud, Dagan, Goldberger, Szpektor, Yuret, 2014</marker>
<rawString>Oren Melamud, Ido Dagan, Jacob Goldberger, Idan Szpektor, and Deniz Yuret. 2014. Probabilistic modeling of joint-context in distributional similarity. In Proceedings of CoNLL, pages 181–190, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<note>http://arxiv.org/ abs/1301.3781/.</note>
<contexts>
<context position="1131" citStr="Mikolov et al., 2013" startWordPosition="149" endWordPosition="152">E outperforms the state-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models. 1 Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baron</context>
<context position="3626" citStr="Mikolov et al. (2013" startWordPosition="541" endWordPosition="544">g, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves. C-PHRASE word representations outperform C-BOW on several word-level benchmarks. In addition, because they are estimated in a compositional way, C-PHRASE word vectors, when combined through simple addition, produce sentence representations that are better than those obtained when adding other kinds of vectors, and competitive against ad-hoc compositional methods on various sentence meaning benchmarks. 2 The C-PHRASE model We start with a brief overview of the models proposed by Mikolov et al. (2013a), as C-PHRASE builds on them. The Skip-gram model derives the vector of a target word by setting its weights to predict the words surrounding it in the corpus. 971 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 971–981, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics More specifically, the objective function is: log p(wt+j|wt) (1) where the word sequence w1, w2, ..., wT is the training corpus and c is the size of the window around the ta</context>
<context position="7362" citStr="Mikolov et al. (2013" startWordPosition="1169" endWordPosition="1172"> log probabilities of the words in the context window of the well-formed constituents with stochastic gradient descent: E E(log p(wl−j |C[wl, ··· , wr]) C[wl,··· ,wr]∈T 1≤j≤cC �+ log p(wr+j|C[wl, ··· , wr]) (3) with p theoretically defined as: p(wO|C[wl,··· ,wr]) �0&gt; Er� i=l vwi exp v wO r−l+1 � � W ex 2I/T Eri=l vwi �w=1 p w r−l+1 where W is the size of the vocabulary, v0 and v denote output (context) and input vectors, respectively, and we take the input vectors to represent the words. In practice, since the normalization constant for the above probability is expensive to compute, we follow Mikolov et al. (2013b) and use negative sampling. We let the context window size cC vary as a function of the height of the constituent in the syntactic tree. The height h(C) of a constituent is given by the maximum number of intermediate nodes separating it from any of the words it dominates (such that h = 0 for words, h = 1 for two-word phrases, etc.). Then, for a constituent of height h(C), C-PHRASE considers cC = c1 + h(C)c2 context words to its left and right (the nonnegative integers c1 and c2 are hyperparameters of the model; with c2 = 0, context becomes constant 2Although here we only use single words as </context>
<context position="18501" citStr="Mikolov et al. (2013" startWordPosition="2958" endWordPosition="2961">rformance by cluster purity with respect to the gold categories. The AlmuharebPoesio benchmark (Almuhareb, 2006) (ap) consists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance (Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI 2008 data set (Baroni et al., 2008) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by Katrenko and Adriaans (2008) by using full-Web queries and manually crafted patterns. Semantic analogy The last lexical task we pick is analogy (an), introduced in Mikolov et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the example pair. Mikolov et al. (2013c) subtract the vector of the first word in a pair from the second, add the vector of the test word and look for the nearest neighbor of the resulting vector (e.g., find the word whose vector is closest to king - man + woman). We follow the method introduced by Levy and Goldb</context>
<context position="24170" citStr="Mikolov et al. (2013" startWordPosition="3887" endWordPosition="3890">created by concatenating three sources: ukWaC,4 a mid-2009 dump of the English Wikipedia5 and the British National Corpus6 (about 2.8B words in total). We build vectors for the 180K words occurring at least 100 times in the corpus. Since our training procedure requires parsed trees, we parse the corpus using the Stanford parser (Klein and Manning, 2003). C-PHRASE has two hyperparameters (see Section 2 above), namely basic window size (c1) and height-dependent window enlargement factor (c2). 4http://wacky.sslmit.unibo.it 5http://en.wikipedia.org 6http://www.natcorp.ox.ac.uk Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5. As already mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, w</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. http://arxiv.org/ abs/1301.3781/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, NV.</location>
<contexts>
<context position="1131" citStr="Mikolov et al., 2013" startWordPosition="149" endWordPosition="152">E outperforms the state-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models. 1 Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baron</context>
<context position="3626" citStr="Mikolov et al. (2013" startWordPosition="541" endWordPosition="544">g, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves. C-PHRASE word representations outperform C-BOW on several word-level benchmarks. In addition, because they are estimated in a compositional way, C-PHRASE word vectors, when combined through simple addition, produce sentence representations that are better than those obtained when adding other kinds of vectors, and competitive against ad-hoc compositional methods on various sentence meaning benchmarks. 2 The C-PHRASE model We start with a brief overview of the models proposed by Mikolov et al. (2013a), as C-PHRASE builds on them. The Skip-gram model derives the vector of a target word by setting its weights to predict the words surrounding it in the corpus. 971 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 971–981, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics More specifically, the objective function is: log p(wt+j|wt) (1) where the word sequence w1, w2, ..., wT is the training corpus and c is the size of the window around the ta</context>
<context position="7362" citStr="Mikolov et al. (2013" startWordPosition="1169" endWordPosition="1172"> log probabilities of the words in the context window of the well-formed constituents with stochastic gradient descent: E E(log p(wl−j |C[wl, ··· , wr]) C[wl,··· ,wr]∈T 1≤j≤cC �+ log p(wr+j|C[wl, ··· , wr]) (3) with p theoretically defined as: p(wO|C[wl,··· ,wr]) �0&gt; Er� i=l vwi exp v wO r−l+1 � � W ex 2I/T Eri=l vwi �w=1 p w r−l+1 where W is the size of the vocabulary, v0 and v denote output (context) and input vectors, respectively, and we take the input vectors to represent the words. In practice, since the normalization constant for the above probability is expensive to compute, we follow Mikolov et al. (2013b) and use negative sampling. We let the context window size cC vary as a function of the height of the constituent in the syntactic tree. The height h(C) of a constituent is given by the maximum number of intermediate nodes separating it from any of the words it dominates (such that h = 0 for words, h = 1 for two-word phrases, etc.). Then, for a constituent of height h(C), C-PHRASE considers cC = c1 + h(C)c2 context words to its left and right (the nonnegative integers c1 and c2 are hyperparameters of the model; with c2 = 0, context becomes constant 2Although here we only use single words as </context>
<context position="18501" citStr="Mikolov et al. (2013" startWordPosition="2958" endWordPosition="2961">rformance by cluster purity with respect to the gold categories. The AlmuharebPoesio benchmark (Almuhareb, 2006) (ap) consists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance (Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI 2008 data set (Baroni et al., 2008) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by Katrenko and Adriaans (2008) by using full-Web queries and manually crafted patterns. Semantic analogy The last lexical task we pick is analogy (an), introduced in Mikolov et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the example pair. Mikolov et al. (2013c) subtract the vector of the first word in a pair from the second, add the vector of the test word and look for the nearest neighbor of the resulting vector (e.g., find the word whose vector is closest to king - man + woman). We follow the method introduced by Levy and Goldb</context>
<context position="24170" citStr="Mikolov et al. (2013" startWordPosition="3887" endWordPosition="3890">created by concatenating three sources: ukWaC,4 a mid-2009 dump of the English Wikipedia5 and the British National Corpus6 (about 2.8B words in total). We build vectors for the 180K words occurring at least 100 times in the corpus. Since our training procedure requires parsed trees, we parse the corpus using the Stanford parser (Klein and Manning, 2003). C-PHRASE has two hyperparameters (see Section 2 above), namely basic window size (c1) and height-dependent window enlargement factor (c2). 4http://wacky.sslmit.unibo.it 5http://en.wikipedia.org 6http://www.natcorp.ox.ac.uk Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5. As already mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, w</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119, Lake Tahoe, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>746--751</pages>
<location>Atlanta,</location>
<contexts>
<context position="1131" citStr="Mikolov et al., 2013" startWordPosition="149" endWordPosition="152">E outperforms the state-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models. 1 Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baron</context>
<context position="3626" citStr="Mikolov et al. (2013" startWordPosition="541" endWordPosition="544">g, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves. C-PHRASE word representations outperform C-BOW on several word-level benchmarks. In addition, because they are estimated in a compositional way, C-PHRASE word vectors, when combined through simple addition, produce sentence representations that are better than those obtained when adding other kinds of vectors, and competitive against ad-hoc compositional methods on various sentence meaning benchmarks. 2 The C-PHRASE model We start with a brief overview of the models proposed by Mikolov et al. (2013a), as C-PHRASE builds on them. The Skip-gram model derives the vector of a target word by setting its weights to predict the words surrounding it in the corpus. 971 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 971–981, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics More specifically, the objective function is: log p(wt+j|wt) (1) where the word sequence w1, w2, ..., wT is the training corpus and c is the size of the window around the ta</context>
<context position="7362" citStr="Mikolov et al. (2013" startWordPosition="1169" endWordPosition="1172"> log probabilities of the words in the context window of the well-formed constituents with stochastic gradient descent: E E(log p(wl−j |C[wl, ··· , wr]) C[wl,··· ,wr]∈T 1≤j≤cC �+ log p(wr+j|C[wl, ··· , wr]) (3) with p theoretically defined as: p(wO|C[wl,··· ,wr]) �0&gt; Er� i=l vwi exp v wO r−l+1 � � W ex 2I/T Eri=l vwi �w=1 p w r−l+1 where W is the size of the vocabulary, v0 and v denote output (context) and input vectors, respectively, and we take the input vectors to represent the words. In practice, since the normalization constant for the above probability is expensive to compute, we follow Mikolov et al. (2013b) and use negative sampling. We let the context window size cC vary as a function of the height of the constituent in the syntactic tree. The height h(C) of a constituent is given by the maximum number of intermediate nodes separating it from any of the words it dominates (such that h = 0 for words, h = 1 for two-word phrases, etc.). Then, for a constituent of height h(C), C-PHRASE considers cC = c1 + h(C)c2 context words to its left and right (the nonnegative integers c1 and c2 are hyperparameters of the model; with c2 = 0, context becomes constant 2Although here we only use single words as </context>
<context position="18501" citStr="Mikolov et al. (2013" startWordPosition="2958" endWordPosition="2961">rformance by cluster purity with respect to the gold categories. The AlmuharebPoesio benchmark (Almuhareb, 2006) (ap) consists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance (Rothenh¨ausler and Sch¨utze, 2009). The ESSLLI 2008 data set (Baroni et al., 2008) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by Katrenko and Adriaans (2008) by using full-Web queries and manually crafted patterns. Semantic analogy The last lexical task we pick is analogy (an), introduced in Mikolov et al. (2013c). We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the example pair. Mikolov et al. (2013c) subtract the vector of the first word in a pair from the second, add the vector of the test word and look for the nearest neighbor of the resulting vector (e.g., find the word whose vector is closest to king - man + woman). We follow the method introduced by Levy and Goldb</context>
<context position="24170" citStr="Mikolov et al. (2013" startWordPosition="3887" endWordPosition="3890">created by concatenating three sources: ukWaC,4 a mid-2009 dump of the English Wikipedia5 and the British National Corpus6 (about 2.8B words in total). We build vectors for the 180K words occurring at least 100 times in the corpus. Since our training procedure requires parsed trees, we parse the corpus using the Stanford parser (Klein and Manning, 2003). C-PHRASE has two hyperparameters (see Section 2 above), namely basic window size (c1) and height-dependent window enlargement factor (c2). 4http://wacky.sslmit.unibo.it 5http://en.wikipedia.org 6http://www.natcorp.ox.ac.uk Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5. As already mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, w</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of NAACL, pages 746–751, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="37508" citStr="Mitchell and Lapata, 2010" startWordPosition="6039" endWordPosition="6042">arse of the training corpus (but no syntactic labels are required, nor parsing at test time). C-PHRASE has only 3 hyperparameters and no composition-specific parameter to tune and store. Having established a strong empirical baseline with this parsimonious approach, in future research we want to investigate the impact of possible extensions on both lexical and sentential tasks. When combining the vectors, either for induction or composition, we will try replacing plain addition with other operations, starting with something as simple as learning scalar weights for different words in a phrase (Mitchell and Lapata, 2010). We also intend to explore more systematic ways to incorporate supervised signals into learning, to fine-tune C-PHRASE vectors to specific tasks. On the testing side, we are fascinated by the good performance of additive models, that (at test time, at least) do not take word order nor syntactic structure into account. We plan to perform a systematic analysis of both existing benchmarks and natural corpus data, both to assess the actual impact that such factors have on the aspects of meaning we are interested in (take two sentences in an entailment relation: how often does shuffling the words </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>A practical and linguistically-motivated approach to compositional distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>90--99</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="25056" citStr="Paperno et al. (2014)" startWordPosition="4039" endWordPosition="4042">dy mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, we compare our model against adding the best C-BOW vectors pretrained by Baroni and colleagues,8 and adding our Skip-gram vectors. We compare the additive approaches to two sophisticated composition models. The first is the Practical Lexical Function (PLF) model of Paperno et al. (2014). This is a linguistically motivated model in the tradition of the “functional composition” approaches of Coecke et al. (2010) and Baroni et al. (2014a), and the only model in this line of research that has been shown to empirically scale up to real-life sentence challenges. In short, in the PLF model all words are represented by vectors. Words acting as argument-taking functions (such as verbs or adjectives) are also associated to one matrix for each argument they take (e.g., each transitive verb has a subject and an object matrix). Vector representations of arguments are recursively multipli</context>
<context position="31869" citStr="Paperno et al. 2014" startWordPosition="5195" endWordPosition="5198"> 70 83 65 84 69 SOA 80 80 70 86 79 91 82 Table 1: Lexical task performance. See Section 3.1 for figures of merit (all in percentage form) and state-of-the-art references. C-BOW results (tuned on rg) are taken from Baroni et al. 2014b. sick-r sick-e msrvid onwn1 onwn2 sst Skip-gram 70 72 74 66 62 78 C-BOW 70 74 74 69 63 79 C-PHRASE 72 75 79 70 65 79 PLF 57 72 79 NA 67 NA PV 67 75 77 66 66 77 SOA 83 85 88 71 75 88 Table 2: Sentential task performance. See Section 3.1 for figures of merit (all in percentage form) and state-of-the-art references. The PLF results on msrvid and onwn2 are taken from Paperno et al. 2014. The relative-length-difference test returns the following words as the ones that are most severely de-emphasized by C-PHRASE compared to CBOW: be, that, an, not, they, he, who, when, well, have. Clearly, C-PHRASE is weighting down grammatical terms that tend to be contextagnostic, and will be accompanied, in phrases, by more context-informative content words. Indeed, the list of terms that are instead emphasized by C-PHRASE include such content-rich, monosemous words as gnawing, smackdown, demographics. This is confirmed by a POS-level analysis that indicates that the categories that are, on</context>
</contexts>
<marker>Paperno, Pham, Baroni, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings ofACL, pages 90–99, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1532--1543</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="1928" citStr="Pennington et al. (2014)" startWordPosition="271" endWordPosition="275">oss many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unitn. it/composes/semantic-vectors.html). The experiments there suggest that only the Glove vectors of Pennington et al. (2014) are competitive with C-BOW, and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW. tic models are estimated by optimizing the prediction of the contexts in which phrases occur in corpora (Baroni and Zamparelli, 2010; Guevara, 2010; Dinu et al., 2013). However, these compositional approaches assume that word vectors have already been constructed, and contextual evidence is only used to induce optimal combination rules to derive representations of phrases and sentences. In this paper, we follow through on this observation to propose the new C-PHRASE mo</context>
<context position="19458" citStr="Pennington et al. (2014)" startWordPosition="3128" endWordPosition="3131">ct the vector of the first word in a pair from the second, add the vector of the test word and look for the nearest neighbor of the resulting vector (e.g., find the word whose vector is closest to king - man + woman). We follow the method introduced by Levy and Goldberg (2014b), which returns the word x maximizing cos(king,x)×cos(woman,x) .This cos(man,x) method yields better results for all models. Performance is measured by accuracy in retrieving the correct answer (in our search space of 180K words). The current state of the art on the semantic part and on the whole data set was reached by Pennington et al. (2014), who trained their word representations on a huge corpus consisting of 42B words. Sentential semantic relatedness Similarly to word relatedness, composed sentence representations can be evaluated against benchmarks where humans provided relatedness/similarity scores for sentence pairs (sentences with high scores, such as “A person in a black jacket is doing tricks on a motorbike”/“A man in a black jacket is doing tricks on a motorbike” from the SICK data-set, tend to be near-paraphrases). Following previous work on these data sets, Pearson correlation is our figure of merit, and we report it </context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP, pages 1532–1543, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Rothenh¨ausler</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Unsupervised classification with dependency based word spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL GEMS Workshop,</booktitle>
<pages>17--24</pages>
<location>Athens, Greece.</location>
<marker>Rothenh¨ausler, Sch¨utze, 2009</marker>
<rawString>Klaus Rothenh¨ausler and Hinrich Sch¨utze. 2009. Unsupervised classification with dependency based word spaces. In Proceedings of the EACL GEMS Workshop, pages 17–24, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="17232" citStr="Rubenstein and Goodenough (1965)" startWordPosition="2756" endWordPosition="2759">the test set. The C-BOW model of Baroni et al. (2014b) achieved state-of-the art performance on MEN test. We also evaluate on the widely used WordSim353 set introduced by Finkelstein et al. (2002), which consists of 353 word pairs. The WordSim353 data were split by Agirre et al. (2009) into similarity (wss) and relatedness (wsr) subsets, focusing on strictly taxonomic (television/radio) vs. broader topical cases (Maradona/football), respectively. State-of-theart performance on both sets is reported by Baroni 974 et al. (2014b), with the C-BOW model. We further consider the classic data set of Rubenstein and Goodenough (1965) (rg), consisting of 65 noun pairs. We report the state-of-the-art from Hassan and Mihalcea (2011), which exploited Wikipedia’s linking structure. Concept categorization Systems are asked to group a set of nominal concepts into broader categories (e.g. arthritis and anthrax into illness; banana and grape into fruit). As in previous work, we treat this as an unsupervised clustering task. We feed the similarity matrix produced by a model for all concepts in a test set to the CLUTO toolkit (Karypis, 2003), that clusters them into n groups, where n is the number of categories. We use standard CLUT</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="12063" citStr="Socher et al. (2012)" startWordPosition="1933" endWordPosition="1936">us (more precisely, it only requires the constituent structure assigned by the parser, as it is blind to syntactic labels). Both large unannotated corpora and efficient pre-trained parsers are available for many languages, making the CPHRASE knowledge demands feasible for practical purposes. There is no need to parse the sentences we want to build representations for at test time, since the component word vectors are simply added. The only parameters of the model are the word vectors; specifically, no extra parameters are needed for composition (composition models such as the one presented in Socher et al. (2012) require an extra parameter matrix for each word in the vocabulary, and even leaner models such as the one of Guevara (2010) must estimate a parameter matrix for each composition rule in the grammar). This makes C-PHRASE as simple as additive and multiplicative composition (Mitchell and 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Supervision Unlike many recent composition models (Kalchbrenner and B</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1631--1642</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="10826" citStr="Socher et al. (2013)" startWordPosition="1738" endWordPosition="1741">d to predict how the contexts of a word change based on its phrasal collocates (cup will have very different contexts in world cup vs. coffee cup ). At the same time, because the vectors are optimized based on their occurrence in phrases of different syntactic complexity, they produce good sentence representations when they are combined. To the best of our knowledge, C-PHRASE is the first model that is jointly optimized for lexical and compositional tasks. C-BOW uses shallow composition information to learn word vectors. Conversely, some compositional models –e.g., Kalchbrenner et al. (2014), Socher et al. (2013)– induce word representations, that are only optimized for a compositional task and are not tested at the lexical level. Somewhat relatedly to what we do, Hill et al. (2014) evaluated representations learned in a sentence translation task on wordlevel benchmarks. Some a priori justification for treating word and sentence learning as joint problems comes from human language acquisition, as it is obvious that children learn word and phrase meanings in parallel and interactively, not sequentially (Tomasello, 2003). Knowledge-leanness and simplicity For training, C-PHRASE requires a large, syntact</context>
<context position="12744" citStr="Socher et al., 2013" startWordPosition="2047" endWordPosition="2050">lary, and even leaner models such as the one of Guevara (2010) must estimate a parameter matrix for each composition rule in the grammar). This makes C-PHRASE as simple as additive and multiplicative composition (Mitchell and 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Supervision Unlike many recent composition models (Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Socher et al., 2012; Socher et al., 2013, among others), the context-prediction objective of C-PHRASE does not require annotated data, and it is meant to provide generalpurpose representations that can serve in different tasks. C-PHRASE vectors can also be used as initialization parameters for fully supervised, task-specific systems. Alternatively, the current unsupervised objective could be combined with task-specific supervised objectives to fine-tune CPHRASE to specific purposes. Sensitivity to syntactic structure During training, C-PHRASE is sensitive to syntactic structure. To cite an extreme example, boy flowers will be joined</context>
<context position="22989" citStr="Socher et al., 2013" startWordPosition="3705" endWordPosition="3708">ilarity between the two sentence vectors, as given by the models, and whether the sentence pair contains a negation word (the latter has been shown to be a very informative feature for SICK entailment). The current state-of-the-art is reached by Lai and Hockenmaier (2014), using a much richer set of features, that include WordNet, the denotation graph of Young et al. (2014) and extra training data from other resources. Sentiment analysis Finally, as sentiment analysis has emerged as a popular area of application for compositional models, we test our methods on the Stanford Sentiment Treebank (Socher et al., 2013) (sst), consisting of 11,855 sentences from movie reviews, using the coarse annotation into 2 sentiment degrees (negative/positive). We follow the official split into train (8,544), development (1,101) and test (2,210) parts. We train an SVM classifier on the training set, using the sentence vectors composed by a model as features, and report accuracy on the test set. State of the art is obtained by Le and Mikolov (2014) with the Paragraph Vector approach we describe below. 3.2 Model implementation The source corpus we use to build the lexical vectors is created by concatenating three sources:</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, pages 1631–1642, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>Constructing a Language: A Usage-Based Theory of Language Acquisition.</title>
<date>2003</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11342" citStr="Tomasello, 2003" startWordPosition="1821" endWordPosition="1822">ctors. Conversely, some compositional models –e.g., Kalchbrenner et al. (2014), Socher et al. (2013)– induce word representations, that are only optimized for a compositional task and are not tested at the lexical level. Somewhat relatedly to what we do, Hill et al. (2014) evaluated representations learned in a sentence translation task on wordlevel benchmarks. Some a priori justification for treating word and sentence learning as joint problems comes from human language acquisition, as it is obvious that children learn word and phrase meanings in parallel and interactively, not sequentially (Tomasello, 2003). Knowledge-leanness and simplicity For training, C-PHRASE requires a large, syntacticallyparsed corpus (more precisely, it only requires the constituent structure assigned by the parser, as it is blind to syntactic labels). Both large unannotated corpora and efficient pre-trained parsers are available for many languages, making the CPHRASE knowledge demands feasible for practical purposes. There is no need to parse the sentences we want to build representations for at test time, since the component word vectors are simply added. The only parameters of the model are the word vectors; specifica</context>
</contexts>
<marker>Tomasello, 2003</marker>
<rawString>Michael Tomasello. 2003. Constructing a Language: A Usage-Based Theory of Language Acquisition. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1158" citStr="Turney and Pantel, 2010" startWordPosition="153" endWordPosition="157">-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models. 1 Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional seman1We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--67</pages>
<contexts>
<context position="22745" citStr="Young et al. (2014)" startWordPosition="3667" endWordPosition="3670">horse is far from a red barrel at the rodeo”) or NEUTRAL (“A man in a black jacket is doing tricks on a motorbike”/”A person is riding the bicycle on one wheel”). For each model, we train a simple SVM classifier based on 2 features: cosine similarity between the two sentence vectors, as given by the models, and whether the sentence pair contains a negation word (the latter has been shown to be a very informative feature for SICK entailment). The current state-of-the-art is reached by Lai and Hockenmaier (2014), using a much richer set of features, that include WordNet, the denotation graph of Young et al. (2014) and extra training data from other resources. Sentiment analysis Finally, as sentiment analysis has emerged as a popular area of application for compositional models, we test our methods on the Stanford Sentiment Treebank (Socher et al., 2013) (sst), consisting of 11,855 sentences from movie reviews, using the coarse annotation into 2 sentiment degrees (negative/positive). We follow the official split into train (8,544), development (1,101) and test (2,210) parts. We train an SVM classifier on the training set, using the sentence vectors composed by a model as features, and report accuracy on</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Tiantian Zhu</author>
<author>Man Lan</author>
</authors>
<title>Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>271--277</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="20531" citStr="Zhao et al. (2014)" startWordPosition="3298" endWordPosition="3301">data-set, tend to be near-paraphrases). Following previous work on these data sets, Pearson correlation is our figure of merit, and we report it between human scores and sentence vector cosine similarities computed by the models. SICK (Marelli et al., 2014) (sick-r) was created specifically for the purpose of evaluating compositional models, focusing on linguistic phenomena such as lexical variation and word order. Here we report performance of the systems on the test part of the data set, which contains 5K sentence pairs. The top performance (from the SICK SemEval shared task) was reached by Zhao et al. (2014) using a heterogeneous set of features that include WordNet and extra training corpora. Agirre et al. (2012) and Agirre et al. (2013) created two collections of sentential similarities consisting of subsets coming from different sources. From these, we pick the Microsoft Research video description dataset (msrvid), where near paraphrases are descriptions of the same short video, and the OnWN 2012 (onwn1) and OnWN 2013 (onwn2) data sets (each of these sets contains 750 pairs). The latter are quite different from other sentence relatedness benchmarks, since they compare definitions for the same </context>
</contexts>
<marker>Zhao, Zhu, Lan, 2014</marker>
<rawString>Jiang Zhao, Tiantian Zhu, and Man Lan. 2014. Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>