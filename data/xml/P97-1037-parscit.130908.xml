<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9979325">
A DP based Search Using Monotone
Alignments in Statistical Translation
</title>
<author confidence="0.737454">
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga
</author>
<affiliation confidence="0.629674">
Lehrstuhl fiir Informatik VI, RWTH Aachen
</affiliation>
<address confidence="0.45347">
D-52056 Aachen, Germany
</address>
<email confidence="0.754821">
itillmann,neylOinformatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.990045" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961086956522">
In this paper, we describe a Dynamic Pro-
gramming (DP) based search algorithm
for statistical translation and present ex-
perimental results. The statistical trans-
lation uses two sources of information: a
translation model and a language mod-
el. The language model used is a stan-
dard bigram model. For the transla-
tion model, the alignment probabilities are
made dependent on the differences in the
alignment positions rather than on the
absolute positions. Thus, the approach
amounts to a first-order Hidden Markov
model (HMM) as they are used. successful-
ly in speech recognition for the time align-
ment problem. Under the assumption that
the alignment is monotone with respect to
the word order in both languages, an ef-
ficient search strategy for translation can
be formulated. The details of the search
algorithm are described. Experiments on
the EuTrans corpus produced a word error
rate of 5.1%.
</bodyText>
<sectionHeader confidence="0.993211" genericHeader="method">
1 Overview: The Statistical
Approach to Translation
</sectionHeader>
<bodyText confidence="0.98989125">
The goal is the translation of a text given in some
source language into a target language. We are given
a source (*French&apos;) string = which
is to be translated into a target (&apos;English&apos;) string
= Among all possible target strings,
we will choose the one with the highest probability
which is given by Hayes&apos; decision rule (Brown et al..
1993):
</bodyText>
<listItem confidence="0.99930425">
• argmax {Pr(eldfil )}
- 1
e
• argmax {Pi&apos;() • Pr( fi/ Id))
</listItem>
<bodyText confidence="0.9667165">
Pr(e) is the language model of the target. language,
whereas Pr( ) is the string translation model.
The argmax operation denotes the search problem.
In this paper, we address
</bodyText>
<listItem confidence="0.87028325">
• the problem of introducing structures into the
probabilistic dependencies in order to model
the string translation probability Pr(ff
• the search procedure. i.e. an algorithm to per-
form the argmax operation in an efficient way.
• transformation steps for both the source and
the target. languages in order to improve the
translation process.
</listItem>
<bodyText confidence="0.9999324">
The transformations are very much dependent on
the language pair and the specific translation task
and are therefore discussed in the context of the task
description. We have to keep in mind that in the
search procedure both the language and the transla-
tion model are applied after the text transformation
steps. However, to keep the notation simple we will
not make this explicit distinction in the subsequent
exposition. The overall architecture of the statistical
translation approach is summarized in Figure 1.
</bodyText>
<sectionHeader confidence="0.994598" genericHeader="method">
2 Alignment Models
</sectionHeader>
<bodyText confidence="0.999127105263158">
A key issue in modeling the string translation prob-
ability Pr(fil is the question of how we define
the correspondence between the words of the target.
sentence and the words of the source sentence. In
typical cases, we can assume a sort of pairwise de-
pendence by considering all word pairs (fi, ei) for
a given sentence pair [fe; ef]. We further constrain
this model by assigning each source word to exact-
ly one target word. Models describing these types
of dependencies are referred to as alignment models
(Brown et al., 1993), (Dagan et al.. 1993). (Kay k
ROscheisen, 1993). (Fung k Church. 1994), (Vogel
et al., 1996).
In this section, we introduce a monotone HMAI
based alignment and an associated DP based search
algorithm for translation. Another approach to sta-
tistical machine translation using DP was presented
in (Wu, 1996). The notational convention will be as
follows. We use the symbol Pr(.) to denote general
</bodyText>
<page confidence="0.9959">
289
</page>
<figure confidence="0.906511166666667">
Source Language Text
1
Transformation
Transformation
1
Target Language Text
</figure>
<figureCaption confidence="0.9893655">
Figure I: Architecture of the translation approach
based on Hayes decision rule.
</figureCaption>
<bodyText confidence="0.934857333333333">
probability distributions with (nearly) no specific as-
sumptions. In contrast for model-based probability
distributions, we use the generic symbol p(.).
</bodyText>
<subsectionHeader confidence="0.999714">
2.1 Alignment with HMM
</subsectionHeader>
<bodyText confidence="0.999879846153846">
When aligning the words in parallel texts (for
Indo-European language pairs like Spanish-English,
German-English, Italian-German,...), we typically
observe a strong localization effect. Figure 2 illus-
trates this effect for the language pair Spanish-bo-
English . In many cases, although not always, there
is an even stronger restriction: the difference in the
position index is smaller than 3 and the alignment
is essentially monotone. To be more precise, the
sentences can be partitioned into a small number
of segments. within each of which the alignment is
monotone with respect to word order in both lan-
pages.
To describe these word-by-word alignments, we
introduce the mapping j — aj, which assigns a. po-
sition j (with source word fj ) to the position i = aj
(with target word fi). The concept of these align-
ments is similar to the ones introduced by (Brown
et al.. 1993), but we will use another type of de-
pendence in the probability distributions. Looking
at such alignments produced by a human expert. it
is evident that the mathematical model should try
to capture the strong dependence of ai on the pre-
ceding alignment aj _ Therefore the probability of
alignment aj for position j should have a dependence
on the previous alignment position aj_i:
</bodyText>
<equation confidence="0.851542">
p(u j j _1)
</equation>
<bodyText confidence="0.992507777777778">
A similar approach ha.s been chosen by (Da.gan et
al., 1993) and (Vogel et al.. 1996). Thus the problem
formulation is similar to that of the time alignment
problem in speech recognition, where the so-called
Hidden Markov models have been successfully used
for a long time (Jelinek, 1976). Using the same basic
principles, we can rewrite the probability by intro-
ducing the &apos;hidden&apos; alignments a.1 := ...aj for
a sentence pair [fi&apos;; ef]:
</bodyText>
<equation confidence="0.99487325">
Pr( fiJ = a(eli )
oil
E(l7 j1
Prui
</equation>
<bodyText confidence="0.9907945">
To avoid any confusion with the term &apos;hidden&apos; in
comparison with speech recognition. we observe that
the model states as such (representing words) are it ol
hidden but. the actual alignments, i.e. the sequence
of position index pairs (j. i = ai ).
So far there has been no basic restriction of the
approach. We now assume a first-order dependence
on the alignments aj only:
</bodyText>
<equation confidence="0.7317575">
Pr(fi, ) = p( fi , a jlai ei,)
= Aajlai-d .P(.6
</equation>
<bodyText confidence="0.942424105263158">
where, in addition, we have assumed that the lexicon
probability p(fle) depends only on ai and not on
aii.-
To reduce the number of alignment parameters,
we assume that the HMM alignment. probabilities
Mil/1) depend only on the jump width (i - 1&apos;). The
monotony condition can than be formulated as:
= 0 for 11 + 0.11 + 1. -1-2.
This monotony requirement limits the applicabili-
ty of our approach. However, by performing simple
word reorderings, it is possible to approach this re-
quirement (see Section 4.2). Additional countermea-
sures will be discussed later. Figure 3 gives an illus-
tration of the possible alignments for the monotone
hidden Markov model. To draw the analogy with
speech recognition. we have to identify the states
(along the vertical axis) with the positions 1 of the
target words ci and the time (along the horizontal
axis) with the positions j of the source words fj .
</bodyText>
<subsectionHeader confidence="0.999336">
2.2 Training
</subsectionHeader>
<bodyText confidence="0.999752">
To train the alignment and the lexicon model, we
use the maximum likelihood criterion in the so-called
maximum approximation. i.e. the likelihood criteri-
on covers only the most likely alignment rather than
the set of all alignments:
</bodyText>
<figure confidence="0.939726788461538">
Pr( le f )
Global Search:
maximize Pr(e). j, I e
over e
Lexicon Model
Pr(V:le&apos;t)
Pr( e1)
Alignment Model
Language Model
PI)
- a • - •
Iaj il) • M.fil a;)]
—1,1) • p(filca:,)] •
290
days • o o room&apos;.
two o the I.
for in Io
room coldI.
double too I.
a is I.
is it I.
much
how
cvuhdpdd e lhhdf
uanaoao naaaer
&apos; labbrsi b c m &apos;
a e i a 1 a a a ieai s o
t e a
night
a
for
tv
a
and
safe o o
a
telephone
a
with
room
a
booked
have
we
t r uhc t c f y t pun
e en a o e a u e an o
n s e a I bnl &apos; j a e 1 r a a c
e m r v a r e
o a
s
I
a
</figure>
<figureCaption confidence="0.995808">
Figure 2: Word alignments for Spanish-English sentence pairs.
</figureCaption>
<page confidence="0.942451">
291
</page>
<equation confidence="0.5673792">
the maximum approximation:
max II p(6i jei_i) max
eli:=1
H [P(a.i lai- 1)P(.6 lea, )1 }
j=1
</equation>
<bodyText confidence="0.9833605">
Here and in the following, we omit a special treat-
ment of the start and end conditions like j = 1 or
j = J in order to simplify the presentation and avoid
confusing details. Having the above criterion in
mind, we try to associate the language model prob-
abilities with the alignments j ai. To this
purpose, we exploit the monotony property of our
alignment model which allows only transitions from
aj...1 to ai if the difference 6 oi - aj_i is 0,1,2.
We define a modified probability pA(ele&apos;) for the lan-
guage model depending on the alignment difference
6. We consider each of the three cases 6 = 0,1.2
separately:
• 6 = 0 (horizontal transition = alignment repe-
tition): This case corresponds to a target word
with two or more aligned source words and
therefore requires c = c&apos; so that there is no
contribution from the language model:
</bodyText>
<equation confidence="0.950963">
1 1 for c = e&apos;
p6=0 (del) = 0 for e e&apos;
</equation>
<listItem confidence="0.98178575">
• 6 = 1 (forward transition = regular alignment):
This case is the regular one, and we can use
directly the probability of the bigram language
model:
</listItem>
<equation confidence="0.564583">
P6=1 (de&apos;) = P(eici.)
</equation>
<bodyText confidence="0.807653">
• 6 = 2 (skip transition = non-aligned word):
This case corresponds to skipping a word. i.e,
there is a word in the target. string with no
aligned word in the source string. We have to
find the highest probability of placing a non-
aligned word 7: between a predecessor word c&apos;
and a successor word c . Thus we optimize the
following product over the non-aligned word
</bodyText>
<equation confidence="0.98374">
P6=2 (elf&apos;) = max [p(c le) •
z
</equation>
<bodyText confidence="0.99559275">
This maximization is done beforehand and the
result is stored in a table.
Using this modified probability p(e)e&apos;), we can
rewrite the overall search criterion:
</bodyText>
<equation confidence="0.604272">
max H )P( .f 1_.„)] .
j=1
</equation>
<bodyText confidence="0.9944314">
The problem now is to find the unknown mapping:
j — (ai, c )
which defines a path through a network with a uni-
form trellis structure. For this trellis, we can still
use Figure 3. However, in each position i along the
</bodyText>
<figure confidence="0.67143475">
TARGET POSITION
1 1 1 1
1 3 4 5 6
SOURCE POSITION
</figure>
<figureCaption confidence="0.699071">
Figure 3: Illustration of alignments for the monotone
</figureCaption>
<table confidence="0.846992">
HMM.
To find the optimal alignment, we use dynamic
programming for which we have the following typical
recursion formula:
Q(i, j) = ) • Q(i&apos; , - 1)]
</table>
<bodyText confidence="0.9390423125">
Here. Q(i. j) is a sort of partial probability as in time
alignment for speech recognition (Jelinek, 1976). As
a result the training procedure amounts to a se-
quence of iterations. each of which consists of two
steps:
• position alignmeni: Given the model parame-
ters, determine the most likely position align-
ment.
• parameter estimation: Given the position align-
ment i.e. going along the alignment paths for
all sentence pairs, perform maximum likelihood
estimation of the model parameters; for model-
free distributions, these estimates result. in rel-
ative frequencies.
The IBM model 1 (Brown et al., 1993) is used to find
an initial estimate of the translation probabilities.
</bodyText>
<sectionHeader confidence="0.808453" genericHeader="method">
3 Search Algorithm for Translation
</sectionHeader>
<bodyText confidence="0.9900315">
For the translation operation. we use a bigram lan-
guage model, which is given in terms of the con-
ditional probability of observing word ci given the
predecessor word
</bodyText>
<equation confidence="0.836942">
P(fi •
</equation>
<bodyText confidence="0.997805">
Using the conditional probability of the bigram lan-
guage model, we have the overall search criterion in
</bodyText>
<page confidence="0.996268">
292
</page>
<tableCaption confidence="0.999807">
Table 1: DP based search algorithm for the monotone translation model.
</tableCaption>
<table confidence="0.9987113">
input: source string h ...fi —Jr
initialization
for each position j = 1, 2, ....J in source sentence do
for each position i = 1,2, ..., /„,,, in target sentence do
for each target word e do
Q(i, j,e) = p(fjle) • max{p(ili - 6) - pb(ele&apos;) • Q(i - li.j - 1, e&apos;)}
tra.ceback:
- find best end hypothesis: max Q(i, J, e)
t,E
- recover optimal word sequence
</table>
<bodyText confidence="0.998188818181818">
vertical axis, we have to allow all possible words c
of the target vocabulary. Due to the monotony of
our alignment model and the bigram language mod-
el. we have only first-order type dependencies such
that the local probabilities (or costs when using the
negative logarithms of the probabilities) depend on-
ly on the arcs (or transitions) in the lattice. Each
possible index triple (i. j. e) defines a grid point in
the lattice. and we have the following set of possi-
ble transitions from one grid point to another grid
point:
</bodyText>
<equation confidence="0.626934">
6 E {0.1.2) : (i- 6. j - 1.f&apos;) — (i, j,e) .
Each of these transitions is assigned a local proba-
bility:
— 6) • MEW) • P(.61e) •
</equation>
<bodyText confidence="0.999853666666667">
Using this formulation of the search task, we can
now use the method of dynamic programming (DP)
to find the best path through the lattice. To this
purpose. we introduce the auxiliary quantity:
Q(i. j. e): probability of the best partial path
which ends in the grid point (i, j, c).
Since we have only first-order dependencies in our
model, it is easy to see that the auxiliary quantity
must. satisfy the following DP recursion equation:
</bodyText>
<equation confidence="0.579205">
f = ).
max {p(i.li - 6) maxpA(clei) • Q(i - 6, j -1,e&apos; )1
</equation>
<bodyText confidence="0.999589285714286">
To explicitly construct the unknown word sequence
. it is convenient to make use of so-called back-
pointers which store for each grid point. (i. j. e) the
best predecessor grid point (Ney et. al., 1992).
The DP equation is evaluated recursively to find
the best partial path to each grid point. (i, j, c). The
resulting algorithm is depicted in Table 1. The com-
plexity of the algorithm is J /mar • E2. where E is
the size of the target language vocabulary and Imaj.
is the maximum lengni of the target sentence con-
sidered. It is possible to reduce this computational
complexity by using so-called pruning methods (Ney
et al.. 1992): due to space limitations, they are not
discussed here.
</bodyText>
<sectionHeader confidence="0.99704" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.997011">
4.1 The Task and the Corpus
</subsectionHeader>
<bodyText confidence="0.99529752">
The search algorithm proposed in this paper was
tested on a subtask of the **Traveler Task- (Vida,
1997). The general domain of the task comprises
typical situations a visitor to a foreign country is
faced with. The chosen subtask corresponds to a sce-
nario of the human-to-human communication situ-
ations at the registration desk in a hotel (see Table
4).
The corpus was generated in a semi-automatic
way. On the basis of examples from traveller book-
lets, a probabilistic grammar for different language
pairs has been constructed from which a large cor-
pus of sentence pairs was generated. The vocabulary
consisted of 692 Spanish and 518 English words (in-
cluding punctuation marks). For the experiments, a.
training corpus of 80,000 sentence pairs with 628,117
Spanish and 684.777 English words was used. In ad-
dition. a test corpus with 2.730 sentence pairs differ-
ent from the training sentence pairs was construct-
ed. This test corpus contained 28.642 Spanish and
24,927 English words. For the English sentences,
we used a bigram language model whose perplexity
on the test corpus varied between 4.7 for the orig-
inal text and 3.5 when all transformation steps as
described below had been applied.
</bodyText>
<tableCaption confidence="0.979027">
Table 2: Effect of the transformation steps on the
vocabulary sizes in both languages.
</tableCaption>
<table confidence="0.996815">
Transformation Step Spanish English
Original (with punctuation) 692 518
+ Categorization 416 227
+ &apos;130r...favor&apos; 417 -
+ Word Splitting 374 -
+ Word Joining - 237
+ Word Reordering -
</table>
<page confidence="0.995427">
293
</page>
<subsectionHeader confidence="0.963649">
4.2 Text Transformations
</subsectionHeader>
<bodyText confidence="0.9999355">
The purpose of the text transformations is to make
the two languages resemble each other as closely as
possible with respect to sentence length and word or-
der. In addition, the size of both vocabularies is re-
duced by exploiting evident regularities; e.g. proper
names and numbers are replaced by category mark-
ers. We used different preprocessing steps which
were applied consecutively:
</bodyText>
<listItem confidence="0.956330666666667">
• Original Corpus: Punctuation marks are
treated like regular words.
• Categorization: Some particular words or
word groups are replaced by word categories.
Seven non-overlapping categories are used:
three categories for names (surnames, male and
female names). two categories for numbers (reg-
ular numbers and room numbers) and two cat-
egories for date and time of day.
• Treatment of &apos;por favor&apos;: The word &apos;por
favor is always moved to the end of the
sentence and replaced by the one-word token
&apos;1)0/.4&apos; attor
• Word Splitting: In Spanish, the personal
pronouns (in subject case and in object case)
can be part of the inflected verb form. To coun-
teract this phenomenon, we split the verb into
a verb part and pronoun part, such as &apos;darnos&apos;
— .dar _710.S. and &apos;pienso. — `_yo pienso&apos;.
• Word Joining: Phrases in the English lan-
guage such as &apos;Would you mind doing ...&apos; and
</listItem>
<bodyText confidence="0.84314225">
&apos;I would like you to do are difficult to ha.n-
dle by our alignment model. Therefore, we
apply some word joining, such as &apos;would you
mind&apos; — &amp;quot;would_you_mind. and &apos;would like &apos;
</bodyText>
<listItem confidence="0.69645825">
• Word Reordering: This step is applied to
the Spanish text to take into account. cases like
the position of the adjective in noun-adjective
phrases and the position of object pronouns.
E.g. &apos;habitaciOn doble&apos; — &apos;doble habitacion&apos;.
By this reordering, our assumption about the
monotony of the alignment model is more often
satisfied.
</listItem>
<bodyText confidence="0.9999364">
The effect of these transformation steps on the sizes
of both vocabularies is shown in Table 2. In addi-
tion to all preprocessing steps, we removed the punc-
tuation marks before translation and resubstituted
them by rule into the target sentence.
</bodyText>
<subsectionHeader confidence="0.998381">
4.3 Translation Results
</subsectionHeader>
<bodyText confidence="0.999954076923077">
For each of the transformation steps described
above, all probability models were trained anew, i.e.
the lexicon probabilities p(fle), the alignment prob-
abilities p(ili — 6) and the bigra.m language proba-
bilities p(eW). To produce the translated sentence
in normal language, the transformation steps in the
target language were inverted.
The translation results are summarized in Table
3. As an automatic and easy-to-use measure of the
translation errors, the Levenshtein distance between
the automatic translation and the reference transla-
tion was calculated. Errors are reported at the word
level and at the sentence level:
</bodyText>
<listItem confidence="0.99005775">
• word level: insertions (INS). deletions (DEL),
and total number of word errors (WER).
• sentence level: a sentence is counted as correct
only if it is identical to the reference sentence.
</listItem>
<bodyText confidence="0.9959835">
Admittedly, this is not a perfect measure. In par-
ticular, the effect. of word ordering is not taken into
account appropriately. Actually, the figures for sen-
tence error rate are overly pessimistic. Many sen-
tences are acceptable and semantically correct trans-
lations (see the example translations in Table 4).
</bodyText>
<tableCaption confidence="0.863981333333333">
Table 3: Word error rates (INS/DEL, WER) and
sentence error rates (SER) for different. transforma-
tion steps.
</tableCaption>
<table confidence="0.999920125">
Transformation Step Translation Errors [%]
INS/DEL WER SER
Original Corpora 4.3/11.2 21.2 85.5
+ Categorization 2.5/9.6 16.1 81.0
+ Ipor_favor&apos; 2.6/8.3 14.3 75.6
+ Word Splitting 2.5/7.4 12.3 65.4
+ Word Joining 1.3/4.9 7.3 44.6
+ Word Reordering 0.9/3.4 5.1 30.1
</table>
<bodyText confidence="0.997311166666667">
As can be seen in Table 3. the translation er-
rors can be reduced systematically by applying all
transformation steps. The word error rate is re-
duced from 21.27( to 5.17: the sentence error rate
is reduced from 85.5% to 30.1%. The two most im-
portant transformation steps are categorization and
word joining. What is striking, is the large fraction
of deletion errors. These deletion errors are often
caused by the omission of word groups like &apos;for in
please&apos; and &apos;could you&apos;. Table 4 shows some example
translations (for the best translation results). It can
be seen that the semantic meaning of the sentence in
the source language may be preserved even if there
are three word errors according to our performance
criterion. To study the dependence on the amount
of training data, we also performed a training with
only 5 000 sentences out of the training corpus. For
this training condition. the word error rate went up
only slightly, namely from 5.1% (for 80,000 training
sentences) to 5.3% (for 5 000 training sentences).
To study the effect of the language model. we test-
ed a zerogram. a unigram and a bigram language
model using the standard set of 80 000 training sen-
tences. The results are shown in Table 5. The
</bodyText>
<page confidence="0.998624">
294
</page>
<tableCaption confidence="0.966705">
Table 4: Examples from the EuTra.ns task: 0= original sentence, R= reference translation. A= automatic
translation.
</tableCaption>
<table confidence="0.991213666666667">
0: He hecho la reserva de una habitacion con television y telefono a nombre del senor Morales.
R: I have made a reservation for a room with TV and telephone for Mr. Morales.
A: I have made a reservation for a room with TV and telephone for Mr. Morales.
0: Stibanme las maletas a mi habitaciOn, por favor.
R: Send up my suitcases 1.0 my room. please.
A: Send up my suitcases to my room, please.
0: Por favor, querria que nos diese las Haves de la habitaciOn.
R: I would like you to give us the keys to the room, please.
A: I would like you to give us the keys to the room, please.
0: Por favor, me pide mi taxi para la habitaciOn tres veintidOs?
R: Could you ask for my taxi for room number three two two for me. please?
A: Could you ask for my taxi for room number three two two, please?
0: Por favor. reservamos dos habitaciones dobles con cuarto de balm.
R: We booked two double rooms with a bathroom.
A: We booked two double rooms with a bathroom. please.
0: Quisiera que nos despertaran manana a las dos y cuarto. por favor.
R: I would like you to wake us up tomorrow at. a quarter past two. please.
A: I want you to wake us up tomorrow at a quarter past two. please.
0: Repaseme la cuenta de la habitaciOn ochocientos veintiuno.
R: Could you check the bill for room number eight two one for me, please?
A: Check the bill for room number eight two one.
</table>
<bodyText confidence="0.9948038">
WER decreases from 31.1% for the zerogra.m model
to 5.1% for the bigram model.
The results presented here can be compared with
the results obtained by the finite-state transducer
approach described in (Vidal, 1996: Vidal, 1997),
where the same training and test. conditions were
used. However the only preprocessing step was cat-
egorization. In that work. a WER of 7.1% was ob-
tained as opposed to 5.17 presented in this paper.
For smaller amounts of training data (say 5 000 sen-
tence pairs). the DP based search seems to be even
more superior.
Table 5: Language model perplexity (PP), word er-
ror rates (INS/DEL. WER) and sentence error rates
(SER) for different language models.
</bodyText>
<table confidence="0.8009854">
Translation Errors [%]
PP INS/DEL WER SER
Zerogram 237.0 0.6/18.6 31.1 98.1
Unigram 74.4 0.9/12.4 20.4 94.8
Bigram 4.1 0.9/3.4 5.1 30.1
</table>
<subsectionHeader confidence="0.980431">
4.4 Effect of the Word Reordering
</subsectionHeader>
<bodyText confidence="0.999851285714286">
In more general cases and applications, there will
always be sentence pairs with word alignments for
which the monotony constraint, is not satisfied. How-
ever even then, the monotony constraint, is satisfied
locally for the lion&apos;s share of all word alignments in
such sentences. Therefore. we expect to extend the
approach presented by the following methods:
</bodyText>
<listItem confidence="0.9901722">
• more systematic approaches to local and global
word reorderings that try to produce the same
word order in both languages.
• a multli-level approach that. allows a small (say
4) number of large forward and backward tran-
</listItem>
<bodyText confidence="0.998694142857143">
sitions. Within each level, the monotone align-
ment model can still be applied, and only when
moving from one level to the next, we have to
handle the problem of different, word orders.
To show the usefulness of global word reorder-
ing. we changed the word order of some sentences
by hand. Table 6 shows the effect. of the global re-
ordering for two sentences. In the first example, we
changed the order of two groups of consecutive words
and placed an additional copy of the Spanish word
&amp;quot;cuesta- into the source sentence. In the second
example, the personal pronoun -me- was placed at
the end of the source sentence. In both cases, we
obtained a correct translation.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9550645">
In this paper, we have presented an HMM based ap-
proach to handling word alignments and an associat-
ed search algorithm for automatic translation. The
characteristic feature of this approach is to make the
alignment probabilities explicitly dependent on the
alignment, position of the previous word and to as-
sume a monotony constraint for the word order in
both languages. Due to this monotony constraint.
we are able to apply an efficient DP based search al-
gorithm. We have tested the model successfully On
the EuTrans traveller task, a limited domain task
with a vocabulary of 200 to 500 words. The result-
Language
Model
</bodyText>
<page confidence="0.996308">
295
</page>
<tableCaption confidence="0.9940295">
Table 6: Effect of the global word reordering: 0= original sentence, R= reference translation, A= automatic
translation, 0&apos;= original sentence reordered, A&apos;= automatic translation after reordering.
</tableCaption>
<table confidence="0.854418333333333">
0: Cuanto cuesta una habitaciOn doble para cinco noches incluyendo servicio de habitaciones ?
R: How much does a double room including room service cost for five nights ?
A: How much does a double room including room service ?
0&apos;: Cuanto cuesta una habitaciOn doble incluyendo servicio de habitaciones cuesta para cinco noches ?
A&apos;: How much does a double room including room service cost for five nights ?
: • Explique _me la factura de la habitaciOn tres dos cuatro.
</table>
<bodyText confidence="0.933295">
R: Explain the bill for room number three two four for me.
A: Explain the bill for room number three two four.
O.: Explique la factura de la habitaciOn tres dos cuatro _me.
N.• Explain the bill for room number three two four for me.
ing word error rate was only 5.1(X. To mitigate the
monotony constraint, we plan to reorder the words
in the source sentences to produce the same word
order in both languages.
</bodyText>
<sectionHeader confidence="0.99316" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.995571833333333">
This work has been supported partly by the Ger-
man Federal Ministry of Education. Science, Re-
search and Technology under the contract. number
01 IV 601 A (Verbmobil) and by the European Com-
munity under the ESPRIT project number 20268
(EliTrans).
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999133125">
A. L. Berger. P. F. Brown. S. A. Della Pietra. V. J.
Della Pietra. J. R. Gillett. J. D. Lafferty. R. L.
Mercer. H. Printz. and L. Ures. 1994. &amp;quot;The Can-
elide System for Machine Translation- . In Proc. of
ARPA Human Language Technology Workshop.
pp. 152-157. Plainsboro. NJ. Morgan Kaufmann
Publishers. San Mateo. CA. March.
P. F. Brown, V. J. Della Pietra. S. A. Della Pietra,
and R. L. Mercer. 1993. -The Mathematics of
Statistical Machine Translation: Parameter Esti-
mation-. Computational Linguistics. Vol. 19. No.
2. pp. 263-311.
I. Dagan. E. IV. Church. and NV. A. Gale. 1993.
-Robust Bilingual Word Alignment for Machine
Aided Translation-. In Proc. of the Workshop on
Very Large Corpora. pp. 1-8. Columbus, OH.
P. Fung. and K. W. Church. 1994. &amp;quot;K-vec: A New
Approach for Aligning Parallel Texts&amp;quot;, In Proc. of
the 15th Int. Conf. on Computational Linguistics,
pp. 1096-1102. Kyoto.
F. Jelinek. 1976. &amp;quot;Speech Recognition by Statistical
Methods&amp;quot;. Proc. of the IEEE. Vol. 64. pp. 532-
556. April.
M. Kay. and M. Roscheisen. 1993. &amp;quot;Text-
Translation Alignment- . Computational Linguis-
tics. Vol. 19. No. 2. pp. 121-142.
H. Ney, D. Merge!, A. Noll, A. Paeseler. 1992. &amp;quot;Da-
ta Driven Search Organization for Continuous
Speech Recognition&amp;quot;. IEEE Trans. on Signal Pro-
cessing, Vol. SP-40. No. 2. pp. 272-281. February.
E. Vidal. 1996. &amp;quot;Final report of Esprit Research
Project 20268 (EuTrans): Example-Based Under-
standing and Translation Systems&amp;quot;. Universidad
Politecnica de Valencia, Instituto TecnolOgio de
Informatica, October.
E. Vidal. 1997. &amp;quot;Finite-State Speech-to-Speech
Translation&amp;quot;. In Proc. of the Int. Conf. on Acous-
tics. Speech and Signal Processing. Munich. April.
S. Vogel, H. Ney, and C. Tillmann. 1996. -HAIM
Based Word Alignment in Statistical Transla-
tion&amp;quot;. In Proc. of the 1616 Int. Conf. on Com-
putational Linguistics. pp. 836-841. Copenhagen,
August.
. Wu. 1996. &amp;quot;A Polynomial-Time Algorithm for
Statistical Machine Translation&amp;quot;. In Proc. of the
341h Annual Conf. of the Association for Compu-
tational Linguistics, pp. 152-158. Santa Cruz, CA.
June.
</reference>
<page confidence="0.998557">
296
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9917835">A DP based Search Using Monotone Alignments in Statistical Translation</title>
<author confidence="0.999026">S Vogel Tillmann</author>
<author confidence="0.999026">A Zubiaga</author>
<affiliation confidence="0.983986">Lehrstuhl fiir Informatik VI, RWTH Aachen</affiliation>
<address confidence="0.998942">D-52056 Aachen, Germany</address>
<abstract confidence="0.957637108384459">In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results. The statistical translation uses two sources of information: a translation model and a language model. The language model used is a standard bigram model. For the translation model, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions. Thus, the approach amounts to a first-order Hidden Markov (HMM) as they are successfully in speech recognition for the time alignment problem. Under the assumption that the alignment is monotone with respect to the word order in both languages, an efficient search strategy for translation can be formulated. The details of the search algorithm are described. Experiments on the EuTrans corpus produced a word error rate of 5.1%. 1 Overview: The Statistical Approach to Translation The goal is the translation of a text given in some source language into a target language. We are given source (*French&apos;) string which is to be translated into a target (&apos;English&apos;) string = Among all possible target strings, we will choose the one with the highest probability which is given by Hayes&apos; decision rule (Brown et al.. 1993): argmax )} e argmax • Id)) is the language model of the target. language, is the string translation model. The argmax operation denotes the search problem. In this paper, we address • the problem of introducing structures into the probabilistic dependencies in order to model string translation probability • the search procedure. i.e. an algorithm to perform the argmax operation in an efficient way. • transformation steps for both the source and the target. languages in order to improve the translation process. The transformations are very much dependent on the language pair and the specific translation task and are therefore discussed in the context of the task description. We have to keep in mind that in the search procedure both the language and the translamodel are applied text transformation steps. However, to keep the notation simple we will not make this explicit distinction in the subsequent exposition. The overall architecture of the statistical translation approach is summarized in Figure 1. 2 Alignment Models key issue in modeling the string translation probthe question of how we define the correspondence between the words of the target. sentence and the words of the source sentence. In typical cases, we can assume a sort of pairwise deby considering all word pairs given sentence pair [fe; further constrain model by assigning each source word to exactone word. Models describing these types dependencies are referred to as models (Brown et al., 1993), (Dagan et al.. 1993). (Kay k ROscheisen, 1993). (Fung k Church. 1994), (Vogel et al., 1996). In this section, we introduce a monotone HMAI based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be as We use the symbol denote general 289 Source Language Text 1 Transformation Transformation 1 Target Language Text Figure I: Architecture of the translation approach based on Hayes decision rule. probability distributions with (nearly) no specific assumptions. In contrast for model-based probability distributions, we use the generic symbol p(.). 2.1 Alignment with HMM When aligning the words in parallel texts (for Indo-European language pairs like Spanish-English, German-English, Italian-German,...), we typically observe a strong localization effect. Figure 2 illusthis effect for the language pair Spanish-bo- . many cases, although not always, there is an even stronger restriction: the difference in the position index is smaller than 3 and the alignment is essentially monotone. To be more precise, the sentences can be partitioned into a small number of segments. within each of which the alignment is monotone with respect to word order in both lanpages. To describe these word-by-word alignments, we the mapping aj, which assigns a. posource word to the position = (with target word fi). The concept of these alignments is similar to the ones introduced by (Brown et al.. 1993), but we will use another type of dependence in the probability distributions. Looking at such alignments produced by a human expert. it is evident that the mathematical model should try capture the strong dependence of the prealignment aj the probability of position have a dependence the previous alignment position j _1) A similar approach ha.s been chosen by (Da.gan et al., 1993) and (Vogel et al.. 1996). Thus the problem formulation is similar to that of the time alignment problem in speech recognition, where the so-called Hidden Markov models have been successfully used for a long time (Jelinek, 1976). Using the same basic we can rewrite the probability by introthe &apos;hidden&apos; alignments := for sentence pair [fi&apos;; = ) j1 Prui avoid any confusion with the term comparison with speech recognition. we observe that model states as such (representing words) are but. the actual alignments, i.e. the position index pairs i = ). So far there has been no basic restriction of the approach. We now assume a first-order dependence on the alignments aj only: = fi , a jlai where, in addition, we have assumed that the lexicon only on and not on To reduce the number of alignment parameters, we assume that the HMM alignment. probabilities only on the jump width - 1&apos;). monotony condition can than be formulated as: for + + 1. This monotony requirement limits the applicability of our approach. However, by performing simple word reorderings, it is possible to approach this requirement (see Section 4.2). Additional countermeasures will be discussed later. Figure 3 gives an illustration of the possible alignments for the monotone hidden Markov model. To draw the analogy with speech recognition. we have to identify the states the vertical axis) with the positions the target words ci and the time (along the horizontal with the positions j of the source words 2.2 Training To train the alignment and the lexicon model, we use the maximum likelihood criterion in the so-called maximum approximation. i.e. the likelihood criterion covers only the most likely alignment rather than the set of all alignments: ) Global Search: Pr(e). I e over e Lexicon Model Alignment Model Language Model • - • • • 290 days two for room double a • o o o room&apos;. is much how in Io coldI. too I. is I. it I. cvuhdpdd e lhhdf uanaoao naaaer &apos; labbrsi b c m &apos; a e i a 1 a a a ieai s o t e a night a for tv a and safe o o a telephone a with room a booked have we t r uhc t c f y t pun e en a o e a u e an o n s e a I bnl a &apos; j a e 1 r a a c e m r v r e o a s I a Figure 2: Word alignments for Spanish-English sentence pairs. 291 the maximum approximation: IIp(6i max lai- )1 } j=1 Here and in the following, we omit a special treatof the start and end conditions like = or order to simplify the presentation and avoid confusing details. Having the above criterion in we try to associate the language model probwith the alignments this purpose, we exploit the monotony property of our alignment model which allows only transitions from to if the difference 6 aj_i 0,1,2. define a modified probability the language model depending on the alignment difference 6. We consider each of the three cases 6 = 0,1.2 separately: 6 = (horizontal transition = alignment repetition): This case corresponds to a target word with two or more aligned source words and requires c = that there is no contribution from the language model: for = for e&apos; 6 = (forward transition = regular alignment): This case is the regular one, and we can use directly the probability of the bigram language model: (de&apos;) = • 6 = 2 (skip transition = non-aligned word): This case corresponds to skipping a word. i.e, there is a word in the target. string with no aligned word in the source string. We have to find the highest probability of placing a nonaligned word 7: between a predecessor word c&apos; and a successor word c . Thus we optimize the following product over the non-aligned word (elf&apos;) = max • z This maximization is done beforehand and the result is stored in a table. this modified probability can rewrite the overall search criterion: .f . The problem now is to find the unknown mapping: c ) which defines a path through a network with a uniform trellis structure. For this trellis, we can still Figure 3. However, in each position the TARGET POSITION 1 1 1 1 3 6 SOURCE POSITION Figure 3: Illustration of HMM. alignments for the monotone To find the optimal programming for which recursion formula: alignment, we use dynamic we have the following typical j) = ) • Q(i&apos; , j) a sort of partial probability as in time alignment for speech recognition (Jelinek, 1976). As a result the training procedure amounts to a sequence of iterations. each of which consists of two steps: position alignmeni: the model parameters, determine the most likely position alignment. parameter estimation: the position alignment i.e. going along the alignment paths for all sentence pairs, perform maximum likelihood estimation of the model parameters; for modelfree distributions, these estimates result. in relative frequencies. The IBM model 1 (Brown et al., 1993) is used to find an initial estimate of the translation probabilities. 3 Search Algorithm for Translation For the translation operation. we use a bigram language model, which is given in terms of the conprobability of observing word given the predecessor word Using the conditional probability of the bigram language model, we have the overall search criterion in 292 Table 1: DP based search algorithm for the monotone translation model. source string h initialization for each 1, 2, source sentence do for each = ..., /„,,, in target sentence do for each word = li.j e&apos;)} tra.ceback: find best end hypothesis: max J, t,E recover optimal word sequence axis, we have to allow words c of the target vocabulary. Due to the monotony of our alignment model and the bigram language model. we have only first-order type dependencies such that the local probabilities (or costs when using the logarithms of the probabilities) depend onthe arcs (or transitions) in the lattice. Each index triple j.e) a grid point in the lattice. and we have the following set of possible transitions from one grid point to another grid point: E {0.1.2) : 6. j - 1.f&apos;) — (i, j,e) . Each of these transitions is assigned a local probability: • MEW) • Using this formulation of the search task, we can now use the method of dynamic programming (DP) to find the best path through the lattice. To this purpose. we introduce the auxiliary quantity: j.e): of the best partial path ends in the grid point j, c). Since we have only first-order dependencies in our model, it is easy to see that the auxiliary quantity must. satisfy the following DP recursion equation: f = ). 6) • - 6, j -1,e&apos; )1 To explicitly construct the unknown word sequence . it is convenient to make use of so-called backwhich store for each grid point. j. e) best predecessor grid point (Ney et. al., 1992). The DP equation is evaluated recursively to find best partial path to each grid point. j, The resulting algorithm is depicted in Table 1. The comof the algorithm is J • size of the target language vocabulary and maximum lengni of the target sentence considered. It is possible to reduce this computational complexity by using so-called pruning methods (Ney 1992): due to space limitations, they are not discussed here. Results 4.1 The Task and the Corpus The search algorithm proposed in this paper was on a subtask of the **Traveler (Vida, 1997). The general domain of the task comprises typical situations a visitor to a foreign country is faced with. The chosen subtask corresponds to a scenario of the human-to-human communication situations at the registration desk in a hotel (see Table 4). The corpus was generated in a semi-automatic way. On the basis of examples from traveller booklets, a probabilistic grammar for different language pairs has been constructed from which a large corpus of sentence pairs was generated. The vocabulary consisted of 692 Spanish and 518 English words (including punctuation marks). For the experiments, a. training corpus of 80,000 sentence pairs with 628,117 Spanish and 684.777 English words was used. In addition. a test corpus with 2.730 sentence pairs different from the training sentence pairs was constructed. This test corpus contained 28.642 Spanish and 24,927 English words. For the English sentences, we used a bigram language model whose perplexity on the test corpus varied between 4.7 for the original text and 3.5 when all transformation steps as described below had been applied. of the transformation steps on the vocabulary sizes in both languages. Transformation Step Spanish English Original (with punctuation) 692 518 + Categorization 416 227 417 - + Word Splitting 374 - + Word Joining - 237 + Word Reordering - 293 4.2 Text Transformations The purpose of the text transformations is to make the two languages resemble each other as closely as possible with respect to sentence length and word order. In addition, the size of both vocabularies is reduced by exploiting evident regularities; e.g. proper names and numbers are replaced by category markers. We used different preprocessing steps which were applied consecutively: Original Corpus: marks are treated like regular words. Categorization: particular words or word groups are replaced by word categories. Seven non-overlapping categories are used: three categories for names (surnames, male and female names). two categories for numbers (regular numbers and room numbers) and two categories for date and time of day. Treatment of &apos;por favor&apos;: word always moved to the end of the sentence and replaced by the one-word token Word Splitting: Spanish, the personal pronouns (in subject case and in object case) can be part of the inflected verb form. To counteract this phenomenon, we split the verb into verb part and pronoun part, such as .dar and — `_yo pienso&apos;. Word Joining: in the English lansuch as you mind doing ...&apos; would like you to do difficult to dle by our alignment model. Therefore, we some word joining, such as you — and like &apos; Word Reordering: step is applied to the Spanish text to take into account. cases like the position of the adjective in noun-adjective phrases and the position of object pronouns. doble&apos; — &apos;doble habitacion&apos;. By this reordering, our assumption about the monotony of the alignment model is more often satisfied. The effect of these transformation steps on the sizes of both vocabularies is shown in Table 2. In addition to all preprocessing steps, we removed the punctuation marks before translation and resubstituted them by rule into the target sentence. 4.3 Translation Results For each of the transformation steps described above, all probability models were trained anew, i.e. the lexicon probabilities p(fle), the alignment prob- — and the bigra.m language probabilities p(eW). To produce the translated sentence in normal language, the transformation steps in the target language were inverted. The translation results are summarized in Table 3. As an automatic and easy-to-use measure of the translation errors, the Levenshtein distance between the automatic translation and the reference translation was calculated. Errors are reported at the word level and at the sentence level: • word level: insertions (INS). deletions (DEL), and total number of word errors (WER). • sentence level: a sentence is counted as correct it is identical to the reference sentence. Admittedly, this is not a perfect measure. In particular, the effect. of word ordering is not taken into account appropriately. Actually, the figures for sentence error rate are overly pessimistic. Many sentences are acceptable and semantically correct translations (see the example translations in Table 4). 3: Word error rates (INS/DEL, sentence error rates (SER) for different. transformation steps. Transformation Step Translation Errors [%] INS/DEL WER SER Original Corpora 4.3/11.2 21.2 85.5 + Categorization 2.5/9.6 16.1 81.0 2.6/8.3 14.3 75.6 + Word Splitting 2.5/7.4 12.3 65.4 + Word Joining 1.3/4.9 7.3 44.6 + Word Reordering 0.9/3.4 5.1 30.1 As can be seen in Table 3. the translation errors can be reduced systematically by applying all transformation steps. The word error rate is reduced from 21.27( to 5.17: the sentence error rate is reduced from 85.5% to 30.1%. The two most important transformation steps are categorization and word joining. What is striking, is the large fraction of deletion errors. These deletion errors are often by the omission of word groups like in you&apos;. 4 shows some example translations (for the best translation results). It can be seen that the semantic meaning of the sentence in the source language may be preserved even if there are three word errors according to our performance criterion. To study the dependence on the amount of training data, we also performed a training with only 5 000 sentences out of the training corpus. For this training condition. the word error rate went up only slightly, namely from 5.1% (for 80,000 training sentences) to 5.3% (for 5 000 training sentences). To study the effect of the language model. we tested a zerogram. a unigram and a bigram language model using the standard set of 80 000 training sentences. The results are shown in Table 5. The 294 4: Examples from the EuTra.ns task: sentence, R= reference translation. A= automatic translation. 0: la reserva de una habitacion con television y telefono a nombre del senor Morales. have made a reservation a room with TV and telephone for Mr. Morales. I have made a reservation for a room with TV and telephone for Mr. Morales.</abstract>
<note confidence="0.89258625">R: A: 0: Stibanme las maletas a mi habitaciOn, por favor. Send up my suitcases 1.0 my room. please. Send up my suitcases to my room, please. R: A: 0: Por favor, querria que nos diese las Haves de la habitaciOn. I would like you to give us the keys to the room, please. I would like you to give us the keys to the room, please. R: A:</note>
<abstract confidence="0.955848417582418">0: Por favor, me pide mi taxi para la habitaciOn tres veintidOs? R: Could you ask for my taxi for room number three two two for me. please? Could you ask for my taxi for room number three two two, please? A: 0: Por favor. reservamos dos habitaciones dobles con cuarto de balm. We booked two double rooms with a bathroom. R: We booked two double rooms with a bathroom. please. A: 0: Quisiera que nos despertaran manana a las dos y cuarto. por favor. R: like you to wake us up tomorrow at. a quarter past two. please. wake us up tomorrow at a quarter past two. please. A: 0: Repaseme la cuenta de la habitaciOn ochocientos veintiuno. R: Could you check the bill for room number eight two one for me, please? Check the bill for room number eight two one. A: from 31.1% for the zerogra.m model to 5.1% for the bigram model. The results presented here can be compared with the results obtained by the finite-state transducer approach described in (Vidal, 1996: Vidal, 1997), where the same training and test. conditions were used. However the only preprocessing step was categorization. In that work. a WER of 7.1% was obtained as opposed to 5.17 presented in this paper. For smaller amounts of training data (say 5 000 sentence pairs). the DP based search seems to be even more superior. Table 5: Language model perplexity (PP), word error rates (INS/DEL. WER) and sentence error rates (SER) for different language models. Translation Errors [%] PP INS/DEL WER SER Zerogram 237.0 0.6/18.6 31.1 98.1 Unigram 74.4 0.9/12.4 20.4 94.8 Bigram 4.1 0.9/3.4 5.1 30.1 4.4 Effect of the Word Reordering In more general cases and applications, there will always be sentence pairs with word alignments for which the monotony constraint, is not satisfied. However even then, the monotony constraint, is satisfied the lion&apos;s share of all word alignments in such sentences. Therefore. we expect to extend the approach presented by the following methods: • more systematic approaches to local and global word reorderings that try to produce the same word order in both languages. • a multli-level approach that. allows a small (say number of and backward transitions. Within each level, the monotone alignment model can still be applied, and only when moving from one level to the next, we have to handle the problem of different, word orders. To show the usefulness of global word reordering. we changed the word order of some sentences by hand. Table 6 shows the effect. of the global reordering for two sentences. In the first example, we changed the order of two groups of consecutive words and placed an additional copy of the Spanish word into the source sentence. In the second the personal pronoun was placed at the end of the source sentence. In both cases, we obtained a correct translation. 5 Conclusion In this paper, we have presented an HMM based approach to handling word alignments and an associated search algorithm for automatic translation. The characteristic feature of this approach is to make the alignment probabilities explicitly dependent on the alignment, position of the previous word and to assume a monotony constraint for the word order in both languages. Due to this monotony constraint. we are able to apply an efficient DP based search al- We have tested the model successfully the EuTrans traveller task, a limited domain task a vocabulary of 200 to 500 words. The result- Language Model 295 Table 6: Effect of the global word reordering: 0= original sentence, R= reference translation, A= automatic translation, 0&apos;= original sentence reordered, A&apos;= automatic translation after reordering. 0: Cuanto cuesta una habitaciOn doble para cinco noches incluyendo servicio de habitaciones ? How much does a double room including room service cost for five nights ? How much does a double room including room service ? R: A: 0&apos;: Cuanto cuesta una habitaciOn doble incluyendo servicio de habitaciones cuesta para cinco noches ? How much does a double room including room service cost for five nights ? A&apos;: : • Explique _me la factura de la habitaciOn tres dos cuatro. Explain the bill for room number three two four for me. Explain the bill for room number three two four. R: A: O.: Explique la factura de la habitaciOn tres dos cuatro _me. Explain the bill for room number three two four for me. word error rate was only To mitigate the monotony constraint, we plan to reorder the words in the source sentences to produce the same word order in both languages. Acknowledgement</abstract>
<note confidence="0.907144267857143">This work has been supported partly by the German Federal Ministry of Education. Science, Research and Technology under the contract. number 01 IV 601 A (Verbmobil) and by the European Community under the ESPRIT project number 20268 (EliTrans). References A. L. Berger. P. F. Brown. S. A. Della Pietra. V. J. Della Pietra. J. R. Gillett. J. D. Lafferty. R. L. Mercer. H. Printz. and L. Ures. 1994. &amp;quot;The Can- System for Machine . In of Language Technology Workshop. pp. 152-157. Plainsboro. NJ. Morgan Kaufmann Publishers. San Mateo. CA. March. F. Brown, V. Pietra. S. A. Della Pietra, R. L. Mercer. 1993. Mathematics of Statistical Machine Translation: Parameter Esti- Linguistics. 19. No. 2. pp. 263-311. I. Dagan. E. IV. Church. and NV. A. Gale. 1993. Bilingual Word Alignment for Machine of the Workshop on Corpora. 1-8. Columbus, OH. P. Fung. and K. W. Church. 1994. &amp;quot;K-vec: A New for Aligning Parallel Texts&amp;quot;, In of the 15th Int. Conf. on Computational Linguistics, pp. 1096-1102. Kyoto. F. Jelinek. 1976. &amp;quot;Speech Recognition by Statistical of the IEEE. 64. pp. 532- 556. April. Kay. and M. Roscheisen. 1993. &amp;quot;Text- . Linguis- 19. No. 2. pp. 121-142. H. Ney, D. Merge!, A. Noll, A. Paeseler. 1992. &amp;quot;Data Driven Search Organization for Continuous Recognition&amp;quot;. Trans. on Signal Pro- SP-40. No. 2. pp. 272-281. February. E. Vidal. 1996. &amp;quot;Final report of Esprit Research Project 20268 (EuTrans): Example-Based Understanding and Translation Systems&amp;quot;. Universidad Politecnica de Valencia, Instituto TecnolOgio de Informatica, October. E. Vidal. 1997. &amp;quot;Finite-State Speech-to-Speech In of the Int. Conf. on Acous- Speech and Signal Processing. April. Vogel, H. Ney, and C. Tillmann. 1996. Based Word Alignment in Statistical Transla- In of the 1616 Int. Conf. on Com- Linguistics. Copenhagen, August. . Wu. 1996. &amp;quot;A Polynomial-Time Algorithm for Machine Translation&amp;quot;. In of the 341h Annual Conf. of the Association for Compu- Linguistics, 152-158. Santa Cruz, CA. June. 296</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown S A Della Pietra V J Della Pietra J R Gillett J D Lafferty R L Mercer H Printz</author>
<author>L Ures</author>
</authors>
<title>The Canelide System for Machine Translation- .</title>
<date>1994</date>
<booktitle>In Proc. of ARPA Human Language Technology Workshop.</booktitle>
<pages>152--157</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Plainsboro. NJ.</location>
<marker>Printz, Ures, 1994</marker>
<rawString>A. L. Berger. P. F. Brown. S. A. Della Pietra. V. J. Della Pietra. J. R. Gillett. J. D. Lafferty. R. L. Mercer. H. Printz. and L. Ures. 1994. &amp;quot;The Canelide System for Machine Translation- . In Proc. of ARPA Human Language Technology Workshop. pp. 152-157. Plainsboro. NJ. Morgan Kaufmann Publishers. San Mateo. CA. March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Parameter Estimation-. Computational Linguistics.</journal>
<volume>19</volume>
<pages>263--311</pages>
<marker>Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra. S. A. Della Pietra, and R. L. Mercer. 1993. -The Mathematics of Statistical Machine Translation: Parameter Estimation-. Computational Linguistics. Vol. 19. No. 2. pp. 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Church</author>
<author>NV A Gale</author>
</authors>
<title>Robust Bilingual Word Alignment for Machine Aided Translation-.</title>
<date>1993</date>
<booktitle>In Proc. of the Workshop on Very Large Corpora.</booktitle>
<pages>1--8</pages>
<location>Columbus, OH.</location>
<marker>Church, Gale, 1993</marker>
<rawString>I. Dagan. E. IV. Church. and NV. A. Gale. 1993. -Robust Bilingual Word Alignment for Machine Aided Translation-. In Proc. of the Workshop on Very Large Corpora. pp. 1-8. Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>K-vec: A New Approach for Aligning Parallel Texts&amp;quot;,</title>
<date>1994</date>
<booktitle>In Proc. of the 15th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1096--1102</pages>
<publisher>Kyoto.</publisher>
<marker>Church, 1994</marker>
<rawString>P. Fung. and K. W. Church. 1994. &amp;quot;K-vec: A New Approach for Aligning Parallel Texts&amp;quot;, In Proc. of the 15th Int. Conf. on Computational Linguistics, pp. 1096-1102. Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Speech Recognition by Statistical Methods&amp;quot;.</title>
<date>1976</date>
<booktitle>Proc. of the IEEE.</booktitle>
<volume>64</volume>
<pages>532--556</pages>
<contexts>
<context position="5446" citStr="Jelinek, 1976" startWordPosition="886" endWordPosition="887">tributions. Looking at such alignments produced by a human expert. it is evident that the mathematical model should try to capture the strong dependence of ai on the preceding alignment aj _ Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position aj_i: p(u j j _1) A similar approach ha.s been chosen by (Da.gan et al., 1993) and (Vogel et al.. 1996). Thus the problem formulation is similar to that of the time alignment problem in speech recognition, where the so-called Hidden Markov models have been successfully used for a long time (Jelinek, 1976). Using the same basic principles, we can rewrite the probability by introducing the &apos;hidden&apos; alignments a.1 := ...aj for a sentence pair [fi&apos;; ef]: Pr( fiJ = a(eli ) oil E(l7 j1 Prui To avoid any confusion with the term &apos;hidden&apos; in comparison with speech recognition. we observe that the model states as such (representing words) are it ol hidden but. the actual alignments, i.e. the sequence of position index pairs (j. i = ai ). So far there has been no basic restriction of the approach. We now assume a first-order dependence on the alignments aj only: Pr(fi, ) = p( fi , a jlai ei,) = Aajlai-d </context>
<context position="10075" citStr="Jelinek, 1976" startWordPosition="1781" endWordPosition="1782"> criterion: max H )P( .f 1_.„)] . j=1 The problem now is to find the unknown mapping: j — (ai, c ) which defines a path through a network with a uniform trellis structure. For this trellis, we can still use Figure 3. However, in each position i along the TARGET POSITION 1 1 1 1 1 3 4 5 6 SOURCE POSITION Figure 3: Illustration of alignments for the monotone HMM. To find the optimal alignment, we use dynamic programming for which we have the following typical recursion formula: Q(i, j) = ) • Q(i&apos; , - 1)] Here. Q(i. j) is a sort of partial probability as in time alignment for speech recognition (Jelinek, 1976). As a result the training procedure amounts to a sequence of iterations. each of which consists of two steps: • position alignmeni: Given the model parameters, determine the most likely position alignment. • parameter estimation: Given the position alignment i.e. going along the alignment paths for all sentence pairs, perform maximum likelihood estimation of the model parameters; for modelfree distributions, these estimates result. in relative frequencies. The IBM model 1 (Brown et al., 1993) is used to find an initial estimate of the translation probabilities. 3 Search Algorithm for Translat</context>
</contexts>
<marker>Jelinek, 1976</marker>
<rawString>F. Jelinek. 1976. &amp;quot;Speech Recognition by Statistical Methods&amp;quot;. Proc. of the IEEE. Vol. 64. pp. 532-556. April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Roscheisen</author>
</authors>
<title>TextTranslation Alignment-</title>
<date>1993</date>
<journal>Computational Linguistics.</journal>
<volume>19</volume>
<pages>121--142</pages>
<marker>Roscheisen, 1993</marker>
<rawString>M. Kay. and M. Roscheisen. 1993. &amp;quot;TextTranslation Alignment- . Computational Linguistics. Vol. 19. No. 2. pp. 121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>D Merge</author>
<author>A Noll</author>
<author>A Paeseler</author>
</authors>
<title>Data Driven Search Organization for Continuous Speech Recognition&amp;quot;.</title>
<date>1992</date>
<journal>IEEE Trans. on Signal Processing,</journal>
<volume>40</volume>
<pages>272--281</pages>
<marker>Ney, Merge, Noll, Paeseler, 1992</marker>
<rawString>H. Ney, D. Merge!, A. Noll, A. Paeseler. 1992. &amp;quot;Data Driven Search Organization for Continuous Speech Recognition&amp;quot;. IEEE Trans. on Signal Processing, Vol. SP-40. No. 2. pp. 272-281. February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Vidal</author>
</authors>
<title>Final report of Esprit Research Project 20268 (EuTrans): Example-Based Understanding and Translation Systems&amp;quot;.</title>
<date>1996</date>
<booktitle>Universidad Politecnica de Valencia, Instituto TecnolOgio de Informatica,</booktitle>
<contexts>
<context position="21204" citStr="Vidal, 1996" startWordPosition="3700" endWordPosition="3701">0: Quisiera que nos despertaran manana a las dos y cuarto. por favor. R: I would like you to wake us up tomorrow at. a quarter past two. please. A: I want you to wake us up tomorrow at a quarter past two. please. 0: Repaseme la cuenta de la habitaciOn ochocientos veintiuno. R: Could you check the bill for room number eight two one for me, please? A: Check the bill for room number eight two one. WER decreases from 31.1% for the zerogra.m model to 5.1% for the bigram model. The results presented here can be compared with the results obtained by the finite-state transducer approach described in (Vidal, 1996: Vidal, 1997), where the same training and test. conditions were used. However the only preprocessing step was categorization. In that work. a WER of 7.1% was obtained as opposed to 5.17 presented in this paper. For smaller amounts of training data (say 5 000 sentence pairs). the DP based search seems to be even more superior. Table 5: Language model perplexity (PP), word error rates (INS/DEL. WER) and sentence error rates (SER) for different language models. Translation Errors [%] PP INS/DEL WER SER Zerogram 237.0 0.6/18.6 31.1 98.1 Unigram 74.4 0.9/12.4 20.4 94.8 Bigram 4.1 0.9/3.4 5.1 30.1</context>
</contexts>
<marker>Vidal, 1996</marker>
<rawString>E. Vidal. 1996. &amp;quot;Final report of Esprit Research Project 20268 (EuTrans): Example-Based Understanding and Translation Systems&amp;quot;. Universidad Politecnica de Valencia, Instituto TecnolOgio de Informatica, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Vidal</author>
</authors>
<title>Finite-State Speech-to-Speech Translation&amp;quot;.</title>
<date>1997</date>
<booktitle>In Proc. of the Int. Conf. on Acoustics. Speech and Signal Processing.</booktitle>
<location>Munich.</location>
<contexts>
<context position="21218" citStr="Vidal, 1997" startWordPosition="3702" endWordPosition="3703">ue nos despertaran manana a las dos y cuarto. por favor. R: I would like you to wake us up tomorrow at. a quarter past two. please. A: I want you to wake us up tomorrow at a quarter past two. please. 0: Repaseme la cuenta de la habitaciOn ochocientos veintiuno. R: Could you check the bill for room number eight two one for me, please? A: Check the bill for room number eight two one. WER decreases from 31.1% for the zerogra.m model to 5.1% for the bigram model. The results presented here can be compared with the results obtained by the finite-state transducer approach described in (Vidal, 1996: Vidal, 1997), where the same training and test. conditions were used. However the only preprocessing step was categorization. In that work. a WER of 7.1% was obtained as opposed to 5.17 presented in this paper. For smaller amounts of training data (say 5 000 sentence pairs). the DP based search seems to be even more superior. Table 5: Language model perplexity (PP), word error rates (INS/DEL. WER) and sentence error rates (SER) for different language models. Translation Errors [%] PP INS/DEL WER SER Zerogram 237.0 0.6/18.6 31.1 98.1 Unigram 74.4 0.9/12.4 20.4 94.8 Bigram 4.1 0.9/3.4 5.1 30.1 4.4 Effect of</context>
</contexts>
<marker>Vidal, 1997</marker>
<rawString>E. Vidal. 1997. &amp;quot;Finite-State Speech-to-Speech Translation&amp;quot;. In Proc. of the Int. Conf. on Acoustics. Speech and Signal Processing. Munich. April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HAIM Based Word Alignment in Statistical Translation&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proc. of the 1616 Int. Conf. on Computational Linguistics.</booktitle>
<pages>836--841</pages>
<location>Copenhagen,</location>
<contexts>
<context position="3252" citStr="Vogel et al., 1996" startWordPosition="528" endWordPosition="531"> key issue in modeling the string translation probability Pr(fil is the question of how we define the correspondence between the words of the target. sentence and the words of the source sentence. In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (fi, ei) for a given sentence pair [fe; ef]. We further constrain this model by assigning each source word to exactly one target word. Models describing these types of dependencies are referred to as alignment models (Brown et al., 1993), (Dagan et al.. 1993). (Kay k ROscheisen, 1993). (Fung k Church. 1994), (Vogel et al., 1996). In this section, we introduce a monotone HMAI based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be as follows. We use the symbol Pr(.) to denote general 289 Source Language Text 1 Transformation Transformation 1 Target Language Text Figure I: Architecture of the translation approach based on Hayes decision rule. probability distributions with (nearly) no specific assumptions. In contrast for model-based probability distributions, we use the gener</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. -HAIM Based Word Alignment in Statistical Translation&amp;quot;. In Proc. of the 1616 Int. Conf. on Computational Linguistics. pp. 836-841. Copenhagen, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wu</author>
</authors>
<title>A Polynomial-Time Algorithm for Statistical Machine Translation&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proc. of the 341h Annual Conf. of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="3465" citStr="Wu, 1996" startWordPosition="563" endWordPosition="564">ssume a sort of pairwise dependence by considering all word pairs (fi, ei) for a given sentence pair [fe; ef]. We further constrain this model by assigning each source word to exactly one target word. Models describing these types of dependencies are referred to as alignment models (Brown et al., 1993), (Dagan et al.. 1993). (Kay k ROscheisen, 1993). (Fung k Church. 1994), (Vogel et al., 1996). In this section, we introduce a monotone HMAI based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be as follows. We use the symbol Pr(.) to denote general 289 Source Language Text 1 Transformation Transformation 1 Target Language Text Figure I: Architecture of the translation approach based on Hayes decision rule. probability distributions with (nearly) no specific assumptions. In contrast for model-based probability distributions, we use the generic symbol p(.). 2.1 Alignment with HMM When aligning the words in parallel texts (for Indo-European language pairs like Spanish-English, German-English, Italian-German,...), we typically observe a strong localizat</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>. Wu. 1996. &amp;quot;A Polynomial-Time Algorithm for Statistical Machine Translation&amp;quot;. In Proc. of the 341h Annual Conf. of the Association for Computational Linguistics, pp. 152-158. Santa Cruz, CA. June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>