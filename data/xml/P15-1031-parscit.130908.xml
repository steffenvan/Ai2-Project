<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000507">
<title confidence="0.991387">
An Effective Neural Network Model for Graph-based Dependency Parsing
</title>
<author confidence="0.991847">
Wenzhe Pei Tao Ge Baobao Chang∗
</author>
<affiliation confidence="0.988904">
Key Laboratory of Computational Linguistics, Ministry of Education,
School of Electronics Engineering and Computer Science, Peking University,
</affiliation>
<address confidence="0.926803">
No.5 Yiheyuan Road, Haidian District, Beijing, 100871, China
Collaborative Innovation Center for Language Ability, Xuzhou, 221009, China.
</address>
<email confidence="0.998137">
{peiwenzhe,getao,chbb}@pku.edu.cn
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999741647058824">
Most existing graph-based parsing models
rely on millions of hand-crafted features,
which limits their generalization ability
and slows down the parsing speed. In this
paper, we propose a general and effective
Neural Network model for graph-based
dependency parsing. Our model can auto-
matically learn high-order feature combi-
nations using only atomic features by ex-
ploiting a novel activation function tanh-
cube. Moreover, we propose a simple yet
effective way to utilize phrase-level infor-
mation that is expensive to use in conven-
tional graph-based parsers. Experiments
on the English Penn Treebank show that
parsers based on our model perform better
than conventional graph-based parsers.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976252157894737">
Dependency parsing is essential for computers to
understand natural languages, whose performance
may have a direct effect on many NLP applica-
tion. Due to its importance, dependency parsing,
has been studied for tens of years. Among a vari-
ety of dependency parsing approaches (McDonald
et al., 2005; McDonald and Pereira, 2006; Car-
reras, 2007; Koo and Collins, 2010; Zhang and
Nivre, 2011), graph-based models seem to be one
of the most successful solutions to the challenge
due to its ability of scoring the parsing decisions
on whole-tree basis. Typical graph-based models
factor the dependency tree into subgraphs, rang-
ing from the smallest edge (first-order) to a con-
trollable bigger subgraph consisting of more than
one single edge (second-order and third order),
and score the whole tree by summing scores of the
subgraphs. In these models, subgraphs are usually
represented as a high-dimensional feature vectors
∗Corresponding author
which are fed into a linear model to learn the fea-
ture weight for scoring the subgraphs.
In spite of their advantages, conventional graph-
based models rely heavily on an enormous num-
ber of hand-crafted features, which brings about
serious problems. First, a mass of features could
put the models in the risk of overfitting and slow
down the parsing speed, especially in the high-
order models where combinational features cap-
turing interactions between head, modifier, sib-
lings and (or) grandparent could easily explode
the feature space. In addition, feature design re-
quires domain expertise, which means useful fea-
tures are likely to be neglected due to a lack of
domain knowledge. As a matter of fact, these two
problems exist in most graph-based models, which
have stuck the development of dependency parsing
for a few years.
To ease the problem of feature engineering, we
propose a general and effective Neural Network
model for graph-based dependency parsing in this
paper. The main advantages of our model are as
follows:
• Instead of using large number of hand-crafted
features, our model only uses atomic fea-
tures (Chen et al., 2014) such as word uni-
grams and POS-tag unigrams. Feature com-
binations and high-order features are auto-
matically learned with our novel activation
function tanh-cube, thus alleviating the heavy
burden of feature engineering in conven-
tional graph-based models (McDonald et al.,
2005; McDonald and Pereira, 2006; Koo and
Collins, 2010). Not only does it avoid the risk
of overfitting but also it discovers useful new
features that have never been used in conven-
tional parsers.
</bodyText>
<listItem confidence="0.764185666666667">
• We propose to exploit phrase-level informa-
tion through distributed representation for
phrases (phrase embeddings). It not only en-
</listItem>
<page confidence="0.991059">
313
</page>
<note confidence="0.989676">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 313–322,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.993473">
Figure 1: First-order and Second-order factoriza-
</figureCaption>
<bodyText confidence="0.935949266666667">
tion strategy. Here h stands for head word, m
stands for modifier word and s stands for the sib-
ling of m.
ables our model to exploit richer context in-
formation that previous work did not consider
due to the curse of dimension but also cap-
tures inherent correlations between phrases.
• Unlike other neural network based models
(Chen et al., 2014; Le and Zuidema, 2014)
where an additional parser is needed for ei-
ther extracting features (Chen et al., 2014) or
generating k-best list for reranking (Le and
Zuidema, 2014), both training and decoding
in our model are performed based on our neu-
ral network architecture in an effective way.
</bodyText>
<listItem confidence="0.867044">
• Our model does not impose any change to
</listItem>
<bodyText confidence="0.953120217391304">
the decoding process of conventional graph-
based parsing model. First-order, second-
order and higher order models can be easily
implemented using our model.
We implement three effective models with in-
creasing expressive capabilities. The first model
is a simple first-order model that uses only atomic
features and does not use any combinational fea-
tures. Despite its simpleness, it outperforms
conventional first-order model (McDonald et al.,
2005) and has a faster parsing speed. To fur-
ther strengthen our parsing model, we incorpo-
rate phrase embeddings into the model, which
significantly improves the parsing accuracy. Fi-
nally, we extend our first-order model to a second-
order model that exploits interactions between two
adjacent dependency edges as in McDonald and
Pereira (2006) thus further improves the model
performance.
We evaluate our models on the English Penn
Treebank. Experiment results show that both our
first-order and second-order models outperform
the corresponding conventional models.
</bodyText>
<sectionHeader confidence="0.995023" genericHeader="method">
2 Neural Network Model
</sectionHeader>
<bodyText confidence="0.99996175">
A dependency tree is a rooted, directed tree span-
ning the whole sentence. Given a sentence x,
graph-based models formulates the parsing pro-
cess as a searching problem:
</bodyText>
<equation confidence="0.999898">
y*(x) = arg max Score(x, ˆy(x); 0) (1)
ˆ�EY (X)
</equation>
<bodyText confidence="0.998562083333333">
where y*(x) is tree with highest score, Y (x) is
the set of all trees compatible with x, 0 are model
parameters and Score(x, ˆy(x); 0) represents how
likely that a particular tree ˆy(x) is the correct anal-
ysis for x. However, the size of Y (x) is expo-
nential large, which makes it impractical to solve
equation (1) directly. Previous work (McDonald et
al., 2005; McDonald and Pereira, 2006; Koo and
Collins, 2010) assumes that the score of ˆy(x) fac-
tors through the scores of subgraphs c of ˆy(x) so
that efficient algorithms can be designed for de-
coding:
</bodyText>
<equation confidence="0.9989585">
Score(x, ˆy(x); 0) = � ScoreF(x, c; 0) (2)
CEˆ�(X)
</equation>
<bodyText confidence="0.9957406875">
Figure 1 gives two examples of commonly used
factorization strategy proposed by Mcdonald et.al
(2005) and Mcdonald and Pereira (2006). The
simplest subgraph uses a first-order factorization
(McDonald et al., 2005) which decomposes a de-
pendency tree into single dependency arcs (Fig-
ure 1(a)). Based on the first-order model, second-
order factorization (McDonald and Pereira, 2006)
(Figure 1(b)) brings sibling information into de-
coding. Specifically, a sibling part consists of a
triple of indices (h, m, s) where (h, m) and (h, s)
are dependencies and s and m are successive mod-
ifiers to the same side of h.
The most common choice for ScoreF(x, c; 0),
which is the score function for subgraph c in the
tree, is a simple linear function:
</bodyText>
<equation confidence="0.995465">
ScoreF(x, c; 0) = w · f(x, c) (3)
</equation>
<bodyText confidence="0.9999754">
where f(x, c) is the feature representation of sub-
graph c and w is the corresponding weight vector.
However, the effectiveness of this function relies
heavily on the design of feature vector f(x, c). In
previous work (McDonald et al., 2005; McDonald
and Pereira, 2006), millions of hand-crafted fea-
tures were used to capture context and structure
information in the subgraph which not only lim-
its the model’s ability to generalize well but only
slows down the parsing speed.
</bodyText>
<page confidence="0.9993">
314
</page>
<figureCaption confidence="0.999427666666667">
Figure 3: Illustration for phrase embeddings. h, m
and x0 to x6 are words in the sentence.
Figure 2: Architecture of the Neural Network
</figureCaption>
<bodyText confidence="0.9976315">
In our work, we propose a neural network
model for scoring subgraph c in the tree:
</bodyText>
<equation confidence="0.998771">
ScoreF(x, c; θ) = NN(x, c) (4)
</equation>
<bodyText confidence="0.983962636363636">
where NN is our scoring function based on neu-
ral network (Figure 2). As we will show in the fol-
lowing sections, it alleviates the heavy burden of
feature engineering in conventional graph-based
models and achieves better performance by auto-
matically learning useful information in the data.
The effectiveness of our neural network de-
pends on five key components: Feature Em-
beddings, Phrase Embeddings, Direction-specific
transformation, Learning Feature Combinations
and Max-Margin Training.
</bodyText>
<subsectionHeader confidence="0.991577">
2.1 Feature Embeddings
</subsectionHeader>
<bodyText confidence="0.999846357142857">
As shown in Figure 2, part of the input to the neu-
ral network is feature representation of the sub-
graph. Instead of using millions of features as in
conventional models, we only use use atomic fea-
tures (Chen et al., 2014) such as word unigrams
and POS-tag unigrams, which are less likely to be
sparse. The detailed atomic features we use will
be described in Section 3. Unlike conventional
models, the atomic features in our model are trans-
formed into their corresponding distributed repre-
sentations (feature embeddings).
The idea of distributed representation for sym-
bolic data is one of the most important reasons
why neural network works in NLP tasks. It is
shown that similar features will have similar em-
beddings which capture the syntactic and seman-
tic information behind features (Bengio et al.,
2003; Collobert et al., 2011; Schwenk et al., 2012;
Mikolov et al., 2013; Socher et al., 2013; Pei et al.,
2014).
Formally, we have a feature dictionary D of size
|D|. Each feature f E D is represented as a real-
valued vector (feature embedding) Embed(f) E
Rd where d is the dimensionality of the vector
space. All feature embeddings stacking together
forms the embedding matrix M E Rd×|D|. The
embedding matrix M is initialized randomly and
trained by our model (Section 2.6).
</bodyText>
<subsectionHeader confidence="0.999567">
2.2 Phrase Embeddings
</subsectionHeader>
<bodyText confidence="0.942820826086957">
Context information of word pairs1 such as the de-
pendency pair (h, m) has been widely believed to
be useful in graph-based models (McDonald et al.,
2005; McDonald and Pereira, 2006). Given a sen-
tence x, the context for h and m includes three
context parts: prefix, infix and suffix, as illustrated
in Figure 3. We call these parts phrases in our
work.
Context representation in conventional mod-
els are limited: First, phrases cannot be used as
features directly because of the data sparseness
problem. Therefore, phrases are backed off to
low-order representation such as bigrams and tri-
grams. For example, Mcdonald et.al (2005) used
tri-gram features of infix between head-modifier
pair (h, m). Sometimes even tri-grams are expen-
sive to use, which is the reason why Mcdonald and
Pereira (2006) chose to ignore features over triples
of words in their second-order model to prevent
from exploding the size of the feature space. Sec-
1A word pair is not limited to the dependency pair (h, m).
It could be any pair with particular relation (e.g., sibling pair
(s, m) in Figure 1). Figure 3 only uses (h, m) as an example.
</bodyText>
<page confidence="0.996903">
315
</page>
<bodyText confidence="0.999985034482758">
ond, bigrams or tri-grams are lexical features thus
cannot capture syntactic and semantic information
behind phrases. For instance, “hit the ball” and
“kick the football” should have similar represen-
tations because they share similar syntactic struc-
tures, but lexical tri-grams will fail to capture their
similarity.
Unlike previous work, we propose to use
distributed representation (phrase embedding) of
phrases to capture phrase-level information. We
use a simple yet effective way to calculate phrase
embeddings from word (POS-tag) embeddings.
As shown in Figure 3, we average the word em-
beddings in prefix, infix and suffix respectively and
get three global word-phrase embeddings. For
pairs where no prefix or suffix exists, the corre-
sponding embedding is set to zero. We also get
three global POS-phrase embeddings which are
calculated in the same way as words. These em-
beddings are then concatenated with feature em-
beddings and fed to the following hidden layer.
Phrase embeddings provide panorama represen-
tation of the context, allowing our model to cap-
ture richer context information compared with the
back-off tri-gram representation. Moreover, as
a distributed representation, phrase embeddings
perform generalization over specific phrases, thus
better capture the syntactic and semantic informa-
tion than back-off tri-grams.
</bodyText>
<subsectionHeader confidence="0.98667">
2.3 Direction-specific Transformation
</subsectionHeader>
<bodyText confidence="0.999994">
In dependency representation of sentence, the
edge direction indicates which one of the words is
the head h and which one is the modifier m. Un-
like previous work (McDonald et al., 2005; Mc-
Donald and Pereira, 2006) that models the edge
direction as feature to be conjoined with other fea-
tures, we model the edge direction with direction-
specific transformation.
As shown in Figure 2, the parameters in hidden
layer (Whd,bdh) and the output layer (Wod, bdo) are
bound with index d E {0, 1} which indicates the
direction between head and modifier (0 for left arc
and 1 for right arc). In this way, the model can
learn direction-specific parameters and automati-
cally capture the interactions between edge direc-
tion and other features.
</bodyText>
<subsectionHeader confidence="0.999596">
2.4 Learning Feature Combination
</subsectionHeader>
<bodyText confidence="0.99848425">
The key to the success of graph-based dependency
parsing is the design of features, especially com-
binational features. Effective as these features are,
as we have said in Section 1, they are prone to
overfitting and hard to design. In our work, we
introduce a new activation function that can auto-
matically learn these feature combinations.
As shown in Figure 2, we first concatenate the
embeddings into a single vector a. Then a is fed
into the next layer which performs linear trans-
formation followed by an element-wise activation
function g:
</bodyText>
<equation confidence="0.990929">
h = g(Whda + bdh) (5)
</equation>
<bodyText confidence="0.7324575">
Our new activation function g is defined as fol-
lows:
</bodyText>
<equation confidence="0.999498">
g(l) = tanh(l3 + l) (6)
</equation>
<bodyText confidence="0.999651916666667">
where l is the result of linear transformation and
tanh is the hyperbolic tangent activation function
widely used in neural networks. We call this new
activation function tanh-cube.
As we can see, without the cube term, tanh-cube
would be just the same as the conventional non-
linear transformation in most neural networks.
The cube extension is added to enhance the abil-
ity to capture complex interactions between input
features. Intuitively, the cube term in each hid-
den unit directly models feature combinations in a
multiplicative way:
</bodyText>
<equation confidence="0.990829">
(w1a1 + w2a2 + ... + wnan + b)3 =
� �(wiwjwk)aiajak + b(wiwj)aiaj...
i,j,k i,j
</equation>
<bodyText confidence="0.993667866666667">
These feature combinations are hand-designed in
conventional graph-based models but our model
learns these combinations automatically and en-
codes them in the model parameters.
Similar ideas were also proposed in previous
works (Socher et al., 2013; Pei et al., 2014; Chen
and Manning, 2014). Socher et.al (2013) and
Pei et.al (2014) used a tensor-based activation
function to learn feature combinations. However,
tensor-based transformation is quite slow even
with tensor factorization (Pei et al., 2014). Chen
and Manning (2014) proposed to use cube func-
tion g(l) = l3 which inspires our tanh-cube func-
tion. Compared with cube function, tanh-cube has
three advantages:
</bodyText>
<listItem confidence="0.9762265">
• The cube function is unbounded, making the
activation output either too small or too big if
the norm of input l is not properly controlled,
especially in deep neural network. On the
</listItem>
<page confidence="0.997203">
316
</page>
<bodyText confidence="0.998952333333333">
contrary, tanh-cube is bounded by the tanh
function thus safer to use in deep neural net-
work.
</bodyText>
<listItem confidence="0.900884785714286">
• Intuitively, the behavior of cube function re-
sembles the “polynomial kernel” in SVM.
In fact, SVM can be seen as a special one-
hidden-layer neural network where the ker-
nel function that performs non-linear trans-
formation is seen as a hidden layer and sup-
port vectors as hidden units. Compared with
cube function, tanh-cube combines the power
of “kernel function” with the tanh non-linear
transformation in neural network.
• Last but not least, as we will show in Section
4, tanh-cube converges faster than the cube
function although the rigorous proof is still
open to investigate.
</listItem>
<subsectionHeader confidence="0.989593">
2.5 Model Output
</subsectionHeader>
<bodyText confidence="0.999623333333333">
After the non-linear transformation of hidden
layer, the score of the subgraph c is calculated in
the output layer using a simple linear function:
</bodyText>
<equation confidence="0.9511375">
ScoreF(x, c) = Wod h + bd (7)
o
</equation>
<bodyText confidence="0.9971164">
The output score ScoreF(x, c) E R|L |is a score
vector where |L |is the number of dependency
types and each dimension of ScoreF(x, c) is the
score for each kind of dependency type of head-
modifier pair (i.e. (h, m) in Figure 1).
</bodyText>
<subsectionHeader confidence="0.973808">
2.6 Max-Margin Training
</subsectionHeader>
<bodyText confidence="0.997796909090909">
The parameters of our model are θ =
{W h d , bd h, W o d , bd o, M}. All parameters are
initialized with uniform distribution within (-0.01,
0.01).
For model training, we use the Max-Margin cri-
terion. Given a training instance (x, y), we search
for the dependency tree with the highest score
computed as equation (1) in Section 2. The object
of Max-Margin training is that the highest scor-
ing tree is the correct one: y∗ = y and its score
will be larger up to a margin to other possible tree
</bodyText>
<equation confidence="0.795355166666667">
yˆ E Y (x):
Score(x, y; θ) &gt; Score(x, ˆy; θ) + A(y, ˆy)
The structured margin loss A(y, ˆy) is defined as:
n
A(y, ˆy) = κ1{h(y, xj) =� h(ˆy, xj)}
j
</equation>
<table confidence="0.9985262">
1-order-atomic h−2.w, h−1.w, h.w, h1.w, h2.w
h−2.p, h−1.p, h.p, h1.p, h2.p
m−2.w, m−1.w, m.w, m1.w, m2.w
m−2.p, m−1.p, m.p, m1.p, m2.p
dis(h, m)
1-order-phrase + hm prefix.w, hm infix.w, hm suffix.w
+ hm prefix.p, hm infix.p, hm suffix.p
2-order-phrase + s−2.w, s−1.w, s.w, s1.w, s2.w
+ s−2.p, s−1.p, s.p, s1.p, s2.p
+ sm infix.w, sm infix.p
</table>
<tableCaption confidence="0.998903">
Table 1: Features in our three models. w is
</tableCaption>
<bodyText confidence="0.9948308">
short for word and p for POS-tag. h indicates
head and m indicates modifier. The subscript rep-
resents the relative position to the center word.
dis(h, m) is the distance between head and modi-
fier. hm prefix, hm infix and hm suffix are phrases
for head-modifier pair (h, m). s indicates the sib-
ling in second-order model. sm infix is the infix
phrase between sibling pair (s, m)
where n is the length of sentence x, h(y, xj) is the
head (with type) for the j-th word of x in tree y and
κ is a discount parameter. The loss is proportional
to the number of word with an incorrect head and
edge type in the proposed tree. This leads to the
regularized objective function for m training ex-
amples:
</bodyText>
<equation confidence="0.973524">
λ
li(θ) + 2||θ||2
li(θ) = max (Score(xi, ˆy; θ) + A(yi, ˆy))
ˆy∈Y (xi)
−Score(xi, yi; θ)) (8)
</equation>
<bodyText confidence="0.999936">
We use the diagonal variant of AdaGrad (Duchi
et al., 2011) with minibatchs (batch size = 20)
to minimize the object function. We also apply
dropout (Hinton et al., 2012) with 0.5 rate to the
hidden layer.
</bodyText>
<sectionHeader confidence="0.994749" genericHeader="method">
3 Model Implementation
</sectionHeader>
<bodyText confidence="0.999627666666667">
Base on our Neural Network model, we present
three model implementations with increasing ex-
pressive capabilities in this section.
</bodyText>
<subsectionHeader confidence="0.953104">
3.1 First-order models
</subsectionHeader>
<bodyText confidence="0.999973714285714">
We first implement two first-order models: 1-
order-atomic and 1-order-phrase. We use the
Eisner (2000) algorithm for decoding. The first
two rows of Table 1 list the features we use in these
two models.
1-order-atomic only uses atomic features as
shown in the first row of Table 1. Specifically, the
</bodyText>
<equation confidence="0.9948104">
1
J(θ) =
m
�m
i=1
</equation>
<page confidence="0.997522">
317
</page>
<table confidence="0.999017363636364">
Models Dev Test Speed (sent/s)
UAS LAS UAS LAS
First-order MSTParser-1-order 92.01 90.77 91.60 90.39 20
1-order-atomic-rand 92.00 90.71 91.62 90.41 55
1-order-atomic 92.19 90.94 92.14 90.92 55
1-order-phrase-rand 92.47 91.19 92.25 91.05 26
1-order-phrase 92.82 91.48 92.59 91.37 26
Second-order MSTParser-2-order 92.70 91.48 92.30 91.06 14
2-order-phrase-rand 93.39 92.10 92.99 91.79 10
2-order-phrase 93.57 92.29 93.29 92.13 10
Third-order (Koo and Collins, 2010) 93.49 N/A 93.04 N/A N/A
</table>
<tableCaption confidence="0.999974">
Table 2: Comparison with conventional graph-based models.
</tableCaption>
<bodyText confidence="0.999621416666667">
head word and its local neighbor words that are
within the distance of 2 are selected as the head’s
word unigram features. The modifier’s word un-
igram features is extracted in the same way. We
also use the POS-tags of the corresponding word
features and the distance between head and modi-
fier as additional atomic features.
We then improved 1-order-atomic to 1-order-
phrase by incorporating additional phrase embed-
dings. The three phrase embeddings of head-
modifier pair (h, m): hm prefix, hm infix and
hm suffix are calculated as in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.981248">
3.2 Second-order model
</subsectionHeader>
<bodyText confidence="0.999941857142857">
Our model can be easily extended to a second-
order model using the second-order decoding al-
gorithm (Eisner, 1996; McDonald and Pereira,
2006). The third row of Table 1 shows the addi-
tional features we use in our second-order model.
Sibling node and its local context are used as
additional atomic features. We also used the in-
fix embedding for the infix between sibling pair
(s, m), which we call sm infix. It is calculated in
the same way as infix between head-modifier pair
(h, m) (i.e., hm infix) in Section 2.2 except that
the word pair is now s and m. For cases where no
sibling information is available, the corresponding
sibling-related embeddings are set to zero vector.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998633">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999979866666667">
We use the English Penn Treebank (PTB) to eval-
uate our model implementations and Yamada and
Matsumoto (2003) head rules are used to extract
dependency trees. We follow the standard splits of
PTB3, using section 2-21 for training, section 22
as development set and 23 as test set. The Stanford
POS Tagger (Toutanova et al., 2003) with ten-way
jackknifing of the training data is used for assign-
ing POS tags (accuracy ≈ 97.2%).
Hyper-parameters of our models are tuned on
the development set and their final settings are
as follows: embedding size d = 50, hidden layer
(Layer 2) size = 200, regularization parameter A =
10−4, discount parameter for margin loss n = 0.3,
initial learning rate of AdaGrad alpha = 0.1.
</bodyText>
<subsectionHeader confidence="0.994321">
4.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.9998604">
Table 2 compares our models with several conven-
tional graph-based parsers. We use MSTParser2
for conventional first-order model (McDonald et
al., 2005) and second-order model (McDonald and
Pereira, 2006). We also include the result of a
third-order model of Koo and Collins (2010) for
comparison3. For our models, we report the results
with and without unsupervised pre-training. Pre-
training only trains the word-based feature embed-
dings on Gigaword corpus (Graff et al., 2003) us-
ing word2vec4 and all other parameters are still
initialized randomly. In all experiments, we re-
port unlabeled attachment scores (UAS) and la-
beled attachment scores (LAS) and punctuation5
is excluded in all evaluation metrics. The parsing
speeds are measured on a workstation with Intel
Xeon 3.4GHz CPU and 32GB RAM.
As we can see, even with random initialization,
1-order-atomic-rand performs as well as conven-
tional first-order model and both 1-order-phrase-
</bodyText>
<footnote confidence="0.99745475">
2http://sourceforge.net/projects/
mstparser
3Note that Koo and Collins (2010)’s third-order model
and our models are not strict comparable since their model
is an unlabeled model.
4https://code.google.com/p/word2vec/
5Following previous work, a token is a punctuation if its
POS tag is {“ ” : , .}
</footnote>
<page confidence="0.994802">
318
</page>
<figureCaption confidence="0.990612">
Figure 4: Convergence curve for tanh-cube and
cube activation function.
</figureCaption>
<table confidence="0.999940352941177">
Feature Type Instance Neighboors
Words in the, of, and,
(word2vec) for, from
his himself, her, he,
him, father
which its, essentially,
similar, that, also
Words in on, at, behind,
(Our model) among, during
his her, my, their,
its, he
which where, who, whom,
whose, though
POS-tags NN NNPS, NNS, EX,
NNP, POS
JJ JJR, JJS, PDT,
RBR, RBS
</table>
<tableCaption confidence="0.984752">
Table 4: Examples of similar words and POS-tags
according to feature embeddings.
</tableCaption>
<bodyText confidence="0.997483818181818">
rand and 2-order-phrase-rand perform better
than conventional models in MSTParser. Pre-
training further improves the performance of all
three models, which is consistent with the conclu-
sion of previous work (Pei et al., 2014; Chen and
Manning, 2014). Moreover, 1-order-phrase per-
forms better than 1-order-atomic, which shows
that phrase embeddings do improve the model. 2-
order-phrase further improves the performance
because of the more expressive second-order fac-
torization. All three models perform significantly
better than their counterparts in MSTParser where
millions of features are used and 1-order-phrase
works surprisingly well that it even beats the con-
ventional second-order model.
With regard to parsing speed, 1-order-atomic
is the fastest while other two models have similar
speeds as MSTParser. Further speed up could be
achieved by using pre-computing strategy as men-
tioned in Chen and Manning (2014). We did not
try this strategy since parsing speed is not the main
focus of this paper.
</bodyText>
<table confidence="0.79703375">
Model tanh-cube cube tanh
1-order-atomic 92.19 91.97 91.73
1-order-phrase 92.82 92.25 92.13
2-order-phrase 93.57 92.95 92.91
</table>
<tableCaption confidence="0.843243">
Table 3: Model Performance of different activa-
tion functions.
</tableCaption>
<bodyText confidence="0.999957866666667">
We also investigated the effect of different acti-
vation functions. We trained our models with the
same configuration except for the activation func-
tion. Table 3 lists the UAS of three models on de-
velopment set.
As we can see, tanh-cube function outperforms
cube function because of advantages we men-
tioned in Section 2.4. Moreover, both tanh-cube
function and cube function performs better than
tanh function. The reason is that the cube term can
capture more interactions between input features.
We also plot the UAS of 2-order-phrase dur-
ing each iteration of training. As shown in Figure
4, tanh-cube function converges faster than cube
function.
</bodyText>
<subsectionHeader confidence="0.999875">
4.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999953">
In order to see why our models work, we made
qualitative analysis on different aspects of our
model.
</bodyText>
<subsectionHeader confidence="0.983126">
Ability of Feature Abstraction
</subsectionHeader>
<bodyText confidence="0.999755294117647">
Feature embeddings give our model the ability of
feature abstraction. They capture the inherent cor-
relations between features so that syntactic similar
features will have similar representations, which
makes our model generalizes well on unseen data.
Table 4 shows the effect of different feature
embeddings which are obtained from 2-order-
phrase after training. For each kind of feature
type, we list several features as well as top 5 fea-
tures that are nearest (measured by Euclidean dis-
tance) to the corresponding feature according to
their embeddings.
We first analysis the effect of word embeddings
after training. For comparison, we also list the
initial word embeddings in word2vec. As we
can see, in word2vec word embeddings, words
that are similar to in and which tends to be those
</bodyText>
<page confidence="0.997372">
319
</page>
<table confidence="0.9998085">
Phrase Neighboor
On a Saturday morning On Monday night football
On Sunday
On Saturday
On Tuesday afternoon
On recent Saturday morning
most of it of it
of it all
some of it also
most of these are
only some of
big investment bank great investment bank
bank investment
entire equity investment
another cash equity investor
real estate lending division
</table>
<tableCaption confidence="0.8719895">
Table 5: Examples of similar phrases according to
phrase embeddings.
</tableCaption>
<bodyText confidence="0.999837125">
co-occuring with them and for word his, similar
words are morphologies of he. On the contrary,
similar words measured by our embeddings have
similar syntactic functions. This is helpful for de-
pendency parsing since parsing models care more
about the syntactic functions of words rather than
their collocations or morphologies.
POS-tag embeddings also show similar behav-
ior with word embeddings. As shown in Table 4,
our model captures similarities between POS-tags
even though their embeddings are initialized ran-
domly.
We also investigated the effect of phrase embed-
dings in the same way as feature embeddings. Ta-
ble 5 lists the examples of similar phrases. Our
phrase embeddings work pretty well given that
only a simple averaging strategy is used. Phrases
that are close to each other tend to share simi-
lar syntactic and semantic information. By using
phrase embeddings, our model sees panorama of
the context rather than limited word tri-grams and
thus captures richer context information, which is
the reason why phrase embeddings significantly
improve the performance.
</bodyText>
<subsectionHeader confidence="0.871865">
Ability of Feature Learning
</subsectionHeader>
<bodyText confidence="0.9990475">
Finally, we try to unveil the mysterious hidden
layer and investigate what features it learns. For
each hidden unit of 2-order-phrase, we get its
connections with embeddings (i.e., Wh in Figure
2) and pick the connections whose weights have
absolute value &gt; 0.1. We sampled several hidden
units and invenstigated which features their highly
weighted connections belong to:
</bodyText>
<listItem confidence="0.9998904">
• Hidden 1: h.w, m.w, h−1.w, ml.w
• Hidden 2: h.p, m.p, s.p
• Hidden 3: hm infix.p, hm infix.w, hm prefix.w
• Hidden 4: hm infix.w, hm prefix.w, sm infix.w
• Hidden 5: hm infix.p, hm infix.w, hm suffix.w
</listItem>
<bodyText confidence="0.972944740740741">
The samples above give qualitative results of what
features the hidden layer learns:
• Hidden unit 1 and 2 show that atomic features
of head, modifier, sibling and their local con-
text words are useful in our model, which is
consistent with our expectations since these
features are also very important features in
conventional graph-based models (McDon-
ald and Pereira, 2006).
• Features in the same hidden unit will “com-
bine” with each other through our tanh-cube
activation function. As we can see, feature
combination in hidden unit 2 were also used
in Mcdonald and Pereira (2006). However,
these feature combinations are automatically
captured by our model without the labor-
intensive feature engineering.
• Hidden unit 3 to 5 show that phrase-level
information like hm prefix, hm suffix and
sm infix are effective in our model. These
features are not used in conventional second-
order model (McDonald and Pereira, 2006)
because they could explode the feature space.
Through our tanh-cube activation function,
our model further captures the interactions
between phrases and other features without
the concern of overfitting.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999844866666666">
Models for dependency parsing have been stud-
ied with considerable effort in the NLP commu-
nity. Among them, we only focus on the graph-
based models here. Most previous systems ad-
dress this task by using linear statistical models
with carefully designed context and structure fea-
tures. The types of features available rely on tree
factorization and decoding algorithm. Mcdonald
et.al (2005) proposed the first-order model which
is also know as arc-factored model. Efficient de-
coding can be performed with Eisner (2000) algo-
rithm in O(n3) time and O(n2) space. Mcdonald
and Pereira (2006) further extend the first-order
model to second-order model where sibling infor-
mation is available during decoding. Eisner (2000)
</bodyText>
<page confidence="0.99033">
320
</page>
<bodyText confidence="0.999772962962963">
algorithm can be modified trivially for second-
order decoding. Carreras (2007) proposed a more
powerful second-order model that can score both
sibling and grandchild parts with the cost of O(n4)
time and O(n3) space. To exploit more struc-
ture information, Koo and Collins (2010) pro-
posed three third-order models with computational
requirements of O(n4) time and O(n3) space.
Recently, neural network models have been in-
creasingly focused on for their ability to minimize
the effort in feature engineering. Chen et.al (2014)
proposed an approach to automatically learning
feature embeddings for graph-based dependency
parsing. The learned feature embeddings are used
as additional features in conventional graph-based
model. Le and Zuidema (2014) proprosed an
infinite-order model based on recursive neural net-
work. However, their model can only be used as
an reranking model since decoding is intractable.
Compared with these work, our model is a
general and standalone neural network model.
Both training and decoding in our model are per-
formed based on our neural network architecture
in an effective way. Although only first-order
and second-order models are implemented in our
work, higher-order graph-based models can be
easily implemented using our model.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999944555555555">
In this paper, we propose a general and effec-
tive neural network model that can automatically
learn feature combinations with our novel acti-
vation function. Moreover, we introduce a sim-
ple yet effect way to utilize phrase-level informa-
tion, which greatly improves the model perfor-
mance. Experiments on the benchmark dataset
show that our model achieves better results than
conventional models.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999261571428571">
This work is supported by National Natural
Science Foundation of China under Grant No.
61273318 and National Key Basic Research Pro-
gram of China 2014CB340504. We want to thank
Miaohong Chen and Pingping Huang for their
valuable comments on the initial idea and helping
pre-process the data.
</bodyText>
<sectionHeader confidence="0.989921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831921568627">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL, pages 957–961.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 740–750, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature embedding for dependency parsing. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 816–826, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 999999:2121–2159.
Jason M Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1, pages 340–345. Association
for Computational Linguistics.
Jason Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In Advances in prob-
abilistic and other parsing technologies, pages 29–
61. Springer.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2003. English gigaword. Linguistic Data
Consortium, Philadelphia.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1–11. Association for
Computational Linguistics.
</reference>
<page confidence="0.981453">
321
</page>
<reference confidence="0.999519049180328">
Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 729–739, Doha, Qatar,
October. Association for Computational Linguistics.
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL. Citeseer.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd an-
nual meeting on association for computational lin-
guistics, pages 91–98. Association for Computa-
tional Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 293–303, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
HLT, pages 11–19. Association for Computational
Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, volume 3, pages
195–206.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.998343">
322
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.290881">
<title confidence="0.999952">An Effective Neural Network Model for Graph-based Dependency Parsing</title>
<author confidence="0.6313795">Pei Tao Ge Baobao Key Laboratory of Computational Linguistics</author>
<author confidence="0.6313795">Ministry of</author>
<affiliation confidence="0.997603">School of Electronics Engineering and Computer Science, Peking</affiliation>
<address confidence="0.977158">No.5 Yiheyuan Road, Haidian District, Beijing, 100871, Collaborative Innovation Center for Language Ability, Xuzhou, 221009,</address>
<abstract confidence="0.999260833333333">Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed. In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing. Our model can automatically learn high-order feature combinations using only atomic features by exa novel activation function tanh- Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="9474" citStr="Bengio et al., 2003" startWordPosition="1512" endWordPosition="1515">omic features (Chen et al., 2014) such as word unigrams and POS-tag unigrams, which are less likely to be sparse. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are transformed into their corresponding distributed representations (feature embeddings). The idea of distributed representation for symbolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar embeddings which capture the syntactic and semantic information behind features (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be usef</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="1471" citStr="Carreras, 2007" startWordPosition="207" endWordPosition="209"> a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors ∗Corresponding author w</context>
<context position="30110" citStr="Carreras (2007)" startWordPosition="4898" endWordPosition="4899">s this task by using linear statistical models with carefully designed context and structure features. The types of features available rely on tree factorization and decoding algorithm. Mcdonald et.al (2005) proposed the first-order model which is also know as arc-factored model. Efficient decoding can be performed with Eisner (2000) algorithm in O(n3) time and O(n2) space. Mcdonald and Pereira (2006) further extend the first-order model to second-order model where sibling information is available during decoding. Eisner (2000) 320 algorithm can be modified trivially for secondorder decoding. Carreras (2007) proposed a more powerful second-order model that can score both sibling and grandchild parts with the cost of O(n4) time and O(n3) space. To exploit more structure information, Koo and Collins (2010) proposed three third-order models with computational requirements of O(n4) time and O(n3) space. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Chen et.al (2014) proposed an approach to automatically learning feature embeddings for graph-based dependency parsing. The learned feature embeddings are used as addition</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In EMNLPCoNLL, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="14780" citStr="Chen and Manning, 2014" startWordPosition="2378" endWordPosition="2381">ansformation in most neural networks. The cube extension is added to enhance the ability to capture complex interactions between input features. Intuitively, the cube term in each hidden unit directly models feature combinations in a multiplicative way: (w1a1 + w2a2 + ... + wnan + b)3 = � �(wiwjwk)aiajak + b(wiwj)aiaj... i,j,k i,j These feature combinations are hand-designed in conventional graph-based models but our model learns these combinations automatically and encodes them in the model parameters. Similar ideas were also proposed in previous works (Socher et al., 2013; Pei et al., 2014; Chen and Manning, 2014). Socher et.al (2013) and Pei et.al (2014) used a tensor-based activation function to learn feature combinations. However, tensor-based transformation is quite slow even with tensor factorization (Pei et al., 2014). Chen and Manning (2014) proposed to use cube function g(l) = l3 which inspires our tanh-cube function. Compared with cube function, tanh-cube has three advantages: • The cube function is unbounded, making the activation output either too small or too big if the norm of input l is not properly controlled, especially in deep neural network. On the 316 contrary, tanh-cube is bounded b</context>
<context position="23532" citStr="Chen and Manning, 2014" startWordPosition="3855" endWordPosition="3858"> the, of, and, (word2vec) for, from his himself, her, he, him, father which its, essentially, similar, that, also Words in on, at, behind, (Our model) among, during his her, my, their, its, he which where, who, whom, whose, though POS-tags NN NNPS, NNS, EX, NNP, POS JJ JJR, JJS, PDT, RBR, RBS Table 4: Examples of similar words and POS-tags according to feature embeddings. rand and 2-order-phrase-rand perform better than conventional models in MSTParser. Pretraining further improves the performance of all three models, which is consistent with the conclusion of previous work (Pei et al., 2014; Chen and Manning, 2014). Moreover, 1-order-phrase performs better than 1-order-atomic, which shows that phrase embeddings do improve the model. 2- order-phrase further improves the performance because of the more expressive second-order factorization. All three models perform significantly better than their counterparts in MSTParser where millions of features are used and 1-order-phrase works surprisingly well that it even beats the conventional second-order model. With regard to parsing speed, 1-order-atomic is the fastest while other two models have similar speeds as MSTParser. Further speed up could be achieved b</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yue Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Feature embedding for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>816--826</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3209" citStr="Chen et al., 2014" startWordPosition="487" endWordPosition="490">feature space. In addition, feature design requires domain expertise, which means useful features are likely to be neglected due to a lack of domain knowledge. As a matter of fact, these two problems exist in most graph-based models, which have stuck the development of dependency parsing for a few years. To ease the problem of feature engineering, we propose a general and effective Neural Network model for graph-based dependency parsing in this paper. The main advantages of our model are as follows: • Instead of using large number of hand-crafted features, our model only uses atomic features (Chen et al., 2014) such as word unigrams and POS-tag unigrams. Feature combinations and high-order features are automatically learned with our novel activation function tanh-cube, thus alleviating the heavy burden of feature engineering in conventional graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). Not only does it avoid the risk of overfitting but also it discovers useful new features that have never been used in conventional parsers. • We propose to exploit phrase-level information through distributed representation for phrases (phrase embeddings). It not only e</context>
<context position="4467" citStr="Chen et al., 2014" startWordPosition="683" endWordPosition="686">ng of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 313–322, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: First-order and Second-order factorization strategy. Here h stands for head word, m stands for modifier word and s stands for the sibling of m. ables our model to exploit richer context information that previous work did not consider due to the curse of dimension but also captures inherent correlations between phrases. • Unlike other neural network based models (Chen et al., 2014; Le and Zuidema, 2014) where an additional parser is needed for either extracting features (Chen et al., 2014) or generating k-best list for reranking (Le and Zuidema, 2014), both training and decoding in our model are performed based on our neural network architecture in an effective way. • Our model does not impose any change to the decoding process of conventional graphbased parsing model. First-order, secondorder and higher order models can be easily implemented using our model. We implement three effective models with increasing expressive capabilities. The first model is a simple first-</context>
<context position="8888" citStr="Chen et al., 2014" startWordPosition="1418" endWordPosition="1421">eviates the heavy burden of feature engineering in conventional graph-based models and achieves better performance by automatically learning useful information in the data. The effectiveness of our neural network depends on five key components: Feature Embeddings, Phrase Embeddings, Direction-specific transformation, Learning Feature Combinations and Max-Margin Training. 2.1 Feature Embeddings As shown in Figure 2, part of the input to the neural network is feature representation of the subgraph. Instead of using millions of features as in conventional models, we only use use atomic features (Chen et al., 2014) such as word unigrams and POS-tag unigrams, which are less likely to be sparse. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are transformed into their corresponding distributed representations (feature embeddings). The idea of distributed representation for symbolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar embeddings which capture the syntactic and semantic information behind features (Bengio et al., 2003; Collobert et</context>
</contexts>
<marker>Chen, Zhang, Zhang, 2014</marker>
<rawString>Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Feature embedding for dependency parsing. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 816–826, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="9498" citStr="Collobert et al., 2011" startWordPosition="1516" endWordPosition="1519">t al., 2014) such as word unigrams and POS-tag unigrams, which are less likely to be sparse. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are transformed into their corresponding distributed representations (feature embeddings). The idea of distributed representation for symbolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar embeddings which capture the syntactic and semantic information behind features (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be useful in graph-based models</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>999999--2121</pages>
<contexts>
<context position="18344" citStr="Duchi et al., 2011" startWordPosition="3020" endWordPosition="3023">ffix are phrases for head-modifier pair (h, m). s indicates the sibling in second-order model. sm infix is the infix phrase between sibling pair (s, m) where n is the length of sentence x, h(y, xj) is the head (with type) for the j-th word of x in tree y and κ is a discount parameter. The loss is proportional to the number of word with an incorrect head and edge type in the proposed tree. This leads to the regularized objective function for m training examples: λ li(θ) + 2||θ||2 li(θ) = max (Score(xi, ˆy; θ) + A(yi, ˆy)) ˆy∈Y (xi) −Score(xi, yi; θ)) (8) We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs (batch size = 20) to minimize the object function. We also apply dropout (Hinton et al., 2012) with 0.5 rate to the hidden layer. 3 Model Implementation Base on our Neural Network model, we present three model implementations with increasing expressive capabilities in this section. 3.1 First-order models We first implement two first-order models: 1- order-atomic and 1-order-phrase. We use the Eisner (2000) algorithm for decoding. The first two rows of Table 1 list the features we use in these two models. 1-order-atomic only uses atomic features as shown in the first row of Tab</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 999999:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics-Volume 1,</booktitle>
<pages>340--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20216" citStr="Eisner, 1996" startWordPosition="3320" endWordPosition="3321"> of 2 are selected as the head’s word unigram features. The modifier’s word unigram features is extracted in the same way. We also use the POS-tags of the corresponding word features and the distance between head and modifier as additional atomic features. We then improved 1-order-atomic to 1-orderphrase by incorporating additional phrase embeddings. The three phrase embeddings of headmodifier pair (h, m): hm prefix, hm infix and hm suffix are calculated as in Section 2.2. 3.2 Second-order model Our model can be easily extended to a secondorder model using the second-order decoding algorithm (Eisner, 1996; McDonald and Pereira, 2006). The third row of Table 1 shows the additional features we use in our second-order model. Sibling node and its local context are used as additional atomic features. We also used the infix embedding for the infix between sibling pair (s, m), which we call sm infix. It is calculated in the same way as infix between head-modifier pair (h, m) (i.e., hm infix) in Section 2.2 except that the word pair is now s and m. For cases where no sibling information is available, the corresponding sibling-related embeddings are set to zero vector. 4 Experiments 4.1 Experiment Setu</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th conference on Computational linguistics-Volume 1, pages 340–345. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms.</title>
<date>2000</date>
<booktitle>In Advances in probabilistic and other parsing technologies,</booktitle>
<pages>29--61</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="18770" citStr="Eisner (2000)" startWordPosition="3088" endWordPosition="3089">function for m training examples: λ li(θ) + 2||θ||2 li(θ) = max (Score(xi, ˆy; θ) + A(yi, ˆy)) ˆy∈Y (xi) −Score(xi, yi; θ)) (8) We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs (batch size = 20) to minimize the object function. We also apply dropout (Hinton et al., 2012) with 0.5 rate to the hidden layer. 3 Model Implementation Base on our Neural Network model, we present three model implementations with increasing expressive capabilities in this section. 3.1 First-order models We first implement two first-order models: 1- order-atomic and 1-order-phrase. We use the Eisner (2000) algorithm for decoding. The first two rows of Table 1 list the features we use in these two models. 1-order-atomic only uses atomic features as shown in the first row of Table 1. Specifically, the 1 J(θ) = m �m i=1 317 Models Dev Test Speed (sent/s) UAS LAS UAS LAS First-order MSTParser-1-order 92.01 90.77 91.60 90.39 20 1-order-atomic-rand 92.00 90.71 91.62 90.41 55 1-order-atomic 92.19 90.94 92.14 90.92 55 1-order-phrase-rand 92.47 91.19 92.25 91.05 26 1-order-phrase 92.82 91.48 92.59 91.37 26 Second-order MSTParser-2-order 92.70 91.48 92.30 91.06 14 2-order-phrase-rand 93.39 92.10 92.99 91</context>
<context position="29830" citStr="Eisner (2000)" startWordPosition="4856" endWordPosition="4857">nteractions between phrases and other features without the concern of overfitting. 5 Related Work Models for dependency parsing have been studied with considerable effort in the NLP community. Among them, we only focus on the graphbased models here. Most previous systems address this task by using linear statistical models with carefully designed context and structure features. The types of features available rely on tree factorization and decoding algorithm. Mcdonald et.al (2005) proposed the first-order model which is also know as arc-factored model. Efficient decoding can be performed with Eisner (2000) algorithm in O(n3) time and O(n2) space. Mcdonald and Pereira (2006) further extend the first-order model to second-order model where sibling information is available during decoding. Eisner (2000) 320 algorithm can be modified trivially for secondorder decoding. Carreras (2007) proposed a more powerful second-order model that can score both sibling and grandchild parts with the cost of O(n4) time and O(n3) space. To exploit more structure information, Koo and Collins (2010) proposed three third-order models with computational requirements of O(n4) time and O(n3) space. Recently, neural netwo</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In Advances in probabilistic and other parsing technologies, pages 29– 61. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2003</date>
<booktitle>English gigaword. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="22032" citStr="Graff et al., 2003" startWordPosition="3622" endWordPosition="3625">rization parameter A = 10−4, discount parameter for margin loss n = 0.3, initial learning rate of AdaGrad alpha = 0.1. 4.2 Experiment Results Table 2 compares our models with several conventional graph-based parsers. We use MSTParser2 for conventional first-order model (McDonald et al., 2005) and second-order model (McDonald and Pereira, 2006). We also include the result of a third-order model of Koo and Collins (2010) for comparison3. For our models, we report the results with and without unsupervised pre-training. Pretraining only trains the word-based feature embeddings on Gigaword corpus (Graff et al., 2003) using word2vec4 and all other parameters are still initialized randomly. In all experiments, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation5 is excluded in all evaluation metrics. The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM. As we can see, even with random initialization, 1-order-atomic-rand performs as well as conventional first-order model and both 1-order-phrase2http://sourceforge.net/projects/ mstparser 3Note that Koo and Collins (2010)’s third-order model and our models are not strict comparable</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2003</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="18455" citStr="Hinton et al., 2012" startWordPosition="3039" endWordPosition="3042"> infix phrase between sibling pair (s, m) where n is the length of sentence x, h(y, xj) is the head (with type) for the j-th word of x in tree y and κ is a discount parameter. The loss is proportional to the number of word with an incorrect head and edge type in the proposed tree. This leads to the regularized objective function for m training examples: λ li(θ) + 2||θ||2 li(θ) = max (Score(xi, ˆy; θ) + A(yi, ˆy)) ˆy∈Y (xi) −Score(xi, yi; θ)) (8) We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs (batch size = 20) to minimize the object function. We also apply dropout (Hinton et al., 2012) with 0.5 rate to the hidden layer. 3 Model Implementation Base on our Neural Network model, we present three model implementations with increasing expressive capabilities in this section. 3.1 First-order models We first implement two first-order models: 1- order-atomic and 1-order-phrase. We use the Eisner (2000) algorithm for decoding. The first two rows of Table 1 list the features we use in these two models. 1-order-atomic only uses atomic features as shown in the first row of Table 1. Specifically, the 1 J(θ) = m �m i=1 317 Models Dev Test Speed (sent/s) UAS LAS UAS LAS First-order MSTPar</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1494" citStr="Koo and Collins, 2010" startWordPosition="210" endWordPosition="213">fective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors ∗Corresponding author which are fed into a lin</context>
<context position="3537" citStr="Koo and Collins, 2010" startWordPosition="536" endWordPosition="539">m of feature engineering, we propose a general and effective Neural Network model for graph-based dependency parsing in this paper. The main advantages of our model are as follows: • Instead of using large number of hand-crafted features, our model only uses atomic features (Chen et al., 2014) such as word unigrams and POS-tag unigrams. Feature combinations and high-order features are automatically learned with our novel activation function tanh-cube, thus alleviating the heavy burden of feature engineering in conventional graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). Not only does it avoid the risk of overfitting but also it discovers useful new features that have never been used in conventional parsers. • We propose to exploit phrase-level information through distributed representation for phrases (phrase embeddings). It not only en313 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 313–322, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: First-order and Second-order factorization strategy.</context>
<context position="6459" citStr="Koo and Collins, 2010" startWordPosition="1006" endWordPosition="1009">dency tree is a rooted, directed tree spanning the whole sentence. Given a sentence x, graph-based models formulates the parsing process as a searching problem: y*(x) = arg max Score(x, ˆy(x); 0) (1) ˆ�EY (X) where y*(x) is tree with highest score, Y (x) is the set of all trees compatible with x, 0 are model parameters and Score(x, ˆy(x); 0) represents how likely that a particular tree ˆy(x) is the correct analysis for x. However, the size of Y (x) is exponential large, which makes it impractical to solve equation (1) directly. Previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010) assumes that the score of ˆy(x) factors through the scores of subgraphs c of ˆy(x) so that efficient algorithms can be designed for decoding: Score(x, ˆy(x); 0) = � ScoreF(x, c; 0) (2) CEˆ�(X) Figure 1 gives two examples of commonly used factorization strategy proposed by Mcdonald et.al (2005) and Mcdonald and Pereira (2006). The simplest subgraph uses a first-order factorization (McDonald et al., 2005) which decomposes a dependency tree into single dependency arcs (Figure 1(a)). Based on the first-order model, secondorder factorization (McDonald and Pereira, 2006) (Figure 1(b)) brings siblin</context>
<context position="19454" citStr="Koo and Collins, 2010" startWordPosition="3192" endWordPosition="3195">e features we use in these two models. 1-order-atomic only uses atomic features as shown in the first row of Table 1. Specifically, the 1 J(θ) = m �m i=1 317 Models Dev Test Speed (sent/s) UAS LAS UAS LAS First-order MSTParser-1-order 92.01 90.77 91.60 90.39 20 1-order-atomic-rand 92.00 90.71 91.62 90.41 55 1-order-atomic 92.19 90.94 92.14 90.92 55 1-order-phrase-rand 92.47 91.19 92.25 91.05 26 1-order-phrase 92.82 91.48 92.59 91.37 26 Second-order MSTParser-2-order 92.70 91.48 92.30 91.06 14 2-order-phrase-rand 93.39 92.10 92.99 91.79 10 2-order-phrase 93.57 92.29 93.29 92.13 10 Third-order (Koo and Collins, 2010) 93.49 N/A 93.04 N/A N/A Table 2: Comparison with conventional graph-based models. head word and its local neighbor words that are within the distance of 2 are selected as the head’s word unigram features. The modifier’s word unigram features is extracted in the same way. We also use the POS-tags of the corresponding word features and the distance between head and modifier as additional atomic features. We then improved 1-order-atomic to 1-orderphrase by incorporating additional phrase embeddings. The three phrase embeddings of headmodifier pair (h, m): hm prefix, hm infix and hm suffix are ca</context>
<context position="21835" citStr="Koo and Collins (2010)" startWordPosition="3592" endWordPosition="3595">g POS tags (accuracy ≈ 97.2%). Hyper-parameters of our models are tuned on the development set and their final settings are as follows: embedding size d = 50, hidden layer (Layer 2) size = 200, regularization parameter A = 10−4, discount parameter for margin loss n = 0.3, initial learning rate of AdaGrad alpha = 0.1. 4.2 Experiment Results Table 2 compares our models with several conventional graph-based parsers. We use MSTParser2 for conventional first-order model (McDonald et al., 2005) and second-order model (McDonald and Pereira, 2006). We also include the result of a third-order model of Koo and Collins (2010) for comparison3. For our models, we report the results with and without unsupervised pre-training. Pretraining only trains the word-based feature embeddings on Gigaword corpus (Graff et al., 2003) using word2vec4 and all other parameters are still initialized randomly. In all experiments, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation5 is excluded in all evaluation metrics. The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM. As we can see, even with random initialization, 1-order-atomic-rand performs as wel</context>
<context position="30310" citStr="Koo and Collins (2010)" startWordPosition="4929" endWordPosition="4932">d et.al (2005) proposed the first-order model which is also know as arc-factored model. Efficient decoding can be performed with Eisner (2000) algorithm in O(n3) time and O(n2) space. Mcdonald and Pereira (2006) further extend the first-order model to second-order model where sibling information is available during decoding. Eisner (2000) 320 algorithm can be modified trivially for secondorder decoding. Carreras (2007) proposed a more powerful second-order model that can score both sibling and grandchild parts with the cost of O(n4) time and O(n3) space. To exploit more structure information, Koo and Collins (2010) proposed three third-order models with computational requirements of O(n4) time and O(n3) space. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Chen et.al (2014) proposed an approach to automatically learning feature embeddings for graph-based dependency parsing. The learned feature embeddings are used as additional features in conventional graph-based model. Le and Zuidema (2014) proprosed an infinite-order model based on recursive neural network. However, their model can only be used as an reranking model si</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>The insideoutside recursive neural network model for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>729--739</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="4490" citStr="Zuidema, 2014" startWordPosition="689" endWordPosition="690">Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 313–322, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: First-order and Second-order factorization strategy. Here h stands for head word, m stands for modifier word and s stands for the sibling of m. ables our model to exploit richer context information that previous work did not consider due to the curse of dimension but also captures inherent correlations between phrases. • Unlike other neural network based models (Chen et al., 2014; Le and Zuidema, 2014) where an additional parser is needed for either extracting features (Chen et al., 2014) or generating k-best list for reranking (Le and Zuidema, 2014), both training and decoding in our model are performed based on our neural network architecture in an effective way. • Our model does not impose any change to the decoding process of conventional graphbased parsing model. First-order, secondorder and higher order models can be easily implemented using our model. We implement three effective models with increasing expressive capabilities. The first model is a simple first-order model that uses o</context>
<context position="30778" citStr="Zuidema (2014)" startWordPosition="4997" endWordPosition="4998">score both sibling and grandchild parts with the cost of O(n4) time and O(n3) space. To exploit more structure information, Koo and Collins (2010) proposed three third-order models with computational requirements of O(n4) time and O(n3) space. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Chen et.al (2014) proposed an approach to automatically learning feature embeddings for graph-based dependency parsing. The learned feature embeddings are used as additional features in conventional graph-based model. Le and Zuidema (2014) proprosed an infinite-order model based on recursive neural network. However, their model can only be used as an reranking model since decoding is intractable. Compared with these work, our model is a general and standalone neural network model. Both training and decoding in our model are performed based on our neural network architecture in an effective way. Although only first-order and second-order models are implemented in our work, higher-order graph-based models can be easily implemented using our model. 6 Conclusion In this paper, we propose a general and effective neural network model</context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014. The insideoutside recursive neural network model for dependency parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 729–739, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL. Citeseer.</booktitle>
<contexts>
<context position="1455" citStr="McDonald and Pereira, 2006" startWordPosition="203" endWordPosition="206">nhcube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors ∗Corres</context>
<context position="3513" citStr="McDonald and Pereira, 2006" startWordPosition="532" endWordPosition="535">ew years. To ease the problem of feature engineering, we propose a general and effective Neural Network model for graph-based dependency parsing in this paper. The main advantages of our model are as follows: • Instead of using large number of hand-crafted features, our model only uses atomic features (Chen et al., 2014) such as word unigrams and POS-tag unigrams. Feature combinations and high-order features are automatically learned with our novel activation function tanh-cube, thus alleviating the heavy burden of feature engineering in conventional graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). Not only does it avoid the risk of overfitting but also it discovers useful new features that have never been used in conventional parsers. • We propose to exploit phrase-level information through distributed representation for phrases (phrase embeddings). It not only en313 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 313–322, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: First-order and Second-order</context>
<context position="5584" citStr="McDonald and Pereira (2006)" startWordPosition="860" endWordPosition="863">e implement three effective models with increasing expressive capabilities. The first model is a simple first-order model that uses only atomic features and does not use any combinational features. Despite its simpleness, it outperforms conventional first-order model (McDonald et al., 2005) and has a faster parsing speed. To further strengthen our parsing model, we incorporate phrase embeddings into the model, which significantly improves the parsing accuracy. Finally, we extend our first-order model to a secondorder model that exploits interactions between two adjacent dependency edges as in McDonald and Pereira (2006) thus further improves the model performance. We evaluate our models on the English Penn Treebank. Experiment results show that both our first-order and second-order models outperform the corresponding conventional models. 2 Neural Network Model A dependency tree is a rooted, directed tree spanning the whole sentence. Given a sentence x, graph-based models formulates the parsing process as a searching problem: y*(x) = arg max Score(x, ˆy(x); 0) (1) ˆ�EY (X) where y*(x) is tree with highest score, Y (x) is the set of all trees compatible with x, 0 are model parameters and Score(x, ˆy(x); 0) rep</context>
<context position="7031" citStr="McDonald and Pereira, 2006" startWordPosition="1098" endWordPosition="1101">005; McDonald and Pereira, 2006; Koo and Collins, 2010) assumes that the score of ˆy(x) factors through the scores of subgraphs c of ˆy(x) so that efficient algorithms can be designed for decoding: Score(x, ˆy(x); 0) = � ScoreF(x, c; 0) (2) CEˆ�(X) Figure 1 gives two examples of commonly used factorization strategy proposed by Mcdonald et.al (2005) and Mcdonald and Pereira (2006). The simplest subgraph uses a first-order factorization (McDonald et al., 2005) which decomposes a dependency tree into single dependency arcs (Figure 1(a)). Based on the first-order model, secondorder factorization (McDonald and Pereira, 2006) (Figure 1(b)) brings sibling information into decoding. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies and s and m are successive modifiers to the same side of h. The most common choice for ScoreF(x, c; 0), which is the score function for subgraph c in the tree, is a simple linear function: ScoreF(x, c; 0) = w · f(x, c) (3) where f(x, c) is the feature representation of subgraph c and w is the corresponding weight vector. However, the effectiveness of this function relies heavily on the design of feature vector f(x, c). In previ</context>
<context position="10150" citStr="McDonald and Pereira, 2006" startWordPosition="1628" endWordPosition="1631">ikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be useful in graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006). Given a sentence x, the context for h and m includes three context parts: prefix, infix and suffix, as illustrated in Figure 3. We call these parts phrases in our work. Context representation in conventional models are limited: First, phrases cannot be used as features directly because of the data sparseness problem. Therefore, phrases are backed off to low-order representation such as bigrams and trigrams. For example, Mcdonald et.al (2005) used tri-gram features of infix between head-modifier pair (h, m). Sometimes even tri-grams are expensive to use, which is the reason why Mcdonald and P</context>
<context position="12678" citStr="McDonald and Pereira, 2006" startWordPosition="2027" endWordPosition="2031">en layer. Phrase embeddings provide panorama representation of the context, allowing our model to capture richer context information compared with the back-off tri-gram representation. Moreover, as a distributed representation, phrase embeddings perform generalization over specific phrases, thus better capture the syntactic and semantic information than back-off tri-grams. 2.3 Direction-specific Transformation In dependency representation of sentence, the edge direction indicates which one of the words is the head h and which one is the modifier m. Unlike previous work (McDonald et al., 2005; McDonald and Pereira, 2006) that models the edge direction as feature to be conjoined with other features, we model the edge direction with directionspecific transformation. As shown in Figure 2, the parameters in hidden layer (Whd,bdh) and the output layer (Wod, bdo) are bound with index d E {0, 1} which indicates the direction between head and modifier (0 for left arc and 1 for right arc). In this way, the model can learn direction-specific parameters and automatically capture the interactions between edge direction and other features. 2.4 Learning Feature Combination The key to the success of graph-based dependency p</context>
<context position="20245" citStr="McDonald and Pereira, 2006" startWordPosition="3322" endWordPosition="3325">cted as the head’s word unigram features. The modifier’s word unigram features is extracted in the same way. We also use the POS-tags of the corresponding word features and the distance between head and modifier as additional atomic features. We then improved 1-order-atomic to 1-orderphrase by incorporating additional phrase embeddings. The three phrase embeddings of headmodifier pair (h, m): hm prefix, hm infix and hm suffix are calculated as in Section 2.2. 3.2 Second-order model Our model can be easily extended to a secondorder model using the second-order decoding algorithm (Eisner, 1996; McDonald and Pereira, 2006). The third row of Table 1 shows the additional features we use in our second-order model. Sibling node and its local context are used as additional atomic features. We also used the infix embedding for the infix between sibling pair (s, m), which we call sm infix. It is calculated in the same way as infix between head-modifier pair (h, m) (i.e., hm infix) in Section 2.2 except that the word pair is now s and m. For cases where no sibling information is available, the corresponding sibling-related embeddings are set to zero vector. 4 Experiments 4.1 Experiment Setup We use the English Penn Tre</context>
<context position="21758" citStr="McDonald and Pereira, 2006" startWordPosition="3578" endWordPosition="3581">a et al., 2003) with ten-way jackknifing of the training data is used for assigning POS tags (accuracy ≈ 97.2%). Hyper-parameters of our models are tuned on the development set and their final settings are as follows: embedding size d = 50, hidden layer (Layer 2) size = 200, regularization parameter A = 10−4, discount parameter for margin loss n = 0.3, initial learning rate of AdaGrad alpha = 0.1. 4.2 Experiment Results Table 2 compares our models with several conventional graph-based parsers. We use MSTParser2 for conventional first-order model (McDonald et al., 2005) and second-order model (McDonald and Pereira, 2006). We also include the result of a third-order model of Koo and Collins (2010) for comparison3. For our models, we report the results with and without unsupervised pre-training. Pretraining only trains the word-based feature embeddings on Gigaword corpus (Graff et al., 2003) using word2vec4 and all other parameters are still initialized randomly. In all experiments, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation5 is excluded in all evaluation metrics. The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM. As we </context>
<context position="28548" citStr="McDonald and Pereira, 2006" startWordPosition="4651" endWordPosition="4655">res their highly weighted connections belong to: • Hidden 1: h.w, m.w, h−1.w, ml.w • Hidden 2: h.p, m.p, s.p • Hidden 3: hm infix.p, hm infix.w, hm prefix.w • Hidden 4: hm infix.w, hm prefix.w, sm infix.w • Hidden 5: hm infix.p, hm infix.w, hm suffix.w The samples above give qualitative results of what features the hidden layer learns: • Hidden unit 1 and 2 show that atomic features of head, modifier, sibling and their local context words are useful in our model, which is consistent with our expectations since these features are also very important features in conventional graph-based models (McDonald and Pereira, 2006). • Features in the same hidden unit will “combine” with each other through our tanh-cube activation function. As we can see, feature combination in hidden unit 2 were also used in Mcdonald and Pereira (2006). However, these feature combinations are automatically captured by our model without the laborintensive feature engineering. • Hidden unit 3 to 5 show that phrase-level information like hm prefix, hm suffix and sm infix are effective in our model. These features are not used in conventional secondorder model (McDonald and Pereira, 2006) because they could explode the feature space. Throug</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting on association for computational linguistics,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1427" citStr="McDonald et al., 2005" startWordPosition="199" endWordPosition="202"> activation function tanhcube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensi</context>
<context position="3485" citStr="McDonald et al., 2005" startWordPosition="528" endWordPosition="531">endency parsing for a few years. To ease the problem of feature engineering, we propose a general and effective Neural Network model for graph-based dependency parsing in this paper. The main advantages of our model are as follows: • Instead of using large number of hand-crafted features, our model only uses atomic features (Chen et al., 2014) such as word unigrams and POS-tag unigrams. Feature combinations and high-order features are automatically learned with our novel activation function tanh-cube, thus alleviating the heavy burden of feature engineering in conventional graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). Not only does it avoid the risk of overfitting but also it discovers useful new features that have never been used in conventional parsers. • We propose to exploit phrase-level information through distributed representation for phrases (phrase embeddings). It not only en313 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 313–322, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: </context>
<context position="5248" citStr="McDonald et al., 2005" startWordPosition="807" endWordPosition="810"> Zuidema, 2014), both training and decoding in our model are performed based on our neural network architecture in an effective way. • Our model does not impose any change to the decoding process of conventional graphbased parsing model. First-order, secondorder and higher order models can be easily implemented using our model. We implement three effective models with increasing expressive capabilities. The first model is a simple first-order model that uses only atomic features and does not use any combinational features. Despite its simpleness, it outperforms conventional first-order model (McDonald et al., 2005) and has a faster parsing speed. To further strengthen our parsing model, we incorporate phrase embeddings into the model, which significantly improves the parsing accuracy. Finally, we extend our first-order model to a secondorder model that exploits interactions between two adjacent dependency edges as in McDonald and Pereira (2006) thus further improves the model performance. We evaluate our models on the English Penn Treebank. Experiment results show that both our first-order and second-order models outperform the corresponding conventional models. 2 Neural Network Model A dependency tree </context>
<context position="6866" citStr="McDonald et al., 2005" startWordPosition="1073" endWordPosition="1076">ct analysis for x. However, the size of Y (x) is exponential large, which makes it impractical to solve equation (1) directly. Previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010) assumes that the score of ˆy(x) factors through the scores of subgraphs c of ˆy(x) so that efficient algorithms can be designed for decoding: Score(x, ˆy(x); 0) = � ScoreF(x, c; 0) (2) CEˆ�(X) Figure 1 gives two examples of commonly used factorization strategy proposed by Mcdonald et.al (2005) and Mcdonald and Pereira (2006). The simplest subgraph uses a first-order factorization (McDonald et al., 2005) which decomposes a dependency tree into single dependency arcs (Figure 1(a)). Based on the first-order model, secondorder factorization (McDonald and Pereira, 2006) (Figure 1(b)) brings sibling information into decoding. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies and s and m are successive modifiers to the same side of h. The most common choice for ScoreF(x, c; 0), which is the score function for subgraph c in the tree, is a simple linear function: ScoreF(x, c; 0) = w · f(x, c) (3) where f(x, c) is the feature representation</context>
<context position="10121" citStr="McDonald et al., 2005" startWordPosition="1624" endWordPosition="1627">Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be useful in graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006). Given a sentence x, the context for h and m includes three context parts: prefix, infix and suffix, as illustrated in Figure 3. We call these parts phrases in our work. Context representation in conventional models are limited: First, phrases cannot be used as features directly because of the data sparseness problem. Therefore, phrases are backed off to low-order representation such as bigrams and trigrams. For example, Mcdonald et.al (2005) used tri-gram features of infix between head-modifier pair (h, m). Sometimes even tri-grams are expensive to use, which is </context>
<context position="12649" citStr="McDonald et al., 2005" startWordPosition="2023" endWordPosition="2026">d to the following hidden layer. Phrase embeddings provide panorama representation of the context, allowing our model to capture richer context information compared with the back-off tri-gram representation. Moreover, as a distributed representation, phrase embeddings perform generalization over specific phrases, thus better capture the syntactic and semantic information than back-off tri-grams. 2.3 Direction-specific Transformation In dependency representation of sentence, the edge direction indicates which one of the words is the head h and which one is the modifier m. Unlike previous work (McDonald et al., 2005; McDonald and Pereira, 2006) that models the edge direction as feature to be conjoined with other features, we model the edge direction with directionspecific transformation. As shown in Figure 2, the parameters in hidden layer (Whd,bdh) and the output layer (Wod, bdo) are bound with index d E {0, 1} which indicates the direction between head and modifier (0 for left arc and 1 for right arc). In this way, the model can learn direction-specific parameters and automatically capture the interactions between edge direction and other features. 2.4 Learning Feature Combination The key to the succes</context>
<context position="21706" citStr="McDonald et al., 2005" startWordPosition="3571" endWordPosition="3574"> as test set. The Stanford POS Tagger (Toutanova et al., 2003) with ten-way jackknifing of the training data is used for assigning POS tags (accuracy ≈ 97.2%). Hyper-parameters of our models are tuned on the development set and their final settings are as follows: embedding size d = 50, hidden layer (Layer 2) size = 200, regularization parameter A = 10−4, discount parameter for margin loss n = 0.3, initial learning rate of AdaGrad alpha = 0.1. 4.2 Experiment Results Table 2 compares our models with several conventional graph-based parsers. We use MSTParser2 for conventional first-order model (McDonald et al., 2005) and second-order model (McDonald and Pereira, 2006). We also include the result of a third-order model of Koo and Collins (2010) for comparison3. For our models, we report the results with and without unsupervised pre-training. Pretraining only trains the word-based feature embeddings on Gigaword corpus (Graff et al., 2003) using word2vec4 and all other parameters are still initialized randomly. In all experiments, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation5 is excluded in all evaluation metrics. The parsing speeds are measured on a worksta</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 91–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="9542" citStr="Mikolov et al., 2013" startWordPosition="1524" endWordPosition="1527"> unigrams, which are less likely to be sparse. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are transformed into their corresponding distributed representations (feature embeddings). The idea of distributed representation for symbolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar embeddings which capture the syntactic and semantic information behind features (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be useful in graph-based models (McDonald et al., 2005; McDonald and Pereir</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Baobao Chang</author>
</authors>
<title>Maxmargin tensor neural network for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>293--303</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="9582" citStr="Pei et al., 2014" startWordPosition="1532" endWordPosition="1535">se. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are transformed into their corresponding distributed representations (feature embeddings). The idea of distributed representation for symbolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar embeddings which capture the syntactic and semantic information behind features (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be useful in graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006). Given a sentence x, the contex</context>
<context position="14755" citStr="Pei et al., 2014" startWordPosition="2374" endWordPosition="2377">ional nonlinear transformation in most neural networks. The cube extension is added to enhance the ability to capture complex interactions between input features. Intuitively, the cube term in each hidden unit directly models feature combinations in a multiplicative way: (w1a1 + w2a2 + ... + wnan + b)3 = � �(wiwjwk)aiajak + b(wiwj)aiaj... i,j,k i,j These feature combinations are hand-designed in conventional graph-based models but our model learns these combinations automatically and encodes them in the model parameters. Similar ideas were also proposed in previous works (Socher et al., 2013; Pei et al., 2014; Chen and Manning, 2014). Socher et.al (2013) and Pei et.al (2014) used a tensor-based activation function to learn feature combinations. However, tensor-based transformation is quite slow even with tensor factorization (Pei et al., 2014). Chen and Manning (2014) proposed to use cube function g(l) = l3 which inspires our tanh-cube function. Compared with cube function, tanh-cube has three advantages: • The cube function is unbounded, making the activation output either too small or too big if the norm of input l is not properly controlled, especially in deep neural network. On the 316 contrar</context>
<context position="23507" citStr="Pei et al., 2014" startWordPosition="3851" endWordPosition="3854">eighboors Words in the, of, and, (word2vec) for, from his himself, her, he, him, father which its, essentially, similar, that, also Words in on, at, behind, (Our model) among, during his her, my, their, its, he which where, who, whom, whose, though POS-tags NN NNPS, NNS, EX, NNP, POS JJ JJR, JJS, PDT, RBR, RBS Table 4: Examples of similar words and POS-tags according to feature embeddings. rand and 2-order-phrase-rand perform better than conventional models in MSTParser. Pretraining further improves the performance of all three models, which is consistent with the conclusion of previous work (Pei et al., 2014; Chen and Manning, 2014). Moreover, 1-order-phrase performs better than 1-order-atomic, which shows that phrase embeddings do improve the model. 2- order-phrase further improves the performance because of the more expressive second-order factorization. All three models perform significantly better than their counterparts in MSTParser where millions of features are used and 1-order-phrase works surprisingly well that it even beats the conventional second-order model. With regard to parsing speed, 1-order-atomic is the fastest while other two models have similar speeds as MSTParser. Further spe</context>
</contexts>
<marker>Pei, Ge, Chang, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Maxmargin tensor neural network for chinese word segmentation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 293–303, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Anthony Rousseau</author>
<author>Mohammed Attik</author>
</authors>
<title>Large, pruned or continuous space language models on a gpu for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9520" citStr="Schwenk et al., 2012" startWordPosition="1520" endWordPosition="1523">d unigrams and POS-tag unigrams, which are less likely to be sparse. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are transformed into their corresponding distributed representations (feature embeddings). The idea of distributed representation for symbolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar embeddings which capture the syntactic and semantic information behind features (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be useful in graph-based models (McDonald et al., 200</context>
</contexts>
<marker>Schwenk, Rousseau, Attik, 2012</marker>
<rawString>Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 11–19. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="9563" citStr="Socher et al., 2013" startWordPosition="1528" endWordPosition="1531">ess likely to be sparse. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are transformed into their corresponding distributed representations (feature embeddings). The idea of distributed representation for symbolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar embeddings which capture the syntactic and semantic information behind features (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f E D is represented as a realvalued vector (feature embedding) Embed(f) E Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M E Rd×|D|. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). 2.2 Phrase Embeddings Context information of word pairs1 such as the dependency pair (h, m) has been widely believed to be useful in graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006). Given a sen</context>
<context position="14737" citStr="Socher et al., 2013" startWordPosition="2370" endWordPosition="2373">e same as the conventional nonlinear transformation in most neural networks. The cube extension is added to enhance the ability to capture complex interactions between input features. Intuitively, the cube term in each hidden unit directly models feature combinations in a multiplicative way: (w1a1 + w2a2 + ... + wnan + b)3 = � �(wiwjwk)aiajak + b(wiwj)aiaj... i,j,k i,j These feature combinations are hand-designed in conventional graph-based models but our model learns these combinations automatically and encodes them in the model parameters. Similar ideas were also proposed in previous works (Socher et al., 2013; Pei et al., 2014; Chen and Manning, 2014). Socher et.al (2013) and Pei et.al (2014) used a tensor-based activation function to learn feature combinations. However, tensor-based transformation is quite slow even with tensor factorization (Pei et al., 2014). Chen and Manning (2014) proposed to use cube function g(l) = l3 which inspires our tanh-cube function. Compared with cube function, tanh-cube has three advantages: • The cube function is unbounded, making the activation output either too small or too big if the norm of input l is not properly controlled, especially in deep neural network. </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21146" citStr="Toutanova et al., 2003" startWordPosition="3479" endWordPosition="3482">ated in the same way as infix between head-modifier pair (h, m) (i.e., hm infix) in Section 2.2 except that the word pair is now s and m. For cases where no sibling information is available, the corresponding sibling-related embeddings are set to zero vector. 4 Experiments 4.1 Experiment Setup We use the English Penn Treebank (PTB) to evaluate our model implementations and Yamada and Matsumoto (2003) head rules are used to extract dependency trees. We follow the standard splits of PTB3, using section 2-21 for training, section 22 as development set and 23 as test set. The Stanford POS Tagger (Toutanova et al., 2003) with ten-way jackknifing of the training data is used for assigning POS tags (accuracy ≈ 97.2%). Hyper-parameters of our models are tuned on the development set and their final settings are as follows: embedding size d = 50, hidden layer (Layer 2) size = 200, regularization parameter A = 10−4, discount parameter for margin loss n = 0.3, initial learning rate of AdaGrad alpha = 0.1. 4.2 Experiment Results Table 2 compares our models with several conventional graph-based parsers. We use MSTParser2 for conventional first-order model (McDonald et al., 2005) and second-order model (McDonald and Pe</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<volume>3</volume>
<pages>195--206</pages>
<contexts>
<context position="20926" citStr="Yamada and Matsumoto (2003)" startWordPosition="3441" endWordPosition="3444">es we use in our second-order model. Sibling node and its local context are used as additional atomic features. We also used the infix embedding for the infix between sibling pair (s, m), which we call sm infix. It is calculated in the same way as infix between head-modifier pair (h, m) (i.e., hm infix) in Section 2.2 except that the word pair is now s and m. For cases where no sibling information is available, the corresponding sibling-related embeddings are set to zero vector. 4 Experiments 4.1 Experiment Setup We use the English Penn Treebank (PTB) to evaluate our model implementations and Yamada and Matsumoto (2003) head rules are used to extract dependency trees. We follow the standard splits of PTB3, using section 2-21 for training, section 22 as development set and 23 as test set. The Stanford POS Tagger (Toutanova et al., 2003) with ten-way jackknifing of the training data is used for assigning POS tags (accuracy ≈ 97.2%). Hyper-parameters of our models are tuned on the development set and their final settings are as follows: embedding size d = 50, hidden layer (Layer 2) size = 200, regularization parameter A = 10−4, discount parameter for margin loss n = 0.3, initial learning rate of AdaGrad alpha =</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT, volume 3, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1518" citStr="Zhang and Nivre, 2011" startWordPosition="214" endWordPosition="217">phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors ∗Corresponding author which are fed into a linear model to learn the f</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>