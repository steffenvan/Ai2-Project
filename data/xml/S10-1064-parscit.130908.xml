<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.042735">
<title confidence="0.9708555">
CityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives within
Context
</title>
<author confidence="0.984588">
Bin LU and Benjamin K. TSOU
</author>
<affiliation confidence="0.998009">
Department of Chinese, Translation and Linguistics &amp;
Language Information Sciences Research Centre
City University of Hong Kong
</affiliation>
<email confidence="0.998608">
{lubin2010, rlbtsou}@gmail.com
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937230769231">
This paper describes our system
participating in task 18 of SemEval-2010,
i.e. disambiguating Sentiment-
Ambiguous Adjectives (SAAs). To
disambiguating SAAs, we compare the
machine learning-based and lexicon-
based methods in our submissions: 1)
Maximum entropy is used to train
classifiers based on the annotated
Chinese data from the NTCIR opinion
analysis tasks, and the clause-level and
sentence-level classifiers are compared;
2) For the lexicon-based method, we first
classify the adjectives into two classes:
intensifiers (i.e. adjectives intensifying
the intensity of context) and suppressors
(i.e. adjectives decreasing the intensity of
context), and then use the polarity of
context to get the SAAs’ contextual
polarity based on a sentiment lexicon.
The results show that the performance of
maximum entropy is not quite high due
to little training data; on the other hand,
the lexicon-based method could improve
the precision by considering the polarity
of context.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927169811321">
In recent years, sentiment analysis, which mines
opinions from information sources such as news,
blogs, and product reviews, has drawn much
attention in the NLP field (Hatzivassiloglou and
McKeown, 1997; Pang et al., 2002; Turney,
2002; Hu and Liu, 2004; Pang and Lee, 2008). It
has many applications such as social media
monitoring, market research, and public
relations.
Some adjectives are neutral in sentiment
polarity out of context, but they could show
positive, neutral or negative meaning within
specific context. Such words can be called
dynamic sentiment-ambiguous adjectives
(SAAs). However, SAAs have not been
intentionally tackled in the researches of
sentiment analysis, and usually have been
discarded or ignored by most previous work. Wu
et al., (2008) presents an approach of combining
collocation information and SVM to
disambiguate SAAs, in which the collocation-
based method was first used to disambiguate
adjectives within the context of collocation (i.e. a
sub-sentence marked by comma), and then the
SVM algorithm was explored for those instances
not covered by the collocation-based method.
According to their experiments, their supervised
algorithm achieves encouraging performance.
The task 18 at SemEval-2010 is intended to
create a benchmark dataset for disambiguating
SAAs. Given only 100 trial sentences, but not
provided with any official training data,
participants are required to tackle this problem
data by unsupervised approaches or use their
own training data. The task consists of 14 SAAs,
which are all high-frequency words in Mandarin
Chinese. They are )C|big, i1N|small, J�|many, �iJ
|few, A|high, 9|low, l*-|thick, 41|thin, M|deep,
&amp;shallow, Ift|heavy, 轻|light, V_)C|huge, Ift)C
|grave. This task deals with Chinese SAAs, but
the disambiguating techniques should be
language-independent. Please refer to (Wu and
Jin, 2010) for more descriptions of the task.
In our participating system, the annotated
Chinese data from the NTCIR opinion analysis
tasks is used as training data with the help of a
combined sentiment lexicon. A machine
learning-based method (namely maximum
entropy) and the lexicon-based method are
compared in our submissions. The results show
that the performance of maximum entropy is not
quite high due to little training data; on the other
hand, the lexicon-based method could improve
</bodyText>
<page confidence="0.958279">
292
</page>
<bodyText confidence="0.908257777777778">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 292–295,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
the precision by considering the context of
SAAs. In Section 2, we briefly describe data
preparation of sentiment lexicon and training
data. Our approaches for disambiguating SAAs
are given in Section 3. The experiment and
results are presented in Section 4, followed by a
conclusion in Section 5.
</bodyText>
<sectionHeader confidence="0.963527" genericHeader="method">
2 Data Preparation
</sectionHeader>
<subsectionHeader confidence="0.996472">
2.1 Sentiment Lexicon
</subsectionHeader>
<bodyText confidence="0.996399055555556">
Several traditional Chinese resources of polar
words/phrases are collected, including NTU
Sentiment Dictionary1, The Lexicon of Chinese
Positive Words (Shi and Zhu, 2006), The Lexicon
of Chinese Negative Words (Yang and Zhu, 2006)
0, and CityU’s sentiment-bearing word/phrase
list (Lu et al, 2008), which were manually
marked in the political news data by trained
annotators (Benjamin and Lu, 2008). Sentiment-
bearing items marked with the SENTIMENT_KW
tag (SKPI), including only positive and negative
items but not neutral ones, were also
automatically extracted from the Chinese sample
data of NTCIR-6 OAPT (Seki et al., 2007). All
these polar item lexicons were combined, and the
combined polar item lexicon consists of 13,437
positive items and 18,365 negative items, a total
of 31,802 items.
</bodyText>
<subsectionHeader confidence="0.99953">
2.2 Training Data
</subsectionHeader>
<bodyText confidence="0.999930363636364">
The training data is extracted from the Chinese
sample and test data from the NTCIR opinion
analysis task, including NTCIR-6 (Seki et al.,
2007), NTCIR-7 (Seki et al., 2008) and NTCIR-8
(Seki et al., 2010). The NTCIR opinion analysis
tasks provide an opportunity to evaluate the
techniques used by different participants based
on a common evaluation framework in Chinese
(simplified and traditional), Japanese and
English.
For data from NTCIR-6 and NTCIR-7, three
annotators manually marked the polarity of each
opinionated sentence, and the lenient polarity is
used here as the gold standard (please refer to
Seki et al., 2008 for explanation of lenient and
strict standard). For each opinionated sentence
from NTCIR-8, only two annotators marked and
the strict polarity is used as the gold standard.
The traditional Chinese sentences are transferred
into simplified Chinese. In total, there are about
12K opinionated sentences annotated with
polarity, out of which about 9K are marked as
</bodyText>
<footnote confidence="0.790233">
1 http://nlg18.csie.ntu.edu.tw:8080/opinion/index.html
</footnote>
<bodyText confidence="0.988179344827586">
positive or negative, and others neutral. All the
9K sentences plus the 100 sentences from the
trial data are used as the sentence-level training
data.
Meanwhile, we also try to get the clause-level
training data since the context of collocation
within sub-sentences are quite crucial for
disambiguating SAAs according to Wu et al.
(2008). From the 9K positive/ negative sentences
above, we automatically extract the clause for
each occurrence of SAAs.
Note the polarity for a whole sentence is not
necessarily the same with that of the clause
containing SAAs. Consider the sentence 在 当前
的 世界 大 格局 中 , 中俄 两国 相互 支持
(In the current large circumstance of the world,
China and Russia support each other). The
polarity of the whole sentence is positive, while
the clause 在当前的世界大格局中(In the current
large circumstance of the world) containing a
SAA 大 (large) is neutral, and the polarity lies in
the second part of the whole sentence, i.e. 相互
支持 (support each other).
Thus, we manually checked the polarity of
clauses containing SAAs. Due to time limitation,
we only checked 465 clauses. Plus the clauses
extracted from 100 trial sentences, the final
clause-level training data consist of 565
positive/negative clauses containing SAAs.
</bodyText>
<sectionHeader confidence="0.988971" genericHeader="method">
3 Our Approach for Disambiguating
SAAs
</sectionHeader>
<bodyText confidence="0.999972333333333">
To disambiguating SAAs, we use the maximum
entropy algorithm and the sentiment lexicon-
based method, and also combine them together.
</bodyText>
<subsectionHeader confidence="0.99541">
3.1 The Maximum Entropy-based Method
</subsectionHeader>
<bodyText confidence="0.999930181818182">
Maximum entropy classification (MaxEnt) is a
technique which has proven effective in a
number of natural language processing
applications (Berger et al., 1996). Le Zhang’s
maximum entropy tool2 is used for classification.
The Chinese sentences are segmented into
words using a production segmentation system.
Unigrams of words are used as basic features for
classification. Bigrams are also tried, but does
not show improvement, and thus are not
described in details here.
</bodyText>
<subsectionHeader confidence="0.999671">
3.2 The Lexicon-based Method
</subsectionHeader>
<bodyText confidence="0.987071">
For the lexicon-based method, we first classify
the 14 adjectives into two classes: intensifiers
</bodyText>
<footnote confidence="0.96175">
2 http:// homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html
</footnote>
<page confidence="0.998809">
293
</page>
<bodyText confidence="0.999975636363636">
and suppressors. Intensifiers refer to adjectives
intensifying the intensity of context, including 大
|big, 多 |many, 高 |high, 厚 |thick, 深 |deep, 重
|heavy, 巨大|huge, 重大|grave, while suppressors
refer to adjectives decreasing the intensity of
context, including 小|small, 少|few, 低|low, 薄
|thin, 浅|shallow, V|light.
Meanwhile, the collocation nouns are also
classified into two classes: positive and negative.
Positive nouns include 素 � |quality, fill 准
|standard, 水 平 |level, 效 益 |benefit, 成 就
|achievement, etc. Negative nouns include �力
|pressure, 差距 |disparity, f a7ig |problem, 1AL,�
|risk, % 1染|pollution etc.
The hypothesis here is that intensifiers will
receive the polarity of their collocations while
suppressors will get the opposite polarity of their
collocations. For example, 成 就 |achievement
could be collocated with one of the following
intensifiers: 大|big, 多|many or 高|high, and the
adjectives just receive the polarity of 成 就
|achievement, which is positive. Meanwhile, % 1
染|pollution could be collocated with one of the
following suppressors: 小|small, 少|few, 低|low,
and the adjectives just receive the opposite
polarity of % 1染|pollution, which is also positive.
Based on this hypothesis, we could get the
polarity of SAAs through theirs collocation
nouns within the clauses containing SAAs. The
context of SAAs is a sub-sentence marked by
comma. The sentiment lexicon mentioned in
Section 2.1 is used to find polarity of collocation
nouns.
</bodyText>
<subsectionHeader confidence="0.975714">
3.3 Combining Maximum Entropy and
Lexicon
</subsectionHeader>
<bodyText confidence="0.999941571428571">
To combine the two methods above, the lexicon-
based method is first used to disambiguate the
sentiment of SAAs, and the context of
collocation is a sub-sentence marked by comma.
Then for those instances that are not covered by
the lexicon-based method, the maximum entropy
algorithm is explored.
</bodyText>
<sectionHeader confidence="0.991488" genericHeader="evaluation">
4 Experiment and Results
</sectionHeader>
<bodyText confidence="0.998564090909091">
The dataset contains two parts: some sentences
were extracted from Chinese Gigaword (LDC
corpus: LDC2005T14), and other sentences were
gathered through the search engine like Google.
Firstly, these sentences were automatically
segmented and POS-tagged, and then the
ambiguous adjectives were manually annotated
with the correct sentiment polarity within the
sentence context. Two annotators annotated the
sentences double blindly, and the third annotator
checks the annotation. All the data of 2,917
sentences is provided as the test set, and
evaluation is performed in terms of micro
accuracy and macro accuracy.
We submitted 4 runs: run 1 is based on the
sentence-level MaxEnt classifier; run 2 on the
clause-level MaxEnt classifier; run 3 is got by
combining the lexicon-based method and the
sentence-level MaxEnt classifier; and run 4 by
combining the lexicon-based method and the
clause-level MaxEnt classifier. The official
scores for the 4 runs are shown in Table 2.
</bodyText>
<tableCaption confidence="0.998706">
Table 2. Results of 4 Runs
</tableCaption>
<table confidence="0.9432295">
Run Micro Acc. (%) Macro Acc. (%)
1 61.98 67.89
2 62.63 60.85
3 71.55 75.54
4 72.47 69.80
From Table 2, we can observe that:
</table>
<listItem confidence="0.718044">
1) Compared the highest scores achieved by
other teams, the performance of maximum
entropy (run 1 and 2) is not quite high due to
little training data;
2) By integrating the lexicon-based method
and maximum entropy (run 3 and 4), we improve
the accuracy by considering the context of SAAs;
</listItem>
<bodyText confidence="0.998628470588235">
3) The sentence-level maximum entropy
classifier shows better macro accuracy, and
clause-level one better micro accuracy.
In addition to the official scores, we also
evaluate the performance of the lexicon-based
method alone. The micro and macro accuracy are
respectively 0.847 and 0.835665, showing that
the lexicon-based method is more accurate than
the maximum entropy algorithm (run 1 and 2).
But it only covers 1,436 (49%) of 2,917 test
instances.
Because the data from the NTCIR opinion
analysis task is not specifically annotated for this
task, and the manually checked clauses are less
than 600, the performance of our system is not
quite high compared to the highest performance
achieved by other teams.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.997959">
To disambiguating SAAs, we compare machine
learning-based and lexicon-based methods in our
submissions: 1) Maximum entropy is used to
train classifiers based on the annotated Chinese
data from the NTCIR opinion analysis tasks, and
the clause-level and sentence-level classifiers are
</bodyText>
<page confidence="0.992007">
294
</page>
<bodyText confidence="0.99989325">
compared; 2) For the lexicon-based method, we
first classify the adjectives into two classes:
intensifiers (i.e. adjectives intensifying the
intensity of context) and suppressors (i.e.
adjectives decreasing the intensity of context),
and then use the polarity of context to get the
SAAs’ contextual polarity. The results show that
the performance of maximum entropy is not
quite high due to little training data; on the other
hand, the lexicon-based method could improve
the precision by considering the context of
SAAs.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821682539683">
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy
approach to natural language processing.
Computational Linguistics, 22(1):39-71.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of
Adjectives. Proceedings of ACL-97. 174-181.
Minqing Hu and Bing Liu. 2004. Mining Opinion
Features in Customer Reviews. In Proceedings of
the 19th National Conference on Artificial
Intelligence, pp. 755-760.
Bin Lu, Benjamin K. Tsou and Oi Yee Kwong. 2008.
Supervised Approaches and Ensemble Techniques
for Chinese Opinion Analysis at NTCIR-7. In
Proceedings of the Seventh NTCIR Workshop
(NTCIR-7). pp. 218-225. Tokyo, Japan.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis, Foundations and Trends in
Information Retrieval, Now Publishers.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of
EMNLP 2002, pp.79–86.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-His Chen, Noriko Kando. 2007. Overview of
Opinion Analysis Pilot Task at NTCIR-6. Proc. of
the Seventh NTCIR Workshop. Japan. 2007.6.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-His Chen, Noriko Kando and Chin-Yew Lin.
2008. Overview of Multilingual Opinion Analysis
Task at NTCIR-7. Proc. of the Seventh NTCIR
Workshop. Japan. Dec. 2008.
Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-His Chen,
Noriko Kando. 2010. Overview of Multilingual
Opinion Analysis Task at NTCIR-8. Proc. of the
Seventh NTCIR Workshop. Japan. June, 2010.
Jilin Shi and Yinggui Zhu. 2006. The Lexicon of
Chinese Positive Words (* U NU A). Sichuan
Lexicon Press.
Benjamin K. Tsou and Bin Lu. 2008. A Political
News Corpus in Chinese for Opinion Analysis. In
Proceedings of the Second International Workshop
on Evaluating Information Access (EVIA2008). pp.
6-7. Tokyo, Japan.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised
classification of reviews, In Proceedings of ACL-
02, Philadelphia, Pennsylvania, 417-424.
Yunfang Wu, Miao Wang, Peng Jin and Shiwen Yu.
2008. Disambiguate sentiment ambiguous
adjectives. In Proceedings of IEEE International
Conference on Natural Language Processing and
Knowledge Engineering (NLP-KE’08).
Yunfang Wu, and Peng Jin. 2010. SemEval-2010
Task 18: Disambiguate sentiment ambiguous
adjectives. In Proceedings of SemEval-2010.
Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2008.
Coarse-Fine Opinion Mining - WIA in NTCIR-7
MOAT Task. In Proceedings of the Seventh NTCIR
Workshop (NTCIR-7). Tokyo, Japan, Dec. 16-19.
Ling Yang and Yinggui Zhu. 2006. The Lexicon of
Chinese Negative Words (0- UMP A). Sichuan
Lexicon Press.
</reference>
<page confidence="0.998564">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473695">
<title confidence="0.9917975">CityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives within Context</title>
<author confidence="0.999303">Bin LU</author>
<author confidence="0.999303">Benjamin K TSOU</author>
<affiliation confidence="0.851739666666667">Department of Chinese, Translation and Linguistics &amp; Language Information Sciences Research Centre City University of Hong Kong</affiliation>
<email confidence="0.977087">lubin2010@gmail.com</email>
<email confidence="0.977087">rlbtsou@gmail.com</email>
<abstract confidence="0.995650407407408">This paper describes our system participating in task 18 of SemEval-2010, disambiguating Ambiguous Adjectives (SAAs). To disambiguating SAAs, we compare the machine learning-based and lexiconbased methods in our submissions: 1) Maximum entropy is used to train classifiers based on the annotated Chinese data from the NTCIR opinion analysis tasks, and the clause-level and sentence-level classifiers are compared; 2) For the lexicon-based method, we first classify the adjectives into two classes: intensifiers (i.e. adjectives intensifying the intensity of context) and suppressors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="7577" citStr="Berger et al., 1996" startWordPosition="1138" endWordPosition="1141">ly checked the polarity of clauses containing SAAs. Due to time limitation, we only checked 465 clauses. Plus the clauses extracted from 100 trial sentences, the final clause-level training data consist of 565 positive/negative clauses containing SAAs. 3 Our Approach for Disambiguating SAAs To disambiguating SAAs, we use the maximum entropy algorithm and the sentiment lexiconbased method, and also combine them together. 3.1 The Maximum Entropy-based Method Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al., 1996). Le Zhang’s maximum entropy tool2 is used for classification. The Chinese sentences are segmented into words using a production segmentation system. Unigrams of words are used as basic features for classification. Bigrams are also tried, but does not show improvement, and thus are not described in details here. 3.2 The Lexicon-based Method For the lexicon-based method, we first classify the 14 adjectives into two classes: intensifiers 2 http:// homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 293 and suppressors. Intensifiers refer to adjectives intensifying the intensity of context, inclu</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Predicting the Semantic Orientation of Adjectives.</title>
<date>1997</date>
<booktitle>Proceedings of ACL-97.</booktitle>
<pages>174--181</pages>
<contexts>
<context position="1457" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="203" endWordPosition="206">the intensity of context) and suppressors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen McKeown. 1997. Predicting the Semantic Orientation of Adjectives. Proceedings of ACL-97. 174-181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining Opinion Features in Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 19th National Conference on Artificial Intelligence,</booktitle>
<pages>755--760</pages>
<contexts>
<context position="1508" citStr="Hu and Liu, 2004" startWordPosition="213" endWordPosition="216"> the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining collocation information and SVM to disambiguate SA</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining Opinion Features in Customer Reviews. In Proceedings of the 19th National Conference on Artificial Intelligence, pp. 755-760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Benjamin K Tsou</author>
<author>Oi Yee Kwong</author>
</authors>
<title>Supervised Approaches and Ensemble Techniques for Chinese Opinion Analysis at NTCIR-7.</title>
<date>2008</date>
<booktitle>In Proceedings of the Seventh NTCIR Workshop (NTCIR-7).</booktitle>
<pages>218--225</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="4419" citStr="Lu et al, 2008" startWordPosition="645" endWordPosition="648">idering the context of SAAs. In Section 2, we briefly describe data preparation of sentiment lexicon and training data. Our approaches for disambiguating SAAs are given in Section 3. The experiment and results are presented in Section 4, followed by a conclusion in Section 5. 2 Data Preparation 2.1 Sentiment Lexicon Several traditional Chinese resources of polar words/phrases are collected, including NTU Sentiment Dictionary1, The Lexicon of Chinese Positive Words (Shi and Zhu, 2006), The Lexicon of Chinese Negative Words (Yang and Zhu, 2006) 0, and CityU’s sentiment-bearing word/phrase list (Lu et al, 2008), which were manually marked in the political news data by trained annotators (Benjamin and Lu, 2008). Sentimentbearing items marked with the SENTIMENT_KW tag (SKPI), including only positive and negative items but not neutral ones, were also automatically extracted from the Chinese sample data of NTCIR-6 OAPT (Seki et al., 2007). All these polar item lexicons were combined, and the combined polar item lexicon consists of 13,437 positive items and 18,365 negative items, a total of 31,802 items. 2.2 Training Data The training data is extracted from the Chinese sample and test data from the NTCIR</context>
</contexts>
<marker>Lu, Tsou, Kwong, 2008</marker>
<rawString>Bin Lu, Benjamin K. Tsou and Oi Yee Kwong. 2008. Supervised Approaches and Ensemble Techniques for Chinese Opinion Analysis at NTCIR-7. In Proceedings of the Seventh NTCIR Workshop (NTCIR-7). pp. 218-225. Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<publisher>Now Publishers.</publisher>
<contexts>
<context position="1529" citStr="Pang and Lee, 2008" startWordPosition="217" endWordPosition="220">context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining collocation information and SVM to disambiguate SAAs, in which the coll</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval, Now Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP 2002,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1476" citStr="Pang et al., 2002" startWordPosition="207" endWordPosition="210">ssors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining collocation inform</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP 2002, pp.79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-His Chen Le Sun</author>
</authors>
<title>Noriko Kando.</title>
<date>2007</date>
<booktitle>Overview of Opinion Analysis Pilot Task at NTCIR-6. Proc. of the Seventh NTCIR Workshop.</booktitle>
<marker>Seki, Evans, Ku, Le Sun, 2007</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-His Chen, Noriko Kando. 2007. Overview of Opinion Analysis Pilot Task at NTCIR-6. Proc. of the Seventh NTCIR Workshop. Japan. 2007.6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-His Chen Le Sun</author>
</authors>
<title>Noriko Kando and Chin-Yew Lin.</title>
<date>2008</date>
<booktitle>Overview of Multilingual Opinion Analysis Task at NTCIR-7. Proc. of the Seventh NTCIR Workshop. Japan.</booktitle>
<marker>Seki, Evans, Ku, Le Sun, 2008</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-His Chen, Noriko Kando and Chin-Yew Lin. 2008. Overview of Multilingual Opinion Analysis Task at NTCIR-7. Proc. of the Seventh NTCIR Workshop. Japan. Dec. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>Lun-Wei Ku</author>
<author>Hsin-His Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<date>2010</date>
<booktitle>Overview of Multilingual Opinion Analysis Task at NTCIR-8. Proc. of the Seventh NTCIR Workshop. Japan.</booktitle>
<marker>Seki, Ku, Le Sun, Kando, 2010</marker>
<rawString>Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-His Chen, Noriko Kando. 2010. Overview of Multilingual Opinion Analysis Task at NTCIR-8. Proc. of the Seventh NTCIR Workshop. Japan. June, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jilin Shi</author>
<author>Yinggui Zhu</author>
</authors>
<date>2006</date>
<journal>The Lexicon of Chinese Positive Words (* U NU A). Sichuan</journal>
<publisher>Lexicon Press.</publisher>
<contexts>
<context position="4292" citStr="Shi and Zhu, 2006" startWordPosition="625" endWordPosition="628"> ACL 2010, pages 292–295, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics the precision by considering the context of SAAs. In Section 2, we briefly describe data preparation of sentiment lexicon and training data. Our approaches for disambiguating SAAs are given in Section 3. The experiment and results are presented in Section 4, followed by a conclusion in Section 5. 2 Data Preparation 2.1 Sentiment Lexicon Several traditional Chinese resources of polar words/phrases are collected, including NTU Sentiment Dictionary1, The Lexicon of Chinese Positive Words (Shi and Zhu, 2006), The Lexicon of Chinese Negative Words (Yang and Zhu, 2006) 0, and CityU’s sentiment-bearing word/phrase list (Lu et al, 2008), which were manually marked in the political news data by trained annotators (Benjamin and Lu, 2008). Sentimentbearing items marked with the SENTIMENT_KW tag (SKPI), including only positive and negative items but not neutral ones, were also automatically extracted from the Chinese sample data of NTCIR-6 OAPT (Seki et al., 2007). All these polar item lexicons were combined, and the combined polar item lexicon consists of 13,437 positive items and 18,365 negative items,</context>
</contexts>
<marker>Shi, Zhu, 2006</marker>
<rawString>Jilin Shi and Yinggui Zhu. 2006. The Lexicon of Chinese Positive Words (* U NU A). Sichuan Lexicon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin K Tsou</author>
<author>Bin Lu</author>
</authors>
<title>A Political News Corpus in Chinese for Opinion Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Information Access (EVIA2008).</booktitle>
<pages>6--7</pages>
<location>Tokyo, Japan.</location>
<marker>Tsou, Lu, 2008</marker>
<rawString>Benjamin K. Tsou and Bin Lu. 2008. A Political News Corpus in Chinese for Opinion Analysis. In Proceedings of the Second International Workshop on Evaluating Information Access (EVIA2008). pp. 6-7. Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews,</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="1490" citStr="Turney, 2002" startWordPosition="211" endWordPosition="212">ves decreasing the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining collocation information and SVM </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews, In Proceedings of ACL02, Philadelphia, Pennsylvania, 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunfang Wu</author>
<author>Miao Wang</author>
<author>Peng Jin</author>
<author>Shiwen Yu</author>
</authors>
<title>Disambiguate sentiment ambiguous adjectives.</title>
<date>2008</date>
<booktitle>In Proceedings of IEEE International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE’08).</booktitle>
<contexts>
<context position="2023" citStr="Wu et al., (2008)" startWordPosition="290" endWordPosition="293">n the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining collocation information and SVM to disambiguate SAAs, in which the collocationbased method was first used to disambiguate adjectives within the context of collocation (i.e. a sub-sentence marked by comma), and then the SVM algorithm was explored for those instances not covered by the collocation-based method. According to their experiments, their supervised algorithm achieves encouraging performance. The task 18 at SemEval-2010 is intended to create a benchmark dataset for disambiguating SAAs. Given only 100 trial sentences, but not provided with any official</context>
<context position="6319" citStr="Wu et al. (2008)" startWordPosition="936" endWordPosition="939">rity is used as the gold standard. The traditional Chinese sentences are transferred into simplified Chinese. In total, there are about 12K opinionated sentences annotated with polarity, out of which about 9K are marked as 1 http://nlg18.csie.ntu.edu.tw:8080/opinion/index.html positive or negative, and others neutral. All the 9K sentences plus the 100 sentences from the trial data are used as the sentence-level training data. Meanwhile, we also try to get the clause-level training data since the context of collocation within sub-sentences are quite crucial for disambiguating SAAs according to Wu et al. (2008). From the 9K positive/ negative sentences above, we automatically extract the clause for each occurrence of SAAs. Note the polarity for a whole sentence is not necessarily the same with that of the clause containing SAAs. Consider the sentence 在 当前 的 世界 大 格局 中 , 中俄 两国 相互 支持 (In the current large circumstance of the world, China and Russia support each other). The polarity of the whole sentence is positive, while the clause 在当前的世界大格局中(In the current large circumstance of the world) containing a SAA 大 (large) is neutral, and the polarity lies in the second part of the whole sentence, i.e. 相互 支持</context>
</contexts>
<marker>Wu, Wang, Jin, Yu, 2008</marker>
<rawString>Yunfang Wu, Miao Wang, Peng Jin and Shiwen Yu. 2008. Disambiguate sentiment ambiguous adjectives. In Proceedings of IEEE International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunfang Wu</author>
<author>Peng Jin</author>
</authors>
<title>SemEval-2010 Task 18: Disambiguate sentiment ambiguous adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2010.</booktitle>
<contexts>
<context position="3118" citStr="Wu and Jin, 2010" startWordPosition="448" endWordPosition="451"> to create a benchmark dataset for disambiguating SAAs. Given only 100 trial sentences, but not provided with any official training data, participants are required to tackle this problem data by unsupervised approaches or use their own training data. The task consists of 14 SAAs, which are all high-frequency words in Mandarin Chinese. They are )C|big, i1N|small, J�|many, �iJ |few, A|high, 9|low, l*-|thick, 41|thin, M|deep, &amp;shallow, Ift|heavy, 轻|light, V_)C|huge, Ift)C |grave. This task deals with Chinese SAAs, but the disambiguating techniques should be language-independent. Please refer to (Wu and Jin, 2010) for more descriptions of the task. In our participating system, the annotated Chinese data from the NTCIR opinion analysis tasks is used as training data with the help of a combined sentiment lexicon. A machine learning-based method (namely maximum entropy) and the lexicon-based method are compared in our submissions. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve 292 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 292–295, Uppsala, Sweden, 1</context>
</contexts>
<marker>Wu, Jin, 2010</marker>
<rawString>Yunfang Wu, and Peng Jin. 2010. SemEval-2010 Task 18: Disambiguate sentiment ambiguous adjectives. In Proceedings of SemEval-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifeng Xu</author>
<author>Kam-Fai Wong</author>
<author>Yunqing Xia</author>
</authors>
<title>Coarse-Fine Opinion Mining - WIA</title>
<date>2008</date>
<booktitle>in NTCIR-7 MOAT Task. In Proceedings of the Seventh NTCIR Workshop (NTCIR-7).</booktitle>
<pages>16--19</pages>
<location>Tokyo, Japan,</location>
<marker>Xu, Wong, Xia, 2008</marker>
<rawString>Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2008. Coarse-Fine Opinion Mining - WIA in NTCIR-7 MOAT Task. In Proceedings of the Seventh NTCIR Workshop (NTCIR-7). Tokyo, Japan, Dec. 16-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling Yang</author>
<author>Yinggui Zhu</author>
</authors>
<title>The Lexicon of Chinese Negative Words (0- UMP A). Sichuan</title>
<date>2006</date>
<publisher>Lexicon Press.</publisher>
<contexts>
<context position="4352" citStr="Yang and Zhu, 2006" startWordPosition="635" endWordPosition="638"> c�2010 Association for Computational Linguistics the precision by considering the context of SAAs. In Section 2, we briefly describe data preparation of sentiment lexicon and training data. Our approaches for disambiguating SAAs are given in Section 3. The experiment and results are presented in Section 4, followed by a conclusion in Section 5. 2 Data Preparation 2.1 Sentiment Lexicon Several traditional Chinese resources of polar words/phrases are collected, including NTU Sentiment Dictionary1, The Lexicon of Chinese Positive Words (Shi and Zhu, 2006), The Lexicon of Chinese Negative Words (Yang and Zhu, 2006) 0, and CityU’s sentiment-bearing word/phrase list (Lu et al, 2008), which were manually marked in the political news data by trained annotators (Benjamin and Lu, 2008). Sentimentbearing items marked with the SENTIMENT_KW tag (SKPI), including only positive and negative items but not neutral ones, were also automatically extracted from the Chinese sample data of NTCIR-6 OAPT (Seki et al., 2007). All these polar item lexicons were combined, and the combined polar item lexicon consists of 13,437 positive items and 18,365 negative items, a total of 31,802 items. 2.2 Training Data The training dat</context>
</contexts>
<marker>Yang, Zhu, 2006</marker>
<rawString>Ling Yang and Yinggui Zhu. 2006. The Lexicon of Chinese Negative Words (0- UMP A). Sichuan Lexicon Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>