<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9982035">
A Chinese Efficient Analyser Integrating Word Segmentation,
Part-Of-Speech Tagging, Partial Parsing and Full Parsing
</title>
<author confidence="0.961542">
GuoDong ZHOU
</author>
<affiliation confidence="0.965559">
Institute for Infocomm Research
</affiliation>
<address confidence="0.984256">
21 Heng Mui Keng Terrace
Singapore, 119613
</address>
<email confidence="0.98716">
zhougd@i2r.a-star.edu.sg
</email>
<author confidence="0.966126">
Jian SU
</author>
<affiliation confidence="0.973143">
Institute for Infocomm Research
</affiliation>
<address confidence="0.9808795">
21 Heng Mui Keng Terrace
Singapore, 119613
</address>
<email confidence="0.581288">
sujian@ i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.99327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999864590909091">
This paper introduces an efficient analyser for
the Chinese language, which efficiently and
effectively integrates word segmentation,
part-of-speech tagging, partial parsing and full
parsing. The Chinese efficient analyser is based
on a Hidden Markov Model (HMM) and an
HMM-based tagger. That is, all the
components are based on the same
HMM-based tagging engine. One advantage of
using the same single engine is that it largely
decreases the code size and makes the
maintenance easy. Another advantage is that it
is easy to optimise the code and thus improve
the speed while speed plays a critical important
role in many applications. Finally, the
performances of all the components can benefit
from the optimisation of existing algorithms
and/or adoption of better algorithms to a single
engine. Experiments show that all the
components can achieve state-of-art
performances with high efficiency for the
Chinese language.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973259259259">
Traditionally, a text parser outputs a complete parse
tree for each input sentence, achieving a speed in
the order of 10 words per second (wps) (Abney
1997). However, for many applications like text
mining, a parse tree is not necessary and a speed of
10 wps is unacceptable when we have to process
millions of words in thousands of documents in a
reasonable time (Feldman 1997). Therefore, there is
a compromise between speed and performance in
many applications.
The objective of this paper is to develop an
efficient analyser for the Chinese language,
exploring different intermediate forms, achieving a
target speed in the region of 1,000 wps for full
parsing, 2,000 wps for partial parsing and 10,000
wps for word segmentation and part-of-speech
tagging, with state-of-art performances.
The layout of this paper is as follows. Section 2
describes the Chinese efficient analyser. Section 3
presents the HMM and the HMM-based tagger.
Sections 4 and 5 describe the applications of the
HMM-based tagger in integrated word
segmentation and part-of-speech tagging, partial
parsing, and full parsing respectively. Section 6
gives the experimental results. Finally, some
conclusions are drawn with possible extensions of
future work in section 7.
</bodyText>
<sectionHeader confidence="0.989941" genericHeader="method">
2 Chinese Efficient Analyser
</sectionHeader>
<bodyText confidence="0.994519285714286">
The Chinese efficient analyser can be described by
the example as shown in Figure 1. Here, &amp;quot;.&amp;quot; in
Figure 1 means that the current node has not been
chunked till now. For convenience, it is regarded as
a &amp;quot;special chunk&amp;quot; in this paper and others as
&amp;quot;normal chunks&amp;quot;. Therefore, every node in Figure 1
can be represented as a 3tuple ci (pi, wi), where ci
</bodyText>
<listItem confidence="0.8184825">
is the i -th chunk in the input chunk sequence and
• is the head word of c and is the POS
</listItem>
<equation confidence="0.972518">
wi pi
i
ci ≠ . (c is a normal chunk).
i
</equation>
<bodyText confidence="0.80711">
In this case, we call node c i (pi , wi) a normal
chunk node.
</bodyText>
<figure confidence="0.887613952380953">
tag of w when
i
Full Parsing = N levels of Partial Parsing
(N = Parsing Depth and for Figure 1, N=3)
S(VB, 存在)
rd
3 -level 3tuple sequence
3rd-level Partial Parsing
NP(NN, 国家) VP(VB, 存在) 2nd-level 3tuple sequence
2nd-level Partial Parsing
.(ADJ,发达) .(NN,国家) .(ADV,也) .(VB,存在) .(ADJ,许多) .(NN,问题)
NP(NN, 国家) .(ADV,也) .(VB,存在) NP(NN, 问题)
1st-level PartialParsing
1st-level 3tuple sequence
0th-level 3tuple
sequence
Integrated Word Segmentation
and POS Tagging
发 达 国 家 也 存 在 许 多 问 题
developed country also exist many problem
(English Translation: There also exists many problems in developed countries)
</figure>
<figureCaption confidence="0.96383">
Figure 1: An Example Sentence “发达国家也存在许多问题” of the Chinese Efficient Analyser
</figureCaption>
<listItem confidence="0.910749">
• is just the word linked with c and is
</listItem>
<equation confidence="0.82956">
wi pi
i
</equation>
<bodyText confidence="0.815081866666667">
the POS tag of when (c is a special
wi ci =. i
chunk). In this case, we call node c (pi, wi ) a
i
special chunk or POS node.
Figure 1 shows that, sequentially from bottom
to top,
1) Given a Chinese sentence (e.g. “发达国家也存在
许多问题”), it is segmented and tagged into a
sequence of special chunk, POS and word
3tuples (.(ADJ, 发 达 ) .(NN, 国 家 ) .(ADV,
也) .(VB, 存在) .(ADJ, 许多) .(NN, 问题) ) via the
integrated word segmentation and POS tagging.
In this paper, this resulting 3tuple sequence is
called &amp;quot;0th-level 3tuple sequence&amp;quot;.
</bodyText>
<listItem confidence="0.980236041666667">
2) The 0th-level 3tuple sequence is then chunked
into 1st-level 3tuple sequence (NP(NN, 国
家) .(ADV, 也) .(VB, 存在) NP(NN, 问题)) via
1st-level partial parsing, while POS nodes .(ADJ,
发达) and .(NN, 国家) are chunked into a normal
chunk node NP(NN, 国 家 ), and POS
nodes .(ADJ, 许多) .(NN, 问题) into NP(NN, 问
题).
3) The 1st-level 3tuple sequence is further chunked
into 2nd-level 3tuple sequence (NP(NN, 国家)
VP(VB, 存 在 )) via 2nd-level partial parsing,
while mixed POS and normal chunk
nodes .(ADV, 也) .(VB, 存在) NP(NN, 问题) are
chunked into a normal chunk node VP(VB, 存
在).
4) Finally, 2nd-level 3tuple sequence is chunked
into 3rd-level 3tuple sequence (S(VB, 存在)) via
3rd-level partial parsing, while normal chunk
nodes NP(NN, 国 家) and VP(VB, 存 在) are
chunked into a normal chunk node S(VB, 存在).
5) In this way, full parsing is completed with a
fully parsed tree after several levels (3 in the
example of Figure 1) of cascaded partial
parsing.
</listItem>
<sectionHeader confidence="0.984702" genericHeader="method">
3 HMM-based Tagger
</sectionHeader>
<bodyText confidence="0.938712666666667">
The Chinese efficient analyser is based on the
HMM-based tagger described in Zhou et al 2000a.
Given a token sequence G n = g1g2 gn , the goal
</bodyText>
<equation confidence="0.90928275">
1
of tagging is to find a stochastic optimal tag
sequence T1n = t1t2 t that maximizes
n
</equation>
<bodyText confidence="0.902239">
By assuming mutual information
independence:
</bodyText>
<equation confidence="0.998265666666667">
n
MI T n Gn
( 1 , 1 ) = ( , 1 ) or
n
i
∑= MI t G
i 1
n
log P(T1n  |G1n) log P(T1n) −∑log P(ti )
n
∑ log P(ti
1
</equation>
<bodyText confidence="0.999288888888889">
Both the first and second items correspond to
the language model component of the tagger. We
will not discuss these two items further in this paper
since they are well studied in ngram modeling. This
paper will focus on the third item
, which is the main difference
between our tagger and other HMM-based taggers.
Ideally, it can be estimated by using the
forward-backward algorithm (Rabiner 1989)
recursively for the first-order (Rabiner 1989) or
second-order HMMs (Watson et al 1992). To
simplify the complexity, several context dependent
approximations on it will be attempted in this paper
instead, as detailed in sections 3 and 4.
All of this modelling would be for naught were
it not for the existence of an efficient algorithm for
finding the optimal state sequence, thereby
&amp;quot;decoding&amp;quot; the original sequence of tags. The
stochastic optimal tag sequence can be found by
maximizing the previous equation over all the
possible tag sequences. This is implemented via the
well-known Viterbi algorithm (Viterbi 1967) by
using dynamic programming and an appropriate
merging of multiple theories when they converge
on a particular state. Since we are interested in
recovering the tag state sequence, we pursue 16
theories at every given step of the algorithm.
</bodyText>
<sectionHeader confidence="0.758931" genericHeader="method">
4 Word Segmentation and POS Tagging
</sectionHeader>
<bodyText confidence="0.999859375">
Traditionally, in Chinese Language Processing,
word segmentation and POS tagging are
implemented sequentially. That is, the input
Chinese sentence is segmented into words first and
then the segmented result (in the form of word
lattice or N-best word sequences) is passed to POS
tagging component. However, this processing
strategy has following disadvantages:
</bodyText>
<listItem confidence="0.97886725">
• The word lexicons used in word segmentation
and POS tagging may be different. This
difference is difficult to overcome and largely
drops the system accuracy although different
optimal algorithms may be applied to word
segmentation and POS tagging.
• With speed in consideration, the two-stage
processing strategy is not efficient.
</listItem>
<bodyText confidence="0.9991993">
Therefore, we apply the strategy of integrating
word segmentation and POS tagging in a single
stage. This can be implemented as follows:
1) Given an input sentence, a 3tuple (special
chunk, POS and word) lattice is generated by
skimming the sentence from left-to-right, and
looking up the word and POS lexicon to
determine all the possible words and get POS
tag probability distribution for each possible
word.
</bodyText>
<equation confidence="0.924800642857143">
log P(T1n  |G;) = log P(T1n) +log P(T n)⋅P(G;
)
)
P T G
( ,
n n
1 1
lo ) _ log P(ti, G1
g P(T n) ⋅ P(G1 P(ti )
we have:
P(Tn�GI
P(Gn
1
)
i
=
1
n G1 )
i
=
n
∑=
i 1
log
P(ti  |Gi )
2) Viterbi algorithm is applied to decode the middle of a chunk and &amp;quot;3&amp;quot; means that the
3tuple lattice to find the most possible POS tag current stuple is at the end of a chunk.
sequence.
</equation>
<bodyText confidence="0.993011222222222">
3) In this way, the given sentence is segmented
into words with POS tags.
The rationale behind the above algorithm is the
ability of HMM in parallel segmentation and
classification (Rabiner 1989).
In order to overcome the coarse n-gram models
raised by the limited number of orignial POS tags
used in current Chinese POS tag bank (corpus), a
word clustering algorithm (Bai et al 1998) is applied
to classify words into classes first and then the N
(e.g. N=500) most frequently occurred word class
and POS pairs are added to the original POS tag set
to achieve more accurate models. For example,
ADJ(&lt; iq: -4; &gt;) represents a special POS tag ADJ
which pairs with the word class &lt;iq:-4;&gt;. Here, &lt;iq:-4;&gt;
is a word class label. For convenience and clarity,
we use the most frequently occurred word in a word
class as the label to represent the word class.
</bodyText>
<sectionHeader confidence="0.918196" genericHeader="method">
5 Partial Parsing and Full Parsing
</sectionHeader>
<bodyText confidence="0.863089684210526">
As discussed in section 2, obviously partial parsing
can have different levels and full parsing can be
achieved by cascading several levels of partial
parsing (e.g. 3 levels of cascaded partial parsing can
achieve full parsing for the example as shown in
Figure 1).
In this paper, a certain level (e.g. l -th level) of
partial parsing is implemented via a chunking
model, built on the HMM-based tagger as described
in section 2, with ( -th level 3tuple sequence
l − 1)
as input. That is, for the l -th level partial parsing,
the chunking model has the ( -th level 3tuple
l − 1)
sequence G1n = g1g2 ... gn (Here, 3tuple
) as input. In the meantime, chunk
tag t used in the chunking model is structural and
i
consists of following three parts:
</bodyText>
<listItem confidence="0.960592941176471">
• Boundary Category B: It is a set of four values
0, 1, 2, 3, where &amp;quot;0&amp;quot; means that the current
3tuple is a whole chunk, &amp;quot;1&amp;quot; means that the
current 3tuple is at the beginning of a chunk,
&amp;quot;2&amp;quot; means that the current 3tuple is in the
• Chunk Category C: It is used to denote the
output chunk category of the chunking model,
which includes normal chunks and the special
chunk (&amp;quot;.&amp;quot;). The reason to include the special
chunk is that some of POS 3tuple in the input
sequence may not be chunked in the current
chunking stage.
• POS Category POS: Because of the limited
number in boundary category and output chunk
category, the POS category is added into the
structural tag to represent more accurate
models.
</listItem>
<equation confidence="0.90401625">
Therefore, can be represented by
ti
bi _ ci _ posi , where b is the boundary type of
i
, is the output chunk type of t and is
ci posi
i
the POS type of t . Obviously, there exist some
i
constraints between t and on the boundary
ti
i− 1
</equation>
<bodyText confidence="0.997756666666667">
categories and output chunk categories, as briefed
in table 1, where &amp;quot;valid&amp;quot;/&amp;quot;invalid&amp;quot; means the chunk
tag sequence t is valid/invalid while &amp;quot;validon&amp;quot;
</bodyText>
<equation confidence="0.58655575">
i −1ti
means is valid on the condition
t i−1ti
0 1 2 3
</equation>
<table confidence="0.97780975">
0 Valid Valid Invalid Invalid
1 Invalid Invalid Valid on Validon
2 Invalid Invalid Valid Valid
3 Valid Valid Invalid Invalid
</table>
<tableCaption confidence="0.949576">
Table 1: Constraints between ti− 1 and ti
(Column: bi− 1 in ti− 1; Row: bi in ti )
</tableCaption>
<bodyText confidence="0.890529">
For the example as shown in Figure 1, we can
see that:
</bodyText>
<listItem confidence="0.982437833333333">
• In the 1st-level partial parsing, the input 3tuple
sequence is the 0th-level 3tuple sequence .(ADJ,
yAiL) .(NN, M*) .(ADV, -tb) .(VB, 4;f�-L) .(ADJ, iq:
-4;) .(NN, fri M) and the output tag sequence
1_NP_ADJ 3_NP_NN 0_._ADV 0_._VB
1_NP_ADJ 3_NP_NN, from where derived is
</listItem>
<equation confidence="0.767876083333333">
,
)
wi
gi = ci
(pi
ti
−1
= .
ci
ci
the 1st-level 3tuple sequence NP(NN, 国
家) .(ADV, 也) .(VB, 存在) NP(NN, 问题).
</equation>
<listItem confidence="0.96661335">
• In the 2nd-level partial parsing, the input 3tuple
sequence is the 1st-level 3tuple sequence
NP(NN, 国家) .(ADV, 也) .(VB, 存在) NP(NN, 问
题 ) and the output tag sequence 0_NP_NN
1_VP_ADV 2_VP_VB 3_VP_NN, from where
derived is the 2nd-level 3tuple sequence NP(NN,
国家) VP(VB, 存在).
• In the 3rd-level partial parsing, the input 3tuple
sequence is the 2nd-level 3tuple sequence
NP(NN, 国家) VP(VB, 存在) and the output tag
sequence 1_S_NN 3_S_VB, from where
derived is the 3rd-level 3tuple sequence S(VB, 存
在). In this way, a fully parsed tree is reached.
• In the cascaded chunking procedure, necessary
information is stored for back-tracing.
Partially/fully parsed trees can be constructed
by tracing from the final 3tuple sequence back
to 0th-level 3tuple sequence. Different levels of
partial parsing can be achieved according to the
need of the application.
</listItem>
<sectionHeader confidence="0.990173" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.990249791666667">
The Chinese efficient analyser is implemented in
C++, providing a rapid and easy
code-compile-train-test development cycle. In fact,
many NLP systems suffer from a lack of software
and computer-science engineering effort: running
efficiency is key to performing numerous
experiments, which, in turn, is key to improving
performance. A system may have excellent
performance on a given task, but if it takes long to
compile and/or run on test data, the rate of
improvement of that system will be contrained
compared to that which can run very efficiently.
Moreover, speed plays a critical role in many
applications such as text mining.
All the experiments are implemented on a
Pentium II/450MHZ PC. All the performances are
measured in precisions, recalls and F-measures.
Here, the precision (P) measures the number of
correct units in the answer file over the total number
of units in the answer file and the recall (R)
measures the number of correct units in the answer
file over the total number of units in the key file
while F-measure is the weighted harmonic mean of
precision and recall: F = (β 2 + 1)RP with
</bodyText>
<equation confidence="0.494865">
β2R+P
</equation>
<bodyText confidence="0.739594">
2=1.
</bodyText>
<subsectionHeader confidence="0.9994">
6.1 Word Segmentation and POS Tagging
</subsectionHeader>
<bodyText confidence="0.998571125">
Table 2 shows the integrated word segmentation
and POS tagging results on the Chinese tag bank
PFR1.0 of 3.69M Chinese characters (1.12 Chinese
Words) developed by Institute of Computational
Linguistics at Beijing Univ. Here, 80% of the
corpus is used as formal training data, another 10%
as development data and remaining 10% as formal
test data.
</bodyText>
<table confidence="0.99973075">
Function P R F Speed
Word Segment. 97.5 98.2 97.8 11,000
wps
POS Tagging 93.5 94.1 93.8
</table>
<tableCaption confidence="0.971015">
Table 2: Performances of Word Segmentation
and POS Tagging (wps: words per second)
</tableCaption>
<bodyText confidence="0.999795272727273">
The word segmentation corresponds to
bracketing of the chunking model while POS
tagging corresponds to bracketing and labelling.
Table 2 shows that recall (P) is higher than
precision (P). The main reason may be the existence
of unknown words. In the Chinese efficient
analyser, unknown words are segmented into
individual Chinese characters. This makes the
number of segmented words/POS tagged words in
the system output higher than that in the correct
answer.
</bodyText>
<subsectionHeader confidence="0.999891">
6.2 Partial Parsing and Full Parsing
</subsectionHeader>
<bodyText confidence="0.999955428571428">
Table 3 shows the results of 1st-level partial parsing
and full parsing, using the PARSEVAL evaluation
methodology (Black et al 1991) on the UPENN
Chinese Tree Bank of 100k words developed by
Univ. of Penn. Here, 80% of the corpus is used as
formal training data, another 10% as development
data and remaining 10% as formal test data.
</bodyText>
<table confidence="0.999106333333333">
Function P R F Speed
Partial Parsing 85.1 82.5 83.8 4500 wps
Full Parsing 77.1 70.3 73.7 2100 wps
</table>
<tableCaption confidence="0.9847835">
Table 3: Performances of 1st-level Partial Parsing
and Full Parsing (wps: words per second)
</tableCaption>
<bodyText confidence="0.988195">
β
Table 3 shows that the performances of partial
parsing and full parsing are quite low, compared to
those of state-of-art partial parsing and full parsing
for the English language (Zhou et al 2000a; Collins
1997). The main reason behind is the small size of
the training corpus used in our experiments.
However, the Chinese PENN Tree Bank is the
largest corpus we can find for partial parsing and
full parsing. Therefore, developing a much larger
Chinese tree bank (comparable to UPENN English
Tree Bank) becomes an urgent task for the Chinese
language processing community. Actually, the best
individual system (Zhou et al 2000b) in
CoNLL’2000 chunking shared task for the English
language (Tjong et al 2000) used the same
HMM-based tagging engine.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998240857142857">
This paper presents an efficient analyser for the
Chinese language, based on a HMM and a single
engine -- HMM-based tagger. Experiments show
that the analyser achieves state-of-art performance
at very high speed, which can meet the requirement
of speed-critical applications such as text mining.
Our future work includes:
</bodyText>
<listItem confidence="0.900623333333333">
• Syntactic analysis of the partial/full parsing
results into a meaningful intermediate form.
• Research and development of Chinese named
entity recognition using the same HMM-based
tagger and its integration to the Chinese
efficient analyser.
</listItem>
<sectionHeader confidence="0.996505" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9965415">
Thanks go to Institute of Computational Linguistics
at Beijing Univ. and LDC at Univ. of Penn. for free
research use of their Chinese Tag Bank and Chinese
Tree Bank.
</bodyText>
<sectionHeader confidence="0.999559" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908148148148">
Abney S. 1997. Part-of-Speech Tagging and Partial
Parsing. Corpus-based Methods in Natural
Language Processing. Edited by Steve Young
and Gerrit Bloothooft. Kluwer Academic
Publishers, Dordrecht.
Bai ShuanHu, Li HaiZhou, Lin ZhiWei and Yuan
BaoSheng. 1998. Building class-based language
models with contextual statistics. Proceedings of
International Conference on Acoustics, Speech
and Signal Processing (ICASSP&apos;1998).
pages173-176. Seattle, Washington, USA.
Black E. and Abney S. 1991. A Procedure for
Quantitatively Comparing the Syntactic Coverage
of English Grammars. Proceedings of DRAPA
workshop on Speech and Natural Language.
pages306-311. Pacific Grove, CA. DRAPA.
Collins M.J. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. Proceedings of the
Thirtieth-Five Annual Meeting of the Association
for Computational Linguistics (ACL’97).
pages184-191.
Feldman R. 1997. Text Mining - Theory and
Practice. Proceedings of the Third International
Conference on Knowledge Discovery &amp; Data
Mining (KDD’1997).
Rabiner L. 1989. A Tutorial on Hidden Markov
Models and Selected Applications in Speech
Recognition. IEEE 77(2), pages257-285.
Tjong K.S. Erik and Buchholz S. 2000. Introduction
to the CoNLL-2000 Shared Task: Chunking.
Proceedings of the Conference on Computational
Language Learning (CoNLL&apos;2000).
Pages127-132. Lisbon, Portugal. 11-14 Sept.
Viterbi A.J. 1967. Error Bounds for Convolutional
Codes and an Asymptotically Optimum Decoding
Algorithm. IEEE Transactions on Information
Theory, IT 13(2), 260-269.
Watson B. and Tsoi A Chunk. 1992. Second order
Hidden Markov Models for speech recognition.
Proceeding of the Fourth Australian
International Conference on Speech Science and
Technology. pages146-151.
Zhou GuoDong and Su Jian. 2000a. Error-driven
HMM-based Chunk Tagger with
Context-dependent Lexicon. Proceedings of the
Joint Conference on Empirical Methods on
Natural Language Processing and Very Large
Corpus (EMNLP/ VLC&apos;2000). Hong Kong, 7-8
Oct.
Zhou GuoDong, Su Jian and Tey TongGuan.
2000b. Hybrid Text Chunking. Proceedings of
the Conference on Computational Language
Learning (CoNLL&apos;2000). Pages163-166. Lisbon,
Portugal, 11-14 Sept.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.234364">
<title confidence="0.9960145">A Chinese Efficient Analyser Integrating Word Part-Of-Speech Tagging, Partial Parsing and Full Parsing</title>
<author confidence="0.398356">GuoDong</author>
<affiliation confidence="0.698567">Institute for Infocomm</affiliation>
<address confidence="0.7520185">21 Heng Mui Keng Singapore,</address>
<author confidence="0.709302">Jian SU</author>
<affiliation confidence="0.999935">Institute for Infocomm Research</affiliation>
<address confidence="0.972854">21 Heng Mui Keng Terrace Singapore, 119613</address>
<abstract confidence="0.992146652173913">This paper introduces an efficient analyser for the Chinese language, which efficiently and effectively integrates word segmentation, part-of-speech tagging, partial parsing and full parsing. The Chinese efficient analyser is based on a Hidden Markov Model (HMM) and an HMM-based tagger. That is, all the components are based on the same HMM-based tagging engine. One advantage of using the same single engine is that it largely decreases the code size and makes the maintenance easy. Another advantage is that it is easy to optimise the code and thus improve the speed while speed plays a critical important role in many applications. Finally, the performances of all the components can benefit from the optimisation of existing algorithms and/or adoption of better algorithms to a single engine. Experiments show that all the components can achieve state-of-art performances with high efficiency for the Chinese language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Part-of-Speech Tagging and Partial Parsing. Corpus-based Methods in Natural Language Processing. Edited by Steve Young and Gerrit Bloothooft.</title>
<date>1997</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1443" citStr="Abney 1997" startWordPosition="212" endWordPosition="213"> Another advantage is that it is easy to optimise the code and thus improve the speed while speed plays a critical important role in many applications. Finally, the performances of all the components can benefit from the optimisation of existing algorithms and/or adoption of better algorithms to a single engine. Experiments show that all the components can achieve state-of-art performances with high efficiency for the Chinese language. 1 Introduction Traditionally, a text parser outputs a complete parse tree for each input sentence, achieving a speed in the order of 10 words per second (wps) (Abney 1997). However, for many applications like text mining, a parse tree is not necessary and a speed of 10 wps is unacceptable when we have to process millions of words in thousands of documents in a reasonable time (Feldman 1997). Therefore, there is a compromise between speed and performance in many applications. The objective of this paper is to develop an efficient analyser for the Chinese language, exploring different intermediate forms, achieving a target speed in the region of 1,000 wps for full parsing, 2,000 wps for partial parsing and 10,000 wps for word segmentation and part-of-speech taggi</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Abney S. 1997. Part-of-Speech Tagging and Partial Parsing. Corpus-based Methods in Natural Language Processing. Edited by Steve Young and Gerrit Bloothooft. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bai ShuanHu</author>
<author>Li HaiZhou</author>
<author>Lin ZhiWei</author>
<author>Yuan BaoSheng</author>
</authors>
<title>Building class-based language models with contextual statistics.</title>
<date>1998</date>
<booktitle>Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;1998).</booktitle>
<pages>173--176</pages>
<location>Seattle, Washington, USA.</location>
<marker>ShuanHu, HaiZhou, ZhiWei, BaoSheng, 1998</marker>
<rawString>Bai ShuanHu, Li HaiZhou, Lin ZhiWei and Yuan BaoSheng. 1998. Building class-based language models with contextual statistics. Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP&apos;1998). pages173-176. Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
</authors>
<title>A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars.</title>
<date>1991</date>
<booktitle>Proceedings of DRAPA workshop on Speech and Natural Language.</booktitle>
<pages>306--311</pages>
<publisher>DRAPA.</publisher>
<location>Pacific Grove, CA.</location>
<marker>Black, Abney, 1991</marker>
<rawString>Black E. and Abney S. 1991. A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars. Proceedings of DRAPA workshop on Speech and Natural Language. pages306-311. Pacific Grove, CA. DRAPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>Proceedings of the Thirtieth-Five Annual Meeting of the Association for Computational Linguistics (ACL’97).</booktitle>
<pages>184--191</pages>
<contexts>
<context position="15661" citStr="Collins 1997" startWordPosition="2701" endWordPosition="2702">al 1991) on the UPENN Chinese Tree Bank of 100k words developed by Univ. of Penn. Here, 80% of the corpus is used as formal training data, another 10% as development data and remaining 10% as formal test data. Function P R F Speed Partial Parsing 85.1 82.5 83.8 4500 wps Full Parsing 77.1 70.3 73.7 2100 wps Table 3: Performances of 1st-level Partial Parsing and Full Parsing (wps: words per second) β Table 3 shows that the performances of partial parsing and full parsing are quite low, compared to those of state-of-art partial parsing and full parsing for the English language (Zhou et al 2000a; Collins 1997). The main reason behind is the small size of the training corpus used in our experiments. However, the Chinese PENN Tree Bank is the largest corpus we can find for partial parsing and full parsing. Therefore, developing a much larger Chinese tree bank (comparable to UPENN English Tree Bank) becomes an urgent task for the Chinese language processing community. Actually, the best individual system (Zhou et al 2000b) in CoNLL’2000 chunking shared task for the English language (Tjong et al 2000) used the same HMM-based tagging engine. 7 Conclusion This paper presents an efficient analyser for the</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins M.J. 1997. Three Generative, Lexicalised Models for Statistical Parsing. Proceedings of the Thirtieth-Five Annual Meeting of the Association for Computational Linguistics (ACL’97). pages184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Feldman</author>
</authors>
<title>Text Mining - Theory and Practice.</title>
<date>1997</date>
<booktitle>Proceedings of the Third International Conference on Knowledge Discovery &amp; Data Mining (KDD’1997).</booktitle>
<contexts>
<context position="1665" citStr="Feldman 1997" startWordPosition="251" endWordPosition="252"> optimisation of existing algorithms and/or adoption of better algorithms to a single engine. Experiments show that all the components can achieve state-of-art performances with high efficiency for the Chinese language. 1 Introduction Traditionally, a text parser outputs a complete parse tree for each input sentence, achieving a speed in the order of 10 words per second (wps) (Abney 1997). However, for many applications like text mining, a parse tree is not necessary and a speed of 10 wps is unacceptable when we have to process millions of words in thousands of documents in a reasonable time (Feldman 1997). Therefore, there is a compromise between speed and performance in many applications. The objective of this paper is to develop an efficient analyser for the Chinese language, exploring different intermediate forms, achieving a target speed in the region of 1,000 wps for full parsing, 2,000 wps for partial parsing and 10,000 wps for word segmentation and part-of-speech tagging, with state-of-art performances. The layout of this paper is as follows. Section 2 describes the Chinese efficient analyser. Section 3 presents the HMM and the HMM-based tagger. Sections 4 and 5 describe the application</context>
</contexts>
<marker>Feldman, 1997</marker>
<rawString>Feldman R. 1997. Text Mining - Theory and Practice. Proceedings of the Third International Conference on Knowledge Discovery &amp; Data Mining (KDD’1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.</title>
<date>1989</date>
<journal>IEEE</journal>
<volume>77</volume>
<issue>2</issue>
<pages>257--285</pages>
<contexts>
<context position="6083" citStr="Rabiner 1989" startWordPosition="1040" endWordPosition="1041">tochastic optimal tag sequence T1n = t1t2 t that maximizes n By assuming mutual information independence: n MI T n Gn ( 1 , 1 ) = ( , 1 ) or n i ∑= MI t G i 1 n log P(T1n |G1n) log P(T1n) −∑log P(ti ) n ∑ log P(ti 1 Both the first and second items correspond to the language model component of the tagger. We will not discuss these two items further in this paper since they are well studied in ngram modeling. This paper will focus on the third item , which is the main difference between our tagger and other HMM-based taggers. Ideally, it can be estimated by using the forward-backward algorithm (Rabiner 1989) recursively for the first-order (Rabiner 1989) or second-order HMMs (Watson et al 1992). To simplify the complexity, several context dependent approximations on it will be attempted in this paper instead, as detailed in sections 3 and 4. All of this modelling would be for naught were it not for the existence of an efficient algorithm for finding the optimal state sequence, thereby &amp;quot;decoding&amp;quot; the original sequence of tags. The stochastic optimal tag sequence can be found by maximizing the previous equation over all the possible tag sequences. This is implemented via the well-known Viterbi algo</context>
<context position="8645" citStr="Rabiner 1989" startWordPosition="1475" endWordPosition="1476"> tag probability distribution for each possible word. log P(T1n |G;) = log P(T1n) +log P(T n)⋅P(G; ) ) P T G ( , n n 1 1 lo ) _ log P(ti, G1 g P(T n) ⋅ P(G1 P(ti ) we have: P(Tn�GI P(Gn 1 ) i = 1 n G1 ) i = n ∑= i 1 log P(ti |Gi ) 2) Viterbi algorithm is applied to decode the middle of a chunk and &amp;quot;3&amp;quot; means that the 3tuple lattice to find the most possible POS tag current stuple is at the end of a chunk. sequence. 3) In this way, the given sentence is segmented into words with POS tags. The rationale behind the above algorithm is the ability of HMM in parallel segmentation and classification (Rabiner 1989). In order to overcome the coarse n-gram models raised by the limited number of orignial POS tags used in current Chinese POS tag bank (corpus), a word clustering algorithm (Bai et al 1998) is applied to classify words into classes first and then the N (e.g. N=500) most frequently occurred word class and POS pairs are added to the original POS tag set to achieve more accurate models. For example, ADJ(&lt; iq: -4; &gt;) represents a special POS tag ADJ which pairs with the word class &lt;iq:-4;&gt;. Here, &lt;iq:-4;&gt; is a word class label. For convenience and clarity, we use the most frequently occurred word </context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner L. 1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. IEEE 77(2), pages257-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tjong K S Erik</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>Proceedings of the Conference on Computational Language Learning (CoNLL&apos;2000). Pages127-132.</booktitle>
<pages>11--14</pages>
<location>Lisbon,</location>
<marker>Erik, Buchholz, 2000</marker>
<rawString>Tjong K.S. Erik and Buchholz S. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. Proceedings of the Conference on Computational Language Learning (CoNLL&apos;2000). Pages127-132. Lisbon, Portugal. 11-14 Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory, IT</journal>
<volume>13</volume>
<issue>2</issue>
<pages>260--269</pages>
<contexts>
<context position="6703" citStr="Viterbi 1967" startWordPosition="1137" endWordPosition="1138">ively for the first-order (Rabiner 1989) or second-order HMMs (Watson et al 1992). To simplify the complexity, several context dependent approximations on it will be attempted in this paper instead, as detailed in sections 3 and 4. All of this modelling would be for naught were it not for the existence of an efficient algorithm for finding the optimal state sequence, thereby &amp;quot;decoding&amp;quot; the original sequence of tags. The stochastic optimal tag sequence can be found by maximizing the previous equation over all the possible tag sequences. This is implemented via the well-known Viterbi algorithm (Viterbi 1967) by using dynamic programming and an appropriate merging of multiple theories when they converge on a particular state. Since we are interested in recovering the tag state sequence, we pursue 16 theories at every given step of the algorithm. 4 Word Segmentation and POS Tagging Traditionally, in Chinese Language Processing, word segmentation and POS tagging are implemented sequentially. That is, the input Chinese sentence is segmented into words first and then the segmented result (in the form of word lattice or N-best word sequences) is passed to POS tagging component. However, this processing</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi A.J. 1967. Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm. IEEE Transactions on Information Theory, IT 13(2), 260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Watson</author>
<author>Tsoi A Chunk</author>
</authors>
<title>Second order Hidden Markov Models for speech recognition.</title>
<date>1992</date>
<booktitle>Proceeding of the Fourth Australian International Conference on Speech Science and Technology.</booktitle>
<pages>146--151</pages>
<marker>Watson, Chunk, 1992</marker>
<rawString>Watson B. and Tsoi A Chunk. 1992. Second order Hidden Markov Models for speech recognition. Proceeding of the Fourth Australian International Conference on Speech Science and Technology. pages146-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
</authors>
<title>Error-driven HMM-based Chunk Tagger with Context-dependent Lexicon.</title>
<date>2000</date>
<booktitle>Proceedings of the Joint Conference on Empirical Methods on Natural Language Processing and Very Large Corpus (EMNLP/ VLC&apos;2000). Hong Kong,</booktitle>
<pages>7--8</pages>
<marker>GuoDong, Jian, 2000</marker>
<rawString>Zhou GuoDong and Su Jian. 2000a. Error-driven HMM-based Chunk Tagger with Context-dependent Lexicon. Proceedings of the Joint Conference on Empirical Methods on Natural Language Processing and Very Large Corpus (EMNLP/ VLC&apos;2000). Hong Kong, 7-8 Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
<author>Tey TongGuan</author>
</authors>
<title>Hybrid Text Chunking.</title>
<date>2000</date>
<booktitle>Proceedings of the Conference on Computational Language Learning (CoNLL&apos;2000). Pages163-166.</booktitle>
<location>Lisbon,</location>
<marker>GuoDong, Jian, TongGuan, 2000</marker>
<rawString>Zhou GuoDong, Su Jian and Tey TongGuan. 2000b. Hybrid Text Chunking. Proceedings of the Conference on Computational Language Learning (CoNLL&apos;2000). Pages163-166. Lisbon, Portugal, 11-14 Sept.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>