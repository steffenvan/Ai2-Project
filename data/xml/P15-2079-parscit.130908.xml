<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024078">
<title confidence="0.996001">
Tagging Performance Correlates with Author Age
</title>
<author confidence="0.999839">
Dirk Hovy1 and Anders Søgaard1
</author>
<affiliation confidence="0.9985405">
Center for Language Technology
University of Copenhagen, Denmark
</affiliation>
<address confidence="0.920855">
Njalsgade 140, DK-2300 Copenhagen S
</address>
<email confidence="0.998612">
{dirk.hovy,soegaard}@hum.ku.dk
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999744066666667">
Many NLP tools for English and German
are based on manually annotated articles
from the Wall Street Journal and Frank-
furter Rundschau. The average readers of
these two newspapers are middle-aged (55
and 47 years old, respectively), and the an-
notated articles are more than 20 years old
by now. This leads us to speculate whether
tools induced from these resources (such
as part-of-speech taggers) put older lan-
guage users at an advantage. We show that
this is actually the case in both languages,
and that the cause goes beyond simple vo-
cabulary differences. In our experiments,
we control for gender and region.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982">
One of the main challenges in natural language
processing (NLP) is to correct for biases in the
manually annotated data available to system en-
gineers. Selection biases are often thought of in
terms of textual domains, motivating work in do-
main adaptation of NLP models (Daume III and
Marcu, 2006; Ben-David et al., 2007; Daume III,
2007; Dredze et al., 2007; Chen et al., 2009; Chen
et al., 2011, inter alia). Domain adaptation prob-
lems are typically framed as adapting models that
were induced on newswire to other domains, such
as spoken language, literature, or social media.
However, newswire is not just a domain with
particular conventions. It is also a source of infor-
mation written by and for particular people. The
reader base of most newspapers is older, richer,
and more well-educated than the average popu-
lation. Also, many newspapers have more read-
ers in some regions of their country. In addition,
</bodyText>
<footnote confidence="0.732539">
1Both authors contributed equally to the paper, and
flipped a heavily biased coin until they were both satisfied
with the order.
</footnote>
<bodyText confidence="0.999962414634147">
newswire text is much more canonical than other
domains, and includes fewer neologisms and non-
standard language. Both, however, are frequent
in the language use of young adults, who are the
main drivers of language change (Holmes, 2013;
Nguyen et al., 2014).
In this paper, we focus on the most widely used
manually annotated resources for English and Ger-
man, namely the English Penn Treebank and the
TIGER Treebank for German. The English tree-
bank consists of manually annotated Wall Street
Journal articles from 1989. The TIGER Treebank
consists of manually annotated Frankfurter Rund-
schau articles from the early 1990s. Both newspa-
pers have regionally and demographically biased
reader bases, e.g., with more old than young read-
ers. We discuss the biases in §2.
In the light of recent research (Volkova et al.,
2013; Hovy, 2015; Jørgensen et al., 2015), we ex-
plore the hypothesis that these biases transfer to
NLP tools induced from these resources. As a re-
sult, these models perform better on texts written
by certain people, namely those whose language
is closer to the training data. Language dynamics
being what they are, we expect English and Ger-
man POS taggers to perform better on texts written
by older people. To evaluate this hypothesis, we
collected English and German user reviews from
a user review site used by representative samples
of the English and German populations. We anno-
tated reviews written by users whose age, gender,
and location were known with POS tags. The re-
sulting data set enables us to test whether there are
significant performance differences between ages,
genders, and regions, while controlling for the two
respective other, potentially confounding, factors.
Contribution We show that age bias leads
to significant performance differences in off-the-
shelf POS taggers for English and German. We
also analyze the relevant linguistic differences be-
tween the age groups, and show that they are not
</bodyText>
<page confidence="0.965559">
483
</page>
<bodyText confidence="0.8421265">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 483–488,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
solely lexical, but instead extend to the grammat-
ical level. As a corollary, we also present several
new evaluation datasets for English and German
that allow us to control for age, gender, and loca-
tion.
</bodyText>
<sectionHeader confidence="0.994674" genericHeader="introduction">
2 Data
</sectionHeader>
<subsectionHeader confidence="0.992009">
2.1 Wall Street Journal and Frankfurter
Rundschau
</subsectionHeader>
<bodyText confidence="0.999849142857143">
The Wall Street Journal is a New York City-based
newspaper, in print since 1889, with about two
million readers. It employs 2,000 journalists in
85 news bureaus across 51 countries. Wall Street
Journal is often considered business-friendly, but
conservative. In 2007, Rupert Murdoch bought
the newspaper. The English Penn Treebank con-
sists of manually annotated articles from 1989, in-
cluding both essays, letters and errata, but the vast
majority are news pieces.1
Frankfurter Rundschau is a German language
newspaper based in Frankfurt am Main. Its first is-
sue dates back to 1945, shortly after the end of the
second world war. It has about 120,000 readers. It
is often considered a left-wing liberal newspaper.
According to a study conducted by the newspa-
per itself,2 its readers are found in “comfortable”
higher jobs, well-educated, and on average in their
mid-forties. While the paper is available interna-
tionally, most of its users come from the Rhine-
Main region.
</bodyText>
<subsectionHeader confidence="0.998964">
2.2 The Trustpilot Corpus
</subsectionHeader>
<bodyText confidence="0.999964166666667">
The Trustpilot Corpus (Hovy et al., 2015a) con-
sists of user reviews scraped from the multi-
lingual website trustpilot.com. The re-
viewer base has been shown to be representative
of the populations in the countries for which large
reviewer bases exist, at least wrt. age, gender, and
geographical spread (Hovy et al., 2015a). The lan-
guage is more informal than newswire, but less
creative than social media posts. This is similar to
the language in the reviews section of the English
Web Treebank.3 For the experiments below, we
annotated parts of the British and German sections
</bodyText>
<footnote confidence="0.996057333333333">
1http://www.let.rug.nl/˜bplank/
metadata/genre_files_updated.html
2http://www.fr-online.de/
</footnote>
<bodyText confidence="0.712439">
of the Trustpilot Corpus with the tag set proposed
in Petrov et al. (2011).
</bodyText>
<subsectionHeader confidence="0.99739">
2.3 POS annotations
</subsectionHeader>
<bodyText confidence="0.9999855">
We use an in-house interface to annotate the En-
glish and German data. For each of the two lan-
guages, we annotate 600 sentences. The data is
sampled in the following way: we first extract all
reviews associated with a location, split and to-
kenize the review using the NLTK tokenizer for
the respective language, and discard any sentences
with fewer than three or more than 100 tokens. We
then map each review to the NUTS region corre-
sponding to the location. If the location name is
ambiguous, we discard it.
We then run two POS taggers (TreeTagger4, and
a model implemented in CRF++5) to obtain log-
likelihoods for each sentence in the English and
German sub corpora. We normalize by sentence
length and compute the average score for each re-
gion under each tagger.
We single out the two regions in England and
Germany with the highest, respectively lowest, av-
erage log-likelihoods from both taggers. We do
this to be able to control for dialectal variation.
In each region, we randomly sample 200 reviews
written by women under 35, 200 reviews written
by men under 35, 200 reviews written by women
over 45, and 200 reviews written by men over 45.
This selection enables us to study and control for
gender, region, and age.
While sociolinguistics agrees on language
change between age groups (Barke, 2000; Schler
et al., 2006; Barbieri, 2008; Rickford and Price,
2013), it is not clear where to draw the line. The
age groups selected here are thus solely based on
the availability of even-sized groups that are sepa-
rated by 10 years.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999988">
3.1 Training data and models
</subsectionHeader>
<bodyText confidence="0.999988">
As training data for our POS tagging models, we
use manually annotated data from the Wall Street
Journal (English Penn Treebank) and Frankfurter
Rundschau (TIGER). We use the training and test
sections provided in the CoNLL 2006–7 shared
tasks, but we convert all tags to the universal POS
tag set (Petrov et al., 2011).
</bodyText>
<footnote confidence="0.84476625">
wir-ueber-uns/studie-wer-sind-unsere-leser-,
4353508,4356262.html 4http://www.cis.uni-muenchen.de/
3https://catalog.ldc.upenn.edu/ ˜schmid/tools/TreeTagger/
LDC2012T13 5http://taku910.github.io/crfpp/
</footnote>
<page confidence="0.997067">
484
</page>
<bodyText confidence="0.998914777777778">
Our POS taggers are trained using TreeTagger
with default parameters, and CRF++ with default
parameters and standard POS features (Owoputi
et al., 2013; Hovy et al., 2015b). We use two dif-
ferent POS tagger induction algorithms in order
to be able to abstract away from their respective
inductive biases. Generally, TreeTagger (TREET)
performs better than CRF++ on German, whereas
CRF++ performs best on English.
</bodyText>
<sectionHeader confidence="0.547823" genericHeader="method">
3.2 Results
</sectionHeader>
<table confidence="0.999866230769231">
country group TREET CRF++ avg.
U35 87.42 85.93 86.68
O45 89.39 87.04 88.22
DE male 88.53 86.11 87.32
female 88.21 86.78 87.50
highest reg. 88.46 86.49 87.48
lowest reg. 88.85 87.41 88.13
U35 87.92 88.23 88.08
O45 88.26 88.40 88.33
EN male 88.19 88.55 88.37
female 87.97 88.08 88.03
highest reg. 88.27 88.57 88.42
lowest reg. 88.24 88.52 88.38
</table>
<tableCaption confidence="0.917813333333333">
Table 1: POS accuracy on different demographic
groups for English and German. Significant dif-
ferences per tagger in bold
</tableCaption>
<bodyText confidence="0.983318772727273">
Table 1 shows the accuracies for both algo-
rithms on the three demographic groups (age, gen-
der, region) for German and English. We see that
there are some consistent differences between the
groups. In both languages, results for both taggers
are better for the older group than for the younger
one. In three out of the four cases, this difference
is statistically significant at p &lt; 0.05, according
to a bootstrap-sample test. The difference between
the genders is less pronounced, although we do see
CRF++ reaching a significantly higher accuracy
for women in German. For regions, we find that
while the models assign low log-likelihood scores
to some regions, this is not reflected in the accu-
racy.
As common in NLP, we treat American (train-
ing) and British English (test data) as variants. It
is possible that this introduces a confounding fac-
tor. However, since we do not see marked effects
for gender or region, and since the English results
closely track the German data, this seems unlikely.
We plan to investigate this in future work.
</bodyText>
<sectionHeader confidence="0.992713" genericHeader="evaluation">
4 Analysis
</sectionHeader>
<bodyText confidence="0.999964266666667">
The last section showed the performance differ-
ences between various groups, but it does not tell
us where the differences come from. In this sec-
tion, we try to look into potential causes, and ana-
lyze the tagging errors for systematic patterns. We
focus on age, since this variable showed the largest
differences between groups.
Holmes (2013) argues that people between 30
and 55 years use standard language the most,
because of societal pressure from their workplace.
Nguyen et al. (2014) made similar observations
for Twitter. Consequently, both young and retired
people often depart from the standard linguistic
norms, young people because of innovation, older
people because of adherence to previous norms.
Our data suggests, however, that young people do
so in ways that are more challenging for off-the-
shelf NLP models induced on age-biased data.
But what exactly are the linguistic differences that
lead to lower performance for this group?
The obvious cause for the difference between
age groups would be lexical change, i.e., the use
of neologisms, spelling variation, or linguistic
change at the structural level in the younger
group. The resulting vocabulary differences
between age groups would result in an increased
out-of-vocabulary (OOV) rate in the younger
group, which in turn negatively affects model
performance.
While we do observe an unsurprising corre-
lation between sentence-level performance and
OOV-rate, the young reviewers in our sample do
not use OOV words more often than the older
age group. Both groups differ from the training
data roughly equally. This strongly suggests that
age-related differences in performance are not a
result of OOV items.
In order to investigate whether the differ-
ences extend beyond the vocabulary, we compare
the tag bigram distributions, both between the
two age groups and between each group and
the training data. We measure similarity by KL
divergence between the distributions, and inspect
the 10 tag bigrams which are most prevalent
for either group. We use Laplace smoothing to
</bodyText>
<page confidence="0.999219">
485
</page>
<figureCaption confidence="0.999706">
Figure 1: Tag bigrams with highest differences between distributions in English data.
</figureCaption>
<bodyText confidence="0.99969686440678">
account for missing bigrams and ensure a proper
distribution.
For the English age groups, we find that a) the
two Trustpilot data sets have a smaller KL diver-
gence with respect to each other (1.86e − 6) than
either has with the training data (young: 3.24e−5,
old.: 2.36e−5, respectively). We do note however,
b), that the KL divergence for the older groups is
much smaller than for the younger group. This
means that there is a cross-domain effect, which is
bigger, measured this way, than the difference in
age groups. The age group difference in KL diver-
gence, however, suggests that the two groups use
different syntactic constructions.
Inspecting the bigrams which are most preva-
lent for each group, we find again that a) the
Trustpilot groups show more instances involving
verbs, such as PRON–VERB, VERB–ADV, and
VERB–DET, while the English Penn Treebank
data set has a larger proportion of instances of
nominal constructions, such as NOUN–VERB,
NOUN–ADP, and NOUN–NOUN.
On the other hand, we find that b) the younger
group has more cases of verbal constructions and
the use of particles, such as PRT–VERB, VERB–
PRT, PRON–PRT, and VERB–ADP, while the
older group–similar to the treebank–shows more
instances of nominal constructions, i.e., again
NOUN–VERB, ADJ–NOUN, NOUN–ADP,
and NOUN–NOUN.
The heatmaps in Figure 1 show all pairwise
comparisons between the three distributions. In
the interest of space and visibility, we select the 10
bigrams that differ most from each other between
the two distributions under comparison. The
color indicates in which of the two distributions
a bigram is more prevalent, and the degree of
shading indicates the size of the difference.
For German, we see similar patterns. The
Trustpilot data shows more instances of ADV–
ADV, PRON–VERB, and ADV–VERB, while
the TIGER treebank contains more NOUN–DET,
NOUN–ADP, and NOUN–NOUN.
Again, the younger group is more dissimilar
to the CoNLL data, but less so than for English,
with CONJ–PRON, NOUN–VERB, VERB–
VERB, and PRON–DET, while the older
group shows more ADV–ADJ, ADP–NOUN,
NOUN–ADV, and ADJ–NOUN.
In all of these cases, vocabulary does not
factor into the differences, since we are at the
POS level. The results indicate that there exist
fundamental grammatical differences between the
age groups, which go well beyond mere lexical
differences. These findings are in line with the
results in Johannsen et al. (2015), who showed
that entire (delexicalized) dependency structures
correlate with age and gender, often across several
languages.
</bodyText>
<subsectionHeader confidence="0.999779">
4.1 Tagging Error Analysis
</subsectionHeader>
<bodyText confidence="0.999897454545454">
Analyzing the tagging errors of our model can give
us an insight into the constructions that differ most
between groups.
In German, most of the errors in the younger
group occur with adverbs, determiners, and verbs.
Adverbs are often confused with adjectives, be-
cause adverbs and adjectives are used as modi-
fiers in similar ways. The taggers also frequently
confused adverbs with nouns, especially sentence-
initially, presumably largely because they are cap-
italized. Sometimes, such errors are also due to
</bodyText>
<page confidence="0.99796">
486
</page>
<bodyText confidence="0.999321333333334">
spelling mistakes and/or English loanwords. De-
terminers are often incorrectly predicted to be pro-
nouns, presumably due to homography: in Ger-
man, der, die, das, etc. can be used as determin-
ers, but also as relative pronouns, depending on
the position. Verbs are often incorrectly predicted
to be nouns. This last error is again mostly due
to capitalization, homographs, and, again, English
loanwords. Another interesting source is sentence-
initial use of verbs, which is unusual in canoni-
cal German declarative sentences, but common in
informal language, where pronouns are dropped,
i.e, “[Ich] Kann mich nicht beschweren” ([I] Can’t
complain).
Errors involving verbs are much less frequent
in the older group, where errors with adjectives
and nouns are more frequent.
For English, the errors in the younger and
older group are mostly on the same tags (nouns,
adjectives, and verbs). Nouns often get mis-
tagged as VERB, usually because of homography
due to null-conversion (ordering, face, needs).
Adjectives are also most commonly mis-tagged
as VERB, almost entirely due to homography in
participles (–ed, –ing). We see more emoticons
(labeled X) in the younger group, and some of
them end up with incorrect tags (NOUN or ADV).
There are no mis-tagged emoticons in the older
group, who generally uses fewer emoticons (see
also Hovy et al. (2015a)).
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999977448275862">
In this position paper, we show that some of the
common training data sets bias NLP tools towards
the language of older people. I.e., there is a statis-
tically significant correlation between tagging per-
formance and age for models trained on CoNLL
data. A study of the actual differences between
age groups shows that they go beyond the vocabu-
lary, and extend to the grammatical level.
The results suggest that NLP’s focus on a lim-
ited set of training data has serious consequences
for model performance on new data sets, but also
demographic groups. Due to language dynam-
ics and the age of the data sets, performance de-
grades significantly for younger speakers. Since
POS tagging is often the first step in any NLP
pipeline, performance differences are likely to in-
crease downstream. As a result, we risk disadvan-
taging younger groups when it comes to the bene-
fits of NLP.
The case study shows that our models are sus-
ceptible to the effects of language change and de-
mographic factors. Luckily, the biases are not in-
herent to the models, but reside mostly in the data.
The problem can thus mostly be addressed with
more thorough training data selection that takes
demographic factors into account. It does high-
light, however, that we also need to develop more
robust technologies that are less susceptible to data
biases.
</bodyText>
<sectionHeader confidence="0.994729" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99975825">
We would like to thank the anonymous reviewers
for their comments, which helped improve the pa-
per. This research is funded by the ERC Starting
Grant LOWLANDS No. 313695.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999556931034483">
Federica Barbieri. 2008. Patterns of age-based lin-
guistic variation in American English. Journal of
sociolinguistics, 12(1):58–88.
Andrew J Barke. 2000. The Effect of Age on the
Style of Discourse among Japanese Women. In Pro-
ceedings of the 14th Pacific Asia Conference on Lan-
guage, Information and Computation, pages 23–34.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In NIPS.
Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.
2009. Extracting discriminative concepts for do-
main adaptation in text mining. In KDD.
Minmin Chen, Killiang Weinberger, and John Blitzer.
2011. Co-training for domain adaptation. In NIPS.
Hal Daume III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101–126.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Jo˜ao Graca, and Fernando
Pereira. 2007. Frustratingly Hard Domain Adap-
tation for Dependency Parsing. In EMNLP-CoNLL.
Janet Holmes. 2013. An introduction to sociolinguis-
tics. Routledge.
Dirk Hovy, Anders Johannsen, and Anders Søgaard.
2015a. User review-sites as a source for large-scale
sociolinguistic studies. In Proceedings of WWW.
</reference>
<page confidence="0.980454">
487
</page>
<reference confidence="0.9988752">
Dirk Hovy, Barbara Plank, H´ector Martinez Alonso,
and Anders Søgaard. 2015b. Mining for unambigu-
ous instances to adapt pos taggers to new domains.
In Proceedings of NAACL-HLT.
Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In Proceedings of ACL.
Anders Johannsen, Dirk Hovy, and Anders Søgaard.
2015. Cross-lingual syntactic variation over age and
gender. In Proceedings of CoNLL.
Anna Jørgensen, Dirk Hovy, and Anders Søgaard.
2015. Challenges of studying and processing di-
alects in social media. In Workshop on Noisy User-
generated Text (W-NUT).
Dong Nguyen, Dolf Trieschnigg, A. Seza Dogru¨oz, Ri-
lana Gravel, Mariet Theune, Theo Meder, and Fran-
ciska De Jong. 2014. Predicting Author Gender
and Age from Tweets: Sociolinguistic Theories and
Crowd Wisdom. In Proceedings of COLING 2014.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011. A universal part-of-speech tagset. CoRR
abs/1104.2086.
John Rickford and Mackenzie Price. 2013. Girlz ii
women: Age-grading, language change and stylistic
variation. Journal of Sociolinguistics, 17(2):143–
179.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W Pennebaker. 2006. Effects of age
and gender on blogging. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, volume 6, pages 199–205.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment anal-
ysis in social media. In Proceedings of EMNLP,
pages 1815–1827.
</reference>
<page confidence="0.997969">
488
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.624172">
<title confidence="0.988299">Tagging Performance Correlates with Author Age</title>
<author confidence="0.679568">Anders</author>
<affiliation confidence="0.995301">Center for Language University of Copenhagen,</affiliation>
<address confidence="0.964456">Njalsgade 140, DK-2300 Copenhagen</address>
<abstract confidence="0.9972229375">Many NLP tools for English and German are based on manually annotated articles from the Wall Street Journal and Frankfurter Rundschau. The average readers of these two newspapers are middle-aged (55 and 47 years old, respectively), and the annotated articles are more than 20 years old by now. This leads us to speculate whether tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Federica Barbieri</author>
</authors>
<title>Patterns of age-based linguistic variation in American English.</title>
<date>2008</date>
<journal>Journal of sociolinguistics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="7464" citStr="Barbieri, 2008" startWordPosition="1206" endWordPosition="1207">ore for each region under each tagger. We single out the two regions in England and Germany with the highest, respectively lowest, average log-likelihoods from both taggers. We do this to be able to control for dialectal variation. In each region, we randomly sample 200 reviews written by women under 35, 200 reviews written by men under 35, 200 reviews written by women over 45, and 200 reviews written by men over 45. This selection enables us to study and control for gender, region, and age. While sociolinguistics agrees on language change between age groups (Barke, 2000; Schler et al., 2006; Barbieri, 2008; Rickford and Price, 2013), it is not clear where to draw the line. The age groups selected here are thus solely based on the availability of even-sized groups that are separated by 10 years. 3 Experiments 3.1 Training data and models As training data for our POS tagging models, we use manually annotated data from the Wall Street Journal (English Penn Treebank) and Frankfurter Rundschau (TIGER). We use the training and test sections provided in the CoNLL 2006–7 shared tasks, but we convert all tags to the universal POS tag set (Petrov et al., 2011). wir-ueber-uns/studie-wer-sind-unsere-leser-</context>
</contexts>
<marker>Barbieri, 2008</marker>
<rawString>Federica Barbieri. 2008. Patterns of age-based linguistic variation in American English. Journal of sociolinguistics, 12(1):58–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Barke</author>
</authors>
<title>The Effect of Age on the Style of Discourse among Japanese Women.</title>
<date>2000</date>
<booktitle>In Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>23--34</pages>
<contexts>
<context position="7427" citStr="Barke, 2000" startWordPosition="1200" endWordPosition="1201"> length and compute the average score for each region under each tagger. We single out the two regions in England and Germany with the highest, respectively lowest, average log-likelihoods from both taggers. We do this to be able to control for dialectal variation. In each region, we randomly sample 200 reviews written by women under 35, 200 reviews written by men under 35, 200 reviews written by women over 45, and 200 reviews written by men over 45. This selection enables us to study and control for gender, region, and age. While sociolinguistics agrees on language change between age groups (Barke, 2000; Schler et al., 2006; Barbieri, 2008; Rickford and Price, 2013), it is not clear where to draw the line. The age groups selected here are thus solely based on the availability of even-sized groups that are separated by 10 years. 3 Experiments 3.1 Training data and models As training data for our POS tagging models, we use manually annotated data from the Wall Street Journal (English Penn Treebank) and Frankfurter Rundschau (TIGER). We use the training and test sections provided in the CoNLL 2006–7 shared tasks, but we convert all tags to the universal POS tag set (Petrov et al., 2011). wir-ue</context>
</contexts>
<marker>Barke, 2000</marker>
<rawString>Andrew J Barke. 2000. The Effect of Age on the Style of Discourse among Japanese Women. In Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation, pages 23–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1166" citStr="Ben-David et al., 2007" startWordPosition="181" endWordPosition="184">her tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region. 1 Introduction One of the main challenges in natural language processing (NLP) is to correct for biases in the manually annotated data available to system engineers. Selection biases are often thought of in terms of textual domains, motivating work in domain adaptation of NLP models (Daume III and Marcu, 2006; Ben-David et al., 2007; Daume III, 2007; Dredze et al., 2007; Chen et al., 2009; Chen et al., 2011, inter alia). Domain adaptation problems are typically framed as adapting models that were induced on newswire to other domains, such as spoken language, literature, or social media. However, newswire is not just a domain with particular conventions. It is also a source of information written by and for particular people. The reader base of most newspapers is older, richer, and more well-educated than the average population. Also, many newspapers have more readers in some regions of their country. In addition, 1Both a</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2007</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2007. Analysis of representations for domain adaptation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Chen</author>
<author>Wai Lam</author>
<author>Ivor Tsang</author>
<author>Tak-Lam Wong</author>
</authors>
<title>Extracting discriminative concepts for domain adaptation in text mining.</title>
<date>2009</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="1223" citStr="Chen et al., 2009" startWordPosition="192" endWordPosition="195"> taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region. 1 Introduction One of the main challenges in natural language processing (NLP) is to correct for biases in the manually annotated data available to system engineers. Selection biases are often thought of in terms of textual domains, motivating work in domain adaptation of NLP models (Daume III and Marcu, 2006; Ben-David et al., 2007; Daume III, 2007; Dredze et al., 2007; Chen et al., 2009; Chen et al., 2011, inter alia). Domain adaptation problems are typically framed as adapting models that were induced on newswire to other domains, such as spoken language, literature, or social media. However, newswire is not just a domain with particular conventions. It is also a source of information written by and for particular people. The reader base of most newspapers is older, richer, and more well-educated than the average population. Also, many newspapers have more readers in some regions of their country. In addition, 1Both authors contributed equally to the paper, and flipped a he</context>
</contexts>
<marker>Chen, Lam, Tsang, Wong, 2009</marker>
<rawString>Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong. 2009. Extracting discriminative concepts for domain adaptation in text mining. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Killiang Weinberger</author>
<author>John Blitzer</author>
</authors>
<title>Co-training for domain adaptation.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1242" citStr="Chen et al., 2011" startWordPosition="196" endWordPosition="199"> language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region. 1 Introduction One of the main challenges in natural language processing (NLP) is to correct for biases in the manually annotated data available to system engineers. Selection biases are often thought of in terms of textual domains, motivating work in domain adaptation of NLP models (Daume III and Marcu, 2006; Ben-David et al., 2007; Daume III, 2007; Dredze et al., 2007; Chen et al., 2009; Chen et al., 2011, inter alia). Domain adaptation problems are typically framed as adapting models that were induced on newswire to other domains, such as spoken language, literature, or social media. However, newswire is not just a domain with particular conventions. It is also a source of information written by and for particular people. The reader base of most newspapers is older, richer, and more well-educated than the average population. Also, many newspapers have more readers in some regions of their country. In addition, 1Both authors contributed equally to the paper, and flipped a heavily biased coin u</context>
</contexts>
<marker>Chen, Weinberger, Blitzer, 2011</marker>
<rawString>Minmin Chen, Killiang Weinberger, and John Blitzer. 2011. Co-training for domain adaptation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daume, Marcu, 2006</marker>
<rawString>Hal Daume III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao Graca, and Fernando Pereira.</title>
<date>2007</date>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao Graca, and Fernando Pereira. 2007. Frustratingly Hard Domain Adaptation for Dependency Parsing. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Holmes</author>
</authors>
<title>An introduction to sociolinguistics.</title>
<date>2013</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="2123" citStr="Holmes, 2013" startWordPosition="341" endWordPosition="342">ce of information written by and for particular people. The reader base of most newspapers is older, richer, and more well-educated than the average population. Also, many newspapers have more readers in some regions of their country. In addition, 1Both authors contributed equally to the paper, and flipped a heavily biased coin until they were both satisfied with the order. newswire text is much more canonical than other domains, and includes fewer neologisms and nonstandard language. Both, however, are frequent in the language use of young adults, who are the main drivers of language change (Holmes, 2013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jør</context>
<context position="10508" citStr="Holmes (2013)" startWordPosition="1689" endWordPosition="1690"> variants. It is possible that this introduces a confounding factor. However, since we do not see marked effects for gender or region, and since the English results closely track the German data, this seems unlikely. We plan to investigate this in future work. 4 Analysis The last section showed the performance differences between various groups, but it does not tell us where the differences come from. In this section, we try to look into potential causes, and analyze the tagging errors for systematic patterns. We focus on age, since this variable showed the largest differences between groups. Holmes (2013) argues that people between 30 and 55 years use standard language the most, because of societal pressure from their workplace. Nguyen et al. (2014) made similar observations for Twitter. Consequently, both young and retired people often depart from the standard linguistic norms, young people because of innovation, older people because of adherence to previous norms. Our data suggests, however, that young people do so in ways that are more challenging for off-theshelf NLP models induced on age-biased data. But what exactly are the linguistic differences that lead to lower performance for this g</context>
</contexts>
<marker>Holmes, 2013</marker>
<rawString>Janet Holmes. 2013. An introduction to sociolinguistics. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Anders Johannsen</author>
<author>Anders Søgaard</author>
</authors>
<title>User review-sites as a source for large-scale sociolinguistic studies.</title>
<date>2015</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="5400" citStr="Hovy et al., 2015" startWordPosition="863" endWordPosition="866">rata, but the vast majority are news pieces.1 Frankfurter Rundschau is a German language newspaper based in Frankfurt am Main. Its first issue dates back to 1945, shortly after the end of the second world war. It has about 120,000 readers. It is often considered a left-wing liberal newspaper. According to a study conducted by the newspaper itself,2 its readers are found in “comfortable” higher jobs, well-educated, and on average in their mid-forties. While the paper is available internationally, most of its users come from the RhineMain region. 2.2 The Trustpilot Corpus The Trustpilot Corpus (Hovy et al., 2015a) consists of user reviews scraped from the multilingual website trustpilot.com. The reviewer base has been shown to be representative of the populations in the countries for which large reviewer bases exist, at least wrt. age, gender, and geographical spread (Hovy et al., 2015a). The language is more informal than newswire, but less creative than social media posts. This is similar to the language in the reviews section of the English Web Treebank.3 For the experiments below, we annotated parts of the British and German sections 1http://www.let.rug.nl/˜bplank/ metadata/genre_files_updated.ht</context>
<context position="8396" citStr="Hovy et al., 2015" startWordPosition="1335" endWordPosition="1338">m the Wall Street Journal (English Penn Treebank) and Frankfurter Rundschau (TIGER). We use the training and test sections provided in the CoNLL 2006–7 shared tasks, but we convert all tags to the universal POS tag set (Petrov et al., 2011). wir-ueber-uns/studie-wer-sind-unsere-leser-, 4353508,4356262.html 4http://www.cis.uni-muenchen.de/ 3https://catalog.ldc.upenn.edu/ ˜schmid/tools/TreeTagger/ LDC2012T13 5http://taku910.github.io/crfpp/ 484 Our POS taggers are trained using TreeTagger with default parameters, and CRF++ with default parameters and standard POS features (Owoputi et al., 2013; Hovy et al., 2015b). We use two different POS tagger induction algorithms in order to be able to abstract away from their respective inductive biases. Generally, TreeTagger (TREET) performs better than CRF++ on German, whereas CRF++ performs best on English. 3.2 Results country group TREET CRF++ avg. U35 87.42 85.93 86.68 O45 89.39 87.04 88.22 DE male 88.53 86.11 87.32 female 88.21 86.78 87.50 highest reg. 88.46 86.49 87.48 lowest reg. 88.85 87.41 88.13 U35 87.92 88.23 88.08 O45 88.26 88.40 88.33 EN male 88.19 88.55 88.37 female 87.97 88.08 88.03 highest reg. 88.27 88.57 88.42 lowest reg. 88.24 88.52 88.38 Tab</context>
<context position="16713" citStr="Hovy et al. (2015" startWordPosition="2667" endWordPosition="2670">h adjectives and nouns are more frequent. For English, the errors in the younger and older group are mostly on the same tags (nouns, adjectives, and verbs). Nouns often get mistagged as VERB, usually because of homography due to null-conversion (ordering, face, needs). Adjectives are also most commonly mis-tagged as VERB, almost entirely due to homography in participles (–ed, –ing). We see more emoticons (labeled X) in the younger group, and some of them end up with incorrect tags (NOUN or ADV). There are no mis-tagged emoticons in the older group, who generally uses fewer emoticons (see also Hovy et al. (2015a)). 5 Conclusion In this position paper, we show that some of the common training data sets bias NLP tools towards the language of older people. I.e., there is a statistically significant correlation between tagging performance and age for models trained on CoNLL data. A study of the actual differences between age groups shows that they go beyond the vocabulary, and extend to the grammatical level. The results suggest that NLP’s focus on a limited set of training data has serious consequences for model performance on new data sets, but also demographic groups. Due to language dynamics and the</context>
</contexts>
<marker>Hovy, Johannsen, Søgaard, 2015</marker>
<rawString>Dirk Hovy, Anders Johannsen, and Anders Søgaard. 2015a. User review-sites as a source for large-scale sociolinguistic studies. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Barbara Plank</author>
<author>H´ector Martinez Alonso</author>
<author>Anders Søgaard</author>
</authors>
<title>Mining for unambiguous instances to adapt pos taggers to new domains.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="5400" citStr="Hovy et al., 2015" startWordPosition="863" endWordPosition="866">rata, but the vast majority are news pieces.1 Frankfurter Rundschau is a German language newspaper based in Frankfurt am Main. Its first issue dates back to 1945, shortly after the end of the second world war. It has about 120,000 readers. It is often considered a left-wing liberal newspaper. According to a study conducted by the newspaper itself,2 its readers are found in “comfortable” higher jobs, well-educated, and on average in their mid-forties. While the paper is available internationally, most of its users come from the RhineMain region. 2.2 The Trustpilot Corpus The Trustpilot Corpus (Hovy et al., 2015a) consists of user reviews scraped from the multilingual website trustpilot.com. The reviewer base has been shown to be representative of the populations in the countries for which large reviewer bases exist, at least wrt. age, gender, and geographical spread (Hovy et al., 2015a). The language is more informal than newswire, but less creative than social media posts. This is similar to the language in the reviews section of the English Web Treebank.3 For the experiments below, we annotated parts of the British and German sections 1http://www.let.rug.nl/˜bplank/ metadata/genre_files_updated.ht</context>
<context position="8396" citStr="Hovy et al., 2015" startWordPosition="1335" endWordPosition="1338">m the Wall Street Journal (English Penn Treebank) and Frankfurter Rundschau (TIGER). We use the training and test sections provided in the CoNLL 2006–7 shared tasks, but we convert all tags to the universal POS tag set (Petrov et al., 2011). wir-ueber-uns/studie-wer-sind-unsere-leser-, 4353508,4356262.html 4http://www.cis.uni-muenchen.de/ 3https://catalog.ldc.upenn.edu/ ˜schmid/tools/TreeTagger/ LDC2012T13 5http://taku910.github.io/crfpp/ 484 Our POS taggers are trained using TreeTagger with default parameters, and CRF++ with default parameters and standard POS features (Owoputi et al., 2013; Hovy et al., 2015b). We use two different POS tagger induction algorithms in order to be able to abstract away from their respective inductive biases. Generally, TreeTagger (TREET) performs better than CRF++ on German, whereas CRF++ performs best on English. 3.2 Results country group TREET CRF++ avg. U35 87.42 85.93 86.68 O45 89.39 87.04 88.22 DE male 88.53 86.11 87.32 female 88.21 86.78 87.50 highest reg. 88.46 86.49 87.48 lowest reg. 88.85 87.41 88.13 U35 87.92 88.23 88.08 O45 88.26 88.40 88.33 EN male 88.19 88.55 88.37 female 87.97 88.08 88.03 highest reg. 88.27 88.57 88.42 lowest reg. 88.24 88.52 88.38 Tab</context>
<context position="16713" citStr="Hovy et al. (2015" startWordPosition="2667" endWordPosition="2670">h adjectives and nouns are more frequent. For English, the errors in the younger and older group are mostly on the same tags (nouns, adjectives, and verbs). Nouns often get mistagged as VERB, usually because of homography due to null-conversion (ordering, face, needs). Adjectives are also most commonly mis-tagged as VERB, almost entirely due to homography in participles (–ed, –ing). We see more emoticons (labeled X) in the younger group, and some of them end up with incorrect tags (NOUN or ADV). There are no mis-tagged emoticons in the older group, who generally uses fewer emoticons (see also Hovy et al. (2015a)). 5 Conclusion In this position paper, we show that some of the common training data sets bias NLP tools towards the language of older people. I.e., there is a statistically significant correlation between tagging performance and age for models trained on CoNLL data. A study of the actual differences between age groups shows that they go beyond the vocabulary, and extend to the grammatical level. The results suggest that NLP’s focus on a limited set of training data has serious consequences for model performance on new data sets, but also demographic groups. Due to language dynamics and the</context>
</contexts>
<marker>Hovy, Plank, Alonso, Søgaard, 2015</marker>
<rawString>Dirk Hovy, Barbara Plank, H´ector Martinez Alonso, and Anders Søgaard. 2015b. Mining for unambiguous instances to adapt pos taggers to new domains. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
</authors>
<title>Demographic factors improve classification performance.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2718" citStr="Hovy, 2015" startWordPosition="439" endWordPosition="440">e (Holmes, 2013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written by users wh</context>
</contexts>
<marker>Hovy, 2015</marker>
<rawString>Dirk Hovy. 2015. Demographic factors improve classification performance. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Johannsen</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Cross-lingual syntactic variation over age and gender.</title>
<date>2015</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="14709" citStr="Johannsen et al. (2015)" startWordPosition="2351" endWordPosition="2354">DV–VERB, while the TIGER treebank contains more NOUN–DET, NOUN–ADP, and NOUN–NOUN. Again, the younger group is more dissimilar to the CoNLL data, but less so than for English, with CONJ–PRON, NOUN–VERB, VERB– VERB, and PRON–DET, while the older group shows more ADV–ADJ, ADP–NOUN, NOUN–ADV, and ADJ–NOUN. In all of these cases, vocabulary does not factor into the differences, since we are at the POS level. The results indicate that there exist fundamental grammatical differences between the age groups, which go well beyond mere lexical differences. These findings are in line with the results in Johannsen et al. (2015), who showed that entire (delexicalized) dependency structures correlate with age and gender, often across several languages. 4.1 Tagging Error Analysis Analyzing the tagging errors of our model can give us an insight into the constructions that differ most between groups. In German, most of the errors in the younger group occur with adverbs, determiners, and verbs. Adverbs are often confused with adjectives, because adverbs and adjectives are used as modifiers in similar ways. The taggers also frequently confused adverbs with nouns, especially sentenceinitially, presumably largely because the</context>
</contexts>
<marker>Johannsen, Hovy, Søgaard, 2015</marker>
<rawString>Anders Johannsen, Dirk Hovy, and Anders Søgaard. 2015. Cross-lingual syntactic variation over age and gender. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Jørgensen</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Challenges of studying and processing dialects in social media.</title>
<date>2015</date>
<booktitle>In Workshop on Noisy Usergenerated Text (W-NUT).</booktitle>
<contexts>
<context position="2743" citStr="Jørgensen et al., 2015" startWordPosition="441" endWordPosition="444">013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written by users whose age, gender, and loca</context>
</contexts>
<marker>Jørgensen, Hovy, Søgaard, 2015</marker>
<rawString>Anna Jørgensen, Dirk Hovy, and Anders Søgaard. 2015. Challenges of studying and processing dialects in social media. In Workshop on Noisy Usergenerated Text (W-NUT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>Dolf Trieschnigg</author>
<author>A Seza Dogru¨oz</author>
<author>Rilana Gravel</author>
<author>Mariet Theune</author>
<author>Theo Meder</author>
<author>Franciska De Jong</author>
</authors>
<title>Predicting Author Gender and Age from Tweets: Sociolinguistic Theories and Crowd Wisdom.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING</booktitle>
<marker>Nguyen, Trieschnigg, Dogru¨oz, Gravel, Theune, Meder, De Jong, 2014</marker>
<rawString>Dong Nguyen, Dolf Trieschnigg, A. Seza Dogru¨oz, Rilana Gravel, Mariet Theune, Theo Meder, and Franciska De Jong. 2014. Predicting Author Gender and Age from Tweets: Sociolinguistic Theories and Crowd Wisdom. In Proceedings of COLING 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2011</date>
<note>CoRR abs/1104.2086.</note>
<contexts>
<context position="6103" citStr="Petrov et al. (2011)" startWordPosition="969" endWordPosition="972">he reviewer base has been shown to be representative of the populations in the countries for which large reviewer bases exist, at least wrt. age, gender, and geographical spread (Hovy et al., 2015a). The language is more informal than newswire, but less creative than social media posts. This is similar to the language in the reviews section of the English Web Treebank.3 For the experiments below, we annotated parts of the British and German sections 1http://www.let.rug.nl/˜bplank/ metadata/genre_files_updated.html 2http://www.fr-online.de/ of the Trustpilot Corpus with the tag set proposed in Petrov et al. (2011). 2.3 POS annotations We use an in-house interface to annotate the English and German data. For each of the two languages, we annotate 600 sentences. The data is sampled in the following way: we first extract all reviews associated with a location, split and tokenize the review using the NLTK tokenizer for the respective language, and discard any sentences with fewer than three or more than 100 tokens. We then map each review to the NUTS region corresponding to the location. If the location name is ambiguous, we discard it. We then run two POS taggers (TreeTagger4, and a model implemented in C</context>
<context position="8019" citStr="Petrov et al., 2011" startWordPosition="1300" endWordPosition="1303">ween age groups (Barke, 2000; Schler et al., 2006; Barbieri, 2008; Rickford and Price, 2013), it is not clear where to draw the line. The age groups selected here are thus solely based on the availability of even-sized groups that are separated by 10 years. 3 Experiments 3.1 Training data and models As training data for our POS tagging models, we use manually annotated data from the Wall Street Journal (English Penn Treebank) and Frankfurter Rundschau (TIGER). We use the training and test sections provided in the CoNLL 2006–7 shared tasks, but we convert all tags to the universal POS tag set (Petrov et al., 2011). wir-ueber-uns/studie-wer-sind-unsere-leser-, 4353508,4356262.html 4http://www.cis.uni-muenchen.de/ 3https://catalog.ldc.upenn.edu/ ˜schmid/tools/TreeTagger/ LDC2012T13 5http://taku910.github.io/crfpp/ 484 Our POS taggers are trained using TreeTagger with default parameters, and CRF++ with default parameters and standard POS features (Owoputi et al., 2013; Hovy et al., 2015b). We use two different POS tagger induction algorithms in order to be able to abstract away from their respective inductive biases. Generally, TreeTagger (TREET) performs better than CRF++ on German, whereas CRF++ perform</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. CoRR abs/1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rickford</author>
<author>Mackenzie Price</author>
</authors>
<title>Girlz ii women: Age-grading, language change and stylistic variation.</title>
<date>2013</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>179</pages>
<contexts>
<context position="7491" citStr="Rickford and Price, 2013" startWordPosition="1208" endWordPosition="1211">ion under each tagger. We single out the two regions in England and Germany with the highest, respectively lowest, average log-likelihoods from both taggers. We do this to be able to control for dialectal variation. In each region, we randomly sample 200 reviews written by women under 35, 200 reviews written by men under 35, 200 reviews written by women over 45, and 200 reviews written by men over 45. This selection enables us to study and control for gender, region, and age. While sociolinguistics agrees on language change between age groups (Barke, 2000; Schler et al., 2006; Barbieri, 2008; Rickford and Price, 2013), it is not clear where to draw the line. The age groups selected here are thus solely based on the availability of even-sized groups that are separated by 10 years. 3 Experiments 3.1 Training data and models As training data for our POS tagging models, we use manually annotated data from the Wall Street Journal (English Penn Treebank) and Frankfurter Rundschau (TIGER). We use the training and test sections provided in the CoNLL 2006–7 shared tasks, but we convert all tags to the universal POS tag set (Petrov et al., 2011). wir-ueber-uns/studie-wer-sind-unsere-leser-, 4353508,4356262.html 4htt</context>
</contexts>
<marker>Rickford, Price, 2013</marker>
<rawString>John Rickford and Mackenzie Price. 2013. Girlz ii women: Age-grading, language change and stylistic variation. Journal of Sociolinguistics, 17(2):143– 179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Schler</author>
<author>Moshe Koppel</author>
<author>Shlomo Argamon</author>
<author>James W Pennebaker</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,</booktitle>
<volume>6</volume>
<pages>199--205</pages>
<contexts>
<context position="7448" citStr="Schler et al., 2006" startWordPosition="1202" endWordPosition="1205">ompute the average score for each region under each tagger. We single out the two regions in England and Germany with the highest, respectively lowest, average log-likelihoods from both taggers. We do this to be able to control for dialectal variation. In each region, we randomly sample 200 reviews written by women under 35, 200 reviews written by men under 35, 200 reviews written by women over 45, and 200 reviews written by men over 45. This selection enables us to study and control for gender, region, and age. While sociolinguistics agrees on language change between age groups (Barke, 2000; Schler et al., 2006; Barbieri, 2008; Rickford and Price, 2013), it is not clear where to draw the line. The age groups selected here are thus solely based on the availability of even-sized groups that are separated by 10 years. 3 Experiments 3.1 Training data and models As training data for our POS tagging models, we use manually annotated data from the Wall Street Journal (English Penn Treebank) and Frankfurter Rundschau (TIGER). We use the training and test sections provided in the CoNLL 2006–7 shared tasks, but we convert all tags to the universal POS tag set (Petrov et al., 2011). wir-ueber-uns/studie-wer-si</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W Pennebaker. 2006. Effects of age and gender on blogging. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, volume 6, pages 199–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Exploring demographic language variations to improve multilingual sentiment analysis in social media.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1815--1827</pages>
<contexts>
<context position="2706" citStr="Volkova et al., 2013" startWordPosition="435" endWordPosition="438">vers of language change (Holmes, 2013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written</context>
</contexts>
<marker>Volkova, Wilson, Yarowsky, 2013</marker>
<rawString>Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring demographic language variations to improve multilingual sentiment analysis in social media. In Proceedings of EMNLP, pages 1815–1827.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>