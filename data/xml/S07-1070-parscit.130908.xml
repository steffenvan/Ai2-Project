<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030354">
<title confidence="0.980025">
TKB-UO: Using Sense Clustering for WSD
</title>
<author confidence="0.987686">
Henry Anaya-S´anchez1, Aurora Pons-Porrata1, Rafael Berlanga-Llavori2
</author>
<affiliation confidence="0.874806">
1 Center of Pattern Recognition and Data Mining, Universidad de Oriente, Cuba
2 Computer Science, Universitat Jaume I, Spain
</affiliation>
<email confidence="0.882624">
1 {henry,aurora}@csd.uo.edu.cu
2berlanga@lsi.uji.es
</email>
<sectionHeader confidence="0.996444" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997373">
This paper describes the clustering-based
approach to Word Sense Disambiguation
that is followed by the TKB-UO system at
SemEval-2007. The underlying disambigua-
tion method only uses WordNet as external
resource, and does not use training data. Re-
sults obtained in both Coarse-grained En-
glish all-words task (task 7) and English
fine-grained all-words subtask (task 17) are
presented.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965115384615">
The TKB-UO system relies on the knowledge-
driven approach to Word Sense Disambiguation
(WSD) presented in (Anaya-S´anchez et al., 2006).
Regarding that meaningful senses of words in a tex-
tual unit must be coherently related, our proposal
uses sense clustering with the aim of determining
cohesive groups of senses that reflect the connectiv-
ity of the disambiguating words.
The way this proposal uses clustering for disam-
biguation purposes is different from those usages re-
ported in other works of the WSD area. For ex-
ample, in (Pedersen et al., 2005) textual contexts
are clustered in order to represent senses for Word
Sense Discrimination. Other works like (Agirre and
L´opez, 2003), cluster fine-grained word senses into
coarse-grained ones for polysemy reduction. In-
stead, our method clusters all possible senses cor-
responding to all words in a disambiguating textual
unit. Thus, our system implements a novel cluster-
ing approach for the contextual disambiguation of
words.
We use the lexical resource WordNet (version 2.1)
as the repository of word senses, and also as the
provider of sense representations. It is worth men-
tioning that our proposal does not require the use of
training data.
</bodyText>
<sectionHeader confidence="0.806431" genericHeader="method">
2 The disambiguation algorithm
</sectionHeader>
<bodyText confidence="0.999946454545455">
Our method starts with a clustering of all possible
senses of the disambiguating words. Such a cluster-
ing tries to identify cohesive groups of word senses,
which are assumed to represent the different mean-
ings for the set of disambiguating words. Then, clus-
ters that match the best with the context are selected
via a filtering process. If the selected clusters dis-
ambiguate all words, the process is stopped and the
senses belonging to the selected clusters are inter-
preted as the disambiguating ones. Otherwise, the
clustering and filtering steps are performed again
(regarding the remaining senses) until the disam-
biguation is achieved.
Algorithm 1 shows the general steps of our pro-
posal for the disambiguation of a set of words W. In
the algorithm, clustering represents the basic clus-
tering method, filter is the function that selects the
clusters, and T denotes the intended textual context
from which words in W are disambiguated (typi-
cally, a broader bag of words than W). Next subsec-
tions describe in detail each component of the whole
process.
</bodyText>
<subsectionHeader confidence="0.995462">
2.1 Sense Representation
</subsectionHeader>
<bodyText confidence="0.999632">
For clustering purposes, word senses are repre-
sented as topic signatures (Lin and Hovy, 2000).
Thus, for each word sense s we define a vector
</bodyText>
<page confidence="0.979095">
322
</page>
<bodyText confidence="0.960237375">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 322–325,
Prague, June 2007. c�2007 Association for Computational Linguistics
Algorithm 1 Clustering-based approach for the dis-
ambiguation of the set of words W in the textual
context T
Input: The finite set of words W and the textual
context T.
Output: The disambiguated word senses.
</bodyText>
<equation confidence="0.948304727272727">
Let S be the set of all senses of words in W, and
i = 0;
repeat
i = i + 1
G = clustering(S, β0(i))
G0 = filter(G, W, T)
S = � {s|s E g}
g∈G
until |S |= |W |or β0(i + 1) = 1
return S
(t1 : σ1, . . . , tm : σm), where each ti is a Word-
</equation>
<bodyText confidence="0.9992423125">
Net term highly correlated to s with an association
weight σi. The set of signature terms for a word
sense includes all its WordNet hyponyms, its di-
rectly related terms (including coordinated terms)
and their filtered and lemmatized glosses. To weight
signature terms, the tf-idf statistics is used, con-
sidering each word as a collection and its senses as
its of documents. Topic signatures of senses form a
Vector Space Model similar to those defined in In-
formation Retrieval Systems. In this way, they can
be compared with measures such as cosine, Dice and
Jaccard (Salton et al., 1975).
In (Anaya-S´anchez et al., 2006), it is shown that
this kind of WordNet-based signatures outperform
those Web-based ones developed by the Ixa Re-
search Group 1 in the disambiguation of nouns.
</bodyText>
<subsectionHeader confidence="0.99972">
2.2 Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.9999322">
Sense clustering is carried out by the Extended Star
Clustering Algorithm (Gil et al., 2003), which builds
star-shaped and overlapped clusters. Each cluster
consists of a star and its satellites, where the star is
the sense with the highest connectivity of the clus-
ter, and the satellites are those senses connected with
the star. The connectivity is defined in terms of the
β0-similarity graph, which is obtained using the co-
sine similarity measure between topic signatures and
the minimum similarity threshold β0. The way this
</bodyText>
<footnote confidence="0.907674">
1http://ixa.si.ehu.es/Ixa/
</footnote>
<bodyText confidence="0.969034">
clustering algorithm relates word senses resembles
the manner in which syntactic and discourse relation
links textual elements.
</bodyText>
<subsectionHeader confidence="0.995374">
2.3 Filtering Process
</subsectionHeader>
<bodyText confidence="0.993593965517241">
Once clustering is performed over the senses of
words in W, a set of sense clusters is obtained. As
some clusters can be more appropriate to describe
the semantics of W than others, they are ranked ac-
cording to a measure w.r.t the textual context T.
As we represent the context T in the same vector
space that the topic signatures of senses, the follow-
ing function can be used to score a cluster of senses
g regarding T:
�s∈g number(s)
where words(g) denotes the set of words having
senses in g, g� is the centroid of g (computed as
the barycenter of the cluster), and number(s) is the
WordNet number of sense s according to its corre-
sponding word.
Then, we rank all clusters by using the lexico-
graphic order of their scores w.r.t. the above func-
tion.
Once the clusters have been ranked, they are or-
derly processed to select clusters for covering the
words in W. A cluster g is selected if it contains
at least one sense of an uncovered word and other
senses corresponding to covered words are included
in the current selected clusters. If g does not con-
tain any sense of uncovered words it is discarded.
Otherwise, g is inserted into a queue Q. Finally, if
the selected clusters do not cover W, clusters in Q
adding senses of uncovered words are chosen until
all words are covered.
</bodyText>
<subsectionHeader confidence="0.996701">
2.4 β0 Threshold and the Stopping Criterion
</subsectionHeader>
<bodyText confidence="0.999633625">
As a result of the filtering process, a set of senses for
all the words in W is obtained (i.e. the union of all
the selected clusters). Each word in W that has only
a sense in such a set is considered disambiguated. If
some word still remains ambiguous, we must refine
the clustering process to get stronger cohesive clus-
ters of senses. In this case, all the remaining senses
must be clustered again but raising the β0 threshold.
</bodyText>
<equation confidence="0.9874614">
E
i
min{¯gi,Ti}
_E
|words(g)|,
Ti},
¯gi,�
i
min{E
i
</equation>
<page confidence="0.990873">
323
</page>
<bodyText confidence="0.99592525">
Notice that this process must be done iteratively un-
til either all words are disambiguated or when it is
impossible to raise 00 again. Initially, 00 is defined
as:
</bodyText>
<equation confidence="0.993042">
,30(1) = pth(90, sim(S))
</equation>
<bodyText confidence="0.94788">
and at the i-th iteration (i &gt; 1) it is raised to:
</bodyText>
<equation confidence="0.987173">
,30(i) = min {,3 = pth(p, sim(S))1,3 &gt; ,30(i − 1)1
PE{90,95,1001
</equation>
<bodyText confidence="0.9617478">
In these equations, S is the set of current senses,
and pth(p, sim(S)) represents the p-th percentile
value of the pairwise similarities between senses
(i.e. sim(S) = {cos(si, sj)|si, sj ∈ S, i =6 j} ∪
{1}).
</bodyText>
<subsectionHeader confidence="0.911072">
2.5 A Disambiguation Example
</subsectionHeader>
<bodyText confidence="0.998767677419355">
In this subsection we illustrate the use of our pro-
posal in the disambiguation of the content words
appearing in the sentence “The runner won the
marathon”. In this example, the set of disam-
biguating words W includes the nouns runner and
marathon, and the verb win (lemma of the verbal
form won). Also, we consider that the context is the
vector T = hrunner : 1, win : 1, marathon : 1i.
The rest of words are not considered because they
are meaningless. As we use WordNet 2.1, we
regard that the correct senses for the context are
runner#6, win#1 and marathon#2.
Figure 1 graphically depicts the disambiguation
process carried out by our method. The boxes in
the figure represent the obtained clusters, which are
sorted regarding the ranking function (scores are un-
der the boxes).
Initially, all word senses are clustered using
00=0.049 (the 90th-percentile of the pairwise
similarities between the senses). It can be seen
in the figure that the first cluster comprises the
sense runner#6 (the star), which is the sense
refering to a trained athlete who competes in foot
races, and runner#4, which is the other sense
of runner related with sports. Also, it includes
the sense win#1 that concerns to the victory in
a race or competition, and marathon#2 that
refers to a footrace. It can be easily appreciated
that this first cluster includes senses that cover
the set of disambiguating words. Hence, it is
selected by the filter and all other clusters are
</bodyText>
<figureCaption confidence="0.9601445">
Figure 1: Disambiguation of words in “The runner
won the marathon”.
</figureCaption>
<bodyText confidence="0.967970941176471">
discarded. After this step, S is updated with the set
{runner#6, runner#4, win#1, marathon#2}. 2
In this point of the process, the senses of S do not
disambiguate W because the noun runner has two
senses in S. Therefore, the sttoping criterion does
not hold because neither |S |=6 |W  |and 00(2) =
0.104 =6 1. Consequently, a new cluster distribution
must be obtained using the current set S.
The boxes in the bottom of Figure 1 represent
the new clusters. In this case, all clusters are sin-
gles. Obviously, the cluster containing the sense
runner#4 is discarded because the cluster that in-
cludes the sense runner#6 overlaps better with the
context, and therefore precedes it in the order.
Then, the final set of selected senses is S =
{runner#6, win#1, marathon#2}, which in-
cludes only one sense for each word in W.
</bodyText>
<sectionHeader confidence="0.977957" genericHeader="method">
3 SemEval-2007 Results
</sectionHeader>
<bodyText confidence="0.998369">
Our system participated in the Coarse-grained En-
glish all-words task (task 7) and in the English fine-
grained all-words subtask (task 17). In both cases,
the disambiguation process was performed at the
sentence level. Thus, we defined the intended tex-
tual context T for a sentence to be the bag of all its
lemmatized content words. However, W was set up
in a different manner for each task.
We present our results only in terms of the F1
measure. Recall and Precision values are omitted
</bodyText>
<footnote confidence="0.991493">
2In the figure, doubly-boxed clusters depict the selected ones
by the filter.
</footnote>
<page confidence="0.992202">
324
</page>
<table confidence="0.996226">
Test set F1
d001 0.78804
d002 0.72559
d003 0.69400
d004 0.70753
d005 0.58551
Total 0.70207
</table>
<tableCaption confidence="0.994324">
Table 1: TKB-UO results in Coarse-grained English
all-words task.
</tableCaption>
<table confidence="0.99981025">
Category Instances F1
Noun 161 0.367
Verb 304 0.303
All 465 0.325
</table>
<tableCaption confidence="0.896658333333333">
Table 2: TKB-UO results in English Fine-grained
all-words subtask.
because our method achieves a 100 % of Coverage.
</tableCaption>
<subsectionHeader confidence="0.995839">
3.1 Coarse-grained English All-words Task
</subsectionHeader>
<bodyText confidence="0.999517769230769">
Firstly, it is worth mentioning that we do not use
the coarse-grained inventory provided by the com-
petition for this task. Indeed, our approach can be
viewed as a method to build such a coarse-grained
inventory as it clusters tightly related senses.
Each W was defined as the set of all tagged words
belonging to the sentence under consideration. Ta-
ble 3.1 shows the official results obtained by our sys-
tem.
As it can be appreciated, the effectiveness of our
method was around the 70 %, except in the fifth
test document (d005), which is an excerpt of stories
about Italian painters.
</bodyText>
<subsectionHeader confidence="0.999393">
3.2 English Fine-grained All-words Subtask
</subsectionHeader>
<bodyText confidence="0.9999829375">
Similar to previous task, we included into each W
those tagged words of the disambiguating sentence.
However, as the set of tagged words per sentence
was verb-plentiful, with very few nouns, we ex-
panded W with the rest of nouns and adjectives of
the sentence.
Table 3.2 summarizes the results (split by word
categories) obtained in this subtask. The second col-
umn of the table shows the number of disambiguat-
ing word occurrences.
As we can see, in this subtask only nouns and
verbs were required to be disambiguated, and over-
all, verbs predominate over nouns. The poor per-
formance obtained by verbs (w.r.t. nouns) can be
explained by its high polysemy degree and its rela-
tively small number of relations in WordNet.
</bodyText>
<sectionHeader confidence="0.999662" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999980785714286">
In this paper, we have described the TKB-UO sys-
tem for WSD at SemEval-2007. This knowledge-
driven system relies on a novel way of using cluster-
ing in the WSD area. Also, it benefits from topic sig-
natures built from WordNet, which in combination
with the clustering algorithm overcomes the sparse-
ness of WordNet relations for associating semanti-
cally related word senses. The system participated
in both the Coarse-grained English all-words task
(task 7) and the English fine-grained all-words sub-
task (task 17). Since we use sense clustering, we do
not use the coarse-grained sense inventory provided
by the competition for task 7. Further work will fo-
cus on improving the results of fine-grained WSD.
</bodyText>
<sectionHeader confidence="0.99869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987128875">
Eneko Agirre and Oier L´opez. 2003. Clustering wordnet
word senses. Proceedings of the Conference on Recent
Advances on Natural Language Processing, pp. 121–
130
Henry Anaya-S´anchez, Aurora Pons-Porrata, and Rafael
Berlanga-Llavori. 2006. Word sense disambiguation
based on word sense clustering. Lecture Notes in Arti-
fzcialIntelligence, 4140:472–481.
Reynaldo Gil-Garcia, Jos´e M. Badia-Contelles, and Au-
rora Pons-Porrata. 2003 Extended Star Clustering
Algorithm. Lecture Notes on Computer Sciences,
2905:480–487
Chin-Yew Lin and Eduard Hovy. 2000. The Automated
Acquisition of Topic Signatures for Text Summariza-
tion. Proceedings of the COLING Conference, pp.
495–501
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name Discrimination by Clustering Sim-
ilar Contexts. Lecture Notes in Computer Science,
3406:226–237
Gerard Salton, A. Wong, and C.S. Yang. 1975. A
Vector Space Model for Information Retrieval. Jour-
nal of the American Society for Information Science,
18(11):613–620
</reference>
<page confidence="0.999019">
325
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.223014">
<title confidence="0.99912">TKB-UO: Using Sense Clustering for WSD</title>
<author confidence="0.998807">Aurora Rafael</author>
<note confidence="0.443939333333333">of Pattern Recognition and Data Mining, Universidad de Oriente, Cuba Science, Universitat Jaume I, Spain 1</note>
<abstract confidence="0.986470818181818">This paper describes the clustering-based approach to Word Sense Disambiguation that is followed by the TKB-UO system at SemEval-2007. The underlying disambiguation method only uses WordNet as external resource, and does not use training data. Results obtained in both Coarse-grained English all-words task (task 7) and English fine-grained all-words subtask (task 17) are presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier L´opez</author>
</authors>
<title>Clustering wordnet word senses.</title>
<date>2003</date>
<booktitle>Proceedings of the Conference on Recent Advances on Natural Language Processing,</booktitle>
<pages>121--130</pages>
<marker>Agirre, L´opez, 2003</marker>
<rawString>Eneko Agirre and Oier L´opez. 2003. Clustering wordnet word senses. Proceedings of the Conference on Recent Advances on Natural Language Processing, pp. 121– 130</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Anaya-S´anchez</author>
<author>Aurora Pons-Porrata</author>
<author>Rafael Berlanga-Llavori</author>
</authors>
<title>Word sense disambiguation based on word sense clustering.</title>
<date>2006</date>
<booktitle>Lecture Notes in ArtifzcialIntelligence,</booktitle>
<pages>4140--472</pages>
<marker>Anaya-S´anchez, Pons-Porrata, Berlanga-Llavori, 2006</marker>
<rawString>Henry Anaya-S´anchez, Aurora Pons-Porrata, and Rafael Berlanga-Llavori. 2006. Word sense disambiguation based on word sense clustering. Lecture Notes in ArtifzcialIntelligence, 4140:472–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reynaldo Gil-Garcia</author>
<author>Jos´e M Badia-Contelles</author>
<author>Aurora Pons-Porrata</author>
</authors>
<title>Extended Star Clustering Algorithm.</title>
<date>2003</date>
<journal>Lecture Notes on Computer Sciences,</journal>
<pages>2905--480</pages>
<marker>Gil-Garcia, Badia-Contelles, Pons-Porrata, 2003</marker>
<rawString>Reynaldo Gil-Garcia, Jos´e M. Badia-Contelles, and Aurora Pons-Porrata. 2003 Extended Star Clustering Algorithm. Lecture Notes on Computer Sciences, 2905:480–487</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The Automated Acquisition of Topic Signatures for Text Summarization.</title>
<date>2000</date>
<booktitle>Proceedings of the COLING Conference,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="3094" citStr="Lin and Hovy, 2000" startWordPosition="478" endWordPosition="481">erformed again (regarding the remaining senses) until the disambiguation is achieved. Algorithm 1 shows the general steps of our proposal for the disambiguation of a set of words W. In the algorithm, clustering represents the basic clustering method, filter is the function that selects the clusters, and T denotes the intended textual context from which words in W are disambiguated (typically, a broader bag of words than W). Next subsections describe in detail each component of the whole process. 2.1 Sense Representation For clustering purposes, word senses are represented as topic signatures (Lin and Hovy, 2000). Thus, for each word sense s we define a vector 322 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 322–325, Prague, June 2007. c�2007 Association for Computational Linguistics Algorithm 1 Clustering-based approach for the disambiguation of the set of words W in the textual context T Input: The finite set of words W and the textual context T. Output: The disambiguated word senses. Let S be the set of all senses of words in W, and i = 0; repeat i = i + 1 G = clustering(S, β0(i)) G0 = filter(G, W, T) S = � {s|s E g} g∈G until |S |= |W |or β0(i + 1) = </context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The Automated Acquisition of Topic Signatures for Text Summarization. Proceedings of the COLING Conference, pp. 495–501</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Amruta Purandare</author>
<author>Anagha Kulkarni</author>
</authors>
<title>Name Discrimination by Clustering Similar Contexts.</title>
<date>2005</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>3406--226</pages>
<contexts>
<context position="1243" citStr="Pedersen et al., 2005" startWordPosition="180" endWordPosition="183">h fine-grained all-words subtask (task 17) are presented. 1 Introduction The TKB-UO system relies on the knowledgedriven approach to Word Sense Disambiguation (WSD) presented in (Anaya-S´anchez et al., 2006). Regarding that meaningful senses of words in a textual unit must be coherently related, our proposal uses sense clustering with the aim of determining cohesive groups of senses that reflect the connectivity of the disambiguating words. The way this proposal uses clustering for disambiguation purposes is different from those usages reported in other works of the WSD area. For example, in (Pedersen et al., 2005) textual contexts are clustered in order to represent senses for Word Sense Discrimination. Other works like (Agirre and L´opez, 2003), cluster fine-grained word senses into coarse-grained ones for polysemy reduction. Instead, our method clusters all possible senses corresponding to all words in a disambiguating textual unit. Thus, our system implements a novel clustering approach for the contextual disambiguation of words. We use the lexical resource WordNet (version 2.1) as the repository of word senses, and also as the provider of sense representations. It is worth mentioning that our propo</context>
</contexts>
<marker>Pedersen, Purandare, Kulkarni, 2005</marker>
<rawString>Ted Pedersen, Amruta Purandare, and Anagha Kulkarni. 2005. Name Discrimination by Clustering Similar Contexts. Lecture Notes in Computer Science, 3406:226–237</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A Vector Space Model for Information Retrieval.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="4344" citStr="Salton et al., 1975" startWordPosition="710" endWordPosition="713">m : σm), where each ti is a WordNet term highly correlated to s with an association weight σi. The set of signature terms for a word sense includes all its WordNet hyponyms, its directly related terms (including coordinated terms) and their filtered and lemmatized glosses. To weight signature terms, the tf-idf statistics is used, considering each word as a collection and its senses as its of documents. Topic signatures of senses form a Vector Space Model similar to those defined in Information Retrieval Systems. In this way, they can be compared with measures such as cosine, Dice and Jaccard (Salton et al., 1975). In (Anaya-S´anchez et al., 2006), it is shown that this kind of WordNet-based signatures outperform those Web-based ones developed by the Ixa Research Group 1 in the disambiguation of nouns. 2.2 Clustering Algorithm Sense clustering is carried out by the Extended Star Clustering Algorithm (Gil et al., 2003), which builds star-shaped and overlapped clusters. Each cluster consists of a star and its satellites, where the star is the sense with the highest connectivity of the cluster, and the satellites are those senses connected with the star. The connectivity is defined in terms of the β0-simi</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, A. Wong, and C.S. Yang. 1975. A Vector Space Model for Information Retrieval. Journal of the American Society for Information Science, 18(11):613–620</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>