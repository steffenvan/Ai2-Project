<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002583">
<title confidence="0.994931">
Automatic Detection of Causal Relations for Question Answering
</title>
<author confidence="0.994194">
Roxana Girju
</author>
<affiliation confidence="0.886964">
Computer Science Department
Baylor University
Waco, Texas
</affiliation>
<email confidence="0.999254">
roxana@cs.baylor.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966642857143">
Causation relations are a pervasive fea-
ture of human language. Despite this, the
automatic acquisition of causal informa-
tion in text has proved to be a difficult
task in NLP. This paper provides a method
for the automatic detection and extraction
of causal relations. We also present an
inductive learning approach to the auto-
matic discovery of lexical and semantic
constraints necessary in the disambigua-
tion of causal relations that are then used
in question answering. We devised a clas-
sification of causal questions and tested
the procedure on a QA system.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999843068965517">
The automatic detection of semantic information in
English texts is a very difficult task, as English is
highly ambiguous. However, there are many appli-
cations which can greatly benefit from in depth se-
mantic analysis of text. Question Answering is one
of them.
An important semantic relation for many applica-
tions is the causation relation. Although many com-
putational linguists focused their attention on this
semantic relation, they used hand-coded patterns to
extract causation information from text.
This work has been motivated by our desire to
analyze cause-effect questions that are currently be-
yond the state-of-the-art in QA technology. This pa-
per provides an inductive learning approach to the
automatic discovery of lexical and semantic con-
straints necessary in the disambiguation of verbal
causal relations. After a brief review of the previ-
ous work in Computational Linguistics on causation
in section 2, we present in section 3 a classification
of lexico-syntactic patterns that are used to express
causation in English texts and show the difficulties
involved in the automatic detection and extraction of
these patterns. A method for automatic detection of
causation patterns and validation of ambiguous ver-
bal lexico-syntactic patterns referring to causation is
proposed in section 4. Results are discussed in sec-
tion 5, and in section 6 the application of causal re-
lations in Question Answering is demonstrated.
</bodyText>
<sectionHeader confidence="0.981296" genericHeader="introduction">
2 Previous Work in Computational
Linguistics
</sectionHeader>
<bodyText confidence="0.999802535714286">
Computational linguists have tried to tackle the no-
tion of causality in natural language focusing on lex-
ical and semantic constructions that can express this
relation.
Many previous studies have attempted to extract
implicit inter-sentential cause-effect relations from
text using knowledge-based inferences (Joskowiscz
et al. 1989), (Kaplan 1991). These studies were
based on hand-coded, domain-specific knowledge
bases difficult to scale up for realistic applications.
More recently, other researchers (Garcia 1997),
(Khoo et al. 2000) used linguistic patterns to iden-
tify explicit causation relations in text without any
knowledge-based inference. Garcia used French
texts to capture causation relationships through lin-
guistic indicators organized in a semantic model
which classifies causative verbal patterns. She found
25 causal relations with an approach based on the
“Force Dynamics” of Leonard Talmy claiming a pre-
cision of 85%.
Khoo at al. used predefined verbal linguistic pat-
terns to extract cause-effect information from busi-
ness and medical newspaper texts. They presented
a simple computational method based on a set of
partially parsed linguistic patterns that usually indi-
cate the presence of a causal relationship. The rela-
tionships were determined by exact matching on text
with a precision of about 68%.
</bodyText>
<sectionHeader confidence="0.964414" genericHeader="method">
3 How are causation relations expressed in
English?
</sectionHeader>
<bodyText confidence="0.9994229">
Any causative construction involves two compo-
nents, the cause and its effect. For example:
“The bus fails to turn up. As a result, I am late
for a meeting”.(Comrie 1981)
Here the cause is represented by the bus’s failing
to turn up, and the effect by my being late for the
meeting.
In English, the causative constructions can be ex-
plicit or implicit. Usually, explicit causation pat-
terns can contain relevant keywords such as cause,
effect, consequence, but also ambiguous ones such
as generate, induce, etc. The implicit causative con-
structions are more complex, involving inference
based on semantic analysis and background knowl-
edge. The English language provides a multitude of
cause-effect expressions that are very productive. In
this paper we focus on explicit but ambiguous verbal
causation patterns and provide a detailed computa-
tional analysis. A list of other causation expressions
were presented in detail elsewhere (Girju 2002).
</bodyText>
<subsectionHeader confidence="0.910513">
Causation verbs
</subsectionHeader>
<bodyText confidence="0.9606505">
Many linguists focused their attention on causative
verbal constructions that can be classified based on
a lexical decomposition. This decomposition builds
a taxonomy of causative verbs according to whether
they define only the causal link or the causal link
plus other components of the two entities that are
causally related (Nedjalkov and Silnickij 1969):
1. Simple causatives (cause, lead to, bring about,
generate, make, force, allow, etc.)
Here the linking verb refers only to the causal
link, being synonymous with the verb cause. E.g.,
“Earthquakes generate tidal waves.”
</bodyText>
<listItem confidence="0.9472066">
2. Resultative causatives (kill, melt, dry, etc.)
These verbs refer to the causal link plus a part of the
resulting situation.
3. Instrumental causatives (poison (killing by poi-
soning), hang, punch, clean, etc.)
</listItem>
<bodyText confidence="0.9958645">
These causatives express a part of the causing event
as well as the result.
</bodyText>
<sectionHeader confidence="0.973503" genericHeader="method">
4 Automatic detection of causation
relationships
</sectionHeader>
<bodyText confidence="0.999947888888889">
In this section we describe a method for automatic
detection of lexico-syntactic patterns that express
causation.
The algorithm consists of two major procedures.
The first procedure discovers lexico-syntactic pat-
terns that can express the causation relation, and the
second procedure presents an inductive learning ap-
proach to the automatic detection of syntactic and
semantic constraints on the constituent components.
</bodyText>
<subsectionHeader confidence="0.9983315">
4.1 Automatic discovery of lexico-syntactic
patterns referring to causation
</subsectionHeader>
<bodyText confidence="0.968798929824561">
One of the most frequent explicit intra-sentential
patterns that can express causation is
. In this paper we focus on this kind of pat-
terns, where the verb is a simple causative.
In order to catch the most frequently used lexico-
syntactic patterns referring to causation, we used the
following procedure (Hearst 1998):
Discovery oflexico-syntactic patterns:
Input: semantic relation R
Output: lexico-syntactic patterns expressing R
STEP 1. Pick a semantic relation R (in this paper,
CAUSATION)
STEP 2. Pick a pair of noun phrases ,among
which R holds.
Since CAUSE-TO is one of the semantic relations
explicitly used in WordNet, this is an excellent re-
source for picking and . The CAUSE-TO rela-
tion is a transitive relation between verb synsets. For
example, in WordNet the second sense of the verb
develop is “causes to grow”. Although WordNet
contains numerous causation relationships between
nouns that are always true, they are not directly men-
tioned. One way to determine such relationships is
to look for all patterns that
occur between a noun entry and another noun in the
corresponding gloss definition. One such example is
the causation relationship between bonyness and
starvation . The gloss of bonyness (#1/1) is (ex-
treme leanness (usually caused by starvation or dis-
ease)).
WordNet 1.7 contains 429 such relations linking
nouns from different domains, the most frequent be-
ing medicine (about 58.28%).
STEP 3. Extract lexico-syntactic patterns that link
the two selected noun phrases by searching a collec-
tion of texts.
For each pair of causation nouns determined
above, search the Internet or any other collection of
documents and retain only the sentences containing
the pair. From these sentences, determine automat-
ically all the patterns verb/verb expression
, where - is the pair consid-
ered.
The result is a list of verbs/verbal expressions that
refer to causation (see Table 1). Some of these verbs
are always referring to causation, but most of them
are ambiguous, as they express a causation relation
only in a particular context and only between spe-
cific pairs of nouns. For example, produces
. In most cases, the verb produce has the
sense of manufacture, and only in some particular
contexts it refers to causation.
In this approach, the acquisition of linguistic pat-
terns is done automatically, as the pattern is prede-
fined ( verb ). As described in the next
subsections, the relationships are disambiguated and
only those referring to causation are retained.
</bodyText>
<subsectionHeader confidence="0.994607">
4.2 Learning Syntactic and Semantic
Constraints for causal relation
</subsectionHeader>
<bodyText confidence="0.99955216">
The learning procedure proposed here is supervised,
for the learning algorithm is provided with a set of
inputs along with the corresponding set of correct
outputs. Based on a set of positive and negative
causal training examples provided and annotated by
the user, the algorithm creates a decision tree and a
set of rules that classify new data. The rules produce
constraints on the noun constituents of the lexical
patterns.
For the discovery of the semantic constraints we
used C4.5 decision tree learning (Quinlan 1999).
The learned function is represented by a decision
tree, or a set of if-then rules. The decision tree learn-
ing searches a complete hypothesis space from sim-
ple to complex hypotheses until it finds a hypothesis
consistent with the data. Its bias is a preference for
the shorter tree that places high information gain at-
tributes closer to the root.
The error in the training examples can be over-
come by using different training and a test corpora,
or by cross-validation techniques.
C4.5 receives in general two input files, the
NAMES file defining the names of the attributes, at-
tribute values and classes, and the DATA file con-
taining the examples.
</bodyText>
<subsectionHeader confidence="0.6310015">
4.2.1 Preprocessing Causal Lezico-Syntactic
Patterns
</subsectionHeader>
<bodyText confidence="0.999973153846154">
Since a part of our constraint learning procedure
is based on the semantic information provided by
WordNet, we need to preprocess the noun phrases
(NPs) extracted and identify the cause and the effect.
For each NP we keep only the largest word sequence
(from left to right) that is defined in WordNet as a
concept.
For example, from the noun phrase “a 7.1 magni-
tude earthquake” the procedure retains only “earth-
quake”, as it is the WordNet concept with the largest
number of words in the noun phrase.
We did not consider those noun phrases in which
the head word had other part of speech than noun.
</bodyText>
<subsectionHeader confidence="0.981871">
4.2.2 Building the Training Corpus and the
Test Corpus
</subsectionHeader>
<bodyText confidence="0.999423375">
In order to learn the constraints, we used the LA
TIMES section of the TREC 9 text collection. For
each of the 60 verbs generated with the procedure
described in section 4.1, we searched the text collec-
tion and retained 120 sentences containing the verb.
Thus, a training corpus “A” of 6,000 sentences, and
respectively, a test corpus of 1,200 sentences were
automatically created. Each sentence in these cor-
pora was then parsed using the syntactic parser de-
veloped by Charniak (Charniak 1999).
Focusing only on the sentences containing rela-
tions indicated by the pattern considered, we manu-
ally annotated all instances matched by the pattern
as referring to causation or not. Using the training
corpus, the system extracted 6,523 relationships of
the type verb , from which 2,101 were
</bodyText>
<table confidence="0.999622434782609">
Causal verbs
give rise (to) stir up create start
induce entail launch make
produce contribute (to) develop begin
generate set up bring rise
effect trigger off stimulate
bring about commence call forth
provoke set off unleash
arouse set in motion effectuate
elicit bring on kick up
lead (to) conduce (to) give birth (to)
trigger educe
derive (from) originate in call down
associate (with) lead off put forward
relate (to) spark cause
link (to) spark off
stem (from) evoke
originate link up
bring forth implicate (in)
lead up activate
trigger off actuate
bring on kindle
result (from) fire up
</table>
<tableCaption confidence="0.999548">
Table 1: Ambiguous causation verbs detected with the procedure described in section 4.1.
</tableCaption>
<bodyText confidence="0.960707">
causal relations, while 4,422 were not.
</bodyText>
<subsectionHeader confidence="0.816057">
4.2.3 Selecting features
</subsectionHeader>
<bodyText confidence="0.999891098039216">
The next step consists of detecting the constraints
necessary on nouns and verb for the pattern
verb such that the lexico-syntactic pattern in-
dicates a causation relationship.
The basic idea we employ here is that only some
categories of noun phrases can be associated with
a causation link. According to the philosophy re-
searcher Jaegwon Kim (Kim 1993), any discussion
of causation implies an ontological framework of
entities among which causal relations are to hold,
and also ”an accompanying logical and semanti-
cal framework in which these entities can be talked
about”. He argues that the entities that represent
either causes or effects are often events, but also
conditions, states, phenomena, processes, and some-
times even facts, and that coherent causal talk is pos-
sible only within a coherent ontological framework
of such states of affairs.
Many researchers ((Blaheta and Charniak 2000),
(Gildea and Jurafsky 2000), showed that lexical and
syntactic information is very useful for predicate-
argument recognition tasks, such as semantic roles.
However, lexical and syntactic information alone is
not sufficient for the detection of complex semantic
relations, such as CAUSE.
Based on these considerents and on our observa-
tions of the English texts, we selected a list of 19
features which are divided here into two categories:
lexical and semantic features.
The lexical feature is represented by the causa-
tion verb in the pattern considered. As verb senses
in WordNet are fine grained providing a large list
of semantic hierarchies the verb can belong to, we
decided to use only the lexical information the verb
provides. The values of this feature are represented
by the 60 verbs detected with the procedure de-
scribed in section 4.1. This feature is very impor-
tant, as our intention here is to capture the semantic
information brought by the verb in combination with
the subject and object noun phrases that attach to it.
As we don’t use word sense disambiguation to
disambiguate each noun phrase in context, we have
to take into consideration all the WordNet semantic
hierarchies they belong to according to each sense.
For each noun phrase representing the cause, and re-
spectively the effect, we used as semantic features
the 9 noun hierarchies in WordNet: entity, psycho-
logical feature, abstraction, state, event, act, group,
possession, and phenomenon. Each feature is true if
it is one of the semantic classes the noun phrase can
belong to, and false otherwise.
</bodyText>
<subsectionHeader confidence="0.750869">
4.2.4 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999779977777778">
Input: positive and negative causal examples
Output: lexical and semantic constraints
Step 1. Generalize the training examples
Initially, the training corpus consists of examples
that contain only lexical features in the following
format:
cause NP; verb; effect NP;
target ,
where target can be either “Yes” or “No”, depending
whether or not an example encodes cause.
For example, earthquake; generate;
Tsunami; Yes indicates that between the noun
“earthquake” and the noun “Tsunami” there is a
cause relation.
From this intermediate corpus a generalized set of
training examples was built, by expanding each in-
termediate example with the list of semantic features
using the following format:
For instance, the initial example becomes f,
f, f, f, f, f,f, f, t, generate,
f, f, f, f, f,t, f, f, f, yes ,as
the noun phrase earthquake belongs only to the
phenomenon noun hierarchy and the noun
phrase Tsunami is only in the event noun
hierarchy in WordNet.
Step 2. Learning constraints from training examples
For the examples in the generalized training cor-
pus (those that are either positive or negative), con-
straints are determined using C4.5. In this context,
the features are the characteristics that distinguish
the causal relation, and the values of the features are
either specific words (e.g., the verb) or their Word-
Net corresponding semantic classes (the furthest an-
cestors in WordNet of the corresponding concept).
On this training corpus we applied C4.5 using a
10-fold cross validation. The output is represented
by 10 sets of rules generated from the positive and
negative examples.
The rules in each set were ranked according to
their frequency of occurrence and average accuracy
obtained for that particular set. In order to use the
best rules, we decided to keep only the ones that had
a frequency above a threshold (occur in at least 7 of
the 10 sets of rules) and with an average accuracy
greater than 60 .
</bodyText>
<subsubsectionHeader confidence="0.759862">
4.2.5 The Constraints
</subsubsectionHeader>
<bodyText confidence="0.999518833333333">
Table 2 summarizes the constraints learned by the
program.
As we can notice, the constraints combine in-
formation about the semantic classes of the noun
phrases representing the cause and effect with the
lexical information about the verb.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.99809515625">
To validate the constraints for extracting causal rela-
tions, we used the test corpus “B”.
For each head of the noun phrases in the CAUSE
and EFFECT positions, the system determined auto-
matically the most general subsumers in WordNet
for each sense. The test corpus contained 683 re-
lationships of the type verb , from
which only 115 were causal patterns. The results
provided by the causal relation discovery procedure
were validated by a human annotator.
Let us define the precision and recall performance
metrics in this context.
The system retrieved 138 relations, of which 102
were causal relations and 36 were non-causal rela-
tions, yielding a precision of 73.91% and a recall of
88.69%. Table 3 shows the results obtained for the
pattern considered.
However, there were other 38 causal relations
found in the corpus, expressed by other than the
lexico-syntactic pattern considered in this paper,
entityNP1,
psychological-featureNP1,
abstractionNP1, stateNP1,
eventNP1, actNP1, groupNP1,
possessionNP1, phenomenonNP1;
verb;
entityNP2,
psychological-featureNP2,
abstractionNP2, stateNP2,
eventNP2, actNP2, groupNP2,
possessionNP2, phenomenonNP2;
target .
</bodyText>
<table confidence="0.966766">
Nr Class-NP1 verb Class-NP2 Target Acc.(%) Freq. Example
0 * cause * 1 100 18 hunger causes headache
1 * * phenomenon 1 98 38 movement triggers earthquake
2 !entity associated-with !abstraction and 1 63.00 26 syndromes are
or related-to !group and associated with disease
!possession
3 !entity * event 1 89 24 inactivation induces events
4 !abstraction * event or act 1 90 12 event generated group action
5 * lead-to !entity and !group 1 88 21 intake leads to immunodeficiency
6 * induce entity or abstraction 0 70.0 10 carcinogens induce fields
7 * * !state and 0 70.7 10 path leads to house
!event and
!act and group
8 entity * !state and 0 70.0 10 cells derived from lymph nodes
!event and
!phenomenon
</table>
<tableCaption confidence="0.9908005">
Table 2: The list of constrains accompanied by examples (! means “is not”, 1 means “Is a causal relation,”,
0 means “Is not a causal relation”, and * means anything)
</tableCaption>
<bodyText confidence="0.990474705882353">
yielding a global causal relation coverage (recall) of
66.6 [102/115+38].
The errors are explained mostly by the fact that
the causal pattern is very ambiguous. This lexico-
syntactic pattern encode numerous relations which
are very difficult to disambiguate based only on the
list of connectors.
The errors were also caused by the incorrect pars-
ing of noun phrases, the use of the rules with smaller
accuracy (e.g. 63 ), and the lack of named enti-
ties recognition in WordNet (e.g., names of people,
places, etc.).
Some of the factors that contributed a lot to the
precision and recall results were the size and the ac-
curacy of the positive and negative examples in the
training corpus. For this experiment we used only a
fairly small training corpus of 6,523 examples.
</bodyText>
<sectionHeader confidence="0.7142405" genericHeader="method">
6 Importance and application of causal
relations in Question Answering
</sectionHeader>
<bodyText confidence="0.999203363636364">
Causation relationships are very pervasive, but most
of the time they are ambiguous or implicit. The de-
gree of ambiguity of these relations varies with the
semantic possibilities of interpretation of the con-
stituent syntactic terms. This disambiguation proves
to be very useful for applications like Question An-
swering.
Causation questions can be mainly introduced by
the following question types: what, which, name
(what causes/be the cause of, what be the effect of,
what happens when/after, what cause vb object ),
</bodyText>
<table confidence="0.9990359">
No. of Relations Causal pattern
Number of patterns 683
Number of correct 115
relations
Number of relations 138
retrieved
Number of correctly 102
retrieved relations
Precision 73.91
Recall 88.69
</table>
<tableCaption confidence="0.998216">
Table 3: The number of relations obtained and the
</tableCaption>
<bodyText confidence="0.926030142857143">
accuracy for the causal pattern used for this research.
how (how causation adj ), and why. However, an
analysis of these question types alone is not suffi-
cient for causation, another classification criteria be-
ing required. Based on our observation of cause-
effect questions, we propose the following question
classes based on their ambiguity:
</bodyText>
<sectionHeader confidence="0.633232" genericHeader="method">
1. Explicit causation questions
</sectionHeader>
<bodyText confidence="0.974700222222222">
The question contains explicit unambiguous key-
words that define the type of relation, and deter-
mines the semantic type of the question (e.g., effect,
cause, consequence, etc.)
“What are the causes of lung cancer?”
“Name the effects of radiation on health.”
“Which were the consequences of Mt. Saint
Elena eruption on fish?”
2. Ambiguous (semi-explicit) causation questions
The question contains explicit but ambiguous key-
words that refer to the causation relation. Once dis-
ambiguated, they help in the detection of the seman-
tic type of the question (e.g., lead to, produce, gen-
erate, trigger, create, etc.)
“Does watching violent cartoons create aggres-
sion in children?”
“What economic events led to the extreme
wealth among Americans in the early 1920’s?”
</bodyText>
<sectionHeader confidence="0.89561" genericHeader="method">
3. Implicit causation questions
</sectionHeader>
<bodyText confidence="0.998788527777778">
This type of questions involves reasoning, based on
deep semantic analysis and background knowledge.
They are usually introduced by the semantic types
why, what, how, and can be further classified in two
important subtypes:
a) Causation questions disambiguated based on the
semantic analysis of question keywords
“Why did Socrates die?”
“What killed Socrates?”
”How dangerous is a volcanic eruption?”
”Is exercise good to the brain?”
It is recognized that questions of type what, and
even how and why, are ambiguous, and usually the
question is disambiguated by other keywords in the
question.
In the example question “What killed Socrates?”,
the verb kill is a causation verb meaning cause to
die, so the second question asks for the cause of the
Socrates’ death.
The why questions are more complex asking for
explanations or justifications. Explanations can be
expressed in English in different ways, not always
referring to causation. Thus, it is very difficult to
determine directly from the question what kind of
information we should look for in the answer.
b) Causation questions that are disambiguated based
on how the answer is expressed in the text
Behavioral psychologists illustrated that there are
several different ways of answering why questions in
biology. For example, the question “Why do robins
sing in the spring?” can have multiple categories of
answers:
Causation. (What is the cause?)
Answer: “Robins sing in spring because increases in
day length trigger hormonal action”.
Development. (How does it develop?)
Answer: “Robins sing in spring because they have
learned songs from their fathers and neighbors.”
Origin. (How did it evolve?)
Answer: “Song evolved as a means of communica-
tion early in the avian lineage”.
Function. (What is the function?)
Answer: “Robins sing in spring to attract mates.”
The algorithm for automatic extraction of causa-
tion relations presented in section 4 was tested on
a list of 50 natural language causation questions
(50 explicit and 50 ambiguous) using a state-of-
the-art Question Answering system (Harabagiu et
al. 2001). The questions were representative for
the first two categories of causation questions pre-
sented above, namely explicit and ambiguous cau-
sation questions. We selected for this purpose the
TREC9 text collection and we (semi-automatically)
searched it for 50 distinct relationships of the type
, where the verb was one of the
60 causal verbs considered. For each such relation-
ship we formulated a cause-effect question of the
first two types presented above. We also made sure
each question had the answer in the documents gen-
erated by the IR module.
Table 4 shows two examples of questions from
each class. We also considered as good answer any
other correct answer different from the one repre-
sented by the causal pattern. However these an-
swers were not taken into consideration in the preci-
sion calculation of the QA system with the causation
module included. The rational was that we wanted
to measure only the contribution of the causal re-
lations method. The 50 questions were tested on
the QA system with (61% precision) and without
(36% precision) the causation module included, with
a gain in precision of 25%.
</bodyText>
<sectionHeader confidence="0.997143" genericHeader="conclusions">
7 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.994153714285714">
The approach presented in this paper for the detec-
tion and validation of causation patterns is a novel
one. Other authors (Khoo et al. 2000) restricted
their text corpus to a medical/business database and
used hand-coded causation patterns that were mostly
unambiguous. Our method discovers automatically
generally applicable lexico-syntactic patterns refer-
ring to causation and disambiguates the causation re-
lationships obtained from the pattern application on
Question Question Answer
Class
QA without causation module QA with causation module
Explicit What causes post- Post-traumatic Stress Disorder - What Post-traumatic stress disorder
traumatic stress disorder? are the Symptoms and Causes? results from a traumatic event.
What are the effects Projects, reports, and information about Acid rain is known to
of acid rain? the effects of acid rain contribute to
the corrosion of metals..
Ambiguous What can trigger The protein is consistent with something An antigen producing an allergic
an allergic reaction? that triggers an allergic reaction reaction is defined as an allergen.
What phenomenon is .. that deglaciation are associated There are often earthquakes
with volcanoes? associated with increased volcanic activity.. generated by volcanism..
</bodyText>
<tableCaption confidence="0.953556">
Table 4: Examples of cause-effect questions tested on a Question Answering system.
</tableCaption>
<bodyText confidence="0.996242">
text. Moreover, we showed that the automatic detec-
tion of causal relations is very important in Question
Answering for answering cause-effect questions.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906">
D. Blaheta and E. Charniak, Assigning Function Tags to
Parsed Text. In Proceedings of the 1st Annual Meet-
ing of the North American Chapter of the Association
for Computational Linguistics, Seattle, May 2000, pp.
234–240.
E. Charniak, A maximum-entropy-inspired parser. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics (NAACL
2000), Seattle, WA.
B. Comrie. Causative constructions In Language
Universals and Linguistic Typology, University of
Chicago Press, Chicago, 1981.
S. Harabagiu, D. Moldovan, M. Pasca, M. Surdeanu, R.
Mihalcea, R. Girju, V. Rus, F. Lacatusu, P. Moraescu,
and R. Bunescu. 2001. Answering Complex, List and
Context Questions with LCC’s Question-Answering
Server. In Proceedings of the TExt Retrieval Confer-
ence for Question Answering (TREC 10).
M. Hearst. Automated Discovery of WordNet Rela-
tions. In WordNet: An Electronic Lexical Database
and Some ofits Applications, editor Fellbaum, C., MIT
Press, 1998.
D. Garcia. COATIS, an NLP system to locate expressions
of actions connected by causality links. In Knowledge
Acquisition, Modeling and Mangement, The Tenth Eu-
ropean Workshop, 1997.
D. Gildea and D. Jurafsky. Automatic Labeling of Se-
mantic Roles. In Proceedings of the 38th Annual Con-
ference of the Association for Computational Linguis-
tics (ACL-00), pages 512-520, Hong Kong, October
2000.
R. Girju. Text Mining for Semantic Relations. Ph.D.
Dissertation, University of Texas at Dallas, May 2002.
L. Joskowiscz, T. Ksiezyk and R. Grishman. Deep do-
main models for discourse anaysis. In The Annual AI
Systems in Government Conference.
R.M. Kaplan, and G. Berry-Rogghe. Knowledge-based
acquisition of causal relationships in text. In Knowl-
edge Acquisition, 3(3), 1991.
C. Khoo, S. Chan and Y. Niu. Extracting Causal Knowl-
edge from a Medical Database Using Graphical Pat-
terns In Proceedings ofACL, Hong Kong, 2000.
J. Kim. Causes and Events: Mackie on Causation. In
Causation, ed. Ernest Sosa, and Michael Tooley, Ox-
ford University Press, 1993.
V.P. Nedjalkov and G. Silnickij. The topology of
causative constructions. In Folia Linguistica (6).
J.R. Quinlan. C4.5: Programs for Machine Learning.
Morgan Kaufmann.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.246420">
<title confidence="0.999835">Automatic Detection of Causal Relations for Question Answering</title>
<author confidence="0.985698">Roxana</author>
<affiliation confidence="0.993611">Computer Science</affiliation>
<address confidence="0.402408">Baylor Waco,</address>
<email confidence="0.999935">roxana@cs.baylor.edu</email>
<abstract confidence="0.9988834">Causation relations are a pervasive feature of human language. Despite this, the automatic acquisition of causal information in text has proved to be a difficult task in NLP. This paper provides a method for the automatic detection and extraction of causal relations. We also present an inductive learning approach to the automatic discovery of lexical and semantic constraints necessary in the disambiguation of causal relations that are then used in question answering. We devised a classification of causal questions and tested the procedure on a QA system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Blaheta</author>
<author>E Charniak</author>
</authors>
<title>Assigning Function Tags to Parsed Text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>234--240</pages>
<location>Seattle,</location>
<contexts>
<context position="12820" citStr="Blaheta and Charniak 2000" startWordPosition="2019" endWordPosition="2022">th a causation link. According to the philosophy researcher Jaegwon Kim (Kim 1993), any discussion of causation implies an ontological framework of entities among which causal relations are to hold, and also ”an accompanying logical and semantical framework in which these entities can be talked about”. He argues that the entities that represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts, and that coherent causal talk is possible only within a coherent ontological framework of such states of affairs. Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicateargument recognition tasks, such as semantic roles. However, lexical and syntactic information alone is not sufficient for the detection of complex semantic relations, such as CAUSE. Based on these considerents and on our observations of the English texts, we selected a list of 19 features which are divided here into two categories: lexical and semantic features. The lexical feature is represented by the causation verb in the pattern considered. As verb senses in WordNet are fine grained pro</context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>D. Blaheta and E. Charniak, Assigning Function Tags to Parsed Text. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, Seattle, May 2000, pp. 234–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="12820" citStr="Charniak 2000" startWordPosition="2021" endWordPosition="2022">on link. According to the philosophy researcher Jaegwon Kim (Kim 1993), any discussion of causation implies an ontological framework of entities among which causal relations are to hold, and also ”an accompanying logical and semantical framework in which these entities can be talked about”. He argues that the entities that represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts, and that coherent causal talk is possible only within a coherent ontological framework of such states of affairs. Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicateargument recognition tasks, such as semantic roles. However, lexical and syntactic information alone is not sufficient for the detection of complex semantic relations, such as CAUSE. Based on these considerents and on our observations of the English texts, we selected a list of 19 features which are divided here into two categories: lexical and semantic features. The lexical feature is represented by the causation verb in the pattern considered. As verb senses in WordNet are fine grained pro</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak, A maximum-entropy-inspired parser. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Comrie</author>
</authors>
<title>Causative constructions In Language Universals and Linguistic Typology,</title>
<date>1981</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago,</location>
<contexts>
<context position="3760" citStr="Comrie 1981" startWordPosition="570" endWordPosition="571">85%. Khoo at al. used predefined verbal linguistic patterns to extract cause-effect information from business and medical newspaper texts. They presented a simple computational method based on a set of partially parsed linguistic patterns that usually indicate the presence of a causal relationship. The relationships were determined by exact matching on text with a precision of about 68%. 3 How are causation relations expressed in English? Any causative construction involves two components, the cause and its effect. For example: “The bus fails to turn up. As a result, I am late for a meeting”.(Comrie 1981) Here the cause is represented by the bus’s failing to turn up, and the effect by my being late for the meeting. In English, the causative constructions can be explicit or implicit. Usually, explicit causation patterns can contain relevant keywords such as cause, effect, consequence, but also ambiguous ones such as generate, induce, etc. The implicit causative constructions are more complex, involving inference based on semantic analysis and background knowledge. The English language provides a multitude of cause-effect expressions that are very productive. In this paper we focus on explicit b</context>
</contexts>
<marker>Comrie, 1981</marker>
<rawString>B. Comrie. Causative constructions In Language Universals and Linguistic Typology, University of Chicago Press, Chicago, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>M Surdeanu</author>
<author>R Mihalcea</author>
<author>R Girju</author>
<author>V Rus</author>
<author>F Lacatusu</author>
<author>P Moraescu</author>
<author>R Bunescu</author>
</authors>
<title>Answering Complex, List and Context Questions with LCC’s Question-Answering Server.</title>
<date>2001</date>
<booktitle>In Proceedings of the TExt Retrieval Conference for Question Answering (TREC 10).</booktitle>
<contexts>
<context position="23487" citStr="Harabagiu et al. 2001" startWordPosition="3732" endWordPosition="3735">es in day length trigger hormonal action”. Development. (How does it develop?) Answer: “Robins sing in spring because they have learned songs from their fathers and neighbors.” Origin. (How did it evolve?) Answer: “Song evolved as a means of communication early in the avian lineage”. Function. (What is the function?) Answer: “Robins sing in spring to attract mates.” The algorithm for automatic extraction of causation relations presented in section 4 was tested on a list of 50 natural language causation questions (50 explicit and 50 ambiguous) using a state-ofthe-art Question Answering system (Harabagiu et al. 2001). The questions were representative for the first two categories of causation questions presented above, namely explicit and ambiguous causation questions. We selected for this purpose the TREC9 text collection and we (semi-automatically) searched it for 50 distinct relationships of the type , where the verb was one of the 60 causal verbs considered. For each such relationship we formulated a cause-effect question of the first two types presented above. We also made sure each question had the answer in the documents generated by the IR module. Table 4 shows two examples of questions from each </context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Surdeanu, Mihalcea, Girju, Rus, Lacatusu, Moraescu, Bunescu, 2001</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, M. Surdeanu, R. Mihalcea, R. Girju, V. Rus, F. Lacatusu, P. Moraescu, and R. Bunescu. 2001. Answering Complex, List and Context Questions with LCC’s Question-Answering Server. In Proceedings of the TExt Retrieval Conference for Question Answering (TREC 10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automated Discovery of WordNet Relations.</title>
<date>1998</date>
<booktitle>In WordNet: An Electronic Lexical Database and Some ofits Applications, editor Fellbaum, C.,</booktitle>
<publisher>MIT Press,</publisher>
<contexts>
<context position="6278" citStr="Hearst 1998" startWordPosition="953" endWordPosition="954">yntactic patterns that can express the causation relation, and the second procedure presents an inductive learning approach to the automatic detection of syntactic and semantic constraints on the constituent components. 4.1 Automatic discovery of lexico-syntactic patterns referring to causation One of the most frequent explicit intra-sentential patterns that can express causation is . In this paper we focus on this kind of patterns, where the verb is a simple causative. In order to catch the most frequently used lexicosyntactic patterns referring to causation, we used the following procedure (Hearst 1998): Discovery oflexico-syntactic patterns: Input: semantic relation R Output: lexico-syntactic patterns expressing R STEP 1. Pick a semantic relation R (in this paper, CAUSATION) STEP 2. Pick a pair of noun phrases ,among which R holds. Since CAUSE-TO is one of the semantic relations explicitly used in WordNet, this is an excellent resource for picking and . The CAUSE-TO relation is a transitive relation between verb synsets. For example, in WordNet the second sense of the verb develop is “causes to grow”. Although WordNet contains numerous causation relationships between nouns that are always t</context>
</contexts>
<marker>Hearst, 1998</marker>
<rawString>M. Hearst. Automated Discovery of WordNet Relations. In WordNet: An Electronic Lexical Database and Some ofits Applications, editor Fellbaum, C., MIT Press, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>COATIS</author>
</authors>
<title>an NLP system to locate expressions of actions connected by causality links.</title>
<date>1997</date>
<booktitle>In Knowledge Acquisition, Modeling and Mangement, The Tenth European Workshop,</booktitle>
<marker>COATIS, 1997</marker>
<rawString>D. Garcia. COATIS, an NLP system to locate expressions of actions connected by causality links. In Knowledge Acquisition, Modeling and Mangement, The Tenth European Workshop, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Conference of the Association for Computational Linguistics (ACL-00),</booktitle>
<pages>512--520</pages>
<location>Hong Kong,</location>
<contexts>
<context position="12848" citStr="Gildea and Jurafsky 2000" startWordPosition="2023" endWordPosition="2026">g to the philosophy researcher Jaegwon Kim (Kim 1993), any discussion of causation implies an ontological framework of entities among which causal relations are to hold, and also ”an accompanying logical and semantical framework in which these entities can be talked about”. He argues that the entities that represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts, and that coherent causal talk is possible only within a coherent ontological framework of such states of affairs. Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicateargument recognition tasks, such as semantic roles. However, lexical and syntactic information alone is not sufficient for the detection of complex semantic relations, such as CAUSE. Based on these considerents and on our observations of the English texts, we selected a list of 19 features which are divided here into two categories: lexical and semantic features. The lexical feature is represented by the causation verb in the pattern considered. As verb senses in WordNet are fine grained providing a large list of seman</context>
</contexts>
<marker>Gildea, Jurafsky, 2000</marker>
<rawString>D. Gildea and D. Jurafsky. Automatic Labeling of Semantic Roles. In Proceedings of the 38th Annual Conference of the Association for Computational Linguistics (ACL-00), pages 512-520, Hong Kong, October 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Text Mining for Semantic Relations.</title>
<date>2002</date>
<tech>Ph.D. Dissertation,</tech>
<institution>University of Texas at Dallas,</institution>
<contexts>
<context position="4531" citStr="Girju 2002" startWordPosition="689" endWordPosition="690">explicit or implicit. Usually, explicit causation patterns can contain relevant keywords such as cause, effect, consequence, but also ambiguous ones such as generate, induce, etc. The implicit causative constructions are more complex, involving inference based on semantic analysis and background knowledge. The English language provides a multitude of cause-effect expressions that are very productive. In this paper we focus on explicit but ambiguous verbal causation patterns and provide a detailed computational analysis. A list of other causation expressions were presented in detail elsewhere (Girju 2002). Causation verbs Many linguists focused their attention on causative verbal constructions that can be classified based on a lexical decomposition. This decomposition builds a taxonomy of causative verbs according to whether they define only the causal link or the causal link plus other components of the two entities that are causally related (Nedjalkov and Silnickij 1969): 1. Simple causatives (cause, lead to, bring about, generate, make, force, allow, etc.) Here the linking verb refers only to the causal link, being synonymous with the verb cause. E.g., “Earthquakes generate tidal waves.” 2.</context>
</contexts>
<marker>Girju, 2002</marker>
<rawString>R. Girju. Text Mining for Semantic Relations. Ph.D. Dissertation, University of Texas at Dallas, May 2002.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Joskowiscz</author>
<author>T Ksiezyk</author>
<author>R Grishman</author>
</authors>
<title>Deep domain models for discourse anaysis.</title>
<booktitle>In The Annual AI Systems in Government Conference.</booktitle>
<marker>Joskowiscz, Ksiezyk, Grishman, </marker>
<rawString>L. Joskowiscz, T. Ksiezyk and R. Grishman. Deep domain models for discourse anaysis. In The Annual AI Systems in Government Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>G Berry-Rogghe</author>
</authors>
<title>Knowledge-based acquisition of causal relationships in text.</title>
<date>1991</date>
<journal>In Knowledge Acquisition,</journal>
<volume>3</volume>
<issue>3</issue>
<marker>Kaplan, Berry-Rogghe, 1991</marker>
<rawString>R.M. Kaplan, and G. Berry-Rogghe. Knowledge-based acquisition of causal relationships in text. In Knowledge Acquisition, 3(3), 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Khoo</author>
<author>S Chan</author>
<author>Y Niu</author>
</authors>
<title>Extracting Causal Knowledge from a Medical Database Using Graphical Patterns</title>
<date>2000</date>
<booktitle>In Proceedings ofACL, Hong Kong,</booktitle>
<contexts>
<context position="2752" citStr="Khoo et al. 2000" startWordPosition="410" endWordPosition="413">uestion Answering is demonstrated. 2 Previous Work in Computational Linguistics Computational linguists have tried to tackle the notion of causality in natural language focusing on lexical and semantic constructions that can express this relation. Many previous studies have attempted to extract implicit inter-sentential cause-effect relations from text using knowledge-based inferences (Joskowiscz et al. 1989), (Kaplan 1991). These studies were based on hand-coded, domain-specific knowledge bases difficult to scale up for realistic applications. More recently, other researchers (Garcia 1997), (Khoo et al. 2000) used linguistic patterns to identify explicit causation relations in text without any knowledge-based inference. Garcia used French texts to capture causation relationships through linguistic indicators organized in a semantic model which classifies causative verbal patterns. She found 25 causal relations with an approach based on the “Force Dynamics” of Leonard Talmy claiming a precision of 85%. Khoo at al. used predefined verbal linguistic patterns to extract cause-effect information from business and medical newspaper texts. They presented a simple computational method based on a set of pa</context>
<context position="24774" citStr="Khoo et al. 2000" startWordPosition="3948" endWordPosition="3951">erent from the one represented by the causal pattern. However these answers were not taken into consideration in the precision calculation of the QA system with the causation module included. The rational was that we wanted to measure only the contribution of the causal relations method. The 50 questions were tested on the QA system with (61% precision) and without (36% precision) the causation module included, with a gain in precision of 25%. 7 Discussion and Conclusions The approach presented in this paper for the detection and validation of causation patterns is a novel one. Other authors (Khoo et al. 2000) restricted their text corpus to a medical/business database and used hand-coded causation patterns that were mostly unambiguous. Our method discovers automatically generally applicable lexico-syntactic patterns referring to causation and disambiguates the causation relationships obtained from the pattern application on Question Question Answer Class QA without causation module QA with causation module Explicit What causes post- Post-traumatic Stress Disorder - What Post-traumatic stress disorder traumatic stress disorder? are the Symptoms and Causes? results from a traumatic event. What are t</context>
</contexts>
<marker>Khoo, Chan, Niu, 2000</marker>
<rawString>C. Khoo, S. Chan and Y. Niu. Extracting Causal Knowledge from a Medical Database Using Graphical Patterns In Proceedings ofACL, Hong Kong, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
</authors>
<title>Causes and Events: Mackie on Causation.</title>
<date>1993</date>
<editor>In Causation, ed. Ernest Sosa, and Michael Tooley,</editor>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="12276" citStr="Kim 1993" startWordPosition="1937" endWordPosition="1938">bring forth implicate (in) lead up activate trigger off actuate bring on kindle result (from) fire up Table 1: Ambiguous causation verbs detected with the procedure described in section 4.1. causal relations, while 4,422 were not. 4.2.3 Selecting features The next step consists of detecting the constraints necessary on nouns and verb for the pattern verb such that the lexico-syntactic pattern indicates a causation relationship. The basic idea we employ here is that only some categories of noun phrases can be associated with a causation link. According to the philosophy researcher Jaegwon Kim (Kim 1993), any discussion of causation implies an ontological framework of entities among which causal relations are to hold, and also ”an accompanying logical and semantical framework in which these entities can be talked about”. He argues that the entities that represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts, and that coherent causal talk is possible only within a coherent ontological framework of such states of affairs. Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and sy</context>
</contexts>
<marker>Kim, 1993</marker>
<rawString>J. Kim. Causes and Events: Mackie on Causation. In Causation, ed. Ernest Sosa, and Michael Tooley, Oxford University Press, 1993.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V P Nedjalkov</author>
<author>G Silnickij</author>
</authors>
<title>The topology of causative constructions.</title>
<journal>In Folia Linguistica</journal>
<volume>6</volume>
<marker>Nedjalkov, Silnickij, </marker>
<rawString>V.P. Nedjalkov and G. Silnickij. The topology of causative constructions. In Folia Linguistica (6).</rawString>
</citation>
<citation valid="false">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<publisher>Morgan Kaufmann.</publisher>
<marker>Quinlan, </marker>
<rawString>J.R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>