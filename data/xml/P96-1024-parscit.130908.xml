<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000088">
<title confidence="0.628578">
Parsing Algorithms and Metrics
</title>
<author confidence="0.979849">
Joshua Goodman
</author>
<affiliation confidence="0.982354">
Harvard University
</affiliation>
<address confidence="0.9727965">
33 Oxford St.
Cambridge, MA 02138
</address>
<email confidence="0.945133">
goodmanOdas.harvard.edu
</email>
<sectionHeader confidence="0.992754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990655">
Many different metrics exist for evaluating
parsing results, including Viterbi, Cross-
ing Brackets Rate, Zero Crossing Brackets
Rate, and several others. However, most
parsing algorithms, including the Viterbi
algorithm, attempt to optimize the same
metric, namely the probability of getting
the correct labelled tree. By choosing
a parsing algorithm appropriate for the
evaluation metric, better performance can
be achieved. We present two new algo-
rithms: the &amp;quot;Labelled Recall Algorithm,&amp;quot;
which maximizes the expected Labelled
Recall Rate, and the &amp;quot;Bracketed Recall
Algorithm,&amp;quot; which maximizes the Brack-
eted Recall Rate. Experimental results
are given, showing that the two new al-
gorithms have improved performance over
the Viterbi algorithm on many criteria, es-
pecially the ones that they optimize
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999285">
In corpus-based approaches to parsing, one is given
a treebank (a collection of text annotated with the
&amp;quot;correct&amp;quot; parse tree) and attempts to find algo-
rithms that, given unlabelled text from the treebank,
produce as similar a parse as possible to the one in
the treebank.
Various methods can be used for finding these
parses. Some of the most common involve induc-
ing Probabilistic Context-Free Grammars (PCFGs),
and then parsing with an algorithm such as the La-
belled Tree (Viterbi) Algorithm, which maximizes
the probability that the output of the parser (the
&amp;quot;guessed&amp;quot; tree) is the one that the PCFG produced.
This implicitly assumes that the induced PCFG does
a good job modeling the corpus.
There are many different ways to evaluate these
parses. The most common include the Labelled
Tree Rate (also called the Viterbi Criterion or Ex-
act Match Rate), Consistent Brackets Recall Rate
(also called the Crossing Brackets Rate), Consis-
tent Brackets Tree Rate (also called the Zero Cross-
ing Brackets Rate), and Precision and Recall. De-
spite the variety of evaluation metrics, nearly all re-
searchers use algorithms that maximize performance
on the Labelled Tree Rate, even in domains where
they are evaluating using other criteria.
We propose that by creating algorithms that op-
timize the evaluation criterion, rather than some
related criterion, improved performance can be
achieved.
In Section 2, we define most of the evaluation
metrics used in this paper and discuss previous ap-
proaches. Then, in Section 3, we discuss the La-
belled Recall Algorithm, a new algorithm that max-
imizes performance on the Labelled Recall Rate. In
Section 4, we discuss another new algorithm, the
Bracketed Recall Algorithm, that maximizes perfor-
mance on the Bracketed Recall Rate (closely related
to the Consistent Brackets Recall Rate). Finally, we
give experimental results in Section 5 using these
two algorithms in appropriate domains, and com-
pare them to the Labelled Tree (Viterbi) Algorithm,
showing that each algorithm generally works best
when evaluated on the criterion that it optimizes.
</bodyText>
<sectionHeader confidence="0.994778" genericHeader="introduction">
2 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.99990575">
In this section, we first define basic terms and sym-
bols. Next, we define the different metrics used in
evaluation. Finally, we discuss the relationship of
these metrics to parsing algorithms.
</bodyText>
<subsectionHeader confidence="0.970746">
2.1 Basic Definitions
</subsectionHeader>
<bodyText confidence="0.999065454545454">
Let wa denote word a of the sentence under consid-
eration. Let wab denote wa wa +1 ... tvb _ ; in partic-
ular let writ denote the entire sequence of terminals
(words) in the sentence under consideration.
In this paper we assume all guessed parse trees are
binary branching. Let a parse tree T be defined as a
set of triples (s, t, X)—where s denotes the position
of the first symbol in a constituent, t denotes the
position of the last symbol, and X represents a ter-
minal or nonterminal symbol—meeting the following
three requirements:
</bodyText>
<page confidence="0.992149">
177
</page>
<listItem confidence="0.978687888888889">
• The sentence was generated by the start sym-
bol, S. Formally, (1, n, 8) E T.
• Every word in the sentence is in the parse tree.
Formally, for every s between 1 and n the triple
(s, s, w3) E T.
• The tree is binary branching and consistent.
Formally, for every (s,t, X) in T, s t, there is
exactly one r, Y, and Z such that .s &lt; r &lt; t and
(s, r, Y) E T and (r + 1, t, Z) E T .
</listItem>
<bodyText confidence="0.991778833333333">
Let Tc denote the &amp;quot;correct&amp;quot; parse (the one in the
treebank) and let TG denote the &amp;quot;guessed&amp;quot; parse
(the one output by the parsing algorithm). Let
NG denote ITGI, the number of nonterminals in the
guessed parse tree, and let Nc denote ITc I, the num-
ber of nonterminals in the correct parse tree.
</bodyText>
<subsectionHeader confidence="0.997114">
2.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999958827586207">
There are various levels of strictness for determin-
ing whether a constituent (element of TG) is &amp;quot;cor-
rect.&amp;quot; The strictest of these is Labelled Match. A
constituent (s,t, X) E TG is correct according to La-
belled Match if and only if (s, t, X) E T. In other
words, a constituent in the guessed parse tree is cor-
rect if and only if it occurs in the correct parse tree.
The next level of strictness is Bracketed Match.
Bracketed match is like labelled match, except that
the nonterminal label is ignored. Formally, a con-
stituent (s, t, X) E TG is correct according to Brack-
eted Match if and only if there exists a Y such that
(s,t,Y) E T.
The least strict level is Consistent Brackets (also
called Crossing Brackets). Consistent Brackets is
like Bracketed Match in that the label is ignored.
It is even less strict in that the observed (s, t, X)
need not be in Tc —it must simply not be ruled out
by any (q,r,Y) E Tc. A particular triple (q,r,Y)
rules out (s, t, X) if there is no way that (s, t, X)
and (q,r,Y) could both be in the same parse tree.
In particular, if the interval (s, t) crosses the interval
(q, r), then (s, t, X) is ruled out and counted as an
error. Formally, we say that (s, t) crosses (q, r) if
and only ifs&lt;q&lt;t&lt;rorq&lt;s&lt;r&lt;t.
If Tc is binary branching, then Consistent Brack-
ets and Bracketed Match are identical. The follow-
ing symbols denote the number of constituents that
match according to each of these criteria.
</bodyText>
<equation confidence="0.82986525">
L = 1Tc nTGI : the number of constituents
in TG that are correct according to Labelled
Match.
B = if(s,t, X) : (s,t, X) E TG and for some
Y (s, t, Y) E TO&apos;: the number of constituents
in TG that are correct according to Bracketed
Match.
C = I{(s,t, X) E TG : there is no (v, w, Y) E Te
</equation>
<bodyText confidence="0.9751088">
crossing (s, t)} : the number of constituents in
TG correct according to Consistent Brackets.
Following are the definitions of the six metrics
used in this paper for evaluating binary branching
trees:
</bodyText>
<listItem confidence="0.937871">
(1) Labelled Recall Rate = LINc.
(2) Labelled Tree Rate = 1 if L = Nc. It is also
called the Viterbi Criterion.
( 3 ) Bracketed Recall Rate = BINc.
(4) Bracketed Tree Rate = 1 if B = N.
</listItem>
<bodyText confidence="0.971887615384616">
( 5 ) Consistent Brackets Recall Rate = CING. It is
often called the Crossing Brackets Rate. In the
case where the parses are binary branching, this
criterion is the same as the Bracketed Recall
Rate.
(6) Consistent Brackets Tree Rate = 1 if C = NG.
This metric is closely related to the Bracketed
Tree Rate. In the case where the parses are
binary branching, the two metrics are the same.
This criterion is also called the Zero Crossing
Brackets Rate.
The preceding six metrics each correspond to cells
in the following table:
</bodyText>
<table confidence="0.89056675">
Recall Tree
Consistent Brackets C/NG 1 if C = NG
Brackets BINc 1 if B= Nc
Labels LINc 1 if L= Nc
</table>
<subsectionHeader confidence="0.999358">
2.3 Maximizing Metrics
</subsectionHeader>
<bodyText confidence="0.98884544">
Despite this long list of possible metrics, there is
only one metric most parsing algorithms attempt to
maximize, namely the Labelled Tree Rate. That is,
most parsing algorithms assume that the test corpus
was generated by the model, and then attempt to
evaluate the following expression, where E denotes
the expected value operator:
TG arg mTaxE( 1 if = Nc) (1)
This is true of the Labelled Tree Algorithm and
stochastic versions of Earley&apos;s Algorithm (Stolcke,
1993), and variations such as those used in Picky
parsing (Magerman and Weir, 1992). Even in prob-
abilistic models not closely related to PCFGs, such
as Spatter parsing (Magerman, 1994), expression (1)
is still computed. One notable exception is Brill&apos;s
Transformation-Based Error Driven system (Brill,
1993), which induces a set of transformations de-
signed to maximize the Consistent Brackets Recall
Rate. However, Brill&apos;s system is not probabilistic.
Intuitively, if one were to match the parsing algo-
rithm to the evaluation criterion, better performance
should be achieved.
Ideally, one might try to directly maximize
the most commonly used evaluation criteria, such
as Consistent Brackets Recall (Crossing Brackets)
</bodyText>
<page confidence="0.991311">
178
</page>
<bodyText confidence="0.9998224">
Rate. Unfortunately, this criterion is relatively diffi-
cult to maximize, since it is time-consuming to com-
pute the probability that a particular constituent
crosses some constituent in the correct parse. On
the other hand, the Bracketed Recall and Bracketed
Tree Rates are easier to handle, since computing the
probability that a bracket matches one in the correct
parse is inexpensive. It is plausible that algorithms
which optimize these closely related criteria will do
well on the analogous Consistent Brackets criteria.
</bodyText>
<subsectionHeader confidence="0.996258">
2.4 Which Metrics to Use
</subsectionHeader>
<bodyText confidence="0.99999040625">
When building an actual system, one should use the
metric most appropriate for the problem. For in-
stance, if one were creating a database query sys-
tem, such as an ATIS system, then the Labelled Tree
(Viterbi) metric would be most appropriate. A sin-
gle error in the syntactic representation of a query
will likely result in an error in the semantic represen-
tation, and therefore in an incorrect database query,
leading to an incorrect result. For instance, if the
user request &amp;quot;Find me all flights on Tuesday&amp;quot; is mis-
parsed with the prepositional phrase attached to the
verb, then the system might wait until Tuesday be-
fore responding: a single error leads to completely
incorrect behavior. Thus, the Labelled Tree crite-
rion is appropriate.
On the other hand, consider a machine assisted
translation system, in which the system provides
translations, and then a fluent human manually ed-
its them. Imagine that the system is given the
foreign language equivalent of &amp;quot;His credentials are
nothing which should be laughed at,&amp;quot; and makes
the single mistake of attaching the relative clause
at the sentential level, translating the sentence as
&amp;quot;His credentials are nothing, which should make you
laugh.&amp;quot; While the human translator must make
some changes, he certainly needs to do less editing
than he would if the sentence were completely mis-
parsed. The more errors there are, the more editing
the human translator needs to do. Thus, a criterion
such as the Labelled Recall criterion is appropriate
for this task, where the number of incorrect con-
stituents correlates to application performance.
</bodyText>
<sectionHeader confidence="0.977707" genericHeader="method">
3 Labelled Recall Parsing
</sectionHeader>
<bodyText confidence="0.9976591875">
Consider writing a parser for a domain such as ma-
chine assisted translation. One could use the La-
belled Tree Algorithm, which would maximize the
expected number of exactly correct parses. How-
ever, since the number of correct constituents is a
better measure of application performance for this
domain than the number of correct trees, perhaps
one should use an algorithm which maximizes the
Labelled Recall criterion, rather than the Labelled
Tree criterion.
The Labelled Recall Algorithm finds that tree TG
which has the highest expected value for the La-
belled Recall Rate, LINc (where L is the number of
correct labelled constituents, and Nc is the number
of nodes in the correct parse). This can be written
as follows:
</bodyText>
<equation confidence="0.951567">
TG = arg max E(LINc) (2)
</equation>
<bodyText confidence="0.9994604">
It is not immediately obvious that the maximiza-
tion of expression (2) is in fact different from the
maximization of expression (1), but a simple exam-
ple illustrates the difference. The following grammar
generates four trees with equal probability:
</bodyText>
<equation confidence="0.859119142857143">
—AC 0.25
—*AD 0.25
EB 0.25 (3)
—+ FB 0.25
- xx 1.0
A
X XX X X XX X
</equation>
<bodyText confidence="0.994185307692308">
For the first tree, the probabilities of being correct
are S: 100%; A:50%; and C: 25%. Similar counting
holds for the other three. Thus, the expected value
of L for any of these trees is 1.75.
On the other hand, the optimal Labelled Recall
parse is
This tree has 0 probability according to the gram-
mar, and thus is non-optimal according to the La-
belled Tree Rate criterion. However, for this tree
the probabilities of each node being correct are S:
100%; A: 50%; and 8: 50%. The expected value of
L is 2.0, the highest of any tree. This tree therefore
optimizes the Labelled Recall Rate.
</bodyText>
<subsectionHeader confidence="0.994105">
3.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.9468164">
We now derive an algorithm for finding the parse
that maximizes the expected Labelled Recall Rate.
We do this by expanding expression (2) out into a
probabilistic form, converting this into a recursive
equation, and finally creating an equivalent dynamic
programming algorithm.
We begin by rewriting expression (2), expanding
out the expected value operator, and removing the
A, B, C, D, E, F
The four trees are
</bodyText>
<page confidence="0.96712">
179
</page>
<bodyText confidence="0.999669">
c, which is the same for all TG, and so plays no
role in the maximization.
</bodyText>
<equation confidence="0.9936695">
TG = arg max E P(Tc w7) IT n Tci (4)
Tc
This can be further expanded to
TG = arg mpxE P(Tc j w7E 1 if (s, t, X) E Tc
Tc (s ,t,X)ET
(5)
</equation>
<bodyText confidence="0.795781">
Now, given a PCFG with start symbol S, the fol-
lowing equality holds:
</bodyText>
<equation confidence="0.988535666666667">
14-1 X14 4 -1 I wi) =
E P(Tc I .7)( 1 if (s, t, X) E TC) (6)
Tc
</equation>
<bodyText confidence="0.997739">
By rearranging the summation in expression (5)
and then substituting this equality, we get
</bodyText>
<equation confidence="0.976858666666667">
TG = argmpx E P (S 11)1-1 X w4i
(8,t,x)ET I w7)
(7)
</equation>
<bodyText confidence="0.973804">
At this point, it is useful to introduce the Inside
and Outside probabilities, due to Baker (1979), and
explained by Lan and Young (1990). The Inside
probability is defined as e(s,t, X) = P(X Os)
and the Outside probability is f(s,t, X) = P(S
3-1 X n
W1 wt-1-1)• Note that while Baker and others
have used these probabilites for inducing grammars,
here they are used only for parsing.
Let us define a new function, g(s,t, X).
</bodyText>
<equation confidence="0.998581">
g(s,t, X) = P(S wrl X4+1 I .7)
P(S .1-1x4+1)p(x .1)
p(s w7)
f(s,t, X) x e(s,t, X)Ie(1,n, S)
</equation>
<bodyText confidence="0.8011795">
Now, the definition of a Labelled Recall Parse can
be rewritten as
</bodyText>
<equation confidence="0.964761">
TG = arg max E g(s,t, X) (8)
(s ,t,X)ET
</equation>
<bodyText confidence="0.998775666666667">
Given the matrix g(s,t, X), it is a simple matter of
dynamic programming to determine the parse that
maximizes the Labelled Recall criterion. Define
</bodyText>
<equation confidence="0.692336166666667">
MAXC(s, t) = mtx g(s, t, X)+
max (MAXC(s, r) MAXC(r 1,t))
rls&lt;r&lt;t
for length := 2 to n
for s := 1 to n-length+1
t := s + length - 1;
</equation>
<bodyText confidence="0.558904">
loop over nonterminals X
let max_g:=maximum of g(s,t,X)
loop over r such that s &lt;= r &lt; t
let best_split:=
max of maxc [s ,r] + maxc [r+1, t]
maxc[s, t] := max_g + best_split;
</bodyText>
<figureCaption confidence="0.999707">
Figure 1: Labelled Recall Algorithm
</figureCaption>
<bodyText confidence="0.9999106875">
It is clear that MAXC(1, n) contains the score of
the best parse according to the Labelled Recall cri-
terion. This equation can be converted into the dy-
namic programming algorithm shown in Figure 1.
For a grammar with r rules and k nonterminals,
the run time of this algorithm is 0(n3 kn2) since
there are two layers of outer loops, each with run
time at most n, and an inner loop, over nonterminals
and n. However, this is dominated by the computa-
tion of the Inside and Outside probabilities, which
takes time 0(rn3).
By modifying the algorithm slightly to record the
actual split used at each node, we can recover the
best parse. The entry maxc [1, n] contains the ex-
pected number of correct constituents, given the
model.
</bodyText>
<sectionHeader confidence="0.989183" genericHeader="method">
4 Bracketed Recall Parsing
</sectionHeader>
<bodyText confidence="0.999862291666667">
The Labelled Recall Algorithm maximizes the ex-
pected number of correct labelled constituents.
However, many commonly used evaluation met-
rics, such as the Consistent Brackets Recall
Rate, ignore labels. Similarly, some gram-
mar induction algorithms, such as those used by
Pereira and Schabes (1992) do not produce mean-
ingful labels. In particular, the Pereira and Schabes
method induces a grammar from the brackets in the
treebank, ignoring the labels. While the induced
grammar has labels, they are not related to those
in the treebank. Thus, although the Labelled Recall
Algorithm could be used in these domains, perhaps
maximizing a criterion that is more closely tied to
the domain will produce better results. Ideally, we
would maximize the Consistent Brackets Recall Rate
directly. However, since it is time-consuming to deal
with Consistent Brackets, we instead use the closely
related Bracketed Recall Rate.
For the Bracketed Recall Algorithm, we find the
parse that maximizes the expected Bracketed Recall
Rate, BINc. (Remember that B is the number of
brackets that are correct, and Nc is the number of
constituents in the correct parse.)
</bodyText>
<equation confidence="0.643015">
TG = arg max E(B 1 Nc) (9)
</equation>
<page confidence="0.973846">
180
</page>
<listItem confidence="0.863467333333333">
Following a derivation similar to that used for the
Labelled Recall Algorithm, we can rewrite equation
(9) as
</listItem>
<equation confidence="0.932926">
TG= arg max E E p(s wri xwtn+i I tv7)
(s ,t)ET X
(10)
</equation>
<bodyText confidence="0.9996424">
The algorithm for Bracketed Recall parsing is ex-
tremely similar to that for Labelled Recall parsing.
The only required change is that we sum over the
symbols X to calculate max_g, rather than maximize
over them.
</bodyText>
<sectionHeader confidence="0.996002" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.997635054054054">
We describe two experiments for testing these algo-
rithms. The first uses a grammar without meaning-
ful nonterminal symbols, and compares the Brack-
eted Recall Algorithm to the traditional Labelled
Tree (Viterbi) Algorithm. The second uses a gram-
mar with meaningful nonterminal symbols and per-
forms a three-way comparison between the Labelled
Recall, Bracketed Recall, and Labelled Tree Algo-
rithms. These experiments show that use of an algo-
rithm matched appropriately to the evaluation cri-
terion can lead to as much as a 10% reduction in
error rate.
In both experiments the grammars could not parse
some sentences, 0.5% and 9%, respectively. The un-
parsable data were assigned a right branching struc-
ture with their rightmost element attached high.
Since all three algorithms fail on the same sentences,
all algorithms were affected equally.
5.1 Experiment with Grammar Induced by
Pereira and Schabes Method
The experiment of Pereira and Schabes (1992) was
duplicated. In that experiment, a grammar was
trained from a bracketed form of the TI section of the
ATIS corpus&apos; using a modified form of the Inside-
Outside Algorithm. Pereira and Schabes then used
the Labelled Tree Algorithm to select the best parse
for sentences in held out test data. The experi-
ment was repeated here, except that both the La-
belled Tree and Labelled Recall Algorithm were run
for each sentence. In contrast to previous research,
we repeated the experiment ten times, with differ-
ent training set, test set, and initial conditions each
time.
Table 1 shows the results of running this ex-
periment, giving the minimum, maximum, mean,
and standard deviation for three criteria, Consis-
tent Brackets Recall, Consistent Brackets Tree, and
</bodyText>
<footnote confidence="0.967859">
&apos;For our experiments the corpus was slightly
cleaned up. A cliff file for &amp;quot;ed&amp;quot; between the orig-
inal ATIS data and the cleaned-up version is avail-
able from ftp://ftp.das.harvard.edu/pub/goodmaniatis-
ed/ ti_tb.par-ed and ti_tb.pos-ed. The number of
changes made was small, less than 0.2%
</footnote>
<table confidence="0.999452">
Differences
Cons Brack Rec -1.55 2.45 1.01 1.07
Cons Brack Tree -3.41 3.41 -0.34 2.34
Brack Rec -1.34 2.02 0.17 1.20
</table>
<tableCaption confidence="0.998347">
Table 1: Percentages Correct for Labelled Tree ver-
</tableCaption>
<bodyText confidence="0.977912555555556">
sus Bracketed Recall for Pereira and Schabes
Bracketed Recall. We also display these statistics
for the paired differences between the algorithms
The only statistically significant difference is that
for Consistent Brackets Recall Rate, which was sig-
nificant to the 2% significance level (paired t-test).
Thus, use of the Bracketed Recall Algorithm leads
to a 10% reduction in error rate.
In addition, the performance of the Bracketed Re-
call Algorithm was also qualitatively more appeal-
ing. Figure 2 shows typical results. Notice that the
Bracketed Recall Algorithm&apos;s Consistent Brackets
Rate (versus iteration) is smoother and more nearly
monotonic than the Labelled Tree Algorithm&apos;s. The
Bracketed Recall Algorithm also gets off to a much
faster start, and is generally (although not always)
above the Labelled Tree level. For the Labelled Tree
Rate, the two are usually very comparable.
</bodyText>
<subsectionHeader confidence="0.969803">
5.2 Experiment with Grammar Induced by
Counting
</subsectionHeader>
<bodyText confidence="0.999991875">
The replication of the Pereira and Schabes experi-
ment was useful for testing the Bracketed Recall Al-
gorithm. However, since that experiment induces a
grammar with nonterminals not comparable to those
in the training, a different experiment is needed to
evaluate the Labelled Recall Algorithm, one in which
the nonterminals in the induced grammar are the
same as the nonterminals in the test set.
</bodyText>
<subsubsectionHeader confidence="0.973536">
5.2.1 Grammar Induction by Counting
</subsubsectionHeader>
<bodyText confidence="0.999867">
For this experiment, a very simple grammar was
induced by counting, using a portion of the Penn
Tree Bank, version 0.5. In particular, the trees were
first made binary branching by removing epsilon pro-
ductions, collapsing singleton productions, and con-
verting n-ary productions (n &gt; 2) as in figure 3. The
resulting trees were treated as the &amp;quot;Correct&amp;quot; trees in
the evaluation. Only trees with forty or fewer sym-
bols were used in this experiment.
</bodyText>
<figure confidence="0.996600949152543">
Criteria
Min I Max
Mean
SDev
Labelled Tree Algorithm
Cons Brack Rec
Cons Brack Tree
86.06
51.14
93.27
77.27
90.13
63.98
2.57
7.96
Brack Rec
71.38
81.88
75.87
3.18
Bracketed Recall Algorithm
Cons Brack Rec
Cons Brack Tree
88.02
53.41
94.34
76.14
91.14
63.64
2.22
7.82
Brack Rec
72.15
80.69
76.03
3.14
181
50 60 70
10 20 30 40
Iteration Number
Percentage Correct
100
------------------------------------------
--------------------
•• • ,•• — — • `
----- ,
80 -
70 I-
60
50 I,
40 t
30 -
Labelled Tree Algorithm: Consistent Brackets Recall —
Bracketed Recall Algorithm: Consistent Brackets Recall -----
Labelled Tree Algorithm: Labelled Tree
Bracketed Recall Algorithm: Labelled Tree
20
o
90 -
</figure>
<figureCaption confidence="0.965037">
Figure 2: Labelled Tree versus Bracketed Recall in Pereira and Schabes Grammar
</figureCaption>
<figure confidence="0.999313285714286">
X
A X_Cont
B X_Cont
X
becomes
Recall
Tree
Brackets
Labels
Bracketed Recall
Labelled Recall
(NP-Complete)
Labelled Tree
CD
</figure>
<figureCaption confidence="0.900151">
Figure 3: Conversion of Productions to Binary
Branching
</figureCaption>
<bodyText confidence="0.99994">
A grammar was then induced in a straightforward
way from these trees, simply by giving one count for
each observed production. No smoothing was done.
There were 1805 sentences and 38610 nonterminals
in the test data.
</bodyText>
<sectionHeader confidence="0.822601" genericHeader="evaluation">
5.2.2 Results
</sectionHeader>
<bodyText confidence="0.999396">
Table 2 shows the results of running all three algo-
rithms, evaluating against five criteria. Notice that
for each algorithm, for the criterion that it optimizes
it is the best algorithm. That is, the Labelled Tree
Algorithm is the best for the Labelled Tree Rate,
the Labelled Recall Algorithm is the best for the
Labelled Recall Rate, and the Bracketed Recall Al-
gorithm is the best for the Bracketed Recall Rate.
</bodyText>
<tableCaption confidence="0.980719">
Table 3: Metrics and Corresponding Algorithms
</tableCaption>
<sectionHeader confidence="0.99667" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999858166666666">
Matching parsing algorithms to evaluation crite-
ria is a powerful technique that can be used to im-
prove performance. In particular, the Labelled Re-
call Algorithm can improve performance versus the
Labelled Tree Algorithm on the Consistent Brack-
ets, Labelled Recall, and Bracketed Recall criteria.
Similarly, the Bracketed Recall Algorithm improves
performance (versus Labelled Tree) on Consistent
Brackets and Bracketed Recall criteria. Thus, these
algorithms improve performance not only on the
measures that they were designed for, but also on
related criteria.
Furthermore, in some cases these techniques can
make parsing fast when it was previously imprac-
tical. We have used the technique outlined in this
paper in other work (Goodman, 1996) to efficiently
parse the DOP model; in that model, the only pre-
viously known algorithm which summed over all the
</bodyText>
<page confidence="0.99529">
182
</page>
<table confidence="0.999766">
Label Tree 4-54% 48.60% 60.98% 66.35% 12.07%
Label Recall 3.71% 49.66% 61.34% 68.39% 11.63%
Bracket Recall 0.11% 4.51% 6/.63% 68.17% 11.19%
</table>
<tableCaption confidence="0.99896">
Table 2: Grammar Induced by Counting: Three Algorithms Evaluated on Five Criteria
</tableCaption>
<figure confidence="0.9978608">
Criterion
Cons Brack
Recall
Cons Brack
Tree
Algorithm
Label Label
Tree Recall
Brack
Recall
</figure>
<bodyText confidence="0.999617888888889">
possible derivations was a slow Monte Carlo algo-
rithm (Bod, 1993). However, by maximizing the
Labelled Recall criterion, rather than the Labelled
Tree criterion, it was possible to use a much sim-
pler algorithm, a variation on the Labelled Recall
Algorithm. Using this technique, along with other
optimizations, we achieved a 500 times speedup.
In future work we will show the surprising re-
sult that the last element of Table 3, maximizing
the Bracketed Tree criterion, equivalent to maximiz-
ing performance on Consistent Brackets Tree (Zero
Crossing Brackets) Rate in the binary branching
case, is NP-complete. Furthermore, we will show
that the two algorithms presented, the Labelled Re-
call Algorithm and the Bracketed Recall Algorithm,
are both special cases of a more general algorithm,
the General Recall Algorithm. Finally, we hope to
extend this work to the n-ary branching case.
</bodyText>
<sectionHeader confidence="0.997626" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.896233666666667">
I would like to acknowledge support from National
Science Foundation Grant IRI-9350192, National
Science Foundation infrastructure grant CDA 94-
01024, and a National Science Foundation Gradu-
ate Student Fellowship. I would also like to thank
Stanley Chen, Andrew Kehler, Lillian Lee, and Stu-
art Shieber for helpful discussions, and comments on
earlier drafts, and the anonymous reviewers for their
comments.
</bodyText>
<sectionHeader confidence="0.981874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9459908">
Baker, J.K. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547-550, Boston, MA, June.
Bod, Rens. 1993. Using an annotated corpus as a
stochastic grammar. In Proceedings of the Sixth
Conference of the European Chapter of the ACL,
pages 37-44.
Brill, Eric. 1993. A Corpus-Based Approach to Lan-
guage Learning. Ph.D. thesis, University of Penn-
sylvania.
Goodman, Joshua. 1996. Efficient algorithms for
parsing the DOP model. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. To appear.
Lan, K. and S.J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Lan-
guage, 4:35-56.
Magerman, David. 1994. Natural Language Parsing
as Statistical Pattern Recognition. Ph.D. thesis,
Stanford University University, February.
Magerman, D.M. and C. Weir. 1992. Efficiency, ro-
bustness, and accuracy in picky chart parsing. In
Proceedings of the Association for Computational
Linguistics.
Pereira, Fernando and Yves Schabes. 1992. Inside-
Outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting
of the ACL, pages 128-135, Newark, Delaware.
Stolcke, Andreas. 1993. An efficient probabilistic
context-free parsing algorithm that computes pre-
fix probabilities. Technical Report TR-93-065, In-
ternational Computer Science Institute, Berkeley,
CA.
</reference>
<page confidence="0.999055">
183
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657248">
<title confidence="0.99979">Parsing Algorithms and Metrics</title>
<author confidence="0.999997">Joshua Goodman</author>
<affiliation confidence="0.99887">University</affiliation>
<address confidence="0.9923965">33 Oxford St. Cambridge, MA 02138</address>
<email confidence="0.999889">goodmanOdas.harvard.edu</email>
<abstract confidence="0.982605142857143">Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the &amp;quot;Labelled Recall Algorithm,&amp;quot; which maximizes the expected Labelled Recall Rate, and the &amp;quot;Bracketed Recall Algorithm,&amp;quot; which maximizes the Bracketed Recall Rate. Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Proceedings of the Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA,</location>
<contexts>
<context position="13259" citStr="Baker (1979)" startWordPosition="2296" endWordPosition="2297">E, F The four trees are 179 c, which is the same for all TG, and so plays no role in the maximization. TG = arg max E P(Tc w7) IT n Tci (4) Tc This can be further expanded to TG = arg mpxE P(Tc j w7E 1 if (s, t, X) E Tc Tc (s ,t,X)ET (5) Now, given a PCFG with start symbol S, the following equality holds: 14-1 X14 4 -1 I wi) = E P(Tc I .7)( 1 if (s, t, X) E TC) (6) Tc By rearranging the summation in expression (5) and then substituting this equality, we get TG = argmpx E P (S 11)1-1 X w4i (8,t,x)ET I w7) (7) At this point, it is useful to introduce the Inside and Outside probabilities, due to Baker (1979), and explained by Lan and Young (1990). The Inside probability is defined as e(s,t, X) = P(X Os) and the Outside probability is f(s,t, X) = P(S 3-1 X n W1 wt-1-1)• Note that while Baker and others have used these probabilites for inducing grammars, here they are used only for parsing. Let us define a new function, g(s,t, X). g(s,t, X) = P(S wrl X4+1 I .7) P(S .1-1x4+1)p(x .1) p(s w7) f(s,t, X) x e(s,t, X)Ie(1,n, S) Now, the definition of a Labelled Recall Parse can be rewritten as TG = arg max E g(s,t, X) (8) (s ,t,X)ET Given the matrix g(s,t, X), it is a simple matter of dynamic programming </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, J.K. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547-550, Boston, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Using an annotated corpus as a stochastic grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the ACL,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="23484" citStr="Bod, 1993" startWordPosition="3986" endWordPosition="3987">hen it was previously impractical. We have used the technique outlined in this paper in other work (Goodman, 1996) to efficiently parse the DOP model; in that model, the only previously known algorithm which summed over all the 182 Label Tree 4-54% 48.60% 60.98% 66.35% 12.07% Label Recall 3.71% 49.66% 61.34% 68.39% 11.63% Bracket Recall 0.11% 4.51% 6/.63% 68.17% 11.19% Table 2: Grammar Induced by Counting: Three Algorithms Evaluated on Five Criteria Criterion Cons Brack Recall Cons Brack Tree Algorithm Label Label Tree Recall Brack Recall possible derivations was a slow Monte Carlo algorithm (Bod, 1993). However, by maximizing the Labelled Recall criterion, rather than the Labelled Tree criterion, it was possible to use a much simpler algorithm, a variation on the Labelled Recall Algorithm. Using this technique, along with other optimizations, we achieved a 500 times speedup. In future work we will show the surprising result that the last element of Table 3, maximizing the Bracketed Tree criterion, equivalent to maximizing performance on Consistent Brackets Tree (Zero Crossing Brackets) Rate in the binary branching case, is NP-complete. Furthermore, we will show that the two algorithms prese</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>Bod, Rens. 1993. Using an annotated corpus as a stochastic grammar. In Proceedings of the Sixth Conference of the European Chapter of the ACL, pages 37-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8016" citStr="Brill, 1993" startWordPosition="1394" endWordPosition="1395"> parsing algorithms assume that the test corpus was generated by the model, and then attempt to evaluate the following expression, where E denotes the expected value operator: TG arg mTaxE( 1 if = Nc) (1) This is true of the Labelled Tree Algorithm and stochastic versions of Earley&apos;s Algorithm (Stolcke, 1993), and variations such as those used in Picky parsing (Magerman and Weir, 1992). Even in probabilistic models not closely related to PCFGs, such as Spatter parsing (Magerman, 1994), expression (1) is still computed. One notable exception is Brill&apos;s Transformation-Based Error Driven system (Brill, 1993), which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate. However, Brill&apos;s system is not probabilistic. Intuitively, if one were to match the parsing algorithm to the evaluation criterion, better performance should be achieved. Ideally, one might try to directly maximize the most commonly used evaluation criteria, such as Consistent Brackets Recall (Crossing Brackets) 178 Rate. Unfortunately, this criterion is relatively difficult to maximize, since it is time-consuming to compute the probability that a particular constituent crosses some constituent in</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Brill, Eric. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient algorithms for parsing the DOP model.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<note>To appear.</note>
<contexts>
<context position="22988" citStr="Goodman, 1996" startWordPosition="3907" endWordPosition="3908"> the Labelled Recall Algorithm can improve performance versus the Labelled Tree Algorithm on the Consistent Brackets, Labelled Recall, and Bracketed Recall criteria. Similarly, the Bracketed Recall Algorithm improves performance (versus Labelled Tree) on Consistent Brackets and Bracketed Recall criteria. Thus, these algorithms improve performance not only on the measures that they were designed for, but also on related criteria. Furthermore, in some cases these techniques can make parsing fast when it was previously impractical. We have used the technique outlined in this paper in other work (Goodman, 1996) to efficiently parse the DOP model; in that model, the only previously known algorithm which summed over all the 182 Label Tree 4-54% 48.60% 60.98% 66.35% 12.07% Label Recall 3.71% 49.66% 61.34% 68.39% 11.63% Bracket Recall 0.11% 4.51% 6/.63% 68.17% 11.19% Table 2: Grammar Induced by Counting: Three Algorithms Evaluated on Five Criteria Criterion Cons Brack Recall Cons Brack Tree Algorithm Label Label Tree Recall Brack Recall possible derivations was a slow Monte Carlo algorithm (Bod, 1993). However, by maximizing the Labelled Recall criterion, rather than the Labelled Tree criterion, it was </context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Goodman, Joshua. 1996. Efficient algorithms for parsing the DOP model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<contexts>
<context position="13298" citStr="Lan and Young (1990)" startWordPosition="2301" endWordPosition="2304">which is the same for all TG, and so plays no role in the maximization. TG = arg max E P(Tc w7) IT n Tci (4) Tc This can be further expanded to TG = arg mpxE P(Tc j w7E 1 if (s, t, X) E Tc Tc (s ,t,X)ET (5) Now, given a PCFG with start symbol S, the following equality holds: 14-1 X14 4 -1 I wi) = E P(Tc I .7)( 1 if (s, t, X) E TC) (6) Tc By rearranging the summation in expression (5) and then substituting this equality, we get TG = argmpx E P (S 11)1-1 X w4i (8,t,x)ET I w7) (7) At this point, it is useful to introduce the Inside and Outside probabilities, due to Baker (1979), and explained by Lan and Young (1990). The Inside probability is defined as e(s,t, X) = P(X Os) and the Outside probability is f(s,t, X) = P(S 3-1 X n W1 wt-1-1)• Note that while Baker and others have used these probabilites for inducing grammars, here they are used only for parsing. Let us define a new function, g(s,t, X). g(s,t, X) = P(S wrl X4+1 I .7) P(S .1-1x4+1)p(x .1) p(s w7) f(s,t, X) x e(s,t, X)Ie(1,n, S) Now, the definition of a Labelled Recall Parse can be rewritten as TG = arg max E g(s,t, X) (8) (s ,t,X)ET Given the matrix g(s,t, X), it is a simple matter of dynamic programming to determine the parse that maximizes t</context>
</contexts>
<marker>Lan, Young, 1990</marker>
<rawString>Lan, K. and S.J. Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Magerman</author>
</authors>
<title>Natural Language Parsing as Statistical Pattern Recognition.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University University,</institution>
<contexts>
<context position="7893" citStr="Magerman, 1994" startWordPosition="1378" endWordPosition="1379">le metrics, there is only one metric most parsing algorithms attempt to maximize, namely the Labelled Tree Rate. That is, most parsing algorithms assume that the test corpus was generated by the model, and then attempt to evaluate the following expression, where E denotes the expected value operator: TG arg mTaxE( 1 if = Nc) (1) This is true of the Labelled Tree Algorithm and stochastic versions of Earley&apos;s Algorithm (Stolcke, 1993), and variations such as those used in Picky parsing (Magerman and Weir, 1992). Even in probabilistic models not closely related to PCFGs, such as Spatter parsing (Magerman, 1994), expression (1) is still computed. One notable exception is Brill&apos;s Transformation-Based Error Driven system (Brill, 1993), which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate. However, Brill&apos;s system is not probabilistic. Intuitively, if one were to match the parsing algorithm to the evaluation criterion, better performance should be achieved. Ideally, one might try to directly maximize the most commonly used evaluation criteria, such as Consistent Brackets Recall (Crossing Brackets) 178 Rate. Unfortunately, this criterion is relatively difficult t</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>Magerman, David. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, Stanford University University, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
<author>C Weir</author>
</authors>
<title>Efficiency, robustness, and accuracy in picky chart parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7792" citStr="Magerman and Weir, 1992" startWordPosition="1360" endWordPosition="1363">C = NG Brackets BINc 1 if B= Nc Labels LINc 1 if L= Nc 2.3 Maximizing Metrics Despite this long list of possible metrics, there is only one metric most parsing algorithms attempt to maximize, namely the Labelled Tree Rate. That is, most parsing algorithms assume that the test corpus was generated by the model, and then attempt to evaluate the following expression, where E denotes the expected value operator: TG arg mTaxE( 1 if = Nc) (1) This is true of the Labelled Tree Algorithm and stochastic versions of Earley&apos;s Algorithm (Stolcke, 1993), and variations such as those used in Picky parsing (Magerman and Weir, 1992). Even in probabilistic models not closely related to PCFGs, such as Spatter parsing (Magerman, 1994), expression (1) is still computed. One notable exception is Brill&apos;s Transformation-Based Error Driven system (Brill, 1993), which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate. However, Brill&apos;s system is not probabilistic. Intuitively, if one were to match the parsing algorithm to the evaluation criterion, better performance should be achieved. Ideally, one might try to directly maximize the most commonly used evaluation criteria, such as Consistent </context>
</contexts>
<marker>Magerman, Weir, 1992</marker>
<rawString>Magerman, D.M. and C. Weir. 1992. Efficiency, robustness, and accuracy in picky chart parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>InsideOutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the ACL,</booktitle>
<pages>128--135</pages>
<location>Newark, Delaware.</location>
<contexts>
<context position="15326" citStr="Pereira and Schabes (1992)" startWordPosition="2669" endWordPosition="2672">s dominated by the computation of the Inside and Outside probabilities, which takes time 0(rn3). By modifying the algorithm slightly to record the actual split used at each node, we can recover the best parse. The entry maxc [1, n] contains the expected number of correct constituents, given the model. 4 Bracketed Recall Parsing The Labelled Recall Algorithm maximizes the expected number of correct labelled constituents. However, many commonly used evaluation metrics, such as the Consistent Brackets Recall Rate, ignore labels. Similarly, some grammar induction algorithms, such as those used by Pereira and Schabes (1992) do not produce meaningful labels. In particular, the Pereira and Schabes method induces a grammar from the brackets in the treebank, ignoring the labels. While the induced grammar has labels, they are not related to those in the treebank. Thus, although the Labelled Recall Algorithm could be used in these domains, perhaps maximizing a criterion that is more closely tied to the domain will produce better results. Ideally, we would maximize the Consistent Brackets Recall Rate directly. However, since it is time-consuming to deal with Consistent Brackets, we instead use the closely related Brack</context>
<context position="17555" citStr="Pereira and Schabes (1992)" startWordPosition="3035" endWordPosition="3038">een the Labelled Recall, Bracketed Recall, and Labelled Tree Algorithms. These experiments show that use of an algorithm matched appropriately to the evaluation criterion can lead to as much as a 10% reduction in error rate. In both experiments the grammars could not parse some sentences, 0.5% and 9%, respectively. The unparsable data were assigned a right branching structure with their rightmost element attached high. Since all three algorithms fail on the same sentences, all algorithms were affected equally. 5.1 Experiment with Grammar Induced by Pereira and Schabes Method The experiment of Pereira and Schabes (1992) was duplicated. In that experiment, a grammar was trained from a bracketed form of the TI section of the ATIS corpus&apos; using a modified form of the InsideOutside Algorithm. Pereira and Schabes then used the Labelled Tree Algorithm to select the best parse for sentences in held out test data. The experiment was repeated here, except that both the Labelled Tree and Labelled Recall Algorithm were run for each sentence. In contrast to previous research, we repeated the experiment ten times, with different training set, test set, and initial conditions each time. Table 1 shows the results of runnin</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Pereira, Fernando and Yves Schabes. 1992. InsideOutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128-135, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1993</date>
<tech>Technical Report TR-93-065,</tech>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="7714" citStr="Stolcke, 1993" startWordPosition="1349" endWordPosition="1350">s in the following table: Recall Tree Consistent Brackets C/NG 1 if C = NG Brackets BINc 1 if B= Nc Labels LINc 1 if L= Nc 2.3 Maximizing Metrics Despite this long list of possible metrics, there is only one metric most parsing algorithms attempt to maximize, namely the Labelled Tree Rate. That is, most parsing algorithms assume that the test corpus was generated by the model, and then attempt to evaluate the following expression, where E denotes the expected value operator: TG arg mTaxE( 1 if = Nc) (1) This is true of the Labelled Tree Algorithm and stochastic versions of Earley&apos;s Algorithm (Stolcke, 1993), and variations such as those used in Picky parsing (Magerman and Weir, 1992). Even in probabilistic models not closely related to PCFGs, such as Spatter parsing (Magerman, 1994), expression (1) is still computed. One notable exception is Brill&apos;s Transformation-Based Error Driven system (Brill, 1993), which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate. However, Brill&apos;s system is not probabilistic. Intuitively, if one were to match the parsing algorithm to the evaluation criterion, better performance should be achieved. Ideally, one might try to dir</context>
</contexts>
<marker>Stolcke, 1993</marker>
<rawString>Stolcke, Andreas. 1993. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Technical Report TR-93-065, International Computer Science Institute, Berkeley, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>