<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005744">
<title confidence="0.953619">
Image and Collateral Text in Support of Auto-annotation and Sentiment
Analysis
</title>
<author confidence="0.983398">
Pamela Zontone and Giulia Boato Jonathon Hare and Paul Lewis
</author>
<affiliation confidence="0.999112">
University of Trento University of Southampton
</affiliation>
<address confidence="0.616514">
Trento, Italy. Southampton, United Kingdom
</address>
<email confidence="0.969172">
{zontone|boato}@disi.unitn.it {jsh2|phl}@ecs.soton.ac.uk
</email>
<author confidence="0.976661">
Stefan Siersdorfer and Enrico Minack
</author>
<affiliation confidence="0.7631325">
L3S Research Centre
Hannover, Germany
</affiliation>
<email confidence="0.994743">
{siersdorfer|minack}@l3s.de
</email>
<sectionHeader confidence="0.993794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998275">
We present a brief overview of the way
in which image analysis, coupled with
associated collateral text, is being used
for auto-annotation and sentiment analy-
sis. In particular, we describe our ap-
proach to auto-annotation using the graph-
theoretic dominant set clustering algo-
rithm and the annotation of images with
sentiment scores from SentiWordNet. Pre-
liminary results are given for both, and our
planned work aims to explore synergies
between the two approaches.
</bodyText>
<sectionHeader confidence="0.9028075" genericHeader="method">
1 Automatic annotation of images using
graph-theoretic clustering
</sectionHeader>
<bodyText confidence="0.999623583333333">
Recently, graph-theoretic approaches have be-
come popular in the computer vision field. There
exist different graph-theoretic clustering algo-
rithms such as minimum cut, spectral clustering,
dominant set clustering. Among all these algo-
rithms, the Dominant Set Clustering (DSC) is a
promising graph-theoretic approach based on the
notion of a dominant set that has been proposed
for different applications, such as image segmen-
tation (Pavan and Pelillo, 2003), video summariza-
tion (Besiris et al., 2009), etc. Here we describe
the application of DSC to image annotation.
</bodyText>
<subsectionHeader confidence="0.991743">
1.1 Dominant Set Clustering
</subsectionHeader>
<bodyText confidence="0.99952945">
The definition of Dominant Set (DS) was intro-
duced in (Pavan and Pelillo, 2003). Let us con-
sider a set of data samples that have to be clus-
tered. These samples can be represented as an
undirected edge-weighted (similarity) graph with
no self-loops G = (V, E, w), where V = 1, ... , n
is the vertex set, E C_ V x V is the edge set,
and w : E → R∗� is the (positive) weight func-
tion. Vertices in G represent the data points,
whereas edges represent neighborhood relation-
ships, and finally edge-weights reflect similarity
between pairs of linked vertices. An n x n sym-
metric matrix A = (aij), called affinity (or simi-
larity) matrix, can be used to represent the graph
G, where aij = w(i, j) if (i, j) E E, and aij = 0
if i = j. To define formally a Dominant Set, other
parameters have to be introduced. Let S be a non-
empty subset of vertices, with S C_ V , and i E S.
The (average) weighted degree of i relative to S is
defined as:
</bodyText>
<equation confidence="0.94908">
1 awdegS(i) = |S |1: aij
</equation>
<bodyText confidence="0.999876555555556">
where |S |denotes the number of elements in S.
It can be observed that awdeg{i}(i) = 0 for any
i E V . If j E� S we can define the parameter
OS(i, j) = aij − awdegS(i) that is the similarity
between nodes j and i with respect to the average
similarity between node i and its neighbors in S. It
can be noted that O{i}(i, j) = aij, for all i, j E V
with i =� j. Now, if i E S, the weight wS(i) of i
relative to S is:
</bodyText>
<equation confidence="0.987752333333333">
_ 1 if |S |= 1
wS(i) — { ) �{i1(9)
�jES�{j} �S�{i}(jf i ws otherwise.
</equation>
<bodyText confidence="0.999922428571429">
This is a recursive equation where to calculate
wS(i) the weights of the set S\{i} are needed. We
can deduce that wS(i) is a measure of the overall
similarity between the node i and the other nodes
in S\{i}, considering the overall similarity among
the nodes in S\{i}. So, the total weight of S can
be defined as:
</bodyText>
<equation confidence="0.8399155">
W(S) =1: wS(i).
i∈S
</equation>
<bodyText confidence="0.999858">
A non-empty subset of vertices S C_ V such that
W(T) &gt; 0 for any non-empty T C_ S is defined
as a dominant set if the following two conditions
</bodyText>
<equation confidence="0.513113">
j∈S
</equation>
<page confidence="0.978291">
88
</page>
<note confidence="0.6274735">
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 88–92,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998441583333333">
are satisfied: 1. bi E S, wS(i) &gt; 0; and 2.bi E� S,
wSu{i}(i) &lt; 0. These conditions characterize the
internal homogeneity of the cluster and the exter-
nal inhomogeneity of S. As a consequence of this
definition, a dominant set cluster can be derived
from a graph by means of a quadratic program (Pa-
van and Pelillo, 2003). Let x be an n-dimensional
vector, where n is the number of vertices of the
graph and its components indicate the presence of
nodes in the cluster. Let A be the affinity matrix of
the graph. Let us consider the following standard
quadratic program:
</bodyText>
<equation confidence="0.9929095">
max f(x) = xTAx (1)
s.t. x E A
</equation>
<bodyText confidence="0.99514725">
where A = {x &gt; 0 and eTx = 11 is the standard
simplex of Rn. If a point x* E A is a local max-
imum of f, and u(x*) = {i E V : x*i &gt; 01 is
the support of x*, it can be shown that the support
u(x*) is a dominant set for the graph. So, a dom-
inant set can be derived by solving the equation
(1). The following iterative equation can be used
to solve (1):
</bodyText>
<equation confidence="0.9922525">
xi(t + 1) = xi(t) (Ax(t))i
x(t)T Ax(t)
</equation>
<bodyText confidence="0.99994">
where t denotes the number of iterations. To sum-
marize the algorithm, a dominant set is found and
removed from the graph. A second dominant clus-
ter is extracted from the remaining part of the
graph, and so on. This procedure finishes when
all the elements in the graph have been assigned to
a cluster.
</bodyText>
<subsectionHeader confidence="0.992725">
1.2 Image annotation using DSC
</subsectionHeader>
<bodyText confidence="0.999990177419355">
Here we present an approach to automatically an-
notate images using the DSC algorithm. In the
initialization phase (training) the image database
is split into L smaller subsets, corresponding to
the different image categories or visual concepts
that characterize the images in the database. In
this process only tags are exploited: an image is
included in all subsets corresponding to its tags.
Given a subset l, the corresponding affinity ma-
trix Al is calculated and used by the DSC algo-
rithm. Following (Wang et al., 2008), the ele-
ments of the affinity matrix Al = (aij) are de-
fined as aij = e−,(i,j)/r2 where w(i, j) represents
the similarity function between images i and j in
the considered subset l, and r &gt; 0 is the scaling
factor used as an adjustment function that allows
the control of clustering sensitivity. We use the
MPEG.7 descriptors (Sikora, 2001) as features for
computing the similarity between images. Follow-
ing the DSC approach, we can construct all clus-
ters of subset l with similar images, and associate
them with the tag of subset l.
In the test phase, a new image is annotated asso-
ciating to it the tag of the cluster that best matches
the image. To do this, we use a decision algo-
rithm based on the computation of the MSE (Mean
Square Error), where for each cluster we derive a
feature vector that represents all the images in that
cluster (e.g., the average of all the feature vectors).
The tag of the cluster with smaller MSE is used for
the annotation.
For our experiments, we consider a subset of
the Corel database, that consists of 4287 images
in 49 categories (L = 49). The 10% of images in
each category have been randomly selected from
the database and used only for testing. In Fig-
ure 1 we report the annotation accuracy results ob-
tained on 15 different classes with optimal param-
eter r = 0.2. For some classes the accuracy is
very high, whereas for others the accuracy is very
low (under 30%). The total annotation accuracy
considering all the 49 classes is roughly 69%.
In a second set of experiments we consider a
set of 6531 images from the MIR Flickr database
(Huiskes and Lew, 2008), where each image is
tagged with at least one of the chosen 30 visual
concepts (L = 30). Images are characterized by
multiple tags associated to them, thus an image is
included in all the corresponding subsets. For test-
ing we use 875 images. To evaluate the annotation
accuracy we compare the automatically associated
tag with the user defined tags of that image. In Fig-
ure 1 we report the annotation accuracy obtained
for the 30 different categories, with the optimal pa-
rameter r = 0.2. The total annotation accuracy is
about 87%.
Further simulations are in progress to evaluate
the accuracy of multiple tags that can be associ-
ated to the test set in the MIR Flickr database. In-
deed, our idea is to annotate the images consider-
ing the other common tags of the images belong-
ing to each cluster.
</bodyText>
<sectionHeader confidence="0.955255" genericHeader="method">
2 Annotating Sentiment
</sectionHeader>
<bodyText confidence="0.996138333333333">
In the previous section we were concerned with
annotating images with visual concepts, typically
object names or descriptors. A separate strand of
</bodyText>
<page confidence="0.999093">
89
</page>
<figureCaption confidence="0.9806605">
Figure 1: Annotation accuracy for 15 classes of the Corel database (left) and for 30 classes of the MIR
Flickr database (right).
</figureCaption>
<bodyText confidence="0.99981335">
our work is concerned with opinion analysis in
multimedia information and the automatic identi-
fication of sentiment. The study of image indexing
and retrieval in the library and information science
fields has long recognized the importance of sen-
timent in image retrieval (J¨orgensen, 2003; Neal,
2006). It is only recently however, that researchers
interested in automated image analysis and re-
trieval have become interested in the sentiment as-
sociated with images (Wang and He, 2008).
To date, investigations that have looked at the
association between sentiment and image con-
tent have been limited to small datasets (typically
much less than 1000) and rather specific, spe-
cially designed image features. Recently, we have
started to explore how sentiment is related to im-
age content using much more generic visual-term
based features and much larger datasets collected
with the aid of lexical resources such as Senti-
WordNet.
</bodyText>
<subsectionHeader confidence="0.991929">
2.1 SentiWordNet and Image Databases
</subsectionHeader>
<bodyText confidence="0.99996">
SentiWordNet (Esuli and Sebastiani, 2006) is a
lexical resource built on top of WordNet. Word-
Net (Fellbaum, 1998) is a thesaurus containing
textual descriptions of terms and relationships be-
tween terms (examples are hypernyms: “car” is a
subconcept of “vehicle” or synonyms: “car” de-
scribes the same concept as “automobile”). Word-
Net distinguishes between different part-of-speech
types (verb, noun, adjective, etc.). A synset in
WordNet comprises all terms referring to the same
concept (e.g., {car, automobile}). In SentiWord-
Net a triple of three senti-values (pos, neg, obj)
(corresponding to positive, negative, or rather neu-
tral sentiment flavor of a word respectively) are
assigned to each WordNet synset (and, thus, to
each term in the synset). The senti-values are in
the range of [0, 1] and sum up to 1 for each triple.
For instance (pos, neg, obj) _ (0.875, 0.0, 0.125)
for the term “good” or (0.25, 0.375, 0.375) for
the term “ill”. Senti-values were partly created
by human assessors and partly automatically as-
signed using an ensemble of different classifiers
(see (Esuli, 2008) for an evaluation of these meth-
ods).
Popular social websites, such as Flickr, con-
tain massive amounts of visual information in the
form of photographs. Many of these photographs
have been collectively tagged and annotated by
members of the respective community. Recently
in the image analysis community it has become
popular to use Flickr as a resource for building
datasets to experiment with. We have been explor-
ing how we can crawl Flickr for images that have
a strong (positive or negative) sentiment associ-
ated with them. Our initial explorations have been
based around crawling Flickr for images tagged
with words that have very high positive or negative
sentiment according to their SentiWordNet classi-
fication.
Our image dataset has been refined by assign-
ing an overall sentiment value to each image based
on its textual metadata and discarding images with
low overall sentiment. At the simplest level we
use a dictionary of clearly positive and negative
SentiWords, with which we assign a positive (+1)
sentiment value if the text representation only con-
</bodyText>
<page confidence="0.993899">
90
</page>
<bodyText confidence="0.9153694">
positive
negative
Figure 2: Top 16 most discriminative colours (from left to right) for positive and negative sentiment
classes.
tains positive sentiment terms, and a negative (-1)
sentiment value if it only contains negative senti-
ment terms. We discarded images with neither a
positive nor negative score. Currently we are also
exploring more powerful ways to assign sentiment
values to images.
</bodyText>
<subsectionHeader confidence="0.995663">
2.2 Combining Senti-values and Visual
Terms
</subsectionHeader>
<bodyText confidence="0.999970789473684">
In the future we intend to exploit the use of tech-
niques such as the one described in Section 1.2
in order to develop systems that are able to pre-
dict sentiment from image features. However, as a
preliminary study, we have performed some small-
scale experiments on a collection of 10000 images
crawled from Flickr in order to try and see whether
a primitive visual-bag-of-terms (Sivic and Zisser-
man, 2003; Hare and Lewis, 2005) can be asso-
ciated with positive and negative sentiment values
using a linear Support Vector Machine and Sup-
port Vector Regression. The visual-term bag-of-
words for the study was based upon a quantisation
of each pixel in the images into a set of 64 dis-
crete colours (i.e., each pixel corresponds to one
of 64 possible visual terms). Our initial results
look promising and indicate a considerable cor-
relation between the visual bag-of-words and the
sentiment scores.
</bodyText>
<subsectionHeader confidence="0.536821">
Discriminative Analysis of Visual Features. In
</subsectionHeader>
<bodyText confidence="0.998728057142857">
our small-scale study we have also performed
some analysis in order to investigate which visual-
term features are most predictive of the positive
and negative sentiment classes. For this analysis
we have used the Mutual Information (MI) mea-
sure (Manning and Schuetze, 1999; Yang and Ped-
ersen, 1997) from information theory which can
be interpreted as a measure of how much the joint
distribution of features (colour-based visual-terms
in our case) deviate from a hypothetical distribu-
tion in which features and categories (“positive”
and “negative” sentiment) are independent of each
other.
Figure 2 illustrates the 16 most discriminative
colours for the positive and negative classes. The
dominant visual-term features for positive senti-
ment are dominated by earthy colours and skin
tones. Conversely, the features for negative sen-
timent are dominated by blue and green tones.
Interestingly, this association can be explained
through intuition because it mirrors human per-
ception of warm (positive) and cold (negative)
colours.
Currently we are working on expanding our
preliminary experiments to a much larger image
dataset of over half a million images and incor-
porating more powerful visual-term based image
features. In addition to seeking improved ways of
determining image sentiment for the training set
we are planning to combine the dominant set clus-
tering approach to annotation presented in Sec-
tion 1.2 with the sentiment annotation task of this
section and compare the combined approach with
other state of the art approaches as a step towards
achieving robust image sentiment annotation.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="conclusions">
3 Conclusions
</sectionHeader>
<bodyText confidence="0.999934625">
The use of dominant set clustering as a basis for
auto-annotation has shown promise on image col-
lections from both Corel and from Flickr. We have
also shown how that visual-term feature represen-
tations show some promise as indicators of sen-
timent in images. In future work we plan to com-
bine these approaches to provide better support for
opinion analysis of multimedia web documents.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996045">
This work was supported by the European
Union under the Seventh Framework Programme
(FP7/2007-2013) project LivingKnowledge (FP7-
IST-231126), and the LiveMemories project, gra-
ciously funded by the Autonomous Province of
Trento (Italy). The authors are also grateful to the
creators of Flickr for providing an API that can
be used in scientific evaluations and the broader
Flickr community for making images and meta-
data available.
</bodyText>
<page confidence="0.995933">
91
</page>
<bodyText confidence="0.924181">
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, pages 412–420.
</bodyText>
<sectionHeader confidence="0.723012" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.568211">
D. Besiris, A. Makedonas, G. Economou, and S. Fo-
topoulos. 2009. Combining graph connectivity and
dominant set clustering for video summarization.
Multimedia Tools and Applications, 44 (2):161–186.
</bodyText>
<reference confidence="0.993860553191489">
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. LREC, 6.
Andrea Esuli. 2008. Automatic Generation of Lexi-
cal Resources for Opinion Mining: Models, Algo-
rithms and Applications. PhD in Information Engi-
neering, PhD School “Leonardo da Vinci”, Univer-
sity of Pisa.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Jonathon S. Hare and Paul H. Lewis. 2005. On
image retrieval using salient regions with vector-
spaces and latent semantics. In Wee Kheng Leow,
Michael S. Lew, Tat-Seng Chua, Wei-Ying Ma,
Lekha Chaisorn, and Erwin M. Bakker, editors,
CIVR, volume 3568 of LNCS, pages 540–549, Sin-
gapore. Springer.
Mark J. Huiskes and Michael S. Lew. 2008. The MIR
Flickr Retrieval Evaluation. In MIR ’08: Proceed-
ings of the 2008 ACM International Conference on
Multimedia Information Retrieval, New York, NY,
USA. ACM.
Corinne J¨orgensen. 2003. Image Retrieval: Theory
and Research. Scarecrow Press, Lanham, MD.
C. Manning and H. Schuetze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press.
Diane Neal. 2006. News Photography Image Retrieval
Practices: Locus of Control in Two Contexts. Ph.D.
thesis, University of North Texas, Denton, TX.
M. Pavan and M. Pelillo. 2003. A new graph-
theoretic approach to clustering and segmentation.
IEEE Conf. Computer Vision and Pattern Recogni-
tion, 1:145–152.
Thomas Sikora. 2001. The mpeg-7 visual standard for
content description - an overview. IEEE Trans. Cir-
cuits and Systems for Video Technology, 11 (6):262–
282.
J Sivic and A Zisserman. 2003. Video google: A text
retrieval approach to object matching in videos. In
ICCV, pages 1470–1477, October.
Weining Wang and Qianhua He. 2008. A survey on
emotional semantic image retrieval. In ICIP, pages
117–120, San Diego, USA. IEEE.
M. Wang, Z. Ye, Y. Wang, and S. Wang. 2008. Domi-
nant sets clustering for image retrieval. Signal Pro-
cessing, 88 (11):2843–2849.
</reference>
<page confidence="0.996023">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000032">
<title confidence="0.9888995">Image and Collateral Text in Support of Auto-annotation and Sentiment Analysis</title>
<author confidence="0.89259">Zontone Boato Jonathon Hare Lewis</author>
<affiliation confidence="0.99956">University of Trento University of Southampton</affiliation>
<address confidence="0.769665">Trento, Italy. Southampton, United</address>
<note confidence="0.396865666666667">Siersdorfer L3S Research Hannover,</note>
<abstract confidence="0.998230212698414">We present a brief overview of the way in which image analysis, coupled with associated collateral text, is being used for auto-annotation and sentiment analysis. In particular, we describe our approach to auto-annotation using the graphtheoretic dominant set clustering algorithm and the annotation of images with sentiment scores from SentiWordNet. Preliminary results are given for both, and our planned work aims to explore synergies between the two approaches. 1 Automatic annotation of images using graph-theoretic clustering Recently, graph-theoretic approaches have become popular in the computer vision field. There exist different graph-theoretic clustering algorithms such as minimum cut, spectral clustering, dominant set clustering. Among all these algorithms, the Dominant Set Clustering (DSC) is a promising graph-theoretic approach based on the of a set has been proposed for different applications, such as image segmentation (Pavan and Pelillo, 2003), video summarization (Besiris et al., 2009), etc. Here we describe the application of DSC to image annotation. 1.1 Dominant Set Clustering The definition of Dominant Set (DS) was introduced in (Pavan and Pelillo, 2003). Let us consider a set of data samples that have to be clustered. These samples can be represented as an undirected edge-weighted (similarity) graph with self-loops E, where ... , n the vertex set, C_ V x V the edge set, → the (positive) weight func- Vertices in the data points, whereas edges represent neighborhood relationships, and finally edge-weights reflect similarity pairs of linked vertices. An x n symmatrix called affinity (or similarity) matrix, can be used to represent the graph where and 0 To define formally a Dominant Set, other have to be introduced. Let a nonsubset of vertices, with C_ V and E (average) weighted degree of to defined as: = the number of elements in can be observed that = 0 any E V If E� S can define the parameter = is the similarity nodes respect to the average between node its neighbors in It be noted that = for all j E V Now, if E the weight to 1 — This is a recursive equation where to calculate weights of the set needed. We deduce that a measure of the overall between the node the other nodes considering the overall similarity among nodes in So, the total weight of be defined as: non-empty subset of vertices C_ V that any non-empty C_ S defined a set the following two conditions 88 of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL pages Sweden, 16 July 2010. Association for Computational Linguistics satisfied: 1. and These conditions characterize the internal homogeneity of the cluster and the exterinhomogeneity of As a consequence of this definition, a dominant set cluster can be derived from a graph by means of a quadratic program (Paand Pelillo, 2003). Let an where the number of vertices of the graph and its components indicate the presence of in the cluster. Let the affinity matrix of the graph. Let us consider the following standard quadratic program: = = the standard of If a point E a local maxof and = &gt; support of it can be shown that the support a dominant set for the graph. So, a dominant set can be derived by solving the equation (1). The following iterative equation can be used to solve (1): 1) = the number of iterations. To summarize the algorithm, a dominant set is found and removed from the graph. A second dominant cluster is extracted from the remaining part of the graph, and so on. This procedure finishes when all the elements in the graph have been assigned to a cluster. 1.2 Image annotation using DSC Here we present an approach to automatically annotate images using the DSC algorithm. In the initialization phase (training) the image database split into subsets, corresponding to the different image categories or visual concepts that characterize the images in the database. In this process only tags are exploited: an image is included in all subsets corresponding to its tags. a subset the corresponding affinity macalculated and used by the DSC algorithm. Following (Wang et al., 2008), the eleof the affinity matrix deas similarity function between images considered subset and &gt; the scaling factor used as an adjustment function that allows the control of clustering sensitivity. We use the MPEG.7 descriptors (Sikora, 2001) as features for computing the similarity between images. Following the DSC approach, we can construct all clusof subset similar images, and associate with the tag of subset In the test phase, a new image is annotated associating to it the tag of the cluster that best matches the image. To do this, we use a decision algorithm based on the computation of the MSE (Mean Square Error), where for each cluster we derive a feature vector that represents all the images in that cluster (e.g., the average of all the feature vectors). The tag of the cluster with smaller MSE is used for the annotation. For our experiments, we consider a subset of the Corel database, that consists of 4287 images 49 categories The images in each category have been randomly selected from the database and used only for testing. In Figure 1 we report the annotation accuracy results obtained on 15 different classes with optimal param- For some classes the accuracy is very high, whereas for others the accuracy is very (under The total annotation accuracy all the 49 classes is roughly In a second set of experiments we consider a set of 6531 images from the MIR Flickr database (Huiskes and Lew, 2008), where each image is tagged with at least one of the chosen 30 visual Images are characterized by multiple tags associated to them, thus an image is included in all the corresponding subsets. For testing we use 875 images. To evaluate the annotation accuracy we compare the automatically associated tag with the user defined tags of that image. In Figure 1 we report the annotation accuracy obtained for the 30 different categories, with the optimal pa- The total annotation accuracy is Further simulations are in progress to evaluate the accuracy of multiple tags that can be associated to the test set in the MIR Flickr database. Indeed, our idea is to annotate the images considering the other common tags of the images belonging to each cluster. 2 Annotating Sentiment In the previous section we were concerned with annotating images with visual concepts, typically object names or descriptors. A separate strand of 89 Figure 1: Annotation accuracy for 15 classes of the Corel database (left) and for 30 classes of the MIR Flickr database (right). our work is concerned with opinion analysis in multimedia information and the automatic identification of sentiment. The study of image indexing and retrieval in the library and information science fields has long recognized the importance of sentiment in image retrieval (J¨orgensen, 2003; Neal, 2006). It is only recently however, that researchers interested in automated image analysis and retrieval have become interested in the sentiment associated with images (Wang and He, 2008). To date, investigations that have looked at the association between sentiment and image content have been limited to small datasets (typically much less than 1000) and rather specific, specially designed image features. Recently, we have started to explore how sentiment is related to image content using much more generic visual-term based features and much larger datasets collected with the aid of lexical resources such as Senti- WordNet. 2.1 SentiWordNet and Image Databases SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource built on top of WordNet. Word- Net (Fellbaum, 1998) is a thesaurus containing textual descriptions of terms and relationships between terms (examples are hypernyms: “car” is a subconcept of “vehicle” or synonyms: “car” describes the same concept as “automobile”). Word- Net distinguishes between different part-of-speech (verb, noun, adjective, etc.). A WordNet comprises all terms referring to the same (e.g., In SentiWorda triple of three neg, (corresponding to positive, negative, or rather neutral sentiment flavor of a word respectively) are assigned to each WordNet synset (and, thus, to each term in the synset). The senti-values are in range of sum up to 1 for each triple. instance neg, _ the term “good” or the term “ill”. Senti-values were partly created by human assessors and partly automatically assigned using an ensemble of different classifiers (see (Esuli, 2008) for an evaluation of these methods). Popular social websites, such as Flickr, contain massive amounts of visual information in the form of photographs. Many of these photographs have been collectively tagged and annotated by members of the respective community. Recently in the image analysis community it has become popular to use Flickr as a resource for building datasets to experiment with. We have been exploring how we can crawl Flickr for images that have a strong (positive or negative) sentiment associated with them. Our initial explorations have been based around crawling Flickr for images tagged with words that have very high positive or negative sentiment according to their SentiWordNet classification. Our image dataset has been refined by assigning an overall sentiment value to each image based on its textual metadata and discarding images with low overall sentiment. At the simplest level we use a dictionary of clearly positive and negative SentiWords, with which we assign a positive (+1) value if the text representation only con- 90 positive negative Figure 2: Top 16 most discriminative colours (from left to right) for positive and negative sentiment classes. tains positive sentiment terms, and a negative (-1) sentiment value if it only contains negative sentiment terms. We discarded images with neither a positive nor negative score. Currently we are also exploring more powerful ways to assign sentiment values to images. 2.2 Combining Senti-values and Visual Terms In the future we intend to exploit the use of techniques such as the one described in Section 1.2 in order to develop systems that are able to predict sentiment from image features. However, as a preliminary study, we have performed some smallscale experiments on a collection of 10000 images crawled from Flickr in order to try and see whether a primitive visual-bag-of-terms (Sivic and Zisserman, 2003; Hare and Lewis, 2005) can be associated with positive and negative sentiment values using a linear Support Vector Machine and Support Vector Regression. The visual-term bag-ofwords for the study was based upon a quantisation of each pixel in the images into a set of 64 discrete colours (i.e., each pixel corresponds to one of 64 possible visual terms). Our initial results look promising and indicate a considerable correlation between the visual bag-of-words and the sentiment scores. Analysis of Visual Features. our small-scale study we have also performed some analysis in order to investigate which visualterm features are most predictive of the positive and negative sentiment classes. For this analysis we have used the Mutual Information (MI) measure (Manning and Schuetze, 1999; Yang and Pedersen, 1997) from information theory which can be interpreted as a measure of how much the joint distribution of features (colour-based visual-terms in our case) deviate from a hypothetical distribution in which features and categories (“positive” and “negative” sentiment) are independent of each other. Figure 2 illustrates the 16 most discriminative colours for the positive and negative classes. The dominant visual-term features for positive sentiment are dominated by earthy colours and skin tones. Conversely, the features for negative sentiment are dominated by blue and green tones. Interestingly, this association can be explained through intuition because it mirrors human perception of warm (positive) and cold (negative) colours. Currently we are working on expanding our preliminary experiments to a much larger image dataset of over half a million images and incorporating more powerful visual-term based image features. In addition to seeking improved ways of determining image sentiment for the training set we are planning to combine the dominant set clustering approach to annotation presented in Section 1.2 with the sentiment annotation task of this section and compare the combined approach with other state of the art approaches as a step towards achieving robust image sentiment annotation. 3 Conclusions The use of dominant set clustering as a basis for auto-annotation has shown promise on image collections from both Corel and from Flickr. We have also shown how that visual-term feature representations show some promise as indicators of sentiment in images. In future work we plan to combine these approaches to provide better support for opinion analysis of multimedia web documents.</abstract>
<note confidence="0.618293666666667">Acknowledgments This work was supported by the European Union under the Seventh Framework Programme (FP7/2007-2013) project LivingKnowledge (FP7- IST-231126), and the LiveMemories project, graciously funded by the Autonomous Province of</note>
<abstract confidence="0.875812307692308">Trento (Italy). The authors are also grateful to the creators of Flickr for providing an API that can be used in scientific evaluations and the broader Flickr community for making images and metadata available. 91 Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. pages 412–420. References D. Besiris, A. Makedonas, G. Economou, and S. Fotopoulos. 2009. Combining graph connectivity and dominant set clustering for video summarization.</abstract>
<note confidence="0.9801808">Tools and 44 (2):161–186. A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion min- 6. Esuli. 2008. Generation of Lexi-</note>
<author confidence="0.749057">cal Resources for Opinion Mining Models</author>
<author confidence="0.749057">Algo-</author>
<affiliation confidence="0.81796">and PhD in Information Engineering, PhD School “Leonardo da Vinci”, University of Pisa.</affiliation>
<address confidence="0.836817">Fellbaum. 1998. An Electronic Lexical</address>
<affiliation confidence="0.681913">MIT Press.</affiliation>
<author confidence="0.526099">On</author>
<title confidence="0.6407335">image retrieval using salient regions with vectorspaces and latent semantics. In Wee Kheng Leow,</title>
<author confidence="0.939777">Michael S Lew</author>
<author confidence="0.939777">Tat-Seng Chua</author>
<author confidence="0.939777">Wei-Ying Ma</author>
<affiliation confidence="0.568792">Lekha Chaisorn, and Erwin M. Bakker, editors,</affiliation>
<address confidence="0.557488">volume 3568 of pages 540–549, Singapore. Springer.</address>
<author confidence="0.63597">The MIR</author>
<note confidence="0.9567366875">Retrieval Evaluation. In ’08: Proceedings of the 2008 ACM International Conference on Information New York, NY, USA. ACM. J¨orgensen. 2003. Retrieval: Theory Scarecrow Press, Lanham, MD. Manning and H. Schuetze. 1999. Statistical Natural Language MIT Press. Neal. 2006. Photography Image Retrieval Locus of Control in Two Ph.D. thesis, University of North Texas, Denton, TX. M. Pavan and M. Pelillo. 2003. A new graphtheoretic approach to clustering and segmentation. IEEE Conf. Computer Vision and Pattern Recogni- 1:145–152.</note>
<author confidence="0.723727">The mpeg- visual standard for</author>
<abstract confidence="0.895237666666667">description an overview. Trans. Cirand Systems for Video 11 (6):262– 282. J Sivic and A Zisserman. 2003. Video google: A text retrieval approach to object matching in videos. In pages 1470–1477, October. Weining Wang and Qianhua He. 2008. A survey on semantic image retrieval. In pages 117–120, San Diego, USA. IEEE. M. Wang, Z. Ye, Y. Wang, and S. Wang. 2008. Domisets clustering for image retrieval. Pro- 88 (11):2843–2849.</abstract>
<intro confidence="0.780992">92</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<journal>LREC,</journal>
<volume>6</volume>
<contexts>
<context position="9192" citStr="Esuli and Sebastiani, 2006" startWordPosition="1619" endWordPosition="1622"> analysis and retrieval have become interested in the sentiment associated with images (Wang and He, 2008). To date, investigations that have looked at the association between sentiment and image content have been limited to small datasets (typically much less than 1000) and rather specific, specially designed image features. Recently, we have started to explore how sentiment is related to image content using much more generic visual-term based features and much larger datasets collected with the aid of lexical resources such as SentiWordNet. 2.1 SentiWordNet and Image Databases SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource built on top of WordNet. WordNet (Fellbaum, 1998) is a thesaurus containing textual descriptions of terms and relationships between terms (examples are hypernyms: “car” is a subconcept of “vehicle” or synonyms: “car” describes the same concept as “automobile”). WordNet distinguishes between different part-of-speech types (verb, noun, adjective, etc.). A synset in WordNet comprises all terms referring to the same concept (e.g., {car, automobile}). In SentiWordNet a triple of three senti-values (pos, neg, obj) (corresponding to positive, negative, or rather neutral sentime</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. LREC, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
</authors>
<title>Automatic Generation of Lexical Resources for Opinion Mining: Models, Algorithms and Applications.</title>
<date>2008</date>
<institution>Information Engineering, PhD School “Leonardo da Vinci”, University of Pisa.</institution>
<note>PhD in</note>
<contexts>
<context position="10242" citStr="Esuli, 2008" startWordPosition="1790" endWordPosition="1791"> concept (e.g., {car, automobile}). In SentiWordNet a triple of three senti-values (pos, neg, obj) (corresponding to positive, negative, or rather neutral sentiment flavor of a word respectively) are assigned to each WordNet synset (and, thus, to each term in the synset). The senti-values are in the range of [0, 1] and sum up to 1 for each triple. For instance (pos, neg, obj) _ (0.875, 0.0, 0.125) for the term “good” or (0.25, 0.375, 0.375) for the term “ill”. Senti-values were partly created by human assessors and partly automatically assigned using an ensemble of different classifiers (see (Esuli, 2008) for an evaluation of these methods). Popular social websites, such as Flickr, contain massive amounts of visual information in the form of photographs. Many of these photographs have been collectively tagged and annotated by members of the respective community. Recently in the image analysis community it has become popular to use Flickr as a resource for building datasets to experiment with. We have been exploring how we can crawl Flickr for images that have a strong (positive or negative) sentiment associated with them. Our initial explorations have been based around crawling Flickr for imag</context>
</contexts>
<marker>Esuli, 2008</marker>
<rawString>Andrea Esuli. 2008. Automatic Generation of Lexical Resources for Opinion Mining: Models, Algorithms and Applications. PhD in Information Engineering, PhD School “Leonardo da Vinci”, University of Pisa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9264" citStr="Fellbaum, 1998" startWordPosition="1634" endWordPosition="1635">es (Wang and He, 2008). To date, investigations that have looked at the association between sentiment and image content have been limited to small datasets (typically much less than 1000) and rather specific, specially designed image features. Recently, we have started to explore how sentiment is related to image content using much more generic visual-term based features and much larger datasets collected with the aid of lexical resources such as SentiWordNet. 2.1 SentiWordNet and Image Databases SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource built on top of WordNet. WordNet (Fellbaum, 1998) is a thesaurus containing textual descriptions of terms and relationships between terms (examples are hypernyms: “car” is a subconcept of “vehicle” or synonyms: “car” describes the same concept as “automobile”). WordNet distinguishes between different part-of-speech types (verb, noun, adjective, etc.). A synset in WordNet comprises all terms referring to the same concept (e.g., {car, automobile}). In SentiWordNet a triple of three senti-values (pos, neg, obj) (corresponding to positive, negative, or rather neutral sentiment flavor of a word respectively) are assigned to each WordNet synset (a</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon S Hare</author>
<author>Paul H Lewis</author>
</authors>
<title>On image retrieval using salient regions with vectorspaces and latent semantics.</title>
<date>2005</date>
<volume>3568</volume>
<pages>540--549</pages>
<editor>In Wee Kheng Leow, Michael S. Lew, Tat-Seng Chua, Wei-Ying Ma, Lekha Chaisorn, and Erwin M. Bakker, editors, CIVR,</editor>
<publisher>Singapore. Springer.</publisher>
<contexts>
<context position="12175" citStr="Hare and Lewis, 2005" startWordPosition="2103" endWordPosition="2106"> discarded images with neither a positive nor negative score. Currently we are also exploring more powerful ways to assign sentiment values to images. 2.2 Combining Senti-values and Visual Terms In the future we intend to exploit the use of techniques such as the one described in Section 1.2 in order to develop systems that are able to predict sentiment from image features. However, as a preliminary study, we have performed some smallscale experiments on a collection of 10000 images crawled from Flickr in order to try and see whether a primitive visual-bag-of-terms (Sivic and Zisserman, 2003; Hare and Lewis, 2005) can be associated with positive and negative sentiment values using a linear Support Vector Machine and Support Vector Regression. The visual-term bag-ofwords for the study was based upon a quantisation of each pixel in the images into a set of 64 discrete colours (i.e., each pixel corresponds to one of 64 possible visual terms). Our initial results look promising and indicate a considerable correlation between the visual bag-of-words and the sentiment scores. Discriminative Analysis of Visual Features. In our small-scale study we have also performed some analysis in order to investigate whic</context>
</contexts>
<marker>Hare, Lewis, 2005</marker>
<rawString>Jonathon S. Hare and Paul H. Lewis. 2005. On image retrieval using salient regions with vectorspaces and latent semantics. In Wee Kheng Leow, Michael S. Lew, Tat-Seng Chua, Wei-Ying Ma, Lekha Chaisorn, and Erwin M. Bakker, editors, CIVR, volume 3568 of LNCS, pages 540–549, Singapore. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark J Huiskes</author>
<author>Michael S Lew</author>
</authors>
<title>The MIR Flickr Retrieval Evaluation.</title>
<date>2008</date>
<booktitle>In MIR ’08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval,</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7088" citStr="Huiskes and Lew, 2008" startWordPosition="1274" endWordPosition="1277"> we consider a subset of the Corel database, that consists of 4287 images in 49 categories (L = 49). The 10% of images in each category have been randomly selected from the database and used only for testing. In Figure 1 we report the annotation accuracy results obtained on 15 different classes with optimal parameter r = 0.2. For some classes the accuracy is very high, whereas for others the accuracy is very low (under 30%). The total annotation accuracy considering all the 49 classes is roughly 69%. In a second set of experiments we consider a set of 6531 images from the MIR Flickr database (Huiskes and Lew, 2008), where each image is tagged with at least one of the chosen 30 visual concepts (L = 30). Images are characterized by multiple tags associated to them, thus an image is included in all the corresponding subsets. For testing we use 875 images. To evaluate the annotation accuracy we compare the automatically associated tag with the user defined tags of that image. In Figure 1 we report the annotation accuracy obtained for the 30 different categories, with the optimal parameter r = 0.2. The total annotation accuracy is about 87%. Further simulations are in progress to evaluate the accuracy of mul</context>
</contexts>
<marker>Huiskes, Lew, 2008</marker>
<rawString>Mark J. Huiskes and Michael S. Lew. 2008. The MIR Flickr Retrieval Evaluation. In MIR ’08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinne J¨orgensen</author>
</authors>
<title>Image Retrieval: Theory and Research.</title>
<date>2003</date>
<publisher>Scarecrow Press,</publisher>
<location>Lanham, MD.</location>
<marker>J¨orgensen, 2003</marker>
<rawString>Corinne J¨orgensen. 2003. Image Retrieval: Theory and Research. Scarecrow Press, Lanham, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schuetze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12959" citStr="Manning and Schuetze, 1999" startWordPosition="2229" endWordPosition="2232">s for the study was based upon a quantisation of each pixel in the images into a set of 64 discrete colours (i.e., each pixel corresponds to one of 64 possible visual terms). Our initial results look promising and indicate a considerable correlation between the visual bag-of-words and the sentiment scores. Discriminative Analysis of Visual Features. In our small-scale study we have also performed some analysis in order to investigate which visualterm features are most predictive of the positive and negative sentiment classes. For this analysis we have used the Mutual Information (MI) measure (Manning and Schuetze, 1999; Yang and Pedersen, 1997) from information theory which can be interpreted as a measure of how much the joint distribution of features (colour-based visual-terms in our case) deviate from a hypothetical distribution in which features and categories (“positive” and “negative” sentiment) are independent of each other. Figure 2 illustrates the 16 most discriminative colours for the positive and negative classes. The dominant visual-term features for positive sentiment are dominated by earthy colours and skin tones. Conversely, the features for negative sentiment are dominated by blue and green t</context>
</contexts>
<marker>Manning, Schuetze, 1999</marker>
<rawString>C. Manning and H. Schuetze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Neal</author>
</authors>
<title>News Photography Image Retrieval Practices: Locus of Control in Two Contexts.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of North Texas,</institution>
<location>Denton, TX.</location>
<contexts>
<context position="8488" citStr="Neal, 2006" startWordPosition="1512" endWordPosition="1513">uster. 2 Annotating Sentiment In the previous section we were concerned with annotating images with visual concepts, typically object names or descriptors. A separate strand of 89 Figure 1: Annotation accuracy for 15 classes of the Corel database (left) and for 30 classes of the MIR Flickr database (right). our work is concerned with opinion analysis in multimedia information and the automatic identification of sentiment. The study of image indexing and retrieval in the library and information science fields has long recognized the importance of sentiment in image retrieval (J¨orgensen, 2003; Neal, 2006). It is only recently however, that researchers interested in automated image analysis and retrieval have become interested in the sentiment associated with images (Wang and He, 2008). To date, investigations that have looked at the association between sentiment and image content have been limited to small datasets (typically much less than 1000) and rather specific, specially designed image features. Recently, we have started to explore how sentiment is related to image content using much more generic visual-term based features and much larger datasets collected with the aid of lexical resour</context>
</contexts>
<marker>Neal, 2006</marker>
<rawString>Diane Neal. 2006. News Photography Image Retrieval Practices: Locus of Control in Two Contexts. Ph.D. thesis, University of North Texas, Denton, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pavan</author>
<author>M Pelillo</author>
</authors>
<title>A new graphtheoretic approach to clustering and segmentation.</title>
<date>2003</date>
<booktitle>IEEE Conf. Computer Vision and Pattern Recognition,</booktitle>
<pages>1--145</pages>
<contexts>
<context position="1388" citStr="Pavan and Pelillo, 2003" startWordPosition="190" endWordPosition="193">sults are given for both, and our planned work aims to explore synergies between the two approaches. 1 Automatic annotation of images using graph-theoretic clustering Recently, graph-theoretic approaches have become popular in the computer vision field. There exist different graph-theoretic clustering algorithms such as minimum cut, spectral clustering, dominant set clustering. Among all these algorithms, the Dominant Set Clustering (DSC) is a promising graph-theoretic approach based on the notion of a dominant set that has been proposed for different applications, such as image segmentation (Pavan and Pelillo, 2003), video summarization (Besiris et al., 2009), etc. Here we describe the application of DSC to image annotation. 1.1 Dominant Set Clustering The definition of Dominant Set (DS) was introduced in (Pavan and Pelillo, 2003). Let us consider a set of data samples that have to be clustered. These samples can be represented as an undirected edge-weighted (similarity) graph with no self-loops G = (V, E, w), where V = 1, ... , n is the vertex set, E C_ V x V is the edge set, and w : E → R∗� is the (positive) weight function. Vertices in G represent the data points, whereas edges represent neighborhood </context>
<context position="3962" citStr="Pavan and Pelillo, 2003" startWordPosition="693" endWordPosition="697"> such that W(T) &gt; 0 for any non-empty T C_ S is defined as a dominant set if the following two conditions j∈S 88 Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 88–92, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics are satisfied: 1. bi E S, wS(i) &gt; 0; and 2.bi E� S, wSu{i}(i) &lt; 0. These conditions characterize the internal homogeneity of the cluster and the external inhomogeneity of S. As a consequence of this definition, a dominant set cluster can be derived from a graph by means of a quadratic program (Pavan and Pelillo, 2003). Let x be an n-dimensional vector, where n is the number of vertices of the graph and its components indicate the presence of nodes in the cluster. Let A be the affinity matrix of the graph. Let us consider the following standard quadratic program: max f(x) = xTAx (1) s.t. x E A where A = {x &gt; 0 and eTx = 11 is the standard simplex of Rn. If a point x* E A is a local maximum of f, and u(x*) = {i E V : x*i &gt; 01 is the support of x*, it can be shown that the support u(x*) is a dominant set for the graph. So, a dominant set can be derived by solving the equation (1). The following iterative equa</context>
</contexts>
<marker>Pavan, Pelillo, 2003</marker>
<rawString>M. Pavan and M. Pelillo. 2003. A new graphtheoretic approach to clustering and segmentation. IEEE Conf. Computer Vision and Pattern Recognition, 1:145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Sikora</author>
</authors>
<title>The mpeg-7 visual standard for content description - an overview.</title>
<date>2001</date>
<booktitle>IEEE Trans. Circuits and Systems for Video Technology,</booktitle>
<volume>11</volume>
<pages>6--262</pages>
<contexts>
<context position="5828" citStr="Sikora, 2001" startWordPosition="1046" endWordPosition="1047">haracterize the images in the database. In this process only tags are exploited: an image is included in all subsets corresponding to its tags. Given a subset l, the corresponding affinity matrix Al is calculated and used by the DSC algorithm. Following (Wang et al., 2008), the elements of the affinity matrix Al = (aij) are defined as aij = e−,(i,j)/r2 where w(i, j) represents the similarity function between images i and j in the considered subset l, and r &gt; 0 is the scaling factor used as an adjustment function that allows the control of clustering sensitivity. We use the MPEG.7 descriptors (Sikora, 2001) as features for computing the similarity between images. Following the DSC approach, we can construct all clusters of subset l with similar images, and associate them with the tag of subset l. In the test phase, a new image is annotated associating to it the tag of the cluster that best matches the image. To do this, we use a decision algorithm based on the computation of the MSE (Mean Square Error), where for each cluster we derive a feature vector that represents all the images in that cluster (e.g., the average of all the feature vectors). The tag of the cluster with smaller MSE is used fo</context>
</contexts>
<marker>Sikora, 2001</marker>
<rawString>Thomas Sikora. 2001. The mpeg-7 visual standard for content description - an overview. IEEE Trans. Circuits and Systems for Video Technology, 11 (6):262– 282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sivic</author>
<author>A Zisserman</author>
</authors>
<title>Video google: A text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In ICCV,</booktitle>
<pages>1470--1477</pages>
<contexts>
<context position="12152" citStr="Sivic and Zisserman, 2003" startWordPosition="2098" endWordPosition="2102">egative sentiment terms. We discarded images with neither a positive nor negative score. Currently we are also exploring more powerful ways to assign sentiment values to images. 2.2 Combining Senti-values and Visual Terms In the future we intend to exploit the use of techniques such as the one described in Section 1.2 in order to develop systems that are able to predict sentiment from image features. However, as a preliminary study, we have performed some smallscale experiments on a collection of 10000 images crawled from Flickr in order to try and see whether a primitive visual-bag-of-terms (Sivic and Zisserman, 2003; Hare and Lewis, 2005) can be associated with positive and negative sentiment values using a linear Support Vector Machine and Support Vector Regression. The visual-term bag-ofwords for the study was based upon a quantisation of each pixel in the images into a set of 64 discrete colours (i.e., each pixel corresponds to one of 64 possible visual terms). Our initial results look promising and indicate a considerable correlation between the visual bag-of-words and the sentiment scores. Discriminative Analysis of Visual Features. In our small-scale study we have also performed some analysis in or</context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>J Sivic and A Zisserman. 2003. Video google: A text retrieval approach to object matching in videos. In ICCV, pages 1470–1477, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weining Wang</author>
<author>Qianhua He</author>
</authors>
<title>A survey on emotional semantic image retrieval.</title>
<date>2008</date>
<booktitle>In ICIP,</booktitle>
<pages>117--120</pages>
<publisher>IEEE.</publisher>
<location>San Diego, USA.</location>
<contexts>
<context position="8671" citStr="Wang and He, 2008" startWordPosition="1539" endWordPosition="1542"> 89 Figure 1: Annotation accuracy for 15 classes of the Corel database (left) and for 30 classes of the MIR Flickr database (right). our work is concerned with opinion analysis in multimedia information and the automatic identification of sentiment. The study of image indexing and retrieval in the library and information science fields has long recognized the importance of sentiment in image retrieval (J¨orgensen, 2003; Neal, 2006). It is only recently however, that researchers interested in automated image analysis and retrieval have become interested in the sentiment associated with images (Wang and He, 2008). To date, investigations that have looked at the association between sentiment and image content have been limited to small datasets (typically much less than 1000) and rather specific, specially designed image features. Recently, we have started to explore how sentiment is related to image content using much more generic visual-term based features and much larger datasets collected with the aid of lexical resources such as SentiWordNet. 2.1 SentiWordNet and Image Databases SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource built on top of WordNet. WordNet (Fellbaum, 1998) is a t</context>
</contexts>
<marker>Wang, He, 2008</marker>
<rawString>Weining Wang and Qianhua He. 2008. A survey on emotional semantic image retrieval. In ICIP, pages 117–120, San Diego, USA. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>Z Ye</author>
<author>Y Wang</author>
<author>S Wang</author>
</authors>
<title>Dominant sets clustering for image retrieval.</title>
<date>2008</date>
<journal>Signal Processing,</journal>
<volume>88</volume>
<pages>11--2843</pages>
<contexts>
<context position="5488" citStr="Wang et al., 2008" startWordPosition="983" endWordPosition="986"> all the elements in the graph have been assigned to a cluster. 1.2 Image annotation using DSC Here we present an approach to automatically annotate images using the DSC algorithm. In the initialization phase (training) the image database is split into L smaller subsets, corresponding to the different image categories or visual concepts that characterize the images in the database. In this process only tags are exploited: an image is included in all subsets corresponding to its tags. Given a subset l, the corresponding affinity matrix Al is calculated and used by the DSC algorithm. Following (Wang et al., 2008), the elements of the affinity matrix Al = (aij) are defined as aij = e−,(i,j)/r2 where w(i, j) represents the similarity function between images i and j in the considered subset l, and r &gt; 0 is the scaling factor used as an adjustment function that allows the control of clustering sensitivity. We use the MPEG.7 descriptors (Sikora, 2001) as features for computing the similarity between images. Following the DSC approach, we can construct all clusters of subset l with similar images, and associate them with the tag of subset l. In the test phase, a new image is annotated associating to it the </context>
</contexts>
<marker>Wang, Ye, Wang, Wang, 2008</marker>
<rawString>M. Wang, Z. Ye, Y. Wang, and S. Wang. 2008. Dominant sets clustering for image retrieval. Signal Processing, 88 (11):2843–2849.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>