<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.9995105">
Filling Statistics with Linguistics –
Property Design for the Disambiguation of German LFG Parses
</title>
<author confidence="0.997713">
Martin Forst
</author>
<affiliation confidence="0.9983365">
Institute of Natural Language Processing
University of Stuttgart, Germany
</affiliation>
<email confidence="0.99397">
forst@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.995562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989666666667">
We present a log-linear model for the disam-
biguation of the analyses produced by a Ger-
man broad-coverage LFG, focussing on the
properties (or features) this model is based
on. We compare this model to an initial
model based only on a part of the proper-
ties provided to the final model and observe
that the performance of a log-linear model
for parse selection depends heavily on the
types of properties that it is based on. In
our case, the error reduction achieved with
the log-linear model based on the extended
set of properties is 51.0% and thus com-
pares very favorably to the error reduction
of 34.5% achieved with the initial model.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992525">
In the development of stochastic disambiguation
modules for ‘deep’ grammars, relatively much work
has gone into the definition of suitable probability
models and the corresponding learning algorithms.
Property design, on the contrary, has rather been un-
deremphasized, and the properties used in stochas-
tic disambiguation modules are most often presented
only superficially. This paper’s aim is to draw more
attention to property design by presenting linguisti-
cally motivated properties that are used for the dis-
ambiguation of the analyses produced by a German
broad-coverage LFG and by showing that property
design is of crucial importance for the quality of
stochastic models for parse selection.
We present, in Section 2, the system that the dis-
ambiguation module was developed for as well as
</bodyText>
<page confidence="0.98558">
17
</page>
<bodyText confidence="0.999934428571429">
the initially used properties. In Section 3, we then
present a selection of the properties that were ex-
pressly designed for the resolution of frequent ambi-
guities in German LFG parses. Section 4 describes
experiments that we carried out with log-linear mod-
els based on the initial set of properties and on an
extended one. Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.995177" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.996387">
2.1 The German ParGram LFG
</subsectionHeader>
<bodyText confidence="0.999954608695652">
The grammar for which the log-linear model for
parse selection described in this paper was devel-
oped is the German ParGram LFG (Dipper, 2003;
Rohrer and Forst, 2006). It has been developed with
and for the grammar development and processing
platform XLE (Crouch et al., 2006) and consists of
a symbolic LFG, which can be employed both for
parsing and generation, and a two-stage disambigua-
tion module, the log-linear model being the compo-
nent that carries out the final selection among the
parses that have been retained by an Optimality-
Theoretically inspired prefilter (Frank et al., 2001;
Forst et al., 2005).
The grammar has a coverage in terms of full
parses that exceeds 80% on newspaper corpora. For
sentences out of coverage, it employs the robust-
ness techniques (fragment parsing, ‘skimming’) im-
plemented in XLE and described in Riezler et al.
(2002), so that 100% of our corpus sentences receive
at least some sort of analysis. A dependency-based
evaluation of the analyses produced by the grammar
on the TiGer Dependency Bank (Forst et al., 2004)
results in an F-score between 80.42% on all gram-
</bodyText>
<note confidence="0.924204">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 17–24,
Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999667166666667">
matical relations and morphosyntactic features (or
72.59% on grammatical relations only) and 85.50%
(or 79.36%). The lower bound is based on an ar-
bitrary selection among the parses built up by the
symbolic grammar; the upper bound is determined
by the best possible selection.
</bodyText>
<subsectionHeader confidence="0.992161">
2.2 Log-linear models for disambiguation
</subsectionHeader>
<bodyText confidence="0.999244">
Since Johnson et al. (1999), log-linear models of
the following form have become standard as disam-
biguation devices for precision grammars:
</bodyText>
<equation confidence="0.99871675">
m=1 λj fj(x,y)
PA(x 2J) = eEj
� j=1 λj�fj(x�,y)
x��X(y) e
</equation>
<bodyText confidence="0.999913166666667">
They are used for parse selection in the English Re-
source Grammar (Toutanova et al., 2002), the En-
glish ParGram LFG (Riezler et al., 2002), the En-
glish Enju HPSG (Miyao and Tsujii, 2002), the
HPSG-inspired Alpino parser for Dutch (Malouf
and van Noord, 2004; van Noord, 2006) and the
English CCG from Edinburgh (Clark and Curran,
2004).
While relatively much work has gone into the
question of how to estimate the property weights
A1 ... Am efficiently and accurately on the basis
of (annotated) corpus data, the question of how
to define suitable and informative property func-
tions f1 ... fm has received relatively little attention.
However, we are convinced that property design is
the possibility of improving log-linear models for
parse selection now that the machine learning ma-
chinery is relatively well established.
</bodyText>
<subsectionHeader confidence="0.997615">
2.3 Initially used properties for disambiguation
</subsectionHeader>
<bodyText confidence="0.999828285714286">
The first set of properties with which we conducted
experiments was built on the model of the property
set used for the disambiguation of English ParGram
LFG parses (Riezler et al., 2002; Riezler and Vasser-
man, 2004). These properties are defined with the
help of thirteen property templates, which are pa-
rameterized for c-structure categories, f-structure at-
tributes and/or their possible values. The templates
are hardwired in XLE, which allows for a very ef-
ficient extraction of properties based on them from
packed c-/f-structure representations. The downside
of the templates being hardwired, however, is that, at
least at first sight, the property developer is confined
to what the developers of the property templates an-
ticipated as potentially relevant for disambiguation
or, more precisely, for the disambiguation of English
LFG analyses.
The thirteen property templates can be subdi-
vided into c-structure-based property templates and
f-structure-based ones. The c-structure-based prop-
erty templates are:
</bodyText>
<listItem confidence="0.972521487179487">
• cs label &lt;XP&gt;: counts the number of XP
nodes in the c-structure of an analysis.
• cs num children &lt;XP&gt;: counts the num-
ber of children of all XP nodes in a c-structure.
• cs adjacent label &lt;XP&gt; &lt;YP&gt;:
counts the number of XP nodes that immedi-
ately dominate a Y P node.
• cs sub label &lt;XP&gt; &lt;YP&gt;: counts the
number of XP nodes that dominate a Y P node
(at arbitrary depth).
• cs embedded &lt;XP&gt; &lt;n&gt;: counts the
number of XP nodes that dominate n other
distinct XP nodes (at arbitrary depth).
• cs conj nonpar &lt;n&gt;: counts the number
of coordinated constituents that are not parallel
at the nth level of embedding.
• cs right branch: counts the number of
right children in the c-structure of an analysis.
The f-structure-based property templates are:
• fs attrs &lt;Attr1 ... Attrn&gt;:
counts the number of times that attributes
Attr1 ... Attrn occur in the f-structure of an
analysis.
• fs attr val &lt;Attr&gt; &lt;Val&gt;: counts
the number of times that the atomic attribute
Attr has the value Val.
• fs adj attrs &lt;Attr1&gt; &lt;Attr2&gt;:
counts the number of times that the com-
plex attribute Attr1 immediately embeds the
attribute Attr2.
• fs subattr &lt;Attr1&gt; &lt;Attr2&gt; counts
the number of times that the complex attribute
Attr1 embeds the attribute Attr2 (at arbitrary
depth).
• lex subcat &lt;Lemma&gt; &lt;SCF1 ...
SCFn&gt;: counts the number of times that
the subcategorizing element Lemma occurs
with one of the subcategorization frames
SCF1 ... SCFn.
</listItem>
<page confidence="0.939173">
18
</page>
<listItem confidence="0.978934333333333">
• verb arg &lt;Lemma&gt; &lt;GF&gt;: counts the
number of times that the element Lemma sub-
categorizes for the argument GF.
</listItem>
<bodyText confidence="0.713178833333334">
Automatically instantiating these templates for all
c-structure categories, f-structure attributes and val-
ues used in the German ParGram LFG as well as for
all lexical elements present in its lexicon results in
460,424 properties.
3 Property design for the disambiguation
of German LFG parses
Despite the very large number of properties that can
be directly constructed on the basis of the thirteen
property templates provided by XLE, many com-
mon ambiguities in German LFG parses cannot be
captured by any of these.
</bodyText>
<subsectionHeader confidence="0.973011">
3.1 Properties that record the relative linear
order of functions
</subsectionHeader>
<bodyText confidence="0.984386">
Consider, e.g., the SUBJ-OBJ ambiguity in (1).
</bodyText>
<listItem confidence="0.937156333333333">
(1) [. . . ] peilt [S/O das Management] [O/S ein
[. . . ] aims the management a
“sichtbar verbessertes” Ergebnis] an.
</listItem>
<bodyText confidence="0.973725034482759">
“visibly improved” result at.
‘[... ] the management aims at a “visibly im-
proved” result.’ (TIGER Corpus s20834)
The c-structure is shared by the two readings
of the sentence, so that c-structure-based proper-
ties cannot contribute to the selection of the cor-
rect reading; the only f-structure-based proper-
ties that differ between the two analyses are of
the kinds fs adj attrs SUBJ ADJUNCT and
fs subattr OBJ ADJUNCT, which are only re-
motely, if at all, related to the observed SUBJ-OBJ
ambiguity. The crucial information from the in-
tended reading, namely that the SUBJ precedes the
OBJ, is not captured directly by any of the ini-
tial properties. We therefore introduce a new prop-
erty template that records the linear order of two
grammatical functions and instantiate it for all rel-
evant combinations. The new properties created
this way make it possible to capture the default
order of nominal arguments, which according to
Lenerz (1977) and Uszkoreit (1987) (among others),
is SUBJ, OBJ-TH, OBJ.
Similarly to the SUBJ-OBJ ambiguity just con-
sidered, the ADJUNCT-OBL ambiguity in (2) can-
not at all be resolved on the basis of c-structure-
based properties, and the f-structure-based proper-
ties whose values differ among the two readings
seem only remotely related to the observed ambi-
guity.
</bodyText>
<listItem confidence="0.9279444">
(2) [A/O Dagegen] sprach sich
Against that/In contrast spoke himself
[. . . ] Micha Guttmann [O/A f¨ur getrennte
[. . . ] Micha Guttmann for separate
Gedenkst¨atten] aus.
</listItem>
<bodyText confidence="0.950877444444445">
memorials out.
‘In contrast, [... ] Michael Guttmann argued
for separate memorials.’ (s2090)
However, the literature on constituent order in Ger-
man, e.g. Helbig and Buscha (2001), documents
the tendency of ADJUNCT PPs to precede OBL PPs,
which also holds in (2). We therefore introduced
properties that record the relative linear order of AD-
JUNCT PPs and OBL PPs.
</bodyText>
<subsectionHeader confidence="0.904007">
3.2 Properties that consider the nature of a
</subsectionHeader>
<bodyText confidence="0.979675823529412">
constituent wrt. its function
Although linear order plays a major role in the func-
tional interpretation of case-ambiguous DPs in Ger-
man, it is only one among several ‘soft’ constraints
involved. The nature of such a DP may actually also
give hints to its grammatical function.
The tendency of SUBJs to be high on the defi-
niteness scale and the animacy scale as well as the
tendency of OBJs to be low on these scales has
mainly been observed in studies on differential ob-
ject/subject marking (see, e.g., Aissen (2003)). Nev-
ertheless, these tendencies also seem to hold in lan-
guages like German, which does not exhibit differ-
ential object/subject marking. In (3), the indefinite
inanimate DP is to be interpreted as the OBJ of the
sentence and the definite human DP, as its SUBJ al-
though the former precedes the latter.
</bodyText>
<listItem confidence="0.937477">
(3) [O/S Nahezu stabile Preise] prognostizieren
</listItem>
<bodyText confidence="0.986074">
Nearly stable prices forecast
[S/O die bayerischen Experten] [. . . ]
the Bavarian experts [. . . ].
‘The Bavarian experts forecast nearly stable
prices [... ].’ (s7357)
</bodyText>
<page confidence="0.977926">
19
</page>
<figure confidence="0.998383146551724">
der
the
[. . . ]
[. . . ]
ein
a
(4) Eine
A
zentrale
central
Autoversicherung
car insurance
Rolle
role
zu,
to,
vereinnahmt.
receives.
[. . . ]
[. . . ]
die
which
F¨unftel
fifth
kommt
comes
SUBJ
434
PRED &apos;pro&apos;
OBJ-TH
PRED
&apos;fünftel&apos;
ADJ-REL
SPEC
OBJ
528
DET PRED &apos;eine&apos;
PRED
&apos;zu#kommen&lt;[21:Rolle], [243:Versicherung]&gt;&apos;
PRED
&apos;Rolle&apos;
PRED
&apos;Versicherung&apos;
PRED
&apos;vereinnahmen&lt;[434:pro], [528:fünftel]&gt;&apos;
633
[434:pro]
TOPIC-REL
&apos;zentral&lt;[21:Rolle]&gt;&apos;
[21:Rolle]
ADJUNCT
107
SUBJ
PRED
SUBJ
21
SPEC
DET PRED
&apos;eine&apos;
MOD
&apos;
&apos;
-12
PRED
Auto
PRON-REL [434:pro]
243
SPEC
DET PRED &apos;die&apos;
TOPIC [21:Rolle]
191
&amp;quot;Eine zentrale Rolle kommt der Autoversicherung zu, die ein Fünftel vereinnahmt.&amp;quot;
PRED
&apos;zu#kommen&lt;[21:Rolle], [243:Versicherung]&gt;&apos;
PRED
&apos;Rolle&apos;
PRED
&apos;vereinnahmen&lt;[434:pro], [528:fünftel]&gt;&apos;
PRED &apos;zentral&lt;[21:Rolle]&gt;&apos;
SUBJ [21:Rolle]
ADJUNCT
107
PRED &apos;pro&apos;
PRED
SPEC
&apos;fünftel&apos;
DET PRED &apos;eine&apos;
434
528
SUBJ
SUBJ
OBJ
ADJ-REL
PRON-REL [434:pro]
[434:pro]
TOPIC-REL
633
SPEC
PRED &apos;Versicherung&apos;
DET PRED
&apos;eine&apos;
MOD
-12 PRED
&apos;
Auto
&apos;
SPEC
DET PRED &apos;die&apos;
21
OBJ-TH
243
TOPIC [21:Rolle]
191
(a) evaluated as relatively improbable due to negative weight of
DISTANCE-TO-ANTECEDENT %X
&amp;quot;Eine zentrale Rolle kommt der Autoversicherung zu, die ein Fünftel vereinnahmt.&amp;quot;
</figure>
<bodyText confidence="0.953968785714286">
In order to allow these regularities to be
learned from corpus data, we defined addi-
tional property templates like isDef &lt;GF&gt; and
isHuman &lt;GF&gt;,1 which are instantiated for all rel-
evant grammatical functions.
3.3 Properties for the resolution of attachment
ambiguities concerning extraposed
constituents
A further common ambiguity in German con-
cerns the functional attachment of extraposed con-
stituents, such as relative clauses, dass clauses and
infinitival VPs. In (4), e.g., there is no hard con-
straint that would allow us to determine whether the
relative clause modifies Rolle or Autoversicherung.
</bodyText>
<figure confidence="0.804497">
(b) evaluated as more probable
</figure>
<figureCaption confidence="0.999742">
Figure 1: Competing f-structures for (4)
</figureCaption>
<bodyText confidence="0.999978625">
In the case of (5), the property DEP21 common
Anwalt APP proper, which counts the num-
ber of occurrences of the common noun Anwalt
(‘lawyer’) that govern a proper name via the depen-
dency APP (close apposition), contributes to the cor-
rect selection among the analyses illustrated in Fig-
ure 2 by capturing the fact that Anwalt is a prototyp-
ical head of a close apposition.2
</bodyText>
<listItem confidence="0.725059333333333">
(5) [. . . ], das den Anwalt Klaus Bollig zum
[. . . ] which the lawyer Klaus Bollig to the
vorl¨aufigen Verwalter bestellte.
</listItem>
<bodyText confidence="0.933098730769231">
interim administrator appointed.
‘[... ] which appointed lawyer Klaus Bollig as
interim administrator.’ (s37596)
2Since we have a list of title nouns available, we might also
introduce a more general property that would count the number
of occurrences of title nouns in general that govern a proper
name via the dependency APP. Note, however, that the nouns
that be heads of APPs comprise not only title nouns, but also
nouns like Abteilung ‘department’, Buch ‘book’, etc.
‘There is a central role for the car insurance,
which receives a fifth [... ].’ (s27539)
In order to allow for an improved resolution of
this kind of attachment ambiguity, we introduced
properties that extract the surface distance of an ex-
traposed constituent to its functional head as well as
properties that record how the functional uncertainty
paths involved in these attachments were instanti-
ated. This way, we hope to extract the information
necessary to model the tendencies observed, e.g., in
Uszkoreit et al. (1998).
3.4 Lexicalized properties capturing
dependencies
Inspired by Malouf and van Noord (2004),
we finally also introduced lexicalized proper-
ties capturing dependencies. These are built
on the following property templates: DEP12
</bodyText>
<figure confidence="0.538">
&lt;PoS1&gt; &lt;Dep&gt; &lt;PoS2&gt; &lt;Lemma2&gt;, DEP21
&lt;PoS1&gt; &lt;Lemma1&gt; &lt;Dep&gt; &lt;PoS2&gt;andDEP22
&lt;PoS1&gt; &lt;Lemma1&gt; &lt;Dep&gt; &lt;PoS2&gt; &lt;Lemma2&gt;.
</figure>
<footnote confidence="0.5315855">
These are intended to capture information on the
subcategorization behavior of lexical elements and
on typical collocations.
1Humanness information is imported from GermaNet.
</footnote>
<page confidence="0.858841">
20
</page>
<figure confidence="0.915478">
&amp;quot;das den Anwalt Klaus Bollig zum vorläufigen Verwalter bestellte&amp;quot;
(a) evaluated as less probable
&amp;quot;das den Anwalt Klaus Bollig zum vorläufigen Verwalter bestellte&amp;quot;
(b) evaluated as relatively probable due to highly positive weight
of DEP21 common Anwalt APP proper
</figure>
<figureCaption confidence="0.997183">
Figure 2: Competing f-structures for (5)
</figureCaption>
<sectionHeader confidence="0.996461" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.952183">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.997500714285714">
All the data we use are from the TIGER Corpus
(Brants et al., 2002), a treebank of German news-
paper texts comprising about 50,000 sentences. The
1,868 dependency annotations of the TiGer Depen-
dency Bank, which have been semi-automatically
derived from the corresponding treebank graphs, are
used for evaluation purposes; we split these into a
held-out set of 371 sentences (and corresponding de-
pendency annotations) and a test set of 1,497 sen-
tences. For training, we use packed, i.e. ambiguous,
c/f-structure representations where a proper subset
of the f-structures can be determined as compatible
with the TIGER graph annotations. Currently, these
are 8,881 pairs of labelled and unlabelled packed
c/f-structure reprentations.
From these 8,881 pairs of c/f-structure reprenta-
tions, we extract two sets of property forests, one
containing only the initially used properties, which
are based on the hardwired templates, and one con-
taining all properties, i.e. both the initially used and
the newly introduced ones.
</bodyText>
<subsectionHeader confidence="0.96973">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.999994666666667">
For training, we use the cometc software by Ste-
fan Riezler, which is part of XLE. Prior to train-
ing, however, we apply a frequency-based cutoff c
to the data that ensures that a property is discrimi-
native between the intended reading(s) and the un-
intended reading(s) in at least c sentences; c is set
to 4 on the basis of the evaluation results achieved
on our held-out set and following a policy of a ‘con-
servative’ cutoff whose only purpose is to prevent
that weights be learned for sparse properties. (For
a longer discussion of frequency-based cutoffs, see
Forst (2007).) For the actual estimation of prop-
erty weights, we then apply the combined method of
incremental property selection and li regularization
proposed in Riezler and Vasserman (2004), adjust-
ing the hyperparameters on our held-out set for each
of the two sets of properties. In order to compara-
tively evaluate the importance of property selection
and regularization, we also train models based on
each of the two sets of properties without applying
any kind of these techniques.
</bodyText>
<subsectionHeader confidence="0.993948">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.985979666666667">
The overall results in terms of F-score and error re-
-Flower
duction, defined as F� Factual that the
Fupper−Flower ,
four resulting systems achieve on our test set of
1,497 TiGer DB structures are shown in Table 1. In
order to give the reader an idea of the size of the dif-
ferent models, we also indicate the number of prop-
erties that they are based on. All of the F-scores
were calculated by means of the evaluation software
by Crouch et al. (2002).
We observe that the models obtained using prop-
erty selection and regularization, in addition to be-
ing much more compact than their unregularized
counterparts, perform significantly better than these.
More importantly though, we can see that the most
important improvement, namely from an error re-
duction of 32.5% to one of 42.0% or from 34.8%
to 51.0% respectively, is achieved by adding more
informative properties to the model.
Table 2 then shows results broken down according
to individual dependencies that are achieved with,
on the one hand, the best-performing model based
on both the XLE template-based and the newly in-
</bodyText>
<figure confidence="0.990346428571429">
PRED &apos;bestellen&lt;[1:pro], [82:Anwalt], [228:Bollig]&gt;&apos;
SUBJ
OBJ
OBJ-TH
1
82
228
NAME-MOD
PRED &apos;pro&apos;
PRED
SPEC
PRED &apos;Bollig&apos;
&apos;Anwalt&apos;
DET PRED &apos;die&apos;
188
PRED
&apos;Klaus
&apos;
PRED
ADJUNCT OBJ
246
PRON-REL [1:pro]
TOPIC-REL [1:pro]
&apos;zu&lt;[246:Verwalter]&gt;&apos;
PRED &apos;Verwalter&apos;
PRED &apos;vorläufig&lt;[246:Verwalter]&gt;&apos;
SUBJ [246:Verwalter]
DET PRED &apos;die&apos;
429
246
ADJUNCT
SPEC
334
PRED &apos;bestellen&lt;[1:pro], [82:Anwalt]&gt;&apos;
&apos;pro&apos;
1 PRED
SUBJ
APP
NAME-MOD
PRED &apos;Klaus&apos;
OBJ
188
228
DET PRED &apos;die&apos;
SPEC
82
PRED &apos;Anwalt&apos;
PRED &apos;Bollig&apos;
ADJUNCT
246
OBJ
246
ADJUNCT
SPEC
PRED &apos;zu&lt;[246:Verwalter]&gt;&apos;
PRED &apos;Verwalter&apos;
PRED &apos;vorläufig&lt;[246:Verwalter]&gt;&apos;
SUBJ [246:Verwalter]
DET PRED &apos;die&apos;
334
PRON-REL [1:pro]
TOPIC-REL [1:pro]
429
</figure>
<page confidence="0.996365">
21
</page>
<table confidence="0.707357545454545">
# prop. F-sc. err. red.
XLE template-based properties,
unregularized MLE 14,263 82.07 32.5%
XLE templ.-based pr. that survive
a freq.-b. cutoff of 4, n-best
grafting with h regularization 3,400 82.19 34.8%
all properties,
unregularized MLE 57,934 82.55 42.0%
all properties that survive a
freq.-b. cutoff of 4, n-best
grafting with h regularization 4,340 83.01 51.0%
</table>
<tableCaption confidence="0.987079">
Table 1: Overall F-score and corresponding error re-
</tableCaption>
<bodyText confidence="0.981677233333333">
duction achieved by the four different systems on the
1,497 TiGer DB structures of our test set
troduced properties and, on the other hand, the best-
performing model based on XLE template-based
properties only. Furthermore, we indicate the re-
spective upper and lower bound F-scores, deter-
mined by the best possible parse selection and by
an arbitrary selection respectively.
We observe that the overall F-score is signifi-
cantly better with a selection based on the model that
includes the newly introduced properties than with a
selection based on the model that relies on the XLE
template-based properties only; overall error reduc-
tion increases from 34.5% to 51.0%. What is partic-
ularly interesting is the considerably better error re-
duction for the core grammatical functions sb (sub-
ject) and oa (accusative object). But also for res
(relative clauses) and mos (modifiers or adjuncts),
which are notoriously difficult for disambiguation
due to PP and ADVP attachment ambiguities, we
observe an improvement in F-score.
Our error reduction of 51.0% also compares fa-
vorably to the 36% error reduction on English LFG
parses reported in Riezler et al. (2002). However,
it is considerably lower than the error reduction of
78% reported for the Dutch Alpino parser (Malouf
and van Noord, 2004), but this may be due to the
fact that our lower bound is calculated on the basis
of analyses that have already passed a prefilter and
is thus relatively high.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999948970588235">
Our results show that property design is of crucial
importance in the development of a disambiguation
module for a ‘deep’ parser. They also indicate that it
is a good idea to carry out property design in a lin-
guistically inspired fashion, i.e. by referring to the
theoretical literature that deals with soft constraints
that are active in the language for which the system
is developed. Property design thus requires a pro-
found knowledge of the language under considera-
tion (and the theoretical literature that deals with its
syntax), and since the disambiguation module oper-
ates on the output of the symbolic grammar, a good
knowledge of the grammar is necessary as well.
Weighting against each other the contributions of
different measures taken for improving log-linear
models for parse selection, we can conclude that
property design is at least as important as prop-
erty selection and/or regularization, since even a
completely unregularized model based on all prop-
erties performs significantly better than the best-
adjusted model among the ones that are based on
the template-based properties only. Moreover, prop-
erty design can be carried out in a targeted way,
i.e. properties can be designed in order to improve
the disambiguation of grammatical relations that, so
far, are disambiguated particularly poorly or that
are of special interest for the task that the system’s
output is used for. By demonstrating that prop-
erty design is the key to good log-linear models for
‘deep’ syntactic disambiguation, our work confirms
that “specifying the features of a SUBG [stochastic
unification-based grammar] is as much an empirical
matter as specifying the grammar itself” (Johnson et
al., 1999).
</bodyText>
<sectionHeader confidence="0.994951" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996551">
The work described in this paper has been carried
out in the DLFG project, which was funded by the
German Research Foundation (DFG).
Furthermore, I thank the audiences at several Par-
Gram meetings, at the Research Workshop of the
Israel Science Foundation on Large-scale Grammar
Development and Grammar Engineering at the Uni-
versity of Haifa and at the SFB 732 Opening Col-
loquium in Stuttgart for their important feedback on
earlier versions of this work.
</bodyText>
<sectionHeader confidence="0.998603" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977701">
Judith Aissen. 2003. Differential Object Marking:
Iconicity vs. Economy. Natural Language and Lin-
guistic Theory, 21:435–483.
</reference>
<page confidence="0.994588">
22
</page>
<table confidence="0.991326">
gramm. relation/morphosynt. feature upper stoch. select. stoch. select. lower
bound all properties templ.-based pr. bound
F-sc. F-sc. err. red. F-sc. err. red. F-sc.
all 85.50 83.01 51.0 82.17 34.5 80.42
PREDs only 79.36 75.74 46.5 74.69 31.0 72.59
app (close apposition) 63 60 63 61 75 55
app cl (appositive clause) 53 53 100 52 86 46
cc (comparative complement) 28 19 -29 19 -29 21
cj (conjunct of coord.) 70 68 50 67 25 66
da (dative object) 67 63 67 62 58 55
det (determiner) 92 91 50 91 50 90
gl (genitive in spec. pos.) 89 88 75 88 75 85
gr (genitive attribute) 88 84 56 84 56 79
mo (modifier) 70 63 36 62 27 59
mod (non-head in compound) 94 89 29 89 29 87
name mod (non-head in compl. name) 82 80 33 81 67 79
number (number as determiner) 83 81 33 81 33 80
oa (accusative object) 78 75 77 69 31 65
obj (arg. of prep. or conj.) 90 88 50 87 25 86
oc fin (finite cl. obj.) 67 64 0 64 0 64
oc inf (infinite cl. obj.) 83 82 0 82 0 82
op (prepositional obj.) 57 54 40 54 40 52
op dir (directional argument) 30 23 13 23 13 22
op loc (local argument) 59 49 29 49 29 45
pd (predicative argument) 62 60 50 59 25 58
pred restr 92 87 62 84 38 79
quant (quantifying determiner) 70 68 33 68 33 67
rc (relative clause) 74 62 20 59 0 59
sb (subject) 76 73 63 71 38 68
sbp (logical subj. in pass. constr.) 68 63 62 61 46 55
case 87 85 75 83 50 79
comp form (complementizer form) 74 72 0 74 100 72
coord form (coordinating conj.) 86 86 100 86 100 85
degree 89 88 50 87 0 87
det type (determiner type) 95 95 – 95 – 95
fut (future) 86 86 – 86 – 86
gend (gender) 92 90 60 89 40 87
mood 90 90 – 90 – 90
num (number) 91 89 50 89 50 87
pass asp (passive aspect) 80 80 100 79 0 79
perf (perfect) 86 85 0 86 100 85
pers (person) 85 84 83 82 50 79
pron form (pronoun form) 73 73 – 73 – 73
pron type (pronoun type) 71 70 0 71 100 70
tense 92 91 0 91 0 91
</table>
<tableCaption confidence="0.998898">
Table 2: F-scores (in %) in the 1,497 TiGer DB examples of our test set
</tableCaption>
<page confidence="0.997519">
23
</page>
<reference confidence="0.999416431372549">
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol, Bulgaria.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCJ and Log-Linear Models. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL ’04), Barcelona,
Spain.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage parser. In Proceedings
of the LREC Workshop ‘Beyond PARSEVAL—Towards
improved evaluation mesures for parsing systems’,
pages 67–74, Las Palmas, Spain.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2006. XLE docu-
mentation. Technical report, Palo Alto Research Cen-
ter, Palo Alto, CA.
Stefanie Dipper. 2003. Implementing and Documenting
Large-scale Grammars – German LFG. Ph.D. thesis,
IMS, University of Stuttgart. Arbeitspapiere des Insti-
tuts f¨ur Maschinelle Sprachverarbeitung (AIMS), Vol-
ume 9, Number 1.
Martin Forst, N´uria Bertomeu, Berthold Crysmann, Fred-
erik Fouvry, Silvia Hansen-Schirra, and Valia Kordoni.
2004. Towards a dependency-based gold standard
for German parsers – The TiGer Dependency Bank.
In Proceedings of the COLING Workshop on Lin-
guistically Interpreted Corpora (LINC ’04), Geneva,
Switzerland.
Martin Forst, Jonas Kuhn, and Christian Rohrer. 2005.
Corpus-based learning of OT constraint rankings for
large-scale LFG grammars. In Proceedings of the 10th
International LFG Conference (LFG’05), Bergen,
Norway. CSLI Publications.
Martin Forst. 2007. Disambiguation for a Linguistically
Precise German Parser. Ph.D. thesis, University of
Stuttgart.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars. In
Peter Sells, editor, Formal and Empirical Issues in Op-
timality Theoretic Syntax, pages 367–397. CSLI Pub-
lications, Stanford, CA.
Gerhard Helbig and Joachim Buscha. 2001.
Deutsche Grammatik – Ein Handbuch f¨ur den
Ausl¨anderunterricht. Langenscheidt, Berlin and
Munich, Germany.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
“unification-based” grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics 1999, College Park, MD.
J¨urgen Lenerz. 1977. ZurAbfolge nominaler Satzglieder
im Deutschen. Number 5 in Studien zur deutschen
Grammatik. Narr, T¨ubingen, Germany.
Robert Malouf and Gertjan van Noord. 2004. Wide Cov-
erage Parsing with Stochastic Attribute Value Gram-
mars. In Proceedings of the IJCNLP-04 Workshop
“Beyond Shallow Analyses - Formalisms and statisti-
cal modeling for deep analyses”.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings
of the Human Language Technology Conference, San
Diego, CA.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and h regularization for maximum
entropy parsing. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP’04), Barcelona, Spain.
Stefan Riezler, Tracy Holloway King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and Discriminative Esti-
mation Techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics 2002, Philadelphia, PA.
Christian Rohrer and Martin Forst. 2006. Improv-
ing coverage and parsing quality of a large-scale
LFG for German. In Proceedings of the Language
Resources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Kristina Toutanova, Christopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT2002), pages 253–263.
Hans Uszkoreit, Thorsten Brants, Brigitte Krenn, Lars
Konieczny, Stephan Oepen, and Wojciech Skut. 1998.
Relative Clause Extraposition in German – Evidence
from Corpus Studies and Acceptability Ratings. In
Proceedings of AMLaP-98, Freiburg, Germany.
Hans Uszkoreit. 1987. Word Order and Constituent
Structure in German. CSLI Publications, Stanford,
CA.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In Piet Mertens, Cedrick Fairon, Anne
Dister, and Patrick Watrin, editors, TALN06. Verbum
Ex Machina. Actes de la 13e conf´erence sur le traite-
ment automatique des langues naturelles, pages 20–
42, Leuven, Belgium.
</reference>
<page confidence="0.999173">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931435">
<title confidence="0.9990415">Filling Statistics with Linguistics – Property Design for the Disambiguation of German LFG Parses</title>
<author confidence="0.976713">Martin</author>
<affiliation confidence="0.999864">Institute of Natural Language University of Stuttgart,</affiliation>
<email confidence="0.998867">forst@ims.uni-stuttgart.de</email>
<abstract confidence="0.9970501875">We present a log-linear model for the disambiguation of the analyses produced by a German broad-coverage LFG, focussing on the properties (or features) this model is based on. We compare this model to an initial model based only on a part of the properties provided to the final model and observe that the performance of a log-linear model for parse selection depends heavily on the types of properties that it is based on. In our case, the error reduction achieved with the log-linear model based on the extended set of properties is 51.0% and thus compares very favorably to the error reduction of 34.5% achieved with the initial model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Judith Aissen</author>
</authors>
<title>Differential Object Marking: Iconicity vs.</title>
<date>2003</date>
<booktitle>Economy. Natural Language and Linguistic Theory,</booktitle>
<pages>21--435</pages>
<contexts>
<context position="10395" citStr="Aissen (2003)" startWordPosition="1701" endWordPosition="1702">e relative linear order of ADJUNCT PPs and OBL PPs. 3.2 Properties that consider the nature of a constituent wrt. its function Although linear order plays a major role in the functional interpretation of case-ambiguous DPs in German, it is only one among several ‘soft’ constraints involved. The nature of such a DP may actually also give hints to its grammatical function. The tendency of SUBJs to be high on the definiteness scale and the animacy scale as well as the tendency of OBJs to be low on these scales has mainly been observed in studies on differential object/subject marking (see, e.g., Aissen (2003)). Nevertheless, these tendencies also seem to hold in languages like German, which does not exhibit differential object/subject marking. In (3), the indefinite inanimate DP is to be interpreted as the OBJ of the sentence and the definite human DP, as its SUBJ although the former precedes the latter. (3) [O/S Nahezu stabile Preise] prognostizieren Nearly stable prices forecast [S/O die bayerischen Experten] [. . . ] the Bavarian experts [. . . ]. ‘The Bavarian experts forecast nearly stable prices [... ].’ (s7357) 19 der the [. . . ] [. . . ] ein a (4) Eine A zentrale central Autoversicherung </context>
</contexts>
<marker>Aissen, 2003</marker>
<rawString>Judith Aissen. 2003. Differential Object Marking: Iconicity vs. Economy. Natural Language and Linguistic Theory, 21:435–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="15242" citStr="Brants et al., 2002" startWordPosition="2456" endWordPosition="2459">ndDEP22 &lt;PoS1&gt; &lt;Lemma1&gt; &lt;Dep&gt; &lt;PoS2&gt; &lt;Lemma2&gt;. These are intended to capture information on the subcategorization behavior of lexical elements and on typical collocations. 1Humanness information is imported from GermaNet. 20 &amp;quot;das den Anwalt Klaus Bollig zum vorläufigen Verwalter bestellte&amp;quot; (a) evaluated as less probable &amp;quot;das den Anwalt Klaus Bollig zum vorläufigen Verwalter bestellte&amp;quot; (b) evaluated as relatively probable due to highly positive weight of DEP21 common Anwalt APP proper Figure 2: Competing f-structures for (5) 4 Experiments 4.1 Data All the data we use are from the TIGER Corpus (Brants et al., 2002), a treebank of German newspaper texts comprising about 50,000 sentences. The 1,868 dependency annotations of the TiGer Dependency Bank, which have been semi-automatically derived from the corresponding treebank graphs, are used for evaluation purposes; we split these into a held-out set of 371 sentences (and corresponding dependency annotations) and a test set of 1,497 sentences. For training, we use packed, i.e. ambiguous, c/f-structure representations where a proper subset of the f-structures can be determined as compatible with the TIGER graph annotations. Currently, these are 8,881 pairs </context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCJ and Log-Linear Models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4179" citStr="Clark and Curran, 2004" startWordPosition="672" endWordPosition="675">nd is determined by the best possible selection. 2.2 Log-linear models for disambiguation Since Johnson et al. (1999), log-linear models of the following form have become standard as disambiguation devices for precision grammars: m=1 λj fj(x,y) PA(x 2J) = eEj � j=1 λj�fj(x�,y) x��X(y) e They are used for parse selection in the English Resource Grammar (Toutanova et al., 2002), the English ParGram LFG (Riezler et al., 2002), the English Enju HPSG (Miyao and Tsujii, 2002), the HPSG-inspired Alpino parser for Dutch (Malouf and van Noord, 2004; van Noord, 2006) and the English CCG from Edinburgh (Clark and Curran, 2004). While relatively much work has gone into the question of how to estimate the property weights A1 ... Am efficiently and accurately on the basis of (annotated) corpus data, the question of how to define suitable and informative property functions f1 ... fm has received relatively little attention. However, we are convinced that property design is the possibility of improving log-linear models for parse selection now that the machine learning machinery is relatively well established. 2.3 Initially used properties for disambiguation The first set of properties with which we conducted experiment</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCJ and Log-Linear Models. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Crouch</author>
<author>Ronald M Kaplan</author>
<author>Tracy H King</author>
<author>Stefan Riezler</author>
</authors>
<title>A comparison of evaluation metrics for a broad-coverage parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC Workshop ‘Beyond PARSEVAL—Towards</booktitle>
<pages>67--74</pages>
<location>Las Palmas,</location>
<contexts>
<context position="17716" citStr="Crouch et al. (2002)" startWordPosition="2863" endWordPosition="2866">ce of property selection and regularization, we also train models based on each of the two sets of properties without applying any kind of these techniques. 4.3 Evaluation The overall results in terms of F-score and error re-Flower duction, defined as F� Factual that the Fupper−Flower , four resulting systems achieve on our test set of 1,497 TiGer DB structures are shown in Table 1. In order to give the reader an idea of the size of the different models, we also indicate the number of properties that they are based on. All of the F-scores were calculated by means of the evaluation software by Crouch et al. (2002). We observe that the models obtained using property selection and regularization, in addition to being much more compact than their unregularized counterparts, perform significantly better than these. More importantly though, we can see that the most important improvement, namely from an error reduction of 32.5% to one of 42.0% or from 34.8% to 51.0% respectively, is achieved by adding more informative properties to the model. Table 2 then shows results broken down according to individual dependencies that are achieved with, on the one hand, the best-performing model based on both the XLE tem</context>
</contexts>
<marker>Crouch, Kaplan, King, Riezler, 2002</marker>
<rawString>Richard Crouch, Ronald M. Kaplan, Tracy H. King, and Stefan Riezler. 2002. A comparison of evaluation metrics for a broad-coverage parser. In Proceedings of the LREC Workshop ‘Beyond PARSEVAL—Towards improved evaluation mesures for parsing systems’, pages 67–74, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Paula Newman</author>
</authors>
<title>XLE documentation. Technical report, Palo Alto Research Center,</title>
<date>2006</date>
<location>Palo Alto, CA.</location>
<contexts>
<context position="2332" citStr="Crouch et al., 2006" startWordPosition="372" endWordPosition="375">on 3, we then present a selection of the properties that were expressly designed for the resolution of frequent ambiguities in German LFG parses. Section 4 describes experiments that we carried out with log-linear models based on the initial set of properties and on an extended one. Section 5 concludes. 2 Background 2.1 The German ParGram LFG The grammar for which the log-linear model for parse selection described in this paper was developed is the German ParGram LFG (Dipper, 2003; Rohrer and Forst, 2006). It has been developed with and for the grammar development and processing platform XLE (Crouch et al., 2006) and consists of a symbolic LFG, which can be employed both for parsing and generation, and a two-stage disambiguation module, the log-linear model being the component that carries out the final selection among the parses that have been retained by an OptimalityTheoretically inspired prefilter (Frank et al., 2001; Forst et al., 2005). The grammar has a coverage in terms of full parses that exceeds 80% on newspaper corpora. For sentences out of coverage, it employs the robustness techniques (fragment parsing, ‘skimming’) implemented in XLE and described in Riezler et al. (2002), so that 100% of</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2006</marker>
<rawString>Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King, John Maxwell, and Paula Newman. 2006. XLE documentation. Technical report, Palo Alto Research Center, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Dipper</author>
</authors>
<title>Implementing and Documenting Large-scale Grammars – German LFG.</title>
<date>2003</date>
<booktitle>Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS), Volume 9, Number 1.</booktitle>
<tech>Ph.D. thesis,</tech>
<institution>IMS, University of Stuttgart.</institution>
<contexts>
<context position="2197" citStr="Dipper, 2003" startWordPosition="352" endWordPosition="353">in Section 2, the system that the disambiguation module was developed for as well as 17 the initially used properties. In Section 3, we then present a selection of the properties that were expressly designed for the resolution of frequent ambiguities in German LFG parses. Section 4 describes experiments that we carried out with log-linear models based on the initial set of properties and on an extended one. Section 5 concludes. 2 Background 2.1 The German ParGram LFG The grammar for which the log-linear model for parse selection described in this paper was developed is the German ParGram LFG (Dipper, 2003; Rohrer and Forst, 2006). It has been developed with and for the grammar development and processing platform XLE (Crouch et al., 2006) and consists of a symbolic LFG, which can be employed both for parsing and generation, and a two-stage disambiguation module, the log-linear model being the component that carries out the final selection among the parses that have been retained by an OptimalityTheoretically inspired prefilter (Frank et al., 2001; Forst et al., 2005). The grammar has a coverage in terms of full parses that exceeds 80% on newspaper corpora. For sentences out of coverage, it empl</context>
</contexts>
<marker>Dipper, 2003</marker>
<rawString>Stefanie Dipper. 2003. Implementing and Documenting Large-scale Grammars – German LFG. Ph.D. thesis, IMS, University of Stuttgart. Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS), Volume 9, Number 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
<author>N´uria Bertomeu</author>
<author>Berthold Crysmann</author>
<author>Frederik Fouvry</author>
<author>Silvia Hansen-Schirra</author>
<author>Valia Kordoni</author>
</authors>
<title>Towards a dependency-based gold standard for German parsers – The TiGer Dependency Bank.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING Workshop on Linguistically Interpreted Corpora (LINC ’04),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3113" citStr="Forst et al., 2004" startWordPosition="501" endWordPosition="504">onent that carries out the final selection among the parses that have been retained by an OptimalityTheoretically inspired prefilter (Frank et al., 2001; Forst et al., 2005). The grammar has a coverage in terms of full parses that exceeds 80% on newspaper corpora. For sentences out of coverage, it employs the robustness techniques (fragment parsing, ‘skimming’) implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. A dependency-based evaluation of the analyses produced by the grammar on the TiGer Dependency Bank (Forst et al., 2004) results in an F-score between 80.42% on all gramProceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 17–24, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics matical relations and morphosyntactic features (or 72.59% on grammatical relations only) and 85.50% (or 79.36%). The lower bound is based on an arbitrary selection among the parses built up by the symbolic grammar; the upper bound is determined by the best possible selection. 2.2 Log-linear models for disambiguation Since Johnson et al. (1999), log-linear models of the following for</context>
</contexts>
<marker>Forst, Bertomeu, Crysmann, Fouvry, Hansen-Schirra, Kordoni, 2004</marker>
<rawString>Martin Forst, N´uria Bertomeu, Berthold Crysmann, Frederik Fouvry, Silvia Hansen-Schirra, and Valia Kordoni. 2004. Towards a dependency-based gold standard for German parsers – The TiGer Dependency Bank. In Proceedings of the COLING Workshop on Linguistically Interpreted Corpora (LINC ’04), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
<author>Jonas Kuhn</author>
<author>Christian Rohrer</author>
</authors>
<title>Corpus-based learning of OT constraint rankings for large-scale LFG grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th International LFG Conference (LFG’05),</booktitle>
<publisher>CSLI Publications.</publisher>
<location>Bergen,</location>
<contexts>
<context position="2667" citStr="Forst et al., 2005" startWordPosition="427" endWordPosition="430">Gram LFG The grammar for which the log-linear model for parse selection described in this paper was developed is the German ParGram LFG (Dipper, 2003; Rohrer and Forst, 2006). It has been developed with and for the grammar development and processing platform XLE (Crouch et al., 2006) and consists of a symbolic LFG, which can be employed both for parsing and generation, and a two-stage disambiguation module, the log-linear model being the component that carries out the final selection among the parses that have been retained by an OptimalityTheoretically inspired prefilter (Frank et al., 2001; Forst et al., 2005). The grammar has a coverage in terms of full parses that exceeds 80% on newspaper corpora. For sentences out of coverage, it employs the robustness techniques (fragment parsing, ‘skimming’) implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. A dependency-based evaluation of the analyses produced by the grammar on the TiGer Dependency Bank (Forst et al., 2004) results in an F-score between 80.42% on all gramProceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 17–24, Prague, Czech Republic, </context>
</contexts>
<marker>Forst, Kuhn, Rohrer, 2005</marker>
<rawString>Martin Forst, Jonas Kuhn, and Christian Rohrer. 2005. Corpus-based learning of OT constraint rankings for large-scale LFG grammars. In Proceedings of the 10th International LFG Conference (LFG’05), Bergen, Norway. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
</authors>
<title>Disambiguation for a Linguistically Precise German Parser.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="16777" citStr="Forst (2007)" startWordPosition="2703" endWordPosition="2704">he newly introduced ones. 4.2 Training For training, we use the cometc software by Stefan Riezler, which is part of XLE. Prior to training, however, we apply a frequency-based cutoff c to the data that ensures that a property is discriminative between the intended reading(s) and the unintended reading(s) in at least c sentences; c is set to 4 on the basis of the evaluation results achieved on our held-out set and following a policy of a ‘conservative’ cutoff whose only purpose is to prevent that weights be learned for sparse properties. (For a longer discussion of frequency-based cutoffs, see Forst (2007).) For the actual estimation of property weights, we then apply the combined method of incremental property selection and li regularization proposed in Riezler and Vasserman (2004), adjusting the hyperparameters on our held-out set for each of the two sets of properties. In order to comparatively evaluate the importance of property selection and regularization, we also train models based on each of the two sets of properties without applying any kind of these techniques. 4.3 Evaluation The overall results in terms of F-score and error re-Flower duction, defined as F� Factual that the Fupper−Fl</context>
</contexts>
<marker>Forst, 2007</marker>
<rawString>Martin Forst. 2007. Disambiguation for a Linguistically Precise German Parser. Ph.D. thesis, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Tracy Holloway King</author>
<author>Jonas Kuhn</author>
<author>John T Maxwell</author>
</authors>
<title>Optimality Theory Style Constraint Ranking in Large-Scale LFG Grammars.</title>
<date>2001</date>
<booktitle>Formal and Empirical Issues in Optimality Theoretic Syntax,</booktitle>
<pages>367--397</pages>
<editor>In Peter Sells, editor,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="2646" citStr="Frank et al., 2001" startWordPosition="423" endWordPosition="426">d 2.1 The German ParGram LFG The grammar for which the log-linear model for parse selection described in this paper was developed is the German ParGram LFG (Dipper, 2003; Rohrer and Forst, 2006). It has been developed with and for the grammar development and processing platform XLE (Crouch et al., 2006) and consists of a symbolic LFG, which can be employed both for parsing and generation, and a two-stage disambiguation module, the log-linear model being the component that carries out the final selection among the parses that have been retained by an OptimalityTheoretically inspired prefilter (Frank et al., 2001; Forst et al., 2005). The grammar has a coverage in terms of full parses that exceeds 80% on newspaper corpora. For sentences out of coverage, it employs the robustness techniques (fragment parsing, ‘skimming’) implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. A dependency-based evaluation of the analyses produced by the grammar on the TiGer Dependency Bank (Forst et al., 2004) results in an F-score between 80.42% on all gramProceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 17–24, Pra</context>
</contexts>
<marker>Frank, King, Kuhn, Maxwell, 2001</marker>
<rawString>Anette Frank, Tracy Holloway King, Jonas Kuhn, and John T. Maxwell. 2001. Optimality Theory Style Constraint Ranking in Large-Scale LFG Grammars. In Peter Sells, editor, Formal and Empirical Issues in Optimality Theoretic Syntax, pages 367–397. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Helbig</author>
<author>Joachim Buscha</author>
</authors>
<date>2001</date>
<booktitle>Deutsche Grammatik – Ein Handbuch f¨ur den Ausl¨anderunterricht. Langenscheidt,</booktitle>
<location>Berlin and Munich, Germany.</location>
<contexts>
<context position="9648" citStr="Helbig and Buscha (2001)" startWordPosition="1569" endWordPosition="1572"> SUBJ-OBJ ambiguity just considered, the ADJUNCT-OBL ambiguity in (2) cannot at all be resolved on the basis of c-structurebased properties, and the f-structure-based properties whose values differ among the two readings seem only remotely related to the observed ambiguity. (2) [A/O Dagegen] sprach sich Against that/In contrast spoke himself [. . . ] Micha Guttmann [O/A f¨ur getrennte [. . . ] Micha Guttmann for separate Gedenkst¨atten] aus. memorials out. ‘In contrast, [... ] Michael Guttmann argued for separate memorials.’ (s2090) However, the literature on constituent order in German, e.g. Helbig and Buscha (2001), documents the tendency of ADJUNCT PPs to precede OBL PPs, which also holds in (2). We therefore introduced properties that record the relative linear order of ADJUNCT PPs and OBL PPs. 3.2 Properties that consider the nature of a constituent wrt. its function Although linear order plays a major role in the functional interpretation of case-ambiguous DPs in German, it is only one among several ‘soft’ constraints involved. The nature of such a DP may actually also give hints to its grammatical function. The tendency of SUBJs to be high on the definiteness scale and the animacy scale as well as </context>
</contexts>
<marker>Helbig, Buscha, 2001</marker>
<rawString>Gerhard Helbig and Joachim Buscha. 2001. Deutsche Grammatik – Ein Handbuch f¨ur den Ausl¨anderunterricht. Langenscheidt, Berlin and Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="3673" citStr="Johnson et al. (1999)" startWordPosition="586" endWordPosition="589">the grammar on the TiGer Dependency Bank (Forst et al., 2004) results in an F-score between 80.42% on all gramProceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 17–24, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics matical relations and morphosyntactic features (or 72.59% on grammatical relations only) and 85.50% (or 79.36%). The lower bound is based on an arbitrary selection among the parses built up by the symbolic grammar; the upper bound is determined by the best possible selection. 2.2 Log-linear models for disambiguation Since Johnson et al. (1999), log-linear models of the following form have become standard as disambiguation devices for precision grammars: m=1 λj fj(x,y) PA(x 2J) = eEj � j=1 λj�fj(x�,y) x��X(y) e They are used for parse selection in the English Resource Grammar (Toutanova et al., 2002), the English ParGram LFG (Riezler et al., 2002), the English Enju HPSG (Miyao and Tsujii, 2002), the HPSG-inspired Alpino parser for Dutch (Malouf and van Noord, 2004; van Noord, 2006) and the English CCG from Edinburgh (Clark and Curran, 2004). While relatively much work has gone into the question of how to estimate the property weight</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics 1999, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨urgen Lenerz</author>
</authors>
<title>ZurAbfolge nominaler Satzglieder im Deutschen.</title>
<date>1977</date>
<journal>Number</journal>
<booktitle>in Studien zur deutschen Grammatik.</booktitle>
<volume>5</volume>
<location>Narr, T¨ubingen, Germany.</location>
<contexts>
<context position="8948" citStr="Lenerz (1977)" startWordPosition="1459" endWordPosition="1460">een the two analyses are of the kinds fs adj attrs SUBJ ADJUNCT and fs subattr OBJ ADJUNCT, which are only remotely, if at all, related to the observed SUBJ-OBJ ambiguity. The crucial information from the intended reading, namely that the SUBJ precedes the OBJ, is not captured directly by any of the initial properties. We therefore introduce a new property template that records the linear order of two grammatical functions and instantiate it for all relevant combinations. The new properties created this way make it possible to capture the default order of nominal arguments, which according to Lenerz (1977) and Uszkoreit (1987) (among others), is SUBJ, OBJ-TH, OBJ. Similarly to the SUBJ-OBJ ambiguity just considered, the ADJUNCT-OBL ambiguity in (2) cannot at all be resolved on the basis of c-structurebased properties, and the f-structure-based properties whose values differ among the two readings seem only remotely related to the observed ambiguity. (2) [A/O Dagegen] sprach sich Against that/In contrast spoke himself [. . . ] Micha Guttmann [O/A f¨ur getrennte [. . . ] Micha Guttmann for separate Gedenkst¨atten] aus. memorials out. ‘In contrast, [... ] Michael Guttmann argued for separate memor</context>
</contexts>
<marker>Lenerz, 1977</marker>
<rawString>J¨urgen Lenerz. 1977. ZurAbfolge nominaler Satzglieder im Deutschen. Number 5 in Studien zur deutschen Grammatik. Narr, T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide Coverage Parsing with Stochastic Attribute Value Grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow</booktitle>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide Coverage Parsing with Stochastic Attribute Value Grammars. In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow Analyses - Formalisms and statistical modeling for deep analyses”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="4030" citStr="Miyao and Tsujii, 2002" startWordPosition="648" endWordPosition="651">ons only) and 85.50% (or 79.36%). The lower bound is based on an arbitrary selection among the parses built up by the symbolic grammar; the upper bound is determined by the best possible selection. 2.2 Log-linear models for disambiguation Since Johnson et al. (1999), log-linear models of the following form have become standard as disambiguation devices for precision grammars: m=1 λj fj(x,y) PA(x 2J) = eEj � j=1 λj�fj(x�,y) x��X(y) e They are used for parse selection in the English Resource Grammar (Toutanova et al., 2002), the English ParGram LFG (Riezler et al., 2002), the English Enju HPSG (Miyao and Tsujii, 2002), the HPSG-inspired Alpino parser for Dutch (Malouf and van Noord, 2004; van Noord, 2006) and the English CCG from Edinburgh (Clark and Curran, 2004). While relatively much work has gone into the question of how to estimate the property weights A1 ... Am efficiently and accurately on the basis of (annotated) corpus data, the question of how to define suitable and informative property functions f1 ... fm has received relatively little attention. However, we are convinced that property design is the possibility of improving log-linear models for parse selection now that the machine learning mach</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of the Human Language Technology Conference, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
</authors>
<title>Gradient feature testing and h regularization for maximum entropy parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4933" citStr="Riezler and Vasserman, 2004" startWordPosition="791" endWordPosition="795">rately on the basis of (annotated) corpus data, the question of how to define suitable and informative property functions f1 ... fm has received relatively little attention. However, we are convinced that property design is the possibility of improving log-linear models for parse selection now that the machine learning machinery is relatively well established. 2.3 Initially used properties for disambiguation The first set of properties with which we conducted experiments was built on the model of the property set used for the disambiguation of English ParGram LFG parses (Riezler et al., 2002; Riezler and Vasserman, 2004). These properties are defined with the help of thirteen property templates, which are parameterized for c-structure categories, f-structure attributes and/or their possible values. The templates are hardwired in XLE, which allows for a very efficient extraction of properties based on them from packed c-/f-structure representations. The downside of the templates being hardwired, however, is that, at least at first sight, the property developer is confined to what the developers of the property templates anticipated as potentially relevant for disambiguation or, more precisely, for the disambig</context>
<context position="16957" citStr="Riezler and Vasserman (2004)" startWordPosition="2728" endWordPosition="2731">uency-based cutoff c to the data that ensures that a property is discriminative between the intended reading(s) and the unintended reading(s) in at least c sentences; c is set to 4 on the basis of the evaluation results achieved on our held-out set and following a policy of a ‘conservative’ cutoff whose only purpose is to prevent that weights be learned for sparse properties. (For a longer discussion of frequency-based cutoffs, see Forst (2007).) For the actual estimation of property weights, we then apply the combined method of incremental property selection and li regularization proposed in Riezler and Vasserman (2004), adjusting the hyperparameters on our held-out set for each of the two sets of properties. In order to comparatively evaluate the importance of property selection and regularization, we also train models based on each of the two sets of properties without applying any kind of these techniques. 4.3 Evaluation The overall results in terms of F-score and error re-Flower duction, defined as F� Factual that the Fupper−Flower , four resulting systems achieve on our test set of 1,497 TiGer DB structures are shown in Table 1. In order to give the reader an idea of the size of the different models, we</context>
</contexts>
<marker>Riezler, Vasserman, 2004</marker>
<rawString>Stefan Riezler and Alexander Vasserman. 2004. Gradient feature testing and h regularization for maximum entropy parsing. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy Holloway King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2915" citStr="Riezler et al. (2002)" startWordPosition="468" endWordPosition="471">ing platform XLE (Crouch et al., 2006) and consists of a symbolic LFG, which can be employed both for parsing and generation, and a two-stage disambiguation module, the log-linear model being the component that carries out the final selection among the parses that have been retained by an OptimalityTheoretically inspired prefilter (Frank et al., 2001; Forst et al., 2005). The grammar has a coverage in terms of full parses that exceeds 80% on newspaper corpora. For sentences out of coverage, it employs the robustness techniques (fragment parsing, ‘skimming’) implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. A dependency-based evaluation of the analyses produced by the grammar on the TiGer Dependency Bank (Forst et al., 2004) results in an F-score between 80.42% on all gramProceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 17–24, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics matical relations and morphosyntactic features (or 72.59% on grammatical relations only) and 85.50% (or 79.36%). The lower bound is based on an arbitrary selection among the parses built </context>
<context position="4903" citStr="Riezler et al., 2002" startWordPosition="787" endWordPosition="790">m efficiently and accurately on the basis of (annotated) corpus data, the question of how to define suitable and informative property functions f1 ... fm has received relatively little attention. However, we are convinced that property design is the possibility of improving log-linear models for parse selection now that the machine learning machinery is relatively well established. 2.3 Initially used properties for disambiguation The first set of properties with which we conducted experiments was built on the model of the property set used for the disambiguation of English ParGram LFG parses (Riezler et al., 2002; Riezler and Vasserman, 2004). These properties are defined with the help of thirteen property templates, which are parameterized for c-structure categories, f-structure attributes and/or their possible values. The templates are hardwired in XLE, which allows for a very efficient extraction of properties based on them from packed c-/f-structure representations. The downside of the templates being hardwired, however, is that, at least at first sight, the property developer is confined to what the developers of the property templates anticipated as potentially relevant for disambiguation or, mo</context>
<context position="20610" citStr="Riezler et al. (2002)" startWordPosition="3306" endWordPosition="3309">ion based on the model that relies on the XLE template-based properties only; overall error reduction increases from 34.5% to 51.0%. What is particularly interesting is the considerably better error reduction for the core grammatical functions sb (subject) and oa (accusative object). But also for res (relative clauses) and mos (modifiers or adjuncts), which are notoriously difficult for disambiguation due to PP and ADVP attachment ambiguities, we observe an improvement in F-score. Our error reduction of 51.0% also compares favorably to the 36% error reduction on English LFG parses reported in Riezler et al. (2002). However, it is considerably lower than the error reduction of 78% reported for the Dutch Alpino parser (Malouf and van Noord, 2004), but this may be due to the fact that our lower bound is calculated on the basis of analyses that have already passed a prefilter and is thus relatively high. 5 Conclusions Our results show that property design is of crucial importance in the development of a disambiguation module for a ‘deep’ parser. They also indicate that it is a good idea to carry out property design in a linguistically inspired fashion, i.e. by referring to the theoretical literature that d</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy Holloway King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics 2002, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Rohrer</author>
<author>Martin Forst</author>
</authors>
<title>Improving coverage and parsing quality of a large-scale LFG for German.</title>
<date>2006</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="2222" citStr="Rohrer and Forst, 2006" startWordPosition="354" endWordPosition="357">the system that the disambiguation module was developed for as well as 17 the initially used properties. In Section 3, we then present a selection of the properties that were expressly designed for the resolution of frequent ambiguities in German LFG parses. Section 4 describes experiments that we carried out with log-linear models based on the initial set of properties and on an extended one. Section 5 concludes. 2 Background 2.1 The German ParGram LFG The grammar for which the log-linear model for parse selection described in this paper was developed is the German ParGram LFG (Dipper, 2003; Rohrer and Forst, 2006). It has been developed with and for the grammar development and processing platform XLE (Crouch et al., 2006) and consists of a symbolic LFG, which can be employed both for parsing and generation, and a two-stage disambiguation module, the log-linear model being the component that carries out the final selection among the parses that have been retained by an OptimalityTheoretically inspired prefilter (Frank et al., 2001; Forst et al., 2005). The grammar has a coverage in terms of full parses that exceeds 80% on newspaper corpora. For sentences out of coverage, it employs the robustness techni</context>
</contexts>
<marker>Rohrer, Forst, 2006</marker>
<rawString>Christian Rohrer and Martin Forst. 2006. Improving coverage and parsing quality of a large-scale LFG for German. In Proceedings of the Language Resources and Evaluation Conference (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Stuart M Shieber</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Parse disambiguation for a rich HPSG grammar.</title>
<date>2002</date>
<booktitle>In First Workshop on Treebanks and Linguistic Theories (TLT2002),</booktitle>
<pages>253--263</pages>
<contexts>
<context position="3934" citStr="Toutanova et al., 2002" startWordPosition="630" endWordPosition="633">onal Linguistics matical relations and morphosyntactic features (or 72.59% on grammatical relations only) and 85.50% (or 79.36%). The lower bound is based on an arbitrary selection among the parses built up by the symbolic grammar; the upper bound is determined by the best possible selection. 2.2 Log-linear models for disambiguation Since Johnson et al. (1999), log-linear models of the following form have become standard as disambiguation devices for precision grammars: m=1 λj fj(x,y) PA(x 2J) = eEj � j=1 λj�fj(x�,y) x��X(y) e They are used for parse selection in the English Resource Grammar (Toutanova et al., 2002), the English ParGram LFG (Riezler et al., 2002), the English Enju HPSG (Miyao and Tsujii, 2002), the HPSG-inspired Alpino parser for Dutch (Malouf and van Noord, 2004; van Noord, 2006) and the English CCG from Edinburgh (Clark and Curran, 2004). While relatively much work has gone into the question of how to estimate the property weights A1 ... Am efficiently and accurately on the basis of (annotated) corpus data, the question of how to define suitable and informative property functions f1 ... fm has received relatively little attention. However, we are convinced that property design is the p</context>
</contexts>
<marker>Toutanova, Manning, Shieber, Flickinger, Oepen, 2002</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, Stuart M. Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse disambiguation for a rich HPSG grammar. In First Workshop on Treebanks and Linguistic Theories (TLT2002), pages 253–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Uszkoreit</author>
<author>Thorsten Brants</author>
<author>Brigitte Krenn</author>
<author>Lars Konieczny</author>
<author>Stephan Oepen</author>
<author>Wojciech Skut</author>
</authors>
<date>1998</date>
<booktitle>Relative Clause Extraposition in German – Evidence from Corpus Studies and Acceptability Ratings. In Proceedings of AMLaP-98,</booktitle>
<location>Freiburg, Germany.</location>
<contexts>
<context position="14331" citStr="Uszkoreit et al. (1998)" startWordPosition="2325" endWordPosition="2328"> APPs comprise not only title nouns, but also nouns like Abteilung ‘department’, Buch ‘book’, etc. ‘There is a central role for the car insurance, which receives a fifth [... ].’ (s27539) In order to allow for an improved resolution of this kind of attachment ambiguity, we introduced properties that extract the surface distance of an extraposed constituent to its functional head as well as properties that record how the functional uncertainty paths involved in these attachments were instantiated. This way, we hope to extract the information necessary to model the tendencies observed, e.g., in Uszkoreit et al. (1998). 3.4 Lexicalized properties capturing dependencies Inspired by Malouf and van Noord (2004), we finally also introduced lexicalized properties capturing dependencies. These are built on the following property templates: DEP12 &lt;PoS1&gt; &lt;Dep&gt; &lt;PoS2&gt; &lt;Lemma2&gt;, DEP21 &lt;PoS1&gt; &lt;Lemma1&gt; &lt;Dep&gt; &lt;PoS2&gt;andDEP22 &lt;PoS1&gt; &lt;Lemma1&gt; &lt;Dep&gt; &lt;PoS2&gt; &lt;Lemma2&gt;. These are intended to capture information on the subcategorization behavior of lexical elements and on typical collocations. 1Humanness information is imported from GermaNet. 20 &amp;quot;das den Anwalt Klaus Bollig zum vorläufigen Verwalter bestellte&amp;quot; (a) evaluated as l</context>
</contexts>
<marker>Uszkoreit, Brants, Krenn, Konieczny, Oepen, Skut, 1998</marker>
<rawString>Hans Uszkoreit, Thorsten Brants, Brigitte Krenn, Lars Konieczny, Stephan Oepen, and Wojciech Skut. 1998. Relative Clause Extraposition in German – Evidence from Corpus Studies and Acceptability Ratings. In Proceedings of AMLaP-98, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Uszkoreit</author>
</authors>
<title>Word Order and Constituent Structure in German.</title>
<date>1987</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="8969" citStr="Uszkoreit (1987)" startWordPosition="1462" endWordPosition="1463">es are of the kinds fs adj attrs SUBJ ADJUNCT and fs subattr OBJ ADJUNCT, which are only remotely, if at all, related to the observed SUBJ-OBJ ambiguity. The crucial information from the intended reading, namely that the SUBJ precedes the OBJ, is not captured directly by any of the initial properties. We therefore introduce a new property template that records the linear order of two grammatical functions and instantiate it for all relevant combinations. The new properties created this way make it possible to capture the default order of nominal arguments, which according to Lenerz (1977) and Uszkoreit (1987) (among others), is SUBJ, OBJ-TH, OBJ. Similarly to the SUBJ-OBJ ambiguity just considered, the ADJUNCT-OBL ambiguity in (2) cannot at all be resolved on the basis of c-structurebased properties, and the f-structure-based properties whose values differ among the two readings seem only remotely related to the observed ambiguity. (2) [A/O Dagegen] sprach sich Against that/In contrast spoke himself [. . . ] Micha Guttmann [O/A f¨ur getrennte [. . . ] Micha Guttmann for separate Gedenkst¨atten] aus. memorials out. ‘In contrast, [... ] Michael Guttmann argued for separate memorials.’ (s2090) Howeve</context>
</contexts>
<marker>Uszkoreit, 1987</marker>
<rawString>Hans Uszkoreit. 1987. Word Order and Constituent Structure in German. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At Last Parsing Is Now Operational. In</title>
<date>2006</date>
<booktitle>TALN06. Verbum Ex Machina. Actes de la 13e conf´erence sur le traitement automatique des langues naturelles,</booktitle>
<pages>20--42</pages>
<editor>Piet Mertens, Cedrick Fairon, Anne Dister, and Patrick Watrin, editors,</editor>
<location>Leuven, Belgium.</location>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At Last Parsing Is Now Operational. In Piet Mertens, Cedrick Fairon, Anne Dister, and Patrick Watrin, editors, TALN06. Verbum Ex Machina. Actes de la 13e conf´erence sur le traitement automatique des langues naturelles, pages 20– 42, Leuven, Belgium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>