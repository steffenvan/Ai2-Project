<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005692">
<title confidence="0.993089">
Detecting controversies in Twitter: a first study
</title>
<author confidence="0.802782">
Marco Pennacchiotti Ana-Maria Popescu
</author>
<affiliation confidence="0.355036">
Yahoo! Labs Yahoo! Labs
</affiliation>
<address confidence="0.274574">
Sunnyvale, CA. Sunnyvale, CA.
</address>
<email confidence="0.342014">
pennac@yahoo-inc.com amp@yahoo-inc.com
</email>
<bodyText confidence="0.999113323529412">
Social media gives researchers a great opportunity
to understand how the public feels and thinks about
a variety of topics, from political issues to entertain-
ment choices. While previous research has explored
the likes and dislikes of audiences, we focus on a
related but different task of detecting controversies
involving popular entities, and understanding their
causes. Intuitively, if people hotly debate an entity
in a given period of time, there is a good chance of a
controversy occurring. Consequently, we use Twit-
ter data, boosted with knowledge extracted from the
Web, as a starting approach: This paper introduces
our task, an initial method and encouraging early re-
sults.
Controversy Detection. We focus on detect-
ing controversies involving known entities in Twit-
ter data. Let a snapshot denote a triple s =
(e, At, tweets), where e is an entity, At is a time
period and tweets is the set of tweets from the tar-
get time period which refer to the target entity.1. Let
cont(s) denote the level of controversy associated
with entity e in the context of the snapshot s. Our
task is as follows:
Task. Given an entity set E and a snapshot set
S = {(e, At, tweets)|e E E}, compute the con-
troversy level cont(s) for each snapshot s in S and
rank S with respect to the resulting scores.
Overall Solution. Figure 1 gives an overview of
our solution. We first select the set B C S, consist-
ing of candidate snapshots that are likely to be con-
troversial (buzzy snapshots). Then, for each snap-
shot in B, we compute the controversy score cont,
by combining a timely controversy score (tcont) and
a historical controversy score (hcont).
</bodyText>
<footnote confidence="0.76536625">
Resources. Our method uses a sentiment lexi-
con SL (7590 terms) and a controversy lexicon CL
1We use 1-day as the time period At. E.g. s=(‘Brad
Pitt’,12/11/2009,tweets)
</footnote>
<page confidence="0.990008">
31
</page>
<figure confidence="0.797300714285714">
Algorithm 0.1: CONTROVERSYDETECTION(S,Twitter)
select buzzy snapshots B C S
for s E B
Jtcont(s) = a * MixSent(s) + (1 − a) * Controv(s))
l cont(s) =,3 * tcont(s) + (1 − /3) * hcont(s)
rank B on scores
return (B)
</figure>
<figureCaption confidence="0.999778">
Figure 1: Controversy Detection: Overview
</figureCaption>
<bodyText confidence="0.99854">
(750 terms). The sentiment lexicon is composed by
augmenting the set of positive and negative polarity
terms in OpinionFinder 1.5 2 (e.g. ‘love’,‘wrong’)
with terms bootstrapped from a large set of user
reviews. The controversy lexicon is compiled by
mining controversial terms (e.g. ‘trial’, ‘apology’)
from Wikipedia pages of people included in the
Wikipedia controversial topic list.
Selecting buzzy snapshots. We make the simple
assumption that if in a given time period, an entity is
discussed more than in the recent past, then a contro-
versy involving the entity is likely to occurr in that
period. We model the intuition with the score:
</bodyText>
<equation confidence="0.9988815">
b(s) = ( E�tweetss�
iEprev(s,N)
</equation>
<bodyText confidence="0.905689625">
where tweetss is the set of tweets in the snapshot
s; and prev(s, N) is the set of snapshots referring to
the same entity of s, in N time periods previous to
s. In our experiment, we use N = 2, i.e. we focus
on two days before s. We retain as buzzy snapshots
only those with b(s) &gt; 3.0.
Historical controversy score. The hcont score
estimates the overall controversy level of an entity
in Web data, independently of time. We consider
hcont our baseline system, to which we compare
the Twitter-based models. The score is estimated
on Web document data using the CL lexicon as fol-
2J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. In Language
Resources and Evaluation.
Itweetsil)/N
</bodyText>
<note confidence="0.759152">
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 31–32,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9929225">
lows: hcont(e) = k/|CL|, where k is the number of
controversy terms t&apos; s.t. PMI(e, t&apos;) &gt; A3.
Timely controversy score. tcont estimates the
controversy of an entity by analyzing the discussion
among Twitter’s users in a given time period, i.e. in
a given snapshot. It is a linear combination (tuned
with α ∈ [0, 1]) of two scores:
MixSent(s): reflects the relative disagreement
about the entity in the Twitter data from snapshot
s. First, each of the N tweets in s is placed in one of
the following sets: Positive (Pos), Negative (Neg),
Neutral (Neu), based on the number of positive and
negative SL terms in the tweet. MixSent is com-
puted as:
</bodyText>
<equation confidence="0.9981215">
MixSent(s) = Min(|Pos|, |Neg|) × |Pos |+ |Neg|
Max(|Pos)|,|Neg|) N
</equation>
<bodyText confidence="0.999068555555556">
Controv(s): this score reflects the presence of
explicit controversy terms in tweets. It is computed
as: Controv(s) = |ctv|/N, where ctv is the set of
tweets in s which contain at least one controversy
term from CL.
Overall controversy score. The overall score
is a linear combination of the timely and historical
scores: cont(s) = Q ∗tcont(s)+(1−Q)∗hcont(s),
where Q ∈ [0, 1] is a parameter.
</bodyText>
<subsectionHeader confidence="0.745266">
Experimental Results
</subsectionHeader>
<bodyText confidence="0.999514736842105">
We evaluate our model on the task of ranking snap-
shots according to their controversy level. Our cor-
pus is a large set of Twitter data from Jul-2009 to
Feb-2010. The set of entities E is composed of
104,713 celebrity names scraped from Wikipedia
for the Actor, Athlete, Politician and Musician cat-
egories. The overall size of S amounts to 661,226
(we consider only snapshots with a minimum of 10
tweets). The number of buzzy snapshots in B is
30,451. For evaluation, we use a gold standard of
120 snapshots randomly sampled from B, and man-
ually annotated as controversial or not-controversial
by two expert annotators (detailed guidelines will be
presented at the workshop). Kappa-agreement be-
tween the annotators, estimated on a subset of 20
snapshots, is 0.89 (‘almost perfect’ agreement). We
experiment with different α and Q values, as re-
ported in Table 1, in order to discern the value of
final score components. We use Average Precision
</bodyText>
<footnote confidence="0.9886265">
3PMI is computed based on the co-occurrences of entities
and terms in Web documents; here we use A = 2.
</footnote>
<table confidence="0.99931475">
Model α /3 AP AROC
hcont (baseline) 0.0 0.0 0.614 0.581
tcont-MixSent 1.0 1.0 0.651 0.642
tcont-Controv 0.0 1.0 0.614 0.611
tcont-combined 0.5 1.0 0.637 0.642
cont 0.5 0.5 0.628 0.646
cont 0.8 0.8 0.643 0.642
cont 1.0 0.5 0.660 0.662
</table>
<tableCaption confidence="0.9853995">
Table 1: Controversial Snapshot Detection: results over
different model parametrizations
</tableCaption>
<bodyText confidence="0.999802078947368">
(AP), and the area under the ROC curve (AROC) as
our evaluation measures.
The results in Table 1 show that all Twitter-based
models perform better than the Web-based baseline.
The most effective basic model is MixSent, sug-
gesting that the presence of mixed polarity sentiment
terms in a snapshot is a good indicator of contro-
versy. For example, ‘Claudia Jordan’ appears in a
snapshot with a mix of positive and negative terms
-in a debate about a red carpet appearance- but the
hcont and Controv scores are low as there is no
record of historical controversy or explicit contro-
versy terms in the target tweets. Best overall per-
formance is achieved by a mixed model combining
the hcont and the MixSent score (last row in Table
label 1). There are indeed cases in which the evi-
dence from MixSent is not enough - e.g., a snap-
shot discussing ‘Jesse Jackson’ ’s appearance on a
tv show lacks common positive or negative terms,
but reflects users’ confusion nevertheless; however,
‘Jesse Jackson’ has a high historical controversy
score, which leads our combined model to correctly
assign a high controversy score to the snapshot. In-
terestingly, most controversies in the gold standard
refer to micro-events (e.g., tv show, award show or
athletic event appearances), rather than more tradi-
tional controversial events found in news streams
(e.g., speeches about climate change, controversial
movie releases, etc.); this further strengthens the
case that Twitter is a complementary information
source wrt news corpora.
We plan to follow up on this very preliminary
investigation by improving our Twitter-based sen-
timent detection, incorporating blog and news data
and generalizing our controversy model (e.g., dis-
covering the ‘what’ and the ‘why’ of a controversy,
and tracking common controversial behaviors of en-
tities over time).
</bodyText>
<page confidence="0.998805">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.080048">
<title confidence="0.998338">Detecting controversies in Twitter: a first study</title>
<author confidence="0.9981">Marco Pennacchiotti Ana-Maria Popescu</author>
<affiliation confidence="0.690152">Yahoo! Labs Yahoo! Labs</affiliation>
<address confidence="0.637565">Sunnyvale, CA. Sunnyvale, CA.</address>
<email confidence="0.911174">pennac@yahoo-inc.comamp@yahoo-inc.com</email>
<abstract confidence="0.983832741176471">Social media gives researchers a great opportunity to understand how the public feels and thinks about a variety of topics, from political issues to entertainment choices. While previous research has explored the likes and dislikes of audiences, we focus on a but different task of detecting involving popular entities, and understanding their causes. Intuitively, if people hotly debate an entity in a given period of time, there is a good chance of a controversy occurring. Consequently, we use Twitter data, boosted with knowledge extracted from the Web, as a starting approach: This paper introduces our task, an initial method and encouraging early results. Detection. focus on detecting controversies involving known entities in Twitdata. Let a a triple where an entity, a time and the set of tweets from the tartime period which refer to the target Let the level of controversy associated entity the context of the snapshot Our task is as follows: an entity set a snapshot set compute the conlevel each snapshot respect to the resulting scores. Solution. 1 gives an overview of solution. We first select the set consisting of candidate snapshots that are likely to be con- Then, for each snapin we compute the controversy score combining a controversy and controversy method uses a sentiment lexiterms) and a controversy lexicon use 1-day as the time period E.g. Pitt’,12/11/2009,tweets) 31 0.1: buzzy snapshots B = + (1 + (1 rank B on scores Figure 1: Controversy Detection: Overview terms). The lexicon composed by augmenting the set of positive and negative polarity in OpinionFinder 1.5 ‘love’,‘wrong’) with terms bootstrapped from a large set of user The lexicon compiled by mining controversial terms (e.g. ‘trial’, ‘apology’) from Wikipedia pages of people included in the topic buzzy snapshots. make the simple assumption that if in a given time period, an entity is discussed more than in the recent past, then a controversy involving the entity is likely to occurr in that period. We model the intuition with the score: = ( the set of tweets in the snapshot and the set of snapshots referring to same entity of in periods previous to In our experiment, we use i.e. we focus two days before We retain as buzzy snapshots those with controversy score. estimates the overall controversy level of an entity in Web data, independently of time. We consider to which we compare the Twitter-based models. The score is estimated Web document data using the as fol- Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. In Language Resources and Evaluation. of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social pages Angeles, California, June 2010. Association for Computational Linguistics where the number of terms controversy score. the controversy of an entity by analyzing the discussion among Twitter’s users in a given time period, i.e. in a given snapshot. It is a linear combination (tuned of two scores: reflects the relative disagreement about the entity in the Twitter data from snapshot First, each of the in placed in one of following sets: Positive Negative based on the number of positive and in the tweet. computed as: = this score reflects the presence of explicit controversy terms in tweets. It is computed = where the set of in contain at least one controversy from controversy score. overall score is a linear combination of the timely and historical = 1] a parameter. Experimental Results We evaluate our model on the task of ranking snapshots according to their controversy level. Our corpus is a large set of Twitter data from Jul-2009 to The set of entities composed of 104,713 celebrity names scraped from Wikipedia for the Actor, Athlete, Politician and Musician cat- The overall size of to 661,226 (we consider only snapshots with a minimum of 10 The number of buzzy snapshots in For evaluation, we use a standard snapshots randomly sampled from and manually annotated as controversial or not-controversial by two expert annotators (detailed guidelines will be presented at the workshop). Kappa-agreement between the annotators, estimated on a subset of 20 snapshots, is 0.89 (‘almost perfect’ agreement). We with different as reported in Table 1, in order to discern the value of score components. We use Precision is computed based on the co-occurrences of entities terms in Web documents; here we use Model α /3 AP AROC hcont (baseline) 0.0 0.0 0.614 0.581 tcont-MixSent 1.0 1.0 0.651 0.642 tcont-Controv 0.0 1.0 0.614 0.611 tcont-combined 0.5 1.0 0.637 0.642 cont 0.5 0.5 0.628 0.646 cont 0.8 0.8 0.643 0.642 cont 1.0 0.5 0.660 0.662 Table 1: Controversial Snapshot Detection: results over different model parametrizations and the under the ROC curve as our evaluation measures. The results in Table 1 show that all Twitter-based models perform better than the Web-based baseline. most effective basic model is suggesting that the presence of mixed polarity sentiment terms in a snapshot is a good indicator of controversy. For example, ‘Claudia Jordan’ appears in a snapshot with a mix of positive and negative terms -in a debate about a red carpet appearancebut the are low as there is no record of historical controversy or explicit controversy terms in the target tweets. Best overall performance is achieved by a mixed model combining the (last row in Table label 1). There are indeed cases in which the evifrom not enough e.g., a snapshot discussing ‘Jesse Jackson’ ’s appearance on a tv show lacks common positive or negative terms, but reflects users’ confusion nevertheless; however, ‘Jesse Jackson’ has a high historical controversy score, which leads our combined model to correctly assign a high controversy score to the snapshot. Interestingly, most controversies in the gold standard to tv show, award show or athletic event appearances), rather than more traditional controversial events found in news streams (e.g., speeches about climate change, controversial movie releases, etc.); this further strengthens the case that Twitter is a complementary information source wrt news corpora. We plan to follow up on this very preliminary investigation by improving our Twitter-based sentiment detection, incorporating blog and news data and generalizing our controversy model (e.g., discovering the ‘what’ and the ‘why’ of a controversy, and tracking common controversial behaviors of entities over time).</abstract>
<intro confidence="0.895796">32</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>