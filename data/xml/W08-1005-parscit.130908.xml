<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014741">
<title confidence="0.99397">
Parsing German with Latent Variable Grammars
</title>
<author confidence="0.836079">
Slav Petrov and Dan Klein
</author>
<email confidence="0.916925">
{petrov,klein}@cs.berkeley.edu
</email>
<affiliation confidence="0.9581">
University of California at Berkeley
</affiliation>
<address confidence="0.33497">
Berkeley, CA 94720
</address>
<sectionHeader confidence="0.989942" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945571428571">
We describe experiments on learning latent
variable grammars for various German tree-
banks, using a language-agnostic statistical
approach. In our method, a minimal ini-
tial grammar is hierarchically refined using an
adaptive split-and-merge EM procedure, giv-
ing compact, accurate grammars. The learn-
ing procedure directly maximizes the likeli-
hood of the training treebank, without the use
of any language specific or linguistically con-
strained features. Nonetheless, the resulting
grammars encode many linguistically inter-
pretable patterns and give the best published
parsing accuracies on three German treebanks.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997011875">
Probabilistic context-free grammars (PCFGs) under-
lie most high-performance parsers in one way or an-
other (Collins, 1999; Charniak, 2000; Charniak and
Johnson, 2005). However, as demonstrated in Char-
niak (1996) and Klein and Manning (2003), a PCFG
which simply takes the empirical rules and probabil-
ities off of a treebank does not perform well. This
naive grammar is a poor one because its context-
freedom assumptions are too strong in some ways
(e.g. it assumes that subject and object NPs share
the same distribution) and too weak in others (e.g.
it assumes that long rewrites do not decompose into
smaller steps). Therefore, a variety of techniques
have been developed to both enrich and generalize
the naive grammar, ranging from simple tree anno-
tation and symbol splitting (Johnson, 1998; Klein
</bodyText>
<page confidence="0.986645">
33
</page>
<bodyText confidence="0.999932176470588">
and Manning, 2003) to full lexicalization and intri-
cate smoothing (Collins, 1999; Charniak, 2000).
We view treebank parsing as the search for an
optimally refined grammar consistent with a coarse
training treebank. As a result, we begin with the
provided evaluation symbols (such as NP, VP, etc.)
but split them based on the statistical patterns in
the training trees. A manual approach might take
the symbol NP and subdivide it into one subsymbol
NP&amp;quot;S for subjects and another subsymbol NP&amp;quot;VP
for objects. However, rather than devising linguis-
tically motivated features or splits, we take a fully
automated approach, in which each symbol is split
into unconstrained subsymbols. For example, NP
would be split into NP-1 through NP-8. We use
the Expectation-Maximization (EM) to then fit our
split model to the observed trees; therein the vari-
ous subsymbols will specialize in ways which may
or may not correspond to our linguistic intuitions.
This approach is relatively language independent,
because the hidden subsymbols are induced auto-
matically from the training trees based solely on data
likelihood, though of course it is most applicable to
strongly configurational languages.
In our experiments, we find that we can learn
compact grammars that give the highest parsing ac-
curacies in the 2008 Parsing German shared task.
Our F1-scores of 69.8/84.0 (TIGER/TueBa-D/Z) are
more than four points higher than those of the
second best systems. Additionally, we investigate
the patterns that are learned and show that the la-
tent variable approach recovers linguistically inter-
pretable phenomena. In our analysis, we pay partic-
ular attention to similarities and differences between
</bodyText>
<note confidence="0.7501355">
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<figure confidence="0.93715">
(b)
</figure>
<figureCaption confidence="0.9922855">
Figure 1: (a) The original tree. (b) The binarized tree
with latent variables.
</figureCaption>
<bodyText confidence="0.926966">
grammars learned from the two treebanks.
</bodyText>
<sectionHeader confidence="0.948039" genericHeader="method">
2 Latent Variable Parsing
</sectionHeader>
<bodyText confidence="0.999918611111111">
In latent variable parsing (Matsuzaki et al., 2005;
Prescher, 2005; Petrov et al., 2006), we learn
rule probabilities on latent annotations that, when
marginalized out, maximize the likelihood of the
unannotated training trees. We use an automatic ap-
proach in which basic nonterminal symbols are al-
ternately split and merged to maximize the likeli-
hood of the training treebank.
In this section we briefly review the main ideas
in latent variable parsing. This work has been pre-
viously published and we therefore provide only
a short overview. For a more detailed exposi-
tion of the learning algorithm the reader is re-
ferred to Petrov et al. (2006). The correspond-
ing inference procedure is described in detail in
Petrov and Klein (2007). The parser, code,
and trained models are available for download at
http://nlp.cs.berkeley.edu.
</bodyText>
<subsectionHeader confidence="0.994089">
2.1 Learning
</subsectionHeader>
<bodyText confidence="0.999979183333334">
Starting with a simple X-bar grammar, we use the
Expectation-Maximization (EM) algorithm to learn
a new grammar whose nonterminals are subsymbols
of the original evaluation nonterminals. The X-bar
grammar is created by binarizing the treebank trees;
for each local tree rooted at an evaluation nonter-
minal X, we introduce a cascade of new nodes la-
beled X so that each node has at most two children,
see Figure 1. This initialization is the absolute mini-
mum starting grammar that distinguishes the evalua-
tion nonterminals (and maintains separate grammars
for each of them).
In Petrov et al. (2006) we show that a hierarchical
split-and-merge strategy learns compact but accurate
grammars, allocating subsymbols adaptively where
they are most effective. Beginning with the base-
line grammar, we repeatedly split and re-train the
grammar. In each iteration, we initialize EM with
the results of the previous round’s grammar, splitting
every previous symbol in two and adding a small
amount of randomness (1%) to break the symme-
try between the various subsymbols. Note that we
split all nonterminal symbols, including the part-of-
speech categories. While creating more latent an-
notations can increase accuracy, it can also lead to
overfitting via oversplitting. Adding subsymbols di-
vides grammar statistics into many bins, resulting in
a tighter fit to the training data. At the same time,
each bin has less support and therefore gives a less
robust estimate of the grammar probabilities. At
some point, the fit no longer generalizes, leading to
overfitting.
To prevent oversplitting, we could measure the
utility of splitting each latent annotation individu-
ally and then split the best ones first. However, not
only is this impractical, requiring an entire training
phase for each new split, but it assumes the contri-
butions of multiple splits are independent. In fact,
extra subsymbols may need to be added to several
nonterminals before they can cooperate to pass in-
formation along the parse tree. This point is cru-
cial to the success of our method: because all splits
are fit simultaneously, local splits can chain together
to propagate information non-locally. We therefore
address oversplitting in the opposite direction; after
training all splits, we measure for each one the loss
in likelihood incurred by removing it. If this loss
is small, the new annotation does not carry enough
useful information and can be removed. Another ad-
vantage of evaluating post-hoc merges is that, unlike
the likelihood gain from splitting, the likelihood loss
from merging can be efficiently approximated.
To summarize, splitting provides an increasingly
tight fit to the training data, while merging improves
generalization and controls grammar size. In order
to further overcome data fragmentation and overfit-
ting, we also smooth our parameters along the split
hierarchy. Smoothing allows us to add a larger num-
ber of annotations, each specializing in only a frac-
tion of the data, without overfitting our training set.
</bodyText>
<figure confidence="0.998322173913044">
ROOT
FRAG-x
FRAG-x
RB-x
Not
NP-x
.
NN
Not
DT
this
year
(a)
RB
.
NP
FRAG
NN-x
year
DT-x
this
.-x
.
</figure>
<page confidence="0.970803">
34
</page>
<subsectionHeader confidence="0.843951">
2.2 Inference
</subsectionHeader>
<bodyText confidence="0.999993897435897">
At inference time, we want to use the learned gram-
mar to efficiently and accurately compute a parse
tree for a give sentence.
For efficiency, we employ a hierarchical coarse-
to-fine inference scheme (Charniak et al., 1998;
Charniak and Johnson, 2005; Petrov and Klein,
2007) which vastly improves inference time with no
loss in test set accuracy. Our method considers the
splitting history of the final grammar, projecting it
onto its increasingly refined prior stages. For each
such projection of the refined grammar, we estimate
the projection’s parameters from the source PCFG
itself (rather than the original treebank), using tech-
niques for infinite tree distributions and iterated fix-
point equations. We then rapidly pre-parse with each
refinement stage in sequence, such that any item
X:[i, j] with sufficiently low posterior probability
triggers the pruning of its further refined variants in
all subsequent finer parses.
Our refined grammars G are over symbols of the
form X-k where X is an evaluation symbol (such as
NP) and k is some indicator of a subsymbol, which
may encode something linguistic like a parent anno-
tation context, but which is formally just an integer.
G therefore induces a derivation distribution over
trees labeled with split symbols. This distribution
in turn induces a parse distribution over (projected)
trees with unsplit evaluation symbols. We have
several choices of how to select a tree given these
posterior distributions over trees. Since computing
the most likely parse tree is NP-complete (Sima’an,
1992), we settle for an approximation that allows us
to (partially) sum out the latent annotation. In Petrov
and Klein (2007) we relate this approximation to
Goodman (1996)’s labeled brackets algorithm ap-
plied to rules and to Matsuzaki et al. (2005)’s sen-
tence specific variational approximation. This pro-
cedure is substantially superior to simply erasing the
latent annotations from the the Viterbi derivation.
</bodyText>
<subsectionHeader confidence="0.809409">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.99792625">
In Petrov and Klein (2007) we trained models for
English, Chinese and German using the standard
corpora and setups. We applied our latent variable
model directly to each of the treebanks, without any
</bodyText>
<table confidence="0.999156">
&lt; 40 words LP all
LP LR LR
Parser
ENGLISH
Charniak et al. (2005) 90.1 90.1 89.5 89.6
Petrov and Klein (2007) 90.7 90.5 90.2 89.9
ENGLISH (reranked)
Charniak et al. (2005) 92.4 91.6 91.8 91.0
GERMAN (NEGRA)
Dubey (2005) Fl 76.3 80.1 -
Petrov and Klein (2007) 80.8 80.7 80.1
CHINESE
Chiang et al. (2002) 81.1 78.8 78.0 75.2
Petrov and Klein (2007) 86.9 85.7 84.8 81.9
</table>
<tableCaption confidence="0.989103333333333">
Table 1: Our split-and-merge latent variable approach
produces the best published parsing performance on
many languages.
</tableCaption>
<bodyText confidence="0.9998829">
language dependent modifications. Specifically, the
same model hyperparameters (merging percentage
and smoothing factor) were used in all experiments.
Table 1 summarizes the results: automatically in-
ducing latent structure is a technique that generalizes
well across language boundaries and results in state
of the art performance for Chinese and German. On
English, the parser is outperformed by the reranked
output of Charniak and Johnson (2005), but it out-
performs their underlying lexicalized parser.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999711555555555">
We conducted experiments on the two treebanks
provided for the 2008 Parsing German shared task.
Both treebanks are annotated collections of Ger-
man newspaper text, covering from similar top-
ics. They are annotated with part-of-speech (POS)
tags, morphological information, phrase structure,
and grammatical functions. TueBa-D/Z addition-
ally uses topological fields to describe fundamental
word order restrictions in German clauses. However,
the treebanks differ significantly in their annotation
schemes: while TIGER relies on crossing branches
to describe long distance relationships, TueBa-D/Z
uses planar tree structures with designated labels
that encode long distance relationships. Addition-
ally, the annotation in TIGER is relatively flat on the
phrasal level, while TueBa-D/Z annotates more in-
ternal phrase structure.
We used the standard splits into training and de-
</bodyText>
<page confidence="0.980831">
35
</page>
<figure confidence="0.9914137">
F1
90
85
80
75
70
65
60
0 1 2 3 4 5
Split &amp; Merge Iterations
</figure>
<figureCaption confidence="0.9975755">
Figure 2: Parsing accuracy improves when the amount of
latent annotation is increased.
</figureCaption>
<bodyText confidence="0.999784090909091">
velopment set, containing roughly 16,000 training
trees and 1,600 development trees, respectively. All
parsing figures in this section are on the develop-
ment set, evaluating on constituents and grammat-
ical functions using gold part-of-speech tags, un-
less noted otherwise. Note that even when we as-
sume gold evaluation part-of-speech tags, we still
assign probabilities to the different subsymbols of
the provided evaluation tag. The parsing accuracies
in the final results section are the official results of
the 2008 Parsing German shared task.
</bodyText>
<subsectionHeader confidence="0.997961">
3.1 Latent Annotation
</subsectionHeader>
<bodyText confidence="0.998398">
As described in Section 2.1, we start with a mini-
mal X-Bar grammar and learn increasingly refined
grammars in a hierarchical split-and-merge fashion.
We conjoined the constituency categories with their
grammatical functions, creating initial categories
like NP-PD and NP-OA which were further split
automatically. Figure 2 shows how held-out accu-
racy improves when we add latent annotation. Our
baseline grammars have low F1-scores (63.3/72.8,
TIGER/TueBa-D/Z), but performance increases as
the complexity of latent annotation increases. After
four split-and-merge iterations, performance levels
off. Interestingly, the gap in performance between
the two treebanks increases from 9.5 to 13.4 F1-
points. It appears that the latent variable approach
is better suited for capturing the rich structure of the
TueBa-D/Z treebank.
As languages vary in their phrase-internal head-
</bodyText>
<table confidence="0.9925">
TIGER TueBa-D/Z
F1 EX F1 EX
Auto Tags 71.12 28.91 83.18 18.46
Gold Tags 71.74 34.04 85.10 20.98
</table>
<tableCaption confidence="0.908396333333333">
Table 2: Parsing accuracies (F1-score and exact match)
with gold POS tags and automatic POS tags. Many parse
errors are due to incorrect tagging.
</tableCaption>
<bodyText confidence="0.999991583333333">
edness, we varied the binarization scheme, but, con-
sistent with our experience in other languages, no-
ticed little difference between right and left bina-
rization. We also experimented with starting from
a more constrained baseline by adding parent and
sibling annotation. Adding initial structural annota-
tion results in a higher baseline performance. How-
ever, since it fragments the grammar, adding latent
annotation has a smaller effect, eventually resulting
in poorer performance compared to starting from a
simple X-Bar grammar. Essentially, the initial gram-
mar is either mis- or oversplit to some degree.
</bodyText>
<subsectionHeader confidence="0.999022">
3.2 Part-of-speech tagging
</subsectionHeader>
<bodyText confidence="0.999996066666667">
When gold parts-of-speech are not assumed, many
parse errors can be traced back to part-of-speech
(POS) tagging errors. It is therefore interesting to in-
vestigate the influence of tagging errors on the over-
all parsing accuracy. For the shared task, we could
assume gold POS tags: during inference we only al-
lowed (and scored) the different subsymbols of the
correct tags. However, this assumption cannot be
made in a more realistic scenario, where we want to
parse text from an unknown source. Table 2 com-
pares the parsing performance with gold POS tags
and with automatic tagging. While POS tagging er-
rors have little influence on the TIGER treebank,
tagging errors on TueBa-D/Z cause an substantial
number of subsequent parse errors.
</bodyText>
<subsectionHeader confidence="0.997377">
3.3 Two pass parsing
</subsectionHeader>
<bodyText confidence="0.99986">
In the previous experiments, we conflated the
phrasal categories and grammatical functions into
single initial grammar symbol. An alternative is
to first determine the categorical constituency struc-
ture and then to assign grammatical functions to the
chosen constituents in a separate, second pass. To
achieve this, we trained latent variable grammars
for base constituency parsing by stripping off the
</bodyText>
<figure confidence="0.697077">
TIGER
TueBa-D/Z
</figure>
<page confidence="0.990046">
36
</page>
<bodyText confidence="0.999962866666667">
grammatical functions. After four rounds of split
and merge training, these grammars achieve very
good constituency accuracies of 85.1/94.1 F1-score
(TIGER/TueBa-D/Z). For the second pass, we es-
timated (but did not split) X-Bar style grammars
on the grammatical functions only. Fixing the con-
stituency structure from the first pass, we used those
to add grammatical functions. Unfortunately, this
approach proved to be inferior to the unified, one
pass approach, giving F1-scores of only 50.0/69.4
(TIGER/TueBa-D/Z). Presumably, the degradation
can be attributed to the fact that grammatical func-
tions model long-distance relations between the con-
stituents, which can only be captured poorly by an
unsplit, highly local X-bar style grammar.
</bodyText>
<subsectionHeader confidence="0.955119">
3.4 Final Results
</subsectionHeader>
<bodyText confidence="0.999988555555555">
The final results of the shared task evaluation are
shown in Table 3. These results were produced by
a latent variable grammar that was trained for four
split-and-merge iterations, starting from an X-Bar
grammar over conjoined categorical/grammatical
symbols, with a left-branching binarization. Our
automatic latent variable approach serves better for
German disambiguation than the competing ap-
proaches, despite its being very language agnostic.
</bodyText>
<sectionHeader confidence="0.986275" genericHeader="method">
4 Analysis
</sectionHeader>
<bodyText confidence="0.999931375">
In this section, we examine the learned grammars,
discussing what is learned. Because the grammat-
ical functions significantly increase the number of
base categories and make the grammars more diffi-
cult to examine, we show examples from grammars
that were trained for categorical constituency pars-
ing by initially stripping off all grammatical function
annotations.
</bodyText>
<subsectionHeader confidence="0.999327">
4.1 Lexical Splits
</subsectionHeader>
<bodyText confidence="0.99996975">
Since both treebanks use the same part-of-speech
categories, it is easy to compare the learned POS
subcategories. To better understand what is being
learned, we selected two grammars after two split
and merge iterations and examined the word dis-
tributions of the subcategories of various symbols.
The three most likely words for a number of POS
tags are shown in Table 4. Interestingly, the sub-
categories learned from the different treebanks ex-
hibit very similar patterns. For example, in both
cases, the nominal category (NE) has been split
into subcategories for first and last names, abbrevi-
ations and places. The cardinal numbers (CARD)
have been split into subcategories for years, spelled
out numbers, and other numbers. There are of-
ten subcategories distinguishing sentence initial and
sentence medial placement (KOND, PDAT, ART,
APPR, etc.), as well as subcategories capturing case
distinctions (PDAT, ART, etc.).
A quantitative way of analyzing the complexity of
what is learned is to compare the number of subcat-
egories that our split-and-merge procedure has allo-
cated to each category. Table 5 shows the automat-
ically determined number of subcategories for each
POS tag. While many categories have been split into
comparably many of subcategories, the POS tags in
the TIGER treebank have in general been refined
more heavily. This increased refinement can be ex-
plained by our merging criterion. We compute the
loss in likelihood that would be incurred from re-
moving a split, and we merge back the least useful
splits. In this process, lexical and phrasal splits com-
pete with each other. In TueBa-D/Z the phrasal cat-
egories have richer internal structure and therefore
get split more heavily. As a consequence, the lexi-
cal categories are often relatively less refined at any
given stage than in TIGER. Having different merg-
ing thresholds for the lexical and phrasal categories
would eliminate this difference and we might expect
the difference in lexical refinement to become less
pronounced. Of course, because of the different un-
derlying statistics in the two treebanks, we do not
expect the number of subcategories to become ex-
actly equal in any case.
</bodyText>
<subsectionHeader confidence="0.997429">
4.2 Phrasal splits
</subsectionHeader>
<bodyText confidence="0.999978083333333">
Analyzing the phrasal splits is much more difficult,
as the splits can model internal as well as exter-
nal context (as well as combinations thereof) and,
in general, several splits must be considered jointly
before their patterning can be described. Further-
more, the two treebanks use different annotation
standards and different constituent categories. Over-
all, the phrasal categories of the TueBa-D/Z tree-
bank have been more heavily refined, in order to bet-
ter capture the rich internal structures. In both tree-
banks, the most heavily split categories are the noun,
verb and prepositional phrase categories (NP/NX,
</bodyText>
<page confidence="0.997662">
37
</page>
<table confidence="0.9983048">
TIGER TueBa-D/Z
LP LR F1 LP LR F1
Berkeley Parser 69.23 70.41 69.81 83.91 84.04 83.97
V¨axj¨o Parser 67.06 63.40 65.18 76.20 74.56 75.37
Stanford Parser 58.52 57.63 58.07 79.26 79.22 79.24
</table>
<tableCaption confidence="0.9893335">
Table 3: Final test set results of the 2008 Parsing German shared task (labeled precision, labeled recall and F1-score)
on both treebanks (including grammatical functions and using gold part-of-speech tags).
</tableCaption>
<table confidence="0.997092">
NE NE
Kohl Klaus SPD Deutschland Milosevic Peter K. Berlin
Rabin Helmut USA dpa M¨uller Wolfgang W. taz
Lafontaine Peter CDU Bonn Clinton Klaus de Kosovo
CARD CARD
1996 zwei 000 zwei 1998 zwei 500 zwei
1994 drei 100 3 1999 drei 100 20
1991 vier 20 2 2000 f¨unf 20 18
KOND KOND
Und und sondern und Und und sondern und
Doch oder aber oder Aber oder weder Denn
Aber aber bis sowie Doch aber sowohl oder
PDAT PDAT
Diese dieser diesem - Dieser diese diesem dieser
Dieser dieses diese - Diese dieser dieser diese
Dieses diese dieser - Dieses dieses diesen dieses
ART ART
Die der der die Die die die der
Der des den der die Die der die
Das Die die den Der das den den
APPR APPR
In als in von In bis in von
Von nach von f¨ur Mit Von auf in
Nach vor mit Nach Bis mit f¨ur
PDS PDS
Das dessen das - dem dessen das Das
Dies deren dies - das die Das das
Diese die diese - jene denen dies diese
</table>
<tableCaption confidence="0.990071">
Table 4: The three most likely words for several part-of-speech (sub-)categories. The left column corresponds to the
TIGER treebank the right column to the TueBa-D/Z treebank. Similar subcategories are learned for both treebanks.
</tableCaption>
<page confidence="0.956333">
38
</page>
<table confidence="0.999785400000001">
POS Ti Tue
ADJA 32 17
NN 32 32
NE 31 32
ADV 30 15
ADJD 30 19
VVFIN 29 5
VVPP 29 4
APPR 25 24
VVINF 18 7
CARD 18 16
ART 10 7
PIS 9 14
PPER 9 2
PIDAT - 9
POS Ti Tue
PIAT 8 7
VAFIN 8 3
KON 8 8
$[ 7 11
PROAV 7 -
APPRART 6 5
$ 6 2
PDS 5 5
PPOSAT 4 4
$. 4 5
PDAT 4 5
KOUS 4 3
VMFIN 4 1
PRELS 3 1
POS Ti Tue
VVIZU 3 2
VAINF 3 3
PTKNEG 3 1
FM 3 8
PWS 2 2
PWAV 2 5
XY 2 2
TRUNC 2 4
KOUI 2 1
PTKVZ 2 1
VAPP 2 2
KOKOM 2 5
PROP - 2
VVIMP 1 1
POS Ti Tue
VAIMP 1 1
VMPP 1 2
PPOSS 1 1
PRELAT 1 1
NNE 1 -
APPO 1 1
PTKA 1 2
PTKANT 1 2
PWAT 1 2
PRF 1 1
PTKZU 1 1
APZR 1 1
VMINF 1 1
ITJ 1 2
</table>
<tableCaption confidence="0.699864666666667">
Table 5: Automatically determined number of subcategories for the part-of-speech tags. The left column corresponds
to the TIGER treebank the right column to the TueBa-D/Z treebank. Many categories are split in the same number of
subcategories, but overall the TIGER categories have been more heavily refined.
</tableCaption>
<bodyText confidence="0.999465">
PP/PX, VP/VX*) as well as the sentential categories
(S/SIMPX). Categories that are rare or that have lit-
tle internal structure, in contrast, have been split
lightly or not at all.
</bodyText>
<sectionHeader confidence="0.999763" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999978">
We presented a series of experiments on pars-
ing German with latent variable grammars. We
showed that our latent variable approach is very
well suited for parsing German, giving the best
parsing figures on several different treebanks, de-
spite being completely language independent. Ad-
ditionally, we examined the learned grammars
and showed examples illustrating the linguistically
meaningful patterns that were learned. The parser,
code, and models are available for download at
http://nlp.cs.berkeley.edu.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952485714286">
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL’05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 61h Workshop on Very
Large Corpora.
E. Charniak. 1996. Tree-bank grammars. In AAAI ’96,
pages 1031–1036.
E. Charniak. 2000. A maximum–entropy–inspired
parser. In NAACL ’00, pages 132–139.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In COLING ’02, pages 183–189.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
A. Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL ’05.
J. Goodman. 1996. Parsing algorithms and metrics. ACL
’96.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24:613–632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL ’03, pages 423–430.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ’05.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ’07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ’06.
D. Prescher. 2005. Inducing head-driven PCFGs with la-
tent heads: Refining a tree-bank grammar for parsing.
In ECML’05.
K. Sima’an. 1992. Computatoinal complexity of proba-
bilistic disambiguation. Grammars, 5:125–151.
</reference>
<page confidence="0.999533">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984715">
<title confidence="0.999936">Parsing German with Latent Variable Grammars</title>
<author confidence="0.999907">Slav Petrov</author>
<author confidence="0.999907">Dan Klein</author>
<affiliation confidence="0.999992">University of California at Berkeley</affiliation>
<address confidence="0.99988">Berkeley, CA 94720</address>
<abstract confidence="0.998895733333333">We describe experiments on learning latent variable grammars for various German treebanks, using a language-agnostic statistical approach. In our method, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine NBest Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In ACL’05.</booktitle>
<contexts>
<context position="959" citStr="Charniak and Johnson, 2005" startWordPosition="128" endWordPosition="131">nitial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symb</context>
<context position="7804" citStr="Charniak and Johnson, 2005" startWordPosition="1220" endWordPosition="1223">er to further overcome data fragmentation and overfitting, we also smooth our parameters along the split hierarchy. Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set. ROOT FRAG-x FRAG-x RB-x Not NP-x . NN Not DT this year (a) RB . NP FRAG NN-x year DT-x this .-x . 34 2.2 Inference At inference time, we want to use the learned grammar to efficiently and accurately compute a parse tree for a give sentence. For efficiency, we employ a hierarchical coarseto-fine inference scheme (Charniak et al., 1998; Charniak and Johnson, 2005; Petrov and Klein, 2007) which vastly improves inference time with no loss in test set accuracy. Our method considers the splitting history of the final grammar, projecting it onto its increasingly refined prior stages. For each such projection of the refined grammar, we estimate the projection’s parameters from the source PCFG itself (rather than the original treebank), using techniques for infinite tree distributions and iterated fixpoint equations. We then rapidly pre-parse with each refinement stage in sequence, such that any item X:[i, j] with sufficiently low posterior probability trigg</context>
<context position="10654" citStr="Charniak and Johnson (2005)" startWordPosition="1668" endWordPosition="1671">78.8 78.0 75.2 Petrov and Klein (2007) 86.9 85.7 84.8 81.9 Table 1: Our split-and-merge latent variable approach produces the best published parsing performance on many languages. language dependent modifications. Specifically, the same model hyperparameters (merging percentage and smoothing factor) were used in all experiments. Table 1 summarizes the results: automatically inducing latent structure is a technique that generalizes well across language boundaries and results in state of the art performance for Chinese and German. On English, the parser is outperformed by the reranked output of Charniak and Johnson (2005), but it outperforms their underlying lexicalized parser. 3 Experiments We conducted experiments on the two treebanks provided for the 2008 Parsing German shared task. Both treebanks are annotated collections of German newspaper text, covering from similar topics. They are annotated with part-of-speech (POS) tags, morphological information, phrase structure, and grammatical functions. TueBa-D/Z additionally uses topological fields to describe fundamental word order restrictions in German clauses. However, the treebanks differ significantly in their annotation schemes: while TIGER relies on cro</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-Fine NBest Parsing and MaxEnt Discriminative Reranking. In ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>S Goldwater</author>
<author>M Johnson</author>
</authors>
<title>Edgebased best-first chart parsing.</title>
<date>1998</date>
<booktitle>61h Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="7776" citStr="Charniak et al., 1998" startWordPosition="1216" endWordPosition="1219">ls grammar size. In order to further overcome data fragmentation and overfitting, we also smooth our parameters along the split hierarchy. Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set. ROOT FRAG-x FRAG-x RB-x Not NP-x . NN Not DT this year (a) RB . NP FRAG NN-x year DT-x this .-x . 34 2.2 Inference At inference time, we want to use the learned grammar to efficiently and accurately compute a parse tree for a give sentence. For efficiency, we employ a hierarchical coarseto-fine inference scheme (Charniak et al., 1998; Charniak and Johnson, 2005; Petrov and Klein, 2007) which vastly improves inference time with no loss in test set accuracy. Our method considers the splitting history of the final grammar, projecting it onto its increasingly refined prior stages. For each such projection of the refined grammar, we estimate the projection’s parameters from the source PCFG itself (rather than the original treebank), using techniques for infinite tree distributions and iterated fixpoint equations. We then rapidly pre-parse with each refinement stage in sequence, such that any item X:[i, j] with sufficiently low</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>E. Charniak, S. Goldwater, and M. Johnson. 1998. Edgebased best-first chart parsing. 61h Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In AAAI ’96,</booktitle>
<pages>1031--1036</pages>
<contexts>
<context position="1004" citStr="Charniak (1996)" startWordPosition="136" endWordPosition="138">e split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein 33 and Man</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-bank grammars. In AAAI ’96, pages 1031–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum–entropy–inspired parser.</title>
<date>2000</date>
<booktitle>In NAACL ’00,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="930" citStr="Charniak, 2000" startWordPosition="126" endWordPosition="127">hod, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from si</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum–entropy–inspired parser. In NAACL ’00, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>D Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In COLING ’02,</booktitle>
<tech>Ph.D. thesis, UPenn.</tech>
<pages>183--189</pages>
<marker>Chiang, Bikel, 2002</marker>
<rawString>D. Chiang and D. Bikel. 2002. Recovering latent information in treebanks. In COLING ’02, pages 183–189. M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, UPenn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
</authors>
<title>What to do when lexicalization fails: parsing German with suffix analysis and smoothing.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="9938" citStr="Dubey (2005)" startWordPosition="1562" endWordPosition="1563">uzaki et al. (2005)’s sentence specific variational approximation. This procedure is substantially superior to simply erasing the latent annotations from the the Viterbi derivation. 2.3 Results In Petrov and Klein (2007) we trained models for English, Chinese and German using the standard corpora and setups. We applied our latent variable model directly to each of the treebanks, without any &lt; 40 words LP all LP LR LR Parser ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov and Klein (2007) 90.7 90.5 90.2 89.9 ENGLISH (reranked) Charniak et al. (2005) 92.4 91.6 91.8 91.0 GERMAN (NEGRA) Dubey (2005) Fl 76.3 80.1 - Petrov and Klein (2007) 80.8 80.7 80.1 CHINESE Chiang et al. (2002) 81.1 78.8 78.0 75.2 Petrov and Klein (2007) 86.9 85.7 84.8 81.9 Table 1: Our split-and-merge latent variable approach produces the best published parsing performance on many languages. language dependent modifications. Specifically, the same model hyperparameters (merging percentage and smoothing factor) were used in all experiments. Table 1 summarizes the results: automatically inducing latent structure is a technique that generalizes well across language boundaries and results in state of the art performance </context>
</contexts>
<marker>Dubey, 2005</marker>
<rawString>A. Dubey. 2005. What to do when lexicalization fails: parsing German with suffix analysis and smoothing. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<journal>ACL</journal>
<volume>96</volume>
<contexts>
<context position="9268" citStr="Goodman (1996)" startWordPosition="1452" endWordPosition="1453">nguistic like a parent annotation context, but which is formally just an integer. G therefore induces a derivation distribution over trees labeled with split symbols. This distribution in turn induces a parse distribution over (projected) trees with unsplit evaluation symbols. We have several choices of how to select a tree given these posterior distributions over trees. Since computing the most likely parse tree is NP-complete (Sima’an, 1992), we settle for an approximation that allows us to (partially) sum out the latent annotation. In Petrov and Klein (2007) we relate this approximation to Goodman (1996)’s labeled brackets algorithm applied to rules and to Matsuzaki et al. (2005)’s sentence specific variational approximation. This procedure is substantially superior to simply erasing the latent annotations from the the Viterbi derivation. 2.3 Results In Petrov and Klein (2007) we trained models for English, Chinese and German using the standard corpora and setups. We applied our latent variable model directly to each of the treebanks, without any &lt; 40 words LP all LP LR LR Parser ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov and Klein (2007) 90.7 90.5 90.2 89.9 ENGLISH (reranked) </context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. ACL ’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--613</pages>
<contexts>
<context position="1586" citStr="Johnson, 1998" startWordPosition="235" endWordPosition="236">demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein 33 and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000). We view treebank parsing as the search for an optimally refined grammar consistent with a coarse training treebank. As a result, we begin with the provided evaluation symbols (such as NP, VP, etc.) but split them based on the statistical patterns in the training trees. A manual approach might take the symbol NP and subdivide it into one subsymbol NP&amp;quot;S for subjects and another subsymbol NP&amp;quot;VP for objects. However, rather than devising linguistically motivated features or splits, we take</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24:613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL ’03,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1033" citStr="Klein and Manning (2003)" startWordPosition="140" endWordPosition="143"> procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein 33 and Manning, 2003) to full lexicaliz</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In ACL ’03, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="3641" citStr="Matsuzaki et al., 2005" startWordPosition="551" endWordPosition="554">e second best systems. Additionally, we investigate the patterns that are learned and show that the latent variable approach recovers linguistically interpretable phenomena. In our analysis, we pay particular attention to similarities and differences between Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics (b) Figure 1: (a) The original tree. (b) The binarized tree with latent variables. grammars learned from the two treebanks. 2 Latent Variable Parsing In latent variable parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006), we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). Th</context>
<context position="9345" citStr="Matsuzaki et al. (2005)" startWordPosition="1463" endWordPosition="1466">t an integer. G therefore induces a derivation distribution over trees labeled with split symbols. This distribution in turn induces a parse distribution over (projected) trees with unsplit evaluation symbols. We have several choices of how to select a tree given these posterior distributions over trees. Since computing the most likely parse tree is NP-complete (Sima’an, 1992), we settle for an approximation that allows us to (partially) sum out the latent annotation. In Petrov and Klein (2007) we relate this approximation to Goodman (1996)’s labeled brackets algorithm applied to rules and to Matsuzaki et al. (2005)’s sentence specific variational approximation. This procedure is substantially superior to simply erasing the latent annotations from the the Viterbi derivation. 2.3 Results In Petrov and Klein (2007) we trained models for English, Chinese and German using the standard corpora and setups. We applied our latent variable model directly to each of the treebanks, without any &lt; 40 words LP all LP LR LR Parser ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov and Klein (2007) 90.7 90.5 90.2 89.9 ENGLISH (reranked) Charniak et al. (2005) 92.4 91.6 91.8 91.0 GERMAN (NEGRA) Dubey (2005) Fl 76.</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL ’07.</booktitle>
<contexts>
<context position="4326" citStr="Petrov and Klein (2007)" startWordPosition="664" endWordPosition="667">ities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). The corresponding inference procedure is described in detail in Petrov and Klein (2007). The parser, code, and trained models are available for download at http://nlp.cs.berkeley.edu. 2.1 Learning Starting with a simple X-bar grammar, we use the Expectation-Maximization (EM) algorithm to learn a new grammar whose nonterminals are subsymbols of the original evaluation nonterminals. The X-bar grammar is created by binarizing the treebank trees; for each local tree rooted at an evaluation nonterminal X, we introduce a cascade of new nodes labeled X so that each node has at most two children, see Figure 1. This initialization is the absolute minimum starting grammar that distinguish</context>
<context position="7829" citStr="Petrov and Klein, 2007" startWordPosition="1224" endWordPosition="1227">fragmentation and overfitting, we also smooth our parameters along the split hierarchy. Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set. ROOT FRAG-x FRAG-x RB-x Not NP-x . NN Not DT this year (a) RB . NP FRAG NN-x year DT-x this .-x . 34 2.2 Inference At inference time, we want to use the learned grammar to efficiently and accurately compute a parse tree for a give sentence. For efficiency, we employ a hierarchical coarseto-fine inference scheme (Charniak et al., 1998; Charniak and Johnson, 2005; Petrov and Klein, 2007) which vastly improves inference time with no loss in test set accuracy. Our method considers the splitting history of the final grammar, projecting it onto its increasingly refined prior stages. For each such projection of the refined grammar, we estimate the projection’s parameters from the source PCFG itself (rather than the original treebank), using techniques for infinite tree distributions and iterated fixpoint equations. We then rapidly pre-parse with each refinement stage in sequence, such that any item X:[i, j] with sufficiently low posterior probability triggers the pruning of its fu</context>
<context position="9221" citStr="Petrov and Klein (2007)" startWordPosition="1443" endWordPosition="1446"> indicator of a subsymbol, which may encode something linguistic like a parent annotation context, but which is formally just an integer. G therefore induces a derivation distribution over trees labeled with split symbols. This distribution in turn induces a parse distribution over (projected) trees with unsplit evaluation symbols. We have several choices of how to select a tree given these posterior distributions over trees. Since computing the most likely parse tree is NP-complete (Sima’an, 1992), we settle for an approximation that allows us to (partially) sum out the latent annotation. In Petrov and Klein (2007) we relate this approximation to Goodman (1996)’s labeled brackets algorithm applied to rules and to Matsuzaki et al. (2005)’s sentence specific variational approximation. This procedure is substantially superior to simply erasing the latent annotations from the the Viterbi derivation. 2.3 Results In Petrov and Klein (2007) we trained models for English, Chinese and German using the standard corpora and setups. We applied our latent variable model directly to each of the treebanks, without any &lt; 40 words LP all LP LR LR Parser ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov and Klein</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL ’06.</booktitle>
<contexts>
<context position="3679" citStr="Petrov et al., 2006" startWordPosition="557" endWordPosition="560">investigate the patterns that are learned and show that the latent variable approach recovers linguistically interpretable phenomena. In our analysis, we pay particular attention to similarities and differences between Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics (b) Figure 1: (a) The original tree. (b) The binarized tree with latent variables. grammars learned from the two treebanks. 2 Latent Variable Parsing In latent variable parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006), we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). The corresponding inference procedure is</context>
<context position="5032" citStr="Petrov et al. (2006)" startWordPosition="775" endWordPosition="778">keley.edu. 2.1 Learning Starting with a simple X-bar grammar, we use the Expectation-Maximization (EM) algorithm to learn a new grammar whose nonterminals are subsymbols of the original evaluation nonterminals. The X-bar grammar is created by binarizing the treebank trees; for each local tree rooted at an evaluation nonterminal X, we introduce a cascade of new nodes labeled X so that each node has at most two children, see Figure 1. This initialization is the absolute minimum starting grammar that distinguishes the evaluation nonterminals (and maintains separate grammars for each of them). In Petrov et al. (2006) we show that a hierarchical split-and-merge strategy learns compact but accurate grammars, allocating subsymbols adaptively where they are most effective. Beginning with the baseline grammar, we repeatedly split and re-train the grammar. In each iteration, we initialize EM with the results of the previous round’s grammar, splitting every previous symbol in two and adding a small amount of randomness (1%) to break the symmetry between the various subsymbols. Note that we split all nonterminal symbols, including the part-ofspeech categories. While creating more latent annotations can increase a</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Prescher</author>
</authors>
<title>Inducing head-driven PCFGs with latent heads: Refining a tree-bank grammar for parsing.</title>
<date>2005</date>
<booktitle>In ECML’05.</booktitle>
<contexts>
<context position="3657" citStr="Prescher, 2005" startWordPosition="555" endWordPosition="556">dditionally, we investigate the patterns that are learned and show that the latent variable approach recovers linguistically interpretable phenomena. In our analysis, we pay particular attention to similarities and differences between Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics (b) Figure 1: (a) The original tree. (b) The binarized tree with latent variables. grammars learned from the two treebanks. 2 Latent Variable Parsing In latent variable parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006), we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). The corresponding </context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>D. Prescher. 2005. Inducing head-driven PCFGs with latent heads: Refining a tree-bank grammar for parsing. In ECML’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
</authors>
<title>Computatoinal complexity of probabilistic disambiguation.</title>
<date>1992</date>
<journal>Grammars,</journal>
<pages>5--125</pages>
<marker>Sima’an, 1992</marker>
<rawString>K. Sima’an. 1992. Computatoinal complexity of probabilistic disambiguation. Grammars, 5:125–151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>