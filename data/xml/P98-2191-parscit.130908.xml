<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<author confidence="0.7117475">
Maximum Entropy Model Learning of the Translation Rules
Kengo Sato and Masakazu Nakanishi
</author>
<affiliation confidence="0.9977245">
Department of Computer Science
Keio University
</affiliation>
<address confidence="0.83208">
3-14-1, Hiyoshi, Kohoku, Yokohama 223-8522, Japan
</address>
<email confidence="0.999511">
e-mail: {satoken,cz1}@nak. ics.keio . ac . jp
</email>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997789375">
This paper proposes a learning method of
translation rules from parallel corpora. This
method applies the maximum entropy prin-
ciple to a probabilistic model of translation
rules. First, we define feature functions
which express statistical properties of this
model. Next, in order to optimize the model,
the system iterates following steps: (1) se-
lects a feature function which maximizes log-
likelihood, and (2) adds this function to the
model incrementally. As computational cost
associated with this model is too expensive,
we propose several methods to suppress the
overhead in order to realize the system. The
result shows that it attained 69.54% recall
rate.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867882352941">
A statistical natural language modeling can
be viewed as estimating a combinational dis-
tribution X x Y -4 [0, 1] using training data
yi), • • • , (xT, YT) E X. x Y observed in
corpora. For this topic, Baum (1972) pro-
posed EM algorithm, which was basis of
Forward-Backward algorithm for the hidden
Markov model (HMM) and Inside-Outside
algorithm (Lafferty, 1993) for the probabilis-
tic context free grammar (PCFG). However,
these methods have problems such as in-
creasing optimization costs which is due to
a lot of parameters. Therefore, estimating a
natural language model based on the max-
imum entropy (ME) method (Pietra et al.,
1995; Berger et al., 1996) has been high-
lighted recently.
On the other hand, dictionaries for multi-
lingual natural language processing such as
the machine translation has been made by
human hand usually. However, since this
work requires a great deal of labor and it
is difficult to keep description of dictionar-
ies consistent, the researches of automatical
dictionaries making for machine translation
(translation rules) from corpora become ac-
tive recently (Kay and R8schesen, 1993; Kaji
and Aizono, 1996).
In this paper, we notice that estimating
a language model based on ME method is
suitable for learning the translation rules,
and propose several methods to resolve prob-
lems in adapting ME method to learning the
translation rules.
</bodyText>
<sectionHeader confidence="0.941311" genericHeader="method">
2 Problem Setting
</sectionHeader>
<bodyText confidence="0.998198">
If there exist (x1, y1), (XT, YT) EX xY
such that each xi is translated into yi in
the parallel corpora X, Y, then its empiri-
cal probability distribution 13 obtained from
observed training data is defined by:
</bodyText>
<equation confidence="0.9991695">
c(x, y)
P(x, Y) = Es,v c(x, y)
</equation>
<bodyText confidence="0.999063166666667">
where c(x, y) is the number of times that x
is translated into y in the training data.
However, since it is difficult to observe
translating between words actually, c(x, y) is
approximated with equation (2) for sentence
aligned parallel corpora.
</bodyText>
<equation confidence="0.9434785">
c(x, y) = E et(x&apos; Y) Xi (2)
IllYii
</equation>
<bodyText confidence="0.997365333333333">
where Xi is i-th sentence in X. We denote
that sentence Xi is translated into sentence
Yi in aligned parallel corpora. And c(x, y)
</bodyText>
<equation confidence="0.941403">
(1)
</equation>
<page confidence="0.933268">
1171
</page>
<construct confidence="0.852053666666667">
is the number of times that x and y appear
in the i-th sentence.
Our task is to learn the translation rules
by estimating probability distribution p(y1x)
that x E X is translated into y E Y from
f)(x, y) given above.
</construct>
<sectionHeader confidence="0.98723" genericHeader="method">
3 Maximum Entropy Method
</sectionHeader>
<subsectionHeader confidence="0.999913">
3.1 Feature Function
</subsectionHeader>
<bodyText confidence="0.978503333333333">
We define binary-valued indicator function
f :X xY {0,1} which divide X x Y
into two subsets. This is called feature func-
tion, which expresses statistical properties of
a language model.
The expected value of f with respected to
</bodyText>
<equation confidence="0.890693666666667">
/3(x, y) is defined such as:
= EP(x, y)i(x, y) (3)
X,y
</equation>
<bodyText confidence="0.9927222">
Thus training data are summarized as the
expected value of feature function f.
The expected value of a feature function
f with respected to p(y1x) which we would
like to estimate is defined such as:
</bodyText>
<equation confidence="0.999768">
P(i) = E3(x)P(Y1x)./(x, Y) (4)
</equation>
<bodyText confidence="0.99979225">
where 23(x) is the empirical probability dis-
tribution on X. Then, the model which we
would like to estimate is under constraint to
satisfy an equation such as:
</bodyText>
<equation confidence="0.999505">
P(i) = /3(i) (5)
</equation>
<bodyText confidence="0.656129">
This is called the constraint equation.
</bodyText>
<subsectionHeader confidence="0.999657">
3.2 Maximum Entropy Principle
</subsectionHeader>
<bodyText confidence="0.93720025">
When there are feature functions fi(i E
{1,2, , n}) which are important to model-
ing processes, the distribution p we estimate
should be included in a set of distributions
defined such as:
C = fp E P I p(f2) =p(f) for i E {1, 2, ... , n}}
(6)
where P is a set of all possible distributions
on X x Y.
For the distribution p, there is no assump-
tion except equation (6), so it is reason-
able that the most uniform distribution is
the most suitable for the training corpora.
The conditional entropy defined in equa-
tion (7) is used as the mathematical measure
of the uniformity of a conditional probability
</bodyText>
<equation confidence="0.922933666666667">
P(Yix).
1 1 (P) = — 23(x)p(y1x) logp(y1x) (7)
x,y
</equation>
<bodyText confidence="0.975242">
That is, the model p* which maximizes the
entropy H should be selected from C.
</bodyText>
<equation confidence="0.991677">
p* argmax H(p) (8)
pEC
</equation>
<bodyText confidence="0.6578275">
This heuristic is called the maximum entropy
principle.
</bodyText>
<subsectionHeader confidence="0.996513">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999447375">
In simple cases, we can find the solution
to the equation (8) analytically. Unfortu-
nately, there is no analytical solution in gen-
eral cases, and we need a numerical algo-
rithm to find the solution.
By applying the Lagrange multiplier to
equation (7), we can introduce the paramet-
ric form of p.
</bodyText>
<equation confidence="0.99473675">
1
p),(y1x) = exp
Z(x) =E exp (E Ai fi(x , y))
Y
</equation>
<bodyText confidence="0.9889134">
where each Ai is the parameter for the fea-
ture ft. pA is known as Gibbs distribution.
Then, to solve p* C in equation (8) is
equivalent to solve A* that maximize the log-
likelihood:
</bodyText>
<equation confidence="0.842975666666667">
W (A) = — Ei3(x) log ZA (x)
A*= argmax t11(A)
A
</equation>
<bodyText confidence="0.706572666666667">
Such A* can be solved by one of the nu-
merical algorithm called the Improved Itera-
tive Scaling Algorithm (Berger et al., 1996).
</bodyText>
<listItem confidence="0.8686035">
1. Start with Ai = 0 for all i E {1,2, , n}
2. Do for each i E {1,2, , n}:
</listItem>
<page confidence="0.909001">
1172
</page>
<figure confidence="0.415048">
(a) Let AAi be the solution to
Efi(x)pcylxvi(x, y) exp (AAif# (x, y)) =23(ii)
(11)
</figure>
<bodyText confidence="0.861129">
where f*(x,y)=E&apos;L fi(x,y)
</bodyText>
<listItem confidence="0.871235">
(b) Update the value of Ai according to:
3. Go to step 2 if not all the Ai have con-
verged
</listItem>
<bodyText confidence="0.9634915">
To solve LA i in the step (2a), the Newton&apos;s
method is applied to equation (11).
</bodyText>
<subsectionHeader confidence="0.960474">
3.4 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999610583333333">
In general cases, there exist a large collec-
tion of candidate features, and because
of the limit of machine resources, we can-
not expect to obtain all PM estimated in
real-life. However, the Maximum Entropy
Principle does not explicitly state how to se-
lect those particular constraints. We build a
subset S C .F incrementally by iterating to
adjoin a feature f E .7 which maximizes log-
likelihood of the model to S. This algorithm
is called the Basic Feature Selection (Berger
et al., 1996).
</bodyText>
<listItem confidence="0.999714166666667">
1. Start with S = 0
2. Do for each candidate feature f E
Compute the model psuf using Improve
Iterative Scaling Algorithm and the
gain in the log-likelihood from adding
this feature
3. Check the termination condition
4. Select the feature f with maximal gain
5. Adjoin f to S
6. Compute Ps using Improve Iterative Al-
gorithm
7. Go to Step 2
</listItem>
<sectionHeader confidence="0.986652" genericHeader="method">
4 Maximum Entropy Model
</sectionHeader>
<subsectionHeader confidence="0.957202">
Learning of the Translation
Rules
</subsectionHeader>
<bodyText confidence="0.999877333333333">
The art of modeling with the maximum en-
tropy method is to define an informative
set of computationally feasible feature func-
tions. In this section, we define two models
of feature functions for learning the transla-
tion rules.
</bodyText>
<subsectionHeader confidence="0.750528">
Model 1: Co-occurrence Information
</subsectionHeader>
<bodyText confidence="0.988022333333333">
The first model is defined with co-occurrence
information between words appeared in the
corpus X.
</bodyText>
<equation confidence="0.998065">
f 1 (x EW(d,w)) (12)
ft&amp;quot;(x&apos; = 0 (otherwise)
</equation>
<bodyText confidence="0.9995796">
where W(d,w) is a set of words which ap-
peared within d words from w E X (in our
experiments, d = 5). fw(x, y) expresses the
information on w for predicting that x is
translated into y (Figure 1).
</bodyText>
<figureCaption confidence="0.729698">
Figure 1: co-occurance information
Model 2: Morphological Information
</figureCaption>
<bodyText confidence="0.9914715">
The second model is defined with morpho-
logical information such as part-of-speech.
</bodyText>
<equation confidence="0.9523914">
ft,(x, Y) = 1 1 0 (otherwise) (13)
1
POS(x) = t
and
POS(y) = s
</equation>
<bodyText confidence="0.9998055">
where POS(x) is a part-of-speech tag for x.
ft,u(x, y) expresses the information on part-
of-speech t, s for predicting that x is trans-
lated into y (Figure 2). If part-of-speech tag-
</bodyText>
<figure confidence="0.943574909090909">
predictive
power
translation rule
X
1173
predictive
power
rail- translation rule
s
X
y
</figure>
<figureCaption confidence="0.999938">
Figure 2: morphological information
</figureCaption>
<bodyText confidence="0.999785666666667">
gers for each language work extremely ac-
curate, then these feature functions can be
generated automatically.
</bodyText>
<sectionHeader confidence="0.99826" genericHeader="method">
5 Implementation
</sectionHeader>
<bodyText confidence="0.99877952631579">
Computational cost associated with the
model described above is too expensive to
realize the system for learning the transla-
tion rules. We propose several methods to
suppress the overhead.
An estimated probability pA(y1x) for a pair
of (x, y) E X x Y which has not been ob-
served as the sample data in the parallel
corpora X, Y should be kept lower. Ac-
cording to equation (9), we can allow to let
fi(x, y) = 0 (for all i E {1, , n}) for non-
observed (x, y). Therefore, we will accept
observed (x, y) only instead of all possible
(x, y) in summation in equation (11), so that
pA(y1x) can be calculated much more effi-
ciently.
Suppose that a set of (x, y) such that each
member activates a feature function f is de-
fined by:
</bodyText>
<equation confidence="0.979364">
D(f) = {(x , y) E X x Ylf (x,y) = 1} (14)
</equation>
<bodyText confidence="0.99973725">
Shirai et al. (1996) showed that if D(f2) and
D(f3) were exclusive to each other, that is
D(f2) n D(fi) = 0, then Ai and Ai could
be estimated independently. Therefore, we
can split a set of candidate feature functions
.F into several exclusive subsets, and calcu-
late pA(ylx) more efficiently by estimating on
each subset independently.
</bodyText>
<sectionHeader confidence="0.982997" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9995981875">
As the training corpora, we used 6,057 pairs
of sentences included in Kodansya Japanese-
English Dictionary, a machine-readable dic-
tionary made by the Electrotechnical Lab-
oratory. By applying morphological anal-
ysis for the corpora, each word was trans-
formed to the infinitive form. We excluded
words which appeared below 3 times or over
1,000 times from the target of learning. Con-
sequently, our target for the experiments
included 1,375 English words and 1,195
Japanese words, and we prepared 1,375 fea-
ture functions for model 1 and 2,744 for
model 2 (56 part-of-speech for English and
49 part-of-speech for Japanese).
We tried to learn the translation rules
from English to Japanese. We had two ex-
periments: one of model 1 as the set of fea-
ture functions, and one of model 1 + 2. For
each experiment, 500 feature functions were
selected according to the feature selection
algorithm described in section 3.4, and we
calculated p(y1x) in equation (9), that is,
the probability that English word x is trans-
lated into Japanese word y. For each English
word, all Japanese word were ordered by es-
timated probability p(y1x), and we evaluated
the recall rates by comparing the dictionary.
Table 1 shows the recall rates for each ex-
periment. The numbers for 13(x, y) are the
1st , 3rd ,-- 10th
ii(x, y) 44.55% 53.47% 58.42%
model 1 41.58% 63.37% 76.24%
model 1 + 2 58.29% 69.54% 80.13%
recall rates when the empirical probability
defined by equation (1) was used instead of
the estimated probability. It is showed that
the model 1 + 2 attains higher recall rates
than the model 1 and .73(x, y).
Figure 3 shows the log-likelihood for each
model plotted by the number of feature func-
tions in the feature selection algorithm. No-
tice that the log-likelihood for the model 1+2
is always higher than the model 1.
Thus, the model 1 + 2 is more effective
than the model 1 for learning the translation
rules.
However, the result shows that the recall
</bodyText>
<page confidence="0.947793">
1174
</page>
<table confidence="0.7449466">
Ino4141 1 -
.003•1142
41,1
50 103 150 200 250 X4) 350 400 450 500
08 number 011001840
</table>
<figureCaption confidence="0.99823">
Figure 3: log-likelihood
</figureCaption>
<bodyText confidence="0.9867985">
rates of the &apos;1st&apos; for all models are not fa-
vorable. We consider that it is the reason
for this to assume word-to-word translation
rules implicitly.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999991857142857">
We have described an approach to learn the
translation rules from parallel corpora based
on the maximum entropy method. As fea-
ture functions, we have defined two mod-
els, one with co-occurrence information and
the other with morphological information.
As computational cost associated with this
method is too expensive, we have proposed
several methods to suppress the overhead in
order to realize the system. We had experi-
ments for each model of features, and the re-
sult showed the effectiveness of this method,
especially for the model of features with co-
occurrence and morphological information.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99988375">
We would like to thank the Electrotechni-
cal Laboratory for giving us the machine-
readable dictionary which was used as the
training data.
</bodyText>
<sectionHeader confidence="0.981601" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.839572888888889">
L. E. Baum. 1972. An inequality and associ-
ated maximumization technique in statis-
tical estimation of probabilistic functions
of a markov process. Inequalities, 3:1-8.
Adam L. Berger, Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A max-
imum entropy approach to natural lan-
guage processing. Computational Linguis-
tics, 22 (1) :39-71.
</bodyText>
<reference confidence="0.967090772727273">
Hiroyuki Kaji and Toshiko Aizono. 1996.
Extracting word correspondences from
bilingual corpora based on word co-
occurrence information. In Proceedings
of the 16th International Conference on
Computational Linguistics, pages 23-28.
M. Kay and M. Roschesen. 1993. Text
translation alignment. Computational
Linguistics, 19(1):121-142.
J. D. Lafferty. 1993. A derivation of the
inside-outside algorithm from the EM al-
gorithm. IBM Research Report. IBM T.J.
Watson Research Center.
Stephen Della Pietra, Vincent Della Pietra,
and John Lafferty. 1995. Inducing fea-
tures of random fields. Technical Report
CMU-CS-95-144, Carnegie Mellon Univer-
sity, May.
Adwait Ratnaparkhi. 1997. A linear ob-
served time statistical parser based on
maximum entropy models. In Proceedings
of Second Conference On Empirical Meth-
ods in Natural Language Processing.
Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997. A maximum entropy approach to
identifying sentence boundaries. In Pro-
ceedings of the 5th Applied Natural Lan-
guage Processing Conference.
Ronald Rosenfeld. 1996. A maximum en-
tropy approach to adaptive statistical lan-
guage modeling. Computer, Speech and
Language, (10):187-228.
Kiyoaki Shirai, Kentaro Inui, Takenobu
Tokunaga, and Hozumi Tanaka. 1996.
A maximum entropy model for estimat-
ing lexical bigrams (in Japanese). In SIG
Notes of the Information Processing Soci-
ety of Japan, number 96—NL-116.
Takehito Utsuro, Takashi Miyata, and Yuji
Matsumoto. 1997. Maximum entropy
model learning of subcategorizatoin pref-
erence. In Proceedings of the 5th Work-
shop on Very Large Corpora, pages 246-
260, August.
</reference>
<page confidence="0.993601">
1175
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.653854">
<title confidence="0.999753">Maximum Entropy Model Learning of the Translation Rules</title>
<author confidence="0.999957">Sato Nakanishi</author>
<affiliation confidence="0.9999295">Department of Computer Science Keio University</affiliation>
<address confidence="0.997909">3-14-1, Hiyoshi, Kohoku, Yokohama 223-8522, Japan</address>
<email confidence="0.689083">ac.jp</email>
<abstract confidence="0.996942764705882">This paper proposes a learning method of translation rules from parallel corpora. This method applies the maximum entropy principle to a probabilistic model of translation rules. First, we define feature functions which express statistical properties of this model. Next, in order to optimize the model, the system iterates following steps: (1) selects a feature function which maximizes loglikelihood, and (2) adds this function to the model incrementally. As computational cost associated with this model is too expensive, we propose several methods to suppress the overhead in order to realize the system. The result shows that it attained 69.54% recall rate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiroyuki Kaji</author>
<author>Toshiko Aizono</author>
</authors>
<title>Extracting word correspondences from bilingual corpora based on word cooccurrence information.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>23--28</pages>
<contexts>
<context position="2055" citStr="Kaji and Aizono, 1996" startWordPosition="320" endWordPosition="323">parameters. Therefore, estimating a natural language model based on the maximum entropy (ME) method (Pietra et al., 1995; Berger et al., 1996) has been highlighted recently. On the other hand, dictionaries for multilingual natural language processing such as the machine translation has been made by human hand usually. However, since this work requires a great deal of labor and it is difficult to keep description of dictionaries consistent, the researches of automatical dictionaries making for machine translation (translation rules) from corpora become active recently (Kay and R8schesen, 1993; Kaji and Aizono, 1996). In this paper, we notice that estimating a language model based on ME method is suitable for learning the translation rules, and propose several methods to resolve problems in adapting ME method to learning the translation rules. 2 Problem Setting If there exist (x1, y1), (XT, YT) EX xY such that each xi is translated into yi in the parallel corpora X, Y, then its empirical probability distribution 13 obtained from observed training data is defined by: c(x, y) P(x, Y) = Es,v c(x, y) where c(x, y) is the number of times that x is translated into y in the training data. However, since it is di</context>
</contexts>
<marker>Kaji, Aizono, 1996</marker>
<rawString>Hiroyuki Kaji and Toshiko Aizono. 1996. Extracting word correspondences from bilingual corpora based on word cooccurrence information. In Proceedings of the 16th International Conference on Computational Linguistics, pages 23-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
<author>M Roschesen</author>
</authors>
<title>Text translation alignment.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<marker>Kay, Roschesen, 1993</marker>
<rawString>M. Kay and M. Roschesen. 1993. Text translation alignment. Computational Linguistics, 19(1):121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Lafferty</author>
</authors>
<title>A derivation of the inside-outside algorithm from the EM algorithm.</title>
<date>1993</date>
<journal>IBM Research Report. IBM T.J. Watson Research Center.</journal>
<contexts>
<context position="1281" citStr="Lafferty, 1993" startWordPosition="199" endWordPosition="200">ction to the model incrementally. As computational cost associated with this model is too expensive, we propose several methods to suppress the overhead in order to realize the system. The result shows that it attained 69.54% recall rate. 1 Introduction A statistical natural language modeling can be viewed as estimating a combinational distribution X x Y -4 [0, 1] using training data yi), • • • , (xT, YT) E X. x Y observed in corpora. For this topic, Baum (1972) proposed EM algorithm, which was basis of Forward-Backward algorithm for the hidden Markov model (HMM) and Inside-Outside algorithm (Lafferty, 1993) for the probabilistic context free grammar (PCFG). However, these methods have problems such as increasing optimization costs which is due to a lot of parameters. Therefore, estimating a natural language model based on the maximum entropy (ME) method (Pietra et al., 1995; Berger et al., 1996) has been highlighted recently. On the other hand, dictionaries for multilingual natural language processing such as the machine translation has been made by human hand usually. However, since this work requires a great deal of labor and it is difficult to keep description of dictionaries consistent, the </context>
</contexts>
<marker>Lafferty, 1993</marker>
<rawString>J. D. Lafferty. 1993. A derivation of the inside-outside algorithm from the EM algorithm. IBM Research Report. IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-144,</tech>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="1553" citStr="Pietra et al., 1995" startWordPosition="242" endWordPosition="245">natural language modeling can be viewed as estimating a combinational distribution X x Y -4 [0, 1] using training data yi), • • • , (xT, YT) E X. x Y observed in corpora. For this topic, Baum (1972) proposed EM algorithm, which was basis of Forward-Backward algorithm for the hidden Markov model (HMM) and Inside-Outside algorithm (Lafferty, 1993) for the probabilistic context free grammar (PCFG). However, these methods have problems such as increasing optimization costs which is due to a lot of parameters. Therefore, estimating a natural language model based on the maximum entropy (ME) method (Pietra et al., 1995; Berger et al., 1996) has been highlighted recently. On the other hand, dictionaries for multilingual natural language processing such as the machine translation has been made by human hand usually. However, since this work requires a great deal of labor and it is difficult to keep description of dictionaries consistent, the researches of automatical dictionaries making for machine translation (translation rules) from corpora become active recently (Kay and R8schesen, 1993; Kaji and Aizono, 1996). In this paper, we notice that estimating a language model based on ME method is suitable for lea</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1995. Inducing features of random fields. Technical Report CMU-CS-95-144, Carnegie Mellon University, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of Second Conference On Empirical Methods in Natural Language Processing.</booktitle>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of Second Conference On Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Applied Natural Language Processing Conference.</booktitle>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the 5th Applied Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187</pages>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language, (10):187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
<author>Kentaro Inui</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A maximum entropy model for estimating lexical bigrams (in Japanese).</title>
<date>1996</date>
<booktitle>In SIG Notes of the Information Processing Society of Japan, number 96—NL-116.</booktitle>
<contexts>
<context position="8814" citStr="Shirai et al. (1996)" startWordPosition="1559" endWordPosition="1562">s to suppress the overhead. An estimated probability pA(y1x) for a pair of (x, y) E X x Y which has not been observed as the sample data in the parallel corpora X, Y should be kept lower. According to equation (9), we can allow to let fi(x, y) = 0 (for all i E {1, , n}) for nonobserved (x, y). Therefore, we will accept observed (x, y) only instead of all possible (x, y) in summation in equation (11), so that pA(y1x) can be calculated much more efficiently. Suppose that a set of (x, y) such that each member activates a feature function f is defined by: D(f) = {(x , y) E X x Ylf (x,y) = 1} (14) Shirai et al. (1996) showed that if D(f2) and D(f3) were exclusive to each other, that is D(f2) n D(fi) = 0, then Ai and Ai could be estimated independently. Therefore, we can split a set of candidate feature functions .F into several exclusive subsets, and calculate pA(ylx) more efficiently by estimating on each subset independently. 6 Experiments and Results As the training corpora, we used 6,057 pairs of sentences included in Kodansya JapaneseEnglish Dictionary, a machine-readable dictionary made by the Electrotechnical Laboratory. By applying morphological analysis for the corpora, each word was transformed t</context>
</contexts>
<marker>Shirai, Inui, Tokunaga, Tanaka, 1996</marker>
<rawString>Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1996. A maximum entropy model for estimating lexical bigrams (in Japanese). In SIG Notes of the Information Processing Society of Japan, number 96—NL-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takehito Utsuro</author>
<author>Takashi Miyata</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Maximum entropy model learning of subcategorizatoin preference.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Workshop on Very Large Corpora,</booktitle>
<pages>246--260</pages>
<marker>Utsuro, Miyata, Matsumoto, 1997</marker>
<rawString>Takehito Utsuro, Takashi Miyata, and Yuji Matsumoto. 1997. Maximum entropy model learning of subcategorizatoin preference. In Proceedings of the 5th Workshop on Very Large Corpora, pages 246-260, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>