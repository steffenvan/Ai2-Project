<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026525">
<title confidence="0.996331">
Latent Semantic Grammar Induction:
Context, Projectivity, and Prior Distributions
</title>
<author confidence="0.9987">
Andrew M. Olney
</author>
<affiliation confidence="0.997861">
Institute for Intelligent Systems
University of Memphis
</affiliation>
<address confidence="0.881271">
Memphis, TN 38152
</address>
<email confidence="0.998224">
aolney@memphis.edu
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999662857142857">
This paper presents latent semantic gram-
mars for the unsupervised induction of
English grammar. Latent semantic gram-
mars were induced by applying singu-
lar value decomposition to n-gram by
context-feature matrices. Parsing was
used to evaluate performance. Exper-
iments with context, projectivity, and
prior distributions show the relative per-
formance effects of these kinds of prior
knowledge. Results show that prior dis-
tributions, projectivity, and part of speech
information are not necessary to beat the
right branching baseline.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999826034482759">
Unsupervised grammar induction (UGI) generates a
grammar from raw text. It is an interesting problem
both theoretically and practically. Theoretically, it
connects to the linguistics debate on innate knowl-
edge (Chomsky, 1957). Practically, it has the po-
tential to supersede techniques requiring structured
text, like treebanks. Finding structure in text with
little or no prior knowledge is therefore a fundamen-
tal issue in the study of language.
However, UGI is still a largely unsolved problem.
Recent work (Klein and Manning, 2002; Klein and
Manning, 2004) has renewed interest by using a UGI
model to parse sentences from the Wall Street Jour-
nal section of the Penn Treebank (WSJ). These pars-
ing results are exciting because they demonstrate
real-world applicability to English UGI. While other
contemporary research in this area is promising, the
case for real-world English UGI has not been as
convincingly made (van Zaanen, 2000; Solan et al.,
2005).
This paper weaves together two threads of in-
quiry. The first thread is latent semantics, which
have not been previously used in UGI. The second
thread is dependency-based UGI, used by Klein and
Manning (2004), which nicely dovetails with our se-
mantic approach. The combination of these threads
allows some exploration of what characteristics are
sufficient for UGI and what characteristics are nec-
essary.
</bodyText>
<sectionHeader confidence="0.965358" genericHeader="method">
2 Latent semantics
</sectionHeader>
<bodyText confidence="0.999928111111111">
Previous work has focused on syntax to the exclu-
sion of semantics (Brill and Marcus, 1992; van Zaa-
nen, 2000; Klein and Manning, 2002; Paskin, 2001;
Klein and Manning, 2004; Solan et al., 2005). How-
ever, results from the speech recognition commu-
nity show that the inclusion of latent semantic infor-
mation can enhance the performance of their mod-
els (Coccaro and Jurafsky, 1998; Bellegarda, 2000;
Deng and Khudanpur, 2003). Using latent semantic
information to improve UGI is therefore both novel
and relevant.
The latent semantic information used by the
speech recognition community above is produced
by latent semantic analysis (LSA), also known as
latent semantic indexing (Deerwester et al., 1990;
Landauer et al., 1998). LSA creates a semantic rep-
resentation of both words and collections of words
in a vector space, using a two part process. First,
</bodyText>
<page confidence="0.992655">
45
</page>
<note confidence="0.8724615">
TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 45–52,
Rochester, April 2007 (c 2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.994752510204082">
a term by document matrix is created in which the
frequency of word wz in document dj is the value
of cell czj. Filters may be applied during this pro-
cess which eliminate undesired terms, e.g. common
words. Weighting may also be applied to decrease
the contributions of frequent words (Dumais, 1991).
Secondly, singular value decomposition (SVD) is
applied to the term by document matrix. The re-
sulting matrix decomposition has the property that
the removal of higher-order dimensions creates an
optimal reduced representation of the original ma-
trix in the least squares sense (Berry et al., 1995).
Therefore, SVD performs a kind of dimensionality
reduction such that words appearing in different doc-
uments can acquire similar row vector representa-
tions (Landauer and Dumais, 1997). Words can be
compared by taking the cosine of their correspond-
ing row vectors. Collections of words can likewise
be compared by first adding the corresponding row
vectors in each collection, then taking the cosine be-
tween the two collection vectors.
A stumbling block to incorporating LSA into UGI
is that grammars are inherently ordered but LSA is
not. LSA is unordered because the sum of vectors is
the same regardless of the order in which they were
added. The incorporation of word order into LSA
has never been successfully carried out before, al-
though there have been attempts to apply word or-
der post-hoc to LSA (Wiemer-Hastings and Zipitria,
2001). A straightforward notion of incorporating
word order into LSA is to use n-grams instead of in-
dividual words. In this way a unigram, bigram, and
trigram would each have an atomic vector represen-
tation and be directly comparable.
It may seem counterintuitive that such an n-gram
scheme has never been used in conjunction with
LSA. Simple as this scheme may be, it quickly falls
prey to memory limitations of modern day comput-
ers for computing the SVD. The standard for com-
puting the SVD in the NLP sphere is Berry (1992)’s
SVDPACK, whose single vector Lanczos recursion
method with re-orthogonalization was incorporated
into the BellCore LSI tools. Subsequently, either
SVDPACK or the LSI tools were used by the ma-
jority of researchers in this area (Sch¨utze, 1995;
Landauer and Dumais, 1997; Landauer et al., 1998;
Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bel-
legarda, 2000; Deng and Khudanpur, 2003). Using
John likes string cheese.
</bodyText>
<figureCaption confidence="0.998994">
Figure 1: A Dependency Graph
</figureCaption>
<bodyText confidence="0.9996365">
the equation reported in Larsen (1998), a standard
orthogonal SVD of a unigram/bigram by sentence
matrix of the LSA Touchstone Applied Science As-
sociates Corpus (Landauer et al., 1998) requires over
60 gigabytes of random access memory. This esti-
mate is prohibitive for all but current supercomput-
ers.
However, it is possible to use a non-orthogonal
SVD approach with significant memory savings
(Cullum and Willoughby, 2002). A non-orthogonal
approach creates the same matrix decomposition as
traditional approaches, but the resulting memory
savings allow dramatically larger matrix decompo-
sitions. Thus a non-orthongonal SVD approach is
key to the inclusion of ordered latent semantics into
our UGI model.
</bodyText>
<sectionHeader confidence="0.990922" genericHeader="method">
3 Dependency grammars
</sectionHeader>
<bodyText confidence="0.99975475">
Dependency structures are an ideal grammar repre-
sentation for evaluating UGI. Because dependency
structures have no higher order nodes, e.g. NP, their
evaluation is simple: one may compare with a ref-
erence parse and count the proportion of correct de-
pendencies. For example, Figure 1 has three depen-
dencies {( John, likes ), ( cheese, likes ), ( string,
cheese ) }, so the trial parse {( John, likes ), ( string,
likes), ( cheese, string )} has 1/3 directed dependen-
cies correct and 2/3 undirected dependencies cor-
rect. This metric avoids the biases created by brack-
eting, where over-generation or undergeneration of
brackets may cloud actual performance (Carroll et
al., 2003). Dependencies are equivalent with lexical-
ized trees (see Figures 1 and 2) so long as the depen-
dencies are projective. Dependencies are projective
when all heads and their dependents are a contigu-
ous sequence.
Dependencies have been used for UGI before with
mixed success (Paskin, 2001; Klein and Manning,
2004). Paskin (2001) created a projective model us-
ing words, and he evaluated on WSJ. Although he
reported beating the random baseline for that task,
both Klein and Manning (2004) and we have repli-
</bodyText>
<page confidence="0.998871">
46
</page>
<figure confidence="0.930079">
Slikes
NPJohn VPlikes
John likes NPcheese
string cheese
</figure>
<figureCaption confidence="0.999474">
Figure 2: A Lexicalized Tree
</figureCaption>
<bodyText confidence="0.999719">
cated the random baseline above Paskin’s results.
Klein and Manning (2004), on the other hand, have
handily beaten a random baseline using a projective
model over part of speech tags and evaluating on a
subset of WSJ, WSJ10.
</bodyText>
<sectionHeader confidence="0.993198" genericHeader="method">
4 Unanswered questions
</sectionHeader>
<bodyText confidence="0.9999765">
There are several unanswered questions in
dependency-based English UGI. Some of these
may be motivated from the Klein and Manning
(2004) model, while others may be motivated
from research efforts outside the UGI community.
Altogether, these questions address what kinds
of prior knowledge are, or are not necessary for
successful UGI.
</bodyText>
<subsectionHeader confidence="0.999533">
4.1 Parts of speech
</subsectionHeader>
<bodyText confidence="0.99611025">
Klein and Manning (2004) used part of speech tags
as basic elements instead of words. Although this
move can be motivated on data sparsity grounds, it
is somewhat at odds with the lexicalized nature of
dependency grammars. Since Paskin (2001)’s previ-
ous attempt using words as basic elements was un-
successful, it is not clear whether parts of speech are
necessary prior knowledge in this context.
</bodyText>
<subsectionHeader confidence="0.989169">
4.2 Projectivity
</subsectionHeader>
<bodyText confidence="0.999750444444444">
Projectivity is an additional constraint that may not
be necessary for successful UGI. English is a projec-
tive language, but other languages, such as Bulgar-
ian, are not (Pericliev and Ilarionov, 1986). Nonpro-
jective UGI has not previously been studied, and it
is not clear how important projectivity assumptions
are to English UGI. Figure 3 gives an example of a
nonprojective construction: not all heads and their
dependents are a contiguous sequence.
</bodyText>
<figure confidence="0.554747">
Words Distant
</figure>
<figureCaption confidence="0.994389">
Figure 4: Distance Between Dependents in WSJ10
</figureCaption>
<subsectionHeader confidence="0.99455">
4.3 Context
</subsectionHeader>
<bodyText confidence="0.999987882352941">
The core of several UGI approaches is distributional
analysis (Brill and Marcus, 1992; van Zaanen, 2000;
Klein and Manning, 2002; Paskin, 2001; Klein and
Manning, 2004; Solan et al., 2005). The key idea in
such distributional analysis is that the function of a
word may be known if it can be substituted for an-
other word (Harris, 1954). If so, both words have the
same function. Substitutability must be defined over
a context. In UGI, this context has typically been the
preceding and following words of the target word.
However, this notion of context has an implicit as-
sumption of word order. This assumption is true for
English, but is not true for other languages such as
Latin. Therefore, it is not clear how dependent En-
glish UGI is on local linear context, e.g. preceding
and following words, or whether an unordered no-
tion of context would also be effective.
</bodyText>
<subsectionHeader confidence="0.996898">
4.4 Prior distributions
</subsectionHeader>
<bodyText confidence="0.913923571428571">
Klein and Manning (2004) point their model in the
right direction by initializing the probability of de-
pendencies inversely proportional to the distance be-
tween the head and the dependent. This is a very
good initialization: Figure 4 shows the actual dis-
tances for the dataset used, WSJ10.
John string likes cheese.
</bodyText>
<figureCaption confidence="0.882086">
Figure 3: A Nonprojective Dependency Graph
</figureCaption>
<figure confidence="0.998697111111111">
1 2 3 4 5 6 7 8 9 10
Number of Dependencies
2.5
0.5
1.5
2
3 x 104
0
1
</figure>
<page confidence="0.996777">
47
</page>
<bodyText confidence="0.9996696">
Klein (2005) states that, “It should be emphasized
that this initialization was important in getting rea-
sonable patterns out of this model.” (p. 89). How-
ever, it is not clear that this is necessarily true for all
UGI models.
</bodyText>
<subsectionHeader confidence="0.991145">
4.5 Semantics
</subsectionHeader>
<bodyText confidence="0.99997425">
Semantics have not been included in previous UGI
models, despite successful application in the speech
recognition community (see Section 2). However,
there have been some related efforts in unsupervised
part of speech induction (Sch¨utze, 1995). These ef-
forts have used SVD as a dimensionality reduction
step between distributional analysis and clustering.
Although not labelled as “semantic” this work has
produced the best unsupervised part of speech in-
duction results. Thus our last question is whether
SVD can be applied to a UGI model to improve re-
sults.
</bodyText>
<sectionHeader confidence="0.96738" genericHeader="method">
5 Method
</sectionHeader>
<subsectionHeader confidence="0.917894">
5.1 Materials
</subsectionHeader>
<bodyText confidence="0.999985157894737">
The WSJ10 dataset was used for evaluation to be
comparable to previous results (Klein and Manning,
2004). WSJ10 is a subset of the Wall Street Jour-
nal section of the Penn Treebank, containing only
those sentences of 10 words or less after punctuation
has been removed. WSJ10 contains 7422 sentences.
To counteract the data sparsity encountered by using
ngrams instead of parts of speech, we used the en-
tire WSJ and year 1994 of the North American News
Text Corpus. These corpora were formatted accord-
ing to the same rules as the WSJ10, split into sen-
tences (as documents) and concatenated. The com-
bined corpus contained roughly 10 million words
and 460,000 sentences.
Dependencies, rather than the original bracketing,
were used as the gold standard for parsing perfor-
mance. Since the Penn Treebank does not label de-
pendencies, it was necessary to apply rules to extract
dependencies from WSJ10 (Collins, 1999).
</bodyText>
<subsectionHeader confidence="0.99076">
5.2 Procedure
</subsectionHeader>
<bodyText confidence="0.999981403846154">
The first step is unsupervised latent semantic gram-
mar induction. This was accomplished by first cre-
ating n-gram by context feature matrices, where the
feature varies as per Section 4.3. The Contextglobal
approach uses a bigram by document matrix such
that word order is eliminated. Therefore the value
of cellij is the number of times ngrami occurred
in documentj. The matrix had approximate dimen-
sions 2.2 million by 460,000.
The Contextlo al approach uses a bigram by local
window matrix. If there are n distinct unigrams in
the corpus, the first n columns contain the counts
of the words preceding a target word, and the last n
columns contain the counts of the words following
a target word. For example, the value of at cellij
is the number of times unigramj occurred before
the target ngrami. The value of celli(j+n) is the
number of times unigramj occurred after the target
ngrami. The matrix had approximate dimensions
2.2 million by 280,000.
After the matrices were constructed, each
was transformed using SVD. Because the non-
orthogonal SVD procedure requires a number of
Lanczos steps approximately proportional to the
square of the number of dimensions desired, the
number of dimensions was limited to 100. This kept
running time and storage requirements within rea-
sonable limits, approximately 4 days and 120 giga-
bytes of disk storage to create each.
Next, a parsing table was constructed. For each
bigram, the closest unigram neighbor, in terms of
cosine, was found, cf. Brill and Marcus (1992). The
neighbor, cosine to that neighbor, and cosines of the
bigram’s constituents to that neighbor were stored.
The constituent with the highest cosine to the neigh-
bor was considered the likely head, based on clas-
sic head test arguments (Hudson, 1987). This data
was stored in a lookup table so that for each bigram
the associated information may be found in constant
time.
Next, the WSJ10 was parsed using the parsing
table described above and a minimum spanning
tree algorithm for dependency parsing (McDonald
et al., 2005). Each input sentence was tokenized
on whitespace and lowercased. Moving from left
to right, each word was paired with all remaining
words on its right. If a pair existed in the pars-
ing table, the associated information was retrieved.
This information was used to populate the fully con-
nected graph that served as input to the minimum
spanning tree algorithm. Specifically, when a pair
was retrieved from the parsing table, the arc from
</bodyText>
<page confidence="0.996269">
48
</page>
<bodyText confidence="0.999852260869565">
the stored head to the dependent was given a weight
equal to the cosine between the head and the near-
est unigram neighbor for that bigram pair. Likewise
the arc from the dependent to the head was given a
weight equal to the cosine between the dependent
and the nearest unigram neighbor for that bigram
pair. Thus the weight on each arc was based on the
degree of substitutability between that word and the
nearest unigram neighbor for the bigram pair.
If a bigram was not in the parsing table, it was
given maximum weight, making that dependency
maximally unlikely. After all the words in the sen-
tence had been processed, the average of all current
weights was found, and this average was used as the
weight from a dummy root node to all other nodes
(the dummy ROOT is further motivated in Section
5.3). Therefore all words were given equal likeli-
hood of being the root of the sentence. The end
result of this graph construction process is an n by
n + 1 matrix, where n is the number of words and
there is one dummy root node. Then this graph was
input to the minimum spanning tree algorithm. The
output of this algorithm is a non-projective depen-
dency tree, which was directly compared to the gold
standard dependency tree, as well as the respective
baselines discussed in Section 5.3.
To gauge the differential effects of projectivity
and prior knowledge, the above procedure was mod-
ified in additional evaluation trials. Projectivity was
incorporated by using a bottom-up algorithm (Cov-
ington, 2001). The algorithm was applied in two
stages. First, it was applied using the nonprojective
parse as input. By comparing the output parse to the
original nonprojective parse, it is possible to identify
independent words that could not be incorporated
into the projective parse. In the second stage, the
projective algorithm was run again on the nonpro-
jective input, except this time the independent words
were allowed to link to any other words defined by
the parsing table. In other words, the first stage iden-
tifies unattached words, and the second stage “re-
pairs” the words by finding a projective attachment
for them. This method of enforcing projectivity was
chosen because it makes use of the same informa-
tion as the nonprojective method, but it goes a step
further to enforce projectivity.
</bodyText>
<figureCaption confidence="0.9041255">
Prior distributions of dependencies, as depicted in
Figure 4, were incorporated by inversely weighting
ROOT John likes string cheese
Figure 5: Right Branching Baseline
John likes string cheese ROOT
Figure 6: Left Branching Baseline
</figureCaption>
<bodyText confidence="0.999906333333333">
graph edges by the distance between words. This
modification transparently applies to both the non-
projective case and the projective case.
</bodyText>
<subsectionHeader confidence="0.992331">
5.3 Scoring
</subsectionHeader>
<bodyText confidence="0.999982818181818">
Two performance baselines for dependency parsing
were used in this experiment, the so-called right and
left branching baselines. A right branching baseline
predicts that the head of each word is the word to the
left, forming a chain from left to right. An example
is given in Figure 5. Conversely, a left branching
baseline predicts that the head of each word is the
word to the right, forming a chain from right to left.
An example is given in Figure 6. Although perhaps
not intuitively very powerful baselines, the right and
left branching baselines can be very effective for the
WSJ10. For WSJ10, most heads are close to their
dependents, as shown in Figure 4. For example, the
percentage of dependencies with a head either im-
mediately to the right or left is 53%. Of these neigh-
boring heads, 17% are right branching, and 36% are
left branching.
By using the sign test, the statistical significance
of parsing results can be determined. The sign test is
perhaps the most basic non-parametric tests and so is
useful for this task because it makes no assumptions
regarding the underlying distribution of data.
Consider each sentence. Every word must have
exactly one head. That means that for n words, there
is a 1/n chance of selecting the correct head (exclud-
ing self-heads and including a dummy root head). If
all dependencies in a sentence are independent, then
a sentence’s dependencies follow a binomial distri-
bution, with n equal to the number of words, p equal
to 1/n, and k equal to the number of correct depen-
dencies. From this it follows that the expected num-
ber of correct dependencies per sentence is np, or 1.
Thus the random baseline for nonprojective depen-
</bodyText>
<page confidence="0.998753">
49
</page>
<bodyText confidence="0.995439588235294">
dency parsing performance is one dependency per
sentence.
Using the gold standard of the WSJ10, the number
of correct dependencies found by the latent seman-
tic model can be established. The null hypothesis
is that one randomly generated dependency should
be correct per sentence. Suppose that r+ sentences
have more correct dependencies and r− sentences
have fewer correct dependencies (i.e. 0). Under the
null hypothesis, half of the values should be above
1 and half below, so p = 1/2. Since signed dif-
ference is being considered, sentences with depen-
dencies equal to 1 are excluded. The correspond-
ing binomial distribution of the signs to calculate
whether the model is better than chance is b(n, p) =
b(r+ +r−, 1/2). The corresponding p-value maybe
calculated using Equation 1.
</bodyText>
<equation confidence="0.926333">
n! n k
k!(n − k)! 1/2(1/2) (1)
</equation>
<bodyText confidence="0.9999866">
This same method can be used for determining
statistically significant improvement over right and
left branching baselines. For each sentence, the dif-
ference between the number of correct dependen-
cies in the candidate parse and the number of cor-
rect dependencies in the baseline may be calculated.
The number of positive and negative signed differ-
ences are counted as r+ and r−, respectively, and
the procedure for calculating statistically significant
improvement is the same.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999888785714286">
Each model in Table 6 has significantly better per-
formance than item above using statistical proce-
dure described in Section 5.2. A number of ob-
servations can be drawn from this table. First, all
the models outperform random and right branching
baselines. This is the first time we are aware of
that this has been shown with lexical items in de-
pendency UGI. Secondly, local context outperforms
global context. This is to be expected given the rel-
atively fixed word order in English, but it is some-
what surprising that the differences between local
and global are not greater. Thirdly, it is clear that the
addition of prior knowledge, whether projectivity or
prior distributions, improves performance. Fourthly,
</bodyText>
<table confidence="0.999702769230769">
Method Dependencies Correct
Context/Projectivity/Prior
Random/no/no 14.2%
Right branching 17.6%
Global/no/no 17.9%
Global/no/yes 21.0%
Global/yes/no 21.4%
Global/yes/yes 21.7%
Local/no/no 22.5%
Local/no/yes 25.7%
Local/yes/yes 26.3%
Local/yes/no 26.7%
Left branching 35.8%
</table>
<tableCaption confidence="0.999818">
Table 1: Parsing results on WSJ10
</tableCaption>
<bodyText confidence="0.983739666666667">
projectivity and prior distributions have little addi-
tive effect. Thus it appears that they bring to bear
similar kinds of constraints.
</bodyText>
<sectionHeader confidence="0.997088" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999931230769231">
The results in Section 6 address the unanswered
questions identified in Section 4, i.e. parts of speech,
semantics, context, projectivity, and prior distribu-
tions.
The most salient result in Section 6 is successful
UGI without part of speech tags. As far as we know,
this is the first time dependency UGI has been suc-
cessful without the hidden syntactic structure pro-
vided by part of speech tags. It is interesting to note
that latent semantic grammars improve upon Paskin
(2001), even though that model is projective. It ap-
pears that lexical semantics are the reason. Thus
these results address two of the unanswered ques-
tions from Section 6 regarding parts of speech and
semantics. Semantics improve dependency UGI. In
fact, they improve dependency UGI so much so that
parts of speech are not necessary to beat a right
branching baseline.
Context has traditionally been defined locally, e.g.
the preceding and following word(s). The results
above indicate that a global definition of context is
also effective, though not quite as highly perform-
ing as a local definition on the WSJ10. This sug-
gests that English UGI is not dependent on local lin-
ear context, and it motivates future exploration of
word-order free languages using global context. It is
</bodyText>
<equation confidence="0.726981">
r+−1
1 − E
���
</equation>
<page confidence="0.96815">
50
</page>
<bodyText confidence="0.999978666666667">
also interesting to note that the differences between
global and local contexts begin to disappear as pro-
jectivity and prior distributions are added. This sug-
gests that there is a certain level of equivalence be-
tween a global context model that favors local at-
tachments and a local context model that has no at-
tachment bias.
Projectivity has been assumed in previous cases
of English UGI (Klein and Manning, 2004; Paskin,
2001). As far as we know, this is the first time a
nonprojective model has outperformed a random or
right branching baseline. It is interesting that a non-
projective model can do so well when it assumes so
little about the structure of a language. Even more
interesting is that the addition of projectivity to the
models above increases performance only slightly.
It is tempting to speculate that projectivity may be
something of a red herring for English dependency
parsing, cf. McDonald et al. (2005).
Prior distributions have been previously assumed
as well (Klein and Manning, 2004). The differential
effect of prior distributions in previous work has not
been clear. Our results indicate that a prior distribu-
tion will increase performance. However, as with
projectivity, it is interesting how well the models
perform without this prior knowledge and how slight
an increase this prior knowledge gives. Overall, the
prior distribution used in the evaluation is not neces-
sary to beat the right branching baseline.
Projectivity and prior distributions have signifi-
cant overlap when the prior distribution favors closer
attachments. Projectivity, by forcing a head to gov-
ern a contiguous subsequence, also favors closer at-
tachments. The results reported in Section 6 suggest
that there is a great deal of overlap in the benefit pro-
vided by projectivity and the prior distribution used
in the evaluation. Either one or the other produces
significant benefits, but the combination is much less
impressive.
It is worthwhile to reiterate the sparseness of prior
knowledge contained in the basic model used in
these evaluations. There are essentially four compo-
nents of prior knowledge. First, the ability to create
an ngram by context feature matrix. Secondly, the
application of SVD to that matrix. Thirdly, the cre-
ation of a fully connected dependency graph from
the post-SVD matrix. And finally, the extraction
of a minimum spanning tree from this graph. Al-
though we have not presented evaluation on word-
order free languages, the basic model just described
has no obvious bias against them. We expect that
latent semantic grammars capture some of the uni-
versals of grammar induction. A fuller exploration
and demonstration is the subject of future research.
</bodyText>
<sectionHeader confidence="0.997471" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999976111111111">
This paper presented latent semantic grammars for
the unsupervised induction of English grammar. The
creation of latent semantic grammars and their appli-
cation to parsing were described. Experiments with
context, projectivity, and prior distributions showed
the relative performance effects of these kinds of
prior knowledge. Results show that assumptions of
prior distributions, projectivity, and part of speech
information are not necessary for this task.
</bodyText>
<sectionHeader confidence="0.998848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998050466666667">
Jerome R. Bellegarda. 2000. Large vocabulary speech
recognition with multispan statistical language mod-
els. IEEE Transactions on Speech and Audio Process-
ing, 8(1):76–84.
Michael W. Berry, Susan T. Dumais, and Gavin W.
O’Brien. 1995. Using linear algebra for intelligent in-
formation retrieval. Society for Industrial and Applied
Mathematics Review, 37(4):573–595.
Michael W. Berry. 1992. Large scale singular value com-
putations. International Journal of Supercomputer
Applications, 6(1):13–49.
Eric Brill and Mitchell Marcus. 1992. Automatically
acquiring phrase structure using distributional analy-
sis. In Speech and Natural Language: Proceedings
of a Workshop Held at Harriman, New York, pages
155–160, Philadelphia, February 23-26. Association
for Computational Linguistics.
John Carroll, Guido Minnen, and Ted Briscoe. 2003.
Parser evaluation using a grammatical relation anno-
tation scheme. In A. Abeill, editor, Treebanks: Build-
ing and Using Syntactically Annotated Corpora, chap-
ter 17, pages 299–316. Kluwer, Dordrecht.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
The Hague.
Noah Coccaro and Daniel Jurafsky. 1998. Towards bet-
ter integration of semantic predictors in statistical lan-
guage modeling. In Proceedings of the International
Conference on Spoken Language Processing, pages
2403–2406, Piscataway, NJ, 30th November-4th De-
cember. IEEE.
</reference>
<page confidence="0.991737">
51
</page>
<reference confidence="0.999865329896907">
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing. In John A. Miller and Jef-
fery W. Smith, editors, Proceedings of the 39th Annual
Association for Computing Machinery Southeast Con-
ference, pages 95–102, Athens, Georgia.
Jane K. Cullum and Ralph A. Willoughby. 2002. Lanc-
zos Algorithms for Large Symmetric Eigenvalue Com-
putations, Volume 1: Theory. Society for Industrial
and Applied Mathematics, Philadelphia.
Scott C. Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391–407.
Yonggang Deng and Sanjeev Khudanpur. 2003. La-
tent semantic information in maximum entropy lan-
guage models for conversational speech recognition.
In Proceedings of Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 56–63,
Philadelphia, May 27-June 1. Association for Compu-
tational Linguistics.
Susan Dumais. 1991. Improving the retrieval of informa-
tion from external sources. Behavior Research Meth-
ods, Instruments and Computers, 23(2):229–236.
Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse Processes,
25(2&amp;3):285–308.
Zellig Harris. 1954. Distributional structure. Word,
10:140–162.
Richard A. Hudson. 1987. Zwicky on heads. Journal of
Linguistics, 23:109–132.
Dan Klein and Christopher D. Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 128–135, Philadelphia, July 7-12. Association
for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, pages 478–485, Philadelphia, July
21-26. Association for Computational Linguistics.
Dan Klein. 2005. The Unsupervised Learning ofNatural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211–240.
Thomas. K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to latent semantic analysis.
Discourse Processes, 25(2&amp;3):259–284.
Rasmus M. Larsen. 1998. Lanczos bidiagonalization
with partial reorthogonalization. Technical Report
DAIMI PB-357, Department of Computer Science,
Aarhus University.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
ofHuman Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 523–530, Philadelphia, October 6-
8. Association for Computational Linguistics.
Mark A. Paskin. 2001. Grammatical bigrams. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Ad-
vances in Neural Information Processing Systems 14,
pages 91–97. MIT Press, Cambridge, MA.
Vladimir Pericliev and Ilarion Ilarionov. 1986. Testing
the projectivity hypothesis. In Proceedings of the 11th
International Conference on Computational Linguis-
tics, pages 56–58, Morristown, NJ, USA. Association
for Computational Linguistics.
Hinrich Sch¨utze. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th European As-
sociation for Computational Linguistics Conference
(EACL-95), pages 141–149, Philadelphia, March 27-
31. Association for Computational Linguistics.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proceedings of the National Academy of Sci-
ences, 102:11629–11634.
Menno M. van Zaanen. 2000. ABL: Alignment-based
learning. In Proceedings of the 18th International
Conference on Computational Linguistics, pages 961–
967, Philadelphia, July 31-August 4. Association for
Computational Linguistics.
Peter Wiemer-Hastings and Iraide Zipitria. 2001. Rules
for syntax, vectors for semantics. In Proceedings of
the 23rd Annual Conference of the Cognitive Science
Society, pages 1112–1117, Mahwah, NJ, August 1-4.
Erlbaum.
</reference>
<page confidence="0.99884">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647112">
<title confidence="0.998347">Latent Semantic Grammar Context, Projectivity, and Prior Distributions</title>
<author confidence="0.999723">M Andrew</author>
<affiliation confidence="0.9994135">Institute for Intelligent University of</affiliation>
<address confidence="0.655279">Memphis, TN</address>
<email confidence="0.998776">aolney@memphis.edu</email>
<abstract confidence="0.9995016">This paper presents latent semantic grammars for the unsupervised induction of English grammar. Latent semantic grammars were induced by applying singular value decomposition to n-gram by context-feature matrices. Parsing was used to evaluate performance. Experiments with context, projectivity, and prior distributions show the relative performance effects of these kinds of prior knowledge. Results show that prior distributions, projectivity, and part of speech information are not necessary to beat the right branching baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Large vocabulary speech recognition with multispan statistical language models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2523" citStr="Bellegarda, 2000" startWordPosition="385" endWordPosition="386"> (2004), which nicely dovetails with our semantic approach. The combination of these threads allows some exploration of what characteristics are sufficient for UGI and what characteristics are necessary. 2 Latent semantics Previous work has focused on syntax to the exclusion of semantics (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). However, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using latent semantic information to improve UGI is therefore both novel and relevant. The latent semantic information used by the speech recognition community above is produced by latent semantic analysis (LSA), also known as latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998). LSA creates a semantic representation of both words and collections of words in a vector space, using a two part process. First, 45 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 45–52, Rochester, April 2007 (c 2007 Association for Computat</context>
<context position="5459" citStr="Bellegarda, 2000" startWordPosition="858" endWordPosition="860">such an n-gram scheme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 2002). A non-orthogonal approach creates the same matrix decomposition as traditional appro</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Large vocabulary speech recognition with multispan statistical language models. IEEE Transactions on Speech and Audio Processing, 8(1):76–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Susan T Dumais</author>
<author>Gavin W O’Brien</author>
</authors>
<title>Using linear algebra for intelligent information retrieval.</title>
<date>1995</date>
<journal>Society for Industrial and Applied Mathematics Review,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Berry, Dumais, O’Brien, 1995</marker>
<rawString>Michael W. Berry, Susan T. Dumais, and Gavin W. O’Brien. 1995. Using linear algebra for intelligent information retrieval. Society for Industrial and Applied Mathematics Review, 37(4):573–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
</authors>
<title>Large scale singular value computations.</title>
<date>1992</date>
<journal>International Journal of Supercomputer Applications,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="5099" citStr="Berry (1992)" startWordPosition="804" endWordPosition="805"> have been attempts to apply word order post-hoc to LSA (Wiemer-Hastings and Zipitria, 2001). A straightforward notion of incorporating word order into LSA is to use n-grams instead of individual words. In this way a unigram, bigram, and trigram would each have an atomic vector representation and be directly comparable. It may seem counterintuitive that such an n-gram scheme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associa</context>
</contexts>
<marker>Berry, 1992</marker>
<rawString>Michael W. Berry. 1992. Large scale singular value computations. International Journal of Supercomputer Applications, 6(1):13–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Mitchell Marcus</author>
</authors>
<title>Automatically acquiring phrase structure using distributional analysis.</title>
<date>1992</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop Held at</booktitle>
<pages>155--160</pages>
<location>Harriman, New York,</location>
<contexts>
<context position="2219" citStr="Brill and Marcus, 1992" startWordPosition="333" endWordPosition="336">he case for real-world English UGI has not been as convincingly made (van Zaanen, 2000; Solan et al., 2005). This paper weaves together two threads of inquiry. The first thread is latent semantics, which have not been previously used in UGI. The second thread is dependency-based UGI, used by Klein and Manning (2004), which nicely dovetails with our semantic approach. The combination of these threads allows some exploration of what characteristics are sufficient for UGI and what characteristics are necessary. 2 Latent semantics Previous work has focused on syntax to the exclusion of semantics (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). However, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using latent semantic information to improve UGI is therefore both novel and relevant. The latent semantic information used by the speech recognition community above is produced by latent semantic analysis (LSA), also known as latent semantic indexing (Deerwester et </context>
<context position="9174" citStr="Brill and Marcus, 1992" startWordPosition="1449" endWordPosition="1452">ojectivity Projectivity is an additional constraint that may not be necessary for successful UGI. English is a projective language, but other languages, such as Bulgarian, are not (Pericliev and Ilarionov, 1986). Nonprojective UGI has not previously been studied, and it is not clear how important projectivity assumptions are to English UGI. Figure 3 gives an example of a nonprojective construction: not all heads and their dependents are a contiguous sequence. Words Distant Figure 4: Distance Between Dependents in WSJ10 4.3 Context The core of several UGI approaches is distributional analysis (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). The key idea in such distributional analysis is that the function of a word may be known if it can be substituted for another word (Harris, 1954). If so, both words have the same function. Substitutability must be defined over a context. In UGI, this context has typically been the preceding and following words of the target word. However, this notion of context has an implicit assumption of word order. This assumption is true for English, but is not true for other languages such as Latin. T</context>
<context position="13669" citStr="Brill and Marcus (1992)" startWordPosition="2204" endWordPosition="2207">ami. The matrix had approximate dimensions 2.2 million by 280,000. After the matrices were constructed, each was transformed using SVD. Because the nonorthogonal SVD procedure requires a number of Lanczos steps approximately proportional to the square of the number of dimensions desired, the number of dimensions was limited to 100. This kept running time and storage requirements within reasonable limits, approximately 4 days and 120 gigabytes of disk storage to create each. Next, a parsing table was constructed. For each bigram, the closest unigram neighbor, in terms of cosine, was found, cf. Brill and Marcus (1992). The neighbor, cosine to that neighbor, and cosines of the bigram’s constituents to that neighbor were stored. The constituent with the highest cosine to the neighbor was considered the likely head, based on classic head test arguments (Hudson, 1987). This data was stored in a lookup table so that for each bigram the associated information may be found in constant time. Next, the WSJ10 was parsed using the parsing table described above and a minimum spanning tree algorithm for dependency parsing (McDonald et al., 2005). Each input sentence was tokenized on whitespace and lowercased. Moving fr</context>
</contexts>
<marker>Brill, Marcus, 1992</marker>
<rawString>Eric Brill and Mitchell Marcus. 1992. Automatically acquiring phrase structure using distributional analysis. In Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, pages 155–160, Philadelphia, February 23-26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Parser evaluation using a grammatical relation annotation scheme.</title>
<date>2003</date>
<booktitle>Treebanks: Building and Using Syntactically Annotated Corpora, chapter 17,</booktitle>
<pages>299--316</pages>
<editor>In A. Abeill, editor,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="6955" citStr="Carroll et al., 2003" startWordPosition="1092" endWordPosition="1095">n for evaluating UGI. Because dependency structures have no higher order nodes, e.g. NP, their evaluation is simple: one may compare with a reference parse and count the proportion of correct dependencies. For example, Figure 1 has three dependencies {( John, likes ), ( cheese, likes ), ( string, cheese ) }, so the trial parse {( John, likes ), ( string, likes), ( cheese, string )} has 1/3 directed dependencies correct and 2/3 undirected dependencies correct. This metric avoids the biases created by bracketing, where over-generation or undergeneration of brackets may cloud actual performance (Carroll et al., 2003). Dependencies are equivalent with lexicalized trees (see Figures 1 and 2) so long as the dependencies are projective. Dependencies are projective when all heads and their dependents are a contiguous sequence. Dependencies have been used for UGI before with mixed success (Paskin, 2001; Klein and Manning, 2004). Paskin (2001) created a projective model using words, and he evaluated on WSJ. Although he reported beating the random baseline for that task, both Klein and Manning (2004) and we have repli46 Slikes NPJohn VPlikes John likes NPcheese string cheese Figure 2: A Lexicalized Tree cated the</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 2003</marker>
<rawString>John Carroll, Guido Minnen, and Ted Briscoe. 2003. Parser evaluation using a grammatical relation annotation scheme. In A. Abeill, editor, Treebanks: Building and Using Syntactically Annotated Corpora, chapter 17, pages 299–316. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1957</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="973" citStr="Chomsky, 1957" startWordPosition="134" endWordPosition="135">e decomposition to n-gram by context-feature matrices. Parsing was used to evaluate performance. Experiments with context, projectivity, and prior distributions show the relative performance effects of these kinds of prior knowledge. Results show that prior distributions, projectivity, and part of speech information are not necessary to beat the right branching baseline. 1 Introduction Unsupervised grammar induction (UGI) generates a grammar from raw text. It is an interesting problem both theoretically and practically. Theoretically, it connects to the linguistics debate on innate knowledge (Chomsky, 1957). Practically, it has the potential to supersede techniques requiring structured text, like treebanks. Finding structure in text with little or no prior knowledge is therefore a fundamental issue in the study of language. However, UGI is still a largely unsolved problem. Recent work (Klein and Manning, 2002; Klein and Manning, 2004) has renewed interest by using a UGI model to parse sentences from the Wall Street Journal section of the Penn Treebank (WSJ). These parsing results are exciting because they demonstrate real-world applicability to English UGI. While other contemporary research in t</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Noam Chomsky. 1957. Syntactic Structures. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in statistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>2403--2406</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2505" citStr="Coccaro and Jurafsky, 1998" startWordPosition="381" endWordPosition="384">I, used by Klein and Manning (2004), which nicely dovetails with our semantic approach. The combination of these threads allows some exploration of what characteristics are sufficient for UGI and what characteristics are necessary. 2 Latent semantics Previous work has focused on syntax to the exclusion of semantics (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). However, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using latent semantic information to improve UGI is therefore both novel and relevant. The latent semantic information used by the speech recognition community above is produced by latent semantic analysis (LSA), also known as latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998). LSA creates a semantic representation of both words and collections of words in a vector space, using a two part process. First, 45 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 45–52, Rochester, April 2007 (c 2007 Associ</context>
<context position="5421" citStr="Coccaro and Jurafsky, 1998" startWordPosition="850" endWordPosition="853">y comparable. It may seem counterintuitive that such an n-gram scheme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 2002). A non-orthogonal approach creates the same mat</context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>Noah Coccaro and Daniel Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. In Proceedings of the International Conference on Spoken Language Processing, pages 2403–2406, Piscataway, NJ, 30th November-4th December. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12145" citStr="Collins, 1999" startWordPosition="1955" endWordPosition="1956">es. To counteract the data sparsity encountered by using ngrams instead of parts of speech, we used the entire WSJ and year 1994 of the North American News Text Corpus. These corpora were formatted according to the same rules as the WSJ10, split into sentences (as documents) and concatenated. The combined corpus contained roughly 10 million words and 460,000 sentences. Dependencies, rather than the original bracketing, were used as the gold standard for parsing performance. Since the Penn Treebank does not label dependencies, it was necessary to apply rules to extract dependencies from WSJ10 (Collins, 1999). 5.2 Procedure The first step is unsupervised latent semantic grammar induction. This was accomplished by first creating n-gram by context feature matrices, where the feature varies as per Section 4.3. The Contextglobal approach uses a bigram by document matrix such that word order is eliminated. Therefore the value of cellij is the number of times ngrami occurred in documentj. The matrix had approximate dimensions 2.2 million by 460,000. The Contextlo al approach uses a bigram by local window matrix. If there are n distinct unigrams in the corpus, the first n columns contain the counts of th</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A fundamental algorithm for dependency parsing. In</title>
<date>2001</date>
<booktitle>Proceedings of the 39th Annual Association for Computing Machinery Southeast Conference,</booktitle>
<pages>95--102</pages>
<editor>John A. Miller and Jeffery W. Smith, editors,</editor>
<location>Athens, Georgia.</location>
<contexts>
<context position="16136" citStr="Covington, 2001" startWordPosition="2629" endWordPosition="2631"> The end result of this graph construction process is an n by n + 1 matrix, where n is the number of words and there is one dummy root node. Then this graph was input to the minimum spanning tree algorithm. The output of this algorithm is a non-projective dependency tree, which was directly compared to the gold standard dependency tree, as well as the respective baselines discussed in Section 5.3. To gauge the differential effects of projectivity and prior knowledge, the above procedure was modified in additional evaluation trials. Projectivity was incorporated by using a bottom-up algorithm (Covington, 2001). The algorithm was applied in two stages. First, it was applied using the nonprojective parse as input. By comparing the output parse to the original nonprojective parse, it is possible to identify independent words that could not be incorporated into the projective parse. In the second stage, the projective algorithm was run again on the nonprojective input, except this time the independent words were allowed to link to any other words defined by the parsing table. In other words, the first stage identifies unattached words, and the second stage “repairs” the words by finding a projective at</context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A fundamental algorithm for dependency parsing. In John A. Miller and Jeffery W. Smith, editors, Proceedings of the 39th Annual Association for Computing Machinery Southeast Conference, pages 95–102, Athens, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane K Cullum</author>
<author>Ralph A Willoughby</author>
</authors>
<title>Lanczos Algorithms for Large Symmetric Eigenvalue Computations, Volume 1: Theory. Society for Industrial and Applied Mathematics,</title>
<date>2002</date>
<location>Philadelphia.</location>
<contexts>
<context position="5973" citStr="Cullum and Willoughby, 2002" startWordPosition="937" endWordPosition="940">Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 2002). A non-orthogonal approach creates the same matrix decomposition as traditional approaches, but the resulting memory savings allow dramatically larger matrix decompositions. Thus a non-orthongonal SVD approach is key to the inclusion of ordered latent semantics into our UGI model. 3 Dependency grammars Dependency structures are an ideal grammar representation for evaluating UGI. Because dependency structures have no higher order nodes, e.g. NP, their evaluation is simple: one may compare with a reference parse and count the proportion of correct dependencies. For example, Figure 1 has three d</context>
</contexts>
<marker>Cullum, Willoughby, 2002</marker>
<rawString>Jane K. Cullum and Ralph A. Willoughby. 2002. Lanczos Algorithms for Large Symmetric Eigenvalue Computations, Volume 1: Theory. Society for Industrial and Applied Mathematics, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2828" citStr="Deerwester et al., 1990" startWordPosition="428" endWordPosition="431">d Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). However, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using latent semantic information to improve UGI is therefore both novel and relevant. The latent semantic information used by the speech recognition community above is produced by latent semantic analysis (LSA), also known as latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998). LSA creates a semantic representation of both words and collections of words in a vector space, using a two part process. First, 45 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 45–52, Rochester, April 2007 (c 2007 Association for Computational Linguistics a term by document matrix is created in which the frequency of word wz in document dj is the value of cell czj. Filters may be applied during this process which eliminate undesired terms, e.g. common words. Weighting may also be applied to decrease the contributions of frequent words (D</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Latent semantic information in maximum entropy language models for conversational speech recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>56--63</pages>
<location>Philadelphia,</location>
<contexts>
<context position="2550" citStr="Deng and Khudanpur, 2003" startWordPosition="387" endWordPosition="390">ely dovetails with our semantic approach. The combination of these threads allows some exploration of what characteristics are sufficient for UGI and what characteristics are necessary. 2 Latent semantics Previous work has focused on syntax to the exclusion of semantics (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). However, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using latent semantic information to improve UGI is therefore both novel and relevant. The latent semantic information used by the speech recognition community above is produced by latent semantic analysis (LSA), also known as latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998). LSA creates a semantic representation of both words and collections of words in a vector space, using a two part process. First, 45 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 45–52, Rochester, April 2007 (c 2007 Association for Computational Linguistics a term by</context>
<context position="5486" citStr="Deng and Khudanpur, 2003" startWordPosition="861" endWordPosition="864">eme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 2002). A non-orthogonal approach creates the same matrix decomposition as traditional approaches, but the resulting me</context>
</contexts>
<marker>Deng, Khudanpur, 2003</marker>
<rawString>Yonggang Deng and Sanjeev Khudanpur. 2003. Latent semantic information in maximum entropy language models for conversational speech recognition. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 56–63, Philadelphia, May 27-June 1. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
</authors>
<title>Improving the retrieval of information from external sources.</title>
<date>1991</date>
<journal>Behavior Research Methods, Instruments and Computers,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="3440" citStr="Dumais, 1991" startWordPosition="529" endWordPosition="530">0; Landauer et al., 1998). LSA creates a semantic representation of both words and collections of words in a vector space, using a two part process. First, 45 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 45–52, Rochester, April 2007 (c 2007 Association for Computational Linguistics a term by document matrix is created in which the frequency of word wz in document dj is the value of cell czj. Filters may be applied during this process which eliminate undesired terms, e.g. common words. Weighting may also be applied to decrease the contributions of frequent words (Dumais, 1991). Secondly, singular value decomposition (SVD) is applied to the term by document matrix. The resulting matrix decomposition has the property that the removal of higher-order dimensions creates an optimal reduced representation of the original matrix in the least squares sense (Berry et al., 1995). Therefore, SVD performs a kind of dimensionality reduction such that words appearing in different documents can acquire similar row vector representations (Landauer and Dumais, 1997). Words can be compared by taking the cosine of their corresponding row vectors. Collections of words can likewise be </context>
</contexts>
<marker>Dumais, 1991</marker>
<rawString>Susan Dumais. 1991. Improving the retrieval of information from external sources. Behavior Research Methods, Instruments and Computers, 23(2):229–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas K Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="5441" citStr="Foltz et al., 1998" startWordPosition="854" endWordPosition="857">unterintuitive that such an n-gram scheme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 2002). A non-orthogonal approach creates the same matrix decomposition as</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter W. Foltz, Walter Kintsch, and Thomas K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Processes, 25(2&amp;3):285–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--140</pages>
<contexts>
<context position="9424" citStr="Harris, 1954" startWordPosition="1496" endWordPosition="1497">ied, and it is not clear how important projectivity assumptions are to English UGI. Figure 3 gives an example of a nonprojective construction: not all heads and their dependents are a contiguous sequence. Words Distant Figure 4: Distance Between Dependents in WSJ10 4.3 Context The core of several UGI approaches is distributional analysis (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). The key idea in such distributional analysis is that the function of a word may be known if it can be substituted for another word (Harris, 1954). If so, both words have the same function. Substitutability must be defined over a context. In UGI, this context has typically been the preceding and following words of the target word. However, this notion of context has an implicit assumption of word order. This assumption is true for English, but is not true for other languages such as Latin. Therefore, it is not clear how dependent English UGI is on local linear context, e.g. preceding and following words, or whether an unordered notion of context would also be effective. 4.4 Prior distributions Klein and Manning (2004) point their model </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10:140–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Hudson</author>
</authors>
<title>Zwicky on heads.</title>
<date>1987</date>
<journal>Journal of Linguistics,</journal>
<pages>23--109</pages>
<contexts>
<context position="13920" citStr="Hudson, 1987" startWordPosition="2247" endWordPosition="2248">mber of dimensions desired, the number of dimensions was limited to 100. This kept running time and storage requirements within reasonable limits, approximately 4 days and 120 gigabytes of disk storage to create each. Next, a parsing table was constructed. For each bigram, the closest unigram neighbor, in terms of cosine, was found, cf. Brill and Marcus (1992). The neighbor, cosine to that neighbor, and cosines of the bigram’s constituents to that neighbor were stored. The constituent with the highest cosine to the neighbor was considered the likely head, based on classic head test arguments (Hudson, 1987). This data was stored in a lookup table so that for each bigram the associated information may be found in constant time. Next, the WSJ10 was parsed using the parsing table described above and a minimum spanning tree algorithm for dependency parsing (McDonald et al., 2005). Each input sentence was tokenized on whitespace and lowercased. Moving from left to right, each word was paired with all remaining words on its right. If a pair existed in the parsing table, the associated information was retrieved. This information was used to populate the fully connected graph that served as input to the</context>
</contexts>
<marker>Hudson, 1987</marker>
<rawString>Richard A. Hudson. 1987. Zwicky on heads. Journal of Linguistics, 23:109–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>Philadelphia,</location>
<contexts>
<context position="1281" citStr="Klein and Manning, 2002" startWordPosition="181" endWordPosition="184">speech information are not necessary to beat the right branching baseline. 1 Introduction Unsupervised grammar induction (UGI) generates a grammar from raw text. It is an interesting problem both theoretically and practically. Theoretically, it connects to the linguistics debate on innate knowledge (Chomsky, 1957). Practically, it has the potential to supersede techniques requiring structured text, like treebanks. Finding structure in text with little or no prior knowledge is therefore a fundamental issue in the study of language. However, UGI is still a largely unsolved problem. Recent work (Klein and Manning, 2002; Klein and Manning, 2004) has renewed interest by using a UGI model to parse sentences from the Wall Street Journal section of the Penn Treebank (WSJ). These parsing results are exciting because they demonstrate real-world applicability to English UGI. While other contemporary research in this area is promising, the case for real-world English UGI has not been as convincingly made (van Zaanen, 2000; Solan et al., 2005). This paper weaves together two threads of inquiry. The first thread is latent semantics, which have not been previously used in UGI. The second thread is dependency-based UGI,</context>
<context position="9217" citStr="Klein and Manning, 2002" startWordPosition="1456" endWordPosition="1459">onstraint that may not be necessary for successful UGI. English is a projective language, but other languages, such as Bulgarian, are not (Pericliev and Ilarionov, 1986). Nonprojective UGI has not previously been studied, and it is not clear how important projectivity assumptions are to English UGI. Figure 3 gives an example of a nonprojective construction: not all heads and their dependents are a contiguous sequence. Words Distant Figure 4: Distance Between Dependents in WSJ10 4.3 Context The core of several UGI approaches is distributional analysis (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). The key idea in such distributional analysis is that the function of a word may be known if it can be substituted for another word (Harris, 1954). If so, both words have the same function. Substitutability must be defined over a context. In UGI, this context has typically been the preceding and following words of the target word. However, this notion of context has an implicit assumption of word order. This assumption is true for English, but is not true for other languages such as Latin. Therefore, it is not clear how dependent Eng</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 128–135, Philadelphia, July 7-12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>478--485</pages>
<location>Philadelphia,</location>
<contexts>
<context position="1307" citStr="Klein and Manning, 2004" startWordPosition="185" endWordPosition="188">t necessary to beat the right branching baseline. 1 Introduction Unsupervised grammar induction (UGI) generates a grammar from raw text. It is an interesting problem both theoretically and practically. Theoretically, it connects to the linguistics debate on innate knowledge (Chomsky, 1957). Practically, it has the potential to supersede techniques requiring structured text, like treebanks. Finding structure in text with little or no prior knowledge is therefore a fundamental issue in the study of language. However, UGI is still a largely unsolved problem. Recent work (Klein and Manning, 2002; Klein and Manning, 2004) has renewed interest by using a UGI model to parse sentences from the Wall Street Journal section of the Penn Treebank (WSJ). These parsing results are exciting because they demonstrate real-world applicability to English UGI. While other contemporary research in this area is promising, the case for real-world English UGI has not been as convincingly made (van Zaanen, 2000; Solan et al., 2005). This paper weaves together two threads of inquiry. The first thread is latent semantics, which have not been previously used in UGI. The second thread is dependency-based UGI, used by Klein and Manning</context>
<context position="7266" citStr="Klein and Manning, 2004" startWordPosition="1142" endWordPosition="1145">, so the trial parse {( John, likes ), ( string, likes), ( cheese, string )} has 1/3 directed dependencies correct and 2/3 undirected dependencies correct. This metric avoids the biases created by bracketing, where over-generation or undergeneration of brackets may cloud actual performance (Carroll et al., 2003). Dependencies are equivalent with lexicalized trees (see Figures 1 and 2) so long as the dependencies are projective. Dependencies are projective when all heads and their dependents are a contiguous sequence. Dependencies have been used for UGI before with mixed success (Paskin, 2001; Klein and Manning, 2004). Paskin (2001) created a projective model using words, and he evaluated on WSJ. Although he reported beating the random baseline for that task, both Klein and Manning (2004) and we have repli46 Slikes NPJohn VPlikes John likes NPcheese string cheese Figure 2: A Lexicalized Tree cated the random baseline above Paskin’s results. Klein and Manning (2004), on the other hand, have handily beaten a random baseline using a projective model over part of speech tags and evaluating on a subset of WSJ, WSJ10. 4 Unanswered questions There are several unanswered questions in dependency-based English UGI. </context>
<context position="9256" citStr="Klein and Manning, 2004" startWordPosition="1462" endWordPosition="1465"> successful UGI. English is a projective language, but other languages, such as Bulgarian, are not (Pericliev and Ilarionov, 1986). Nonprojective UGI has not previously been studied, and it is not clear how important projectivity assumptions are to English UGI. Figure 3 gives an example of a nonprojective construction: not all heads and their dependents are a contiguous sequence. Words Distant Figure 4: Distance Between Dependents in WSJ10 4.3 Context The core of several UGI approaches is distributional analysis (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). The key idea in such distributional analysis is that the function of a word may be known if it can be substituted for another word (Harris, 1954). If so, both words have the same function. Substitutability must be defined over a context. In UGI, this context has typically been the preceding and following words of the target word. However, this notion of context has an implicit assumption of word order. This assumption is true for English, but is not true for other languages such as Latin. Therefore, it is not clear how dependent English UGI is on local linear context, e.</context>
<context position="11339" citStr="Klein and Manning, 2004" startWordPosition="1819" endWordPosition="1822">e successful application in the speech recognition community (see Section 2). However, there have been some related efforts in unsupervised part of speech induction (Sch¨utze, 1995). These efforts have used SVD as a dimensionality reduction step between distributional analysis and clustering. Although not labelled as “semantic” this work has produced the best unsupervised part of speech induction results. Thus our last question is whether SVD can be applied to a UGI model to improve results. 5 Method 5.1 Materials The WSJ10 dataset was used for evaluation to be comparable to previous results (Klein and Manning, 2004). WSJ10 is a subset of the Wall Street Journal section of the Penn Treebank, containing only those sentences of 10 words or less after punctuation has been removed. WSJ10 contains 7422 sentences. To counteract the data sparsity encountered by using ngrams instead of parts of speech, we used the entire WSJ and year 1994 of the North American News Text Corpus. These corpora were formatted according to the same rules as the WSJ10, split into sentences (as documents) and concatenated. The combined corpus contained roughly 10 million words and 460,000 sentences. Dependencies, rather than the origin</context>
<context position="23140" citStr="Klein and Manning, 2004" startWordPosition="3775" endWordPosition="3778">l definition on the WSJ10. This suggests that English UGI is not dependent on local linear context, and it motivates future exploration of word-order free languages using global context. It is r+−1 1 − E ��� 50 also interesting to note that the differences between global and local contexts begin to disappear as projectivity and prior distributions are added. This suggests that there is a certain level of equivalence between a global context model that favors local attachments and a local context model that has no attachment bias. Projectivity has been assumed in previous cases of English UGI (Klein and Manning, 2004; Paskin, 2001). As far as we know, this is the first time a nonprojective model has outperformed a random or right branching baseline. It is interesting that a nonprojective model can do so well when it assumes so little about the structure of a language. Even more interesting is that the addition of projectivity to the models above increases performance only slightly. It is tempting to speculate that projectivity may be something of a red herring for English dependency parsing, cf. McDonald et al. (2005). Prior distributions have been previously assumed as well (Klein and Manning, 2004). The</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 478–485, Philadelphia, July 21-26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The Unsupervised Learning ofNatural Language Structure.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="10425" citStr="Klein (2005)" startWordPosition="1674" endWordPosition="1675"> English UGI is on local linear context, e.g. preceding and following words, or whether an unordered notion of context would also be effective. 4.4 Prior distributions Klein and Manning (2004) point their model in the right direction by initializing the probability of dependencies inversely proportional to the distance between the head and the dependent. This is a very good initialization: Figure 4 shows the actual distances for the dataset used, WSJ10. John string likes cheese. Figure 3: A Nonprojective Dependency Graph 1 2 3 4 5 6 7 8 9 10 Number of Dependencies 2.5 0.5 1.5 2 3 x 104 0 1 47 Klein (2005) states that, “It should be emphasized that this initialization was important in getting reasonable patterns out of this model.” (p. 89). However, it is not clear that this is necessarily true for all UGI models. 4.5 Semantics Semantics have not been included in previous UGI models, despite successful application in the speech recognition community (see Section 2). However, there have been some related efforts in unsupervised part of speech induction (Sch¨utze, 1995). These efforts have used SVD as a dimensionality reduction step between distributional analysis and clustering. Although not lab</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>Dan Klein. 2005. The Unsupervised Learning ofNatural Language Structure. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="3922" citStr="Landauer and Dumais, 1997" startWordPosition="600" endWordPosition="603">which eliminate undesired terms, e.g. common words. Weighting may also be applied to decrease the contributions of frequent words (Dumais, 1991). Secondly, singular value decomposition (SVD) is applied to the term by document matrix. The resulting matrix decomposition has the property that the removal of higher-order dimensions creates an optimal reduced representation of the original matrix in the least squares sense (Berry et al., 1995). Therefore, SVD performs a kind of dimensionality reduction such that words appearing in different documents can acquire similar row vector representations (Landauer and Dumais, 1997). Words can be compared by taking the cosine of their corresponding row vectors. Collections of words can likewise be compared by first adding the corresponding row vectors in each collection, then taking the cosine between the two collection vectors. A stumbling block to incorporating LSA into UGI is that grammars are inherently ordered but LSA is not. LSA is unordered because the sum of vectors is the same regardless of the order in which they were added. The incorporation of word order into LSA has never been successfully carried out before, although there have been attempts to apply word o</context>
<context position="5370" citStr="Landauer and Dumais, 1997" startWordPosition="842" endWordPosition="845">ave an atomic vector representation and be directly comparable. It may seem counterintuitive that such an n-gram scheme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 20</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="2852" citStr="Landauer et al., 1998" startWordPosition="432" endWordPosition="435">n, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). However, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using latent semantic information to improve UGI is therefore both novel and relevant. The latent semantic information used by the speech recognition community above is produced by latent semantic analysis (LSA), also known as latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998). LSA creates a semantic representation of both words and collections of words in a vector space, using a two part process. First, 45 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 45–52, Rochester, April 2007 (c 2007 Association for Computational Linguistics a term by document matrix is created in which the frequency of word wz in document dj is the value of cell czj. Filters may be applied during this process which eliminate undesired terms, e.g. common words. Weighting may also be applied to decrease the contributions of frequent words (Dumais, 1991). Secondly, </context>
<context position="5393" citStr="Landauer et al., 1998" startWordPosition="846" endWordPosition="849">entation and be directly comparable. It may seem counterintuitive that such an n-gram scheme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 2002). A non-orthogonal a</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas. K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. Introduction to latent semantic analysis. Discourse Processes, 25(2&amp;3):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rasmus M Larsen</author>
</authors>
<title>Lanczos bidiagonalization with partial reorthogonalization.</title>
<date>1998</date>
<tech>Technical Report DAIMI PB-357,</tech>
<institution>Department of Computer Science, Aarhus University.</institution>
<contexts>
<context position="5587" citStr="Larsen (1998)" startWordPosition="879" endWordPosition="880">mitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalization was incorporated into the BellCore LSI tools. Subsequently, either SVDPACK or the LSI tools were used by the majority of researchers in this area (Sch¨utze, 1995; Landauer and Dumais, 1997; Landauer et al., 1998; Coccaro and Jurafsky, 1998; Foltz et al., 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using John likes string cheese. Figure 1: A Dependency Graph the equation reported in Larsen (1998), a standard orthogonal SVD of a unigram/bigram by sentence matrix of the LSA Touchstone Applied Science Associates Corpus (Landauer et al., 1998) requires over 60 gigabytes of random access memory. This estimate is prohibitive for all but current supercomputers. However, it is possible to use a non-orthogonal SVD approach with significant memory savings (Cullum and Willoughby, 2002). A non-orthogonal approach creates the same matrix decomposition as traditional approaches, but the resulting memory savings allow dramatically larger matrix decompositions. Thus a non-orthongonal SVD approach is </context>
</contexts>
<marker>Larsen, 1998</marker>
<rawString>Rasmus M. Larsen. 1998. Lanczos bidiagonalization with partial reorthogonalization. Technical Report DAIMI PB-357, Department of Computer Science, Aarhus University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings ofHuman Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<location>Philadelphia,</location>
<contexts>
<context position="14194" citStr="McDonald et al., 2005" startWordPosition="2291" endWordPosition="2294"> bigram, the closest unigram neighbor, in terms of cosine, was found, cf. Brill and Marcus (1992). The neighbor, cosine to that neighbor, and cosines of the bigram’s constituents to that neighbor were stored. The constituent with the highest cosine to the neighbor was considered the likely head, based on classic head test arguments (Hudson, 1987). This data was stored in a lookup table so that for each bigram the associated information may be found in constant time. Next, the WSJ10 was parsed using the parsing table described above and a minimum spanning tree algorithm for dependency parsing (McDonald et al., 2005). Each input sentence was tokenized on whitespace and lowercased. Moving from left to right, each word was paired with all remaining words on its right. If a pair existed in the parsing table, the associated information was retrieved. This information was used to populate the fully connected graph that served as input to the minimum spanning tree algorithm. Specifically, when a pair was retrieved from the parsing table, the arc from 48 the stored head to the dependent was given a weight equal to the cosine between the head and the nearest unigram neighbor for that bigram pair. Likewise the arc</context>
<context position="23651" citStr="McDonald et al. (2005)" startWordPosition="3861" endWordPosition="3864">has no attachment bias. Projectivity has been assumed in previous cases of English UGI (Klein and Manning, 2004; Paskin, 2001). As far as we know, this is the first time a nonprojective model has outperformed a random or right branching baseline. It is interesting that a nonprojective model can do so well when it assumes so little about the structure of a language. Even more interesting is that the addition of projectivity to the models above increases performance only slightly. It is tempting to speculate that projectivity may be something of a red herring for English dependency parsing, cf. McDonald et al. (2005). Prior distributions have been previously assumed as well (Klein and Manning, 2004). The differential effect of prior distributions in previous work has not been clear. Our results indicate that a prior distribution will increase performance. However, as with projectivity, it is interesting how well the models perform without this prior knowledge and how slight an increase this prior knowledge gives. Overall, the prior distribution used in the evaluation is not necessary to beat the right branching baseline. Projectivity and prior distributions have significant overlap when the prior distribu</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings ofHuman Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 523–530, Philadelphia, October 6-8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Paskin</author>
</authors>
<title>Grammatical bigrams. In</title>
<date>2001</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<pages>91--97</pages>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2276" citStr="Paskin, 2001" startWordPosition="345" endWordPosition="346">e (van Zaanen, 2000; Solan et al., 2005). This paper weaves together two threads of inquiry. The first thread is latent semantics, which have not been previously used in UGI. The second thread is dependency-based UGI, used by Klein and Manning (2004), which nicely dovetails with our semantic approach. The combination of these threads allows some exploration of what characteristics are sufficient for UGI and what characteristics are necessary. 2 Latent semantics Previous work has focused on syntax to the exclusion of semantics (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). However, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Deng and Khudanpur, 2003). Using latent semantic information to improve UGI is therefore both novel and relevant. The latent semantic information used by the speech recognition community above is produced by latent semantic analysis (LSA), also known as latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998). LSA creates a semantic</context>
<context position="7240" citStr="Paskin, 2001" startWordPosition="1140" endWordPosition="1141">ng, cheese ) }, so the trial parse {( John, likes ), ( string, likes), ( cheese, string )} has 1/3 directed dependencies correct and 2/3 undirected dependencies correct. This metric avoids the biases created by bracketing, where over-generation or undergeneration of brackets may cloud actual performance (Carroll et al., 2003). Dependencies are equivalent with lexicalized trees (see Figures 1 and 2) so long as the dependencies are projective. Dependencies are projective when all heads and their dependents are a contiguous sequence. Dependencies have been used for UGI before with mixed success (Paskin, 2001; Klein and Manning, 2004). Paskin (2001) created a projective model using words, and he evaluated on WSJ. Although he reported beating the random baseline for that task, both Klein and Manning (2004) and we have repli46 Slikes NPJohn VPlikes John likes NPcheese string cheese Figure 2: A Lexicalized Tree cated the random baseline above Paskin’s results. Klein and Manning (2004), on the other hand, have handily beaten a random baseline using a projective model over part of speech tags and evaluating on a subset of WSJ, WSJ10. 4 Unanswered questions There are several unanswered questions in depe</context>
<context position="9231" citStr="Paskin, 2001" startWordPosition="1460" endWordPosition="1461"> necessary for successful UGI. English is a projective language, but other languages, such as Bulgarian, are not (Pericliev and Ilarionov, 1986). Nonprojective UGI has not previously been studied, and it is not clear how important projectivity assumptions are to English UGI. Figure 3 gives an example of a nonprojective construction: not all heads and their dependents are a contiguous sequence. Words Distant Figure 4: Distance Between Dependents in WSJ10 4.3 Context The core of several UGI approaches is distributional analysis (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). The key idea in such distributional analysis is that the function of a word may be known if it can be substituted for another word (Harris, 1954). If so, both words have the same function. Substitutability must be defined over a context. In UGI, this context has typically been the preceding and following words of the target word. However, this notion of context has an implicit assumption of word order. This assumption is true for English, but is not true for other languages such as Latin. Therefore, it is not clear how dependent English UGI is on</context>
<context position="21936" citStr="Paskin (2001)" startWordPosition="3574" endWordPosition="3575">d prior distributions have little additive effect. Thus it appears that they bring to bear similar kinds of constraints. 7 Discussion The results in Section 6 address the unanswered questions identified in Section 4, i.e. parts of speech, semantics, context, projectivity, and prior distributions. The most salient result in Section 6 is successful UGI without part of speech tags. As far as we know, this is the first time dependency UGI has been successful without the hidden syntactic structure provided by part of speech tags. It is interesting to note that latent semantic grammars improve upon Paskin (2001), even though that model is projective. It appears that lexical semantics are the reason. Thus these results address two of the unanswered questions from Section 6 regarding parts of speech and semantics. Semantics improve dependency UGI. In fact, they improve dependency UGI so much so that parts of speech are not necessary to beat a right branching baseline. Context has traditionally been defined locally, e.g. the preceding and following word(s). The results above indicate that a global definition of context is also effective, though not quite as highly performing as a local definition on the</context>
<context position="23155" citStr="Paskin, 2001" startWordPosition="3779" endWordPosition="3780">. This suggests that English UGI is not dependent on local linear context, and it motivates future exploration of word-order free languages using global context. It is r+−1 1 − E ��� 50 also interesting to note that the differences between global and local contexts begin to disappear as projectivity and prior distributions are added. This suggests that there is a certain level of equivalence between a global context model that favors local attachments and a local context model that has no attachment bias. Projectivity has been assumed in previous cases of English UGI (Klein and Manning, 2004; Paskin, 2001). As far as we know, this is the first time a nonprojective model has outperformed a random or right branching baseline. It is interesting that a nonprojective model can do so well when it assumes so little about the structure of a language. Even more interesting is that the addition of projectivity to the models above increases performance only slightly. It is tempting to speculate that projectivity may be something of a red herring for English dependency parsing, cf. McDonald et al. (2005). Prior distributions have been previously assumed as well (Klein and Manning, 2004). The differential e</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>Mark A. Paskin. 2001. Grammatical bigrams. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 91–97. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Pericliev</author>
<author>Ilarion Ilarionov</author>
</authors>
<title>Testing the projectivity hypothesis.</title>
<date>1986</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Linguistics,</booktitle>
<pages>56--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8763" citStr="Pericliev and Ilarionov, 1986" startWordPosition="1385" endWordPosition="1388">Parts of speech Klein and Manning (2004) used part of speech tags as basic elements instead of words. Although this move can be motivated on data sparsity grounds, it is somewhat at odds with the lexicalized nature of dependency grammars. Since Paskin (2001)’s previous attempt using words as basic elements was unsuccessful, it is not clear whether parts of speech are necessary prior knowledge in this context. 4.2 Projectivity Projectivity is an additional constraint that may not be necessary for successful UGI. English is a projective language, but other languages, such as Bulgarian, are not (Pericliev and Ilarionov, 1986). Nonprojective UGI has not previously been studied, and it is not clear how important projectivity assumptions are to English UGI. Figure 3 gives an example of a nonprojective construction: not all heads and their dependents are a contiguous sequence. Words Distant Figure 4: Distance Between Dependents in WSJ10 4.3 Context The core of several UGI approaches is distributional analysis (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). The key idea in such distributional analysis is that the function of a word may be k</context>
</contexts>
<marker>Pericliev, Ilarionov, 1986</marker>
<rawString>Vladimir Pericliev and Ilarion Ilarionov. 1986. Testing the projectivity hypothesis. In Proceedings of the 11th International Conference on Computational Linguistics, pages 56–58, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th European Association for Computational Linguistics Conference (EACL-95),</booktitle>
<pages>141--149</pages>
<location>Philadelphia,</location>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of the 7th European Association for Computational Linguistics Conference (EACL-95), pages 141–149, Philadelphia, March 27-31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zach Solan</author>
<author>David Horn</author>
<author>Eytan Ruppin</author>
<author>Shimon Edelman</author>
</authors>
<title>Unsupervised learning of natural languages.</title>
<date>2005</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>102--11629</pages>
<contexts>
<context position="1704" citStr="Solan et al., 2005" startWordPosition="250" endWordPosition="253">tructure in text with little or no prior knowledge is therefore a fundamental issue in the study of language. However, UGI is still a largely unsolved problem. Recent work (Klein and Manning, 2002; Klein and Manning, 2004) has renewed interest by using a UGI model to parse sentences from the Wall Street Journal section of the Penn Treebank (WSJ). These parsing results are exciting because they demonstrate real-world applicability to English UGI. While other contemporary research in this area is promising, the case for real-world English UGI has not been as convincingly made (van Zaanen, 2000; Solan et al., 2005). This paper weaves together two threads of inquiry. The first thread is latent semantics, which have not been previously used in UGI. The second thread is dependency-based UGI, used by Klein and Manning (2004), which nicely dovetails with our semantic approach. The combination of these threads allows some exploration of what characteristics are sufficient for UGI and what characteristics are necessary. 2 Latent semantics Previous work has focused on syntax to the exclusion of semantics (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; S</context>
<context position="9277" citStr="Solan et al., 2005" startWordPosition="1466" endWordPosition="1469">is a projective language, but other languages, such as Bulgarian, are not (Pericliev and Ilarionov, 1986). Nonprojective UGI has not previously been studied, and it is not clear how important projectivity assumptions are to English UGI. Figure 3 gives an example of a nonprojective construction: not all heads and their dependents are a contiguous sequence. Words Distant Figure 4: Distance Between Dependents in WSJ10 4.3 Context The core of several UGI approaches is distributional analysis (Brill and Marcus, 1992; van Zaanen, 2000; Klein and Manning, 2002; Paskin, 2001; Klein and Manning, 2004; Solan et al., 2005). The key idea in such distributional analysis is that the function of a word may be known if it can be substituted for another word (Harris, 1954). If so, both words have the same function. Substitutability must be defined over a context. In UGI, this context has typically been the preceding and following words of the target word. However, this notion of context has an implicit assumption of word order. This assumption is true for English, but is not true for other languages such as Latin. Therefore, it is not clear how dependent English UGI is on local linear context, e.g. preceding and foll</context>
</contexts>
<marker>Solan, Horn, Ruppin, Edelman, 2005</marker>
<rawString>Zach Solan, David Horn, Eytan Ruppin, and Shimon Edelman. 2005. Unsupervised learning of natural languages. Proceedings of the National Academy of Sciences, 102:11629–11634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno M van Zaanen</author>
</authors>
<title>ABL: Alignment-based learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>961--967</pages>
<location>Philadelphia,</location>
<marker>van Zaanen, 2000</marker>
<rawString>Menno M. van Zaanen. 2000. ABL: Alignment-based learning. In Proceedings of the 18th International Conference on Computational Linguistics, pages 961– 967, Philadelphia, July 31-August 4. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Wiemer-Hastings</author>
<author>Iraide Zipitria</author>
</authors>
<title>Rules for syntax, vectors for semantics.</title>
<date>2001</date>
<booktitle>In Proceedings of the 23rd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1112--1117</pages>
<publisher>Erlbaum.</publisher>
<location>Mahwah, NJ,</location>
<contexts>
<context position="4579" citStr="Wiemer-Hastings and Zipitria, 2001" startWordPosition="711" endWordPosition="714">d by taking the cosine of their corresponding row vectors. Collections of words can likewise be compared by first adding the corresponding row vectors in each collection, then taking the cosine between the two collection vectors. A stumbling block to incorporating LSA into UGI is that grammars are inherently ordered but LSA is not. LSA is unordered because the sum of vectors is the same regardless of the order in which they were added. The incorporation of word order into LSA has never been successfully carried out before, although there have been attempts to apply word order post-hoc to LSA (Wiemer-Hastings and Zipitria, 2001). A straightforward notion of incorporating word order into LSA is to use n-grams instead of individual words. In this way a unigram, bigram, and trigram would each have an atomic vector representation and be directly comparable. It may seem counterintuitive that such an n-gram scheme has never been used in conjunction with LSA. Simple as this scheme may be, it quickly falls prey to memory limitations of modern day computers for computing the SVD. The standard for computing the SVD in the NLP sphere is Berry (1992)’s SVDPACK, whose single vector Lanczos recursion method with re-orthogonalizati</context>
</contexts>
<marker>Wiemer-Hastings, Zipitria, 2001</marker>
<rawString>Peter Wiemer-Hastings and Iraide Zipitria. 2001. Rules for syntax, vectors for semantics. In Proceedings of the 23rd Annual Conference of the Cognitive Science Society, pages 1112–1117, Mahwah, NJ, August 1-4. Erlbaum.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>