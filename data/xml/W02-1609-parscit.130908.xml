<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000549">
<title confidence="0.7922145">
Measuring User Acceptability of Machine Translations to
Diagnose System Errors: An Experience Report
</title>
<author confidence="0.99617">
Bowen Hui
</author>
<affiliation confidence="0.9988915">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.442524">
Canada
</address>
<email confidence="0.776791">
bowenAcs.utoronto.ca
</email>
<sectionHeader confidence="0.988334" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818095238095">
Conventional ways of measuring machine transla-
tion quality compares the accuracy of system output
without clearly specifying what &amp;quot;accuracy&amp;quot; entails.
Many current evaluation methods suffer from requir-
ing too much time commitment from expert human
evaluators. Moreover, these methods do not give di-
rect feedback on user acceptability of the system,
and do not hint on areas of focus for researchers or
developers. In this work, we explore an output in-
spection method that measures user acceptance and
pokes at system errors so that developers and re-
searchers can walk away knowing what was accept-
able and what to improve on. The evaluation frame-
work for machine translation is described and exper-
imental results for two systems are presented. The
results of the experiments are very encouraging. We
provide a discussion on identifying important trans-
lation quality factors for users, a pilot study of run-
ning this evaluation in the text summarization do-
main, and ideas on how to use the gathered data to
create user profiles.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991975">
Many researchers have criticized and proposed eval-
uation techniques for natural language (NL) systems
(Sparck-Jones, 1996; Dorr et al., 1999), and espe-
cially for areas such as machine translation (MT)
where there is no single correct answer for a given
text. Thus, conventional ways of measuring preci-
sion and recall become misleading and uninformative
for the untrained consumer or average user of MT.
In fact, &amp;quot;so-called evaluations of MT technology ...
[give] claims of upwards of 90% accuracy for systems,
without a clear specification of what &apos;accuracy&apos; en-
tails&amp;quot; (Miller, 2000). Moreover, most existing eval-
uations suffer from two major deficiencies: they do
not measure user acceptance of translation quality
and do not &amp;quot;provide the slightest hint about the ease
with which the system can be extended or modi-
fied&amp;quot; (King and Falkedal, 1990). Possible audiences
who are interested in system evaluation outcomes in-
clude users, developers, and managers; however, cur-
rent methods tend to concentrate on getting results
for developers. Most existing MT evaluations focus
on gathering fine-grained results that either require
too much time or are cognitively overwhelming and
labour-intensive for non-expert translators to com-
plete. For example, some methods ask evaluators to
identify and correct errors of many translation pas-
sages, some ask evaluators to rank translation qual-
ity based on finely differentiated criteria, and some
assess system performance indirectly via evaluators&apos;
intelligence (Carroll, 1966; White et al., 1994; Bohan
et al., 2000; Hovy, 1999). From her critical account
of MT evaluations (King, 1997), King proposes that
researchers focus on developing methods that allow
users and developers understand the quality of an
MT system and allow developers to relate the eval-
uation results to fixing system errors. We believe
these are exceedingly important factors in research
technologies, and proposes to approach MT evalua-
tion that attempts to address these issues.
In this work, we explore an inspection method,
called the heuristic evaluation, that measures user
acceptance by implicitly asking users to diagnose
system misbehaviour. The evaluation method is pre-
sented to the user as a free-trial of a system and
survey — a concept that non-experts are already fa-
miliar with. Also, the evaluation groups the results
in terms of system functionality, so that the results
can be used quickly by developers to fix the prob-
lems. This method is an attempt to directly address
the need of assessing user acceptance of NL systems
and to provide useful development directions for re-
searchers at the same time. Adapting this frame-
work for MT is described in Sections 2 and 3 and
experimental results for two systems are presented
in Section 4. The results suggest that heuristic eval-
uation has clear advantages over existing NL evalu-
ations and is worth investigating further. A discus-
sion on identifying translation quality factors that
are important to users is provided in Section 5. We
believe that this framework can be generalized to
other NL domains, and demonstrate this with a pi-
lot study for a text summarization (TS) system in
Section 6.
An interesting way of using the evaluation data
</bodyText>
<figure confidence="0.993555222222222">
0 5 10 15
Number of Evaluators
Proportion of
Usability Problems Found
100%
75%
50%
25%
0%
</figure>
<bodyText confidence="0.719912">
the original text is reproduced in the transla-
tions.
</bodyText>
<listItem confidence="0.994148083333333">
7. Fit For Audience: The information and the
style of presentation fit the intended audience.
The same group of audience (e.g., children,
politicians) intended in the original language is
also the audience of the translated language.
Cultural or linguistic differences are therefore
also &amp;quot;translated&amp;quot;.
8. Accountability: The kinds and frequency of
errors (punctuation, words, syntax, style) are
tolerable. Readers are generally satisfied with
the translation and are likely to recommend the
system to other users.
</listItem>
<bodyText confidence="0.999846545454545">
In this way, a principle receiving an unfavourable
score and comments indicates specific modules for
further development. We intend for these princi-
ples to be refined through iterations of applying this
method in MT. For example, special-purpose MT
systems may include a principle that addresses the
quality of the translation of domain-specific termi-
nology. In Section 5, we reflect on our evaluators&apos; ex-
perience using this method, and suggest that there is
a smaller set of translation criteria that are relevant
to users.
</bodyText>
<subsectionHeader confidence="0.999093">
3.2 Defining the Task
</subsectionHeader>
<bodyText confidence="0.9982395">
To help evaluators examine translations, they were
asked to answer the following questions:
</bodyText>
<listItem confidence="0.995220583333333">
1. What is the genre exhibited in the writing (e.g.,
story, advertisement, instructions, diary entry,
job posting, etc.)?
2. What is the purpose of this writing (intended
by the author)?
3. Suggest some intended audience for this writ-
ing (e.g., children, students, athletes, computer
users, photographers, etc.).
4. List the entities (people or objects) involved or
discussed by the author.
5. What would be a coherent sentence that follows
the excerpt, based on what you have read?
</listItem>
<bodyText confidence="0.999748666666667">
In essence, well-known tasks, such as the Shan-
non Game and the Classification Game (Hovy and
Marcu, 1998; Teufel, 2001; Hirao et al., 2001), can
be used as well, so long as the task allows evaluators
to go through enough of the output to comment on
each principle afterwards. This convenience holds
for designing tasks for other NL systems.
Experimenters may decide to ask the evaluators to
provide answers to the questions in the task, if they
would like to measure the accuracy of the answers
and the time taken for them to complete the task.
However, in a heuristic evaluation, the focus is on the
principles — which is the novel aspect that current
NL evaluations lack. Therefore, we focus our results
and discussion on the use of these principles only.
</bodyText>
<subsectionHeader confidence="0.998512">
3.3 Selecting Test Material
</subsectionHeader>
<bodyText confidence="0.9993725">
In total, we selected passages that exhibit a wide
range of styles and language usage from four genres:
</bodyText>
<listItem confidence="0.99909425">
1. comic descriptions — humour, irony, satire
2. fairy tale — narrative, figurative, dialogue
3. medicinal instructions — technical, special terms
4. movie review — colloquial, dialogue, slang
</listItem>
<bodyText confidence="0.932583">
For each genre, we took 2 samples, labeled A and
B. Table 1 shows the number of words (w) and sen-
tences (s) for each sample.
</bodyText>
<table confidence="0.630603">
lA 1B 2A 2B 3A 3B 4A 4B
w 146 180 326 342 87 90 245 242
s 8 9 19 15 13 9 17 13
</table>
<tableCaption confidence="0.981363">
Table 1: Number of Words/Sentences Per Sample
</tableCaption>
<sectionHeader confidence="0.987576" genericHeader="introduction">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999962875">
The systems we evaluated were Babel Fish&apos; (here-
after referred to as System 1) and Pratique3 (here-
after, System 2) focusing on translations from En-
glish to French. A total of 283 participants were
divided into 4 groups evenly — group 1 evaluated
samples 1A,2A,3A,4A for System 1; group 2 evalu-
ated 1B,2B,3B,4B for System 1; group 3 evaluated
1A,2A,3A,4A for System 2; and group 4 evaluated
1B,2B,3B,4B for System 2. Thus, each system had a
total of 11,606 words or 721 sentences as evaluation
material.
First, participants were given a description of the
principles and question-answering task (cf. Sections
3.1 and 3.2). Then, they were asked to complete the
task for 4 French samples. For each sample, they
were asked to rate each principle on a scale of 5
and provide comments while having access to both
the English and French texts. Participants spent
between 30 minutes to 2 hours to complete the eval-
uation.
Figure 2 shows the total acceptability score for
the two systems with respect to each principle. The
score is calculated by Ein_i si, where n = 14 is
the number of evaluators per system and si is the
</bodyText>
<footnote confidence="0.9944585">
&apos;Available at http: //babelf ish.altavista. com/.
2Available at http: //chaines . free . fr/traduct ion/.
</footnote>
<bodyText confidence="0.9037018">
3Although Nielsen demonstrated that 5 evaluators is
enough for identifying 75% of system problems (cf. Figure
1), we decided to use a large number of evaluators because
we want to be able to achieve statistical convergence on our
results.
score given by an individual evaluator. The max-
imum score is 560 (4 groups x 7 participants per
group x 4 samples each x 5 points). Figure 3 shows
the average acceptability score under the same per-
spective. These results indicate that principles 1, 2,
5, and 8 are especially low. Indeed, many evalua-
tors made lists of corrections to words and phrases
and commented: &amp;quot;Big problems in conjugating the
verbs&amp;quot;, &amp;quot;Important words are translated so wrong
that the point is completely missed&amp;quot;, &amp;quot;no agree-
ment&amp;quot;. Consequently, consistency and accountabil-
ity suffered (e.g., accompanying comments such as
&amp;quot;because of word translations&amp;quot;, &amp;quot;syntax problems
have to be overcome first to ensure easy compre-
hensibility&amp;quot;).
</bodyText>
<figure confidence="0.992119076923077">
450. •
400
350
300 -
250
200
150
100
50 -
0
1 2 3 4 5 6 7 13
Principles
Fk— System 1 g- System 2 Bath
</figure>
<figureCaption confidence="0.927106">
Figure 2: MT Evaluation Results
</figureCaption>
<figure confidence="0.846086375">
3 5 -
E3
25
...00•11
0
1 2 3 4 5 6 7 a
101111114111).
Sysion 1 4. Srxim 2 ELM
</figure>
<figureCaption confidence="0.993362">
Figure 3: Average System Scores: System 1 (blue),
System 2 (red), Both (yellow).
</figureCaption>
<bodyText confidence="0.994133">
With the exception of Principle 7, Fit for Audi-
ence, System 1 scored higher than System 2. In fact,
calculating the statistical significance of the average
of all the scores showed that System 1 is more ac-
ceptable than System 2 (p &lt; 0.05) and that the four
genres contributed to this significance (p &lt; 0.001).
We wanted to also compare the two systems when
genre is conditioned. We found that System 1 is
significantly better than System 2 (p &lt; 0.01) in
translating the fairy tale, medicinal instructions, and
movie review genres.
</bodyText>
<sectionHeader confidence="0.997744" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999961088888889">
Finding fluent speakers of French and English to vol-
unteer 1 to 2 hours of time for this experiment was
not difficult. In most cases, we imagine that if En-
glish were one of the languages under investigation,
it would not be very hard to find bilingual speakers.
Due to the nature of these experiments, evaluators
need not be translation experts nor domain experts
of the test materials. Although the quantitative re-
sults obtained from these experiments are objective,
King (King, 1997) suggests that using a large sample
population may be amenable. In this experiment,
we used 28 evaluators and showed statistical conver-
gence on their agreements with the acceptability of
a system.
Conducting this experiment proved that the pro-
cedure and analysis was not demanding on the ex-
perimenter at all. Once ready, the procedure can be
repeated by different evaluators without changing
the set-up. Depending on the scale of the tests, the
experimenter may wish to add more test materials,
which is also easy to modify. However, one evaluator
found the experiment very overwhelming in terms of
the terminology used. When asked for further feed-
back on the experimental procedure, about of the
evaluators voiced concern of having redundant prin-
ciples or encountering difficulty in attributing an er-
ror to a particular principle. When assigning scores
and determining which principles explained a mis-
take in the translation, evaluators found that they
only had an intuition as to why a translation is bad.
Thus, some principles that they considered to be
similar were treated the same and received similar
scores. This problem has been documented before
although we did not expected it to surface with as
few as 8 metrics. To address this issue, common fac-
tor analysis was used as a first step to minimizing
the number of factors that play a role in explaining
the findings. Usually, the number of factors is deter-
mined by a combination of several criteria (Loehlin,
1992). Based on the Kaiser criterion, scree test, and
interpretability, factor analysis suggests that the ac-
ceptability data can be explained by 4 factors un-
derlying 8 principles. Further work to identify the
relevant factors and their inter-correlations needs to
be done.
</bodyText>
<sectionHeader confidence="0.614256" genericHeader="method">
6 Second Test-bed: Summarization
</sectionHeader>
<bodyText confidence="0.999988454545454">
To test the generalizability of the evaluation frame-
work, we defined the approach for TS systems. A
pilot experiment was conducted using a system that
automatically extracts information from specialized
text documents and presents the information in
point-form organized by a graphical hierarchy of
concepts (Hui and Yu, 2002). The main objective
of this system is to allow users to learn and find in-
formation in documents quickly and easily. Next,
we turn to the design of principles, task, and test
materials, following the same format as in Section 3.
</bodyText>
<subsectionHeader confidence="0.99729">
6.1 Principles
</subsectionHeader>
<bodyText confidence="0.99981025">
After amalgamating criteria for the &amp;quot;worthiness&amp;quot; of
text summaries (Sparck-Jones and Galliers, 1996;
Sparck-Jones, 1996; Hovy and Marcu, 1998), the fol-
lowing set of principles were chosen.
</bodyText>
<listItem confidence="0.973517175">
1. Conciseness: Components should not contain
information that is irrelevant or redundant. Ev-
ery extra unit of information competes with
the relevant units of information and diminishes
their relative visibility. All information should
appear in a natural and logical order.
2. Retention: Information retained in the sys-
tem output should be representative of the key
concepts and main points made in the original
document. Are the major objectives of the pa-
per captured in the summary? What about the
major steps in the proposed solution and the
results?
3. Coherence: All information should be coher-
ent within each component as well as the over-
all summary. Sentences need not be perfectly
grammatical, but each point should make sense
in its context.
4. Consistency: Each component should be ex-
pressed clearly in words, phrases, and concepts
consistent with those in the original document.
Users should not have to wonder whether differ-
ent words, situations, or actions mean the same
thing.
5. Informativeness: Information should be pre-
sented in a useful and easily accessible way.
Some interface issues may be influential here as
well. Irrelevant information should be omitted
and words should not clutter the display of the
information.
6. Comprehensibility: Each point of informa-
tion should be easy to understand. Users should
not have to look up related information in an-
other part of the system in order to understand
a particular component.
7. Fit For Audience: The information and the
style of presentation fits for the intended audi-
ence. Audience may vary in their experience
with domain knowledge. Access to different
kinds of information should be easy and clear.
</listItem>
<bodyText confidence="0.921612333333333">
The ability to show, modify, and hide informa-
tion should be made obvious to the users.
8. Fit For Purpose: The information and
the style of presentation fits for the intended
task (e.g., question-answering) or purpose (fast
learning, easy to read).
</bodyText>
<subsectionHeader confidence="0.997869">
6.2 Task
</subsectionHeader>
<bodyText confidence="0.9481416">
In this experiment, we designed a question-
answering task that is modeled after the job of a
conference referee. This way, evaluators acted as re-
viewers using only the system output. The questions
used follow:
</bodyText>
<listItem confidence="0.988294111111111">
1. What is the problem addressed by this work?
Does it describe why the problem is significant?
2. Does the work present the approach taken to
solve the problem targeted? Is the design or
implementation of a system described in terms
of key ideas of the approach?
3. What are the contributions of this work? Are
the benefits and limitations clear? Are the re-
sults positive or negative?
</listItem>
<subsectionHeader confidence="0.999923">
6.3 Test Materials
</subsectionHeader>
<bodyText confidence="0.9317485">
Excerpts were extracted as input to the system.
In particular, abstract and summary sections from
3 patents (P1,P2,P3) and abstract, introduction,
and conclusion sections from 3 scientific articles
(51,52,53) were used. The number of words (w) and
sentences (s) for each excerpt are shown in Table 2.
</bodyText>
<table confidence="0.995014333333333">
P1 Si P2 S2 P3 S3
w 828 257 1041 688 512 709
s 27 16 27 34 23 22
</table>
<tableCaption confidence="0.999265">
Table 2: Number of Words/Sentences Per Sample
</tableCaption>
<bodyText confidence="0.999808833333333">
Excerpts P1 and 51, totaling to 3255 words, or
129 sentences, were evaluated by 3 participants and
the others, totaling 11,800 words, or 424 sentences,
were evaluated by 4 participants. Thus, a total of
7 participants evaluated the system output which
yields a total of 15,055 words, or 553 sentences.
</bodyText>
<subsectionHeader confidence="0.801195">
6.4 Results
</subsectionHeader>
<bodyText confidence="0.999300416666667">
Seven participants were presented with the descrip-
tion of the principles and the question-answering
task (cf. Sections 6.1 and 6.2). Then, they were
asked to complete the task for each of the summa-
rized samples. For each sample, they were asked to
rate each principle on a scale of 5 and provide com-
ments while having access to the original document
and the graphical summary. Each participant spent
about 1 to 1.5 hour for the entire session. Figure
4 shows the sum of all the scores (9 documents x 5
points = 45 maximum) for each principle in terms
of scientific articles and patents.
</bodyText>
<figure confidence="0.9165564">
50
WWI
2 4 6 7
Principles
• scietili* sulkies • peiertt I
</figure>
<figureCaption confidence="0.999807">
Figure 4: TS Evaluation Results
</figureCaption>
<bodyText confidence="0.94485695">
Figure 4 shows that scientific articles were gener-
ally more accepted than patents. Principle 6, com-
prehensibility, has the lowest score which indicates
that the inaccuracy of extracting sentences into the
right concepts dampened the acceptance of this cri-
teria. The low score on Principle 1 (conciseness) in-
dicates that more heuristics should be incorporated
so that the resulting text is more condensed.
Due to the small number of observations in this
experiment, statistical significance was not found be-
tween the two document types. However, when the
same data is duplicated 4 times to increase data size
(but preserving the pattern of the results), scien-
tific articles were statistically significantly more ac-
ceptable than patents (p &lt; 0.001). Furthermore,
evidence from the Kaiser criterion, scree test, and
interpretability suggest that the acceptability data
can be explained by 5 factors underlying 8 princi-
ples. Further work is necessary to identify these fac-
tors and how they interplay.
</bodyText>
<sectionHeader confidence="0.972206" genericHeader="evaluation">
7 Conclusions and Extensions
</sectionHeader>
<bodyText confidence="0.999943768115943">
We advocated for the need of measuring usability
of NL system output to assess user satisfaction with
the translation quality. We adapted a heuristic eval-
uation method for comparative MT evaluation and
extended the framework to evaluate text summariz-
ers. Through experimenting with 28 human evalua-
tors for MT and 7 for TS, our experience point out
many features that this method effectively assesses
user acceptance of a system; compares user prefer-
ences of multiple systems; is not time consuming for
the experimenter; requires about 1 to 2 hours of an
evaluator&apos;s time; is not cognitively overwhelming for
evaluators; that the quantitative data analysis can
be automated; the qualitative analysis gives insight
to system problems for developers; a summary of re-
sults can be used to generate survey results for con-
sumers; and changing the principles and task of the
framework according to application works well for
NL systems that do not have a gold standard. Com-
paring with the difficulties faced by existing evalua-
tions (Sparck-Jones and Galliers, 1996; King, 1997;
Reeder and Hovy, 2000; Hovy and Marcu, 1998; Jing
et al., 1998) as mentioned in Section 1, the effective-
ness of this method is remarkably encouraging.
Establishing translation principles. Cur-
rently, as an extension to the discussion in Section
5, we are working on using methods such as con-
firmatory factor analysis and principle components
analysis to identify the core set of evaluation princi-
ples that are important to human users.
User profiling. We are very interested in us-
ing the collected data in these experiments as pref-
erences of translational criteria elicited from users.
Discussion on user demographics (e.g. native vs.
non-native French speakers) was not provided be-
cause we wanted to focus on how the data may be ex-
plained by individual translational preferences, not
by personal background as is usually done. Although
not designed in our experiments, we are currently
collecting data where users rank the evaluation prin-
ciples to reflect which criteria are more important to
them. (Doing so also gives an indication of errors
that are more forgivable.) To indicate an emphasis
on certain principles, weights that designate &amp;quot;im-
portance&amp;quot; can be assigned to them. In particular,
for k principles, weights wi, wk must satisfy
0 &lt; w3 &lt;1 and Ejk w3 = 1. Then, to determine a
single, overall score for a system, we take:
Therefore, each user&apos;s criteria preference can be rep-
resented by a weight vector. Cluster analysis can
create meaningful user groups based on user prefer-
ences over language task rather than demographics
and software can be customized for users who share
language preferences for an MT application.
Component selection. A syntactic module may
score well in one MT system while a stylistic mod-
ule may do better in another. Ideally, one would
want to pick and choose the individual components
that perform well in different systems. Since the
evaluation results are grouped in terms of system
functionality, by comparing user acceptance across
multiple systems allows us to identify which module
from which system works well. From there, we can
select the &amp;quot;best&amp;quot; (most widely accepted) components
from different MT systems and combine them into
one abstract machine. Obviously there will be in-
compatibility issues, however, this analysis will pro-
vide interesting insights on analyzing current MT
methodology.
</bodyText>
<sectionHeader confidence="0.99152" genericHeader="conclusions">
Appendix A: Output Characteristics
</sectionHeader>
<bodyText confidence="0.95864125">
Quality of translation: (a) quality of text as a whole —
acceptability to the end user, clarity, coherence, compre-
hensibility, consistency, fidelity, informativeness, read-
ability, style, terminology, utility of output; (b) quality
of each individual sentence — morphology, syntax (sen- K.
tence and phrase structure). Errors: (a) diction errors;
(b) punctuation errors; (c) syntax errors; (d) stylistic
errors. S.
</bodyText>
<sectionHeader confidence="0.984582" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997294867647059">
N. Bohan, E. Breidt, and M. Volk. 2000. Evaluating
translation quality as input to product development.
In Proceedings of 2nd International Conference on
Language Resources and Evaluation, Athens, Greece.
J.B. Carroll. 1966. An experiment in evaluating the
quality of translations. Mechanical Translation, 9(3-
4):55-66.
B. Dorr, P.W. Jordan, and J.W. Benoit. 1999. A Sur-
vey of Current Research in Machine Translation. Ad-
vances in Computers, M. Zelkowitz (ed), 49:1-68.
EAGLES, 1994. Interim Report. Obtainable from Cen-
ter for Language Technology, Njalsgade 80, DK 2300
Copenhagen.
T. Hirao, Y. Sasaki, and H. Isozaki. 2001. An Extrinsic
Evaluation for Question-Biased Text Summarization
on QA Tasks. In NAACL Workshop on Automatic
Summarization, pages 61-68.
E. Hovy and D. Marcu, 1998. Automated Text
Summarization: Tutorial Notes. COLING-ACL&apos;98,
Montreal, Canada.
E. Hovy. 1999. Toward Finely Differentiated Evaluation
Metrics for Machine Translation. In EAGLES Work-
shop on Standards and Evaluation, Pisa, Italy.
B. Hui and E. Yu. 2002. Extracting Conceptual Re-
lationships from Specialized Documents. In 21st In-
ternational Conference on Conceptual Modeling (ER
2002), To appear. Tampere, Finland.
H. Jing, R. Barzilay, K. McKeown, and M. Elhadad.
1998. Summarization Evaluation Methods: Experi-
ments and Analysis. In AAAI Intelligent Text Sum-
marization Workshop, pages 60-68.
M. King and K. Falkedal. 1990. Using test suites in eval-
uation of machine translation systems. In Proceedings
of the 19th Conference of COLING.
M. King. 1997. Evaluating translation. In C. Hauen-
schild 14 S. Heizmann (eds.), Machine Translation and
Translation Theory. Walter de Gruyter Sz Co.: Berlin.
J.C. Loehlin. 1992. Latent Variable Models. Erlbaum
Associates, Hillsdale NJ.
K. Miller. 2000. The Lexical Choice of Prepositions in
Machine Translation. Ph.D. thesis, Georgetown Uni-
versity, Maryland, USA.
J. Nielsen. 1993. Usability Engineering. Academic
Press, Inc.
E.H. Nyberg, T. Mitamura, and J.G. Carbonnell.
1994. Evaluation Metrics for Knowledge-Based Ma-
chine Translation. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics
(COLING&apos;94), pages 95-99, Kyoto, Japan.
F. Reeder and E. Hovy, 2000. Workshop on Machine
Translation Evaluation. AMTA-00, October.
K. Sparck-Jones and J.R. Galliers. 1996. Evaluating
Natural Language Processing Systems: An Analysis
and Review. New York: Springer.
Sparck-Jones. 1996. Towards Better NLP System
Evaluation. In Proceedings of the Human Language
Technology Workshop, pages 102-107. ARPA.
Teufel. 2001. Task-Based Evaluation of Summary
Quality: Describing Relationships Between Scientific
Papers. In NAACL Workshop on Automatic Summa-
rization.
J.S. White, T. O&apos;Connell, and F.E. O&apos;Mara. 1994.
The ARPA MT evaluation methodologies: Evolution,
lessons and further approaches. In Technology part-
nerships for corssing the language barrier: Proceed-
ings of the first conference of the Association for Ma-
chine Translation in the Americas, pages 193-205,
Columbia, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.860092">
<title confidence="0.9565195">Measuring User Acceptability of Machine Translations Diagnose System Errors: An Experience Report</title>
<author confidence="0.957024">Bowen</author>
<affiliation confidence="0.998835">Department of Computer University of</affiliation>
<email confidence="0.964569">bowenAcs.utoronto.ca</email>
<abstract confidence="0.999701181818182">Conventional ways of measuring machine translation quality compares the accuracy of system output without clearly specifying what &amp;quot;accuracy&amp;quot; entails. Many current evaluation methods suffer from requiring too much time commitment from expert human evaluators. Moreover, these methods do not give direct feedback on user acceptability of the system, and do not hint on areas of focus for researchers or developers. In this work, we explore an output inspection method that measures user acceptance and pokes at system errors so that developers and researchers can walk away knowing what was acceptable and what to improve on. The evaluation framework for machine translation is described and experimental results for two systems are presented. The results of the experiments are very encouraging. We provide a discussion on identifying important translation quality factors for users, a pilot study of running this evaluation in the text summarization domain, and ideas on how to use the gathered data to create user profiles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bohan</author>
<author>E Breidt</author>
<author>M Volk</author>
</authors>
<title>Evaluating translation quality as input to product development.</title>
<date>2000</date>
<booktitle>In Proceedings of 2nd International Conference on Language Resources and Evaluation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2793" citStr="Bohan et al., 2000" startWordPosition="426" endWordPosition="429">lopers, and managers; however, current methods tend to concentrate on getting results for developers. Most existing MT evaluations focus on gathering fine-grained results that either require too much time or are cognitively overwhelming and labour-intensive for non-expert translators to complete. For example, some methods ask evaluators to identify and correct errors of many translation passages, some ask evaluators to rank translation quality based on finely differentiated criteria, and some assess system performance indirectly via evaluators&apos; intelligence (Carroll, 1966; White et al., 1994; Bohan et al., 2000; Hovy, 1999). From her critical account of MT evaluations (King, 1997), King proposes that researchers focus on developing methods that allow users and developers understand the quality of an MT system and allow developers to relate the evaluation results to fixing system errors. We believe these are exceedingly important factors in research technologies, and proposes to approach MT evaluation that attempts to address these issues. In this work, we explore an inspection method, called the heuristic evaluation, that measures user acceptance by implicitly asking users to diagnose system misbeha</context>
</contexts>
<marker>Bohan, Breidt, Volk, 2000</marker>
<rawString>N. Bohan, E. Breidt, and M. Volk. 2000. Evaluating translation quality as input to product development. In Proceedings of 2nd International Conference on Language Resources and Evaluation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Carroll</author>
</authors>
<title>An experiment in evaluating the quality of translations.</title>
<date>1966</date>
<journal>Mechanical Translation,</journal>
<pages>9--3</pages>
<contexts>
<context position="2753" citStr="Carroll, 1966" startWordPosition="420" endWordPosition="421">uation outcomes include users, developers, and managers; however, current methods tend to concentrate on getting results for developers. Most existing MT evaluations focus on gathering fine-grained results that either require too much time or are cognitively overwhelming and labour-intensive for non-expert translators to complete. For example, some methods ask evaluators to identify and correct errors of many translation passages, some ask evaluators to rank translation quality based on finely differentiated criteria, and some assess system performance indirectly via evaluators&apos; intelligence (Carroll, 1966; White et al., 1994; Bohan et al., 2000; Hovy, 1999). From her critical account of MT evaluations (King, 1997), King proposes that researchers focus on developing methods that allow users and developers understand the quality of an MT system and allow developers to relate the evaluation results to fixing system errors. We believe these are exceedingly important factors in research technologies, and proposes to approach MT evaluation that attempts to address these issues. In this work, we explore an inspection method, called the heuristic evaluation, that measures user acceptance by implicitly</context>
</contexts>
<marker>Carroll, 1966</marker>
<rawString>J.B. Carroll. 1966. An experiment in evaluating the quality of translations. Mechanical Translation, 9(3-4):55-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
<author>P W Jordan</author>
<author>J W Benoit</author>
</authors>
<title>A Survey of Current Research</title>
<date>1999</date>
<booktitle>in Machine Translation. Advances in Computers, M. Zelkowitz (ed),</booktitle>
<pages>49--1</pages>
<contexts>
<context position="1382" citStr="Dorr et al., 1999" startWordPosition="210" endWordPosition="213">alk away knowing what was acceptable and what to improve on. The evaluation framework for machine translation is described and experimental results for two systems are presented. The results of the experiments are very encouraging. We provide a discussion on identifying important translation quality factors for users, a pilot study of running this evaluation in the text summarization domain, and ideas on how to use the gathered data to create user profiles. 1 Introduction Many researchers have criticized and proposed evaluation techniques for natural language (NL) systems (Sparck-Jones, 1996; Dorr et al., 1999), and especially for areas such as machine translation (MT) where there is no single correct answer for a given text. Thus, conventional ways of measuring precision and recall become misleading and uninformative for the untrained consumer or average user of MT. In fact, &amp;quot;so-called evaluations of MT technology ... [give] claims of upwards of 90% accuracy for systems, without a clear specification of what &apos;accuracy&apos; entails&amp;quot; (Miller, 2000). Moreover, most existing evaluations suffer from two major deficiencies: they do not measure user acceptance of translation quality and do not &amp;quot;provide the sl</context>
</contexts>
<marker>Dorr, Jordan, Benoit, 1999</marker>
<rawString>B. Dorr, P.W. Jordan, and J.W. Benoit. 1999. A Survey of Current Research in Machine Translation. Advances in Computers, M. Zelkowitz (ed), 49:1-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES</author>
</authors>
<title>Interim Report. Obtainable from Center for Language Technology, Njalsgade 80, DK 2300</title>
<date>1994</date>
<location>Copenhagen.</location>
<marker>EAGLES, 1994</marker>
<rawString>EAGLES, 1994. Interim Report. Obtainable from Center for Language Technology, Njalsgade 80, DK 2300 Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>Y Sasaki</author>
<author>H Isozaki</author>
</authors>
<title>An Extrinsic Evaluation for Question-Biased Text Summarization on QA Tasks.</title>
<date>2001</date>
<booktitle>In NAACL Workshop on Automatic Summarization,</booktitle>
<pages>61--68</pages>
<contexts>
<context position="6371" citStr="Hirao et al., 2001" startWordPosition="1001" endWordPosition="1004">What is the genre exhibited in the writing (e.g., story, advertisement, instructions, diary entry, job posting, etc.)? 2. What is the purpose of this writing (intended by the author)? 3. Suggest some intended audience for this writing (e.g., children, students, athletes, computer users, photographers, etc.). 4. List the entities (people or objects) involved or discussed by the author. 5. What would be a coherent sentence that follows the excerpt, based on what you have read? In essence, well-known tasks, such as the Shannon Game and the Classification Game (Hovy and Marcu, 1998; Teufel, 2001; Hirao et al., 2001), can be used as well, so long as the task allows evaluators to go through enough of the output to comment on each principle afterwards. This convenience holds for designing tasks for other NL systems. Experimenters may decide to ask the evaluators to provide answers to the questions in the task, if they would like to measure the accuracy of the answers and the time taken for them to complete the task. However, in a heuristic evaluation, the focus is on the principles — which is the novel aspect that current NL evaluations lack. Therefore, we focus our results and discussion on the use of thes</context>
</contexts>
<marker>Hirao, Sasaki, Isozaki, 2001</marker>
<rawString>T. Hirao, Y. Sasaki, and H. Isozaki. 2001. An Extrinsic Evaluation for Question-Biased Text Summarization on QA Tasks. In NAACL Workshop on Automatic Summarization, pages 61-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>D Marcu</author>
</authors>
<title>Automated Text Summarization: Tutorial Notes.</title>
<date>1998</date>
<location>COLING-ACL&apos;98, Montreal, Canada.</location>
<contexts>
<context position="6336" citStr="Hovy and Marcu, 1998" startWordPosition="995" endWordPosition="998"> answer the following questions: 1. What is the genre exhibited in the writing (e.g., story, advertisement, instructions, diary entry, job posting, etc.)? 2. What is the purpose of this writing (intended by the author)? 3. Suggest some intended audience for this writing (e.g., children, students, athletes, computer users, photographers, etc.). 4. List the entities (people or objects) involved or discussed by the author. 5. What would be a coherent sentence that follows the excerpt, based on what you have read? In essence, well-known tasks, such as the Shannon Game and the Classification Game (Hovy and Marcu, 1998; Teufel, 2001; Hirao et al., 2001), can be used as well, so long as the task allows evaluators to go through enough of the output to comment on each principle afterwards. This convenience holds for designing tasks for other NL systems. Experimenters may decide to ask the evaluators to provide answers to the questions in the task, if they would like to measure the accuracy of the answers and the time taken for them to complete the task. However, in a heuristic evaluation, the focus is on the principles — which is the novel aspect that current NL evaluations lack. Therefore, we focus our result</context>
<context position="13564" citStr="Hovy and Marcu, 1998" startWordPosition="2228" endWordPosition="2231">stems. A pilot experiment was conducted using a system that automatically extracts information from specialized text documents and presents the information in point-form organized by a graphical hierarchy of concepts (Hui and Yu, 2002). The main objective of this system is to allow users to learn and find information in documents quickly and easily. Next, we turn to the design of principles, task, and test materials, following the same format as in Section 3. 6.1 Principles After amalgamating criteria for the &amp;quot;worthiness&amp;quot; of text summaries (Sparck-Jones and Galliers, 1996; Sparck-Jones, 1996; Hovy and Marcu, 1998), the following set of principles were chosen. 1. Conciseness: Components should not contain information that is irrelevant or redundant. Every extra unit of information competes with the relevant units of information and diminishes their relative visibility. All information should appear in a natural and logical order. 2. Retention: Information retained in the system output should be representative of the key concepts and main points made in the original document. Are the major objectives of the paper captured in the summary? What about the major steps in the proposed solution and the results</context>
<context position="19700" citStr="Hovy and Marcu, 1998" startWordPosition="3242" endWordPosition="3245">ming for the experimenter; requires about 1 to 2 hours of an evaluator&apos;s time; is not cognitively overwhelming for evaluators; that the quantitative data analysis can be automated; the qualitative analysis gives insight to system problems for developers; a summary of results can be used to generate survey results for consumers; and changing the principles and task of the framework according to application works well for NL systems that do not have a gold standard. Comparing with the difficulties faced by existing evaluations (Sparck-Jones and Galliers, 1996; King, 1997; Reeder and Hovy, 2000; Hovy and Marcu, 1998; Jing et al., 1998) as mentioned in Section 1, the effectiveness of this method is remarkably encouraging. Establishing translation principles. Currently, as an extension to the discussion in Section 5, we are working on using methods such as confirmatory factor analysis and principle components analysis to identify the core set of evaluation principles that are important to human users. User profiling. We are very interested in using the collected data in these experiments as preferences of translational criteria elicited from users. Discussion on user demographics (e.g. native vs. non-nativ</context>
</contexts>
<marker>Hovy, Marcu, 1998</marker>
<rawString>E. Hovy and D. Marcu, 1998. Automated Text Summarization: Tutorial Notes. COLING-ACL&apos;98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Toward Finely Differentiated Evaluation Metrics for Machine Translation.</title>
<date>1999</date>
<booktitle>In EAGLES Workshop on Standards and Evaluation,</booktitle>
<location>Pisa, Italy.</location>
<contexts>
<context position="2806" citStr="Hovy, 1999" startWordPosition="430" endWordPosition="431">; however, current methods tend to concentrate on getting results for developers. Most existing MT evaluations focus on gathering fine-grained results that either require too much time or are cognitively overwhelming and labour-intensive for non-expert translators to complete. For example, some methods ask evaluators to identify and correct errors of many translation passages, some ask evaluators to rank translation quality based on finely differentiated criteria, and some assess system performance indirectly via evaluators&apos; intelligence (Carroll, 1966; White et al., 1994; Bohan et al., 2000; Hovy, 1999). From her critical account of MT evaluations (King, 1997), King proposes that researchers focus on developing methods that allow users and developers understand the quality of an MT system and allow developers to relate the evaluation results to fixing system errors. We believe these are exceedingly important factors in research technologies, and proposes to approach MT evaluation that attempts to address these issues. In this work, we explore an inspection method, called the heuristic evaluation, that measures user acceptance by implicitly asking users to diagnose system misbehaviour. The ev</context>
</contexts>
<marker>Hovy, 1999</marker>
<rawString>E. Hovy. 1999. Toward Finely Differentiated Evaluation Metrics for Machine Translation. In EAGLES Workshop on Standards and Evaluation, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hui</author>
<author>E Yu</author>
</authors>
<title>Extracting Conceptual Relationships from Specialized Documents.</title>
<date>2002</date>
<booktitle>In 21st International Conference on Conceptual Modeling (ER 2002), To appear.</booktitle>
<location>Tampere, Finland.</location>
<contexts>
<context position="13178" citStr="Hui and Yu, 2002" startWordPosition="2166" endWordPosition="2169"> the Kaiser criterion, scree test, and interpretability, factor analysis suggests that the acceptability data can be explained by 4 factors underlying 8 principles. Further work to identify the relevant factors and their inter-correlations needs to be done. 6 Second Test-bed: Summarization To test the generalizability of the evaluation framework, we defined the approach for TS systems. A pilot experiment was conducted using a system that automatically extracts information from specialized text documents and presents the information in point-form organized by a graphical hierarchy of concepts (Hui and Yu, 2002). The main objective of this system is to allow users to learn and find information in documents quickly and easily. Next, we turn to the design of principles, task, and test materials, following the same format as in Section 3. 6.1 Principles After amalgamating criteria for the &amp;quot;worthiness&amp;quot; of text summaries (Sparck-Jones and Galliers, 1996; Sparck-Jones, 1996; Hovy and Marcu, 1998), the following set of principles were chosen. 1. Conciseness: Components should not contain information that is irrelevant or redundant. Every extra unit of information competes with the relevant units of informat</context>
</contexts>
<marker>Hui, Yu, 2002</marker>
<rawString>B. Hui and E. Yu. 2002. Extracting Conceptual Relationships from Specialized Documents. In 21st International Conference on Conceptual Modeling (ER 2002), To appear. Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Barzilay</author>
<author>K McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Summarization Evaluation Methods: Experiments and Analysis.</title>
<date>1998</date>
<booktitle>In AAAI Intelligent Text Summarization Workshop,</booktitle>
<pages>60--68</pages>
<contexts>
<context position="19720" citStr="Jing et al., 1998" startWordPosition="3246" endWordPosition="3249">ter; requires about 1 to 2 hours of an evaluator&apos;s time; is not cognitively overwhelming for evaluators; that the quantitative data analysis can be automated; the qualitative analysis gives insight to system problems for developers; a summary of results can be used to generate survey results for consumers; and changing the principles and task of the framework according to application works well for NL systems that do not have a gold standard. Comparing with the difficulties faced by existing evaluations (Sparck-Jones and Galliers, 1996; King, 1997; Reeder and Hovy, 2000; Hovy and Marcu, 1998; Jing et al., 1998) as mentioned in Section 1, the effectiveness of this method is remarkably encouraging. Establishing translation principles. Currently, as an extension to the discussion in Section 5, we are working on using methods such as confirmatory factor analysis and principle components analysis to identify the core set of evaluation principles that are important to human users. User profiling. We are very interested in using the collected data in these experiments as preferences of translational criteria elicited from users. Discussion on user demographics (e.g. native vs. non-native French speakers) w</context>
</contexts>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>H. Jing, R. Barzilay, K. McKeown, and M. Elhadad. 1998. Summarization Evaluation Methods: Experiments and Analysis. In AAAI Intelligent Text Summarization Workshop, pages 60-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M King</author>
<author>K Falkedal</author>
</authors>
<title>Using test suites in evaluation of machine translation systems.</title>
<date>1990</date>
<booktitle>In Proceedings of the 19th Conference of COLING.</booktitle>
<contexts>
<context position="2086" citStr="King and Falkedal, 1990" startWordPosition="324" endWordPosition="327">ngle correct answer for a given text. Thus, conventional ways of measuring precision and recall become misleading and uninformative for the untrained consumer or average user of MT. In fact, &amp;quot;so-called evaluations of MT technology ... [give] claims of upwards of 90% accuracy for systems, without a clear specification of what &apos;accuracy&apos; entails&amp;quot; (Miller, 2000). Moreover, most existing evaluations suffer from two major deficiencies: they do not measure user acceptance of translation quality and do not &amp;quot;provide the slightest hint about the ease with which the system can be extended or modified&amp;quot; (King and Falkedal, 1990). Possible audiences who are interested in system evaluation outcomes include users, developers, and managers; however, current methods tend to concentrate on getting results for developers. Most existing MT evaluations focus on gathering fine-grained results that either require too much time or are cognitively overwhelming and labour-intensive for non-expert translators to complete. For example, some methods ask evaluators to identify and correct errors of many translation passages, some ask evaluators to rank translation quality based on finely differentiated criteria, and some assess system</context>
</contexts>
<marker>King, Falkedal, 1990</marker>
<rawString>M. King and K. Falkedal. 1990. Using test suites in evaluation of machine translation systems. In Proceedings of the 19th Conference of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M King</author>
</authors>
<title>Evaluating translation. In</title>
<date>1997</date>
<booktitle>Machine Translation and Translation Theory. Walter de Gruyter Sz Co.:</booktitle>
<editor>C. Hauenschild 14 S. Heizmann (eds.),</editor>
<location>Berlin.</location>
<contexts>
<context position="2864" citStr="King, 1997" startWordPosition="439" endWordPosition="440">results for developers. Most existing MT evaluations focus on gathering fine-grained results that either require too much time or are cognitively overwhelming and labour-intensive for non-expert translators to complete. For example, some methods ask evaluators to identify and correct errors of many translation passages, some ask evaluators to rank translation quality based on finely differentiated criteria, and some assess system performance indirectly via evaluators&apos; intelligence (Carroll, 1966; White et al., 1994; Bohan et al., 2000; Hovy, 1999). From her critical account of MT evaluations (King, 1997), King proposes that researchers focus on developing methods that allow users and developers understand the quality of an MT system and allow developers to relate the evaluation results to fixing system errors. We believe these are exceedingly important factors in research technologies, and proposes to approach MT evaluation that attempts to address these issues. In this work, we explore an inspection method, called the heuristic evaluation, that measures user acceptance by implicitly asking users to diagnose system misbehaviour. The evaluation method is presented to the user as a free-trial o</context>
<context position="11077" citStr="King, 1997" startWordPosition="1831" endWordPosition="1832"> better than System 2 (p &lt; 0.01) in translating the fairy tale, medicinal instructions, and movie review genres. 5 Discussion Finding fluent speakers of French and English to volunteer 1 to 2 hours of time for this experiment was not difficult. In most cases, we imagine that if English were one of the languages under investigation, it would not be very hard to find bilingual speakers. Due to the nature of these experiments, evaluators need not be translation experts nor domain experts of the test materials. Although the quantitative results obtained from these experiments are objective, King (King, 1997) suggests that using a large sample population may be amenable. In this experiment, we used 28 evaluators and showed statistical convergence on their agreements with the acceptability of a system. Conducting this experiment proved that the procedure and analysis was not demanding on the experimenter at all. Once ready, the procedure can be repeated by different evaluators without changing the set-up. Depending on the scale of the tests, the experimenter may wish to add more test materials, which is also easy to modify. However, one evaluator found the experiment very overwhelming in terms of t</context>
<context position="19655" citStr="King, 1997" startWordPosition="3236" endWordPosition="3237">multiple systems; is not time consuming for the experimenter; requires about 1 to 2 hours of an evaluator&apos;s time; is not cognitively overwhelming for evaluators; that the quantitative data analysis can be automated; the qualitative analysis gives insight to system problems for developers; a summary of results can be used to generate survey results for consumers; and changing the principles and task of the framework according to application works well for NL systems that do not have a gold standard. Comparing with the difficulties faced by existing evaluations (Sparck-Jones and Galliers, 1996; King, 1997; Reeder and Hovy, 2000; Hovy and Marcu, 1998; Jing et al., 1998) as mentioned in Section 1, the effectiveness of this method is remarkably encouraging. Establishing translation principles. Currently, as an extension to the discussion in Section 5, we are working on using methods such as confirmatory factor analysis and principle components analysis to identify the core set of evaluation principles that are important to human users. User profiling. We are very interested in using the collected data in these experiments as preferences of translational criteria elicited from users. Discussion on</context>
</contexts>
<marker>King, 1997</marker>
<rawString>M. King. 1997. Evaluating translation. In C. Hauenschild 14 S. Heizmann (eds.), Machine Translation and Translation Theory. Walter de Gruyter Sz Co.: Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Loehlin</author>
</authors>
<title>Latent Variable Models. Erlbaum Associates,</title>
<date>1992</date>
<location>Hillsdale NJ.</location>
<contexts>
<context position="12551" citStr="Loehlin, 1992" startWordPosition="2074" endWordPosition="2075">termining which principles explained a mistake in the translation, evaluators found that they only had an intuition as to why a translation is bad. Thus, some principles that they considered to be similar were treated the same and received similar scores. This problem has been documented before although we did not expected it to surface with as few as 8 metrics. To address this issue, common factor analysis was used as a first step to minimizing the number of factors that play a role in explaining the findings. Usually, the number of factors is determined by a combination of several criteria (Loehlin, 1992). Based on the Kaiser criterion, scree test, and interpretability, factor analysis suggests that the acceptability data can be explained by 4 factors underlying 8 principles. Further work to identify the relevant factors and their inter-correlations needs to be done. 6 Second Test-bed: Summarization To test the generalizability of the evaluation framework, we defined the approach for TS systems. A pilot experiment was conducted using a system that automatically extracts information from specialized text documents and presents the information in point-form organized by a graphical hierarchy of </context>
</contexts>
<marker>Loehlin, 1992</marker>
<rawString>J.C. Loehlin. 1992. Latent Variable Models. Erlbaum Associates, Hillsdale NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Miller</author>
</authors>
<title>The Lexical Choice of Prepositions</title>
<date>2000</date>
<booktitle>in Machine Translation. Ph.D. thesis,</booktitle>
<institution>Georgetown University,</institution>
<location>Maryland, USA.</location>
<contexts>
<context position="1823" citStr="Miller, 2000" startWordPosition="283" endWordPosition="284"> user profiles. 1 Introduction Many researchers have criticized and proposed evaluation techniques for natural language (NL) systems (Sparck-Jones, 1996; Dorr et al., 1999), and especially for areas such as machine translation (MT) where there is no single correct answer for a given text. Thus, conventional ways of measuring precision and recall become misleading and uninformative for the untrained consumer or average user of MT. In fact, &amp;quot;so-called evaluations of MT technology ... [give] claims of upwards of 90% accuracy for systems, without a clear specification of what &apos;accuracy&apos; entails&amp;quot; (Miller, 2000). Moreover, most existing evaluations suffer from two major deficiencies: they do not measure user acceptance of translation quality and do not &amp;quot;provide the slightest hint about the ease with which the system can be extended or modified&amp;quot; (King and Falkedal, 1990). Possible audiences who are interested in system evaluation outcomes include users, developers, and managers; however, current methods tend to concentrate on getting results for developers. Most existing MT evaluations focus on gathering fine-grained results that either require too much time or are cognitively overwhelming and labour-</context>
</contexts>
<marker>Miller, 2000</marker>
<rawString>K. Miller. 2000. The Lexical Choice of Prepositions in Machine Translation. Ph.D. thesis, Georgetown University, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nielsen</author>
</authors>
<title>Usability Engineering.</title>
<date>1993</date>
<publisher>Academic Press, Inc.</publisher>
<marker>Nielsen, 1993</marker>
<rawString>J. Nielsen. 1993. Usability Engineering. Academic Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Nyberg</author>
<author>T Mitamura</author>
<author>J G Carbonnell</author>
</authors>
<title>Evaluation Metrics for Knowledge-Based Machine Translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94),</booktitle>
<pages>95--99</pages>
<location>Kyoto, Japan.</location>
<marker>Nyberg, Mitamura, Carbonnell, 1994</marker>
<rawString>E.H. Nyberg, T. Mitamura, and J.G. Carbonnell. 1994. Evaluation Metrics for Knowledge-Based Machine Translation. In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94), pages 95-99, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Reeder</author>
<author>E Hovy</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review.</title>
<date>2000</date>
<booktitle>Workshop on Machine Translation Evaluation. AMTA-00, October. K. Sparck-Jones</booktitle>
<publisher>Springer.</publisher>
<location>New York:</location>
<contexts>
<context position="19678" citStr="Reeder and Hovy, 2000" startWordPosition="3238" endWordPosition="3241">tems; is not time consuming for the experimenter; requires about 1 to 2 hours of an evaluator&apos;s time; is not cognitively overwhelming for evaluators; that the quantitative data analysis can be automated; the qualitative analysis gives insight to system problems for developers; a summary of results can be used to generate survey results for consumers; and changing the principles and task of the framework according to application works well for NL systems that do not have a gold standard. Comparing with the difficulties faced by existing evaluations (Sparck-Jones and Galliers, 1996; King, 1997; Reeder and Hovy, 2000; Hovy and Marcu, 1998; Jing et al., 1998) as mentioned in Section 1, the effectiveness of this method is remarkably encouraging. Establishing translation principles. Currently, as an extension to the discussion in Section 5, we are working on using methods such as confirmatory factor analysis and principle components analysis to identify the core set of evaluation principles that are important to human users. User profiling. We are very interested in using the collected data in these experiments as preferences of translational criteria elicited from users. Discussion on user demographics (e.g</context>
</contexts>
<marker>Reeder, Hovy, 2000</marker>
<rawString>F. Reeder and E. Hovy, 2000. Workshop on Machine Translation Evaluation. AMTA-00, October. K. Sparck-Jones and J.R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. New York: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck-Jones</author>
</authors>
<title>Towards Better NLP System Evaluation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>102--107</pages>
<publisher>ARPA.</publisher>
<contexts>
<context position="1362" citStr="Sparck-Jones, 1996" startWordPosition="208" endWordPosition="209">nd researchers can walk away knowing what was acceptable and what to improve on. The evaluation framework for machine translation is described and experimental results for two systems are presented. The results of the experiments are very encouraging. We provide a discussion on identifying important translation quality factors for users, a pilot study of running this evaluation in the text summarization domain, and ideas on how to use the gathered data to create user profiles. 1 Introduction Many researchers have criticized and proposed evaluation techniques for natural language (NL) systems (Sparck-Jones, 1996; Dorr et al., 1999), and especially for areas such as machine translation (MT) where there is no single correct answer for a given text. Thus, conventional ways of measuring precision and recall become misleading and uninformative for the untrained consumer or average user of MT. In fact, &amp;quot;so-called evaluations of MT technology ... [give] claims of upwards of 90% accuracy for systems, without a clear specification of what &apos;accuracy&apos; entails&amp;quot; (Miller, 2000). Moreover, most existing evaluations suffer from two major deficiencies: they do not measure user acceptance of translation quality and do</context>
<context position="13541" citStr="Sparck-Jones, 1996" startWordPosition="2226" endWordPosition="2227">e approach for TS systems. A pilot experiment was conducted using a system that automatically extracts information from specialized text documents and presents the information in point-form organized by a graphical hierarchy of concepts (Hui and Yu, 2002). The main objective of this system is to allow users to learn and find information in documents quickly and easily. Next, we turn to the design of principles, task, and test materials, following the same format as in Section 3. 6.1 Principles After amalgamating criteria for the &amp;quot;worthiness&amp;quot; of text summaries (Sparck-Jones and Galliers, 1996; Sparck-Jones, 1996; Hovy and Marcu, 1998), the following set of principles were chosen. 1. Conciseness: Components should not contain information that is irrelevant or redundant. Every extra unit of information competes with the relevant units of information and diminishes their relative visibility. All information should appear in a natural and logical order. 2. Retention: Information retained in the system output should be representative of the key concepts and main points made in the original document. Are the major objectives of the paper captured in the summary? What about the major steps in the proposed s</context>
</contexts>
<marker>Sparck-Jones, 1996</marker>
<rawString>Sparck-Jones. 1996. Towards Better NLP System Evaluation. In Proceedings of the Human Language Technology Workshop, pages 102-107. ARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teufel</author>
</authors>
<title>Task-Based Evaluation of Summary Quality: Describing Relationships Between Scientific Papers.</title>
<date>2001</date>
<booktitle>In NAACL Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="6350" citStr="Teufel, 2001" startWordPosition="999" endWordPosition="1000">questions: 1. What is the genre exhibited in the writing (e.g., story, advertisement, instructions, diary entry, job posting, etc.)? 2. What is the purpose of this writing (intended by the author)? 3. Suggest some intended audience for this writing (e.g., children, students, athletes, computer users, photographers, etc.). 4. List the entities (people or objects) involved or discussed by the author. 5. What would be a coherent sentence that follows the excerpt, based on what you have read? In essence, well-known tasks, such as the Shannon Game and the Classification Game (Hovy and Marcu, 1998; Teufel, 2001; Hirao et al., 2001), can be used as well, so long as the task allows evaluators to go through enough of the output to comment on each principle afterwards. This convenience holds for designing tasks for other NL systems. Experimenters may decide to ask the evaluators to provide answers to the questions in the task, if they would like to measure the accuracy of the answers and the time taken for them to complete the task. However, in a heuristic evaluation, the focus is on the principles — which is the novel aspect that current NL evaluations lack. Therefore, we focus our results and discussi</context>
</contexts>
<marker>Teufel, 2001</marker>
<rawString>Teufel. 2001. Task-Based Evaluation of Summary Quality: Describing Relationships Between Scientific Papers. In NAACL Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S White</author>
<author>T O&apos;Connell</author>
<author>F E O&apos;Mara</author>
</authors>
<title>The ARPA MT evaluation methodologies: Evolution, lessons and further approaches. In Technology partnerships for corssing the language barrier:</title>
<date>1994</date>
<booktitle>Proceedings of the first conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>193--205</pages>
<location>Columbia, USA.</location>
<contexts>
<context position="2773" citStr="White et al., 1994" startWordPosition="422" endWordPosition="425"> include users, developers, and managers; however, current methods tend to concentrate on getting results for developers. Most existing MT evaluations focus on gathering fine-grained results that either require too much time or are cognitively overwhelming and labour-intensive for non-expert translators to complete. For example, some methods ask evaluators to identify and correct errors of many translation passages, some ask evaluators to rank translation quality based on finely differentiated criteria, and some assess system performance indirectly via evaluators&apos; intelligence (Carroll, 1966; White et al., 1994; Bohan et al., 2000; Hovy, 1999). From her critical account of MT evaluations (King, 1997), King proposes that researchers focus on developing methods that allow users and developers understand the quality of an MT system and allow developers to relate the evaluation results to fixing system errors. We believe these are exceedingly important factors in research technologies, and proposes to approach MT evaluation that attempts to address these issues. In this work, we explore an inspection method, called the heuristic evaluation, that measures user acceptance by implicitly asking users to dia</context>
</contexts>
<marker>White, O&apos;Connell, O&apos;Mara, 1994</marker>
<rawString>J.S. White, T. O&apos;Connell, and F.E. O&apos;Mara. 1994. The ARPA MT evaluation methodologies: Evolution, lessons and further approaches. In Technology partnerships for corssing the language barrier: Proceedings of the first conference of the Association for Machine Translation in the Americas, pages 193-205, Columbia, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>