<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001511">
<title confidence="0.991074">
Fast LR Parsing Using Rich (Tree Adjoining) Grammars
</title>
<author confidence="0.98417">
Carlos A. Prolo
</author>
<affiliation confidence="0.998702">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<email confidence="0.998019">
prolo@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999909">
We describe an LR parser of parts-of-
speech (and punctuation labels) for Tree
Adjoining Grammars (TAGs), that solves
table conflicts in a greedy way, with lim-
ited amount of backtracking. We evaluate
the parser using the Penn Treebank show-
ing that the method yield very fast parsers
with at least reasonable accuracy, confirm-
ing the intuition that LR parsing benefits
from the use of rich grammars.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999414796296296">
The LR approach for parsing has long been con-
sidered for natural language parsing (Lang, 1974;
Tomita, 1985; Wright and Wrigley, 1991; Shieber,
1983; Pereira, 1985; Merlo, 1996), but it was not
until a more recent past, with the advent of corpus-
based techniques made possible by the availability
of large treebanks, that parsing results and evalu-
ation started being reported (Briscoe and Carroll,
1993; Inui et al., 1997; Carroll and Briscoe, 1996;
Ruland, 2000).
The appeal of LR parsing (Knuth, 1965) derives
from its high capacity of postponement of struc-
tural decisions, therefore allowing for much of the
spurious local ambiguity to be automatically dis-
carded. But it is still the case that conflicts arise in
the LR table for natural language grammars, and in
large quantity. The key question is how one can use
the contextual information contained in the parsing
stack to cope with the remaining (local) ambiguity
manifested as conflicts in the LR tables. The afore-
mentioned work has concentrated on LR parsing for
CFGs which has a clear deficiency in making avail-
able sufficient context in the LR states. (Shieber
and Johnson, 1993) hints at the relevance of rich
grammars on this respect. They use Tree Adjoining
Grammars (TAGs) (Joshi and Schabes, 1997; Joshi
et al., 1975) to defend the possibility of granular in-
cremental computations in LR parsing. Incidentally
or not, they make use of disambiguation contexts
that are only possible in a state of a conceptual LR
parser for a rich grammar formalism such as TAG,
but not for a CFG.
Concrete LR-like algorithms for TAGs have only
recently been proposed (Prolo, 2000; Nederhof,
1998), though their evaluation was restricted to the
quality of the parsing table (see also (Schabes and
Vijay-Shanker, 1990; Kinyon, 1997) for earlier at-
tempts).
In this paper, we revisit the LR parsing technique,
applied to a rich grammar formalism: TAG. Follow-
ing (Briscoe and Carroll, 1993), conflict resolution
is based on contextual information extracted from
the so called Instantaneous Description or Configu-
ration: a stack, representing the control memory of
the LR parser, and a lookahead sequence, here lim-
ited to one symbol.&apos;
However, while Briscoe and Carroll invested on
massive parallel computation of the possible parsing
paths, with pruning and posterior ranking, we ex-
&apos;Unlike (Wright and Wrigley, 1991)’s approach who tries
to transpose PCFG probabilities to LR tables, facing difficulties
which, to the best of our knowledge, have not been yet solved
to content (cf. also (Ng and Tomita, 1991; Wright et al., 1991;
Abney et al., 1999)).
</bodyText>
<affiliation confidence="0.555371333333333">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 103-110.
Association for Computational Linguistics.
</affiliation>
<bodyText confidence="0.981664428571429">
periment with a simple greedy depth-first technique
with limited amount of backtracking, that resembles
to a certain extent the commitment/recovery models
from the psycholinguistic research on human lan-
guage processing, supported by the occurrence of
“garden paths”.2
We use the Penn Treebank WSJ corpus, release 2
(Marcus et al., 1994), to evaluate the approach.
2 The architecture of the parser
Table 1 shows the architecture of our parsing appli-
cation. We extract a TAG from a piece of the Penn
Treebank, the training corpus, and submit it to an
LR parser generator. The same training corpus is
used again to extract statistical information that is
used by the driver as follows. The grammar gen-
eration process generates as a subproduct the TAG
derivation trees for the annotated sentences compat-
ible with the extracted grammar trees. This deriva-
tion tree is then converted into the sequence of LR
parsing actions that would have to be used by the
parser to generate exactly that analysis. A parser ex-
ecution simulation is then performed, guided by the
obtained sequence of parsing actions, collecting the
statistical information defined in Section 3.
In possession of the LR table, grammar and sta-
tistical information, the parser is then able to parse
fast natural language sentences annotated for parts-
of-speech.
</bodyText>
<subsectionHeader confidence="0.99348">
2.1 The extracted grammar
</subsectionHeader>
<bodyText confidence="0.999047357142857">
Our target grammar is extracted with a customized
version of the extractor defined in (Xia, 2001),
which we will not describe here. However, a key as-
pect to mention is that grammar trees are extracted
by factoring of recursion. Even constituents anno-
tated flat in the Treebank are first given a more hi-
erarchical, recursive structure. Therefore the trees
generated during parsing will be richer than those in
the Treebank. We will return to this point later.
Before grammar extraction, Treebank labels are
merged to allow for the generation of a more com-
pact grammar and parsing table, and to concentrate
statistical information (e.g., NN and NNS; NNP and
NNPS; all labels for finite verb forms). The gram-
</bodyText>
<footnote confidence="0.670685">
2See, e.g., (Tanenhaus and Trueswell, 1995) for a survey on
human sentence comprehension.
</footnote>
<figureCaption confidence="0.999866">
Figure 1: Architecture of the parser
</figureCaption>
<bodyText confidence="0.998834333333333">
mar extractor assigns a plausibility judgment to each
extracted tree. When a tree is judged implausible, it
is discarded from the grammar, and so are the sen-
tences in the training corpus in which the tree is
used. This reduced our training corpus by about 15
%.
</bodyText>
<subsectionHeader confidence="0.998368">
2.2 The LR parser generator
</subsectionHeader>
<bodyText confidence="0.999388571428571">
We used the grammar generator in (Prolo, 2000). In
this section we only present a parsing example, to il-
lustrate the kinds of action inserted in the generated
tables; details concerning how the table is generated
are omitted. Consider the TAG fragment in Figure 2
for simple sentences with modal and adverb adjunc-
tion. Figure 3 contains a derivation for the sentence
</bodyText>
<figure confidence="0.7404475">
“John will leave soon”.
np vintrans rbmod modal
</figure>
<figureCaption confidence="0.9401225">
Figure 2: A TAG fragment for simple sentences with
VP adjunction
</figureCaption>
<bodyText confidence="0.994820666666667">
We sketch in Figure 4 the sequence of actions ex-
ecuted by the parser. Technically, each element of
the stack would be a pair: the second element being
</bodyText>
<figure confidence="0.962435161290323">
P
E
N
N
T
r
e
e
b
a
n
k
Extract Statistics
for
Solving Conflicts
Statistical
Info
Grammar Extraction
G r a m m a r
Derivation Tree
Derived Tree
LR Table
Generator
LR Table
DRIVER
I
n
p
u
t
NP
NN
NP
S
VP
VB
VP
VP*
ADVP
RB
ivD
VP
VP
*
rbmod [soon]
a) derived tree
b) derivation tree
[John] [will]
intrans [leave]
S
NP
VP
np [John]
NN
MD
VP
VP
ADVP
modal [will]
VB
RB
[leave] [soon]
</figure>
<table confidence="0.779095727272727">
stack input sel. action
[] NN MD ... shift
[NN] MD VB ... reduce np
[NP] MD VB ... shift
[NP MD] VB RB shift
[NP MD VB] RB bpack VP,1
[NP MD [ VB]] RB shift
[NP MD [ VB] RB] bpack VP,2
[NP MD [ [ VB] RB] ] reduce modal
[NP [ VB] RB] reduce rbmod
[NP VB] accept vintrans
</table>
<figureCaption confidence="0.997395666666667">
Figure 4: Parsing “John/NN will/MD leave/VB soon/RB”
(parsing states were omitted in “stack” for clarity).
Figure 3: Derivation of “John will leave soon
</figureCaption>
<bodyText confidence="0.99998004">
an LR state; the first can be either a grammar sym-
bol or an embedded stack. Although the state is the
only relevant component of the pair for parsing, in
the figure, for presentational purposes, we omit the
state and instead show only the symbol/embedded
stack component (despite the misleading presence
of embedded stacks, actions are executed in con-
stant time). Stacks are surrounded by square brack-
ets. Only the parts of speech have been represented.
The bpack action is not standard in LR parsing. It
represents an earlier partial commitment for struc-
ture. In its first appearance, it acknowledges that
some material will be adjoined to a VP that domi-
nates the element at the top of the stack (in fact it
dominates the topmost elements, where is the
second parameter of bpack). The material is then
enclosed in a substack (the subscript VP at the left
bracket is for presentation purposes only; that infor-
mation is in fact in the LR state that would pair with
the substack). The next line contains another bpack
with , that proposes another adjunction, dom-
inating the VB and the RB. Reductions for auxiliary
trees leave no visible trace in the stack after they are
executed. The parser executes reductions in a bot-
tom up order with respect to the derivation tree
</bodyText>
<sectionHeader confidence="0.985883" genericHeader="method">
3 Conflict Resolution
</sectionHeader>
<bodyText confidence="0.996684217391304">
In this session we focus on how to resolve con-
flicts in the generated parsing table to obtain a single
3Notice that the notion of top-down/bottom-up parsing can-
not be defined on the derived tree for TAGs, unless one wants to
break the actions into independent subatomic units, e.g., single-
level context-free-like expansions.
“best” parse for each input sentence. At each step of
the parsing process the driver is faced with the task
of choosing among a certain number of available ac-
tions. At the end, the sequence of actions taken will
uniquely define one derivation tree that represents
the chosen syntactic analysis.
In our approach, the parser proceeds greedily try-
ing to compute a single successful sequence of ac-
tions. Whenever it fails, a backtracking strategy is
employed to re-conduce the parser to an alternative
path, up to a certain limited number of attempts pro-
vided as a parameter to the parser. Choices are made
locally. We have not tried to maximize any global
measure, such as the probability of the sentence, the
probability of a parse given a string, etc.
An instantaneous description (or “configuration”)
can be characterized by two components:
</bodyText>
<listItem confidence="0.854408692307692">
1. The current content of the stack, which in-
cludes the current (automaton) state;
2. The suffix of the input not yet shifted into the
stack.
The basic parsing approach has two main compo-
nents:
1. A strategy for ranking the actions available at
any given instantaneous description;
2. A greedy strategy for computing the sequence
of parsing actions. At each instantaneous de-
scription:
Choose the highest-ranked action that has
not been tried yet and execute it.
</listItem>
<bodyText confidence="0.9887955">
If there is no action available, then back-
track: move back to one of the instanta-
neous descriptions previously entered and
choose an alternative action.
</bodyText>
<subsectionHeader confidence="0.948127">
3.1 Strategies for ranking conflicting actions
</subsectionHeader>
<bodyText confidence="0.999342628571429">
Let be the number of positions in the stack and
be the sequence of states in
the positions.4 is the current state. Let be the
lookahead symbol, the leftmost symbol in the not yet
shifted suffix. Let be the (finite) set of possible
actions given by the grammar.
We use two basic ranking functions: and
. The first, naive one considers only the cur-
rent automaton state (the state at the top of the stack,
) and the lookahead symbol, . It is a poor statis-
tic but it does not suffer of sparse data problems in
our training corpus, and hence is used for smooth-
ing. For any instantaneous description as described
above, we trivially define the , for any ac-
tion , as a probability estimate for , given the
current state and lookahead symbol :
where is the number of times
occurs in an instantaneous description when
parsing the annotated corpus.
It can be observed that individual actions tend to
depend on different additional states in the stack.
For a shift action there is no reason to assume that
the previous state is particularly relevant, or that
state, say, , is not. But for a or
action we should suspect that the state from where
the action is taken is highly relevant. So, for
instance, the action reduce , where the number
of non-empty leaves of is , would have strong de-
pendency on the state . An approximation of its
rank would be: . This
observation is certainly not new. A similar ranking
function is in fact used by Briscoe and Carroll. How-
ever, an inconsistency immediately arises: we can-
not compare probabilities of events drawn on dis-
tinct sample spaces. For instance, if we have two
</bodyText>
<footnote confidence="0.989027666666667">
4Recall each position in the stack contains a pair where the
second element is an automaton state and the first element is
either a symbol or an embedded stack.
</footnote>
<bodyText confidence="0.948558586956522">
competing actions , an reduce , and , a shift,
and we affirm that depends on , then, it can-
not be true that the shift does not depend on . In
fact, it has to be the case that it depends on as
much as . One could suggest calculating the prob-
abilities for all actions conditional to the same set
of states, . But, in general, we have many
more than two actions to decide among. And they
are likely to stress their dependencies on different
past states. We see that this is not going to work; the
number of dependencies, and hence the number of
parameters, will grow too big.
A striking solution arises from a notable fact from
LR parsing theory for CFGs: If state contains an
action reduce p, where is a production with sym-
bols on its right side, then, the pair ( ), from
the instantaneous description, uniquely identifies the
entire sequence . Although
this property does not hold for the parser generation
algorithm we are using, it is still a good approxima-
tion to the true statistical dependencies.5
We can use this “approximately correct” property
in our benefit: “if state contains an action re-
duce or bpack for a number of leaves , then the
dependency on the sequence
can be approximated by a dependency on the pair
( )”. So a natural candidate for the second
state to be considered is the state , where
= max has an action bpack(X,l) for some
X or has some action reduce for a tree with non-
empty leaves . We define our second ranking func-
tion based on that.
5That the property does not hold in the algorithm we are us-
ing is a consequence of the way the goto function for adjunction
is defined in (Prolo, 2000), as a function of two states (instead of
just a simple transition in the automaton). A detailed argument
is beyond the scope of this paper and can be found in (Prolo,
2002), available upon request to the author. That the statement
is a good approximation to the true statistical dependencies fol-
lows from: (1) adjuncts (that can cause distinct states to inter-
vene between the considered pair), are generally regarded as
not restricting the syntactic possibilities of the clause they ad-
join to; and (2) in practice, the intermediate states at positions
that could be distinct for theoretically different sequences most
often have exactly the same characteristics, i.e., they are likely
to “accidentally” collapse to the same state.
</bodyText>
<subsectionHeader confidence="0.996864">
3.2 The backtracking strategy
</subsectionHeader>
<bodyText confidence="0.9992135">
We have a (quite narrow) notion of confidence for
parsing paths: as long as our sequence of decisions
allows the parser to proceed we trust the sequence,
and if it has taken us to an acceptance state, we be-
lieve we have the correct parse. On the other hand,
a crash is our other binary value for confidence, an
untrustworthy parsing sequence. In these cases, we
know we have made a bad decision somewhere in
the path and that we have to start again from that
point by following another alternative. This is a
backtracking strategy, although not in the common
sense of a depth-first walk, (i.e., exploring all the
possibilities left before undoing some earlier action).
We want to explore strategies of intelligently guess-
ing the restart point.
We use a simple strategy of returning to the deci-
sion point that left the highest amount of probability
mass unexplored. In order to implement it, we main-
tain a tree with the entire parsing attempts’ history.
There will be one path from the root correspond-
ing to the current parsing attempt, the leaf being the
current instantaneous description. All other leaves
correspond to instantaneous descriptions that have
been abandoned (crashing points). If the current leaf
crashes, all nodes in the tree (except for the leaves)
compete to be the restart point. Keeping all nodes
in the tree alive is a direct consequence of the fact
that we do not intend to do exhaustive backtracking.
We trade space (a tree instead of just a sequence) for
time: presumably, by doing smart backtracking we
can find a successful path by trying only a fraction
of the possible ones. Moreover, we want to find the
best (or approximately best) successful path, and a
crashing point is a good point to re-evaluate the pro-
cess. Limits may be added through parameters, so
that the parser may give up after a certain amount of
attempts or time.
In addition to the instantaneous description, each
node contains a record of the alternatives previously
tried (the edges to its child nodes in the tree) with
their corresponding probabilities, plus a ranked list
of the alternatives not yet tried. In particular we
maintain the probability mass left unexplored in a
node: the sum of the probabilities of the actions not
yet tried. Notice that alternatives already tried are in-
directly kept alive through their corresponding child
nodes.
Let be the set of actions not yet tried at
node . The probability mass left is
.6 The backtracking process
chooses for which is maxi-
mum (efficiently maintained using a priority queue).
Then we update and
start another branch in the tree by executing .
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.996184818181818">
We evaluated the approach using the Penn Treebank
WSJ Corpus, release 2 (Marcus et al., 1994), using
Sections 2 to 21 for grammar extraction and training,
section 0 for development and 23 for testing.
Only parts-of-speech were used as input.7 8
A smoothed ranking function is defined as fol-
lows:
The best was experimentally determined to be 1.
That is: in general, even if there is minimal evidence
of the context including the second state, the statis-
tics using this context lead to a better result than us-
ing only one state.
For each sentence there is an initial parsing at-
tempt using only as the ranking function with
an a maximum of 500 backtracking occurrences. If
it fails, then the sentence is parsed using ,
with a maximum of 3,000 backtracking occurrences.
In table 1 we report the following figures for the
development set (Section 0) and test set (Section
23):
%failed is the percentage of sentences for
which the parser failed (in the two attempts).
</bodyText>
<subsectionHeader confidence="0.795618">
6Where can be any of the ranking functions, at
</subsectionHeader>
<bodyText confidence="0.999298714285714">
the state , applied to . Elsewhere in the paper we have omitted
the explicit reference to the state.
7However, two new categories were defined: one for time
nouns, namely those that appear in the Penn Treebank as heads
of constituents marked “TMP” (for temporal); another for the
word “that”. This is similar to (Collins, 1997)’s and Char-
niak97’s definition of a separate category for auxiliary verbs.
</bodyText>
<footnote confidence="0.97963725">
8We also included some punctuation symbols among the ter-
minals such as comma, colon and semicolon. They are extracted
into the grammar as if they were regular modifiers. Their main
use is in guiding parsing decisions.
</footnote>
<table confidence="0.89103475">
if
then
else
Section %failed tput recall prec.
0 1.3 18 81.76
23 1.9 19 81.41
0 (flat) 1.3 18 78.21 77.35 77.78
23 (flat) 1.9 19 77.52 76.96 77.24
</table>
<tableCaption confidence="0.999844">
Table 1: Results on the development and test set
</tableCaption>
<bodyText confidence="0.997854234567901">
recall and prec. are the labeled parsing re-
call and precision, respectively, as defined in
(Collins, 1997) (slightly different from (Black
et al., 1991)). is their harmonic average.
tput is the average number of sentences parsed
per second. To obtain the average, the number
of sentences submitted as input (not only those
that parsed successfully) is divided by the to-
tal time (excluded the time overhead before it
starts parsing the first sentence). The programs
were run under Linux, in a PC with a Pentium
III 930MHz processor.
The first two lines report the measures for the
parsed sentences as originally generated by the
parser. We purposefully do not report precision.
As we mentioned in the beginning of the paper, the
parser assigns to the sentences a much richer hierar-
chical structure than the Penn Treebank does, which
is penalized by the precision measure. The reason
for such increase in structure is not quite a particular
decision of ours, but a consequence of using a sound
grammar under the TAG grammatical formalism.9
However, having concluded our manifesto, we un-
derstand that algorithms that try to keep precision as
high as the recall necessarily have losses in recall
compared to if they ignored the precision, and there-
fore in order to have fair comparison with them and
to improve the credibility of our results, we flattened
the parse trees in a post-processing step, using a sim-
ple rule-based technique on top of some frequency
measures for individual grammar trees gathered by
(Xia, 2001) and the result is presented in the bottom
lines of the table.
9By sound we mean a grammar that properly factors recur-
sion in one way or another. Grammars have been extracted
where the right side of a rule reflects exactly each single-level
expansion found in the Penn Treebank. We are also aware of a
few alternatives in grammatical formalisms that could capture
such flatness, e.g., sister adjunction (Chiang, 2000).
The most salient positive result is that the parser
is able to parse sentences at a rate of about 20 sen-
tences per second. Most of the medium-to-high ac-
curacy parsers take at least a few seconds per sen-
tence under the same conditions.10 This is an enor-
mous speed-up. As for the accuracy, it is not far
from the top performing parser for parts-of-speech
that we are aware of, reported by (Sima’an, 2000):
recall/precision =
Perhaps the most similar work to ours is Briscoe
and Carroll’s (1993; 1995; 1992; 1996). They im-
plemented a standard LR parser for CFGs, and a
probabilistic method for conflict resolution similar
to ours in that the decisions are conditioned to the
LR states but with different methods. In particular,
they proceed in a parallel way accumulating proba-
bilities along the paths and using a Viterbi decoder at
the end. Their best published result is of unlabeled
bracket recall and precision of 74 % and 73 %, pars-
ing the Susanne corpus. Since the unlabeled bracket
measures are much easier than the ones we are re-
porting, on labeled brackets, our results are clearly
superior to theirs. Also the Susanne corpus is easier
than the Penn Treebank.
There are two additional points we want to make.
One is with respect to the ranking function ,
based on two states. It is a very rich statistic, but
suffers from sparse data problems. Parsing section 0
with only this statistics (no form of smoothing), with
backtracking limit of 3,000 attempts, we could parse
only 31 % of the sentences but the non-flattened
recall was 88.33 %, which is quite high for using
only parts-of-speech. The second observation is that
when parsing with the smoothed function
most of the sentences use very few number of back-
tracking attempts. In fact a graph relating number of
backtracking attempts with number of sentences
that parse using attempts shows an relation
characteristic of Zipf’s law. Most of the time spent
with computation is spent with sentences that either
fail parsing or parse with difficulty, showing low
bracketing accuracy.
</bodyText>
<footnote confidence="0.916289">
10The fastest parser we are aware of is from BBN, with a
throughput of 3 sentences per second under similar conditions.
We also emphasize we have not taken particular care with opti-
mization for speed yet.
</footnote>
<sectionHeader confidence="0.988822" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999972930232558">
The results presented here suggest that: (1) the use
of a rich grammar as the underlying formalism for
the LR techniques makes available enough informa-
tion to the driver so as to allow for a greedy strategy
to achieve reasonable parsing accuracy. (2) LR pars-
ing allows for very fast parsing with at least reason-
able accuracy.
The approach seems to have much yet to be ex-
plored, mostly to improve the accuracy side. In par-
ticular we have not yet come with a solid approach
to lexicalization. Using words (as opposed to pos
tags) as the terminals of the grammar to be pre-
compiled leads to an explosion in the size of the
table: not only the average number of transitions
per state grows, but also the number of states it-
self grows wildly. One very promising approach for
a partial solution is to expand the set of terminals
by adding some selected syntactic sub-categories
that have distinguished syntactic behavior, as we re-
ported in this paper for time nouns, or by individuat-
ing frequent words with peculiar behavior, as we did
for the word “that”. Although we have also done
some initial work on a more general approach to
clustering words according to their syntactic distri-
bution, they are not still adequate for our purposes.
Finally, an earlier simple experiment of adding a de-
pendency on the lookahead’s word (recall that in
and was the pos tag only), gave us a
small improvement of about a couple of percents in
the accuracy measures.
A limited amount of parallelism is an important
topic to be considered, perhaps together with a bet-
ter notion (non-binary) of confidence. The high reli-
ability of , suggests that we should look for a
way to enrich the parsing table.
LR parser for the full class of TAGs is prob-
lematic. The bpack action of early structural com-
mitment is involved in most of the decision points
where the wrong action is taken. We are currently
working on a version of the LR parser for a subclass
of TAGs, the Tree Insertion Grammars (Schabes and
Waters, 1995), for which efficient true LR parsers
can be obtained.
</bodyText>
<sectionHeader confidence="0.989908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998394614864865">
Steven Abney, David McAllester, and Fernando Pereira.
1999. Relating probabilistic grammar and automata.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, College Park,
MD, USA.
Ezra Black, Steven Abney, C. Gdaniec, Ralph Grish-
man, P. Harrison, Don Hindle, R. Ingria, Fred Je-
linek, Judith Klavans, Mark Liberman, Mitchell Mar-
cus, Salim Roukos, Beatrice Santorini, and T. Strza-
lkowski. 1991. A procedure for quantitatively com-
paring the syntactic coverage of english grammars. In
Proceedings of the DARPA Speech and Natural Lan-
guage Workshop, San Mateo, CA, USA.
Ted Briscoe and John Carroll. 1993. Generalized proba-
bilistic LR parsing of natural language (corpora) with
unification-based grammars. Computational Linguis-
tics, 19(1):25–59.
Ted Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic LR parser of part-of-speech
and punctuation labels. In Proceedings of the 4th In-
ternational Workshop on Parsing Technologies (IWPT-
95), pages 48–58, Prague/Karlovy Vary, Czech Repub-
lic.
John Carroll and Ted Briscoe. 1992. Probabilistic nor-
malisation and unpacking of packed parse forests for
unification-based grammars. In Proceedings of the
AAAI Fall Symposium on Probabilistic Approaches to
Natural Language, Cambridge, MA, USA.
John Carroll and Ted Briscoe. 1996. Apportioning de-
velopment effort in a probabilistic LR parsing sys-
tem through evaluation. In Proceedings of the Con-
ference on Empirical Methods in NLP, pages 92–100,
Philadelphia, PA, USA.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting of the As-
sociation for Computational Linguistics, Hong Kong,
China.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16–23, Madrid, Spain.
Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka,
and Takenobu Tokunaga. 1997. A new formalization
of probabilistic GLR parsing. In Proceedings of the
5th International Workshop on Parsing Technologies
(IWPT-97), Cambridge, MA, USA.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Handbook of Formal Lan-
guages, volume 3, pages 69–123. Springer-Verlag,
Berlin.
Aravind K. Joshi, L. Levy, and M. Takahashi. 1975. Tree
Adjunct Grammars. Journal of Computer and System
Sciences, 10(1).
Alexandra Kinyon. 1997. Un algorithme d’analyse
LR(0) pour les grammaires d’arbres adjoints lex-
icalise´ees. In D. Genthial, editor, Quatri`eme
conf´erence annuelle sur Le Traitement Automatique
du Langage Naturel, Actes, pages 93–102, Grenoble,
France.
Donald E. Knuth. 1965. On the translation of languages
from left to right. Information and Control, 8(6):607–
639.
Bernard Lang. 1974. Deterministic techniques for
efficient non-deterministic parsers. In Automata,
Languages and Programming, 2nd Colloquium, vol-
ume 14 of Lecture Notes in Computer Science, pages
255–269, Saarbr¨ucken. Springer-Verlag, Berlin.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 Human Language Technology
Workshop.
Paola Merlo. 1996. Parsing with Principles and Classes
ofInformation. Kluwer Academic Publishers, Boston,
MA, USA.
Mark-Jan Nederhof. 1998. An alternative LR algorithm
for TAGs. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 16th International Conference on Computational
Linguistics, Montreal, Canada.
See-Kiong Ng and Masaru Tomita. 1991. Probabilistic
LR parsing for Generalized context-free grammars. In
Proceedings of the Second International Workshop on
Parsing Technologies (IWPT-91), Cancun, Mexico.
Fernado Pereira. 1985. A new characterization of attach-
ment preferences. In David R. Dowty, Lauri Kartunen,
and Arnold M. Zwicky, editors, Natural Language
Parsing: Psychological, computational, and theoret-
ical perspectives, pages 307–319. Cambridge Univer-
sity Press, New York, NY, USA.
Carlos A. Prolo. 2000. An efficient LR parser generator
for Tree Adjoining Grammars. In Proceedings of the
6th International Workshop on Parsing Technologies
(IWPT-2000), Trento, Italy.
Carlos A. Prolo. 2002. LR parsing for Tree Adjoioning
Grammars and its application to corpus-based natural
language parsing. Ph.D. thesis proposal, University of
Pennsylvania.
Tobias Ruland. 2000. A context-sensitive model for
probabilistic LR parsing of spoken language with
transformation-based postprocessing. In Proceedings
of the 18th International Conference on Computa-
tional Linguistics (COLING’2000), pages 677–683,
Saarbr¨ucken, Germany.
Y. Schabes and K. Vijay-Shanker. 1990. Determinis-
tic left to right parsing of tree adjoining languages.
In Proceedings of 28th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 276–283,
Pittsburgh, Pennsylvania, USA.
Yves Schabes and Richard C. Waters. 1995. Tree In-
sertion Grammar: a cubic-time, parsable formalism
that lexicalizes Context-Free Grammar without chang-
ing the trees produced. Computational Linguistics,
21(4):479–513.
Stuart Shieber and Mark Johnson. 1993. Variations on
incremental interpretation. Journal of Psycholinguis-
tic Research, 22(2):287–318.
Stuart M. Shieber. 1983. Sentence disambiguation by a
Shift-Reduce parsing technique. In Proceedings of the
21st Annual Meeting of the Association for Compu-
tational Linguistics, pages 119–122, Cambridge, MA,
USA.
Khalil Sima’an. 2000. Tree-gram parsing: Lexical de-
pendencies and structural relations. In Proceedings of
the 38th Annual Meeting of the Association for Com-
putational Linguistics, Hong Kong, China.
Michael K. Tanenhaus and John C. Trueswell. 1995.
Sentence comprehension. In Joanne L. Miller and Pe-
ter D. Eiwas, editors, Speech, Language, and Commu-
nication, pages 217–262. Academic Press, San Diego,
CA, USA.
Masaru Tomita. 1985. Efficient Parsingfor Natural Lan-
guage. Kluwer Academic Publishers, Boston, MA,
USA.
J. H. Wright and E. N. Wrigley. 1991. GLR parsing with
probability. In Masaru Tomita, editor, Generalized LR
Parsing, pages 113–128. Kluwer Academic Publish-
ers, Boston, MA, USA.
Jerry Wright, Ave Wrigley, and Richard Sharman. 1991.
Adaptive probabilistic Generalized LR parsing. In
Proceedings of the Second International Workshop on
Parsing Technologies (IWPT-91), Cancun, Mexico.
Fei Xia. 2001. Investigating the Relationship between
Grammars and Treebanks for Natural Languages.
Ph.D. thesis, Department of Computer and Informa-
tion Science, University of Pennsylvania.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957347">
<title confidence="0.999975">Fast LR Parsing Using Rich (Tree Adjoining) Grammars</title>
<author confidence="0.997689">A Carlos</author>
<affiliation confidence="0.999694">Department of Computer and Information University of</affiliation>
<email confidence="0.99759">prolo@linc.cis.upenn.edu</email>
<abstract confidence="0.996340545454545">We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>David McAllester</author>
<author>Fernando Pereira</author>
</authors>
<title>Relating probabilistic grammar and automata.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>College Park, MD, USA.</location>
<contexts>
<context position="3181" citStr="Abney et al., 1999" startWordPosition="508" endWordPosition="511">tual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex&apos;Unlike (Wright and Wrigley, 1991)’s approach who tries to transpose PCFG probabilities to LR tables, facing difficulties which, to the best of our knowledge, have not been yet solved to content (cf. also (Ng and Tomita, 1991; Wright et al., 1991; Abney et al., 1999)). Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 103-110. Association for Computational Linguistics. periment with a simple greedy depth-first technique with limited amount of backtracking, that resembles to a certain extent the commitment/recovery models from the psycholinguistic research on human language processing, supported by the occurrence of “garden paths”.2 We use the Penn Treebank WSJ corpus, release 2 (Marcus et al., 1994), to evaluate the approach. 2 The architecture of the parser Table 1 shows the architectu</context>
</contexts>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>Steven Abney, David McAllester, and Fernando Pereira. 1999. Relating probabilistic grammar and automata. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Steven Abney</author>
<author>C Gdaniec</author>
<author>Ralph Grishman</author>
<author>P Harrison</author>
<author>Don Hindle</author>
<author>R Ingria</author>
<author>Fred Jelinek</author>
<author>Judith Klavans</author>
<author>Mark Liberman</author>
<author>Mitchell Marcus</author>
<author>Salim Roukos</author>
<author>Beatrice Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<location>San Mateo, CA, USA.</location>
<contexts>
<context position="19098" citStr="Black et al., 1991" startWordPosition="3275" endWordPosition="3278">finition of a separate category for auxiliary verbs. 8We also included some punctuation symbols among the terminals such as comma, colon and semicolon. They are extracted into the grammar as if they were regular modifiers. Their main use is in guiding parsing decisions. if then else Section %failed tput recall prec. 0 1.3 18 81.76 23 1.9 19 81.41 0 (flat) 1.3 18 78.21 77.35 77.78 23 (flat) 1.9 19 77.52 76.96 77.24 Table 1: Results on the development and test set recall and prec. are the labeled parsing recall and precision, respectively, as defined in (Collins, 1997) (slightly different from (Black et al., 1991)). is their harmonic average. tput is the average number of sentences parsed per second. To obtain the average, the number of sentences submitted as input (not only those that parsed successfully) is divided by the total time (excluded the time overhead before it starts parsing the first sentence). The programs were run under Linux, in a PC with a Pentium III 930MHz processor. The first two lines report the measures for the parsed sentences as originally generated by the parser. We purposefully do not report precision. As we mentioned in the beginning of the paper, the parser assigns to the se</context>
</contexts>
<marker>Black, Abney, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Ezra Black, Steven Abney, C. Gdaniec, Ralph Grishman, P. Harrison, Don Hindle, R. Ingria, Fred Jelinek, Judith Klavans, Mark Liberman, Mitchell Marcus, Salim Roukos, Beatrice Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of english grammars. In Proceedings of the DARPA Speech and Natural Language Workshop, San Mateo, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="990" citStr="Briscoe and Carroll, 1993" startWordPosition="151" endWordPosition="154"> evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrat</context>
<context position="2522" citStr="Briscoe and Carroll, 1993" startWordPosition="404" endWordPosition="407">ty of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex&apos;Unlike (Wright and Wrigley, 1991)’s approach who tries to transpose PCFG probabilities to LR tables, facing difficulties which, to the best of our knowledge, have not been yet solved to content (cf. also (Ng</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Developing and evaluating a probabilistic LR parser of part-of-speech and punctuation labels.</title>
<date>1995</date>
<booktitle>In Proceedings of the 4th International Workshop on Parsing Technologies (IWPT95),</booktitle>
<pages>48--58</pages>
<location>Prague/Karlovy Vary, Czech Republic.</location>
<marker>Briscoe, Carroll, 1995</marker>
<rawString>Ted Briscoe and John Carroll. 1995. Developing and evaluating a probabilistic LR parser of part-of-speech and punctuation labels. In Proceedings of the 4th International Workshop on Parsing Technologies (IWPT95), pages 48–58, Prague/Karlovy Vary, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
</authors>
<title>Probabilistic normalisation and unpacking of packed parse forests for unification-based grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language,</booktitle>
<location>Cambridge, MA, USA.</location>
<marker>Carroll, Briscoe, 1992</marker>
<rawString>John Carroll and Ted Briscoe. 1992. Probabilistic normalisation and unpacking of packed parse forests for unification-based grammars. In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
</authors>
<title>Apportioning development effort in a probabilistic LR parsing system through evaluation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in NLP,</booktitle>
<pages>92--100</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="1036" citStr="Carroll and Briscoe, 1996" startWordPosition="159" endWordPosition="162">howing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrated on LR parsing for CFGs which has a clear de</context>
</contexts>
<marker>Carroll, Briscoe, 1996</marker>
<rawString>John Carroll and Ted Briscoe. 1996. Apportioning development effort in a probabilistic LR parsing system through evaluation. In Proceedings of the Conference on Empirical Methods in NLP, pages 92–100, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted Tree Adjoining Grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="20880" citStr="Chiang, 2000" startWordPosition="3575" endWordPosition="3576">y of our results, we flattened the parse trees in a post-processing step, using a simple rule-based technique on top of some frequency measures for individual grammar trees gathered by (Xia, 2001) and the result is presented in the bottom lines of the table. 9By sound we mean a grammar that properly factors recursion in one way or another. Grammars have been extracted where the right side of a rule reflects exactly each single-level expansion found in the Penn Treebank. We are also aware of a few alternatives in grammatical formalisms that could capture such flatness, e.g., sister adjunction (Chiang, 2000). The most salient positive result is that the parser is able to parse sentences at a rate of about 20 sentences per second. Most of the medium-to-high accuracy parsers take at least a few seconds per sentence under the same conditions.10 This is an enormous speed-up. As for the accuracy, it is not far from the top performing parser for parts-of-speech that we are aware of, reported by (Sima’an, 2000): recall/precision = Perhaps the most similar work to ours is Briscoe and Carroll’s (1993; 1995; 1992; 1996). They implemented a standard LR parser for CFGs, and a probabilistic method for conflic</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted Tree Adjoining Grammar. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="18457" citStr="Collins, 1997" startWordPosition="3167" endWordPosition="3168"> maximum of 3,000 backtracking occurrences. In table 1 we report the following figures for the development set (Section 0) and test set (Section 23): %failed is the percentage of sentences for which the parser failed (in the two attempts). 6Where can be any of the ranking functions, at the state , applied to . Elsewhere in the paper we have omitted the explicit reference to the state. 7However, two new categories were defined: one for time nouns, namely those that appear in the Penn Treebank as heads of constituents marked “TMP” (for temporal); another for the word “that”. This is similar to (Collins, 1997)’s and Charniak97’s definition of a separate category for auxiliary verbs. 8We also included some punctuation symbols among the terminals such as comma, colon and semicolon. They are extracted into the grammar as if they were regular modifiers. Their main use is in guiding parsing decisions. if then else Section %failed tput recall prec. 0 1.3 18 81.76 23 1.9 19 81.41 0 (flat) 1.3 18 78.21 77.35 77.78 23 (flat) 1.9 19 77.52 76.96 77.24 Table 1: Results on the development and test set recall and prec. are the labeled parsing recall and precision, respectively, as defined in (Collins, 1997) (sli</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Inui</author>
<author>Virach Sornlertlamvanich</author>
<author>Hozumi Tanaka</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>A new formalization of probabilistic GLR parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th International Workshop on Parsing Technologies (IWPT-97),</booktitle>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1009" citStr="Inui et al., 1997" startWordPosition="155" endWordPosition="158">the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrated on LR parsing fo</context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 1997</marker>
<rawString>Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka, and Takenobu Tokunaga. 1997. A new formalization of probabilistic GLR parsing. In Proceedings of the 5th International Workshop on Parsing Technologies (IWPT-97), Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>TreeAdjoining Grammars.</title>
<date>1997</date>
<booktitle>In Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--123</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="1851" citStr="Joshi and Schabes, 1997" startWordPosition="294" endWordPosition="297">e automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrated on LR parsing for CFGs which has a clear deficiency in making available sufficient context in the LR states. (Shieber and Johnson, 1993) hints at the relevance of rich grammars on this respect. They use Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997; Joshi et al., 1975) to defend the possibility of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied </context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. TreeAdjoining Grammars. In Handbook of Formal Languages, volume 3, pages 69–123. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>L Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree Adjunct Grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="1872" citStr="Joshi et al., 1975" startWordPosition="298" endWordPosition="301">. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrated on LR parsing for CFGs which has a clear deficiency in making available sufficient context in the LR states. (Shieber and Johnson, 1993) hints at the relevance of rich grammars on this respect. They use Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997; Joshi et al., 1975) to defend the possibility of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied to a rich grammar for</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, L. Levy, and M. Takahashi. 1975. Tree Adjunct Grammars. Journal of Computer and System Sciences, 10(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Kinyon</author>
</authors>
<title>Un algorithme d’analyse LR(0) pour les grammaires d’arbres adjoints lexicalise´ees. In</title>
<date>1997</date>
<booktitle>Quatri`eme conf´erence annuelle sur Le Traitement Automatique du Langage Naturel, Actes,</booktitle>
<pages>93--102</pages>
<editor>D. Genthial, editor,</editor>
<location>Grenoble, France.</location>
<contexts>
<context position="2367" citStr="Kinyon, 1997" startWordPosition="380" endWordPosition="381">f rich grammars on this respect. They use Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997; Joshi et al., 1975) to defend the possibility of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex&apos;Unlike (Wright and Wrigley, 1991)’s approach who tri</context>
</contexts>
<marker>Kinyon, 1997</marker>
<rawString>Alexandra Kinyon. 1997. Un algorithme d’analyse LR(0) pour les grammaires d’arbres adjoints lexicalise´ees. In D. Genthial, editor, Quatri`eme conf´erence annuelle sur Le Traitement Automatique du Langage Naturel, Actes, pages 93–102, Grenoble, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>On the translation of languages from left to right.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<issue>6</issue>
<pages>639</pages>
<contexts>
<context position="1091" citStr="Knuth, 1965" startWordPosition="170" endWordPosition="171">ble accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrated on LR parsing for CFGs which has a clear deficiency in making available sufficient context in the </context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>Donald E. Knuth. 1965. On the translation of languages from left to right. Information and Control, 8(6):607– 639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>In Automata, Languages and Programming, 2nd Colloquium,</booktitle>
<volume>14</volume>
<pages>255--269</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="682" citStr="Lang, 1974" startWordPosition="104" endWordPosition="105">artment of Computer and Information Science University of Pennsylvania prolo@linc.cis.upenn.edu Abstract We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case th</context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>Bernard Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In Automata, Languages and Programming, 2nd Colloquium, volume 14 of Lecture Notes in Computer Science, pages 255–269, Saarbr¨ucken. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 Human Language Technology Workshop.</booktitle>
<contexts>
<context position="3692" citStr="Marcus et al., 1994" startWordPosition="579" endWordPosition="582"> have not been yet solved to content (cf. also (Ng and Tomita, 1991; Wright et al., 1991; Abney et al., 1999)). Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 103-110. Association for Computational Linguistics. periment with a simple greedy depth-first technique with limited amount of backtracking, that resembles to a certain extent the commitment/recovery models from the psycholinguistic research on human language processing, supported by the occurrence of “garden paths”.2 We use the Penn Treebank WSJ corpus, release 2 (Marcus et al., 1994), to evaluate the approach. 2 The architecture of the parser Table 1 shows the architecture of our parsing application. We extract a TAG from a piece of the Penn Treebank, the training corpus, and submit it to an LR parser generator. The same training corpus is used again to extract statistical information that is used by the driver as follows. The grammar generation process generates as a subproduct the TAG derivation trees for the annotated sentences compatible with the extracted grammar trees. This derivation tree is then converted into the sequence of LR parsing actions that would have to </context>
<context position="17214" citStr="Marcus et al., 1994" startWordPosition="2949" endWordPosition="2952">natives not yet tried. In particular we maintain the probability mass left unexplored in a node: the sum of the probabilities of the actions not yet tried. Notice that alternatives already tried are indirectly kept alive through their corresponding child nodes. Let be the set of actions not yet tried at node . The probability mass left is .6 The backtracking process chooses for which is maximum (efficiently maintained using a priority queue). Then we update and start another branch in the tree by executing . 4 Evaluation We evaluated the approach using the Penn Treebank WSJ Corpus, release 2 (Marcus et al., 1994), using Sections 2 to 21 for grammar extraction and training, section 0 for development and 23 for testing. Only parts-of-speech were used as input.7 8 A smoothed ranking function is defined as follows: The best was experimentally determined to be 1. That is: in general, even if there is minimal evidence of the context including the second state, the statistics using this context lead to a better result than using only one state. For each sentence there is an initial parsing attempt using only as the ranking function with an a maximum of 500 backtracking occurrences. If it fails, then the sent</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the 1994 Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
</authors>
<title>Parsing with Principles and Classes ofInformation.</title>
<date>1996</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="766" citStr="Merlo, 1996" startWordPosition="116" endWordPosition="117">cis.upenn.edu Abstract We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quant</context>
</contexts>
<marker>Merlo, 1996</marker>
<rawString>Paola Merlo. 1996. Parsing with Principles and Classes ofInformation. Kluwer Academic Publishers, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>An alternative LR algorithm for TAGs.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 16th International Conference on Computational Linguistics,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2233" citStr="Nederhof, 1998" startWordPosition="360" endWordPosition="361">ich has a clear deficiency in making available sufficient context in the LR states. (Shieber and Johnson, 1993) hints at the relevance of rich grammars on this respect. They use Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997; Joshi et al., 1975) to defend the possibility of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel com</context>
</contexts>
<marker>Nederhof, 1998</marker>
<rawString>Mark-Jan Nederhof. 1998. An alternative LR algorithm for TAGs. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 16th International Conference on Computational Linguistics, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>See-Kiong Ng</author>
<author>Masaru Tomita</author>
</authors>
<title>Probabilistic LR parsing for Generalized context-free grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the Second International Workshop on Parsing Technologies (IWPT-91),</booktitle>
<location>Cancun, Mexico.</location>
<contexts>
<context position="3139" citStr="Ng and Tomita, 1991" startWordPosition="500" endWordPosition="503">3), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex&apos;Unlike (Wright and Wrigley, 1991)’s approach who tries to transpose PCFG probabilities to LR tables, facing difficulties which, to the best of our knowledge, have not been yet solved to content (cf. also (Ng and Tomita, 1991; Wright et al., 1991; Abney et al., 1999)). Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 103-110. Association for Computational Linguistics. periment with a simple greedy depth-first technique with limited amount of backtracking, that resembles to a certain extent the commitment/recovery models from the psycholinguistic research on human language processing, supported by the occurrence of “garden paths”.2 We use the Penn Treebank WSJ corpus, release 2 (Marcus et al., 1994), to evaluate the approach. 2 The architecture </context>
</contexts>
<marker>Ng, Tomita, 1991</marker>
<rawString>See-Kiong Ng and Masaru Tomita. 1991. Probabilistic LR parsing for Generalized context-free grammars. In Proceedings of the Second International Workshop on Parsing Technologies (IWPT-91), Cancun, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernado Pereira</author>
</authors>
<title>A new characterization of attachment preferences.</title>
<date>1985</date>
<booktitle>Natural Language Parsing: Psychological, computational, and theoretical perspectives,</booktitle>
<pages>307--319</pages>
<editor>In David R. Dowty, Lauri Kartunen, and Arnold M. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="752" citStr="Pereira, 1985" startWordPosition="114" endWordPosition="115">nia prolo@linc.cis.upenn.edu Abstract We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and </context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>Fernado Pereira. 1985. A new characterization of attachment preferences. In David R. Dowty, Lauri Kartunen, and Arnold M. Zwicky, editors, Natural Language Parsing: Psychological, computational, and theoretical perspectives, pages 307–319. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos A Prolo</author>
</authors>
<title>An efficient LR parser generator for Tree Adjoining Grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Workshop on Parsing Technologies (IWPT-2000),</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2216" citStr="Prolo, 2000" startWordPosition="358" endWordPosition="359">g for CFGs which has a clear deficiency in making available sufficient context in the LR states. (Shieber and Johnson, 1993) hints at the relevance of rich grammars on this respect. They use Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997; Joshi et al., 1975) to defend the possibility of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on mas</context>
<context position="5868" citStr="Prolo, 2000" startWordPosition="945" endWordPosition="946">ct grammar and parsing table, and to concentrate statistical information (e.g., NN and NNS; NNP and NNPS; all labels for finite verb forms). The gram2See, e.g., (Tanenhaus and Trueswell, 1995) for a survey on human sentence comprehension. Figure 1: Architecture of the parser mar extractor assigns a plausibility judgment to each extracted tree. When a tree is judged implausible, it is discarded from the grammar, and so are the sentences in the training corpus in which the tree is used. This reduced our training corpus by about 15 %. 2.2 The LR parser generator We used the grammar generator in (Prolo, 2000). In this section we only present a parsing example, to illustrate the kinds of action inserted in the generated tables; details concerning how the table is generated are omitted. Consider the TAG fragment in Figure 2 for simple sentences with modal and adverb adjunction. Figure 3 contains a derivation for the sentence “John will leave soon”. np vintrans rbmod modal Figure 2: A TAG fragment for simple sentences with VP adjunction We sketch in Figure 4 the sequence of actions executed by the parser. Technically, each element of the stack would be a pair: the second element being P E N N T r e e</context>
<context position="13747" citStr="Prolo, 2000" startWordPosition="2366" endWordPosition="2367">e can use this “approximately correct” property in our benefit: “if state contains an action reduce or bpack for a number of leaves , then the dependency on the sequence can be approximated by a dependency on the pair ( )”. So a natural candidate for the second state to be considered is the state , where = max has an action bpack(X,l) for some X or has some action reduce for a tree with nonempty leaves . We define our second ranking function based on that. 5That the property does not hold in the algorithm we are using is a consequence of the way the goto function for adjunction is defined in (Prolo, 2000), as a function of two states (instead of just a simple transition in the automaton). A detailed argument is beyond the scope of this paper and can be found in (Prolo, 2002), available upon request to the author. That the statement is a good approximation to the true statistical dependencies follows from: (1) adjuncts (that can cause distinct states to intervene between the considered pair), are generally regarded as not restricting the syntactic possibilities of the clause they adjoin to; and (2) in practice, the intermediate states at positions that could be distinct for theoretically differ</context>
</contexts>
<marker>Prolo, 2000</marker>
<rawString>Carlos A. Prolo. 2000. An efficient LR parser generator for Tree Adjoining Grammars. In Proceedings of the 6th International Workshop on Parsing Technologies (IWPT-2000), Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos A Prolo</author>
</authors>
<title>LR parsing for Tree Adjoioning Grammars and its application to corpus-based natural language parsing. Ph.D. thesis proposal,</title>
<date>2002</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="13920" citStr="Prolo, 2002" startWordPosition="2398" endWordPosition="2399"> be approximated by a dependency on the pair ( )”. So a natural candidate for the second state to be considered is the state , where = max has an action bpack(X,l) for some X or has some action reduce for a tree with nonempty leaves . We define our second ranking function based on that. 5That the property does not hold in the algorithm we are using is a consequence of the way the goto function for adjunction is defined in (Prolo, 2000), as a function of two states (instead of just a simple transition in the automaton). A detailed argument is beyond the scope of this paper and can be found in (Prolo, 2002), available upon request to the author. That the statement is a good approximation to the true statistical dependencies follows from: (1) adjuncts (that can cause distinct states to intervene between the considered pair), are generally regarded as not restricting the syntactic possibilities of the clause they adjoin to; and (2) in practice, the intermediate states at positions that could be distinct for theoretically different sequences most often have exactly the same characteristics, i.e., they are likely to “accidentally” collapse to the same state. 3.2 The backtracking strategy We have a (</context>
</contexts>
<marker>Prolo, 2002</marker>
<rawString>Carlos A. Prolo. 2002. LR parsing for Tree Adjoioning Grammars and its application to corpus-based natural language parsing. Ph.D. thesis proposal, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Ruland</author>
</authors>
<title>A context-sensitive model for probabilistic LR parsing of spoken language with transformation-based postprocessing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING’2000),</booktitle>
<pages>677--683</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="1051" citStr="Ruland, 2000" startWordPosition="163" endWordPosition="164">d very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrated on LR parsing for CFGs which has a clear deficiency in mak</context>
</contexts>
<marker>Ruland, 2000</marker>
<rawString>Tobias Ruland. 2000. A context-sensitive model for probabilistic LR parsing of spoken language with transformation-based postprocessing. In Proceedings of the 18th International Conference on Computational Linguistics (COLING’2000), pages 677–683, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Deterministic left to right parsing of tree adjoining languages.</title>
<date>1990</date>
<booktitle>In Proceedings of 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="2352" citStr="Schabes and Vijay-Shanker, 1990" startWordPosition="376" endWordPosition="379">n, 1993) hints at the relevance of rich grammars on this respect. They use Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997; Joshi et al., 1975) to defend the possibility of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex&apos;Unlike (Wright and Wrigley, 1991)’s a</context>
</contexts>
<marker>Schabes, Vijay-Shanker, 1990</marker>
<rawString>Y. Schabes and K. Vijay-Shanker. 1990. Deterministic left to right parsing of tree adjoining languages. In Proceedings of 28th Annual Meeting of the Association for Computational Linguistics, pages 276–283, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Tree Insertion Grammar: a cubic-time, parsable formalism that lexicalizes Context-Free Grammar without changing the trees produced.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<marker>Schabes, Waters, 1995</marker>
<rawString>Yves Schabes and Richard C. Waters. 1995. Tree Insertion Grammar: a cubic-time, parsable formalism that lexicalizes Context-Free Grammar without changing the trees produced. Computational Linguistics, 21(4):479–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Mark Johnson</author>
</authors>
<title>Variations on incremental interpretation.</title>
<date>1993</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="1729" citStr="Shieber and Johnson, 1993" startWordPosition="274" endWordPosition="277">m its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity. The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables. The aforementioned work has concentrated on LR parsing for CFGs which has a clear deficiency in making available sufficient context in the LR states. (Shieber and Johnson, 1993) hints at the relevance of rich grammars on this respect. They use Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997; Joshi et al., 1975) to defend the possibility of granular incremental computations in LR parsing. Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG. Concrete LR-like algorithms for TAGs have only recently been proposed (Prolo, 2000; Nederhof, 1998), though their evaluation was restricted to the quality of the parsing table (see also (Schabes </context>
</contexts>
<marker>Shieber, Johnson, 1993</marker>
<rawString>Stuart Shieber and Mark Johnson. 1993. Variations on incremental interpretation. Journal of Psycholinguistic Research, 22(2):287–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Sentence disambiguation by a Shift-Reduce parsing technique.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>119--122</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="737" citStr="Shieber, 1983" startWordPosition="112" endWordPosition="113">ty of Pennsylvania prolo@linc.cis.upenn.edu Abstract We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for natural language</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Stuart M. Shieber. 1983. Sentence disambiguation by a Shift-Reduce parsing technique. In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, pages 119–122, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Tree-gram parsing: Lexical dependencies and structural relations.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong, China.</location>
<marker>Sima’an, 2000</marker>
<rawString>Khalil Sima’an. 2000. Tree-gram parsing: Lexical dependencies and structural relations. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>John C Trueswell</author>
</authors>
<title>Sentence comprehension.</title>
<date>1995</date>
<booktitle>Communication,</booktitle>
<pages>217--262</pages>
<editor>In Joanne L. Miller and Peter D. Eiwas, editors, Speech, Language, and</editor>
<publisher>Academic Press,</publisher>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="5448" citStr="Tanenhaus and Trueswell, 1995" startWordPosition="870" endWordPosition="873">escribe here. However, a key aspect to mention is that grammar trees are extracted by factoring of recursion. Even constituents annotated flat in the Treebank are first given a more hierarchical, recursive structure. Therefore the trees generated during parsing will be richer than those in the Treebank. We will return to this point later. Before grammar extraction, Treebank labels are merged to allow for the generation of a more compact grammar and parsing table, and to concentrate statistical information (e.g., NN and NNS; NNP and NNPS; all labels for finite verb forms). The gram2See, e.g., (Tanenhaus and Trueswell, 1995) for a survey on human sentence comprehension. Figure 1: Architecture of the parser mar extractor assigns a plausibility judgment to each extracted tree. When a tree is judged implausible, it is discarded from the grammar, and so are the sentences in the training corpus in which the tree is used. This reduced our training corpus by about 15 %. 2.2 The LR parser generator We used the grammar generator in (Prolo, 2000). In this section we only present a parsing example, to illustrate the kinds of action inserted in the generated tables; details concerning how the table is generated are omitted. </context>
</contexts>
<marker>Tanenhaus, Trueswell, 1995</marker>
<rawString>Michael K. Tanenhaus and John C. Trueswell. 1995. Sentence comprehension. In Joanne L. Miller and Peter D. Eiwas, editors, Speech, Language, and Communication, pages 217–262. Academic Press, San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsingfor Natural Language.</title>
<date>1985</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="696" citStr="Tomita, 1985" startWordPosition="106" endWordPosition="107">omputer and Information Science University of Pennsylvania prolo@linc.cis.upenn.edu Abstract We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts a</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Masaru Tomita. 1985. Efficient Parsingfor Natural Language. Kluwer Academic Publishers, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>E N Wrigley</author>
</authors>
<title>GLR parsing with probability.</title>
<date>1991</date>
<booktitle>In Masaru Tomita, editor, Generalized LR Parsing,</booktitle>
<pages>113--128</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="722" citStr="Wright and Wrigley, 1991" startWordPosition="108" endWordPosition="111">formation Science University of Pennsylvania prolo@linc.cis.upenn.edu Abstract We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars. 1 Introduction The LR approach for parsing has long been considered for natural language parsing (Lang, 1974; Tomita, 1985; Wright and Wrigley, 1991; Shieber, 1983; Pereira, 1985; Merlo, 1996), but it was not until a more recent past, with the advent of corpusbased techniques made possible by the availability of large treebanks, that parsing results and evaluation started being reported (Briscoe and Carroll, 1993; Inui et al., 1997; Carroll and Briscoe, 1996; Ruland, 2000). The appeal of LR parsing (Knuth, 1965) derives from its high capacity of postponement of structural decisions, therefore allowing for much of the spurious local ambiguity to be automatically discarded. But it is still the case that conflicts arise in the LR table for n</context>
<context position="2948" citStr="Wright and Wrigley, 1991" startWordPosition="468" endWordPosition="471">habes and Vijay-Shanker, 1990; Kinyon, 1997) for earlier attempts). In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex&apos;Unlike (Wright and Wrigley, 1991)’s approach who tries to transpose PCFG probabilities to LR tables, facing difficulties which, to the best of our knowledge, have not been yet solved to content (cf. also (Ng and Tomita, 1991; Wright et al., 1991; Abney et al., 1999)). Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 103-110. Association for Computational Linguistics. periment with a simple greedy depth-first technique with limited amount of backtracking, that resembles to a certain extent the commitment/recovery models from the psycholinguistic research on</context>
</contexts>
<marker>Wright, Wrigley, 1991</marker>
<rawString>J. H. Wright and E. N. Wrigley. 1991. GLR parsing with probability. In Masaru Tomita, editor, Generalized LR Parsing, pages 113–128. Kluwer Academic Publishers, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Wright</author>
<author>Ave Wrigley</author>
<author>Richard Sharman</author>
</authors>
<title>Adaptive probabilistic Generalized LR parsing.</title>
<date>1991</date>
<booktitle>In Proceedings of the Second International Workshop on Parsing Technologies (IWPT-91),</booktitle>
<location>Cancun, Mexico.</location>
<contexts>
<context position="3160" citStr="Wright et al., 1991" startWordPosition="504" endWordPosition="507">on is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.&apos; However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex&apos;Unlike (Wright and Wrigley, 1991)’s approach who tries to transpose PCFG probabilities to LR tables, facing difficulties which, to the best of our knowledge, have not been yet solved to content (cf. also (Ng and Tomita, 1991; Wright et al., 1991; Abney et al., 1999)). Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 103-110. Association for Computational Linguistics. periment with a simple greedy depth-first technique with limited amount of backtracking, that resembles to a certain extent the commitment/recovery models from the psycholinguistic research on human language processing, supported by the occurrence of “garden paths”.2 We use the Penn Treebank WSJ corpus, release 2 (Marcus et al., 1994), to evaluate the approach. 2 The architecture of the parser Table 1</context>
</contexts>
<marker>Wright, Wrigley, Sharman, 1991</marker>
<rawString>Jerry Wright, Ave Wrigley, and Richard Sharman. 1991. Adaptive probabilistic Generalized LR parsing. In Proceedings of the Second International Workshop on Parsing Technologies (IWPT-91), Cancun, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Investigating the Relationship between Grammars and Treebanks for Natural Languages.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="4797" citStr="Xia, 2001" startWordPosition="764" endWordPosition="765">trees. This derivation tree is then converted into the sequence of LR parsing actions that would have to be used by the parser to generate exactly that analysis. A parser execution simulation is then performed, guided by the obtained sequence of parsing actions, collecting the statistical information defined in Section 3. In possession of the LR table, grammar and statistical information, the parser is then able to parse fast natural language sentences annotated for partsof-speech. 2.1 The extracted grammar Our target grammar is extracted with a customized version of the extractor defined in (Xia, 2001), which we will not describe here. However, a key aspect to mention is that grammar trees are extracted by factoring of recursion. Even constituents annotated flat in the Treebank are first given a more hierarchical, recursive structure. Therefore the trees generated during parsing will be richer than those in the Treebank. We will return to this point later. Before grammar extraction, Treebank labels are merged to allow for the generation of a more compact grammar and parsing table, and to concentrate statistical information (e.g., NN and NNS; NNP and NNPS; all labels for finite verb forms). </context>
<context position="20463" citStr="Xia, 2001" startWordPosition="3505" endWordPosition="3506">cture is not quite a particular decision of ours, but a consequence of using a sound grammar under the TAG grammatical formalism.9 However, having concluded our manifesto, we understand that algorithms that try to keep precision as high as the recall necessarily have losses in recall compared to if they ignored the precision, and therefore in order to have fair comparison with them and to improve the credibility of our results, we flattened the parse trees in a post-processing step, using a simple rule-based technique on top of some frequency measures for individual grammar trees gathered by (Xia, 2001) and the result is presented in the bottom lines of the table. 9By sound we mean a grammar that properly factors recursion in one way or another. Grammars have been extracted where the right side of a rule reflects exactly each single-level expansion found in the Penn Treebank. We are also aware of a few alternatives in grammatical formalisms that could capture such flatness, e.g., sister adjunction (Chiang, 2000). The most salient positive result is that the parser is able to parse sentences at a rate of about 20 sentences per second. Most of the medium-to-high accuracy parsers take at least </context>
</contexts>
<marker>Xia, 2001</marker>
<rawString>Fei Xia. 2001. Investigating the Relationship between Grammars and Treebanks for Natural Languages. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>