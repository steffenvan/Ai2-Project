<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011132">
<title confidence="0.993288">
Applications of Lexical Information for Algorithmically Composing
Multiple-Choice Cloze Items
</title>
<author confidence="0.988907">
Chao-Lin Liu† Chun-Hung Wang† Zhao-Ming Gao‡ Shang-Ming Huang††Department of Computer Science, National Chengchi University, Taipei 11605, Taiwan
</author>
<affiliation confidence="0.94497">
‡Dept. of Foreign Lang. and Lit., National Taiwan University, Taipei 10617, Taiwan
</affiliation>
<email confidence="0.981385">
†chaolin@nccu.edu.tw, ‡zmgao@ntu.edu.tw
</email>
<bodyText confidence="0.997602052631579">
ABSTRACT1
We report experience in applying techniques for nat-
ural language processing to algorithmically generat-
ing test items for both reading and listening cloze
items. We propose a word sense disambiguation-
based method for locating sentences in which des-
ignated words carry specific senses, and apply a
collocation-based method for selecting distractors
that are necessary for multiple-choice cloze items.
Experimental results indicate that our system was
able to produce a usable item for every 1.6 items it
returned. We also attempt to measure distance be-
tween sounds of words by considering phonetic fea-
tures of the words. With the help of voice synthe-
sizers, we were able to assist the task of compos-
ing listening cloze items. By providing both reading
and listening cloze items, we would like to offer a
somewhat adaptive system for assisting Taiwanese
children in learning English vocabulary.
</bodyText>
<sectionHeader confidence="0.99887" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999248833333333">
Computer-assisted item generation (CAIG) allows
the creation of large-scale item banks, and has at-
tracted active study in the past decade (Deane and
Sheehan, 2003; Irvine and Kyllonen, 2002). Ap-
plying techniques for natural language processing
(NLP), CAIG offers the possibility of creating a
large number of items of different challenging lev-
els, thereby paving a way to make computers more
adaptive to students of different competence. More-
over, with the proliferation of Web contents, one
may search and sift online text files for candidate
sentences, and come up with a list of candidate cloze
</bodyText>
<note confidence="0.7939255">
1A portion of results reported in this paper will be expanded
in (Liu et al., 2005; Huang et al., 2005).
</note>
<bodyText confidence="0.9995905">
items economically. This unleashes the topics of the
test items from being confined by item creators’ per-
sonal interests.
NLP techniques serve to generate multiple-choice
cloze items in different ways. (For brevity, we use
cloze items or items for multiple-choice cloze items
henceforth.) One may create sentences from scratch
by applying template-based methods (Dennis et al.,
2002) or more complex methods based on some pre-
determined principles (Deane and Sheehan, 2003).
Others may take existing sentences from a corpus,
and select those that meet the criteria for becoming
test items. The former approach provides specific
and potentially well-controlled test items at the costs
of more complex systems than the latter, e.g., (Shee-
han et al., 2003). Nevertheless, as the Web provides
ample text files at our disposal, we may filter the
text sources stringently for obtaining candidate test
items of higher quality. Administrators can then se-
lect really usable items from these candidates at a
relatively lower cost.
Some researchers have already applied NLP tech-
niques to the generation of sentences for multiple-
choice cloze items. Stevens (1991) employs the con-
cepts of concordance and collocation for generating
items with general corpora. Coniam (1997) relies on
factors such as word frequencies in a tagged corpus
for creating test items of particular types.
There are other advanced NLP techniques that
may help to create test items of higher quality. For
instance, many words in English may carry multiple
senses, and test administrators usually want to test a
particular usage of the word in an item. In this case,
blindly applying a keyword matching method, such
as a concordancer, may lead us to a list of irrelevant
sentences that would demand a lot of postprocess-
</bodyText>
<page confidence="0.822326">
1
</page>
<note confidence="0.9856635">
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 1–8, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<figure confidence="0.990331">
Tagged
Corpus
Target-Dependent
Item Requirements
Target
Sentence
</figure>
<figureCaption confidence="0.879395">
Figure 1: A multiple-choice cloze item for English
</figureCaption>
<figure confidence="0.990538">
Cloze
Item
Item
Specification
Sentence
Retriever with WSD
Distractor
Generator
</figure>
<bodyText confidence="0.999737131578947">
ing workload. In addition, composing a cloze item
requires not just a useful sentence.
Figure 1 shows a multiple-choice item, where we
call the sentence with a gap the stem, the answer to
the gap the key, and the other choices the distrac-
tors. Given a sentence, we still need distractors for
a multiple-choice item. The selection of distractors
affects the item facility and item discrimination of
the cloze items (Poel and Weatherly, 1997). There-
fore, the selection of distractors calls for deliberate
strategies, and simple considerations alone, such as
word frequencies, may not satisfy the demands.
To remedy these shortcomings, we employ the
techniques for word sense disambiguation (WSD)
for choosing sentences in which the keys carries spe-
cific senses, and utilize the techniques for comput-
ing collocations (Manning and Sch¨utze, 1999) for
selecting distractors. Results of empirical evaluation
show that our methods could create items of satisfac-
tory quality, and we have actually used the generated
cloze items in freshmen-level English classes.
For broadening the formats of cloze items, we
also design software that assists teachers to create
listening cloze items. After we defining a metric
for measuring similarity between pronunciations of
words, our system could choose distractors for lis-
tening cloze items. This addition opens a door to
offering different challenging levels of cloze items.
We sketch the flow of the item generation pro-
cess in Section 2, and explain the preparation of the
source corpus in Section 3. In Section 4, we elab-
orate on the application of WSD to selecting sen-
tences for cloze items, and, in Section 5, we delve
into the application of collocations to distractor gen-
eration. Results of evaluating the created reading
cloze items are presented in Section 6. We then
outline methods for creating listening cloze items in
Section 7 before making some concluding remarks.
</bodyText>
<sectionHeader confidence="0.928615" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.76451225">
Figure 2 shows major steps for creating cloze items.
Constrained by test administrator’s specifications
and domain dependent requirements, the Sentence
Retriever chooses a candidate sentence from the
</bodyText>
<figureCaption confidence="0.984183">
Figure 2: Main components of our item generator
</figureCaption>
<bodyText confidence="0.964805">
Tagged Corpus. Target-Dependent Item Require-
ments specify general principles that should be fol-
lowed by all items for a particular test. For example,
the number of words in cloze items for College En-
trance Examinations in Taiwan (CEET) ranges be-
tween 6 and 28 (Liu et al., 2005), and one may want
to follow this tradition in creating drill tests.
Figure 3 shows the interface to the Item Specifi-
cation. Through this interface, test administrators
select the key for the desired cloze item, and specify
part-of-speech and sense of the key that will be used
in the item. Our system will attempt to create the re-
quested number of items. After retrieving the target
sentence, the Distractor Generator considers such
constraining factors as word frequencies and collo-
cations in selecting the distractors at the second step.
</bodyText>
<figureCaption confidence="0.990441">
Figure 3: Interface for specifying cloze items
</figureCaption>
<bodyText confidence="0.999518333333333">
Figure 4 shows a sample output for the specifica-
tion shown in Figure 3. Given the generated items,
the administrator may choose and edit the items, and
save the edited items into the item bank. It is possi-
ble to retrieve previously saved items from the item
bank, and compile the items for different tests.
</bodyText>
<sectionHeader confidence="0.903841" genericHeader="method">
3 Source Corpus and Lexicons
</sectionHeader>
<bodyText confidence="0.988147875">
Employing a web crawler, we retrieve the con-
tents of Taiwan Review &lt;publish.gio.gov.tw&gt;, Tai-
wan Journal &lt;taiwanjournal.nat.gov.tw&gt;, and China
Post &lt;www.chinapost.com.tw&gt;. Currently, we have
127,471 sentences that consist of 2,771,503 words
in 36,005 types in the corpus. We look for use-
ful sentences from web pages that are encoded in
the HTML format. We need to extract texts from
</bodyText>
<page confidence="0.997773">
2
</page>
<figureCaption confidence="0.999216">
Figure 4: An output after Figure 3
</figureCaption>
<bodyText confidence="0.999908865384616">
the mixture of titles, main body of the reports,
and multimedia contents, and then segment the ex-
tracted paragraphs into individual sentences. We
segment sentences with the help of MXTERMINA-
TOR (Reynar and Ratnaparkhi, 1997). We then tok-
enize words in the sentences before assigning useful
tags to the tokens.
We augment the text with an array of tags that
facilitate cloze item generation. We assign tags of
part-of-speech (POS) to the words with MXPOST
that adopts the Penn Treebank tag set (Ratnaparkhi,
1996). Based on the assigned POS tags, we annotate
words with their lemmas. For instance, we annotate
classified with classify and classified, respectively,
when the original word has VBN and JJ as its POS
tag. We also employ MINIPAR (Lin, 1998) to ob-
tain the partial parses of sentences that we use exten-
sively in our system. Words with direct relationships
can be identified easily in the partially parsed trees,
and we rely heavily on these relationships between
words for WSD. For easy reference, we will call
words that have direct syntactic relationship with a
word W as W’s signal words or simply signals.
Since we focus on creating items for verbs, nouns,
adjectives, and adverbs (Liu et al., 2005), we care
about signals of words with these POS tags in sen-
tences for disambiguating word senses. Specifically,
the signals of a verb include its subject, object, and
the adverbs that modify the verb. The signals of a
noun include the adjectives that modify the noun and
the verb that uses the noun as its object or predicate.
For instance, in “Jimmy builds a grand building.”,
both “build” and “grand” are signals of “building”.
The signals of adjectives and adverbs include the
words that they modify and the words that modify
the adjectives and adverbs.
When we need lexical information about English
words, we resort to electronic lexicons. We use
WordNet &lt;www.cogsci.princeton.edu/˜wn/&gt; when
we need definitions and sample sentences of words
for disambiguating word senses, and we employ
HowNet &lt;www.keenage.com&gt; when we need infor-
mation about classes of verbs, nouns, adjectives, and
adverbs.
HowNet is a bilingual lexicon. An entry in
HowNet includes slots for Chinese words, English
words, POS information, etc. We rely heavily on the
slot that records the semantic ingredients related to
the word being defined. HowNet uses a limited set
of words in the slot for semantic ingredient, and the
leading ingredient in the slot is considered to be the
most important one generally.
</bodyText>
<sectionHeader confidence="0.986747" genericHeader="method">
4 Target Sentence Retriever
</sectionHeader>
<bodyText confidence="0.999940666666667">
The sentence retriever in Figure 2 extracts qualified
sentences from the corpus. A sentence must contain
the desired key of the requested POS to be consid-
ered as a candidate target sentence. Having identi-
fied such a candidate sentence, the item generator
needs to determine whether the sense of the key also
meets the requirement. We conduct this WSD task
based on an extended notion of selectional prefer-
ences.
</bodyText>
<subsectionHeader confidence="0.998572">
4.1 Extended Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.999919636363637">
Selectional preferences generally refer to the phe-
nomenon that, under normal circumstances, some
verbs constrain the meanings of other words in
a sentence (Manning and Sch¨utze, 1999; Resnik,
1997). We can extend this notion to the relation-
ships between a word of interest and its signals, with
the help of HowNet. Let w be the word of interest,
and 7r be the first listed class, in HowNet, of a signal
word that has the syntactic relationship p with w.
We define the strength of the association of w and 7r
as follows:
</bodyText>
<equation confidence="0.9946685">
_ Pr µ w 7r
Aµ(w,7r) Prµ(w) , (1)
</equation>
<bodyText confidence="0.998848666666667">
where Prµ(w) is the probability of w participating in
the p relationship, and Prµ(w, 7r) is the probability
that both w and 7r participate in the p relationship.
</bodyText>
<subsectionHeader confidence="0.997175">
4.2 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999844333333333">
We employ the generalized selectional preferences
to determine the sense of a polysemous word in a
sentence. Consider the task of determining the sense
</bodyText>
<page confidence="0.994901">
3
</page>
<bodyText confidence="0.99367975">
of “spend” in the candidate target sentence “They
say film makers don’t spend enough time developing
a good story.” The word “spend” has two possible
meanings in WordNet.
</bodyText>
<listItem confidence="0.9900474">
1. (99) spend, pass – (pass (time) in a specific
way; “How are you spending your summer va-
cation?”)
2. (36) spend, expend, drop – (pay out; “I spend
all my money in two days.”)
</listItem>
<bodyText confidence="0.999954696428572">
Each definition of the possible senses include (1)
the head words that summarize the intended mean-
ing and (2) a sample sentence for sense. When we
work on the disambiguation of a word, we do not
consider the word itself as a head word in the follow-
ing discussion. Hence, “spend” has one head word,
i.e., “pass”, in the first sense and two head words,
i.e., “extend” and “drop”, in the second sense.
An intuitive method for determining the mean-
ing of “spend” in the target sentence is to replace
“spend” with its head words in the target sentence.
The head words of the correct sense should go with
the target sentence better than head words of other
senses. This intuition leads to the a part of the scores
for senses, i.e., St that we present shortly.
In addition, we can compare the similarity of the
contexts of “spend” in the target sentence and sam-
ple sentences, where context refers to the classes of
the signals of the word being disambiguated. For the
current example, we can check whether the subject
and object of “spend” in the target sentence have the
same classes as the subjects and objects of “spend”
in the sample sentences. The sense whose sample
sentence offers a more similar context for “spend” in
the target sentence receives a higher score. This in-
tuition leads to the other part of the scores for senses,
i.e., Ss that we present below.
Assume that the key w has n senses. Let O =
{θ1, θ2, · · · , θn} be the set of senses of w. Assume
that sense θj of word w has mj head words in Word-
Net. (Note that we do not consider w as its own head
word.) We use the set Aj = {λj,1, λj,2, · · · , λj,mi}
to denote the set of head words that WordNet pro-
vides for sense θj of word w.
When we use the partial parser to parse the tar-
get sentence T for a key, we obtain information
about the signal words of the key. Moreover, for
each of these signals, we look up their classes in
HowNet, and adopt the first listed class for each of
the signals when the signal covers multiple classes.
Assume that there are µ(T) signals for the key
w in a sentence T. We use the set IF(T, w) =
{ψ1,T, ψ2,T, · · · , ψµ(T),T } to denote the set of sig-
nals for w in T. Correspondingly, we use υj,T to de-
note the syntactic relationship between w and ψj,T
in T, use T(T, w) = {υ1,T, υ2,T, · · · , υµ(T),T} for
the set of relationships between signals in IF(T, w)
and w, use πj,T for the class of ψj,T, and use
H(T, w) = {π1,T, π2,T, · · · , πµ(T),T} for the set of
classes of the signals in IF(T, w).
Equation (2) measures the average strength of as-
sociation of the head words of a sense with signals
of the key in T, so we use (2) as a part of the score
for w to take the sense θj in the target sentence T.
Note that both the strength of association and St fall
in the range of [0,1].
</bodyText>
<equation confidence="0.993894">
St(θj|w, T)
Aµl,T (λj,k, πl,T) (2)
</equation>
<bodyText confidence="0.985184190476191">
In (2), we have assumed that the signal words
are not polysemous. If they are polysemous, we as-
sume that each of the candidate sense of the signal
words are equally possible, and employ a slightly
more complicated formula for (2). This assumption
may introduce errors into our decisions, but relieves
us from the needs to disambiguate the signal words
in the first place (Liu et al., 2005).
Since WordNet provides sample sentences for im-
portant words, we also use the degrees of similarity
between the sample sentences and the target sen-
tence to disambiguate the word senses of the key
word in the target sentence. Let T and S be the tar-
get sentence of w and a sample sentence of sense θj
of w, respectively. We compute this part of score,
Ss, for θj using the following three-step procedure.
If there are multiple sample sentences for a given
sense, say θj of w, we will compute the score in (3)
for each sample sentence of θj, and use the average
score as the final score for θj.
Procedure for computing Ss(θj|w, T)
</bodyText>
<footnote confidence="0.446141666666667">
1. Compute signals of the key and their relation-
ships with the key in the target and sample sen-
tences.
</footnote>
<equation confidence="0.921511666666667">
1 mi
E
k=1
1 µ(T)
µ(T) l=1
mj
</equation>
<page confidence="0.957476">
4
</page>
<table confidence="0.8684445">
IF(T, w) = {ψ1,T, ψ2,T, ··· , ψµ(T),T },
T(T, w) = {υ1,T, υ2,T, ··· , υµ(T),T},
&apos;F(S, w) = {ψ1,S, ψ2,S, ··· , ψµ(S),S}, and
T(S, w) = {υ1,S, υ2,S, ··· , υµ(S),S}
</table>
<listItem confidence="0.893709272727273">
2. We look for ψj,T and ψk,S such that υj,T =
υk,S, and then check whether πj,T = πk,S.
Namely, for each signal of the key in T, we
check the signals of the key in S for matching
syntactic relationships and word classes, and
record the counts of matched relationship in
M(θj, T) (Liu et al., 2005).
3. The following score measures the proportion of
matched relationships among all relationships
between the key and its signals in the target sen-
tence.
</listItem>
<equation confidence="0.982989666666667">
M(θj, T )
Ss(θj|w,T) = (3)
µ(T )
</equation>
<bodyText confidence="0.999554333333333">
The score for w to take sense θj in a target sen-
tence T is the sum of St(θj|w, T) defined in (2)
and Ss(θj|w, T) defined in (3), so the sense of w
in T will be set to the sense defined in (4) when
the score exceeds a selected threshold. When the
sum of St(θj|w, T) and Ss(θj|w, T) is smaller than
the threshold, we avoid making arbitrary decisions
about the word senses. We discuss and illustrate ef-
fects of choosing different thresholds in Section 6.
</bodyText>
<equation confidence="0.760986">
arg max St(θj|w, T) + Ss(θj|w, T) (4)
θjEO
</equation>
<sectionHeader confidence="0.964598" genericHeader="method">
5 Distractor Generation
</sectionHeader>
<bodyText confidence="0.999976018867925">
Distractors in multiple-choice items influence the
possibility of making lucky guesses to the answers.
Should we use extremely impossible distractors in
the items, examinees may be able to identify the
correct answers without really knowing the keys.
Hence, we need to choose distractors that appear to
fit the gap, and must avoid having multiple answers
to items in a typical cloze test at the same time.
There are some conceivable principles and al-
ternatives that are easy to implement and follow.
Antonyms of the key are choices that average exam-
inees will identify and ignore. The part-of-speech
tags of the distractors should be the same as the
key in the target sentence. We may also take cul-
tural background into consideration. Students in
Taiwan tend to associate English vocabularies with
their Chinese translations. Although this learning
strategy works most of the time, students may find
it difficult to differentiate English words that have
very similar Chinese translations. Hence, a culture-
dependent strategy is to use English words that have
similar Chinese translations with the key as the dis-
tractors.
To generate distractors systematically, we employ
ranks of word frequencies for selecting distractors
(Poel and Weatherly, 1997). Assume that we are
generating an item for a key whose part-of-speech
is ρ, that there are n word types whose part-of-
speech may be ρ in the dictionary, and that the rank
of frequency of the key among these n types is m.
We randomly select words that rank in the range
[m−n/10, m+n/10] among these n types as candi-
date distractors. These distractors are then screened
by their fitness into the target sentence, where fitness
is defined based on the concept of collocations of
word classes, defined in HowNet, of the distractors
and other words in the stem of the target sentence.
Recall that we have marked words in the corpus
with their signals in Section 3. The words that have
more signals in a sentence usually contribute more to
the meaning of the sentence, so should play a more
important role in the selection of distractors. Since
we do not really look into the semantics of the tar-
get sentences, a relatively safer method for selecting
distractors is to choose those words that seldom col-
locate with important words in the target sentence.
Let T = {t1, t2, · · · , tn} denote the set of words
in the target sentence. We select a set T&apos; ⊂ T such
that each t&apos;i ∈ T&apos; has two or more signals in T and is
a verb, noun, adjective, or adverb. Let κ be the first
listed class, in HowNet, of the candidate distractor,
and ℵ = {τi|τi is the first listed class of a t&apos;i ∈ T&apos;}.
The fitness of a candidate distractor is defined in (5).
</bodyText>
<equation confidence="0.991655">
−1
|ℵ |τaER log Pr(κ) Pr(τi)
Pr(κ, τi) (5)
</equation>
<bodyText confidence="0.999909666666667">
The candidate whose score is better than 0.3 will
be admitted as a distractor. Pr(κ) and Pr(τi) are
the probabilities that each word class appears indi-
vidually in the corpus, and Pr(κ, τi) is the proba-
bility that the two classes appear in the same sen-
tence. Operational definitions of these probabilities
</bodyText>
<page confidence="0.998356">
5
</page>
<tableCaption confidence="0.999299">
Table 1: Accuracy of WSD
</tableCaption>
<table confidence="0.755717">
POS baseline threshold=0.4 threshold=0.7
verb 38.0%(19/50) 57.1%(16/28) 68.4%(13/19)
noun 34.0%(17/50) 63.3%(19/30) 71.4%(15/21)
adj. 26.7%(8/30) 55.6%(10/18) 60.0%(6/10)
adv. 36.7%(11/30) 52.4%(11/21) 58.3%(7/12)
</table>
<bodyText confidence="0.997955625">
are provided in (Liu et al., 2005). The term in the
summation is a pointwise mutual information, and
measures how often the classes r. and TZ collocate
in the corpus. We negate the averaged sum so that
classes that seldom collocate receive higher scores.
We set the threshold to 0.3, based on statistics of (5)
that are observed from the cloze items used in the
1992-2003 CEET.
</bodyText>
<sectionHeader confidence="0.971572" genericHeader="method">
6 Evaluations and Applications
</sectionHeader>
<subsectionHeader confidence="0.998128">
6.1 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999751689655172">
Different approaches to WSD were evaluated in dif-
ferent setups, and a very wide range of accuracies in
[40%, 90%] were reported (Resnik, 1997; Wilks and
Stevenson, 1997). Objective comparisons need to be
carried out on a common test environment like SEN-
SEVAL, so we choose to present only our results.
We arbitrarily chose, respectively, 50, 50, 30,
and 30 sentences that contained polysemous verbs,
nouns, adjectives, and adverbs for disambiguation.
Table 1 shows the percentage of correctly disam-
biguated words in these 160 samples.
The baseline column shows the resulting accu-
racy when we directly use the most frequent sense,
recorded in WordNet, for the polysemous words.
The rightmost two columns show the resulting accu-
racy when we used different thresholds for applying
(4). As we noted in Section 4.2, our system selected
fewer sentences when we increased the threshold, so
the selected threshold affected the performance. A
larger threshold led to higher accuracy, but increased
the rejection rate at the same time. Since the cor-
pus can be extended to include more and more sen-
tences, we afford to care about the accuracy more
than the rejection rate of the sentence retriever.
We note that not every sense of all words have
sample sentences in the WordNet. When a sense
does not have any sample sentence, this sense will
receive no credit, i.e., 0, for C5, Consequently,
our current reliance on sample sentences in Word-
</bodyText>
<tableCaption confidence="0.994081">
Table 2: Correctness of the generated sentences
</tableCaption>
<table confidence="0.9752615">
POS of the key # of items % of correct sentences
verb 77 66.2%
noun 62 69.4%
adjective 35 60.0%
adverb 26 61.5%
overall 65.5%
</table>
<tableCaption confidence="0.9792">
Table 3: Uniqueness of answers
</tableCaption>
<table confidence="0.9951485">
item category key’s POS number of items results
cloze verb 64 90.6%
noun 57 94.7%
adjective 46 93.5%
adverb 33 84.8%
overall 91.5%
</table>
<bodyText confidence="0.9986934">
Net makes us discriminate against senses that do not
have sample sentences. This is an obvious draw-
back in our current design, but the problem is not
really detrimental and unsolvable. There are usually
sample sentences for important and commonly-used
senses of polysemous words, so the discrimination
problem does not happen frequently. When we do
want to avoid this problem once and for all, we can
customize WordNet by adding sample sentences to
all senses of important words.
</bodyText>
<subsectionHeader confidence="0.999503">
6.2 Cloze Item Generation
</subsectionHeader>
<bodyText confidence="0.999929368421053">
We asked the item generator to create 200 items in
the evaluation. To mimic the distribution over keys
of the cloze items that were used in CEET, we used
77, 62, 35, and 26 items for verbs, nouns, adjectives,
and adverbs, respectively, in the evaluation.
In the evaluation, we requested one item at a time,
and examined whether the sense and part-of-speech
of the key in the generated item really met the re-
quests. The threshold for using (4) to disambiguate
word sense was set to 0.7. Results of this experi-
ment, shown in Table 2, do not differ significantly
from those reported in Table 1. For all four major
classes of cloze items, our system was able to re-
turn a correct sentence for less than every 2 items
it generated. In addition, we checked the quality of
the distractors, and marked those items that permit-
ted unique answers as good items. Table 3 shows
that our system was able to create items with unique
answers for another 200 items most of the time.
</bodyText>
<page confidence="0.997714">
6
</page>
<figureCaption confidence="0.99914">
Figure 5: A phonetic concordancer
</figureCaption>
<subsectionHeader confidence="0.999462">
6.3 More Applications
</subsectionHeader>
<bodyText confidence="0.999995117647059">
We have used the generated items in real tests in a
freshman-level English class at National Chengchi
University, and have integrated the reported item
generator in a Web-based system for learning En-
glish. In this system, we have two major subsys-
tems: the authoring and the assessment subsystems.
Using the authoring subsystem, test administrators
may select items from the interface shown in Fig-
ure 4, save the selected items to an item bank, edit
the items, including their stems if necessary, and fi-
nalize the selection of the items for a particular ex-
amination. Using the assessment subsystem, stu-
dents answer the test items via the Internet, and
can receive grades immediately if the administra-
tors choose to do so. The answers of students are
recorded for student modelling and analysis of the
item facility and the item discrimination.
</bodyText>
<sectionHeader confidence="0.982453" genericHeader="method">
7 Generating Listening Cloze Items
</sectionHeader>
<bodyText confidence="0.99984680952381">
We apply the same infrastructure for generating
reading cloze items, shown in Figure 2, for the gen-
eration of listening cloze items (Huang et al., 2005).
Due to the educational styles in Taiwan, students
generally find it more difficult to comprehend mes-
sages by listening than by reading. Hence, we can
regard listening cloze tests as an advanced format of
reading cloze tests. Having constructed a database
of sentences, we can extract sentences that contain
the key for which the test administrator would like
to have a listening cloze, and employ voice synthe-
sizers to create the necessary recordings.
Figure 5 shows an interface through which ad-
ministrators choose and edit sentences for listening
cloze items. Notice that we employ the concept that
is related to ordinary concordance in arranging the
extracted sentences. By defining a metric for mea-
suring similarity between sounds, we can put sen-
tences that have similar phonetic contexts around the
key near each other. We hope this would better help
teachers in selecting sentences by this rudimentary
</bodyText>
<figureCaption confidence="0.996946">
Figure 6: The most simple form of listening cloze
</figureCaption>
<bodyText confidence="0.999371285714286">
clustering of sentences.
Figure 6 shows the most simple format of listen-
ing cloze items. In this format, students click on the
options, listen to the recorded sounds, and choose
the option that fit the gap. The item shown in this
figure is very similar to that shown in Figure 1, ex-
cept that students read and hear the options. From
this most primitive format, we can image and imple-
ment other more challenging formats. For instance,
we can replace the stem, currently in printed form in
Figure 6, into clickable links, demanding students
to hear the stem rather than reading the stem. A
middle ground between this more challenging for-
mat and the original format in the figure is to allow
the gap to cover more words in the original sentence.
This would require the students to listen to a longer
stream of sound, so can be a task more challenging
than the original test. In addition to controlling the
lengths of the answer voices, we can try to modulate
the speed that the voices are replayed. Moreover,
for multiple-word listening cloze, we may try to find
word sequences that sound similar to the answer se-
quence to control the difficulty of the test item.
Defining a metric for measuring similarity be-
tween two recordings is the key to support the afore-
mentioned functions. In (Huang et al., 2005), we
consider such features of phonemes as place and
manner of pronunciation in calculating the similarity
between sounds. Using this metric we choose as dis-
tractors those sounds of words that have similar pro-
nunciation with the key of the listening cloze. We
have to define the distance between each phoneme
so that we could employ the minimal-edit-distance
algorithm for computing the distance between the
sounds of different words.
</bodyText>
<page confidence="0.999243">
7
</page>
<sectionHeader confidence="0.980835" genericHeader="conclusions">
8 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999996078947368">
We believe that NLP techniques can play an impor-
tant role in computer assisted language learning, and
this belief is supported by papers in this workshop
and the literature. What we have just explored is
limited to the composition of cloze items for English
vocabulary. With the assistance of WSD techniques,
our system was able to identify sentences that were
qualified as candidate cloze items 65% of the time.
Considering both word frequencies and collocation,
our system recommended distractors for cloze items,
resulting in items that had unique answers 90% of
the time. In addition to assisting the composition
of cloze items in the printed format, our system is
also capable of helping the composition of listening
cloze items. The current system considers features
of phonemes in computing distances between pro-
nunciations of different word strings.
We imagine that NLP and other software tech-
niques could empower us to create cloze items for a
wide range of applications. We could control the for-
mats, contents, and timing of the presented material
to manipulate the challenging levels of the test items.
As we have indicated in Section 7, cloze items in the
listening format are harder than comparable items in
the printed format. We can also control when and
what the students can hear to fine tune the difficul-
ties of the listening cloze items.
We must admit, however, that we do not have suf-
ficient domain knowledge in how human learn lan-
guages. Consequently, tools offered by computing
technologies that appear attractive to computer sci-
entists or computational linguists might not provide
effective assistance for language learning or diagno-
sis. Though we have begun to study item compari-
son from a mathematical viewpoint (Liu, 2005), the
current results are far from being practical. Exper-
tise in psycholinguistics may offer a better guidance
on our system design, we suppose.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997737285714286">
We thank anonymous reviewers for their invaluable
comments on a previous version of this report. We
will respond to some suggestions that we do not have
space to do so in this report in the workshop. This
research was supported in part by Grants 93-2213-E-
004-004 and 93-2411-H-002-013 from the National
Science Council of Taiwan.
</bodyText>
<sectionHeader confidence="0.993308" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943388888889">
D. Coniam. 1997. A preliminary inquiry into using corpus
word frequency data in the automatic generation of English
language cloze tests. Computer Assisted Language Instruc-
tion Consortium, 16(2–4):15–33.
P. Deane and K. Sheehan. 2003. Automatic item gen-
eration via frame semantics. Education Testing Service:
http://www.ets.org/research/dload/ncme03-deane.pdf.
I. Dennis, S. Handley, P. Bradon, J. Evans, and S. Nestead.
2002. Approaches to modeling item-generative tests. In
Item generation for test development (Irvine and Kyllonen,
2002), pages 53–72.
S.-M. Huang, C.-L. Liu, and Z.-M. Gao. 2005. Computer-
assisted item generation for listening cloze tests and dictation
practice in English. In Proc. of the 4th Int. Conf. on Web-
based Learning. to appear.
S. H. Irvine and P. C. Kyllonen, editors. 2002. Item Genera-
tion for Test Development. Lawrence Erlbaum Associates,
Mahwah, NJ.
D. Lin. 1998. Dependency-based evaluation of MINIPAR. In
Proc. of the Workshop on the Evaluation of Parsing Systems
in the 1st Int. Conf. on Language Resources and Evaluation.
C.-L. Liu, C.-H. Wang, and Z.-M. Gao. 2005. Using lexi-
cal constraints for enhancing computer-generated multiple-
choice cloze items. Int. J. of Computational Linguistics and
Chinese Language Processing, 10:to appear.
C.-L. Liu. 2005. Using mutual information for adaptive item
comparison and student assessment. J. of Educational Tech-
nology &amp; Society, 8(4):to appear.
C. D. Manning and H. Sch¨utze. 1999. Foundations of Statisti-
cal Natural Language Processing. MIT Press, Cambridge.
C. J. Poel and S. D. Weatherly. 1997. A cloze look at place-
ment testing. Shiken: JALT (Japanese Assoc. for Language
Teaching) Testing &amp; Evaluation SIG Newsletter, 1(1):4–10.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Proc. of the Conf. on Empirical Methods in Natural
Language Processing, pages 133–142.
P. Resnik. 1997. Selectional preference and sense disambigua-
tion. In Proc. of the Applied NLP Workshop on Tagging Text
with Lexical Semantics: Why, What and How, pages 52–57.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy
approach to identifying sentence boundaries. In Proc. of the
Conf. on Applied Natural Language Processing, pages 16–
19.
K. M. Sheehan, P. Deane, and I. Kostin. 2003. A partially auto-
mated system for generating passage-based multiple-choice
verbal reasoning items. Paper presented at the Nat’l Council
on Measurement in Education Annual Meeting.
V. Stevens. 1991. Classroom concordancing: vocabulary ma-
terials derived from relevant authentic text. English for Spe-
cific Purposes, 10(1):35–46.
Y. Wilks and M. Stevenson. 1997. Combining independent
knowledge sources for word sense disambiguation. In Proc.
of the Conf. on Recent Advances in Natural Language Pro-
cessing, pages 1–7.
</reference>
<page confidence="0.998493">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.612589">
<title confidence="0.985872">Applications of Lexical Information for Algorithmically Multiple-Choice Cloze Items</title>
<abstract confidence="0.9772128">of Computer Science, National Chengchi University, Taipei 11605, of Foreign Lang. and Lit., National Taiwan University, Taipei 10617, We report experience in applying techniques for natural language processing to algorithmically generating test items for both reading and listening cloze items. We propose a word sense disambiguationbased method for locating sentences in which designated words carry specific senses, and apply a collocation-based method for selecting distractors that are necessary for multiple-choice cloze items. Experimental results indicate that our system was able to produce a usable item for every 1.6 items it returned. We also attempt to measure distance between sounds of words by considering phonetic features of the words. With the help of voice synthesizers, we were able to assist the task of composing listening cloze items. By providing both reading and listening cloze items, we would like to offer a somewhat adaptive system for assisting Taiwanese children in learning English vocabulary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Coniam</author>
</authors>
<title>A preliminary inquiry into using corpus word frequency data in the automatic generation of English language cloze tests. Computer Assisted Language Instruction Consortium,</title>
<date>1997</date>
<pages>16--2</pages>
<contexts>
<context position="3242" citStr="Coniam (1997)" startWordPosition="492" endWordPosition="493">l-controlled test items at the costs of more complex systems than the latter, e.g., (Sheehan et al., 2003). Nevertheless, as the Web provides ample text files at our disposal, we may filter the text sources stringently for obtaining candidate test items of higher quality. Administrators can then select really usable items from these candidates at a relatively lower cost. Some researchers have already applied NLP techniques to the generation of sentences for multiplechoice cloze items. Stevens (1991) employs the concepts of concordance and collocation for generating items with general corpora. Coniam (1997) relies on factors such as word frequencies in a tagged corpus for creating test items of particular types. There are other advanced NLP techniques that may help to create test items of higher quality. For instance, many words in English may carry multiple senses, and test administrators usually want to test a particular usage of the word in an item. In this case, blindly applying a keyword matching method, such as a concordancer, may lead us to a list of irrelevant sentences that would demand a lot of postprocess1 Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,</context>
</contexts>
<marker>Coniam, 1997</marker>
<rawString>D. Coniam. 1997. A preliminary inquiry into using corpus word frequency data in the automatic generation of English language cloze tests. Computer Assisted Language Instruction Consortium, 16(2–4):15–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Deane</author>
<author>K Sheehan</author>
</authors>
<title>Automatic item generation via frame semantics. Education Testing Service:</title>
<date>2003</date>
<contexts>
<context position="1442" citStr="Deane and Sheehan, 2003" startWordPosition="206" endWordPosition="209">as able to produce a usable item for every 1.6 items it returned. We also attempt to measure distance between sounds of words by considering phonetic features of the words. With the help of voice synthesizers, we were able to assist the task of composing listening cloze items. By providing both reading and listening cloze items, we would like to offer a somewhat adaptive system for assisting Taiwanese children in learning English vocabulary. 1 Introduction Computer-assisted item generation (CAIG) allows the creation of large-scale item banks, and has attracted active study in the past decade (Deane and Sheehan, 2003; Irvine and Kyllonen, 2002). Applying techniques for natural language processing (NLP), CAIG offers the possibility of creating a large number of items of different challenging levels, thereby paving a way to make computers more adaptive to students of different competence. Moreover, with the proliferation of Web contents, one may search and sift online text files for candidate sentences, and come up with a list of candidate cloze 1A portion of results reported in this paper will be expanded in (Liu et al., 2005; Huang et al., 2005). items economically. This unleashes the topics of the test i</context>
</contexts>
<marker>Deane, Sheehan, 2003</marker>
<rawString>P. Deane and K. Sheehan. 2003. Automatic item generation via frame semantics. Education Testing Service: http://www.ets.org/research/dload/ncme03-deane.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dennis</author>
<author>S Handley</author>
<author>P Bradon</author>
<author>J Evans</author>
<author>S Nestead</author>
</authors>
<title>Approaches to modeling item-generative tests. In Item generation for test development (Irvine and Kyllonen,</title>
<date>2002</date>
<pages>53--72</pages>
<contexts>
<context position="2366" citStr="Dennis et al., 2002" startWordPosition="353" endWordPosition="356">tion of Web contents, one may search and sift online text files for candidate sentences, and come up with a list of candidate cloze 1A portion of results reported in this paper will be expanded in (Liu et al., 2005; Huang et al., 2005). items economically. This unleashes the topics of the test items from being confined by item creators’ personal interests. NLP techniques serve to generate multiple-choice cloze items in different ways. (For brevity, we use cloze items or items for multiple-choice cloze items henceforth.) One may create sentences from scratch by applying template-based methods (Dennis et al., 2002) or more complex methods based on some predetermined principles (Deane and Sheehan, 2003). Others may take existing sentences from a corpus, and select those that meet the criteria for becoming test items. The former approach provides specific and potentially well-controlled test items at the costs of more complex systems than the latter, e.g., (Sheehan et al., 2003). Nevertheless, as the Web provides ample text files at our disposal, we may filter the text sources stringently for obtaining candidate test items of higher quality. Administrators can then select really usable items from these ca</context>
</contexts>
<marker>Dennis, Handley, Bradon, Evans, Nestead, 2002</marker>
<rawString>I. Dennis, S. Handley, P. Bradon, J. Evans, and S. Nestead. 2002. Approaches to modeling item-generative tests. In Item generation for test development (Irvine and Kyllonen, 2002), pages 53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-M Huang</author>
<author>C-L Liu</author>
<author>Z-M Gao</author>
</authors>
<title>Computerassisted item generation for listening cloze tests and dictation practice in English.</title>
<date>2005</date>
<booktitle>In Proc. of the 4th Int. Conf. on Webbased Learning.</booktitle>
<note>to appear.</note>
<contexts>
<context position="1981" citStr="Huang et al., 2005" startWordPosition="296" endWordPosition="299">anks, and has attracted active study in the past decade (Deane and Sheehan, 2003; Irvine and Kyllonen, 2002). Applying techniques for natural language processing (NLP), CAIG offers the possibility of creating a large number of items of different challenging levels, thereby paving a way to make computers more adaptive to students of different competence. Moreover, with the proliferation of Web contents, one may search and sift online text files for candidate sentences, and come up with a list of candidate cloze 1A portion of results reported in this paper will be expanded in (Liu et al., 2005; Huang et al., 2005). items economically. This unleashes the topics of the test items from being confined by item creators’ personal interests. NLP techniques serve to generate multiple-choice cloze items in different ways. (For brevity, we use cloze items or items for multiple-choice cloze items henceforth.) One may create sentences from scratch by applying template-based methods (Dennis et al., 2002) or more complex methods based on some predetermined principles (Deane and Sheehan, 2003). Others may take existing sentences from a corpus, and select those that meet the criteria for becoming test items. The forme</context>
<context position="25309" citStr="Huang et al., 2005" startWordPosition="4303" endWordPosition="4306"> selected items to an item bank, edit the items, including their stems if necessary, and finalize the selection of the items for a particular examination. Using the assessment subsystem, students answer the test items via the Internet, and can receive grades immediately if the administrators choose to do so. The answers of students are recorded for student modelling and analysis of the item facility and the item discrimination. 7 Generating Listening Cloze Items We apply the same infrastructure for generating reading cloze items, shown in Figure 2, for the generation of listening cloze items (Huang et al., 2005). Due to the educational styles in Taiwan, students generally find it more difficult to comprehend messages by listening than by reading. Hence, we can regard listening cloze tests as an advanced format of reading cloze tests. Having constructed a database of sentences, we can extract sentences that contain the key for which the test administrator would like to have a listening cloze, and employ voice synthesizers to create the necessary recordings. Figure 5 shows an interface through which administrators choose and edit sentences for listening cloze items. Notice that we employ the concept th</context>
<context position="27571" citStr="Huang et al., 2005" startWordPosition="4688" endWordPosition="4691">allow the gap to cover more words in the original sentence. This would require the students to listen to a longer stream of sound, so can be a task more challenging than the original test. In addition to controlling the lengths of the answer voices, we can try to modulate the speed that the voices are replayed. Moreover, for multiple-word listening cloze, we may try to find word sequences that sound similar to the answer sequence to control the difficulty of the test item. Defining a metric for measuring similarity between two recordings is the key to support the aforementioned functions. In (Huang et al., 2005), we consider such features of phonemes as place and manner of pronunciation in calculating the similarity between sounds. Using this metric we choose as distractors those sounds of words that have similar pronunciation with the key of the listening cloze. We have to define the distance between each phoneme so that we could employ the minimal-edit-distance algorithm for computing the distance between the sounds of different words. 7 8 Concluding Remarks We believe that NLP techniques can play an important role in computer assisted language learning, and this belief is supported by papers in th</context>
</contexts>
<marker>Huang, Liu, Gao, 2005</marker>
<rawString>S.-M. Huang, C.-L. Liu, and Z.-M. Gao. 2005. Computerassisted item generation for listening cloze tests and dictation practice in English. In Proc. of the 4th Int. Conf. on Webbased Learning. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Irvine</author>
<author>P C Kyllonen</author>
<author>editors</author>
</authors>
<title>Item Generation for Test Development. Lawrence Erlbaum Associates,</title>
<date>2002</date>
<location>Mahwah, NJ.</location>
<marker>Irvine, Kyllonen, editors, 2002</marker>
<rawString>S. H. Irvine and P. C. Kyllonen, editors. 2002. Item Generation for Test Development. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proc. of the Workshop on the Evaluation of Parsing Systems in the 1st Int. Conf. on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="8677" citStr="Lin, 1998" startWordPosition="1366" endWordPosition="1367"> We segment sentences with the help of MXTERMINATOR (Reynar and Ratnaparkhi, 1997). We then tokenize words in the sentences before assigning useful tags to the tokens. We augment the text with an array of tags that facilitate cloze item generation. We assign tags of part-of-speech (POS) to the words with MXPOST that adopts the Penn Treebank tag set (Ratnaparkhi, 1996). Based on the assigned POS tags, we annotate words with their lemmas. For instance, we annotate classified with classify and classified, respectively, when the original word has VBN and JJ as its POS tag. We also employ MINIPAR (Lin, 1998) to obtain the partial parses of sentences that we use extensively in our system. Words with direct relationships can be identified easily in the partially parsed trees, and we rely heavily on these relationships between words for WSD. For easy reference, we will call words that have direct syntactic relationship with a word W as W’s signal words or simply signals. Since we focus on creating items for verbs, nouns, adjectives, and adverbs (Liu et al., 2005), we care about signals of words with these POS tags in sentences for disambiguating word senses. Specifically, the signals of a verb inclu</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based evaluation of MINIPAR. In Proc. of the Workshop on the Evaluation of Parsing Systems in the 1st Int. Conf. on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-L Liu</author>
<author>C-H Wang</author>
<author>Z-M Gao</author>
</authors>
<title>Using lexical constraints for enhancing computer-generated multiplechoice cloze items.</title>
<date>2005</date>
<journal>Int. J. of Computational Linguistics and Chinese Language Processing,</journal>
<note>10:to appear.</note>
<contexts>
<context position="1960" citStr="Liu et al., 2005" startWordPosition="292" endWordPosition="295">large-scale item banks, and has attracted active study in the past decade (Deane and Sheehan, 2003; Irvine and Kyllonen, 2002). Applying techniques for natural language processing (NLP), CAIG offers the possibility of creating a large number of items of different challenging levels, thereby paving a way to make computers more adaptive to students of different competence. Moreover, with the proliferation of Web contents, one may search and sift online text files for candidate sentences, and come up with a list of candidate cloze 1A portion of results reported in this paper will be expanded in (Liu et al., 2005; Huang et al., 2005). items economically. This unleashes the topics of the test items from being confined by item creators’ personal interests. NLP techniques serve to generate multiple-choice cloze items in different ways. (For brevity, we use cloze items or items for multiple-choice cloze items henceforth.) One may create sentences from scratch by applying template-based methods (Dennis et al., 2002) or more complex methods based on some predetermined principles (Deane and Sheehan, 2003). Others may take existing sentences from a corpus, and select those that meet the criteria for becoming </context>
<context position="6582" citStr="Liu et al., 2005" startWordPosition="1018" endWordPosition="1021">ning cloze items in Section 7 before making some concluding remarks. 2 System Architecture Figure 2 shows major steps for creating cloze items. Constrained by test administrator’s specifications and domain dependent requirements, the Sentence Retriever chooses a candidate sentence from the Figure 2: Main components of our item generator Tagged Corpus. Target-Dependent Item Requirements specify general principles that should be followed by all items for a particular test. For example, the number of words in cloze items for College Entrance Examinations in Taiwan (CEET) ranges between 6 and 28 (Liu et al., 2005), and one may want to follow this tradition in creating drill tests. Figure 3 shows the interface to the Item Specification. Through this interface, test administrators select the key for the desired cloze item, and specify part-of-speech and sense of the key that will be used in the item. Our system will attempt to create the requested number of items. After retrieving the target sentence, the Distractor Generator considers such constraining factors as word frequencies and collocations in selecting the distractors at the second step. Figure 3: Interface for specifying cloze items Figure 4 sho</context>
<context position="9138" citStr="Liu et al., 2005" startWordPosition="1443" endWordPosition="1446"> we annotate classified with classify and classified, respectively, when the original word has VBN and JJ as its POS tag. We also employ MINIPAR (Lin, 1998) to obtain the partial parses of sentences that we use extensively in our system. Words with direct relationships can be identified easily in the partially parsed trees, and we rely heavily on these relationships between words for WSD. For easy reference, we will call words that have direct syntactic relationship with a word W as W’s signal words or simply signals. Since we focus on creating items for verbs, nouns, adjectives, and adverbs (Liu et al., 2005), we care about signals of words with these POS tags in sentences for disambiguating word senses. Specifically, the signals of a verb include its subject, object, and the adverbs that modify the verb. The signals of a noun include the adjectives that modify the noun and the verb that uses the noun as its object or predicate. For instance, in “Jimmy builds a grand building.”, both “build” and “grand” are signals of “building”. The signals of adjectives and adverbs include the words that they modify and the words that modify the adjectives and adverbs. When we need lexical information about Engl</context>
<context position="15347" citStr="Liu et al., 2005" startWordPosition="2572" endWordPosition="2575">signals of the key in T, so we use (2) as a part of the score for w to take the sense θj in the target sentence T. Note that both the strength of association and St fall in the range of [0,1]. St(θj|w, T) Aµl,T (λj,k, πl,T) (2) In (2), we have assumed that the signal words are not polysemous. If they are polysemous, we assume that each of the candidate sense of the signal words are equally possible, and employ a slightly more complicated formula for (2). This assumption may introduce errors into our decisions, but relieves us from the needs to disambiguate the signal words in the first place (Liu et al., 2005). Since WordNet provides sample sentences for important words, we also use the degrees of similarity between the sample sentences and the target sentence to disambiguate the word senses of the key word in the target sentence. Let T and S be the target sentence of w and a sample sentence of sense θj of w, respectively. We compute this part of score, Ss, for θj using the following three-step procedure. If there are multiple sample sentences for a given sense, say θj of w, we will compute the score in (3) for each sample sentence of θj, and use the average score as the final score for θj. Procedu</context>
<context position="16570" citStr="Liu et al., 2005" startWordPosition="2810" endWordPosition="2813"> computing Ss(θj|w, T) 1. Compute signals of the key and their relationships with the key in the target and sample sentences. 1 mi E k=1 1 µ(T) µ(T) l=1 mj 4 IF(T, w) = {ψ1,T, ψ2,T, ··· , ψµ(T),T }, T(T, w) = {υ1,T, υ2,T, ··· , υµ(T),T}, &apos;F(S, w) = {ψ1,S, ψ2,S, ··· , ψµ(S),S}, and T(S, w) = {υ1,S, υ2,S, ··· , υµ(S),S} 2. We look for ψj,T and ψk,S such that υj,T = υk,S, and then check whether πj,T = πk,S. Namely, for each signal of the key in T, we check the signals of the key in S for matching syntactic relationships and word classes, and record the counts of matched relationship in M(θj, T) (Liu et al., 2005). 3. The following score measures the proportion of matched relationships among all relationships between the key and its signals in the target sentence. M(θj, T ) Ss(θj|w,T) = (3) µ(T ) The score for w to take sense θj in a target sentence T is the sum of St(θj|w, T) defined in (2) and Ss(θj|w, T) defined in (3), so the sense of w in T will be set to the sense defined in (4) when the score exceeds a selected threshold. When the sum of St(θj|w, T) and Ss(θj|w, T) is smaller than the threshold, we avoid making arbitrary decisions about the word senses. We discuss and illustrate effects of choos</context>
<context position="20579" citStr="Liu et al., 2005" startWordPosition="3502" endWordPosition="3505">|τaER log Pr(κ) Pr(τi) Pr(κ, τi) (5) The candidate whose score is better than 0.3 will be admitted as a distractor. Pr(κ) and Pr(τi) are the probabilities that each word class appears individually in the corpus, and Pr(κ, τi) is the probability that the two classes appear in the same sentence. Operational definitions of these probabilities 5 Table 1: Accuracy of WSD POS baseline threshold=0.4 threshold=0.7 verb 38.0%(19/50) 57.1%(16/28) 68.4%(13/19) noun 34.0%(17/50) 63.3%(19/30) 71.4%(15/21) adj. 26.7%(8/30) 55.6%(10/18) 60.0%(6/10) adv. 36.7%(11/30) 52.4%(11/21) 58.3%(7/12) are provided in (Liu et al., 2005). The term in the summation is a pointwise mutual information, and measures how often the classes r. and TZ collocate in the corpus. We negate the averaged sum so that classes that seldom collocate receive higher scores. We set the threshold to 0.3, based on statistics of (5) that are observed from the cloze items used in the 1992-2003 CEET. 6 Evaluations and Applications 6.1 Word Sense Disambiguation Different approaches to WSD were evaluated in different setups, and a very wide range of accuracies in [40%, 90%] were reported (Resnik, 1997; Wilks and Stevenson, 1997). Objective comparisons ne</context>
</contexts>
<marker>Liu, Wang, Gao, 2005</marker>
<rawString>C.-L. Liu, C.-H. Wang, and Z.-M. Gao. 2005. Using lexical constraints for enhancing computer-generated multiplechoice cloze items. Int. J. of Computational Linguistics and Chinese Language Processing, 10:to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-L Liu</author>
</authors>
<title>Using mutual information for adaptive item comparison and student assessment.</title>
<date>2005</date>
<journal>J. of Educational Technology &amp; Society,</journal>
<volume>8</volume>
<issue>4</issue>
<note>appear.</note>
<marker>Liu, 2005</marker>
<rawString>C.-L. Liu. 2005. Using mutual information for adaptive item comparison and student assessment. J. of Educational Technology &amp; Society, 8(4):to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Poel</author>
<author>S D Weatherly</author>
</authors>
<title>A cloze look at placement testing.</title>
<date>1997</date>
<journal>Shiken: JALT (Japanese Assoc. for Language Teaching) Testing &amp; Evaluation SIG Newsletter,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="4561" citStr="Poel and Weatherly, 1997" startWordPosition="701" endWordPosition="704">pus Target-Dependent Item Requirements Target Sentence Figure 1: A multiple-choice cloze item for English Cloze Item Item Specification Sentence Retriever with WSD Distractor Generator ing workload. In addition, composing a cloze item requires not just a useful sentence. Figure 1 shows a multiple-choice item, where we call the sentence with a gap the stem, the answer to the gap the key, and the other choices the distractors. Given a sentence, we still need distractors for a multiple-choice item. The selection of distractors affects the item facility and item discrimination of the cloze items (Poel and Weatherly, 1997). Therefore, the selection of distractors calls for deliberate strategies, and simple considerations alone, such as word frequencies, may not satisfy the demands. To remedy these shortcomings, we employ the techniques for word sense disambiguation (WSD) for choosing sentences in which the keys carries specific senses, and utilize the techniques for computing collocations (Manning and Sch¨utze, 1999) for selecting distractors. Results of empirical evaluation show that our methods could create items of satisfactory quality, and we have actually used the generated cloze items in freshmen-level En</context>
<context position="18524" citStr="Poel and Weatherly, 1997" startWordPosition="3138" endWordPosition="3141"> should be the same as the key in the target sentence. We may also take cultural background into consideration. Students in Taiwan tend to associate English vocabularies with their Chinese translations. Although this learning strategy works most of the time, students may find it difficult to differentiate English words that have very similar Chinese translations. Hence, a culturedependent strategy is to use English words that have similar Chinese translations with the key as the distractors. To generate distractors systematically, we employ ranks of word frequencies for selecting distractors (Poel and Weatherly, 1997). Assume that we are generating an item for a key whose part-of-speech is ρ, that there are n word types whose part-ofspeech may be ρ in the dictionary, and that the rank of frequency of the key among these n types is m. We randomly select words that rank in the range [m−n/10, m+n/10] among these n types as candidate distractors. These distractors are then screened by their fitness into the target sentence, where fitness is defined based on the concept of collocations of word classes, defined in HowNet, of the distractors and other words in the stem of the target sentence. Recall that we have </context>
</contexts>
<marker>Poel, Weatherly, 1997</marker>
<rawString>C. J. Poel and S. D. Weatherly. 1997. A cloze look at placement testing. Shiken: JALT (Japanese Assoc. for Language Teaching) Testing &amp; Evaluation SIG Newsletter, 1(1):4–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In Proc. of the Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="8437" citStr="Ratnaparkhi, 1996" startWordPosition="1326" endWordPosition="1327">s that are encoded in the HTML format. We need to extract texts from 2 Figure 4: An output after Figure 3 the mixture of titles, main body of the reports, and multimedia contents, and then segment the extracted paragraphs into individual sentences. We segment sentences with the help of MXTERMINATOR (Reynar and Ratnaparkhi, 1997). We then tokenize words in the sentences before assigning useful tags to the tokens. We augment the text with an array of tags that facilitate cloze item generation. We assign tags of part-of-speech (POS) to the words with MXPOST that adopts the Penn Treebank tag set (Ratnaparkhi, 1996). Based on the assigned POS tags, we annotate words with their lemmas. For instance, we annotate classified with classify and classified, respectively, when the original word has VBN and JJ as its POS tag. We also employ MINIPAR (Lin, 1998) to obtain the partial parses of sentences that we use extensively in our system. Words with direct relationships can be identified easily in the partially parsed trees, and we rely heavily on these relationships between words for WSD. For easy reference, we will call words that have direct syntactic relationship with a word W as W’s signal words or simply s</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tagger. In Proc. of the Conf. on Empirical Methods in Natural Language Processing, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proc. of the Applied NLP Workshop on Tagging Text with Lexical Semantics: Why, What and How,</booktitle>
<pages>52--57</pages>
<contexts>
<context position="11098" citStr="Resnik, 1997" startWordPosition="1759" endWordPosition="1760">2 extracts qualified sentences from the corpus. A sentence must contain the desired key of the requested POS to be considered as a candidate target sentence. Having identified such a candidate sentence, the item generator needs to determine whether the sense of the key also meets the requirement. We conduct this WSD task based on an extended notion of selectional preferences. 4.1 Extended Selectional Preferences Selectional preferences generally refer to the phenomenon that, under normal circumstances, some verbs constrain the meanings of other words in a sentence (Manning and Sch¨utze, 1999; Resnik, 1997). We can extend this notion to the relationships between a word of interest and its signals, with the help of HowNet. Let w be the word of interest, and 7r be the first listed class, in HowNet, of a signal word that has the syntactic relationship p with w. We define the strength of the association of w and 7r as follows: _ Pr µ w 7r Aµ(w,7r) Prµ(w) , (1) where Prµ(w) is the probability of w participating in the p relationship, and Prµ(w, 7r) is the probability that both w and 7r participate in the p relationship. 4.2 Word Sense Disambiguation We employ the generalized selectional preferences t</context>
<context position="21125" citStr="Resnik, 1997" startWordPosition="3596" endWordPosition="3597">11/30) 52.4%(11/21) 58.3%(7/12) are provided in (Liu et al., 2005). The term in the summation is a pointwise mutual information, and measures how often the classes r. and TZ collocate in the corpus. We negate the averaged sum so that classes that seldom collocate receive higher scores. We set the threshold to 0.3, based on statistics of (5) that are observed from the cloze items used in the 1992-2003 CEET. 6 Evaluations and Applications 6.1 Word Sense Disambiguation Different approaches to WSD were evaluated in different setups, and a very wide range of accuracies in [40%, 90%] were reported (Resnik, 1997; Wilks and Stevenson, 1997). Objective comparisons need to be carried out on a common test environment like SENSEVAL, so we choose to present only our results. We arbitrarily chose, respectively, 50, 50, 30, and 30 sentences that contained polysemous verbs, nouns, adjectives, and adverbs for disambiguation. Table 1 shows the percentage of correctly disambiguated words in these 160 samples. The baseline column shows the resulting accuracy when we directly use the most frequent sense, recorded in WordNet, for the polysemous words. The rightmost two columns show the resulting accuracy when we us</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>P. Resnik. 1997. Selectional preference and sense disambiguation. In Proc. of the Applied NLP Workshop on Tagging Text with Lexical Semantics: Why, What and How, pages 52–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proc. of the Conf. on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="8149" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="1275" endWordPosition="1278">b crawler, we retrieve the contents of Taiwan Review &lt;publish.gio.gov.tw&gt;, Taiwan Journal &lt;taiwanjournal.nat.gov.tw&gt;, and China Post &lt;www.chinapost.com.tw&gt;. Currently, we have 127,471 sentences that consist of 2,771,503 words in 36,005 types in the corpus. We look for useful sentences from web pages that are encoded in the HTML format. We need to extract texts from 2 Figure 4: An output after Figure 3 the mixture of titles, main body of the reports, and multimedia contents, and then segment the extracted paragraphs into individual sentences. We segment sentences with the help of MXTERMINATOR (Reynar and Ratnaparkhi, 1997). We then tokenize words in the sentences before assigning useful tags to the tokens. We augment the text with an array of tags that facilitate cloze item generation. We assign tags of part-of-speech (POS) to the words with MXPOST that adopts the Penn Treebank tag set (Ratnaparkhi, 1996). Based on the assigned POS tags, we annotate words with their lemmas. For instance, we annotate classified with classify and classified, respectively, when the original word has VBN and JJ as its POS tag. We also employ MINIPAR (Lin, 1998) to obtain the partial parses of sentences that we use extensively in ou</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proc. of the Conf. on Applied Natural Language Processing, pages 16– 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Sheehan</author>
<author>P Deane</author>
<author>I Kostin</author>
</authors>
<title>A partially automated system for generating passage-based multiple-choice verbal reasoning items.</title>
<date>2003</date>
<booktitle>Paper presented at the Nat’l Council on Measurement in Education Annual Meeting.</booktitle>
<contexts>
<context position="2735" citStr="Sheehan et al., 2003" startWordPosition="411" endWordPosition="415">iques serve to generate multiple-choice cloze items in different ways. (For brevity, we use cloze items or items for multiple-choice cloze items henceforth.) One may create sentences from scratch by applying template-based methods (Dennis et al., 2002) or more complex methods based on some predetermined principles (Deane and Sheehan, 2003). Others may take existing sentences from a corpus, and select those that meet the criteria for becoming test items. The former approach provides specific and potentially well-controlled test items at the costs of more complex systems than the latter, e.g., (Sheehan et al., 2003). Nevertheless, as the Web provides ample text files at our disposal, we may filter the text sources stringently for obtaining candidate test items of higher quality. Administrators can then select really usable items from these candidates at a relatively lower cost. Some researchers have already applied NLP techniques to the generation of sentences for multiplechoice cloze items. Stevens (1991) employs the concepts of concordance and collocation for generating items with general corpora. Coniam (1997) relies on factors such as word frequencies in a tagged corpus for creating test items of par</context>
</contexts>
<marker>Sheehan, Deane, Kostin, 2003</marker>
<rawString>K. M. Sheehan, P. Deane, and I. Kostin. 2003. A partially automated system for generating passage-based multiple-choice verbal reasoning items. Paper presented at the Nat’l Council on Measurement in Education Annual Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stevens</author>
</authors>
<title>Classroom concordancing: vocabulary materials derived from relevant authentic text. English for Specific Purposes,</title>
<date>1991</date>
<contexts>
<context position="3133" citStr="Stevens (1991)" startWordPosition="476" endWordPosition="477">hose that meet the criteria for becoming test items. The former approach provides specific and potentially well-controlled test items at the costs of more complex systems than the latter, e.g., (Sheehan et al., 2003). Nevertheless, as the Web provides ample text files at our disposal, we may filter the text sources stringently for obtaining candidate test items of higher quality. Administrators can then select really usable items from these candidates at a relatively lower cost. Some researchers have already applied NLP techniques to the generation of sentences for multiplechoice cloze items. Stevens (1991) employs the concepts of concordance and collocation for generating items with general corpora. Coniam (1997) relies on factors such as word frequencies in a tagged corpus for creating test items of particular types. There are other advanced NLP techniques that may help to create test items of higher quality. For instance, many words in English may carry multiple senses, and test administrators usually want to test a particular usage of the word in an item. In this case, blindly applying a keyword matching method, such as a concordancer, may lead us to a list of irrelevant sentences that would</context>
</contexts>
<marker>Stevens, 1991</marker>
<rawString>V. Stevens. 1991. Classroom concordancing: vocabulary materials derived from relevant authentic text. English for Specific Purposes, 10(1):35–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>M Stevenson</author>
</authors>
<title>Combining independent knowledge sources for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proc. of the Conf. on Recent Advances in Natural Language Processing,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="21153" citStr="Wilks and Stevenson, 1997" startWordPosition="3598" endWordPosition="3601">1/21) 58.3%(7/12) are provided in (Liu et al., 2005). The term in the summation is a pointwise mutual information, and measures how often the classes r. and TZ collocate in the corpus. We negate the averaged sum so that classes that seldom collocate receive higher scores. We set the threshold to 0.3, based on statistics of (5) that are observed from the cloze items used in the 1992-2003 CEET. 6 Evaluations and Applications 6.1 Word Sense Disambiguation Different approaches to WSD were evaluated in different setups, and a very wide range of accuracies in [40%, 90%] were reported (Resnik, 1997; Wilks and Stevenson, 1997). Objective comparisons need to be carried out on a common test environment like SENSEVAL, so we choose to present only our results. We arbitrarily chose, respectively, 50, 50, 30, and 30 sentences that contained polysemous verbs, nouns, adjectives, and adverbs for disambiguation. Table 1 shows the percentage of correctly disambiguated words in these 160 samples. The baseline column shows the resulting accuracy when we directly use the most frequent sense, recorded in WordNet, for the polysemous words. The rightmost two columns show the resulting accuracy when we used different thresholds for </context>
</contexts>
<marker>Wilks, Stevenson, 1997</marker>
<rawString>Y. Wilks and M. Stevenson. 1997. Combining independent knowledge sources for word sense disambiguation. In Proc. of the Conf. on Recent Advances in Natural Language Processing, pages 1–7.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>