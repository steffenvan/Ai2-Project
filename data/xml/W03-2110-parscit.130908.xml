<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.999378">
Spoken Dialogue for Virtual Advisers in a semi-immersive Command
and Control environment
</title>
<author confidence="0.995523">
Dominique Estival, Michael Broughton, Andrew Zschorn, Elizabeth Pronger
</author>
<affiliation confidence="0.9745435">
Human Systems Integration Group, Command and Control Division
Defence Science and Technology Organisation
</affiliation>
<address confidence="0.8268005">
PO Box 1500, Edinburgh SA 5111
AUSTRALIA
</address>
<email confidence="0.950631">
{Dominique.Estival, Michael.Broughton, Andrew Zschorn}@dsto.defence.gov.au
</email>
<sectionHeader confidence="0.998133" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999585153846154">
We present the spoken dialogue system
designed and implemented for Virtual
Advisers in the FOCAL environment. Its
architecture is based on: Dialogue Agents
using propositional attitudes, a Natural
Language Understanding component
using typed unification grammar, and a
commercial speaker-independent speech
recognition system. The current
application aims to facilitate the multi-
media presentation of military planning
information in a semi-immersive
environment.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999830111111111">
In this paper, we present the spoken dialogue
system implemented for communicating with the
virtual advisers (VAs) in the Future Operations
Centre Analysis Laboratory (FOCAL) at the
Australian Defence Science and Technology
Organisation (DSTO). We are experimenting with
the use of spoken dialogue with virtual
conversational characters to access multi-media
information during the conduct of military
operations and in particular to facilitate the
planning of such operations.
Unlike telephone-based dialogue systems
(Estival, 2002), which are mainly created for new
commercial applications, dialogue systems for
Command and Control applications (Moore et al.
1997) generally seek to simulate the military
domain and therefore require an understanding of
that domain.
</bodyText>
<sectionHeader confidence="0.891142" genericHeader="introduction">
2 Using Virtual Advisers in FOCAL
</sectionHeader>
<bodyText confidence="0.9999091875">
FOCAL was established to &amp;quot;pioneer a paradigm
shift in command environments through a superior
use of capability and greater situation awareness&amp;quot;.
The facility was designed to experiment with
innovative technologies to support this goal, and it
has now been running for two years.
FOCAL contains a large-screen, semi-
immersive virtual reality environment as its
primary display, allowing vast quantities of
information to be displayed. Our current VAs can
be described as 3-dimensional &amp;quot;Talking Heads&amp;quot;,
i.e. only the head and upper portions of the body
are represented. They can display expression, lip-
synchronisation and head movement, along with
certain autonomous behaviours such as blinking
and gaze (Taplin et al., 2001). These factors all
combine to add life-likeness to the VAs and create
more engaging interaction with users.
Presenting information via a Talking Head has
been commercially demonstrated by the virtual
newscaster “Ananova” (Ananova, 2002).
Embodied characters are also being developed and
include the PPP (Andre, Rist and Muller, 1998)
and Rea (Cassell, 2000). PPP is a cartoon style
Personalized Plan-based Presenter that combines
pointing, head movements and facial expressions
to draw the viewer’s attention to the information
being presented. Rea is a virtual real-estate agent
that takes an active role in conversation, she nods
her head to indicate understanding of spoken input,
or can raise her hand to indicate a desire to speak.
Several VAs have been implemented for
FOCAL, each having a particular role or
knowledge expertise. For example, one adviser
may have specialist knowledge relating to legal
issues, another may have information relating to
the geography of a region. Each VA has a
different facial appearance, voice and mannerisms.
To demonstrate and evaluate the performance
of VAs (and of the other FOCAL projects), a
fictitious scenario has been developed that
incorporates key elements of military planning at
the operational level (see section 8). The VAs
provide information rich briefs through the
combined use of spoken output via Text-to-Speech
(TTS) and multimedia. Relevant questions can be
asked at the end of the briefs through the use of
spoken dialogue.
</bodyText>
<sectionHeader confidence="0.99695" genericHeader="method">
3 Previous implementation: Franco
</sectionHeader>
<bodyText confidence="0.999951541666667">
As described in (Taplin et al., 2001) the first VA in
FOCAL, named Franco, was also an animated 3-
dimensional &amp;quot;Talking Head&amp;quot; model, intended to
either deliver prepared information, such as a
briefing or slide show, or to interact
conversationally with users. To demonstrate the
conversational functionality (Broughton et al.,
2002), it was implemented with a commercial
speaker-dependent automated speech recogniser
(ASR), Dragon NaturallySpeakingTM. The Natural
Language understanding component was
implemented in NatLink (Gould, 2001) and a
simple user-driven dialogue management, based on
key-word recognition and nesting of dialogue
states to provide context, was also implemented in
Python.
Franco has been successful in demonstrating
the proof-of-concept of a VA in the FOCAL
environment. Answering spoken questions about
specific military assets and platforms, it also
permits the display of other types of information
such as pictures, animated video clips, tabular
information from a database, and location details
on digital maps.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="method">
4 Improvements
</sectionHeader>
<bodyText confidence="0.999949666666667">
Although Franco was successful in demonstrating
the potential usefulness of a VA in a Command
and Control environment for operational planning,
it suffers from certain limitations which we are
now addressing in a follow-up project.
The first limitation, and the easiest to remedy,
was the unnaturalness of the synthetic voice we
had given Franco. For greater effectiveness, we
had to provide our VA with a more natural voice
and with an Australian accent. We chose the new
Australian TTS voice from Rhetorical, developed
by Appen (rVoice, 2002). This required making
some changes, some of them relatively important,
to the interface with the talking head model to
achieve lip-synchronisation, but that aspect of the
work will not be addressed in this paper.
The second limitation was the relative rigidity
of the dialogue management strategy we were
using. The alternative approach we have
developed is to create Dialogue Agents
implemented in ATTITUDE. This is described in
section 6.
The third limitation was due to the speaker-
dependent nature of the ASR. While a speaker-
dependent ASR allows greater flexibility in the
input to which the VA can respond, we wanted to
develop a system which could not only be
demonstrated by the few people who have trained
the speech recogniser, but where visitors
themselves could be participants and could interact
with the VA. Switching to a speaker-independent
ASR led us to radically modify our Spoken
Language Understanding component, and this is
described in section 7.
The new implementation we describe here has
allowed us not only to address those three
limitations, but also to alter fundamentally the
architecture of the system, opening up the dialogue
management components to control and interaction
by other tools and agents in the FOCAL
environment. The resulting system is now fully
modular and provides scalability as well as
flexibility.
This new implementation allows us to focus our
research into dialogue management issue, to
investigate the use of ATTITUDE for dialogue
management and to experiment with more natural
language input.
</bodyText>
<sectionHeader confidence="0.999292" genericHeader="method">
5 Integration
</sectionHeader>
<bodyText confidence="0.9995348">
Communication between the various components
of the system (speech recogniser, dialogue control,
virtual adviser control and multimedia display) is
now achieved with the CoABS (Control of Agent
Based Systems) Grid infrastructure (Global
InfoTek, 2002). The CoABS Grid was designed to
allow a large number of heterogeneous procedural,
object-oriented and agent-based systems to
communicate. Using the CoABS Grid as our
infrastructure has allowed us to integrate all the
components of the dialogue system and it will
provide an easy way to integrate other agents and a
variety of input and output devices.
Communication between CoABS agents is
accomplished via string messages.
</bodyText>
<sectionHeader confidence="0.996876" genericHeader="method">
6 Dialogue Management with ATTITUDE
</sectionHeader>
<bodyText confidence="0.999680520833333">
ATTITUDE is a multi-agent architecture developed
at DSTO, capable of representing and reasoning
both with uncertainty and about multiple
alternative scenarios (Lambert, 1999). It is a
multi-agent extension of the MetaCon reactive
planner developed for control of phased array
radars on the Swedish Airborne Early Warning
aircraft (Lambert and Relbe, 1998). ATTITUDE has
some similarities with Prolog and other logic
programming languages as well as with AI
research on blackboard and multi-agent
architectures. Because ATTITUDE was designed
specifically to support the programming of reactive
systems, it possesses powerful facilities for
handling interactions of the internal system
entities, both with each other and with the external
world.
ATTITUDE is very high-level, weakly-typed, and
thanks to the agent paradigm, it produces loosely
coupled and modularised systems. For these
reasons, and because ATTITUDE implements
reasoning about propositional attitudes, it provides
a very attractive framework in which to develop
and express dialogue management control
strategies. It is worth emphasizing here that
ATTITUDE is not merely a notation to represent
speech acts or communicative acts between
agents, but that it is actually the programming
language and environment in which both the
agents themselves and the control structure for
interaction between the agents are implemented
and executed.
Because ATTITUDE has never been used for this
purpose before, this is an interesting area of
research in itself, and one of the goals of the
project has been to see how ATTITUDE needs to be
extended to implement dialogue management.
Further, this allows us to investigate how far
attitude programming (see section 6.2) can go
towards expressing speech acts and communicative
act type. However, we do not claim to employ the
full power of propositional attitudes in our
implementation yet. This is another area of
research which we are now exploring. Neither are
we yet at the stage where we could perform
automatic detection of utterance type (Wright,
1998) or of dialogue act (Carberry and Lambert,
1999; Prasad and Walker, 2002).
</bodyText>
<subsectionHeader confidence="0.99926">
6.1 Propositional attitudes
</subsectionHeader>
<bodyText confidence="0.998658">
The ATTITUDE programming environment is so
named because it utilises propositional attitude
instructions as programming instructions (this has
been dubbed attitude programming). Propositional
attitudes are alleged mental states characterised by
propositional attitude expressions, which are the
means by which individuals relate their own
mental behaviour to others&apos;.
Propositional attitude instructions are of the form
shown in (1).
</bodyText>
<equation confidence="0.563849">
(1) [subject][attitude][propositional expression]
In (1):
</equation>
<bodyText confidence="0.989667333333333">
- [subject] denotes the individual whose mental
state is being characterised;
- [propositional expression] describes some
propositional claim about the world; and
- [attitude] expresses the subject&apos;s dispositional
attitude toward that claim about the world.
</bodyText>
<subsectionHeader confidence="0.999431">
6.2 ATTITUDE programming
</subsectionHeader>
<bodyText confidence="0.99992737037037">
When software agent Mary encounters the
propositional attitude instruction &amp;quot;Fred desire [the
door is closed]&amp;quot;, Mary will issue a message to
software agent Fred instructing Fred to desire that
the door be closed. Similarly, when encountering
the propositional attitude instruction &amp;quot;I believe [the
sky is blue]&amp;quot;, Mary herself will attempt to believe
that the sky is blue.
An important characteristic of ATTITUDE
programming is that each propositional attitude
instruction either succeeds or fails, possibly with
side effects, depending upon whether the recipient
agent is able to satisfy the instructional request. As
each propositional attitude instruction either
succeeds or fails, the execution path selected
through a network of propositional attitude
instructions (routine) is determined by the
successes and failures of the propositional attitude
instructions attempted along the way. The control
structure is therefore governed by a semantics of
success.
Computational routines for a software agent
arise by linking together particular choices of
propositional attitude instructions. These networks
of propositional attitude instructions then prescribe
recipes defining the possible mental behaviour of a
software agent.
</bodyText>
<subsectionHeader confidence="0.996179">
6.3 The ATTITUDE Dialogue Agents
</subsectionHeader>
<bodyText confidence="0.99997335">
We have implemented a number of ATTITUDE
Dialogue Agents. The main agent in our Dialogue
Management architecture (shown in Figure 1) is
the Conductor. It is the agent responsible for the
flow of information between the other agents and it
manages multi-modal interactions. The other
agents, also described further in this section, are
the Speaker, the NLG (Natural Language
Generator), the MMP (Multimedia Presenter) and
several IS (Information Source) agents. In addition
to these agents, each dialogue state (see section 8)
is also implemented as an ATTITUDE agent, with its
own set of routines.
As explained in section 6.2, each ATTITUDE
agent’s behaviour is programmed as a set of
routines
The interaction between the ATTITUDE
Dialogue agents is shown in Figure 1, in which the
frame around the ATTITUDE agents can be
interpreted as representing the CoABS grid.
</bodyText>
<subsectionHeader confidence="0.963789">
Speaker Agent
</subsectionHeader>
<bodyText confidence="0.999969235294118">
When speech from the user has been detected and
recognised, the attribute-value pairs for that
utterance (see section 7) are sent to Speaker.
Speaker takes that information and produces a
corresponding ATTITUDE expression, which is then
forwarded to Conductor.
The linguistic coverage of the system is
determined by the grammars which are available at
each dialogue state. For now, the coverage is
limited to a set of utterances appropriate for the
briefing scenario described in section 8. These
were used to define the Regulus1 grammars from
which the Nuance grammars are compiled. We are
now planning to move from Regulus1 to Regulus2,
which will allow us to derive dialogue state
grammars from a large English grammar using the
EBL strategy described in (Rayner et al., 2002b)
</bodyText>
<figureCaption confidence="0.984407">
Figure 1. Dialogue with ATTITUDE
</figureCaption>
<bodyText confidence="0.996628769230769">
English question from user
This agent is responsible for dialogue flow control and all othe r dialogue
agents must register with it.
Conductor receives communicative acts from Speaker . For example:
( whquestion (property mig - 29 flying - range ?value ?units))
This query is forwarded on to all registered agents. Conductor chooses the
most appropriate response received and sends this to MMP to present the
answer.
Multimedia Presenter
This agent receives a list of expressions from Conductor and directs the appropriate services to present
multimedia data to the user. For example:
(MMP)(( whanswer (property mig - 29 flying - range 810 nautical - miles)) (image mig - 29))
In this case, MMP requests an English form of the whanswer expression and sends the result to the TTS
</bodyText>
<figure confidence="0.9540631">
application. Similarly, an appropriate application is directed t o display the requested image.
Nuance/Regulus
Conductor
Attribute/Value pairs
Presentation directives
Speaker
Speaker receives speech recognition results in the form
of attribute - value pairs, and translates these into
Attitude expressions to send to Conductor .
Attitude expression
Multimedia displayed
NLG directive
Query
Response
Natural Language Generator
Receives expressions from MMP and uses templates to return
corresponding English sentences.
(NLG)
English string
This category of agents each register with Conductor and
interface with a background data source, for example, a
(IS)
database of aircraft properties.
Each uses their data source to respond to queries from
Conductor .
Information Source
English string
Virtual Advisor speaking
TTS
Conductor Agent
</figure>
<bodyText confidence="0.929254575757576">
Conductor takes an ATTITUDE expression from
Speaker and forwards it on to all the IS agents that
have registered with it. It then waits for all the
responses to come back from those agents, in the
form of lists of expressions.
Every response Conductor receives is put into
its knowledge base, along with some extra
information:
- Sender: which IS agent sent the response.
- In-Reply-To: which previous communicative
act this is a response to.
- Strength: whether every expression of the
response is &apos;strong&apos; (the sender believes it is
either absolute truth or absolute negation) or if
one or more is &apos;weak&apos; (the sender believes it is
neither absolute truth nor absolute negation).
- Bound-State: if there are any free variables in
the response, or if it is fully ground.
- Unifiability: whether one or more of the
expressions in the response is of the same form
as Speaker’s initial expression.
The final expression in Conductor’s knowledge
base is as shown in (2).
(2) (response ?in-reply-to ?sender ?strength
?bound_state ?unifiability ?content)
Given the initial expression from Speaker and the
replies it receives from the IS agents, Conductor
chooses the &apos;best&apos; response. For example, a
response that is strong, fully ground and unifies
with Speaker’s expression is deemed to be more
relevant and informative than a response that is
weak and contains free variables. Conductor
forwards this response to MMP.
</bodyText>
<subsectionHeader confidence="0.472153">
Multimedia Presenter (MMP)
</subsectionHeader>
<bodyText confidence="0.999726882352941">
MMP iterates through the list of expressions sent
by Conductor and presents each expression to the
user. MMP recognises classes of expressions and
chooses to present them using certain media. For
example, some expressions are instructions to
change the VA head model, while others are to be
translated into English sentences and spoken by the
VA. For the latter function MMP uses NLG (see
below).
Other media through which MMP can choose to
present the information contained in the
expressions include: imagery from a database (e.g.
pictures of military platforms, or of strategic
locations), video clips, images from weather or
radar information sources, virtual video, 3-
dimensional virtual battle space maps, textual
information and audio.
</bodyText>
<subsectionHeader confidence="0.563925">
Natural Language Generator (NLG)
</subsectionHeader>
<bodyText confidence="0.8569475">
For now, NLG uses templates to transform
ATTITUDE expressions into English. For example,
the instruction in (3) provides two possible
responses for the ATTITUDE expression specified:1
</bodyText>
<equation confidence="0.43929">
(3) (property ?asset overview ?value text)
whanswer priority 10
( ( response 1 (&amp;quot;The &amp;quot;?asset&amp;quot; is a &amp;quot;?value&amp;quot;.&amp;quot;) )
( ( response 2 (&amp;quot;I understand that the &amp;quot;?asset&amp;quot; is a
&amp;quot;?value&amp;quot;.&amp;quot;) )))
</equation>
<bodyText confidence="0.977714333333333">
When NLG is first requested to generate the
English output for the expression in (4.a), intended
to be a communicative act of type whanswer, it
uses the template given in (4.b), corresponding to
&amp;quot;response 1&amp;quot; in (3), to produce the English answer
given in (4.c).
</bodyText>
<figure confidence="0.69409925">
(4.a) (property mig-29 overview &amp;quot;Russian multi-role
fighter&amp;quot; text)
b. (&amp;quot;The &amp;quot;?asset&amp;quot; is a &amp;quot;?value&amp;quot;.&amp;quot;)
c. The Mig-29 is a Russian multi-role fighter.
</figure>
<bodyText confidence="0.98677625">
When NLG is requested a second time to generate
the output for (3), it uses the template in (5.a),
corresponding to &amp;quot;response 2&amp;quot; in (3), to produce
the English answer given in (5.b).
</bodyText>
<equation confidence="0.486968">
(5.a) (&amp;quot;I understand that the &amp;quot;?asset&amp;quot; is a &amp;quot;?value&amp;quot;.&amp;quot;)
b. I understand that the Mig-29 is a Russian multi-
role fighter.
</equation>
<bodyText confidence="0.9998975">
Thus NLG cycles through the list of templates for
appropriate responses. Priorities can also be given
to templates, enabling NLG to use general
templates together with more specific and tailored
ones.
It is clear that template-based language
generation is too rigid for fully natural dialogues,
and we intend to explore more flexible techniques
after we implement a wider coverage English
grammar; however, it has so far been sufficient for
</bodyText>
<footnote confidence="0.748889">
1Variables are denoted with &amp;quot;?&amp;quot;, while text strings (to be sent
to speech synthesis, or displayed on a slide) are between
double quotes, &amp;quot;&amp;quot;.
</footnote>
<bodyText confidence="0.970264263157895">
our purposes, namely to demonstrate and
investigate agent-based dialogue management.
Information Source Agent (IS)
The IS agents, e.g. a Weather Agent or a Platform
Capabilities Agent, can answer users&apos; questions,
either by using their own internal knowledge base
or by accessing external Information Sources, such
as a weather information server, or a database of
military assets. All IS agents register with
Conductor, and when an expression is sent by
Speaker, all IS agents try to respond to it.
By using the CoABS Grid as the infrastructure and
implementing the agent with ATTITUDE, we leave
the architecture extremely flexible and scalable
(Kahn and Della Torre Cicalese, 2001). For
instance, it is possible to increase the amount of
information at the system’s disposal during run-
time by launching a new IS agent and by adding
some templates to NLG.
</bodyText>
<subsectionHeader confidence="0.988501">
6.4 Dialogue design
</subsectionHeader>
<bodyText confidence="0.999975">
For now, the dialogue is specified as a finite state
machine and is still very much system directed. In
the briefing application (see section 8.1), the VAs
first &amp;quot;push&amp;quot; the information that needs to be
presented, as briefing officers do in a normal
briefing. Some of the information is also presented
using visual aids, such as power point slides and
maps for specifying location information. The
information to be presented and the media to be
used are determined by the agent for that particular
dialogue state.
The VA then allows users to ask questions to
repeat or clarify particular points, or to gain
additional information.
</bodyText>
<sectionHeader confidence="0.994858" genericHeader="method">
7 Spoken Language Processing
</sectionHeader>
<subsectionHeader confidence="0.999781">
7.1 Speaker-independent speech recognition
</subsectionHeader>
<bodyText confidence="0.999861666666667">
As stated in section 4, one of the main motivations
for moving from a speaker-dependent to a speaker-
independent ASR was to allow visitors in FOCAL
the possibility of using the system themselves,
rather than relying on a small set of trained
individuals to run demonstrations. We chose to
use the Nuance Toolkit (Nuance, 2002) for several
reasons: besides its reliability as a speaker-
independent ASR for both telephone and
microphone speech, Nuance 8.0 provides
Australian-New Zealand English, as well as US
and UK English, acoustic language models. Even
more importantly for our purposes, Nuance
grammars can be compiled from Regulus, a higher-
level language processing component which has
already been used to develop several spoken
dialogue systems in different domains (Rayner et
al., 2001, Rayner and Bouillon, 2002).
</bodyText>
<subsectionHeader confidence="0.998027">
7.2 Spoken Language Understanding
</subsectionHeader>
<bodyText confidence="0.999975653846154">
Following our decision to move from a speaker-
dependent to a speaker-independent ASR, we
decided to use Regulus to implement our Natural
Language Understanding component. Regulus is
an Open Source environment which compiles
typed unification grammars into context-free
grammar language models compatible with the
Nuance Toolkit. It is &amp;quot;written in a Prolog-based
feature-value notation and compiles into Nuance
GSL grammars.&amp;quot; (Rayner et al., 2002a). Regulus
is also described in detail in (Rayner et al., 2001).
The main motivation for using Regulus is the
usual one of greater efficiency due to the more
compact nature of a unification grammar
representation compared with a context-free
grammar. In addition, using Regulus to define a
higher level grammar, we are able to obtain as our
semantic representation a list of attribute-value
pairs, and this permits a more sophisticated
processing of the information by the other agents.
Regulus also allows the development of bi-
directional grammars, and we intend to make use
of this functionality in later implementations of the
NLG agent. However, for now, the grammars we
have developed have been limited to recognition
and understanding.
</bodyText>
<sectionHeader confidence="0.905962" genericHeader="method">
8 Current application implementation
</sectionHeader>
<subsectionHeader confidence="0.984914">
8.1 Dialogue scenario
</subsectionHeader>
<bodyText confidence="0.999940083333334">
The scenario for the current application was
developed by members of the Human Systems
Integration (HSI) group and is grounded on their
experience with, and observations of, military
operational planning. It is based on a fictitious
scenario developed for training (the examples
given here have all been modified) and exemplifies
the Joint Military Appreciation Process (JMAP) for
military planning across the three services (Army,
Navy and Air Force). A sub-scenario was chosen
for the development of the spoken dialogue with
the VAs.2
</bodyText>
<subsectionHeader confidence="0.987248">
8.2 Dialogue flow
</subsectionHeader>
<bodyText confidence="0.999761333333333">
The structured nature of a military planning task
such as this one makes it very easy to partition it
into different stages, which can then be mapped to
different dialogue states. In our dialogue script,
each top-level dialogue state corresponds to a
section of the planning exercise, given in (6).
</bodyText>
<listItem confidence="0.996371714285714">
(6) Commander&apos;s Initial Guidance
- CDF (Chief of Defence Forces) Intent
- Planning Guidance
- Constraints
- Restrictions
- Legal Issues
- Command and Control
</listItem>
<bodyText confidence="0.99991276">
These 6 top level dialogue states are then followed
by an Overall Question Time.
The mixed-initiative nature of the system can
be modelled in a finite state diagram, allowing for
a) briefing-like system ‘pushes’, b) confirmation
queries from the system and c) questions from the
user. However, because the system is primarily
agent-based, the dialogue can also evolve
dynamically. For instance, once the system is in a
‘question’ state, the dialogue flow then allows
users to ask a number of questions, until they are
satisfied, and the dialogue can move to a different
state.
Each of the top level dialogue states also
corresponds to an IS agent with its own set of
ATTITUDE routines. These agents register with
Conductor and act as experts in their particular
fields (e.g., the Legal Issues adviser). The agents
contain knowledge which they use to answer
questions posed to them by Conductor. All agents
have the ability to keep track of which state (or
topic) they are in. This allows not only Conductor,
but also the other dialogue agents, to distinguish
between providing the user with new information
or information that has already been presented.
</bodyText>
<footnote confidence="0.956228333333333">
2 This is the Commander’s initial guidance to the Theatre
Planning Group (TPG), which is part of the Mission Analysis
section of JMAP.
</footnote>
<subsectionHeader confidence="0.988534">
8.3 Knowledge Representation
</subsectionHeader>
<bodyText confidence="0.938886866666667">
The current ontology developed for this application
is only a small part of the larger Knowledge
Representation ontology to be used throughout the
whole FOCAL system. For now, we only
represent the concepts needed in our small domain,
and their relationships are translated into
ATTITUDE statements, allowing agents to draw
inferences. For example, if a user can ask the
question given in (7.a), it will be translated into
the list of attribute value pairs given in (7.b) and
sent to Speaker. Speaker then translates these
attribute value pairs into the ATTITUDE expression
in (7.c) and forwards it on to Conductor.
(7.a) What department oversees negotiations
with unions and industry?
</bodyText>
<listItem confidence="0.9046414">
b. [question whatquestion, concept
negotiation, attribute oversee, obj1 department]
c. conductor desire (comm_act (negotiation
oversee ?department) from speaker type
whatquestion in-response-to null)
</listItem>
<bodyText confidence="0.979297409090909">
As described in section 6, when Conductor poses
the question to the appropriate agents, they respond
with the information in their knowledge base or
information they can extract from a database.
Agents store knowledge as believe statements such
as the one shown in (8):
(8) I believe (negotiation oversee “department
of workplace relations”)
These believe statements are then unified with the
propositions translated by Speaker, and if
unification is successful, a reply is sent back to
Conductor. Finally, Conductor passes the answer
on to NLG to match a template and produce an
English answer, for instance (9).
(9) The Department of Workplace Relations
oversees negotiations with unions and industry.
An agent which has access to a database can also
translate a user&apos;s question into the relevant
database query to obtain the answer. An important
issue under research concerns the automatic
derivation of ATTITUDE statements from a pre-
existing database.
</bodyText>
<subsectionHeader confidence="0.984678">
8.4 Several different VAs
</subsectionHeader>
<bodyText confidence="0.999978769230769">
As explained above, each stage of the planning
process is presented to the user by a particular VA
with its associated IS agent and the VA then allows
users to ask further questions. Besides their
specialised knowledge, the VAs are differentiated
through different head models, different TTS
voices (male or female, different regional accents)
and different personalities.
Once a dialogue state is completed and the user
has no further questions, the VA for that state
sends a message to Conductor to move to the next
state. Conductor can then initiate the change in
recognition grammar, voice for the next VA and
model for the next VA head.
Having several VAs coming on at different
stages to present different information allows for a
VA to be specialised in a particular domain, just
as real briefing officers are during a real military
planning exercise.
For now, we only display one VA at a time, but
we intend to experiment with having multiple VAs
at the same time. The final state of the dialogue
flow allows users to ask questions about any aspect
of the planning process, and questions can be
posed to all the VAs, so it would be natural for the
users to see all the VAs at that stage.
</bodyText>
<subsectionHeader confidence="0.998414">
8.5 Rapid Prototyping and Evaluation
</subsectionHeader>
<bodyText confidence="0.99998328">
The key word version developed previously (see
Broughton et al., 2002) has been maintained as a
rapid prototyping environment for evaluating new
scripts and dialogues. It allows new dialogues to
be quickly tested by entering suitable key words,
sufficient to discriminate one question from
another. This system proves faster for testing than
the more precise method of grammar building.
Multiple response strings can be generated,
providing more naturalness for those interacting
with the VAs on a regular basis. By rapidly
prototyping questions and responses, we can test
the intuitiveness of expected questions and the
smoothness and timeliness of responses,
particularly when presented combined with
multimedia.
The implemented system described here has
so far only been tested with other members of the
group, but demonstrations to visitors and potential
users will provide a more rigorous form of
evaluation on an on-going basis. An evaluation
phase for the project is scheduled for 2003-2004,
during which time we will have access to more
users and will be able to conduct more structured
experiments.
</bodyText>
<sectionHeader confidence="0.853666" genericHeader="method">
9 Natural Interaction with VAs
</sectionHeader>
<bodyText confidence="0.999948537037037">
In addition to the ASR and TTS systems
previously discussed, other technologies can be
combined into the overall system to increase
naturalness of interaction, and we are investigating
speaker recognition as well as a range of pointing
technologies.
The need for a speaker recognition system has
emerged with the move to a speaker independent
ASR. With a speaker dependent ASR, users would
load their individual profile before use, thus
enabling the system to know who was using it.
With a speaker-independent ASR, a speaker
recognition system would allow the VAs to
recognise who is talking to them and enable them
to address known users by name. We plan to
integrate within FOCAL the speaker recognition
system which has been developed at DSTO
(Roberts, 1998). This system uses statistical
modelling techniques and is capable of both
speaker identification (recognising users from a
database of stored speech profiles) and speaker
verification (verifying the identity of a particular
user).
We are also proposing to use pointing
techniques in combination with the speech and
language technologies to build a multimodal
system. Multimodal systems were originally
demonstrated by Bolts (1980) and research is
continuing across varied applications (e.g., Oviatt
et al., 2000 and Gibbon et al., 2000). However,
unlike systems such as MATCH (Johnston et al.,
2002), where the issue is allowing multimodal
interaction on portable devices with very small
screens, in FOCAL we are concerned with
ensuring that users get the full benefit of the very
large screen and with allowing several users to
interact at a distance from the screen. It is also
worth mentioning that, unlike the interactive
system described in (Rickel et al., 2002), which is
concerned with training in a military environment,
we are not trying to simulate a complete virtual
world with embodied agents.
However, we propose to include traditional
pointing technologies, such as the standard desktop
mouse, through to 3-dimensional tracking systems
for gaze, gesture and user tracking. This will
involve integrating more complex language
understanding, as information will need to be
derived from both the user&apos;s utterance and from
what is being pointed to. For example, to interpret
an utterance such as (10) uttered while the user
points to a location on a map, we need to perform
reference resolution on &amp;quot;this region&amp;quot;, and match
that referent to the item being pointed at.
</bodyText>
<listItem confidence="0.505249">
(10) What do we know about this region?
</listItem>
<sectionHeader confidence="0.993435" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999982725490196">
We have now implemented in FOCAL the
infrastructure needed to perform spoken and
multimodal dialogue with several VAs. This is of
interest in itself, as it will allow us to continue our
research on spoken language understanding and
spoken dialogue systems and also to address issues
of language generation which have for now been
left aside. Already we have been able to move
from a rigid dialogue control structure, with very
constrained input, to a more flexible and scalable
control structure allowing real connectivity
between agents.
Having moved to a speaker-independent ASR,
and taking advantage of the open source nature of
Regulus, we intend to pursue research issues
regarding robust processing of spoken input, such
as using grammar specialisation from a corpus and
devising techniques for ignoring parts of the input.
We have implemented a dialogue management
architecture based on ATTITUDE agents which
communicate with each other using propositional
attitude expressions. Other agents can now be
developed to perform additional functions, in
particular to launch the display of other types of
information and to interpret other types of input.
This will allow us to explore how spoken
dialogue with VAs can be combined with other
virtual interaction technologies (e.g., gesture,
pointing, gaze tracking). In this respect, the next
step in our project is the development of a full
fledge MMP agent based on the framework
described in (Colineau and Paris, 2003).
However, the work we have reported here must
also be seen as part of the larger research
programme undertaken within FOCAL. From this
perspective, this work is of interest because it
allows other members of the HSI group to pursue
research in the usability of new technologies to
perform the paradigm shift in command
environments. In particular, this project is
providing the support for further research into
whether this way of presenting information is
helpful in an operational command environment.
It allows us to devise experiments to explore the
crucial issue of trust in the information being
presented, and how the way the information being
presented can affect that trust.
Integrating spoken dialogue with planning tools
will also allow us to explore whether VAs can help
in military operation planning, and how best to use
these tools.
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999993454545454">
We wish to thank the Chief of C2D, and the
Director of Information Sciences Laboratory, for
sponsoring and funding this work. We wish to
acknowledge the work of Paul Taplin in
integrating speech synthesis and lip-
synchronisation, and the work of Benjamin Fry
from the University of South Australia in
developing the Regulus/Nuance grammars.
Finally we wish to thank the other members of the
HSI group in C2D for their constant and invaluable
help with the FOCAL project.
</bodyText>
<sectionHeader confidence="0.998585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892763157895">
Ananova. 2002. http://www.ananova.com.
E. Andre, T. Rist, and J. Muller. 1998. Integrating
Reactive and Scripted Behaviours in a Life-Like
Presentation Agent, Proceedings of the Second
International Conference on Autonomous Agents,
261-268.
Appen. 2002. http://www.appen.com.au.
R. A. Bolt. 1980. &amp;quot;Put-that-there&amp;quot;: voice and gesture at
the graphics interface. Proceedings of the
SIGGRAPH, July, 262-270.
Michael Broughton, Oliver Carr, Dominique Estival,
Paul Taplin, Steven Wark, Dale Lambert. 2002.
&amp;quot;Conversing with Franco, FOCAL’s Virtual
Adviser&amp;quot;. Conversation Characters Workshop,
Human Factors 2002, Melbourne, Australia.
Sandra Carberry and Lynn Lambert. 1999. &amp;quot;A Process
Model for Recognizing Communicative Acts and
Modeling Negotiation Subdialogues&amp;quot;. Computational
Linguistics. 25,1, pp. 1-53
Justine Cassell. 2000. Embodied Conversational
Interface Agents, Communications of the ACM, Vol.
43, No. 4, 70-78.
Nathalie Colineau and Cécile Paris. 2003. Framework
for the Design of Intelligent Multimedia Presentation
Systems: An architecture proposal for FOCAL.
CMIS Technical Report 03/92, CSIRO, May 2003.
Dominique Estival. 2002. &amp;quot;The Syrinx Spoken
Language System&amp;quot;. International Journal of Speech
Technology. vol.5. no.1. pp.85-96.
Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn
Walker, Steve Whittaker, Preetam Maloor. 2002.
&amp;quot;MATCH: an Architecture for Multimodal Dialogue
Systems&amp;quot;. Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL&apos;02). pp. 376-383. Philadelphia..
Dafydd Gibbon, Inge Mertins, Roger K. Moore (Eds.).
2000. Handbook of Multimodal and Spoken
Dialogue Systems: Resources, Terminology and
Product Evaluation. Kluwer Academic Publishers.
Global InfoTek Inc. 2002. Control of Agent Based
Systems. http://coabs.globalinfotek.com.
Joel Gould. 2001. &amp;quot;Implementation and Acceptance of
NatLink, a Python-Based Macro System for Dragon
NaturallySpeaking&amp;quot;, The Ninth International Python
Conference, March 5-8, California
Martha L. Kahn and Cynthia Della Torre Cicalese.
2001. &amp;quot;CoABS Grid Scalability Experiments&amp;quot;.
Proceedings of the Second International Workshop
on Infrastructure for Agents, MAS, and Scalable
MAS, Autonomous Agents 2001 Conference.
Dale A. Lambert and Mikael G. Relbe. 1998.
&amp;quot;Reasoning with Tolerance&amp;quot;. 2nd International
Conference on Knowledge-Based Intelligent
Electronic Systems. IEEE. pp. 418-427.
Dale A. Lambert. 1999. &amp;quot;Advisers With ATTITUDE for
Situation Awareness&amp;quot;. Proceedings of the 1999
Workshop on Defence Applications of Signal
Processing. pp.113-118, Edited A. Lindsey, B.
Moran, J. Schroeder, M. Smith and L. White.
LaSalle, Illinois.
Dale A. Lambert. 2003. &amp;quot;Automating Cognitive
Routines&amp;quot;, accepted for publication in the 6th
International Conference on Information Fusion.
R. Moore, J. Dowding, H. Bratt, J. Gawron, Y. Gorfu,
A. Cheyer. 1997. &amp;quot;CommandTalk: A spoken-
language interface for battlefield simulations&amp;quot;. In
Proceedings of the Fifth Conference on Applied
Natural Language Processing, pp 1-7.
Nuance. 2002. http://www.nuance.com/.
Oviatt, S., Cohen, P., Wu, L., Vergo, J., Duncan, L.,
Suhm, B., Bers, J., Holzman, T., Winograd, T.,
Landay, J., Larson, J., Ferro, D. 2000. &amp;quot;Designing the
user interface for multimodal speech and pen-based
gesture applications: state-of-the-art systems and
future research directions&amp;quot;. Human Computer
Interaction.
Rashmi Prasad and Marilyn Walker. 2002. &amp;quot;Training a
Dialogue Act Tagger for Human-Human and
Human-Computer Travel Dialogues&amp;quot;. Proceedings of
3rd SIGDIAL Workshop. Philadelphia. pp.162-173.
Manny Rayner, John Dowding, Beth Ann Hockey.
2001. &amp;quot;A Baseline method for compiling typed
unification grammars into context free language
models&amp;quot;. In Proceedings of Eurospeech 2001, pp
729-732. Aalborg, Denmark.
Manny Rayner, John Dowding, Beth Ann Hockey.
2002a. &amp;quot;Regulus Documentation&amp;quot;.
Manny Rayner, Beth Ann Hockey, John Dowding.
2002b. &amp;quot;Grammar Specialisation meets Language
Modelling&amp;quot;. ICSLP 2002. Denver.
Manny Rayner and Pierrette Bouillon. 2002. &amp;quot;A
Flexible Speech to Speech Phrasebook Translator&amp;quot;.
Proceedings of the ACL-02 Speech-Speech
Translation Workshop, pp 69-76.
Jeff Rickel, Stacy Marsella, Jonathan Gratch, Randall
Hill, David Traum, William Swartout. 2002. Toward
a New Generation of Virtual Humans for Interactive
Experiences. IEEE Intelligent Systems, 1094-7167,
pp. 32-38.
William Roberts. 1998. &amp;quot;Automatic Speaker
Recognition Using Statistical Models&amp;quot;. DSTO
Research Report, DSTO-RR-0131, DSTO Electronics
and Surveillance Research Laboratory.
rVoice. 2002. Rhetorical Systems,
http://www.rhetoricalsystems.com/rvoice.html.
Paul Taplin, Geoffrey Fox, Michael Coleman, Steven
Wark, Dale Lambert. 2001. &amp;quot;Situation Awareness
Using a Virtual Adviser&amp;quot;, Talking Head Workshop,
OzCHI 2001, Fremantle, Australia.
Helen Wright. 1998. &amp;quot;Automatic utterance type
detection using suprasegmental features&amp;quot;.
Proceedings of the 5th International Conference on
Spoken Language Processing (ICSLP&apos;98). Sydney.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.114387">
<title confidence="0.9892715">Spoken Dialogue for Virtual Advisers in a semi-immersive and Control environment</title>
<author confidence="0.970626">Dominique Estival</author>
<author confidence="0.970626">Michael Broughton</author>
<author confidence="0.970626">Andrew Zschorn</author>
<author confidence="0.970626">Elizabeth</author>
<affiliation confidence="0.786144">Human Systems Integration Group, Command and Control Defence Science and Technology</affiliation>
<address confidence="0.66805">PO Box 1500, Edinburgh SA</address>
<author confidence="0.406917">Michael Broughton Dominique Estival</author>
<author confidence="0.406917">Andrew Zschorndsto defence gov au</author>
<abstract confidence="0.991243785714286">We present the spoken dialogue system designed and implemented for Virtual Advisers in the FOCAL environment. Its architecture is based on: Dialogue Agents using propositional attitudes, a Natural Language Understanding component using typed unification grammar, and a commercial speaker-independent speech recognition system. The current application aims to facilitate the multimedia presentation of military planning information in a semi-immersive environment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ananova</author>
</authors>
<date>2002</date>
<note>http://www.ananova.com.</note>
<contexts>
<context position="2634" citStr="Ananova, 2002" startWordPosition="361" endWordPosition="362">onment as its primary display, allowing vast quantities of information to be displayed. Our current VAs can be described as 3-dimensional &amp;quot;Talking Heads&amp;quot;, i.e. only the head and upper portions of the body are represented. They can display expression, lipsynchronisation and head movement, along with certain autonomous behaviours such as blinking and gaze (Taplin et al., 2001). These factors all combine to add life-likeness to the VAs and create more engaging interaction with users. Presenting information via a Talking Head has been commercially demonstrated by the virtual newscaster “Ananova” (Ananova, 2002). Embodied characters are also being developed and include the PPP (Andre, Rist and Muller, 1998) and Rea (Cassell, 2000). PPP is a cartoon style Personalized Plan-based Presenter that combines pointing, head movements and facial expressions to draw the viewer’s attention to the information being presented. Rea is a virtual real-estate agent that takes an active role in conversation, she nods her head to indicate understanding of spoken input, or can raise her hand to indicate a desire to speak. Several VAs have been implemented for FOCAL, each having a particular role or knowledge expertise. </context>
</contexts>
<marker>Ananova, 2002</marker>
<rawString>Ananova. 2002. http://www.ananova.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Andre</author>
<author>T Rist</author>
<author>J Muller</author>
</authors>
<title>Integrating Reactive and Scripted Behaviours in a Life-Like Presentation Agent,</title>
<date>1998</date>
<booktitle>Proceedings of the Second International Conference on Autonomous Agents,</booktitle>
<pages>261--268</pages>
<marker>Andre, Rist, Muller, 1998</marker>
<rawString>E. Andre, T. Rist, and J. Muller. 1998. Integrating Reactive and Scripted Behaviours in a Life-Like Presentation Agent, Proceedings of the Second International Conference on Autonomous Agents, 261-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Appen</author>
</authors>
<date>2002</date>
<note>http://www.appen.com.au.</note>
<marker>Appen, 2002</marker>
<rawString>Appen. 2002. http://www.appen.com.au.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Bolt</author>
</authors>
<title>Put-that-there&amp;quot;: voice and gesture at the graphics interface.</title>
<date>1980</date>
<booktitle>Proceedings of the SIGGRAPH,</booktitle>
<pages>262--270</pages>
<marker>Bolt, 1980</marker>
<rawString>R. A. Bolt. 1980. &amp;quot;Put-that-there&amp;quot;: voice and gesture at the graphics interface. Proceedings of the SIGGRAPH, July, 262-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Broughton</author>
<author>Oliver Carr</author>
<author>Dominique Estival</author>
<author>Paul Taplin</author>
<author>Steven Wark</author>
<author>Dale Lambert</author>
</authors>
<title>Conversing with Franco, FOCAL’s Virtual Adviser&amp;quot;. Conversation Characters Workshop, Human Factors</title>
<date>2002</date>
<location>Melbourne, Australia.</location>
<contexts>
<context position="4251" citStr="Broughton et al., 2002" startWordPosition="610" endWordPosition="613">t the operational level (see section 8). The VAs provide information rich briefs through the combined use of spoken output via Text-to-Speech (TTS) and multimedia. Relevant questions can be asked at the end of the briefs through the use of spoken dialogue. 3 Previous implementation: Franco As described in (Taplin et al., 2001) the first VA in FOCAL, named Franco, was also an animated 3- dimensional &amp;quot;Talking Head&amp;quot; model, intended to either deliver prepared information, such as a briefing or slide show, or to interact conversationally with users. To demonstrate the conversational functionality (Broughton et al., 2002), it was implemented with a commercial speaker-dependent automated speech recogniser (ASR), Dragon NaturallySpeakingTM. The Natural Language understanding component was implemented in NatLink (Gould, 2001) and a simple user-driven dialogue management, based on key-word recognition and nesting of dialogue states to provide context, was also implemented in Python. Franco has been successful in demonstrating the proof-of-concept of a VA in the FOCAL environment. Answering spoken questions about specific military assets and platforms, it also permits the display of other types of information such </context>
<context position="28359" citStr="Broughton et al., 2002" startWordPosition="4350" endWordPosition="4353">ent stages to present different information allows for a VA to be specialised in a particular domain, just as real briefing officers are during a real military planning exercise. For now, we only display one VA at a time, but we intend to experiment with having multiple VAs at the same time. The final state of the dialogue flow allows users to ask questions about any aspect of the planning process, and questions can be posed to all the VAs, so it would be natural for the users to see all the VAs at that stage. 8.5 Rapid Prototyping and Evaluation The key word version developed previously (see Broughton et al., 2002) has been maintained as a rapid prototyping environment for evaluating new scripts and dialogues. It allows new dialogues to be quickly tested by entering suitable key words, sufficient to discriminate one question from another. This system proves faster for testing than the more precise method of grammar building. Multiple response strings can be generated, providing more naturalness for those interacting with the VAs on a regular basis. By rapidly prototyping questions and responses, we can test the intuitiveness of expected questions and the smoothness and timeliness of responses, particula</context>
</contexts>
<marker>Broughton, Carr, Estival, Taplin, Wark, Lambert, 2002</marker>
<rawString>Michael Broughton, Oliver Carr, Dominique Estival, Paul Taplin, Steven Wark, Dale Lambert. 2002. &amp;quot;Conversing with Franco, FOCAL’s Virtual Adviser&amp;quot;. Conversation Characters Workshop, Human Factors 2002, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
<author>Lynn Lambert</author>
</authors>
<title>A Process Model for Recognizing Communicative Acts and Modeling Negotiation Subdialogues&amp;quot;.</title>
<date>1999</date>
<journal>Computational Linguistics.</journal>
<volume>25</volume>
<pages>1--53</pages>
<contexts>
<context position="9898" citStr="Carberry and Lambert, 1999" startWordPosition="1468" endWordPosition="1471">esting area of research in itself, and one of the goals of the project has been to see how ATTITUDE needs to be extended to implement dialogue management. Further, this allows us to investigate how far attitude programming (see section 6.2) can go towards expressing speech acts and communicative act type. However, we do not claim to employ the full power of propositional attitudes in our implementation yet. This is another area of research which we are now exploring. Neither are we yet at the stage where we could perform automatic detection of utterance type (Wright, 1998) or of dialogue act (Carberry and Lambert, 1999; Prasad and Walker, 2002). 6.1 Propositional attitudes The ATTITUDE programming environment is so named because it utilises propositional attitude instructions as programming instructions (this has been dubbed attitude programming). Propositional attitudes are alleged mental states characterised by propositional attitude expressions, which are the means by which individuals relate their own mental behaviour to others&apos;. Propositional attitude instructions are of the form shown in (1). (1) [subject][attitude][propositional expression] In (1): - [subject] denotes the individual whose mental stat</context>
</contexts>
<marker>Carberry, Lambert, 1999</marker>
<rawString>Sandra Carberry and Lynn Lambert. 1999. &amp;quot;A Process Model for Recognizing Communicative Acts and Modeling Negotiation Subdialogues&amp;quot;. Computational Linguistics. 25,1, pp. 1-53</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
</authors>
<title>Embodied Conversational Interface Agents,</title>
<date>2000</date>
<journal>Communications of the ACM,</journal>
<volume>43</volume>
<pages>70--78</pages>
<contexts>
<context position="2755" citStr="Cassell, 2000" startWordPosition="380" endWordPosition="381">as 3-dimensional &amp;quot;Talking Heads&amp;quot;, i.e. only the head and upper portions of the body are represented. They can display expression, lipsynchronisation and head movement, along with certain autonomous behaviours such as blinking and gaze (Taplin et al., 2001). These factors all combine to add life-likeness to the VAs and create more engaging interaction with users. Presenting information via a Talking Head has been commercially demonstrated by the virtual newscaster “Ananova” (Ananova, 2002). Embodied characters are also being developed and include the PPP (Andre, Rist and Muller, 1998) and Rea (Cassell, 2000). PPP is a cartoon style Personalized Plan-based Presenter that combines pointing, head movements and facial expressions to draw the viewer’s attention to the information being presented. Rea is a virtual real-estate agent that takes an active role in conversation, she nods her head to indicate understanding of spoken input, or can raise her hand to indicate a desire to speak. Several VAs have been implemented for FOCAL, each having a particular role or knowledge expertise. For example, one adviser may have specialist knowledge relating to legal issues, another may have information relating to</context>
</contexts>
<marker>Cassell, 2000</marker>
<rawString>Justine Cassell. 2000. Embodied Conversational Interface Agents, Communications of the ACM, Vol. 43, No. 4, 70-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Colineau</author>
<author>Cécile Paris</author>
</authors>
<title>Framework for the Design of Intelligent Multimedia Presentation Systems: An architecture proposal for FOCAL.</title>
<date>2003</date>
<tech>CMIS Technical Report 03/92, CSIRO,</tech>
<contexts>
<context position="33394" citStr="Colineau and Paris, 2003" startWordPosition="5141" endWordPosition="5144">d a dialogue management architecture based on ATTITUDE agents which communicate with each other using propositional attitude expressions. Other agents can now be developed to perform additional functions, in particular to launch the display of other types of information and to interpret other types of input. This will allow us to explore how spoken dialogue with VAs can be combined with other virtual interaction technologies (e.g., gesture, pointing, gaze tracking). In this respect, the next step in our project is the development of a full fledge MMP agent based on the framework described in (Colineau and Paris, 2003). However, the work we have reported here must also be seen as part of the larger research programme undertaken within FOCAL. From this perspective, this work is of interest because it allows other members of the HSI group to pursue research in the usability of new technologies to perform the paradigm shift in command environments. In particular, this project is providing the support for further research into whether this way of presenting information is helpful in an operational command environment. It allows us to devise experiments to explore the crucial issue of trust in the information be</context>
</contexts>
<marker>Colineau, Paris, 2003</marker>
<rawString>Nathalie Colineau and Cécile Paris. 2003. Framework for the Design of Intelligent Multimedia Presentation Systems: An architecture proposal for FOCAL. CMIS Technical Report 03/92, CSIRO, May 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Estival</author>
</authors>
<title>The Syrinx Spoken Language System&amp;quot;.</title>
<date>2002</date>
<journal>International Journal of Speech Technology.</journal>
<volume>5</volume>
<pages>85--96</pages>
<contexts>
<context position="1404" citStr="Estival, 2002" startWordPosition="178" endWordPosition="179">of military planning information in a semi-immersive environment. 1 Introduction In this paper, we present the spoken dialogue system implemented for communicating with the virtual advisers (VAs) in the Future Operations Centre Analysis Laboratory (FOCAL) at the Australian Defence Science and Technology Organisation (DSTO). We are experimenting with the use of spoken dialogue with virtual conversational characters to access multi-media information during the conduct of military operations and in particular to facilitate the planning of such operations. Unlike telephone-based dialogue systems (Estival, 2002), which are mainly created for new commercial applications, dialogue systems for Command and Control applications (Moore et al. 1997) generally seek to simulate the military domain and therefore require an understanding of that domain. 2 Using Virtual Advisers in FOCAL FOCAL was established to &amp;quot;pioneer a paradigm shift in command environments through a superior use of capability and greater situation awareness&amp;quot;. The facility was designed to experiment with innovative technologies to support this goal, and it has now been running for two years. FOCAL contains a large-screen, semiimmersive virtu</context>
</contexts>
<marker>Estival, 2002</marker>
<rawString>Dominique Estival. 2002. &amp;quot;The Syrinx Spoken Language System&amp;quot;. International Journal of Speech Technology. vol.5. no.1. pp.85-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
<author>Srinivas Bangalore</author>
<author>Gunaranjan Vasireddy</author>
<author>Amanda Stent</author>
<author>Patrick Ehlen</author>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
<author>Preetam Maloor</author>
</authors>
<title>MATCH: an Architecture for Multimodal Dialogue Systems&amp;quot;.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL&apos;02).</booktitle>
<pages>376--383</pages>
<location>Philadelphia..</location>
<contexts>
<context position="30789" citStr="Johnston et al., 2002" startWordPosition="4726" endWordPosition="4729">t DSTO (Roberts, 1998). This system uses statistical modelling techniques and is capable of both speaker identification (recognising users from a database of stored speech profiles) and speaker verification (verifying the identity of a particular user). We are also proposing to use pointing techniques in combination with the speech and language technologies to build a multimodal system. Multimodal systems were originally demonstrated by Bolts (1980) and research is continuing across varied applications (e.g., Oviatt et al., 2000 and Gibbon et al., 2000). However, unlike systems such as MATCH (Johnston et al., 2002), where the issue is allowing multimodal interaction on portable devices with very small screens, in FOCAL we are concerned with ensuring that users get the full benefit of the very large screen and with allowing several users to interact at a distance from the screen. It is also worth mentioning that, unlike the interactive system described in (Rickel et al., 2002), which is concerned with training in a military environment, we are not trying to simulate a complete virtual world with embodied agents. However, we propose to include traditional pointing technologies, such as the standard deskto</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor. 2002. &amp;quot;MATCH: an Architecture for Multimodal Dialogue Systems&amp;quot;. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL&apos;02). pp. 376-383. Philadelphia..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dafydd Gibbon</author>
<author>Inge Mertins</author>
<author>Roger K Moore</author>
</authors>
<date>2000</date>
<booktitle>Handbook of Multimodal and Spoken Dialogue Systems: Resources, Terminology and Product Evaluation.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="30726" citStr="Gibbon et al., 2000" startWordPosition="4716" endWordPosition="4719">CAL the speaker recognition system which has been developed at DSTO (Roberts, 1998). This system uses statistical modelling techniques and is capable of both speaker identification (recognising users from a database of stored speech profiles) and speaker verification (verifying the identity of a particular user). We are also proposing to use pointing techniques in combination with the speech and language technologies to build a multimodal system. Multimodal systems were originally demonstrated by Bolts (1980) and research is continuing across varied applications (e.g., Oviatt et al., 2000 and Gibbon et al., 2000). However, unlike systems such as MATCH (Johnston et al., 2002), where the issue is allowing multimodal interaction on portable devices with very small screens, in FOCAL we are concerned with ensuring that users get the full benefit of the very large screen and with allowing several users to interact at a distance from the screen. It is also worth mentioning that, unlike the interactive system described in (Rickel et al., 2002), which is concerned with training in a military environment, we are not trying to simulate a complete virtual world with embodied agents. However, we propose to include</context>
</contexts>
<marker>Gibbon, Mertins, Moore, 2000</marker>
<rawString>Dafydd Gibbon, Inge Mertins, Roger K. Moore (Eds.). 2000. Handbook of Multimodal and Spoken Dialogue Systems: Resources, Terminology and Product Evaluation. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<title>Global InfoTek Inc.</title>
<date>2002</date>
<location>http://coabs.globalinfotek.com.</location>
<marker>2002</marker>
<rawString>Global InfoTek Inc. 2002. Control of Agent Based Systems. http://coabs.globalinfotek.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Gould</author>
</authors>
<title>Implementation and Acceptance of NatLink, a Python-Based Macro System for Dragon NaturallySpeaking&amp;quot;,</title>
<date>2001</date>
<booktitle>The Ninth International Python Conference,</booktitle>
<pages>5--8</pages>
<location>California</location>
<contexts>
<context position="4456" citStr="Gould, 2001" startWordPosition="636" endWordPosition="637">iefs through the use of spoken dialogue. 3 Previous implementation: Franco As described in (Taplin et al., 2001) the first VA in FOCAL, named Franco, was also an animated 3- dimensional &amp;quot;Talking Head&amp;quot; model, intended to either deliver prepared information, such as a briefing or slide show, or to interact conversationally with users. To demonstrate the conversational functionality (Broughton et al., 2002), it was implemented with a commercial speaker-dependent automated speech recogniser (ASR), Dragon NaturallySpeakingTM. The Natural Language understanding component was implemented in NatLink (Gould, 2001) and a simple user-driven dialogue management, based on key-word recognition and nesting of dialogue states to provide context, was also implemented in Python. Franco has been successful in demonstrating the proof-of-concept of a VA in the FOCAL environment. Answering spoken questions about specific military assets and platforms, it also permits the display of other types of information such as pictures, animated video clips, tabular information from a database, and location details on digital maps. 4 Improvements Although Franco was successful in demonstrating the potential usefulness of a VA</context>
</contexts>
<marker>Gould, 2001</marker>
<rawString>Joel Gould. 2001. &amp;quot;Implementation and Acceptance of NatLink, a Python-Based Macro System for Dragon NaturallySpeaking&amp;quot;, The Ninth International Python Conference, March 5-8, California</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha L Kahn</author>
<author>Cynthia Della Torre Cicalese</author>
</authors>
<title>CoABS Grid Scalability Experiments&amp;quot;.</title>
<date>2001</date>
<booktitle>Proceedings of the Second International Workshop on Infrastructure for Agents, MAS, and Scalable MAS, Autonomous Agents</booktitle>
<note>Conference.</note>
<marker>Kahn, Cicalese, 2001</marker>
<rawString>Martha L. Kahn and Cynthia Della Torre Cicalese. 2001. &amp;quot;CoABS Grid Scalability Experiments&amp;quot;. Proceedings of the Second International Workshop on Infrastructure for Agents, MAS, and Scalable MAS, Autonomous Agents 2001 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale A Lambert</author>
<author>Mikael G Relbe</author>
</authors>
<title>Reasoning with Tolerance&amp;quot;.</title>
<date>1998</date>
<booktitle>2nd International Conference on Knowledge-Based Intelligent Electronic Systems. IEEE.</booktitle>
<pages>418--427</pages>
<contexts>
<context position="8148" citStr="Lambert and Relbe, 1998" startWordPosition="1200" endWordPosition="1203">d us to integrate all the components of the dialogue system and it will provide an easy way to integrate other agents and a variety of input and output devices. Communication between CoABS agents is accomplished via string messages. 6 Dialogue Management with ATTITUDE ATTITUDE is a multi-agent architecture developed at DSTO, capable of representing and reasoning both with uncertainty and about multiple alternative scenarios (Lambert, 1999). It is a multi-agent extension of the MetaCon reactive planner developed for control of phased array radars on the Swedish Airborne Early Warning aircraft (Lambert and Relbe, 1998). ATTITUDE has some similarities with Prolog and other logic programming languages as well as with AI research on blackboard and multi-agent architectures. Because ATTITUDE was designed specifically to support the programming of reactive systems, it possesses powerful facilities for handling interactions of the internal system entities, both with each other and with the external world. ATTITUDE is very high-level, weakly-typed, and thanks to the agent paradigm, it produces loosely coupled and modularised systems. For these reasons, and because ATTITUDE implements reasoning about propositional </context>
</contexts>
<marker>Lambert, Relbe, 1998</marker>
<rawString>Dale A. Lambert and Mikael G. Relbe. 1998. &amp;quot;Reasoning with Tolerance&amp;quot;. 2nd International Conference on Knowledge-Based Intelligent Electronic Systems. IEEE. pp. 418-427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale A Lambert</author>
</authors>
<title>Advisers With ATTITUDE for Situation Awareness&amp;quot;.</title>
<date>1999</date>
<booktitle>Proceedings of the 1999 Workshop on Defence Applications of Signal Processing.</booktitle>
<pages>113--118</pages>
<location>LaSalle, Illinois.</location>
<note>Edited</note>
<contexts>
<context position="7967" citStr="Lambert, 1999" startWordPosition="1174" endWordPosition="1175">designed to allow a large number of heterogeneous procedural, object-oriented and agent-based systems to communicate. Using the CoABS Grid as our infrastructure has allowed us to integrate all the components of the dialogue system and it will provide an easy way to integrate other agents and a variety of input and output devices. Communication between CoABS agents is accomplished via string messages. 6 Dialogue Management with ATTITUDE ATTITUDE is a multi-agent architecture developed at DSTO, capable of representing and reasoning both with uncertainty and about multiple alternative scenarios (Lambert, 1999). It is a multi-agent extension of the MetaCon reactive planner developed for control of phased array radars on the Swedish Airborne Early Warning aircraft (Lambert and Relbe, 1998). ATTITUDE has some similarities with Prolog and other logic programming languages as well as with AI research on blackboard and multi-agent architectures. Because ATTITUDE was designed specifically to support the programming of reactive systems, it possesses powerful facilities for handling interactions of the internal system entities, both with each other and with the external world. ATTITUDE is very high-level, w</context>
<context position="9898" citStr="Lambert, 1999" startWordPosition="1470" endWordPosition="1471">f research in itself, and one of the goals of the project has been to see how ATTITUDE needs to be extended to implement dialogue management. Further, this allows us to investigate how far attitude programming (see section 6.2) can go towards expressing speech acts and communicative act type. However, we do not claim to employ the full power of propositional attitudes in our implementation yet. This is another area of research which we are now exploring. Neither are we yet at the stage where we could perform automatic detection of utterance type (Wright, 1998) or of dialogue act (Carberry and Lambert, 1999; Prasad and Walker, 2002). 6.1 Propositional attitudes The ATTITUDE programming environment is so named because it utilises propositional attitude instructions as programming instructions (this has been dubbed attitude programming). Propositional attitudes are alleged mental states characterised by propositional attitude expressions, which are the means by which individuals relate their own mental behaviour to others&apos;. Propositional attitude instructions are of the form shown in (1). (1) [subject][attitude][propositional expression] In (1): - [subject] denotes the individual whose mental stat</context>
</contexts>
<marker>Lambert, 1999</marker>
<rawString>Dale A. Lambert. 1999. &amp;quot;Advisers With ATTITUDE for Situation Awareness&amp;quot;. Proceedings of the 1999 Workshop on Defence Applications of Signal Processing. pp.113-118, Edited A. Lindsey, B. Moran, J. Schroeder, M. Smith and L. White. LaSalle, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale A Lambert</author>
</authors>
<title>Automating Cognitive Routines&amp;quot;, accepted for publication</title>
<date>2003</date>
<booktitle>in the 6th International Conference on Information Fusion.</booktitle>
<marker>Lambert, 2003</marker>
<rawString>Dale A. Lambert. 2003. &amp;quot;Automating Cognitive Routines&amp;quot;, accepted for publication in the 6th International Conference on Information Fusion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
<author>H Bratt</author>
<author>J Gawron</author>
<author>Y Gorfu</author>
<author>A Cheyer</author>
</authors>
<title>CommandTalk: A spokenlanguage interface for battlefield simulations&amp;quot;.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="1537" citStr="Moore et al. 1997" startWordPosition="195" endWordPosition="198">stem implemented for communicating with the virtual advisers (VAs) in the Future Operations Centre Analysis Laboratory (FOCAL) at the Australian Defence Science and Technology Organisation (DSTO). We are experimenting with the use of spoken dialogue with virtual conversational characters to access multi-media information during the conduct of military operations and in particular to facilitate the planning of such operations. Unlike telephone-based dialogue systems (Estival, 2002), which are mainly created for new commercial applications, dialogue systems for Command and Control applications (Moore et al. 1997) generally seek to simulate the military domain and therefore require an understanding of that domain. 2 Using Virtual Advisers in FOCAL FOCAL was established to &amp;quot;pioneer a paradigm shift in command environments through a superior use of capability and greater situation awareness&amp;quot;. The facility was designed to experiment with innovative technologies to support this goal, and it has now been running for two years. FOCAL contains a large-screen, semiimmersive virtual reality environment as its primary display, allowing vast quantities of information to be displayed. Our current VAs can be descri</context>
</contexts>
<marker>Moore, Dowding, Bratt, Gawron, Gorfu, Cheyer, 1997</marker>
<rawString>R. Moore, J. Dowding, H. Bratt, J. Gawron, Y. Gorfu, A. Cheyer. 1997. &amp;quot;CommandTalk: A spokenlanguage interface for battlefield simulations&amp;quot;. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pp 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuance</author>
</authors>
<date>2002</date>
<note>http://www.nuance.com/.</note>
<contexts>
<context position="21081" citStr="Nuance, 2002" startWordPosition="3200" endWordPosition="3201">presented and the media to be used are determined by the agent for that particular dialogue state. The VA then allows users to ask questions to repeat or clarify particular points, or to gain additional information. 7 Spoken Language Processing 7.1 Speaker-independent speech recognition As stated in section 4, one of the main motivations for moving from a speaker-dependent to a speakerindependent ASR was to allow visitors in FOCAL the possibility of using the system themselves, rather than relying on a small set of trained individuals to run demonstrations. We chose to use the Nuance Toolkit (Nuance, 2002) for several reasons: besides its reliability as a speakerindependent ASR for both telephone and microphone speech, Nuance 8.0 provides Australian-New Zealand English, as well as US and UK English, acoustic language models. Even more importantly for our purposes, Nuance grammars can be compiled from Regulus, a higherlevel language processing component which has already been used to develop several spoken dialogue systems in different domains (Rayner et al., 2001, Rayner and Bouillon, 2002). 7.2 Spoken Language Understanding Following our decision to move from a speakerdependent to a speaker-in</context>
</contexts>
<marker>Nuance, 2002</marker>
<rawString>Nuance. 2002. http://www.nuance.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oviatt</author>
<author>P Cohen</author>
<author>L Wu</author>
<author>J Vergo</author>
<author>L Duncan</author>
<author>B Suhm</author>
<author>J Bers</author>
<author>T Holzman</author>
<author>T Winograd</author>
<author>J Landay</author>
<author>J Larson</author>
<author>D Ferro</author>
</authors>
<title>Designing the user interface for multimodal speech and pen-based gesture applications: state-of-the-art systems and future research directions&amp;quot;. Human Computer Interaction.</title>
<date>2000</date>
<contexts>
<context position="30701" citStr="Oviatt et al., 2000" startWordPosition="4711" endWordPosition="4714">n to integrate within FOCAL the speaker recognition system which has been developed at DSTO (Roberts, 1998). This system uses statistical modelling techniques and is capable of both speaker identification (recognising users from a database of stored speech profiles) and speaker verification (verifying the identity of a particular user). We are also proposing to use pointing techniques in combination with the speech and language technologies to build a multimodal system. Multimodal systems were originally demonstrated by Bolts (1980) and research is continuing across varied applications (e.g., Oviatt et al., 2000 and Gibbon et al., 2000). However, unlike systems such as MATCH (Johnston et al., 2002), where the issue is allowing multimodal interaction on portable devices with very small screens, in FOCAL we are concerned with ensuring that users get the full benefit of the very large screen and with allowing several users to interact at a distance from the screen. It is also worth mentioning that, unlike the interactive system described in (Rickel et al., 2002), which is concerned with training in a military environment, we are not trying to simulate a complete virtual world with embodied agents. Howev</context>
</contexts>
<marker>Oviatt, Cohen, Wu, Vergo, Duncan, Suhm, Bers, Holzman, Winograd, Landay, Larson, Ferro, 2000</marker>
<rawString>Oviatt, S., Cohen, P., Wu, L., Vergo, J., Duncan, L., Suhm, B., Bers, J., Holzman, T., Winograd, T., Landay, J., Larson, J., Ferro, D. 2000. &amp;quot;Designing the user interface for multimodal speech and pen-based gesture applications: state-of-the-art systems and future research directions&amp;quot;. Human Computer Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Marilyn Walker</author>
</authors>
<title>Training a Dialogue Act Tagger for Human-Human and Human-Computer Travel Dialogues&amp;quot;.</title>
<date>2002</date>
<booktitle>Proceedings of 3rd SIGDIAL Workshop. Philadelphia.</booktitle>
<pages>162--173</pages>
<contexts>
<context position="9924" citStr="Prasad and Walker, 2002" startWordPosition="1472" endWordPosition="1475">tself, and one of the goals of the project has been to see how ATTITUDE needs to be extended to implement dialogue management. Further, this allows us to investigate how far attitude programming (see section 6.2) can go towards expressing speech acts and communicative act type. However, we do not claim to employ the full power of propositional attitudes in our implementation yet. This is another area of research which we are now exploring. Neither are we yet at the stage where we could perform automatic detection of utterance type (Wright, 1998) or of dialogue act (Carberry and Lambert, 1999; Prasad and Walker, 2002). 6.1 Propositional attitudes The ATTITUDE programming environment is so named because it utilises propositional attitude instructions as programming instructions (this has been dubbed attitude programming). Propositional attitudes are alleged mental states characterised by propositional attitude expressions, which are the means by which individuals relate their own mental behaviour to others&apos;. Propositional attitude instructions are of the form shown in (1). (1) [subject][attitude][propositional expression] In (1): - [subject] denotes the individual whose mental state is being characterised; </context>
</contexts>
<marker>Prasad, Walker, 2002</marker>
<rawString>Rashmi Prasad and Marilyn Walker. 2002. &amp;quot;Training a Dialogue Act Tagger for Human-Human and Human-Computer Travel Dialogues&amp;quot;. Proceedings of 3rd SIGDIAL Workshop. Philadelphia. pp.162-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>John Dowding</author>
<author>Beth Ann Hockey</author>
</authors>
<title>A Baseline method for compiling typed unification grammars into context free language models&amp;quot;.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<pages>729--732</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="21547" citStr="Rayner et al., 2001" startWordPosition="3268" endWordPosition="3271">the system themselves, rather than relying on a small set of trained individuals to run demonstrations. We chose to use the Nuance Toolkit (Nuance, 2002) for several reasons: besides its reliability as a speakerindependent ASR for both telephone and microphone speech, Nuance 8.0 provides Australian-New Zealand English, as well as US and UK English, acoustic language models. Even more importantly for our purposes, Nuance grammars can be compiled from Regulus, a higherlevel language processing component which has already been used to develop several spoken dialogue systems in different domains (Rayner et al., 2001, Rayner and Bouillon, 2002). 7.2 Spoken Language Understanding Following our decision to move from a speakerdependent to a speaker-independent ASR, we decided to use Regulus to implement our Natural Language Understanding component. Regulus is an Open Source environment which compiles typed unification grammars into context-free grammar language models compatible with the Nuance Toolkit. It is &amp;quot;written in a Prolog-based feature-value notation and compiles into Nuance GSL grammars.&amp;quot; (Rayner et al., 2002a). Regulus is also described in detail in (Rayner et al., 2001). The main motivation for us</context>
</contexts>
<marker>Rayner, Dowding, Hockey, 2001</marker>
<rawString>Manny Rayner, John Dowding, Beth Ann Hockey. 2001. &amp;quot;A Baseline method for compiling typed unification grammars into context free language models&amp;quot;. In Proceedings of Eurospeech 2001, pp 729-732. Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>John Dowding</author>
</authors>
<date>2002</date>
<location>Beth Ann Hockey.</location>
<note>Regulus Documentation&amp;quot;.</note>
<marker>Rayner, Dowding, 2002</marker>
<rawString>Manny Rayner, John Dowding, Beth Ann Hockey. 2002a. &amp;quot;Regulus Documentation&amp;quot;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>Beth Ann Hockey</author>
<author>John Dowding</author>
</authors>
<title>Grammar Specialisation meets Language Modelling&amp;quot;. ICSLP</title>
<date>2002</date>
<location>Denver.</location>
<contexts>
<context position="13646" citStr="Rayner et al., 2002" startWordPosition="2018" endWordPosition="2021">t information and produces a corresponding ATTITUDE expression, which is then forwarded to Conductor. The linguistic coverage of the system is determined by the grammars which are available at each dialogue state. For now, the coverage is limited to a set of utterances appropriate for the briefing scenario described in section 8. These were used to define the Regulus1 grammars from which the Nuance grammars are compiled. We are now planning to move from Regulus1 to Regulus2, which will allow us to derive dialogue state grammars from a large English grammar using the EBL strategy described in (Rayner et al., 2002b) Figure 1. Dialogue with ATTITUDE English question from user This agent is responsible for dialogue flow control and all othe r dialogue agents must register with it. Conductor receives communicative acts from Speaker . For example: ( whquestion (property mig - 29 flying - range ?value ?units)) This query is forwarded on to all registered agents. Conductor chooses the most appropriate response received and sends this to MMP to present the answer. Multimedia Presenter This agent receives a list of expressions from Conductor and directs the appropriate services to present multimedia data to th</context>
<context position="22055" citStr="Rayner et al., 2002" startWordPosition="3340" endWordPosition="3343">hich has already been used to develop several spoken dialogue systems in different domains (Rayner et al., 2001, Rayner and Bouillon, 2002). 7.2 Spoken Language Understanding Following our decision to move from a speakerdependent to a speaker-independent ASR, we decided to use Regulus to implement our Natural Language Understanding component. Regulus is an Open Source environment which compiles typed unification grammars into context-free grammar language models compatible with the Nuance Toolkit. It is &amp;quot;written in a Prolog-based feature-value notation and compiles into Nuance GSL grammars.&amp;quot; (Rayner et al., 2002a). Regulus is also described in detail in (Rayner et al., 2001). The main motivation for using Regulus is the usual one of greater efficiency due to the more compact nature of a unification grammar representation compared with a context-free grammar. In addition, using Regulus to define a higher level grammar, we are able to obtain as our semantic representation a list of attribute-value pairs, and this permits a more sophisticated processing of the information by the other agents. Regulus also allows the development of bidirectional grammars, and we intend to make use of this functionality i</context>
</contexts>
<marker>Rayner, Hockey, Dowding, 2002</marker>
<rawString>Manny Rayner, Beth Ann Hockey, John Dowding. 2002b. &amp;quot;Grammar Specialisation meets Language Modelling&amp;quot;. ICSLP 2002. Denver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>Pierrette Bouillon</author>
</authors>
<title>A Flexible Speech to Speech Phrasebook Translator&amp;quot;.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL-02 Speech-Speech Translation Workshop,</booktitle>
<pages>69--76</pages>
<contexts>
<context position="21575" citStr="Rayner and Bouillon, 2002" startWordPosition="3272" endWordPosition="3275">, rather than relying on a small set of trained individuals to run demonstrations. We chose to use the Nuance Toolkit (Nuance, 2002) for several reasons: besides its reliability as a speakerindependent ASR for both telephone and microphone speech, Nuance 8.0 provides Australian-New Zealand English, as well as US and UK English, acoustic language models. Even more importantly for our purposes, Nuance grammars can be compiled from Regulus, a higherlevel language processing component which has already been used to develop several spoken dialogue systems in different domains (Rayner et al., 2001, Rayner and Bouillon, 2002). 7.2 Spoken Language Understanding Following our decision to move from a speakerdependent to a speaker-independent ASR, we decided to use Regulus to implement our Natural Language Understanding component. Regulus is an Open Source environment which compiles typed unification grammars into context-free grammar language models compatible with the Nuance Toolkit. It is &amp;quot;written in a Prolog-based feature-value notation and compiles into Nuance GSL grammars.&amp;quot; (Rayner et al., 2002a). Regulus is also described in detail in (Rayner et al., 2001). The main motivation for using Regulus is the usual one</context>
</contexts>
<marker>Rayner, Bouillon, 2002</marker>
<rawString>Manny Rayner and Pierrette Bouillon. 2002. &amp;quot;A Flexible Speech to Speech Phrasebook Translator&amp;quot;. Proceedings of the ACL-02 Speech-Speech Translation Workshop, pp 69-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Rickel</author>
<author>Stacy Marsella</author>
<author>Jonathan Gratch</author>
<author>Randall Hill</author>
<author>David Traum</author>
<author>William Swartout</author>
</authors>
<title>Toward a New Generation of Virtual Humans for Interactive Experiences.</title>
<date>2002</date>
<journal>IEEE Intelligent Systems,</journal>
<pages>1094--7167</pages>
<contexts>
<context position="31157" citStr="Rickel et al., 2002" startWordPosition="4787" endWordPosition="4790"> multimodal system. Multimodal systems were originally demonstrated by Bolts (1980) and research is continuing across varied applications (e.g., Oviatt et al., 2000 and Gibbon et al., 2000). However, unlike systems such as MATCH (Johnston et al., 2002), where the issue is allowing multimodal interaction on portable devices with very small screens, in FOCAL we are concerned with ensuring that users get the full benefit of the very large screen and with allowing several users to interact at a distance from the screen. It is also worth mentioning that, unlike the interactive system described in (Rickel et al., 2002), which is concerned with training in a military environment, we are not trying to simulate a complete virtual world with embodied agents. However, we propose to include traditional pointing technologies, such as the standard desktop mouse, through to 3-dimensional tracking systems for gaze, gesture and user tracking. This will involve integrating more complex language understanding, as information will need to be derived from both the user&apos;s utterance and from what is being pointed to. For example, to interpret an utterance such as (10) uttered while the user points to a location on a map, we</context>
</contexts>
<marker>Rickel, Marsella, Gratch, Hill, Traum, Swartout, 2002</marker>
<rawString>Jeff Rickel, Stacy Marsella, Jonathan Gratch, Randall Hill, David Traum, William Swartout. 2002. Toward a New Generation of Virtual Humans for Interactive Experiences. IEEE Intelligent Systems, 1094-7167, pp. 32-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Roberts</author>
</authors>
<title>Automatic Speaker Recognition Using Statistical Models&amp;quot;.</title>
<date>1998</date>
<booktitle>DSTO Research Report, DSTO-RR-0131, DSTO Electronics and Surveillance Research Laboratory. rVoice.</booktitle>
<note>Rhetorical Systems, http://www.rhetoricalsystems.com/rvoice.html.</note>
<contexts>
<context position="30189" citStr="Roberts, 1998" startWordPosition="4640" endWordPosition="4641">interaction, and we are investigating speaker recognition as well as a range of pointing technologies. The need for a speaker recognition system has emerged with the move to a speaker independent ASR. With a speaker dependent ASR, users would load their individual profile before use, thus enabling the system to know who was using it. With a speaker-independent ASR, a speaker recognition system would allow the VAs to recognise who is talking to them and enable them to address known users by name. We plan to integrate within FOCAL the speaker recognition system which has been developed at DSTO (Roberts, 1998). This system uses statistical modelling techniques and is capable of both speaker identification (recognising users from a database of stored speech profiles) and speaker verification (verifying the identity of a particular user). We are also proposing to use pointing techniques in combination with the speech and language technologies to build a multimodal system. Multimodal systems were originally demonstrated by Bolts (1980) and research is continuing across varied applications (e.g., Oviatt et al., 2000 and Gibbon et al., 2000). However, unlike systems such as MATCH (Johnston et al., 2002)</context>
</contexts>
<marker>Roberts, 1998</marker>
<rawString>William Roberts. 1998. &amp;quot;Automatic Speaker Recognition Using Statistical Models&amp;quot;. DSTO Research Report, DSTO-RR-0131, DSTO Electronics and Surveillance Research Laboratory. rVoice. 2002. Rhetorical Systems, http://www.rhetoricalsystems.com/rvoice.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taplin</author>
<author>Geoffrey Fox</author>
<author>Michael Coleman</author>
<author>Steven Wark</author>
<author>Dale Lambert</author>
</authors>
<title>Situation Awareness Using a Virtual Adviser&amp;quot;, Talking Head Workshop, OzCHI</title>
<date>2001</date>
<location>Fremantle, Australia.</location>
<contexts>
<context position="2397" citStr="Taplin et al., 2001" startWordPosition="325" endWordPosition="328">pability and greater situation awareness&amp;quot;. The facility was designed to experiment with innovative technologies to support this goal, and it has now been running for two years. FOCAL contains a large-screen, semiimmersive virtual reality environment as its primary display, allowing vast quantities of information to be displayed. Our current VAs can be described as 3-dimensional &amp;quot;Talking Heads&amp;quot;, i.e. only the head and upper portions of the body are represented. They can display expression, lipsynchronisation and head movement, along with certain autonomous behaviours such as blinking and gaze (Taplin et al., 2001). These factors all combine to add life-likeness to the VAs and create more engaging interaction with users. Presenting information via a Talking Head has been commercially demonstrated by the virtual newscaster “Ananova” (Ananova, 2002). Embodied characters are also being developed and include the PPP (Andre, Rist and Muller, 1998) and Rea (Cassell, 2000). PPP is a cartoon style Personalized Plan-based Presenter that combines pointing, head movements and facial expressions to draw the viewer’s attention to the information being presented. Rea is a virtual real-estate agent that takes an activ</context>
<context position="3956" citStr="Taplin et al., 2001" startWordPosition="566" endWordPosition="569">rmation relating to the geography of a region. Each VA has a different facial appearance, voice and mannerisms. To demonstrate and evaluate the performance of VAs (and of the other FOCAL projects), a fictitious scenario has been developed that incorporates key elements of military planning at the operational level (see section 8). The VAs provide information rich briefs through the combined use of spoken output via Text-to-Speech (TTS) and multimedia. Relevant questions can be asked at the end of the briefs through the use of spoken dialogue. 3 Previous implementation: Franco As described in (Taplin et al., 2001) the first VA in FOCAL, named Franco, was also an animated 3- dimensional &amp;quot;Talking Head&amp;quot; model, intended to either deliver prepared information, such as a briefing or slide show, or to interact conversationally with users. To demonstrate the conversational functionality (Broughton et al., 2002), it was implemented with a commercial speaker-dependent automated speech recogniser (ASR), Dragon NaturallySpeakingTM. The Natural Language understanding component was implemented in NatLink (Gould, 2001) and a simple user-driven dialogue management, based on key-word recognition and nesting of dialogue</context>
</contexts>
<marker>Taplin, Fox, Coleman, Wark, Lambert, 2001</marker>
<rawString>Paul Taplin, Geoffrey Fox, Michael Coleman, Steven Wark, Dale Lambert. 2001. &amp;quot;Situation Awareness Using a Virtual Adviser&amp;quot;, Talking Head Workshop, OzCHI 2001, Fremantle, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Wright</author>
</authors>
<title>Automatic utterance type detection using suprasegmental features&amp;quot;.</title>
<date>1998</date>
<booktitle>Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP&apos;98).</booktitle>
<location>Sydney.</location>
<contexts>
<context position="9851" citStr="Wright, 1998" startWordPosition="1462" endWordPosition="1463">s purpose before, this is an interesting area of research in itself, and one of the goals of the project has been to see how ATTITUDE needs to be extended to implement dialogue management. Further, this allows us to investigate how far attitude programming (see section 6.2) can go towards expressing speech acts and communicative act type. However, we do not claim to employ the full power of propositional attitudes in our implementation yet. This is another area of research which we are now exploring. Neither are we yet at the stage where we could perform automatic detection of utterance type (Wright, 1998) or of dialogue act (Carberry and Lambert, 1999; Prasad and Walker, 2002). 6.1 Propositional attitudes The ATTITUDE programming environment is so named because it utilises propositional attitude instructions as programming instructions (this has been dubbed attitude programming). Propositional attitudes are alleged mental states characterised by propositional attitude expressions, which are the means by which individuals relate their own mental behaviour to others&apos;. Propositional attitude instructions are of the form shown in (1). (1) [subject][attitude][propositional expression] In (1): - [su</context>
</contexts>
<marker>Wright, 1998</marker>
<rawString>Helen Wright. 1998. &amp;quot;Automatic utterance type detection using suprasegmental features&amp;quot;. Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP&apos;98). Sydney.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>