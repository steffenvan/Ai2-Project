<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000865">
<title confidence="0.995889">
Microsoft Research Treelet Translation System:
NAACL 2006 Europarl Evaluation
</title>
<author confidence="0.982683">
Arul Menezes, Kristina Toutanova and Chris Quirk
</author>
<affiliation confidence="0.951437">
Microsoft Research
</affiliation>
<address confidence="0.9517445">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.999291">
{arulm,kristout,chrisq}@microsoft.com
</email>
<sectionHeader confidence="0.99252" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999613692307692">
The Microsoft Research translation system is a
syntactically informed phrasal SMT system that
uses a phrase translation model based on
dependency treelets and a global reordering model
based on the source dependency tree. These
models are combined with several other
knowledge sources in a log-linear manner. The
weights of the individual components in the log-
linear model are set by an automatic parameter-
tuning method. We give a brief overview of the
components of the system and discuss our
experience with the Europarl data translating from
English to Spanish.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999933857142857">
The dependency treelet translation system
developed at MSR is a statistical MT system that
takes advantage of linguistic tools, namely a
source language dependency parser, as well as a
word alignment component. [1]
To train a translation system, we require a
sentence-aligned parallel corpus. First the source
side is parsed to obtain dependency trees. Next the
corpus is word-aligned, and the source
dependencies are projected onto the target
sentences using the word alignments. From the
aligned dependency corpus we extract all treelet
translation pairs, and train an order model and a
bi-lexical dependency model.
To translate, we parse the input sentence, and
employ a decoder to find a combination and
ordering of treelet translation pairs that cover the
source tree and are optimal according to a set of
models. In a now-common generalization of the
classic noisy-channel framework, we use a log-
linear combination of models [2], as in below:
</bodyText>
<equation confidence="0.7132715">
translation(S , F , Λ) = argmaxf ∑ λ f f (S , T )
T If∈F
</equation>
<bodyText confidence="0.999789">
Such an approach toward translation scoring has
proven very effective in practice, as it allows a
translation system to incorporate information from
a variety of probabilistic or non-probabilistic
sources. The weights A = { λf } are selected by
discriminatively training against held out data.
</bodyText>
<sectionHeader confidence="0.966778" genericHeader="method">
2. System Details
</sectionHeader>
<bodyText confidence="0.976948814814815">
A brief word on notation: s and t represent source
and target lexical nodes; S and T represent source
and target trees; s and t represent source and target
treelets (connected subgraphs of the dependency
tree). The expression VtE T refers to all the
lexical items in the target language tree T and |T|
refers to the count of lexical items in T. We use
subscripts to indicate selected words: Tn represents
th
the nlexical item in an in-order traversal of T.
2.1.Training
We use the broad coverage dependency parser
NLPWIN [3] to obtain source language
dependency trees, and we use GIZA++ [4] to
produce word alignments. The GIZA++ training
regimen and parameters are tuned to optimize
BLEU [5] scores on held-out data. Using the word
alignments, we follow a set of dependency tree
projection heuristics [1] to construct target
dependency trees, producing a word-aligned
parallel dependency tree corpus. Treelet
translation pairs are extracted by enumerating all
source treelets (to a maximum size) aligned to a
target treelet.
2.2.Decoding
We use a tree-based decoder, inspired by dynamic
programming. It searches for an approximation of
</bodyText>
<page confidence="0.954371">
158
</page>
<subsectionHeader confidence="0.5432185">
Proceedings of the Workshop on Statistical Machine Translation, pages 158–161,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.587824">
we should -1 follow the Rio agenda
</bodyText>
<equation confidence="0.929547">
-2 -2 -1 +1
hemos -1 de +1 cumplir el -1 programa +1 de -1 Río +1
</equation>
<bodyText confidence="0.859098666666667">
the n-best translations of each subtree of the input
dependency tree. Translation candidates are
composed from treelet translation pairs extracted
from the training corpus. This process is described
in more detail in [1].
2.3.Models
</bodyText>
<subsubsectionHeader confidence="0.907873">
2.3.1. Channel models
</subsubsectionHeader>
<bodyText confidence="0.999591714285714">
We employ several channel models: a direct
maximum likelihood estimate of the probability of
target given source, as well as an estimate of
source given target and target given source using
the word-based IBM Model 1 [6]. For MLE, we
use absolute discounting to smooth the
probabilities:
</bodyText>
<equation confidence="0.8271895">
PMLEt∣s= cs ,t −λ
cs,*
</equation>
<bodyText confidence="0.999628625">
Here, c represents the count of instances of the
treelet pair 〈s, t〉 in the training corpus, and λ is
determined empirically.
For Model 1 probabilities we compute the sum
over all possible alignments of the treelet without
normalizing for length. The calculation of source
given target is presented below; target given
source is calculated symmetrically.
</bodyText>
<equation confidence="0.7524875">
PM1t∣s=∏
t∈t
</equation>
<subsubsectionHeader confidence="0.590443">
2.3.2. Bilingual n-gram channel models
</subsubsectionHeader>
<bodyText confidence="0.9999361875">
Traditional phrasal SMT systems are beset by a
number of theoretical problems, such as the ad hoc
estimation of phrasal probability, the failure to
model the partition probability, and the tenuous
connection between the phrases and the
underlying word-based alignment model. In
string-based SMT systems, these problems are
outweighed by the key role played by phrases in
capturing “local” order. In the absence of good
global ordering models, this has led to an
inexorable push towards longer and longer
phrases, resulting in serious practical problems of
scale, without, in the end, obviating the need for a
real global ordering story.
In [13] we discuss these issues in greater detail
and also present our approach to this problem.
Briefly, we take as our basic unit the Minimal
Translation Unit (MTU) which we define as a set
of source and target word pairs such that there are
no word alignment links between distinct MTUs,
and no smaller MTUs can be extracted without
violating the previous constraint. In other words,
these are the minimal non-compositional phrases.
We then build models based on n-grams of MTUs
in source string, target string and source
dependency tree order. These bilingual n-gram
models in combination with our global ordering
model allow us to use shorter phrases without any
loss in quality, or alternately to improve quality
while keeping phrase size constant.
As an example, consider the aligned sentence
pair in Figure 1. There are seven MTUs:
</bodyText>
<equation confidence="0.999800571428571">
m1 = &lt;we should / hemos&gt;
m2 = &lt;NULL / de&gt;
m3 = &lt;follow / cumplir&gt;
m4 = &lt;the / el&gt;
m5 = &lt;Rio / Rio&gt;
m6 = &lt;agenda / programa&gt;
m7 = &lt;NULL / de&gt;
</equation>
<bodyText confidence="0.999953125">
We can then predict the probability of each MTU
in the context of (a) the previous MTUs in source
order, (b) the previous MTUs in target order, or
(c) the ancestor MTUs in the tree. We consider all
of these traversal orders, each acting as a separate
feature function in the log linear combination. For
source and target traversal order we use a trigram
model, and a bigram model for tree order.
</bodyText>
<subsubsectionHeader confidence="0.907784">
2.3.3. Target language models
</subsubsectionHeader>
<bodyText confidence="0.997699">
We use both a surface level trigram language
model and a dependency-based bigram language
model [7], similar to the bilexical dependency
modes used in some English Treebank parsers
(e.g. [8]).
</bodyText>
<equation confidence="0.90529">
Ptrisurf Ti∣Ti−2,Ti−1
PbidepT i∣parentTi
</equation>
<bodyText confidence="0.996086666666667">
Ptrisurf is a Kneser-Ney smoothed trigram language
model trained on the target side of the training
corpus, and Pbilex is a Kneser-Ney smoothed
</bodyText>
<equation confidence="0.966010777777778">
∑
s∈s
Pt∣s
∣T∣
Psurf T =∏
i=1
∣T∣
PbilexT =∏
i=1
</equation>
<bodyText confidence="0.991370333333333">
bigram language model trained on target language
dependencies extracted from the aligned parallel
dependency tree corpus.
</bodyText>
<subsubsectionHeader confidence="0.941731">
2.3.4. Order model
</subsubsectionHeader>
<bodyText confidence="0.9995495">
The order model assigns a probability to the
position (pos) of each target node relative to its
head based on information in both the source and
target trees:
</bodyText>
<equation confidence="0.996634">
P(order (T )IS ,T )=rl
order
tET
</equation>
<bodyText confidence="0.996003111111111">
Here, position is modeled in terms of closeness to
the head in the dependency tree. The closest pre-
modifier of a given head has position -1; the
closest post-modifier has a position 1. Figure 1
shows an example dependency tree pair annotated
with head-relative positions.
We use a small set of features reflecting local
information in the dependency tree to model P(pos
(t,parent(t))  |S, T):
</bodyText>
<listItem confidence="0.994221">
• Lexical items of t and parent(t), the parent of t
in the dependency tree.
• Lexical items of the source nodes aligned to t
and head(t).
• Part-of-speech (&amp;quot;cat&amp;quot;) of the source nodes
aligned to the head and modifier.
• Head-relative position of the source node
aligned to the source modifier.
</listItem>
<bodyText confidence="0.999955448275862">
These features along with the target feature are
gathered from the word-aligned parallel
dependency tree corpus and used to train a
statistical model. In previous versions of the
system, we trained a decision tree model [9]. In
the current version, we explored log-linear
models. In addition to providing a different way of
combining information from multiple features,
log-linear models allow us to model the similarity
among different classes (target positions), which is
advantageous for our task.
We implemented a method for automatic
selection of features and feature conjunctions in
the log-linear model. The method greedily selects
feature conjunction templates that maximize the
accuracy on a development set. Our feature
selection study showed that the part-of-speech
labels of the source nodes aligned to the head and
the modifier and the head-relative position of the
source node corresponding to the modifier were
the most important features. It was useful to
concatenate the part-of-speech of the source head
with every feature. This effectively achieves
learning of separate movement models for each
source head category. Lexical information on the
pairs of head and dependent in the source and
target was also very useful.
To model the similarity among different target
classes and to achieve pooling of data across
similar classes, we added multiple features of the
target position. These features let our model know,
for example, that position -5 looks more like
position -6 than like position 3. We added a
feature “positive”/“negative” which is shared by
all positive/negative positions. We also added a
feature looking at the displacement of a position in
the target from the corresponding position in the
source and features which group the target
positions into bins. These features of the target
position are combined with features of the input.
This model was trained on the provided
parallel corpus. As described in Section 2.1 we
parsed the source sentences, and projected target
dependencies. Each head-modifier pair in the
resulting target trees constituted a training instance
for the order model.
The score computed by the log-linear order
model is used as a single feature in the overall log-
linear combination of models (see Section 1),
whose parameters were optimized using
MaxBLEU [2]. This order model replaced the
decision tree-based model described in [1].
We compared the decision tree model to the
log-linear model on predicting the position of a
modifier using reference parallel sentences,
independent of the full MT system. The decision
tree achieved per decision accuracy of 69%
whereas the log-linear model achieved per
</bodyText>
<equation confidence="0.511649">
1
</equation>
<bodyText confidence="0.981028">
decision accuracy of 79%.
full MT system, however, the new order model
provided a more modest improvement in the
BLEU score of 0.39%.
</bodyText>
<subsubsectionHeader confidence="0.91361">
2.3.5. Other models
</subsubsectionHeader>
<bodyText confidence="0.9996335">
We include two pseudo-models that help balance
certain biases inherent in our other models.
</bodyText>
<listItem confidence="0.998236">
• Treelet count. This feature is a count of
treelets used to construct the candidate. It
acts as a bias toward translations that use a
smaller number of treelets; hence toward
larger sized treelets incorporating more
context.
• Word count. We also include a count of the
words in the target sentence. This feature
</listItem>
<footnote confidence="0.3917045">
1 The per-decision accuracy numbers were obtained on
different (random) splits of training and test data.
</footnote>
<bodyText confidence="0.834947">
P( pos(t , parent (t ))IS ,T )
In the context of the
</bodyText>
<page confidence="0.988326">
160
</page>
<bodyText confidence="0.998044">
helps to offset the bias of the target
language model toward shorter sentences.
</bodyText>
<sectionHeader confidence="0.997187" genericHeader="method">
3. Discussion
</sectionHeader>
<bodyText confidence="0.999836333333333">
We participated in the English to Spanish track,
using the supplied bilingual data only. We used
only the target side of the bilingual corpus for the
target language model, rather than the larger
supplied language model. We did find that
increasing the target language order from 3 to 4
had a noticeable impact on translation quality. It is
likely that a larger target language corpus would
have an impact, but we did not explore this.
</bodyText>
<table confidence="0.988274">
BLEU
Baseline treelet system 27.60
Add bilingual MTU models 28.42
Replace DT order model with log-linear model 28.81
</table>
<tableCaption confidence="0.999961">
Table 1: Results on development set
</tableCaption>
<bodyText confidence="0.99999235">
We found that the addition of bilingual n-gram
based models had a substantial impact on
translation quality. Adding these models raised
BLEU scores about 0.8%, but anecdotal evidence
suggests that human-evaluated quality rose by
much more than the BLEU score difference would
suggest. In general, we felt that in this corpus, due
to the great diversity in translations for the same
source language words and phrases, and given just
one reference translation, BLEU score correlated
rather poorly with human judgments. This was
borne out in the human evaluation of the final test
results. Humans ranked our system first and
second, in-domain and out-of-domain
respectively, even though it was in the middle of a
field of ten systems by BLEU score. Furthermore,
n-gram channel models may provide greater
robustness. While our BLEU score dropped 3.61%
on out-of-domain data, the average BLEU score of
the other nine competing systems dropped 5.11%.
</bodyText>
<sectionHeader confidence="0.99724" genericHeader="method">
4. References
</sectionHeader>
<reference confidence="0.999993270833333">
[1] Quirk, C., Menezes, A., and Cherry, C., &amp;quot;Dependency
Tree Translation: Syntactically Informed Phrasal SMT&amp;quot;,
Proceedings of ACL 2005, Ann Arbor, MI, USA, 2005.
[2] Och, F. J., and Ney, H., &amp;quot;Discriminative Training and
Maximum Entropy Models for Statistical Machine
Translation&amp;quot;, Proceedings of ACL 2002, Philadelphia,
PA, USA, 2002.
[3] Heidorn, G., “Intelligent writing assistance”, in Dale et
al. Handbook of Natural Language Processing, Marcel
Dekker, 2000.
[4] Och, F. J., and Ney H., &amp;quot;A Systematic Comparison of
Various Statistical Alignment Models&amp;quot;, Computational
Linguistics, 29(1):19-51, March 2003.
[5] Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.,
&amp;quot;BLEU: a method for automatic evaluation of machine
translation&amp;quot;, Proceedings of ACL 2002, Philadelphia,
PA, USA, 2002.
[6] Brown, P. F., Della Pietra, S., Della Pietra, V. J., and
Mercer, R. L., &amp;quot;The Mathematics of Statistical Machine
Translation: Parameter Estimation&amp;quot;, Computational
Linguistics 19(2): 263-311, 1994.
[7] Aue, A., Menezes, A., Moore, R., Quirk, C., and
Ringger, E., &amp;quot;Statistical Machine Translation Using
Labeled Semantic Dependency Graphs.&amp;quot; Proceedings of
TMI 2004, Baltimore, MD, USA, 2004.
[8] Collins, M., &amp;quot;Three generative, lexicalised models for
statistical parsing&amp;quot;, Proceedings of ACL 1997, Madrid,
Spain, 1997.
[9] Chickering, D.M., &amp;quot;The WinMine Toolkit&amp;quot;, Microsoft
Research Technical Report MSR-TR-2002-103,
Redmond, WA, USA, 2002.
[10] Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D.,
Eng, K., Jain, V., Jin, Z., and Radev, D., &amp;quot;A
Smorgasbord of Features for Statistical Machine
Translation&amp;quot;. Proceedings of HLT/NAACL 2004, Boston,
MA, USA, 2004.
[11] Bender, O., Zens, R., Matsuov, E. and Ney, H.,
&amp;quot;Alignment Templates: the RWTH SMT System&amp;quot;.
IWSLT Workshop at INTERSPEECH 2004, Jeju Island,
Korea, 2004.
[12] Och, F. J., &amp;quot;Minimum Error Rate Training for Statistical
Machine Translation&amp;quot;, Proceedings of ACL 2003,
Sapporo, Japan, 2003.
[13] Quirk, C and Menezes, A, “Do we need phrases?
Challenging the conventional wisdom in Statistical
Machine Translation”, Proceedings of HLT/NAACL
2006, New York, NY, USA, 2006
</reference>
<page confidence="0.998235">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.376776">
<title confidence="0.99233">Microsoft Research Treelet Translation NAACL 2006 Europarl Evaluation</title>
<author confidence="0.998559">Arul Menezes</author>
<author confidence="0.998559">Kristina Toutanova</author>
<author confidence="0.998559">Chris Quirk</author>
<affiliation confidence="0.964529">Microsoft</affiliation>
<address confidence="0.8917645">One Microsoft Redmond, WA 98052</address>
<email confidence="0.999656">arulm@microsoft.com</email>
<email confidence="0.999656">kristout@microsoft.com</email>
<email confidence="0.999656">chrisq@microsoft.com</email>
<abstract confidence="0.998119846153846">The Microsoft Research translation system is a syntactically informed phrasal SMT system that uses a phrase translation model based on dependency treelets and a global reordering model based on the source dependency tree. These models are combined with several other knowledge sources in a log-linear manner. The weights of the individual components in the loglinear model are set by an automatic parametertuning method. We give a brief overview of the components of the system and discuss our experience with the Europarl data translating from</abstract>
<note confidence="0.523354">English to Spanish.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency Tree Translation: Syntactically Informed Phrasal SMT&amp;quot;,</title>
<date>2005</date>
<booktitle>Proceedings of ACL 2005,</booktitle>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="1024" citStr="[1]" startWordPosition="149" endWordPosition="149">n the source dependency tree. These models are combined with several other knowledge sources in a log-linear manner. The weights of the individual components in the loglinear model are set by an automatic parametertuning method. We give a brief overview of the components of the system and discuss our experience with the Europarl data translating from English to Spanish. 1. Introduction The dependency treelet translation system developed at MSR is a statistical MT system that takes advantage of linguistic tools, namely a source language dependency parser, as well as a word alignment component. [1] To train a translation system, we require a sentence-aligned parallel corpus. First the source side is parsed to obtain dependency trees. Next the corpus is word-aligned, and the source dependencies are projected onto the target sentences using the word alignments. From the aligned dependency corpus we extract all treelet translation pairs, and train an order model and a bi-lexical dependency model. To translate, we parse the input sentence, and employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of mod</context>
<context position="2933" citStr="[1]" startWordPosition="468" endWordPosition="468">y tree). The expression VtE T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the nlexical item in an in-order traversal of T. 2.1.Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2.Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of 158 Proceedings of the Workshop on Statistical Machine Translation, pages 158–161, New York City, June 2006. c�2006 Association for Computational Linguistics we should -1 follow the Rio agenda -2 -2 -1 +1 hemos -1 de +1 cumplir el -1 programa +1 de -1 Río +1 the n-bes</context>
<context position="10397" citStr="[1]" startWordPosition="1675" endWordPosition="1675">. These features of the target position are combined with features of the input. This model was trained on the provided parallel corpus. As described in Section 2.1 we parsed the source sentences, and projected target dependencies. Each head-modifier pair in the resulting target trees constituted a training instance for the order model. The score computed by the log-linear order model is used as a single feature in the overall loglinear combination of models (see Section 1), whose parameters were optimized using MaxBLEU [2]. This order model replaced the decision tree-based model described in [1]. We compared the decision tree model to the log-linear model on predicting the position of a modifier using reference parallel sentences, independent of the full MT system. The decision tree achieved per decision accuracy of 69% whereas the log-linear model achieved per 1 decision accuracy of 79%. full MT system, however, the new order model provided a more modest improvement in the BLEU score of 0.39%. 2.3.5. Other models We include two pseudo-models that help balance certain biases inherent in our other models. • Treelet count. This feature is a count of treelets used to construct the candi</context>
</contexts>
<marker>[1]</marker>
<rawString>Quirk, C., Menezes, A., and Cherry, C., &amp;quot;Dependency Tree Translation: Syntactically Informed Phrasal SMT&amp;quot;, Proceedings of ACL 2005, Ann Arbor, MI, USA, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation&amp;quot;,</title>
<date>2002</date>
<booktitle>Proceedings of ACL 2002,</booktitle>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="1744" citStr="[2]" startWordPosition="263" endWordPosition="263">ain dependency trees. Next the corpus is word-aligned, and the source dependencies are projected onto the target sentences using the word alignments. From the aligned dependency corpus we extract all treelet translation pairs, and train an order model and a bi-lexical dependency model. To translate, we parse the input sentence, and employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of models. In a now-common generalization of the classic noisy-channel framework, we use a loglinear combination of models [2], as in below: translation(S , F , Λ) = argmaxf ∑ λ f f (S , T ) T If∈F Such an approach toward translation scoring has proven very effective in practice, as it allows a translation system to incorporate information from a variety of probabilistic or non-probabilistic sources. The weights A = { λf } are selected by discriminatively training against held out data. 2. System Details A brief word on notation: s and t represent source and target lexical nodes; S and T represent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The e</context>
<context position="10323" citStr="[2]" startWordPosition="1664" endWordPosition="1664">tion in the source and features which group the target positions into bins. These features of the target position are combined with features of the input. This model was trained on the provided parallel corpus. As described in Section 2.1 we parsed the source sentences, and projected target dependencies. Each head-modifier pair in the resulting target trees constituted a training instance for the order model. The score computed by the log-linear order model is used as a single feature in the overall loglinear combination of models (see Section 1), whose parameters were optimized using MaxBLEU [2]. This order model replaced the decision tree-based model described in [1]. We compared the decision tree model to the log-linear model on predicting the position of a modifier using reference parallel sentences, independent of the full MT system. The decision tree achieved per decision accuracy of 69% whereas the log-linear model achieved per 1 decision accuracy of 79%. full MT system, however, the new order model provided a more modest improvement in the BLEU score of 0.39%. 2.3.5. Other models We include two pseudo-models that help balance certain biases inherent in our other models. • Tree</context>
</contexts>
<marker>[2]</marker>
<rawString>Och, F. J., and Ney, H., &amp;quot;Discriminative Training and Maximum Entropy Models for Statistical Machine Translation&amp;quot;, Proceedings of ACL 2002, Philadelphia, PA, USA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heidorn</author>
</authors>
<title>Intelligent writing assistance”,</title>
<date>2000</date>
<booktitle>in Dale et al. Handbook of Natural Language Processing,</booktitle>
<publisher>Marcel Dekker,</publisher>
<contexts>
<context position="2652" citStr="[3]" startWordPosition="423" endWordPosition="423">selected by discriminatively training against held out data. 2. System Details A brief word on notation: s and t represent source and target lexical nodes; S and T represent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The expression VtE T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the nlexical item in an in-order traversal of T. 2.1.Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2.Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an appr</context>
</contexts>
<marker>[3]</marker>
<rawString>Heidorn, G., “Intelligent writing assistance”, in Dale et al. Handbook of Natural Language Processing, Marcel Dekker, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models&amp;quot;,</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="2718" citStr="[4]" startWordPosition="434" endWordPosition="434">stem Details A brief word on notation: s and t represent source and target lexical nodes; S and T represent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The expression VtE T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the nlexical item in an in-order traversal of T. 2.1.Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2.Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of 158 Proceedings of the Workshop on Statistical Machin</context>
</contexts>
<marker>[4]</marker>
<rawString>Och, F. J., and Ney H., &amp;quot;A Systematic Comparison of Various Statistical Alignment Models&amp;quot;, Computational Linguistics, 29(1):19-51, March 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation&amp;quot;,</title>
<date>2002</date>
<booktitle>Proceedings of ACL 2002,</booktitle>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="2820" citStr="[5]" startWordPosition="450" endWordPosition="450">esent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The expression VtE T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the nlexical item in an in-order traversal of T. 2.1.Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2.Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of 158 Proceedings of the Workshop on Statistical Machine Translation, pages 158–161, New York City, June 2006. c�2006 Association for Computational Linguisti</context>
</contexts>
<marker>[5]</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J., &amp;quot;BLEU: a method for automatic evaluation of machine translation&amp;quot;, Proceedings of ACL 2002, Philadelphia, PA, USA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V J</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation&amp;quot;,</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="3999" citStr="[6]" startWordPosition="634" endWordPosition="634"> Computational Linguistics we should -1 follow the Rio agenda -2 -2 -1 +1 hemos -1 de +1 cumplir el -1 programa +1 de -1 Río +1 the n-best translations of each subtree of the input dependency tree. Translation candidates are composed from treelet translation pairs extracted from the training corpus. This process is described in more detail in [1]. 2.3.Models 2.3.1. Channel models We employ several channel models: a direct maximum likelihood estimate of the probability of target given source, as well as an estimate of source given target and target given source using the word-based IBM Model 1 [6]. For MLE, we use absolute discounting to smooth the probabilities: PMLEt∣s= cs ,t −λ cs,* Here, c represents the count of instances of the treelet pair 〈s, t〉 in the training corpus, and λ is determined empirically. For Model 1 probabilities we compute the sum over all possible alignments of the treelet without normalizing for length. The calculation of source given target is presented below; target given source is calculated symmetrically. PM1t∣s=∏ t∈t 2.3.2. Bilingual n-gram channel models Traditional phrasal SMT systems are beset by a number of theoretical problems, such as the ad </context>
</contexts>
<marker>[6]</marker>
<rawString>Brown, P. F., Della Pietra, S., Della Pietra, V. J., and Mercer, R. L., &amp;quot;The Mathematics of Statistical Machine Translation: Parameter Estimation&amp;quot;, Computational Linguistics 19(2): 263-311, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aue</author>
<author>A Menezes</author>
<author>R Moore</author>
<author>C Quirk</author>
<author>E Ringger</author>
</authors>
<title>Statistical Machine Translation Using Labeled Semantic Dependency Graphs.&amp;quot;</title>
<date>2004</date>
<booktitle>Proceedings of TMI 2004,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="6648" citStr="[7]" startWordPosition="1079" endWordPosition="1079">r&gt; m4 = &lt;the / el&gt; m5 = &lt;Rio / Rio&gt; m6 = &lt;agenda / programa&gt; m7 = &lt;NULL / de&gt; We can then predict the probability of each MTU in the context of (a) the previous MTUs in source order, (b) the previous MTUs in target order, or (c) the ancestor MTUs in the tree. We consider all of these traversal orders, each acting as a separate feature function in the log linear combination. For source and target traversal order we use a trigram model, and a bigram model for tree order. 2.3.3. Target language models We use both a surface level trigram language model and a dependency-based bigram language model [7], similar to the bilexical dependency modes used in some English Treebank parsers (e.g. [8]). Ptrisurf Ti∣Ti−2,Ti−1 PbidepT i∣parentTi Ptrisurf is a Kneser-Ney smoothed trigram language model trained on the target side of the training corpus, and Pbilex is a Kneser-Ney smoothed ∑ s∈s Pt∣s ∣T∣ Psurf T =∏ i=1 ∣T∣ PbilexT =∏ i=1 bigram language model trained on target language dependencies extracted from the aligned parallel dependency tree corpus. 2.3.4. Order model The order model assigns a probability to the position (pos) of each target node relative to its head based on informati</context>
</contexts>
<marker>[7]</marker>
<rawString>Aue, A., Menezes, A., Moore, R., Quirk, C., and Ringger, E., &amp;quot;Statistical Machine Translation Using Labeled Semantic Dependency Graphs.&amp;quot; Proceedings of TMI 2004, Baltimore, MD, USA, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing&amp;quot;,</title>
<date>1997</date>
<booktitle>Proceedings of ACL 1997,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="6739" citStr="[8]" startWordPosition="1093" endWordPosition="1093">redict the probability of each MTU in the context of (a) the previous MTUs in source order, (b) the previous MTUs in target order, or (c) the ancestor MTUs in the tree. We consider all of these traversal orders, each acting as a separate feature function in the log linear combination. For source and target traversal order we use a trigram model, and a bigram model for tree order. 2.3.3. Target language models We use both a surface level trigram language model and a dependency-based bigram language model [7], similar to the bilexical dependency modes used in some English Treebank parsers (e.g. [8]). Ptrisurf Ti∣Ti−2,Ti−1 PbidepT i∣parentTi Ptrisurf is a Kneser-Ney smoothed trigram language model trained on the target side of the training corpus, and Pbilex is a Kneser-Ney smoothed ∑ s∈s Pt∣s ∣T∣ Psurf T =∏ i=1 ∣T∣ PbilexT =∏ i=1 bigram language model trained on target language dependencies extracted from the aligned parallel dependency tree corpus. 2.3.4. Order model The order model assigns a probability to the position (pos) of each target node relative to its head based on information in both the source and target trees: P(order (T )IS ,T )=rl order tET Here, position is </context>
</contexts>
<marker>[8]</marker>
<rawString>Collins, M., &amp;quot;Three generative, lexicalised models for statistical parsing&amp;quot;, Proceedings of ACL 1997, Madrid, Spain, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
</authors>
<title>The WinMine Toolkit&amp;quot;,</title>
<date>2002</date>
<tech>Microsoft Research Technical Report MSR-TR-2002-103,</tech>
<location>Redmond, WA, USA,</location>
<contexts>
<context position="8229" citStr="[9]" startWordPosition="1339" endWordPosition="1339">flecting local information in the dependency tree to model P(pos (t,parent(t)) |S, T): • Lexical items of t and parent(t), the parent of t in the dependency tree. • Lexical items of the source nodes aligned to t and head(t). • Part-of-speech (&amp;quot;cat&amp;quot;) of the source nodes aligned to the head and modifier. • Head-relative position of the source node aligned to the source modifier. These features along with the target feature are gathered from the word-aligned parallel dependency tree corpus and used to train a statistical model. In previous versions of the system, we trained a decision tree model [9]. In the current version, we explored log-linear models. In addition to providing a different way of combining information from multiple features, log-linear models allow us to model the similarity among different classes (target positions), which is advantageous for our task. We implemented a method for automatic selection of features and feature conjunctions in the log-linear model. The method greedily selects feature conjunction templates that maximize the accuracy on a development set. Our feature selection study showed that the part-of-speech labels of the source nodes aligned to the head</context>
</contexts>
<marker>[9]</marker>
<rawString>Chickering, D.M., &amp;quot;The WinMine Toolkit&amp;quot;, Microsoft Research Technical Report MSR-TR-2002-103, Redmond, WA, USA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A Smorgasbord of Features for Statistical Machine Translation&amp;quot;.</title>
<date>2004</date>
<booktitle>Proceedings of HLT/NAACL 2004,</booktitle>
<location>Boston, MA, USA,</location>
<marker>[10]</marker>
<rawString>Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A., Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D., Eng, K., Jain, V., Jin, Z., and Radev, D., &amp;quot;A Smorgasbord of Features for Statistical Machine Translation&amp;quot;. Proceedings of HLT/NAACL 2004, Boston, MA, USA, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bender</author>
<author>R Zens</author>
<author>E Matsuov</author>
<author>H Ney</author>
</authors>
<title>Alignment Templates: the RWTH SMT System&amp;quot;.</title>
<date>2004</date>
<booktitle>IWSLT Workshop at INTERSPEECH 2004,</booktitle>
<location>Jeju Island,</location>
<marker>[11]</marker>
<rawString>Bender, O., Zens, R., Matsuov, E. and Ney, H., &amp;quot;Alignment Templates: the RWTH SMT System&amp;quot;. IWSLT Workshop at INTERSPEECH 2004, Jeju Island, Korea, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation&amp;quot;,</title>
<date>2003</date>
<booktitle>Proceedings of ACL 2003,</booktitle>
<location>Sapporo, Japan,</location>
<marker>[12]</marker>
<rawString>Och, F. J., &amp;quot;Minimum Error Rate Training for Statistical Machine Translation&amp;quot;, Proceedings of ACL 2003, Sapporo, Japan, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
</authors>
<title>Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation”,</title>
<date>2006</date>
<booktitle>Proceedings of HLT/NAACL 2006,</booktitle>
<location>New York, NY, USA,</location>
<contexts>
<context position="5149" citStr="[13]" startWordPosition="814" endWordPosition="814">t by a number of theoretical problems, such as the ad hoc estimation of phrasal probability, the failure to model the partition probability, and the tenuous connection between the phrases and the underlying word-based alignment model. In string-based SMT systems, these problems are outweighed by the key role played by phrases in capturing “local” order. In the absence of good global ordering models, this has led to an inexorable push towards longer and longer phrases, resulting in serious practical problems of scale, without, in the end, obviating the need for a real global ordering story. In [13] we discuss these issues in greater detail and also present our approach to this problem. Briefly, we take as our basic unit the Minimal Translation Unit (MTU) which we define as a set of source and target word pairs such that there are no word alignment links between distinct MTUs, and no smaller MTUs can be extracted without violating the previous constraint. In other words, these are the minimal non-compositional phrases. We then build models based on n-grams of MTUs in source string, target string and source dependency tree order. These bilingual n-gram models in combination with our globa</context>
</contexts>
<marker>[13]</marker>
<rawString>Quirk, C and Menezes, A, “Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation”, Proceedings of HLT/NAACL 2006, New York, NY, USA, 2006</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>