<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.542828666666667">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.977606">
Context Clustering for Word Sense Disambiguation Based on
Modeling Pairwise Context Similarities
</title>
<author confidence="0.979912">
Cheng Niu, Wei Li, Rohini K. Srihari, Huifeng Li, Laurie Crist
</author>
<affiliation confidence="0.922274">
Cymfony Inc.
</affiliation>
<address confidence="0.962911">
600 Essjay Road, Williamsville, NY 14221. USA.
</address>
<email confidence="0.983523">
{cniu, wei, rohini, hli, lcrist}@cymfony.com
</email>
<sectionHeader confidence="0.992375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998595">
Traditionally, word sense disambiguation
(WSD) involves a different context model for
each individual word. This paper presents a
new approach to WSD using weakly
supervised learning. Statistical models are not
trained for the contexts of each individual
word, but for the similarities between context
pairs at category level. The insight is that the
correlation regularity between the sense
distinction and the context distinction can be
captured at category level, independent of
individual words. This approach only requires
a limited amount of existing annotated training
corpus in order to disambiguate the entire
vocabulary. A context clustering scheme is
developed within the Bayesian framework. A
maximum entropy model is then trained to
represent the generative probability
distribution of context similarities based on
heterogeneous features, including trigger
words and parsing structures. Statistical
annealing is applied to derive the final context
clusters by globally fitting the pairwise
context similarity distribution. Benchmarking
shows that this new approach significantly
outperforms the existing WSD systems in the
unsupervised category, and rivals supervised
WSD systems.
</bodyText>
<sectionHeader confidence="0.99908" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99940246969697">
Word Sense Disambiguation (WSD) is one of the
central problems in Natural Language Processing.
The difficulty of this task lies in the fact that
context features and the corresponding statistical
distribution are different for each individual word.
Traditionally, WSD involves modeling the
contexts for each word. [Gale et al. 1992] uses the
Naïve Bayes method for context modeling which
requires a manually truthed corpus for each
ambiguous word. This causes a serious Knowledge
Bottleneck. The situation is worse when
considering the domain dependency of word
senses. To avoid the Knowledge Bottleneck,
unsupervised or weakly supervised learning
approaches have been proposed. These include the
bootstrapping approach [Yarowsky 1995] and the
context clustering approach [Schutze 1998].
Although the above unsupervised or weakly
supervised learning approaches are less subject to
the Knowledge Bottleneck, some weakness exists:
i) for each individual keyword, the sense number
has to be provided and in the bootstrapping case,
seeds for each sense are also required; ii) the
modeling usually assumes some form of evidence
independency, e.g. the vector space model used in
[Schutze 1998] and [Niu et al. 2003]: this limits the
performance and its potential enhancement; iii)
most WSD systems either use selectional
restriction in parsing relations, and/or trigger
words which co-occur within a window size of the
ambiguous word. We previously at-tempted
combining both types of evidence but only
achieved limited improvement due to the lack of a
proper modeling of information over-lapping [Niu
et al. 2003].
This paper presents a new algorithm that
addresses these problems. A novel context
clustering scheme based on modeling the
similarities between pairwise contexts at category
level is presented in the Bayesian framework. A
generative maximum entropy model is then trained
to represent the generative probability distribution
of pairwise context similarities based on
heterogeneous features that cover both co-
occurring words and parsing structures. Statistical
annealing is used to derive the final context
clusters by globally fitting the pairwise context
similarities.
This new algorithm only requires a limited
amount of existing annotated corpus to train the
generative maximum entropy model for the entire
vocabulary. This capability is based on the
observation that a system does not necessarily
require training data for word A in order to
disambiguate A. The insight is that the correlation
regularity between the sense distinction and the
context distinction can be captured at category
level, independent of individual words.
In what follows, Section 2 formulates WSD as a
context clustering task based on the pairwise
context similarity model. The context clustering
algorithm is described in Sections 3 and 4,
corresponding to the two key aspects of the
algorithm, i.e. the generative maximum entropy
modeling and the annealing-based optimization.
Section 5 describes benchmarks and conclusion.
</bodyText>
<sectionHeader confidence="0.699863" genericHeader="method">
2 Task Definition and Algorithm Design
</sectionHeader>
<bodyText confidence="0.976159653846154">
Given n mentions of a key word, we first
introduce the following symbols. Ci refers to the
i -th context. Si refers to the sense of the i -th
context. CSi,j refers to the context similarity
between the i -th context and the j -th context,
which is a subset of the predefined context
similarity features. fα refers to the α -th
predefined context similarity feature. So CSi,j
takes the form of {f α }.
The WSD task is defined as the hard clustering
of multiple contexts of the key word. Its final
solution is represented as {K,M } where K refers
to the number of distinct senses, and M represents
the many-to-one mapping (from contexts to a
cluster) such that M(i )= j, i ∈ [1,n], j ∈ [1, K].
For any given context pair, a set of context
similarity features are defined. With n mentions of
n(n − 1)
the same key word, context similarities
2CS i,j (i ∈ [1, n,]j ∈ 1,[i ) )are computed. The WSD task
is formulated as searching for {K,M } which
maximizes the following conditional probability:
( { K ,M}{CSi,j} ) ( i ∈[ 1,n, ] j ∈ 1,[i) )
Based on Bayesian Equity, this is equivalent to
maximizing the joint probability in Eq. (1), which
contains a prior probability distribution of WSD,
</bodyText>
<equation confidence="0.991649">
Pr({K,M }.)
Pr (k,MQ{CSi,j}[ 0∈qn,5∈ O))
( CS { K M } ) ( { K M })
, Pr ,
ij,
N
−1
</equation>
<bodyText confidence="0.9972218">
Because there is no prior knowledge available
about what solution is preferred, it is reasonable to
take an equal distribution as the prior probability
distribution. So WSD is equivalent to searching for
{K, M } which maximizes Expression (2).
</bodyText>
<equation confidence="0.990462333333333">
j = −
1 , 1
i
</equation>
<bodyText confidence="0.877395">
where
</bodyText>
<equation confidence="0.98690704">
( { } ) ( CS S S ) ( ) ( )
=
, if M iM j
�� � Pr =
ij i
, j
CS K M
,
i j
, =
Pr
�� ( CS S S
≠ , otherwise
)
ij i
, j
Si
and Pr
Si
(CSi,j|
=Sj)
(CSi,j|
≠Sj) in Eq. (3), a
given
M
</equation>
<bodyText confidence="0.971286">
candidate, we can compute the
conditional probability of Expression (2). In the
final step, optimization is performed to search for
{K,
</bodyText>
<equation confidence="0.666729333333333">
}
{K, M } that maximizes the value of Expression
(2).
</equation>
<bodyText confidence="0.983283666666667">
similarity features, an
d how to estimate the
generative probabilities of context similarity
</bodyText>
<subsectionHeader confidence="0.451141">
Pr(CSi,j Si =S j ) and Pr(CSi,j Si ≠S j )
</subsectionHeader>
<bodyText confidence="0.9164914">
Using the Senseval-2 training
we have
constructed Corpus I and Corpus II for each Part-
of-speech (POS) tag. Corpus I is constructed using
context pairs involving the same sense of a word.
Corpus II is constructed using context pairs that
refer to different senses of a word. Each corpus
contains about 18,000 context pairs. The instances
in the corpora are represented as pairwise context
similarities, taking the form of
</bodyText>
<figure confidence="0.857708333333333">
The two
conditional probabilities
Si
and
Pr
Si
</figure>
<figureCaption confidence="0.315933">
can be represented as
</figureCaption>
<bodyText confidence="0.8106364">
and
which are
generative probabilities by maximum entropy for
Corpus I and Corpus II.
We now present how to compute the context
similarities. Each context contains the following
two categories of features:
i) Trigger words centering around the key word
within a predefined window size equal to 50
tokens to both sides of the key word. Trigger
</bodyText>
<equation confidence="0.983182454545454">
words are learn
corpus,1
{fα}.
Pr(CSi,j
=Sj)
(CSi,j
≠Sj)
PrmaxEnt ( { } )
I fα
PrmaxEnt ( { } )
II fα
</equation>
<bodyText confidence="0.856846">
ed using the same technique as
in [Niu et al. 2003].
ii) Parsing relationships associated with the key
word automatically decoded by our parser
</bodyText>
<equation confidence="0.8826155">
=Pr4CSij Ek,M 4r(k,M }) (1)
(3)
</equation>
<bodyText confidence="0.935578888888889">
To learn the conditional probabilities
Pr
maximum entropy model is trained. There are two
major advantages of this maximum entropy model:
i) the model is independent of individual words; ii)
the model takes no information independence
assumption about the data, and hence is powerful
enough to utilize heterogeneous features. With the
learned conditional probabilities in Eq. (3), for a
</bodyText>
<sectionHeader confidence="0.992023" genericHeader="method">
3 Maximum Entropy Modeling
</sectionHeader>
<bodyText confidence="0.630109666666667">
This section presents the definition of context
using
maximum entropy modeling.
</bodyText>
<footnote confidence="0.9687">
1 Note that the words that appear in the Senseval-3
lexical sample evaluation are removed in the corpus
construction process.
</footnote>
<equation confidence="0.575271285714286">
∏ ( { } )
Pr , ,
CS i j K M
N
(2)
,
=1
i
Pr
=i
,
=1
∏ Pr
,
=1
i
j
Pr
InfoXtract [Srihari et al. 2003]. The
relationships being utilized are listed below.
Noun: subject-of, object-of, complement-of,
has-adjective-modifier, has-noun-
modifier, modifier-of, possess,
possessed-by, appositive-of
Verb: has-subject, has-object, has-
complement, has-adverb-modifier,
has-prepositional-modifier
Adjective: modifier-of, has-adverb-modifier
</equation>
<bodyText confidence="0.998500333333333">
Based on the above context features, the
following three categories of context similarity
features are defined:
</bodyText>
<listItem confidence="0.788712714285714">
(1) Context similarity based on a vector space
model using co-occurring trigger words: the
trigger words centering around the key word
are represented as a vector, and the tf*idf
scheme is used to weigh each trigger word.
The cosine of the angle between two resulting
vectors is used as a context similarity
measure.
(2) Context similarity based on Latent
semantic analysis (LSA) using trigger words:
LSA [Deerwester et al. 1990] is a technique
used to uncover the underlying semantics
based on co-occurrence data. Using LSA,
each word is represented as a vector in the
semantic space. The trigger words are
represented as a vector summation. Then the
cosine of the angle between the two resulting
vector summations is computed, and used as a
context similarity measure.
(3) LSA-based Parsing Structure Similarity:
each relationship is in the form of Ro(w).
</listItem>
<bodyText confidence="0.988856869565218">
Using LSA, each word w is represented as
semantic vector V�w .L Then, the similarity
between Ro(w1) and Ro(w2) is represented as
the cosine of angle between V �w1 Land V Ov2 .L
Two special values are assigned to two
exceptional cases: i) when no relationship
Ro is decoded in both contexts; ii) when the
relationship Ro is decoded only for one
context.
To facilitate the maximum entropy modeling in
the later stage, the resulting similarity measure is
discretized into 10 integer values. Now the
pairwise context similarity is a set of similarity
features, e.g.
{VSM-Similairty-equal-to-2, LSA-Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2}.
In addition to the three categories of basic
context similarity features defined above, we also
define induced context similarity features by
combining basic context similarity features using
the logical AND operator. With induced features,
the context similarity vector in the previous
example is represented as
{VSM-Similairty-equal-to-2, LSA- Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2,
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1], [VSM-Similairty-
equal-to-2 and LSA-Subject-Similarity-equal-to-
2],
❑❑❑,
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1 and LSA-Subject-
Similarity-equal-to-2]}.
The induced features provide direct and fine-
grained information, but suffer from less sampling
space. To make the computation feasible, we
regulate 3 as the maximum number of logical AND
in the induced features. Combining basic features
and induced features under a smoothing scheme,
maximum entropy modeling may achieve optimal
performance.
Now the maximum entropy modeling can be
formulated as follows: given a pairwise context
similarity { fo} , the generative probability of
</bodyText>
<equation confidence="0.7357995">
{ fo} in Corpus I or Corpus II is given as
Pr maxEnt UA Di Z ❑ wf (4)
</equation>
<bodyText confidence="0.900124684210526">
where Z is the normalization factor, wf is the
weight associated with feature f . The Iterative
Scaling algorithm combined with Monte Carlo
simulation [Pietra, Pietra, &amp; Lafferty 1995] is used
to train the weights in this generative model.
Unlike the commonly used conditional maximum
entropy modeling which approximates the feature
configuration space as the training corpus
[Ratnaparkhi 1998], Monte Carlo techniques are
required in the generative modeling to simulate the
possible feature configurations. The exponential
prior smoothing scheme [Goodman 2003] is
adopted. The same training procedure is performed
using Corpus I and Corpus II to estimate
Pr maxEnt � { } L
I fi and Pr maxEnt � { } L
II fi respectively.
f 0 D �
fo
</bodyText>
<sectionHeader confidence="0.98108" genericHeader="method">
4 Statistical Annealing
</sectionHeader>
<bodyText confidence="0.9206637">
With the maximum entropy modeling presented
above, the WSD task is performed as follows: i)
for a given set of contexts, the pairwise context
similarity measures are computed; ii) for each
context similarity { fi } , the two generative
probabilities Pr maxEnt ( { } )
I fi and Pr maxEnt ( { } )
II fi are
computed; iii) for a given WSD candidate
solution{K,M }, the conditional probability (2) can
be computed. Optimization based on statistical
annealing (Neal 1993) is used to search for {K,M }
which maximizes Expression (2).
The optimization process consists of two steps.
First, a local optimal solution{K,M }0 is computed
by a greedy algorithm. Then by setting {K,M }0 as
the initial state, statistical annealing is applied to
search for the global optimal solution. To reduce
the search time, we set the maximum value of K
to 5.
</bodyText>
<sectionHeader confidence="0.957724" genericHeader="evaluation">
5 Benchmarking and Conclusion
</sectionHeader>
<bodyText confidence="0.999937965517241">
To enter the Senseval-3 evaluation, we
implemented the following procedure to map the
context clusters to Senseval-3 standards: i) process
the Senseval-3 training corpus and testing corpus
using our parser; ii) for each word to be
benchmarked, retrieve the related contexts from
the corpora and cluster them; iii) Based on 10% of
the sense tags in the Senseval-3 training corpus
(10% data correspond roughly to an average of 2-3
instances for each sense), the context cluster is
mapped onto the most frequent WSD sense
associated with the cluster members. By design,
the context clusters correspond to distinct senses,
therefore, we do not allow multiple context clusters
to be mapped onto one sense. In case multiple
clusters correspond to one sense, only the largest
cluster is retained; iv), each instance in the testing
corpus is tagged with the same sense as the one to
which its context cluster corresponds.
We are not able to compare our performance
with other systems in Senseval-3 because at the
time of writing, the Senseval-3 evaluation results
are not publicly available. As a note, compared
with the Senseval-2 English Lexical Sample
evaluation, the benchmarks of our new algorithm
(Table 1) are significantly above the performance
of the WSD systems in the unsupervised category,
and rival the performance of the supervised WSD
systems.
</bodyText>
<tableCaption confidence="0.998117">
Table 1. Senseval-3 Lexical Sample Evaluation
</tableCaption>
<table confidence="0.999194166666666">
Category Accuracy
Fine grain (%) Coarse grain (%)
Adjective (5) 49.1 64.8
Noun (20) 57.9 66.6
Verb (32) 55.3 66.3
Average 56.3% 66.4%
</table>
<sectionHeader confidence="0.998153" genericHeader="conclusions">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.97719">
This work was supported by the Navy SBIR
program under contract N00178-03-C-1047.
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999695583333333">
Gale, W., K. Church, and D. Yarowsky. 1992. A
Method for Disambiguating Word Senses in a
Large Corpus. Computers and the Humanities,
26.
Yarowsky, D. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods.
In Proceedings of ACL 1995.
Schutze, H. 1998. Automatic Word Sense
Disambiguation. Computational Linguistics, 23.
C. Niu, Zhaohui Zheng, R. Srihari, H. Li, and W.
Li 2003. Unsupervised Learning for Verb Sense
Disambiguation Using Both trigger Words and
Parsing Relations. In Proceeding of PACLING
2003, Halifax, Canada.
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
Latent Semantic Analysis. In Journal of the
American Society of Information Science
Goodman, J. 2003. Exponential Priors for
Maximum Entropy Models.
Neal, R.M. 1993. Probabilistic Inference Using
Markov Chain Monte Carlo Methods. Technical
Report, Univ. of Toronto.
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995.
Inducing Features Of Random Fields. In IEEE
Transactions on Pattern Analysis and Machine
Intelligence.
Adwait Ratnaparkhi. (1998). Maximum Entropy
Models for Natural Language Ambiguity
Resolution. Ph.D. Dissertation. University of
Pennsylvania.
Srihari, R., W. Li, C. Niu and T. Cornell. 2003.
InfoXtract: A Customizable Intermediate Level
Information Extraction Engine. In Proceedings
of HLT/NAACL 2003 Workshop on SEALTS.
Edmonton, Canada.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.300103">
<note confidence="0.7723765">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.75044">Association for Computational Linguistics Context Clustering for Word Sense Disambiguation Based on Modeling Pairwise Context Similarities</title>
<author confidence="0.999314">Cheng Niu</author>
<author confidence="0.999314">Wei Li</author>
<author confidence="0.999314">Rohini K Srihari</author>
<author confidence="0.999314">Huifeng Li</author>
<author confidence="0.999314">Laurie Crist</author>
<affiliation confidence="0.999875">Cymfony Inc.</affiliation>
<address confidence="0.999935">600 Essjay Road, Williamsville, NY 14221. USA.</address>
<email confidence="0.999788">wei,rohini,hli,lcrist}@cymfony.com</email>
<abstract confidence="0.998127275862069">Traditionally, word sense disambiguation (WSD) involves a different context model for each individual word. This paper presents a new approach to WSD using weakly supervised learning. Statistical models are not trained for the contexts of each individual word, but for the similarities between context pairs at category level. The insight is that the correlation regularity between the sense distinction and the context distinction can be captured at category level, independent of individual words. This approach only requires a limited amount of existing annotated training corpus in order to disambiguate the entire vocabulary. A context clustering scheme is developed within the Bayesian framework. A maximum entropy model is then trained to represent the generative probability distribution of context similarities based on heterogeneous features, including trigger words and parsing structures. Statistical annealing is applied to derive the final context clusters by globally fitting the pairwise context similarity distribution. Benchmarking shows that this new approach significantly outperforms the existing WSD systems in the unsupervised category, and rivals supervised WSD systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A Method for Disambiguating Word Senses in a Large Corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26</pages>
<contexts>
<context position="1990" citStr="Gale et al. 1992" startWordPosition="276" endWordPosition="279"> applied to derive the final context clusters by globally fitting the pairwise context similarity distribution. Benchmarking shows that this new approach significantly outperforms the existing WSD systems in the unsupervised category, and rivals supervised WSD systems. 1 Introduction Word Sense Disambiguation (WSD) is one of the central problems in Natural Language Processing. The difficulty of this task lies in the fact that context features and the corresponding statistical distribution are different for each individual word. Traditionally, WSD involves modeling the contexts for each word. [Gale et al. 1992] uses the Naïve Bayes method for context modeling which requires a manually truthed corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The situation is worse when considering the domain dependency of word senses. To avoid the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. Although the above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck, some weakness exists: i)</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W., K. Church, and D. Yarowsky. 1992. A Method for Disambiguating Word Senses in a Large Corpus. Computers and the Humanities, 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2393" citStr="Yarowsky 1995" startWordPosition="335" endWordPosition="336">k lies in the fact that context features and the corresponding statistical distribution are different for each individual word. Traditionally, WSD involves modeling the contexts for each word. [Gale et al. 1992] uses the Naïve Bayes method for context modeling which requires a manually truthed corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The situation is worse when considering the domain dependency of word senses. To avoid the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. Although the above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck, some weakness exists: i) for each individual keyword, the sense number has to be provided and in the bootstrapping case, seeds for each sense are also required; ii) the modeling usually assumes some form of evidence independency, e.g. the vector space model used in [Schutze 1998] and [Niu et al. 2003]: this limits the performance and its potential enhancement; iii) most WSD systems either use selectional restriction in pars</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of ACL 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Automatic Word Sense Disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<contexts>
<context position="2444" citStr="Schutze 1998" startWordPosition="342" endWordPosition="343">responding statistical distribution are different for each individual word. Traditionally, WSD involves modeling the contexts for each word. [Gale et al. 1992] uses the Naïve Bayes method for context modeling which requires a manually truthed corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The situation is worse when considering the domain dependency of word senses. To avoid the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. Although the above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck, some weakness exists: i) for each individual keyword, the sense number has to be provided and in the bootstrapping case, seeds for each sense are also required; ii) the modeling usually assumes some form of evidence independency, e.g. the vector space model used in [Schutze 1998] and [Niu et al. 2003]: this limits the performance and its potential enhancement; iii) most WSD systems either use selectional restriction in parsing relations, and/or trigger words which co-occur </context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Schutze, H. 1998. Automatic Word Sense Disambiguation. Computational Linguistics, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Niu</author>
<author>Zhaohui Zheng</author>
<author>R Srihari</author>
<author>H Li</author>
<author>W Li</author>
</authors>
<title>Unsupervised Learning for Verb Sense Disambiguation Using Both trigger Words and Parsing Relations.</title>
<date>2003</date>
<booktitle>In Proceeding of PACLING 2003,</booktitle>
<location>Halifax, Canada.</location>
<contexts>
<context position="2867" citStr="Niu et al. 2003" startWordPosition="407" endWordPosition="410">Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. Although the above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck, some weakness exists: i) for each individual keyword, the sense number has to be provided and in the bootstrapping case, seeds for each sense are also required; ii) the modeling usually assumes some form of evidence independency, e.g. the vector space model used in [Schutze 1998] and [Niu et al. 2003]: this limits the performance and its potential enhancement; iii) most WSD systems either use selectional restriction in parsing relations, and/or trigger words which co-occur within a window size of the ambiguous word. We previously at-tempted combining both types of evidence but only achieved limited improvement due to the lack of a proper modeling of information over-lapping [Niu et al. 2003]. This paper presents a new algorithm that addresses these problems. A novel context clustering scheme based on modeling the similarities between pairwise contexts at category level is presented in the</context>
<context position="7731" citStr="Niu et al. 2003" startWordPosition="1263" endWordPosition="1266"> pairwise context similarities, taking the form of The two conditional probabilities Si and Pr Si can be represented as and which are generative probabilities by maximum entropy for Corpus I and Corpus II. We now present how to compute the context similarities. Each context contains the following two categories of features: i) Trigger words centering around the key word within a predefined window size equal to 50 tokens to both sides of the key word. Trigger words are learn corpus,1 {fα}. Pr(CSi,j =Sj) (CSi,j ≠Sj) PrmaxEnt ( { } ) I fα PrmaxEnt ( { } ) II fα ed using the same technique as in [Niu et al. 2003]. ii) Parsing relationships associated with the key word automatically decoded by our parser =Pr4CSij Ek,M 4r(k,M }) (1) (3) To learn the conditional probabilities Pr maximum entropy model is trained. There are two major advantages of this maximum entropy model: i) the model is independent of individual words; ii) the model takes no information independence assumption about the data, and hence is powerful enough to utilize heterogeneous features. With the learned conditional probabilities in Eq. (3), for a 3 Maximum Entropy Modeling This section presents the definition of context using maximu</context>
</contexts>
<marker>Niu, Zheng, Srihari, Li, Li, 2003</marker>
<rawString>C. Niu, Zhaohui Zheng, R. Srihari, H. Li, and W. Li 2003. Unsupervised Learning for Verb Sense Disambiguation Using Both trigger Words and Parsing Relations. In Proceeding of PACLING 2003, Halifax, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>In Journal of the American Society of Information Science</journal>
<contexts>
<context position="9445" citStr="Deerwester et al. 1990" startWordPosition="1524" endWordPosition="1527">-modifier, has-prepositional-modifier Adjective: modifier-of, has-adverb-modifier Based on the above context features, the following three categories of context similarity features are defined: (1) Context similarity based on a vector space model using co-occurring trigger words: the trigger words centering around the key word are represented as a vector, and the tf*idf scheme is used to weigh each trigger word. The cosine of the angle between two resulting vectors is used as a context similarity measure. (2) Context similarity based on Latent semantic analysis (LSA) using trigger words: LSA [Deerwester et al. 1990] is a technique used to uncover the underlying semantics based on co-occurrence data. Using LSA, each word is represented as a vector in the semantic space. The trigger words are represented as a vector summation. Then the cosine of the angle between the two resulting vector summations is computed, and used as a context similarity measure. (3) LSA-based Parsing Structure Similarity: each relationship is in the form of Ro(w). Using LSA, each word w is represented as semantic vector V�w .L Then, the similarity between Ro(w1) and Ro(w2) is represented as the cosine of angle between V �w1 Land V </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by Latent Semantic Analysis. In Journal of the American Society of Information Science</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Exponential Priors for Maximum Entropy Models.</title>
<date>2003</date>
<contexts>
<context position="12323" citStr="Goodman 2003" startWordPosition="1938" endWordPosition="1939">or Corpus II is given as Pr maxEnt UA Di Z ❑ wf (4) where Z is the normalization factor, wf is the weight associated with feature f . The Iterative Scaling algorithm combined with Monte Carlo simulation [Pietra, Pietra, &amp; Lafferty 1995] is used to train the weights in this generative model. Unlike the commonly used conditional maximum entropy modeling which approximates the feature configuration space as the training corpus [Ratnaparkhi 1998], Monte Carlo techniques are required in the generative modeling to simulate the possible feature configurations. The exponential prior smoothing scheme [Goodman 2003] is adopted. The same training procedure is performed using Corpus I and Corpus II to estimate Pr maxEnt � { } L I fi and Pr maxEnt � { } L II fi respectively. f 0 D � fo 4 Statistical Annealing With the maximum entropy modeling presented above, the WSD task is performed as follows: i) for a given set of contexts, the pairwise context similarity measures are computed; ii) for each context similarity { fi } , the two generative probabilities Pr maxEnt ( { } ) I fi and Pr maxEnt ( { } ) II fi are computed; iii) for a given WSD candidate solution{K,M }, the conditional probability (2) can be com</context>
</contexts>
<marker>Goodman, 2003</marker>
<rawString>Goodman, J. 2003. Exponential Priors for Maximum Entropy Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
</authors>
<title>Probabilistic Inference Using Markov Chain Monte Carlo Methods.</title>
<date>1993</date>
<tech>Technical Report,</tech>
<institution>Univ. of Toronto.</institution>
<contexts>
<context position="12985" citStr="Neal 1993" startWordPosition="2062" endWordPosition="2063">ed using Corpus I and Corpus II to estimate Pr maxEnt � { } L I fi and Pr maxEnt � { } L II fi respectively. f 0 D � fo 4 Statistical Annealing With the maximum entropy modeling presented above, the WSD task is performed as follows: i) for a given set of contexts, the pairwise context similarity measures are computed; ii) for each context similarity { fi } , the two generative probabilities Pr maxEnt ( { } ) I fi and Pr maxEnt ( { } ) II fi are computed; iii) for a given WSD candidate solution{K,M }, the conditional probability (2) can be computed. Optimization based on statistical annealing (Neal 1993) is used to search for {K,M } which maximizes Expression (2). The optimization process consists of two steps. First, a local optimal solution{K,M }0 is computed by a greedy algorithm. Then by setting {K,M }0 as the initial state, statistical annealing is applied to search for the global optimal solution. To reduce the search time, we set the maximum value of K to 5. 5 Benchmarking and Conclusion To enter the Senseval-3 evaluation, we implemented the following procedure to map the context clusters to Senseval-3 standards: i) process the Senseval-3 training corpus and testing corpus using our pa</context>
</contexts>
<marker>Neal, 1993</marker>
<rawString>Neal, R.M. 1993. Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical Report, Univ. of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing Features Of Random Fields.</title>
<date>1995</date>
<booktitle>In IEEE Transactions on Pattern Analysis and Machine Intelligence.</booktitle>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. Inducing Features Of Random Fields. In IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<institution>Ph.D. Dissertation. University of Pennsylvania.</institution>
<contexts>
<context position="12156" citStr="Ratnaparkhi 1998" startWordPosition="1916" endWordPosition="1917">al performance. Now the maximum entropy modeling can be formulated as follows: given a pairwise context similarity { fo} , the generative probability of { fo} in Corpus I or Corpus II is given as Pr maxEnt UA Di Z ❑ wf (4) where Z is the normalization factor, wf is the weight associated with feature f . The Iterative Scaling algorithm combined with Monte Carlo simulation [Pietra, Pietra, &amp; Lafferty 1995] is used to train the weights in this generative model. Unlike the commonly used conditional maximum entropy modeling which approximates the feature configuration space as the training corpus [Ratnaparkhi 1998], Monte Carlo techniques are required in the generative modeling to simulate the possible feature configurations. The exponential prior smoothing scheme [Goodman 2003] is adopted. The same training procedure is performed using Corpus I and Corpus II to estimate Pr maxEnt � { } L I fi and Pr maxEnt � { } L II fi respectively. f 0 D � fo 4 Statistical Annealing With the maximum entropy modeling presented above, the WSD task is performed as follows: i) for a given set of contexts, the pairwise context similarity measures are computed; ii) for each context similarity { fi } , the two generative p</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. (1998). Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. Dissertation. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>W Li</author>
<author>C Niu</author>
<author>T Cornell</author>
</authors>
<title>InfoXtract: A Customizable Intermediate Level Information Extraction Engine.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL 2003 Workshop on SEALTS.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="8577" citStr="Srihari et al. 2003" startWordPosition="1410" endWordPosition="1413">tages of this maximum entropy model: i) the model is independent of individual words; ii) the model takes no information independence assumption about the data, and hence is powerful enough to utilize heterogeneous features. With the learned conditional probabilities in Eq. (3), for a 3 Maximum Entropy Modeling This section presents the definition of context using maximum entropy modeling. 1 Note that the words that appear in the Senseval-3 lexical sample evaluation are removed in the corpus construction process. ∏ ( { } ) Pr , , CS i j K M N (2) , =1 i Pr =i , =1 ∏ Pr , =1 i j Pr InfoXtract [Srihari et al. 2003]. The relationships being utilized are listed below. Noun: subject-of, object-of, complement-of, has-adjective-modifier, has-nounmodifier, modifier-of, possess, possessed-by, appositive-of Verb: has-subject, has-object, hascomplement, has-adverb-modifier, has-prepositional-modifier Adjective: modifier-of, has-adverb-modifier Based on the above context features, the following three categories of context similarity features are defined: (1) Context similarity based on a vector space model using co-occurring trigger words: the trigger words centering around the key word are represented as a vect</context>
</contexts>
<marker>Srihari, Li, Niu, Cornell, 2003</marker>
<rawString>Srihari, R., W. Li, C. Niu and T. Cornell. 2003. InfoXtract: A Customizable Intermediate Level Information Extraction Engine. In Proceedings of HLT/NAACL 2003 Workshop on SEALTS. Edmonton, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>