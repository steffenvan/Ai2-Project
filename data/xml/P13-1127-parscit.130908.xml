<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.746515">
From Natural Language Specifications to Program Input Parsers
</title>
<author confidence="0.994289">
Tao Lei, Fan Long, Regina Barzilay, and Martin Rinard
</author>
<affiliation confidence="0.998233">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.990481">
{taolei, fanl, regina, rinard}@csail.mit.edu
</email>
<sectionHeader confidence="0.997269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990708">
We present a method for automatically
generating input parsers from English
specifications of input file formats. We
use a Bayesian generative model to cap-
ture relevant natural language phenomena
and translate the English specification into
a specification tree, which is then trans-
lated into a C++ input parser. We model
the problem as a joint dependency pars-
ing and semantic role labeling task. Our
method is based on two sources of infor-
mation: (1) the correlation between the
text and the specification tree and (2) noisy
supervision as determined by the success
of the generated C++ parser in reading in-
put examples. Our results show that our
approach achieves 80.0% F-Score accu-
racy compared to an F-Score of 66.7%
produced by a state-of-the-art semantic
parser on a dataset of input format speci-
fications from the ACM International Col-
legiate Programming Contest (which were
written in English for humans with no in-
tention of providing support for automated
processing).1
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999868571428571">
The general problem of translating natural lan-
guage specifications into executable code has been
around since the field of computer science was
founded. Early attempts to solve this problem
produced what were essentially verbose, clumsy,
and ultimately unsuccessful versions of standard
formal programming languages. In recent years
</bodyText>
<footnote confidence="0.963901">
1The code, data, and experimental setup for this research
are available at http://groups.csail.mit.edu/rbg/code/nl2p
</footnote>
<figureCaption confidence="0.993504">
Figure 1: An example of (a) one natural language
</figureCaption>
<bodyText confidence="0.931350611111111">
specification describing program input data; (b)
the corresponding specification tree representing
the program input structure; and (c) two input ex-
amples
however, researchers have had success address-
ing specific aspects of this problem. Recent ad-
vances in this area include the successful transla-
tion of natural language commands into database
queries (Wong and Mooney, 2007; Zettlemoyer
and Collins, 2009; Poon and Domingos, 2009;
Liang et al., 2011) and the successful mapping of
natural language instructions into Windows com-
mand sequences (Branavan et al., 2009; Branavan
et al., 2010).
In this paper we explore a different aspect of
this general problem: the translation of natural
language input specifications into executable code
that correctly parses the input data and generates
</bodyText>
<figure confidence="0.999691714285714">
(b) Specification Tree:
the input
a single integer T test cases
an integer N the next N lines
N characters
(c) Two Program Input Examples:
1
10
YYWYYWWWWW
YWWWYWWWWW
YYWYYWWWWW
...
WWWWWWWWWW
2
1
Y
5
YWYWW
...
WWYYY
(a) Text Specification:
</figure>
<bodyText confidence="0.9972125">
The input contains a single integer T that indicates the
number of test cases. Then follow the T cases. Each test
case begins with a line contains an integer N, representing
the size of wall. The next N lines represent the original
wall. Each line contains N characters. The j-th character of
the i-th line figures out the color ...
</bodyText>
<page confidence="0.943455">
1294
</page>
<note confidence="0.9135525">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294–1303,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999929326530613">
data structures for holding the data. The need
to automate this task arises because input format
specifications are almost always described in natu-
ral languages, with these specifications then man-
ually translated by a programmer into the code
for reading the program inputs. Our method
highlights potential to automate this translation,
thereby eliminating the manual software develop-
ment overhead.
Consider the text specification in Figure 1a.
If the desired parser is implemented in C++, it
should create a C++ class whose instance objects
hold the different fields of the input. For exam-
ple, one of the fields of this class is an integer, i.e.,
“a single integer T” identified in the text specifi-
cation in Figure 1a. Instead of directly generating
code from the text specification, we first translate
the specification into a specification tree (see Fig-
ure 1b), then map this tree into parser code (see
Figure 2). We focus on the translation from the
text specification to the specification tree.2
We assume that each text specification is ac-
companied by a set of input examples that the de-
sired input parser is required to successfully read.
In standard software development contexts, such
input examples are usually available and are used
to test the correctness of the input parser. Note that
this source of supervision is noisy — the generated
parser may still be incorrect even when it success-
fully reads all of the input examples. Specifically,
the parser may interpret the input examples differ-
ently from the text specification. For example, the
program input in Figure 1c can be interpreted sim-
ply as a list of strings. The parser may also fail
to parse some correctly formatted input files not in
the set of input examples. Therefore, our goal is to
design a technique that can effectively learn from
this weak supervision.
We model our problem as a joint depen-
dency parsing and role labeling task, assuming
a Bayesian generative process. The distribution
over the space of specification trees is informed
by two sources of information: (1) the correla-
tion between the text and the corresponding spec-
ification tree and (2) the success of the generated
parser in reading input examples. Our method uses
a joint probability distribution to take both of these
sources of information into account, and uses a
sampling framework for the inference of specifi-
</bodyText>
<footnote confidence="0.998491">
2During the second step of the process, the specification
tree is deterministically translated into code.
</footnote>
<figure confidence="0.974597">
1 struct TestCaseType {
2 int N;
3 vector&lt;NLinesType*&gt; lstLines;
4 InputType* pParentLink;
5 }
6
7 struct InputType {
8 int T;
9 vector&lt;TestCaseType*&gt; lstTestCase;
10 }
11
12 TestCaseType* ReadTestCase(FILE * pStream,
13 InputType* pParentLink) {
14 TestCaseType* pTestCase
15 = new TestCaseType;
16 pTestCase→pParentLink = pParentLink;
17
18 ...
19
20 return pTestCase;
21 }
22
23 InputType* ReadInput(FILE * pStream) {
24 InputType* pInput = new InputType;
25
26 pInput→T = ReadInteger(pStream);
27 for (int i = 0; i &lt; pInput→T; ++i) {
28 TestCaseType* pTestCase
29 = new TestCaseType;
30 pTestCase = ReadTestCase (pStream,
31 pInput);
32 pInput→lstTestCase.push back (pTestCase);
33 }
34
35 return pInput;
36 }
</figure>
<figureCaption confidence="0.9710895">
Figure 2: Input parser code for reading input files
specified in Figure 1.
</figureCaption>
<bodyText confidence="0.999816571428571">
cation trees given text specifications. A specifica-
tion tree is rejected in the sampling framework if
the corresponding code fails to successfully read
all of the input examples. The sampling frame-
work also rejects the tree if the text/specification
tree pair has low probability.
We evaluate our method on a dataset of in-
put specifications from ACM International Colle-
giate Programming Contests, along with the cor-
responding input examples. These specifications
were written for human programmers with no in-
tention of providing support for automated pro-
cessing. However, when trained using the noisy
supervision, our method achieves substantially
more accurate translations than a state-of-the-art
semantic parser (Clarke et al., 2010) (specifically,
80.0% in F-Score compared to an F-Score of
66.7%). The strength of our model in the face of
such weak supervision is also highlighted by the
fact that it retains an F-Score of 77% even when
only one input example is provided for each input
</bodyText>
<page confidence="0.974315">
1295
</page>
<figure confidence="0.616684666666667">
Specification Tree:
the input
one integer N N lines the following lines
two real four real
numbers Xi, Yi numbers
Formal Input Grammar Definition:
Input := N
Lines [size = N]
FollowingLines [size = *]
</figure>
<equation confidence="0.988875">
N := int
Lines := Xi Yi
Xi := float
Yi := float
FollowingLines := F1 F2 F3 F4
F1 := float
Text Specification:
</equation>
<bodyText confidence="0.966034125">
Your program is supposed to read the input from the standard
input and write its output to the standard output.
The first line of the input contains one integer N.
N lines follow, the i-th of them contains two real numbers Xi, Yi
separated by a single space - the coordinates of the i-th house.
Each of the following lines contains four real numbers separated
by a single space. These numbers are the coordinates of two
different points (X1, Y1) and (X2, Y2), lying on the highway.
</bodyText>
<figure confidence="0.980101">
(a) (b) (c)
</figure>
<figureCaption confidence="0.9960644">
Figure 3: An example of generating input parser code from text: (a) a natural language input specifica-
tion; (b) a specification tree representing the input format structure (we omit the background phrases in
this tree in order to give a clear view of the input format structure); and (c) formal definition of the input
format constructed from the specification tree, represented as a context-free grammar in Backus-Naur
Form with additional size constraints.
</figureCaption>
<bodyText confidence="0.86812">
specification.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.927562">
Learning Meaning Representation from Text
</subsectionHeader>
<bodyText confidence="0.999743365853659">
Mapping sentences into structural meaning rep-
resentations is an active and extensively studied
task in NLP. Examples of meaning representations
considered in prior research include logical forms
based on database query (Tang and Mooney, 2000;
Zettlemoyer and Collins, 2005; Kate and Mooney,
2007; Wong and Mooney, 2007; Poon and Domin-
gos, 2009; Liang et al., 2011; Goldwasser et al.,
2011), semantic frames (Das et al., 2010; Das
and Smith, 2011) and database records (Chen and
Mooney, 2008; Liang et al., 2009).
Learning Semantics from Feedback Our ap-
proach is related to recent research on learn-
ing from indirect supervision. Examples include
leveraging feedback available via responses from
a virtual world (Branavan et al., 2009) or from ex-
ecuting predicted database queries (Chang et al.,
2010; Clarke et al., 2010). While Branavan et
al. (2009) formalize the task as a sequence of de-
cisions and learns from local rewards in a Rein-
forcement Learning framework, our model learns
to predict the whole structure at a time. Another
difference is the way our model incorporates the
noisy feedback. While previous approaches rely
on the feedback to train a discriminative prediction
model, our approach models a generative process
to guide structure predictions when the feedback
is noisy or unavailable.
NLP in Software Engineering Researchers
have recently developed a number of approaches
that apply natural language processing techniques
to software engineering problems. Examples in-
clude analyzing API documents to infer API li-
brary specifications (Zhong et al., 2009; Pandita
et al., 2012) and analyzing code comments to de-
tect concurrency bugs (Tan et al., 2007; Tan et al.,
2011). This research analyzes natural language in
documentation or comments to better understand
existing application programs. Our mechanism, in
contrast, automatically generates parser programs
from natural language input format descriptions.
</bodyText>
<sectionHeader confidence="0.985555" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.9951115">
The task of translating text specifications to input
parsers consists of two steps, as shown in Figure 3.
First, given a text specification describing an input
format, we wish to infer a parse tree (which we
call a specification tree) implied by the text. Sec-
ond, we convert each specification tree into for-
mal grammar of the input format (represented in
Backus-Naur Form) and then generate code that
reads the input into data structures. In this paper,
we focus on the NLP techniques used in the first
step, i.e., learning to infer the specification trees
from text. The second step is achieved using a de-
terministic rule-based tool. 3
As input, we are given a set of text specifica-
tions w = {wi, · · · ,wN}, where each wi is a text
specification represented as a sequence of noun
phrases {wik}. We use UIUC shallow parser to
preprocess each text specificaton into a sequence
of the noun phrases.4 In addition, we are given a
set of input examples for each wi. We use these
examples to test the generated input parsers to re-
3Specifically, the specification tree is first translated into
the grammar using a set of rules and seed words that identi-
fies basic data types such as int. Our implementation then
generates a top-down parser since the generated grammar is
simple. In general, standard techniques such as Bison and
Yacc (Johnson, 1979) can generate bottom-up parsers given
such grammar.
</bodyText>
<footnote confidence="0.789305">
4http://cogcomp.cs.illinois.edu/demo/shallowparse/?id=7
</footnote>
<page confidence="0.982026">
1296
</page>
<bodyText confidence="0.999682392857143">
ject incorrect predictions made by our probabilis-
tic model.
We formalize the learning problem as a de-
pendency parsing and role labeling problem.
Our model predicts specification trees t =
{t1, · · · , tN} for the text specifications, where
each specification tree tz is a dependency tree over
noun phrases {wzk}. In general many program in-
put formats are nested tree structures, in which the
tree root denotes the entire chunk of program in-
put data and each chunk (tree node) can be further
divided into sub-chunks or primitive fields that ap-
pear in the program input (see Figure 3). There-
fore our objective is to predict a dependency tree
that correctly represents the structure of the pro-
gram input.
In addition, the role labeling problem is to as-
sign a tag zzk to each noun phrase wzk in a specifi-
cation tree, indicating whether the phrase is a key
phrase or a background phrase. Key phrases are
named entities that identify input fields or input
chunks appear in the program input data, such as
“the input” or “the following lines” in Figure 3b.
In contrast, background phrases do not define in-
put fields or chunks. These phrases are used to or-
ganize the document (e.g., “your program”) or to
refer to key phrases described before (e.g., “each
line”).
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.99999675">
We use two kinds of information to bias our
model: (1) the quality of the generated code as
measured by its ability to read the given input ex-
amples and (2) the features over the observed text
wz and the hidden specification tree tz (this is stan-
dard in traditional parsing problems). We combine
these two kinds of information into a Bayesian
generative model in which the code quality of the
specification tree is captured by the prior probabil-
ity P(t) and the feature observations are encoded
in the likelihood probability P(w|t). The infer-
ence jointly optimizes these two factors:
</bodyText>
<equation confidence="0.991029">
P(t|w) ∝ P(t) · P(w|t).
</equation>
<bodyText confidence="0.999208777777778">
Modeling the Generative Process. We assume
the generative model operates by first generating
the model parameters from a set of Dirichlet dis-
tributions. The model then generates text spec-
ification trees. Finally, it generates natural lan-
guage feature observations conditioned on the hid-
den specification trees.
The generative process is described formally as
follows:
</bodyText>
<listItem confidence="0.982026333333333">
• Generating Model Parameters: For every
pair of feature type f and phrase tag z, draw
a multinomial distribution parameter θzf from
a Dirichlet prior P(θzf). The multinomial pa-
rameters provide the probabilities of observ-
ing different feature values in the text.
• Generating Specification Tree: For each
text specification, draw a specification tree t
from all possible trees over the sequence of
noun phrases in this specification. We denote
the probability of choosing a particular spec-
ification tree t as P(t).
</listItem>
<bodyText confidence="0.98997875">
Intuitively, this distribution should assign
high probability to good specification trees
that can produce C++ code that reads all input
examples without errors, we therefore define
P(t) as follows:5
{ 1 the input parser of tree t
reads all input examples
without error
</bodyText>
<equation confidence="0.528326">
E otherwise
</equation>
<bodyText confidence="0.999108666666667">
where Z is a normalization factor and E is em-
pirically set to 10−6. In other words, P(·)
treats all specification trees that pass the input
example test as equally probable candidates
and inhibits the model from generating trees
which fail the test. Note that we do not know
this distribution a priori until the specification
trees are evaluated by testing the correspond-
ing C++ code. Because it is intractable to test
all possible trees and all possible generated
code for a text specification, we never explic-
itly compute the normalization factor 1/Z of
this distribution. We therefore use sampling
methods to tackle this problem during infer-
ence.
</bodyText>
<listItem confidence="0.912934">
• Generating Features: The final step gener-
ates lexical and contextual features for each
</listItem>
<bodyText confidence="0.8856695">
tree. For each phrase wk associated with tag
zk, let wp be its parent phrase in the tree and
ws be the non-background sibling phrase to
its left in the tree. The model generates the
corresponding set of features O(wp, ws, wk)
for each text phrase tuple (wp, ws, wk), with
</bodyText>
<footnote confidence="0.6953335">
5When input examples are not available, P(t) is just uni-
form distribution.
</footnote>
<equation confidence="0.9789549">
1
P(t) =
Z ·
1297
probability P(φ(wp, ws, wk)). We assume
that each feature fj is generated indepen-
dently:
P(w|t) = P(φ(wp, ws, wk))
fl=
fj∈φ(wp,ws,wk)
</equation>
<bodyText confidence="0.99828152631579">
where θzk
fj is the j-th component in the multi-
nomial distribution θzk
f denoting the proba-
bility of observing a feature fj associated
with noun phrase wk labeled with tag zk. We
define a range of features that capture the cor-
respondence between the input format and its
description in natural language. For example,
at the unigram level we aim to capture that
noun phrases containing specific words such
as “cases” and “lines” may be key phrases
(correspond to data chunks appear in the in-
put), and that verbs such as “contain” may
indicate that the next noun phrase is a key
phrase.
The full joint probability of a set w of N spec-
ifications and hidden text specification trees t is
defined as:
</bodyText>
<equation confidence="0.998099">
P(θ, t, w) = P(θ) flN P(ti)P(wi|ti, θ)
i=1
</equation>
<bodyText confidence="0.9202676">
P(φ(wip, wis, wik)).
Learning the Model During inference, we want
to estimate the hidden specification trees t given
the observed natural language specifications w, af-
ter integrating the model parameters out, i.e.
</bodyText>
<equation confidence="0.9881135">
t ∼ P(t|w) = J P(t,θ|w)dθ.
e
</equation>
<bodyText confidence="0.994605368421053">
We use Gibbs sampling to sample variables t from
this distribution. In general, the Gibbs sampling
algorithm randomly initializes the variables and
then iteratively solves one subproblem at a time.
The subproblem is to sample only one variable
conditioned on the current values of all other vari-
ables. In our case, we sample one hidden spec-
ification tree ti while holding all other trees t−i
fixed:
ti ∼ P(ti|w,t−i) (1)
where t−i = (t1, · · · , ti−1, ti+1, ··· , tN).
However directly solving the subproblem (1)
in our case is still hard, we therefore use a
Metropolis-Hastings sampler that is similarly ap-
plied in traditional sentence parsing problems.
Specifically, the Hastings sampler approximates
(1) by first drawing a new ti0 from a tractable pro-
posal distribution Q instead of P(ti|w, t−i). We
choose Q to be:
</bodyText>
<equation confidence="0.928718">
Q(ti0|θ0,wi) ∝ P(wi|ti0,θ0). (2)
</equation>
<bodyText confidence="0.99931375">
Then the probability of accepting the new sample
is determined using the typical Metropolis Hast-
ings process. Specifically, ti0 will be accepted to
replace the last ti with probability:
</bodyText>
<equation confidence="0.997941">
R(ti, ti0) = min �
1, P(ti0|w, t−i) Q(tiil θ0, wi)
P(ti |w, t−i) Q(ti 1θ0, wi)
= min �
1,P(ti0, t−i, w)P(wi  |ti, θ0) ,
P(ti, t−i, w)P(wi|ti0, θ0)
</equation>
<bodyText confidence="0.997951458333333">
in which the normalization factors 1/Z are can-
celled out. We choose θ0 to be the parameter ex-
pectation based on the current observations, i.e.
θ0 = E [θ|w, t−i], so that the proposal distribu-
tion is close to the true distribution. This sampling
algorithm with a changing proposal distribution
has been shown to work well in practice (John-
son and Griffiths, 2007; Cohn et al., 2010; Naseem
and Barzilay, 2011). The algorithm pseudo code is
shown in Algorithm 1.
To sample from the proposal distribution (2) ef-
ficiently, we implement a dynamic programming
algorithm which calculates marginal probabilities
of all subtrees. The algorithm works similarly to
the inside algorithm (Baker, 1979), except that we
do not assume the tree is binary. We therefore per-
form one additional dynamic programming step
that sums over all possible segmentations of each
span. Once the algorithm obtains the marginal
probabilities of all subtrees, a specification tree
can be drawn recursively in a top-down manner.
Calculating P(t, w) in R(t, t0) requires inte-
grating the parameters θ out. This has a closed
form due to the Dirichlet-multinomial conjugacy:
</bodyText>
<equation confidence="0.80748">
P(t, w) = P(t) · J P(w|t, θ)P(θ)dθ
B
a P(t) · fl Beta (count(f) + α) .
</equation>
<bodyText confidence="0.99970675">
Here α are the Dirichlet hyper parameters and
count(f) are the feature counts observed in data
(t, w). The closed form is a product of the Beta
functions of each feature type.
</bodyText>
<table confidence="0.758699636363636">
zk
θfj
= P(θ) flN fl
i=1 P (ti)
k
1298
Feature Type Description Feature Value
Word each word in noun phrase wk lines, VAR
Verb verbs in noun phrase wk and the verb phrase before wk contains
Distance sentence distance between wk and its parent phrase wp 1
Coreference wk share duplicate nouns or variable names with wp or ws True
</table>
<tableCaption confidence="0.9558495">
Table 1: Example of feature types and values. To deal with sparsity, we map variable names such as “N”
and “X” into a category word “VAR” in word features.
</tableCaption>
<figure confidence="0.974091416666666">
Input: Set off text specification documents
W, l N},
Number of iterations T
1 Randomly initialize specification trees
t = {tl, ··· ,tN}
2 for iter = 1 ··· T do
3 Sample tree ti for i-th document:
4 for i = 1 ··· N do
5 Estimate model parameters:
θ&apos; = E [θ&apos;|w, t−i]
6
7 Sample a new specification tree from distribution
Q:
8 t&apos; ∼ Q(t&apos;|θ&apos;, wi)
9 Generate and test code, and return feedback:
10 f&apos; = CodeGenerator(wi, t&apos;)
11 Calculate accept probability r:
12 r = R(ti, t&apos;)
13 Accept the new tree with probability r:
14 With probability r : ti = t&apos;
15 end
16 end
17 Produce final structures:
18 return { ti if ti gets positive feedback }
</figure>
<figureCaption confidence="0.9886825">
Algorithm 1: The sampling framework for learn-
ing the model.
</figureCaption>
<bodyText confidence="0.991218705882353">
Model Implementation: We define several
types of features to capture the correlation be-
tween the hidden structure and its expression in
natural language. For example, verb features are
introduced because certain preceding verbs such
as “contains” and “consists” are good indicators of
key phrases. There are 991 unique features in total
in our experiments. Examples of features appear
in Table 1.
We use a small set of 8 seed words to bias the
search space. Specifically, we require each leaf
key phrase to contain at least one seed word that
identifies the C++ primitive data type (such as “in-
teger”, “float”, “byte” and “string”).
We also encourage a phrase containing the word
“input” to be the root of the tree (for example, “the
input file”) and each coreference phrase to be a
</bodyText>
<figureCaption confidence="0.886731444444445">
Total # of words 7330
Total # of noun phrases 1829
Vocabulary size 781
Avg. # of words per sentence 17.29
Avg. # of noun phrase per document 17.26
Avg. # of possible trees per document 52K
Median # of possible trees per document 79
Min # of possible trees per document 1
Max # of possible trees per document 2M
</figureCaption>
<tableCaption confidence="0.900251">
Table 2: Statistics for 106 ICPC specifications.
</tableCaption>
<bodyText confidence="0.997388666666667">
background phrase (for example, “each test case”
after mentioning “test cases”), by initially adding
pseudo counts to Dirichlet priors.
</bodyText>
<sectionHeader confidence="0.99862" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.978462125">
Datasets: Our dataset consists of problem de-
scriptions from ACM International Collegiate Pro-
gramming Contests.6 We collected 106 problems
from ACM-ICPC training websites.7 From each
problem description, we extracted the portion that
provides input specifications. Because the test
input examples are not publicly available on the
ACM-ICPC training websites, for each specifica-
tion, we wrote simple programs to generate 100
random input examples.
Table 2 presents statistics for the text specifica-
tion set. The data set consists of 424 sentences,
where an average sentence contains 17.3 words.
The data set contains 781 unique words. The
length of each text specification varies from a sin-
gle sentence to eight sentences. The difference be-
tween the average and median number of trees is
large. This is because half of the specifications are
relatively simple and have a small number of pos-
sible trees, while a few difficult specifications have
over thousands of possible trees (as the number of
trees grows exponentially when the text length in-
creases).
Evaluation Metrics: We evaluate the model
</bodyText>
<footnote confidence="0.999646333333333">
6Official Website: http://cm.baylor.edu/welcome.icpc
7PKU Online Judge: http://poj.org/; UVA Online Judge:
http://uva.onlinejudge.org/
</footnote>
<page confidence="0.997546">
1299
</page>
<bodyText confidence="0.998398666666667">
performance in terms of its success in generating a
formal grammar that correctly represents the input
format (see Figure 3c). As a gold annotation, we
construct formal grammars for all text specifica-
tions. Our results are generated by automatically
comparing the machine-generated grammars with
their golden counterparts. If the formal grammar
is correct, then the generated C++ parser will cor-
rectly read the input file into corresponding C++
data structures.
We use Recall and Precision as evaluation mea-
sures:
</bodyText>
<figure confidence="0.719627333333333">
# correct structures
Recall =
# text specifications
# correct structures
Precision =
# produced structures
</figure>
<bodyText confidence="0.98671803125">
where the produced structures are the positive
structures returned by our framework whose corre-
sponding code successfully reads all input exam-
ples (see Algorithm 1 line 18). Note the number of
produced structures may be less than the number
of text specifications, because structures that fail
the input test are not returned.
Baselines: To evaluate the performance of our
model, we compare against four baselines.
The No Learning baseline is a variant of our
model that selects a specification tree without
learning feature correspondence. It continues
sampling a specification tree for each text speci-
fication until it finds one which successfully reads
all of the input examples.
The second baseline Aggressive is a state-of-
the-art semantic parsing framework (Clarke et al.,
2010).8 The framework repeatedly predicts hidden
structures (specification trees in our case) using a
structure learner, and trains the structure learner
based on the execution feedback of its predictions.
Specifically, at each iteration the structure learner
predicts the most plausible specification tree for
each text document:
tz = argmaxt f(wz, t).
Depending on whether the corresponding code
reads all input examples successfully or not, the
(wz, tz) pairs are added as an positive or negative
sample to populate a training set. After each it-
eration the structure learner is re-trained with the
training samples to improve the prediction accu-
racy. In our experiment, we follow (Clarke et al.,
</bodyText>
<footnote confidence="0.90743">
8We take the name Aggressive from this paper.
</footnote>
<table confidence="0.999687833333333">
Model Recall Precision F-Score
No Learning 52.0 57.2 54.5
Aggressive 63.2 70.5 66.7
Full Model 72.5 89.3 80.0
Full Model (Oracle) 72.5 100.0 84.1
Aggressive (Oracle) 80.2 100.0 89.0
</table>
<tableCaption confidence="0.95869">
Table 3: Average % Recall and % Precision of our
model and all baselines over 20 independent runs.
</tableCaption>
<bodyText confidence="0.991984">
2010) and choose a structural Support Vector Ma-
chine SVMstruct 9 as the structure learner.
The remaining baselines provide an upper
bound on the performance of our model. The base-
line Full Model (Oracle) is the same as our full
model except that the feedback comes from an or-
acle which tells whether the specification tree is
correct or not. We use this oracle information in
the prior P(t) same as we use the noisy feedback.
Similarly the baseline Aggressive (Oracle) is the
Aggressive baseline with access to the oracle.
Experimental Details: Because no human an-
notation is required for learning, we train our
model and all baselines on all 106 ICPC text spec-
ifications (similar to unsupervised learning). We
report results averaged over 20 independent runs.
For each of these runs, the model and all baselines
run 100 iterations. For baseline Aggressive, in
each iteration the SVM structure learner predicts
one tree with the highest score for each text spec-
ification. If two different specification trees of the
same text specification get positive feedback, we
take the one generated in later iteration for evalu-
ation.
</bodyText>
<sectionHeader confidence="0.989304" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999829285714286">
Comparison with Baselines Table 3 presents
the performance of various models in predicting
correct specification trees. As can be seen, our
model achieves an F-Score of 80%. Our model
therefore significantly outperforms the No Learn-
ing baseline (by more than 25%). Note that the
No Learning baseline achieves a low Precision
of 57.2%. This low precision reflects the noisi-
ness of the weak supervision - nearly one half of
the parsers produced by No Learning are actually
incorrect even though they read all of the input
examples without error. This comparison shows
the importance of capturing correlations between
the specification trees and their text descriptions.
</bodyText>
<footnote confidence="0.994652">
9www.cs.cornell.edu/people/tj/svm light/svm struct.html
</footnote>
<page confidence="0.971177">
1300
</page>
<figure confidence="0.4347144">
The input contains
several testcases.
Each is specified by two strings S, T of alphanumeric ASCII characters
The next N lines of the input file contain the Cartesian coordinates of
watchtowers, one pair of coordinates per line.
</figure>
<figureCaption confidence="0.983637">
Figure 4: Examples of dependencies and key phrases predicted by our model. Green marks correct key
phrases and dependencies and red marks incorrect ones. The missing key phrases are marked in gray.
</figureCaption>
<figure confidence="0.662518">
%supervision
</figure>
<figureCaption confidence="0.977212">
Figure 5: Precision and Recall of our model by
</figureCaption>
<bodyText confidence="0.977485586206896">
varying the percentage of weak supervision. The
green lines are the performance of Aggressive
baseline trained with full weak supervision.
Because our model learns correlations via feature
representations, it produces substantially more ac-
curate translations.
While both the Full Model and Aggressive base-
line use the same source of feedback, they capi-
talize on it in a different way. The baseline uses
the noisy feedback to train features capturing the
correlation between trees and text. Our model, in
contrast, combines these two sources of informa-
tion in a complementary fashion. This combina-
tion allows our model to filter false positive feed-
back and produce 13% more correct translations
than the Aggressive baseline.
Clean versus Noisy Supervision To assess the
impact of noise on model accuracy, we compare
the Full Model against the Full Model (Oracle).
The two versions achieve very close performance
(80% v.s 84% in F-Score), even though Full Model
is trained with noisy feedback. This demonstrates
the strength of our model in learning from such
weak supervision. Interestingly, Aggressive (Ora-
cle) outperforms our oracle model by a 5% mar-
gin. This result shows that when the supervision
is reliable, the generative assumption limits our
model’s ability to gain the same performance im-
provement as discriminative models.
</bodyText>
<figure confidence="0.530545">
#input examples
</figure>
<figureCaption confidence="0.671166666666667">
Figure 6: Precision and Recall of our model by
varying the number of available input examples
per text specification.
</figureCaption>
<bodyText confidence="0.993229037037037">
Impact of Input Examples Our model can also
be trained in a fully unsupervised or a semi-
supervised fashion. In real cases, it may not be
possible to obtain input examples for all text spec-
ifications. We evaluate such cases by varying the
amount of supervision, i.e. how many text specifi-
cations are paired with input examples. In each
run, we randomly select text specifications and
only these selected specifications have access to
input examples. Figure 5 gives the performance of
our model with 0% supervision (totally unsuper-
vised) to 100% supervision (our full model). With
much less supervision, our model is still able to
achieve performance comparable with the Aggres-
sive baseline.
We also evaluate how the number of provided
input examples influences the performance of the
model. Figure 6 indicates that the performance is
largely insensitive to the number of input exam-
ples — once the model is given even one input
example, its performance is close to the best per-
formance it obtains with 100 input examples. We
attribute this phenomenon to the fact that if the
generated code is incorrect, it is unlikely to suc-
cessfully parse any input.
Case Study Finally, we consider some text spec-
ifications that our model does not correctly trans-
</bodyText>
<page confidence="0.972292">
1301
</page>
<bodyText confidence="0.999857333333333">
late. In Figure 4a, the program input is interpreted
as a list of character strings, while the correct in-
terpretation is that the input is a list of string pairs.
Note that both interpretations produce C++ input
parsers that successfully read all of the input ex-
amples. One possible way to resolve this problem
is to add other features such as syntactic depen-
dencies between words to capture more language
phenomena. In Figure 4b, the missing key phrase
is not identified because our model is not able to
ground the meaning of “pair of coordinates” to two
integers. Possible future extensions to our model
include using lexicon learning methods for map-
ping words to C++ primitive types for example
“coordinates” to (int, int).
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999974285714286">
It is standard practice to write English language
specifications for input formats. Programmers
read the specifications, then develop source code
that parses inputs in the format. Known disadvan-
tages of this approach include development cost,
parsers that contain errors, specification misunder-
standings, and specifications that become out of
date as the implementation evolves.
Our results show that taking both the correlation
between the text and the specification tree and the
success of the generated C++ parser in reading in-
put examples into account enables our method to
correctly generate C++ parsers for 72.5% of our
natural language specifications.
</bodyText>
<sectionHeader confidence="0.998665" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999977444444444">
The authors acknowledge the support of Battelle
Memorial Institute (PO #300662) and the NSF
(Grant IIS-0835652). Thanks to Mirella Lapata,
members of the MIT NLP group and the ACL re-
viewers for their suggestions and comments. Any
opinions, findings, conclusions, or recommenda-
tions expressed in this paper are those of the au-
thors, and do not necessarily reflect the views of
the funding organizations.
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999767766666667">
James K. Baker. 1979. Trainable grammars for speech
recognition. In DH Klatt and JJ Wolf, editors,
Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society ofAmerica, pages 547–
550.
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to
map high-level instructions to commands. In Pro-
ceedings of ACL, pages 1268–1277.
Mingwei Chang, Vivek Srikumar, Dan Goldwasser,
and Dan Roth. 2010. Structured output learning
with indirect supervision. In Proceedings of the 27th
International Conference on Machine Learning.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of 25th International Confer-
ence on Machine Learning (ICML-2008).
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Jour-
nal of Machine Learning Research, 11.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown
predicates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1435–
1444.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 948–956.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ’11.
Mark Johnson and Thomas L. Griffiths. 2007.
Bayesian inference for pcfgs via markov chain
monte carlo. In Proceedings of the North American
Conference on Computational Linguistics (NAACL
’07).
Stephen C. Johnson. 1979. Yacc: Yet another
compiler-compiler. Unix Programmer’s Manual,
vol 2b.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the 22nd national confer-
ence on Artificial intelligence - Volume 1, AAAI’07.
</reference>
<page confidence="0.905976">
1302
</page>
<reference confidence="0.999871655172414">
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and In-
ternational Joint Conference on Natural Language
Processing (ACL-IJCNLP).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics.
Tahira Naseem and Regina Barzilay. 2011. Using se-
mantic cues to learn syntax. In Proceedings of the
25th National Conference on Artificial Intelligence
(AAAI).
Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie,
Stephen Oney, and Amit Paradkar. 2012. Inferring
method specifications from natural language api de-
scriptions. In Proceedings of the 2012 International
Conference on Software Engineering, ICSE 2012,
pages 815–825, Piscataway, NJ, USA. IEEE Press.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
’09.
Lin Tan, Ding Yuan, Gopal Krishna, and Yuanyuan
Zhou. 2007. /* iComment: Bugs or bad comments?
*/. In Proceedings of the 21st ACM Symposium on
Operating Systems Principles (SOSP07), October.
Lin Tan, Yuanyuan Zhou, and Yoann Padioleau. 2011.
aComment: Mining annotations from comments and
code to detect interrupt-related concurrency bugs. In
Proceedings of the 33rd International Conference on
Software Engineering (ICSE11), May.
Lappoon R. Tang and Raymond J. Mooney. 2000. Au-
tomated construction of database interfaces: inte-
grating statistical and relational learning for seman-
tic parsing. In Proceedings of the conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’00.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In ACL.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI, pages 658–666.
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics.
Hao Zhong, Lu Zhang, Tao Xie, and Hong Mei. 2009.
Inferring resource specifications from natural lan-
guage api documentation. In Proceedings of the
2009 IEEE/ACM International Conference on Auto-
mated Software Engineering, ASE ’09, pages 307–
318, Washington, DC, USA. IEEE Computer Soci-
ety.
</reference>
<page confidence="0.976178">
1303
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418733">
<title confidence="0.998953">From Natural Language Specifications to Program Input Parsers</title>
<author confidence="0.996548">Tao Lei</author>
<author confidence="0.996548">Fan Long</author>
<author confidence="0.996548">Regina Barzilay</author>
<author confidence="0.996548">Martin</author>
<affiliation confidence="0.994266">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<email confidence="0.662874">fanl,regina,</email>
<abstract confidence="0.999661916666667">We present a method for automatically generating input parsers from English specifications of input file formats. We use a Bayesian generative model to capture relevant natural language phenomena and translate the English specification into a specification tree, which is then translated into a C++ input parser. We model the problem as a joint dependency parsing and semantic role labeling task. Our method is based on two sources of information: (1) the correlation between the text and the specification tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading input examples. Our results show that our approach achieves 80.0% F-Score accuracy compared to an F-Score of 66.7% produced by a state-of-the-art semantic parser on a dataset of input format specifications from the ACM International Collegiate Programming Contest (which were written in English for humans with no in-</abstract>
<intro confidence="0.645654">tention of providing support for automated</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica,</booktitle>
<pages>547--550</pages>
<editor>In DH Klatt and JJ Wolf, editors,</editor>
<contexts>
<context position="19438" citStr="Baker, 1979" startWordPosition="3171" endWordPosition="3172"> the parameter expectation based on the current observations, i.e. θ0 = E [θ|w, t−i], so that the proposal distribution is close to the true distribution. This sampling algorithm with a changing proposal distribution has been shown to work well in practice (Johnson and Griffiths, 2007; Cohn et al., 2010; Naseem and Barzilay, 2011). The algorithm pseudo code is shown in Algorithm 1. To sample from the proposal distribution (2) efficiently, we implement a dynamic programming algorithm which calculates marginal probabilities of all subtrees. The algorithm works similarly to the inside algorithm (Baker, 1979), except that we do not assume the tree is binary. We therefore perform one additional dynamic programming step that sums over all possible segmentations of each span. Once the algorithm obtains the marginal probabilities of all subtrees, a specification tree can be drawn recursively in a top-down manner. Calculating P(t, w) in R(t, t0) requires integrating the parameters θ out. This has a closed form due to the Dirichlet-multinomial conjugacy: P(t, w) = P(t) · J P(w|t, θ)P(θ)dθ B a P(t) · fl Beta (count(f) + α) . Here α are the Dirichlet hyper parameters and count(f) are the feature counts ob</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In DH Klatt and JJ Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica, pages 547– 550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2323" citStr="Branavan et al., 2009" startWordPosition="341" endWordPosition="344">/nl2p Figure 1: An example of (a) one natural language specification describing program input data; (b) the corresponding specification tree representing the program input structure; and (c) two input examples however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates (b) Specification Tree: the input a single integer T test cases an integer N the next N lines N characters (c) Two Program Input Examples: 1 10 YYWYYWWWWW YWWWYWWWWW YYWYYWWWWW ... WWWWWWWWWW 2 1 Y 5 YWYWW ... WWYYY (a) Text Specification: The input contains a single integer T that indicates the number of test cases. Then follow the T cases. Each test case begins with a lin</context>
<context position="9639" citStr="Branavan et al., 2009" startWordPosition="1534" endWordPosition="1537">mples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering R</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1268--1277</pages>
<contexts>
<context position="2347" citStr="Branavan et al., 2010" startWordPosition="345" endWordPosition="348">ple of (a) one natural language specification describing program input data; (b) the corresponding specification tree representing the program input structure; and (c) two input examples however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates (b) Specification Tree: the input a single integer T test cases an integer N the next N lines N characters (c) Two Program Input Examples: 1 10 YYWYYWWWWW YWWWYWWWWW YYWYYWWWWW ... WWWWWWWWWW 2 1 Y 5 YWYWW ... WWYYY (a) Text Specification: The input contains a single integer T that indicates the number of test cases. Then follow the T cases. Each test case begins with a line contains an integer N,</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of ACL, pages 1268–1277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingwei Chang</author>
<author>Vivek Srikumar</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Structured output learning with indirect supervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="9704" citStr="Chang et al., 2010" startWordPosition="1545" endWordPosition="1548"> logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering Researchers have recently developed a number of approaches that ap</context>
</contexts>
<marker>Chang, Srikumar, Goldwasser, Roth, 2010</marker>
<rawString>Mingwei Chang, Vivek Srikumar, Dan Goldwasser, and Dan Roth. 2010. Structured output learning with indirect supervision. In Proceedings of the 27th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of 25th International Conference on Machine Learning (ICML-2008).</booktitle>
<contexts>
<context position="9396" citStr="Chen and Mooney, 2008" startWordPosition="1497" endWordPosition="1500">mmar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy fe</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In Proceedings of 25th International Conference on Machine Learning (ICML-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="7309" citStr="Clarke et al., 2010" startWordPosition="1143" endWordPosition="1146">ing code fails to successfully read all of the input examples. The sampling framework also rejects the tree if the text/specification tree pair has low probability. We evaluate our method on a dataset of input specifications from ACM International Collegiate Programming Contests, along with the corresponding input examples. These specifications were written for human programmers with no intention of providing support for automated processing. However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%). The strength of our model in the face of such weak supervision is also highlighted by the fact that it retains an F-Score of 77% even when only one input example is provided for each input 1295 Specification Tree: the input one integer N N lines the following lines two real four real numbers Xi, Yi numbers Formal Input Grammar Definition: Input := N Lines [size = N] FollowingLines [size = *] N := int Lines := Xi Yi Xi := float Yi := float FollowingLines := F1 F2 F3 F4 F1 := float Text Specification: Your program is supposed to </context>
<context position="9726" citStr="Clarke et al., 2010" startWordPosition="1549" endWordPosition="1552"> on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering Researchers have recently developed a number of approaches that apply natural language p</context>
<context position="25253" citStr="Clarke et al., 2010" startWordPosition="4141" endWordPosition="4144"> line 18). Note the number of produced structures may be less than the number of text specifications, because structures that fail the input test are not returned. Baselines: To evaluate the performance of our model, we compare against four baselines. The No Learning baseline is a variant of our model that selects a specification tree without learning feature correspondence. It continues sampling a specification tree for each text specification until it finds one which successfully reads all of the input examples. The second baseline Aggressive is a state-ofthe-art semantic parsing framework (Clarke et al., 2010).8 The framework repeatedly predicts hidden structures (specification trees in our case) using a structure learner, and trains the structure learner based on the execution feedback of its predictions. Specifically, at each iteration the structure learner predicts the most plausible specification tree for each text document: tz = argmaxt f(wz, t). Depending on whether the corresponding code reads all input examples successfully or not, the (wz, tz) pairs are added as an positive or negative sample to populate a training set. After each iteration the structure learner is re-trained with the trai</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<contexts>
<context position="19130" citStr="Cohn et al., 2010" startWordPosition="3124" endWordPosition="3127">tings process. Specifically, ti0 will be accepted to replace the last ti with probability: R(ti, ti0) = min � 1, P(ti0|w, t−i) Q(tiil θ0, wi) P(ti |w, t−i) Q(ti 1θ0, wi) = min � 1,P(ti0, t−i, w)P(wi |ti, θ0) , P(ti, t−i, w)P(wi|ti0, θ0) in which the normalization factors 1/Z are cancelled out. We choose θ0 to be the parameter expectation based on the current observations, i.e. θ0 = E [θ|w, t−i], so that the proposal distribution is close to the true distribution. This sampling algorithm with a changing proposal distribution has been shown to work well in practice (Johnson and Griffiths, 2007; Cohn et al., 2010; Naseem and Barzilay, 2011). The algorithm pseudo code is shown in Algorithm 1. To sample from the proposal distribution (2) efficiently, we implement a dynamic programming algorithm which calculates marginal probabilities of all subtrees. The algorithm works similarly to the inside algorithm (Baker, 1979), except that we do not assume the tree is binary. We therefore perform one additional dynamic programming step that sums over all possible segmentations of each span. Once the algorithm obtains the marginal probabilities of all subtrees, a specification tree can be drawn recursively in a to</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semisupervised frame-semantic parsing for unknown predicates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1435--1444</pages>
<contexts>
<context position="9352" citStr="Das and Smith, 2011" startWordPosition="1490" endWordPosition="1493">ion tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is</context>
</contexts>
<marker>Das, Smith, 2011</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2011. Semisupervised frame-semantic parsing for unknown predicates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1435– 1444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>948--956</pages>
<contexts>
<context position="9330" citStr="Das et al., 2010" startWordPosition="1486" endWordPosition="1489">rom the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time.</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic frame-semantic parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 948–956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</booktitle>
<volume>1</volume>
<contexts>
<context position="9295" citStr="Goldwasser et al., 2011" startWordPosition="1480" endWordPosition="1483">efinition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to pre</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
</authors>
<title>Bayesian inference for pcfgs via markov chain monte carlo.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Conference on Computational Linguistics (NAACL ’07).</booktitle>
<contexts>
<context position="19111" citStr="Johnson and Griffiths, 2007" startWordPosition="3119" endWordPosition="3123">ng the typical Metropolis Hastings process. Specifically, ti0 will be accepted to replace the last ti with probability: R(ti, ti0) = min � 1, P(ti0|w, t−i) Q(tiil θ0, wi) P(ti |w, t−i) Q(ti 1θ0, wi) = min � 1,P(ti0, t−i, w)P(wi |ti, θ0) , P(ti, t−i, w)P(wi|ti0, θ0) in which the normalization factors 1/Z are cancelled out. We choose θ0 to be the parameter expectation based on the current observations, i.e. θ0 = E [θ|w, t−i], so that the proposal distribution is close to the true distribution. This sampling algorithm with a changing proposal distribution has been shown to work well in practice (Johnson and Griffiths, 2007; Cohn et al., 2010; Naseem and Barzilay, 2011). The algorithm pseudo code is shown in Algorithm 1. To sample from the proposal distribution (2) efficiently, we implement a dynamic programming algorithm which calculates marginal probabilities of all subtrees. The algorithm works similarly to the inside algorithm (Baker, 1979), except that we do not assume the tree is binary. We therefore perform one additional dynamic programming step that sums over all possible segmentations of each span. Once the algorithm obtains the marginal probabilities of all subtrees, a specification tree can be drawn </context>
</contexts>
<marker>Johnson, Griffiths, 2007</marker>
<rawString>Mark Johnson and Thomas L. Griffiths. 2007. Bayesian inference for pcfgs via markov chain monte carlo. In Proceedings of the North American Conference on Computational Linguistics (NAACL ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen C Johnson</author>
</authors>
<title>Yacc: Yet another compiler-compiler.</title>
<date>1979</date>
<journal>Unix Programmer’s Manual,</journal>
<volume>vol</volume>
<pages>2</pages>
<contexts>
<context position="12197" citStr="Johnson, 1979" startWordPosition="1954" endWordPosition="1955">text specification represented as a sequence of noun phrases {wik}. We use UIUC shallow parser to preprocess each text specificaton into a sequence of the noun phrases.4 In addition, we are given a set of input examples for each wi. We use these examples to test the generated input parsers to re3Specifically, the specification tree is first translated into the grammar using a set of rules and seed words that identifies basic data types such as int. Our implementation then generates a top-down parser since the generated grammar is simple. In general, standard techniques such as Bison and Yacc (Johnson, 1979) can generate bottom-up parsers given such grammar. 4http://cogcomp.cs.illinois.edu/demo/shallowparse/?id=7 1296 ject incorrect predictions made by our probabilistic model. We formalize the learning problem as a dependency parsing and role labeling problem. Our model predicts specification trees t = {t1, · · · , tN} for the text specifications, where each specification tree tz is a dependency tree over noun phrases {wzk}. In general many program input formats are nested tree structures, in which the tree root denotes the entire chunk of program input data and each chunk (tree node) can be furt</context>
</contexts>
<marker>Johnson, 1979</marker>
<rawString>Stephen C. Johnson. 1979. Yacc: Yet another compiler-compiler. Unix Programmer’s Manual, vol 2b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language semantics from ambiguous supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd national conference on Artificial intelligence -</booktitle>
<volume>1</volume>
<pages>07</pages>
<contexts>
<context position="9201" citStr="Kate and Mooney, 2007" startWordPosition="1463" endWordPosition="1466">in this tree in order to give a clear view of the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decision</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Proceedings of the 22nd national conference on Artificial intelligence - Volume 1, AAAI’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</booktitle>
<contexts>
<context position="9417" citStr="Liang et al., 2009" startWordPosition="1501" endWordPosition="1504">m with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previou</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2209" citStr="Liang et al., 2011" startWordPosition="324" endWordPosition="327"> 1The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p Figure 1: An example of (a) one natural language specification describing program input data; (b) the corresponding specification tree representing the program input structure; and (c) two input examples however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates (b) Specification Tree: the input a single integer T test cases an integer N the next N lines N characters (c) Two Program Input Examples: 1 10 YYWYYWWWWW YWWWYWWWWW YYWYYWWWWW ... WWWWWWWWWW 2 1 Y 5 YWYWW ... WWYYY (a) Text Specification: The input contains a s</context>
<context position="9269" citStr="Liang et al., 2011" startWordPosition="1476" endWordPosition="1479">e); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framewor</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Using semantic cues to learn syntax.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="19158" citStr="Naseem and Barzilay, 2011" startWordPosition="3128" endWordPosition="3131">ifically, ti0 will be accepted to replace the last ti with probability: R(ti, ti0) = min � 1, P(ti0|w, t−i) Q(tiil θ0, wi) P(ti |w, t−i) Q(ti 1θ0, wi) = min � 1,P(ti0, t−i, w)P(wi |ti, θ0) , P(ti, t−i, w)P(wi|ti0, θ0) in which the normalization factors 1/Z are cancelled out. We choose θ0 to be the parameter expectation based on the current observations, i.e. θ0 = E [θ|w, t−i], so that the proposal distribution is close to the true distribution. This sampling algorithm with a changing proposal distribution has been shown to work well in practice (Johnson and Griffiths, 2007; Cohn et al., 2010; Naseem and Barzilay, 2011). The algorithm pseudo code is shown in Algorithm 1. To sample from the proposal distribution (2) efficiently, we implement a dynamic programming algorithm which calculates marginal probabilities of all subtrees. The algorithm works similarly to the inside algorithm (Baker, 1979), except that we do not assume the tree is binary. We therefore perform one additional dynamic programming step that sums over all possible segmentations of each span. Once the algorithm obtains the marginal probabilities of all subtrees, a specification tree can be drawn recursively in a top-down manner. Calculating P</context>
</contexts>
<marker>Naseem, Barzilay, 2011</marker>
<rawString>Tahira Naseem and Regina Barzilay. 2011. Using semantic cues to learn syntax. In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Pandita</author>
<author>Xusheng Xiao</author>
<author>Hao Zhong</author>
<author>Tao Xie</author>
<author>Stephen Oney</author>
<author>Amit Paradkar</author>
</authors>
<title>Inferring method specifications from natural language api descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Conference on Software Engineering, ICSE 2012,</booktitle>
<pages>815--825</pages>
<publisher>IEEE Press.</publisher>
<location>Piscataway, NJ, USA.</location>
<contexts>
<context position="10500" citStr="Pandita et al., 2012" startWordPosition="1668" endWordPosition="1671">el learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering Researchers have recently developed a number of approaches that apply natural language processing techniques to software engineering problems. Examples include analyzing API documents to infer API library specifications (Zhong et al., 2009; Pandita et al., 2012) and analyzing code comments to detect concurrency bugs (Tan et al., 2007; Tan et al., 2011). This research analyzes natural language in documentation or comments to better understand existing application programs. Our mechanism, in contrast, automatically generates parser programs from natural language input format descriptions. 3 Problem Formulation The task of translating text specifications to input parsers consists of two steps, as shown in Figure 3. First, given a text specification describing an input format, we wish to infer a parse tree (which we call a specification tree) implied by </context>
</contexts>
<marker>Pandita, Xiao, Zhong, Xie, Oney, Paradkar, 2012</marker>
<rawString>Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie, Stephen Oney, and Amit Paradkar. 2012. Inferring method specifications from natural language api descriptions. In Proceedings of the 2012 International Conference on Software Engineering, ICSE 2012, pages 815–825, Piscataway, NJ, USA. IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<contexts>
<context position="2188" citStr="Poon and Domingos, 2009" startWordPosition="320" endWordPosition="323">anguages. In recent years 1The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p Figure 1: An example of (a) one natural language specification describing program input data; (b) the corresponding specification tree representing the program input structure; and (c) two input examples however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates (b) Specification Tree: the input a single integer T test cases an integer N the next N lines N characters (c) Two Program Input Examples: 1 10 YYWYYWWWWW YWWWYWWWWW YYWYYWWWWW ... WWWWWWWWWW 2 1 Y 5 YWYWW ... WWYYY (a) Text Specification: T</context>
<context position="9249" citStr="Poon and Domingos, 2009" startWordPosition="1471" endWordPosition="1475">the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforceme</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Tan</author>
<author>Ding Yuan</author>
<author>Gopal Krishna</author>
<author>Yuanyuan Zhou</author>
</authors>
<title>iComment: Bugs or bad comments? */.</title>
<date>2007</date>
<booktitle>In Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP07),</booktitle>
<contexts>
<context position="10573" citStr="Tan et al., 2007" startWordPosition="1681" endWordPosition="1684">way our model incorporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering Researchers have recently developed a number of approaches that apply natural language processing techniques to software engineering problems. Examples include analyzing API documents to infer API library specifications (Zhong et al., 2009; Pandita et al., 2012) and analyzing code comments to detect concurrency bugs (Tan et al., 2007; Tan et al., 2011). This research analyzes natural language in documentation or comments to better understand existing application programs. Our mechanism, in contrast, automatically generates parser programs from natural language input format descriptions. 3 Problem Formulation The task of translating text specifications to input parsers consists of two steps, as shown in Figure 3. First, given a text specification describing an input format, we wish to infer a parse tree (which we call a specification tree) implied by the text. Second, we convert each specification tree into formal grammar </context>
</contexts>
<marker>Tan, Yuan, Krishna, Zhou, 2007</marker>
<rawString>Lin Tan, Ding Yuan, Gopal Krishna, and Yuanyuan Zhou. 2007. /* iComment: Bugs or bad comments? */. In Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP07), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Tan</author>
<author>Yuanyuan Zhou</author>
<author>Yoann Padioleau</author>
</authors>
<title>aComment: Mining annotations from comments and code to detect interrupt-related concurrency bugs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd International Conference on Software Engineering (ICSE11),</booktitle>
<contexts>
<context position="10592" citStr="Tan et al., 2011" startWordPosition="1685" endWordPosition="1688">rporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering Researchers have recently developed a number of approaches that apply natural language processing techniques to software engineering problems. Examples include analyzing API documents to infer API library specifications (Zhong et al., 2009; Pandita et al., 2012) and analyzing code comments to detect concurrency bugs (Tan et al., 2007; Tan et al., 2011). This research analyzes natural language in documentation or comments to better understand existing application programs. Our mechanism, in contrast, automatically generates parser programs from natural language input format descriptions. 3 Problem Formulation The task of translating text specifications to input parsers consists of two steps, as shown in Figure 3. First, given a text specification describing an input format, we wish to infer a parse tree (which we call a specification tree) implied by the text. Second, we convert each specification tree into formal grammar of the input format</context>
</contexts>
<marker>Tan, Zhou, Padioleau, 2011</marker>
<rawString>Lin Tan, Yuanyuan Zhou, and Yoann Padioleau. 2011. aComment: Mining annotations from comments and code to detect interrupt-related concurrency bugs. In Proceedings of the 33rd International Conference on Software Engineering (ICSE11), May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Automated construction of database interfaces: integrating statistical and relational learning for semantic parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the conference on Empirical Methods in Natural Language Processing, EMNLP ’00.</booktitle>
<contexts>
<context position="9147" citStr="Tang and Mooney, 2000" startWordPosition="1455" endWordPosition="1458">nput format structure (we omit the background phrases in this tree in order to give a clear view of the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et a</context>
</contexts>
<marker>Tang, Mooney, 2000</marker>
<rawString>Lappoon R. Tang and Raymond J. Mooney. 2000. Automated construction of database interfaces: integrating statistical and relational learning for semantic parsing. In Proceedings of the conference on Empirical Methods in Natural Language Processing, EMNLP ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2132" citStr="Wong and Mooney, 2007" startWordPosition="312" endWordPosition="315">unsuccessful versions of standard formal programming languages. In recent years 1The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p Figure 1: An example of (a) one natural language specification describing program input data; (b) the corresponding specification tree representing the program input structure; and (c) two input examples however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates (b) Specification Tree: the input a single integer T test cases an integer N the next N lines N characters (c) Two Program Input Examples: 1 10 YYWYYWWWWW YWWWYWWWWW YYWYYWWWWW ... WWWW</context>
<context position="9224" citStr="Wong and Mooney, 2007" startWordPosition="1467" endWordPosition="1470">o give a clear view of the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="9178" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1459" endWordPosition="1462">we omit the background phrases in this tree in order to give a clear view of the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI, pages 658–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2163" citStr="Zettlemoyer and Collins, 2009" startWordPosition="316" endWordPosition="319">f standard formal programming languages. In recent years 1The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p Figure 1: An example of (a) one natural language specification describing program input data; (b) the corresponding specification tree representing the program input structure; and (c) two input examples however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates (b) Specification Tree: the input a single integer T test cases an integer N the next N lines N characters (c) Two Program Input Examples: 1 10 YYWYYWWWWW YWWWYWWWWW YYWYYWWWWW ... WWWWWWWWWW 2 1 Y 5 YWYWW ... WWYYY </context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhong</author>
<author>Lu Zhang</author>
<author>Tao Xie</author>
<author>Hong Mei</author>
</authors>
<title>Inferring resource specifications from natural language api documentation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering, ASE ’09,</booktitle>
<pages>307--318</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="10477" citStr="Zhong et al., 2009" startWordPosition="1664" endWordPosition="1667">g framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering Researchers have recently developed a number of approaches that apply natural language processing techniques to software engineering problems. Examples include analyzing API documents to infer API library specifications (Zhong et al., 2009; Pandita et al., 2012) and analyzing code comments to detect concurrency bugs (Tan et al., 2007; Tan et al., 2011). This research analyzes natural language in documentation or comments to better understand existing application programs. Our mechanism, in contrast, automatically generates parser programs from natural language input format descriptions. 3 Problem Formulation The task of translating text specifications to input parsers consists of two steps, as shown in Figure 3. First, given a text specification describing an input format, we wish to infer a parse tree (which we call a specific</context>
</contexts>
<marker>Zhong, Zhang, Xie, Mei, 2009</marker>
<rawString>Hao Zhong, Lu Zhang, Tao Xie, and Hong Mei. 2009. Inferring resource specifications from natural language api documentation. In Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering, ASE ’09, pages 307– 318, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>