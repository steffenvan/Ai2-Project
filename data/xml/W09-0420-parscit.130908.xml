<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051070">
<title confidence="0.9622705">
Experiments in morphosyntactic processing for translating to and from
German
</title>
<author confidence="0.98471">
Alexander Fraser
</author>
<affiliation confidence="0.993904">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.992652">
fraser@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993784" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885625">
We describe two shared task systems and
associated experiments. The German to
English system used reordering rules ap-
plied to parses and morphological split-
ting and stemming. The English to Ger-
man system used an additional translation
step which recreated compound words and
generated morphological inflection.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911666666667">
The Institute for Natural Language Processing
(IfNLP), Stuttgart, participated in the WMT-2009
shared tasks for German to English and English
to German translation with constrained systems
which employed morphological and syntactic pro-
cessing techniques. The systems were based on
the open source Moses docoder (Koehn et al.,
2007). We combined IfNLP tools for syntactic and
morphological analysis (which are publicly avail-
able and widely used) with preprocessing tech-
niques that were successfully used by other groups
in WMT-2008, and extended these. For English to
German translation, we additionally performed a
step which recreated compound words and gener-
ated morphological inflection.
</bodyText>
<subsectionHeader confidence="0.959134">
1.1 Baseline
</subsectionHeader>
<bodyText confidence="0.9995642">
The baseline is the standard system supplied for
the shared task. We used the default parameters
of the Moses toolkit, except for a small difference
in the generation of the word alignments, see sec-
tion 3.
</bodyText>
<sectionHeader confidence="0.999156" genericHeader="introduction">
2 Improvements
</sectionHeader>
<subsectionHeader confidence="0.963012">
2.1 Character Normalization
</subsectionHeader>
<bodyText confidence="0.999888666666667">
We normalize both the English and German by
converting all characters to their nearest equivalent
in Latin-1 (ISO 8859-1) encoding1, except for the
euro sign, which is handled specially. We did not
modify the SGML files used for calculating BLEU
and METEOR scores in any way.
</bodyText>
<subsectionHeader confidence="0.99186">
2.2 German Writing Reform
</subsectionHeader>
<bodyText confidence="0.999574322580645">
German underwent a writing reform from the alte
Rechtschreibung (old spelling rules/orthography)
to the neue Rechtschreibung (gloss: new spelling
rules/orthography) recently. Early Europarl
data are written using the alte Rechtschreibung
and hence need to be converted to the neue
Rechtschreibung in order to match the news data,
which is in the new form.
We began the process by mapping all cased vari-
ants of a particular word to a single class (such
as by mapping two words which are written with
ue and ¨u, but are otherwise identical, to a single
class). We then tried to automatically identify the
correct variant under the writing reform for each
class. Initially we tried the linux tool aspell but
found that its coverage (the recall of its lexicon)
was poor.
We used a simple technique for finding the best
variant. We separated the Europarl corpus into
portions written using the old and new forms. We
used the incidence of the word dass (the comple-
mentizer meaning that) and its old rules variant
daf3. We used a chunk size of 70 sentences to
segment Europarl into old and new by counting
whether there were more instances of daf3 or dass,
respectively, in each chunk. We added the news
corpora to the new portion. For each variant we
counted the number of times it occurred in the
new data and subtracted the number of times it oc-
curred in the old data; the variant with the highest
adjusted count was selected.
</bodyText>
<footnote confidence="0.9960855">
1Latin-1 is an 8-bit encoding which has the common ac-
cented characters used in Western European languages. A
reviewer pointed out that ISO 8859-15 has superseded ISO
8859-1.
</footnote>
<note confidence="0.4979535">
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99815">
115
</page>
<subsectionHeader confidence="0.998861">
2.3 Reordering German
</subsectionHeader>
<bodyText confidence="0.99323054">
German word order differs from English substan-
tially. Preprocessing approaches involving the
use of a syntactic parse of the source sentence to
change the word order to more closely match the
word order of the target language have been stud-
ied by Niessen and Ney (2004), Xia and McCord
(2004), Dr´abek and Yarowsky (2004), Collins et
al. (2005), Popovi´c and Ney (2006), Wang et al.
(2007) and many others.
To obtain a parse of each German sentence in
the training, dev and test corpora, we employed the
IfNLP BitPar probabilistic parser (Schmid, 2004),
using models learned from the Tiger Treebank for
German.
Dealing with morphological productivity is im-
portant in the syntactic parsing of German. Bit-
Par has been designed with this in mind. IfNLP’s
SMOR analyzer is used for morphological analy-
sis (Schmid et al., 2004). SMOR is run over a list
of types in each German sentence, and outputs a
list of analyses for each type, each of which corre-
sponds to a POS tag. BitPar is limited to choosing
one of these POS tags for this type. Words which
SMOR fails to analyze are allowed to occur with
any POS tag.
We reimplemented the syntactic preprocessing
approach of Collins et al. (2005), with modifica-
tions. Reordering rules are applied to a German
parse tree (generated by BitPar), and focus on re-
ordering the words in the German clause structure
to more closely resemble English clause structure.
The rules are applied to both the training data for
the SMT system, and the input (the dev and test
sets). We previously performed an error analysis
of this approach and for the work described here
we addressed some of the shortcomings identified
through the analysis. The analysis was performed
on the Europarl dev2006 set.
The first error that we noticed occurring fre-
quently was that some large clausal units which
were labeled as subjects were being moved for-
ward in the sentence. We modified the rule moving
subjects forward to not apply to the constituents S,
CS, VP and CVP. See the first part of table 3 for an
example. The phrase “dass der Balkan ist kein Ge-
biet” is moved under the original rules, and with
the modification is no longer moved2.
2Note that there is an unrelated reordering error at the end
of the sentence for both BEFORE and AFTER, gibt (gloss:
gives) should have moved to follow das (gloss: that).
</bodyText>
<table confidence="0.8506062">
System BLEU METEOR LR
no processing 18.91 49.50 1.0097
c+w 19.37 49.69 1.0067
c+w, s/s 19.18 51.13 1.0035
c+w, old reordering 19.61 50.44 1.0092
c+w, new reordering 19.91 50.84 1.0059
c+w, new reordering, s/s 19.65 51.57 1.0093
(submitted, bug)
* c+w, new reordering, s/s 19.73 51.59 1.0062
as * IRSTLM quantized 19.52 51.33 1.0003
as * IRSTLM 19.75 51.61 1.0013
as * IRSTLM 21.2 quan- 19.52 51.51 1.0095
tized
as * RANDLM 19.67 51.73 1.0067
as * RANDLM 21.2 21.03 51.96 1.0111
</table>
<tableCaption confidence="0.842475666666667">
Table 1: German to English, dev-2009b (case
sensitive), c+w = char+word normalization, s/s =
splitting/stemming, 21.2 = larger LM
</tableCaption>
<table confidence="0.9187065">
System BLEU METEOR LR
no processing 13.55 38.31 0.9910
c+w (no second step) 14.11 38.27 0.9991
c+w, s/s, second step 12.34 37.89 1.0338
(submitted, bug)
c+w, s/s, second step 13.05 37.94 1.0157
</table>
<tableCaption confidence="0.883474">
Table 2: English to German, dev-2009b (case
</tableCaption>
<bodyText confidence="0.989438090909091">
sensitive), c+w = char+word normalization, s/s =
splitting/stemming
The second error that we handled was that S-RC
constituents which do not have a complementizer
are reordered incorrectly. We modified the orig-
inal verb 2nd rule, so that if there is no comple-
mentizer in a S-RC constituent, then the head is
moved to the second position, see the second part
of table 3 for an example. Using the original rules,
the verb 2nd rule fails to fire, incorrectly leaving
haben (gloss: have) at the end of the clause.
</bodyText>
<subsectionHeader confidence="0.997079">
2.4 Morphological Decomposition
</subsectionHeader>
<bodyText confidence="0.999874933333333">
We implemented the frequency-based word split-
ting approach of Koehn and Knight (2003), and
made modifications, including some similar to
those described by Stymne et al. (2008). This
well-known technique splits compound words. In
addition, we performed simple suffix elimination,
aimed at removing inflection marking features
such as gender and case that are not necessary for
translation to English. We took the stem combi-
nation with the highest geometric mean of the fre-
quencies of the stems, but following Stymne et al.
(2008), we restricted stems to minimum length 4,
and we allowed an extended list of infixes: s, n,
en, nen, es, er and ien. For suffixes, we allowed:
e, en, n, es, s, em and er, which is more aggressive
</bodyText>
<page confidence="0.98987">
116
</page>
<table confidence="0.999835921052632">
INPUT Mir ist bewusst , dass der Balkan kein
gloss Gebiet ist , das Anlass zu Optimismus
gibt.
me is clear, that the Balkans not area is
, that opportunity for optimism gives.
BEFORE Mir dass der Balkan ist kein Gebiet ist
gloss bewusst , , das Anlass zu Optimismus
gibt.
me that the Balkans is not area is clear,
that opportunity for optimism gives.
AFTER Mir ist bewusst , dass der Balkan ist kein
gloss Gebiet , das Anlass zu Optimismus gibt
.
me is clear, that the Balkans is not area
, that opportunity for optimism gives.
REF I am aware that the Balkans are not the
most promising area for optimism.
INPUT Am 23. November 1999 hat ein Partner-
gloss schaftstag stattgefunden , an dem viele
von uns teilgenommen haben .
on 23 November 1999 have a
partnership-day took-place , in which
many of us participated have.
BEFORE Am 23. November 1999 ein Partner-
gloss schaftstag hat stattgefunden , an dem
teilgenommen viele von uns haben .
on 23 November 1999 a partnership-day
have took-place , in which participated
many of us have.
AFTER Am 23. November 1999 ein Partner-
gloss schaftstag hat stattgefunden , an dem
viele von uns haben teilgenommen .
on 23 November 1999 a partnership-day
have took-place , in which many of us
have participated.
REF A partnership day was held on 23
November 1999 , in which many of us
participated.
</table>
<tableCaption confidence="0.997238">
Table 3: Differences in reordering: BEFORE is re-
</tableCaption>
<bodyText confidence="0.958369818181818">
ordering using rules in (Collins et al., 2005), AF-
TER is our modified reordering
than used in previous work (and therefore gener-
alizes more but at the same time causes some er-
roneous conflation). We stripped e, en and n from
all stems (but remembered the most frequent vari-
ant, so that applying the procedure to Kirchturm
results in Kirche Turm (gloss: church tower)). We
store an alignment from the original German to the
simplified German which we will use in the next
section.
</bodyText>
<subsectionHeader confidence="0.988903">
2.5 Morphological Generation
</subsectionHeader>
<bodyText confidence="0.999753">
For translation from English to German, we first
translated from English to the simplified German
presented in the previous section, and then per-
formed an independent translation step from sim-
plified German to fully inflected German.
Two processes are handled by this step. First,
series of stems corresponding to compound words
are recomposed (along with infixes which are not
present in the simplified German form) into com-
pound words. Second, inflection is added (e.g.,
case and gender agreement is handled). Both of
these processes are implemented using a Moses
system trained on a parallel corpus where the
source language is simplified German and the tar-
get language is fully inflected German. The align-
ment is error-free as it was generated as a side
effect of the splitting and stemming process de-
scribed in the previous section. In translation, re-
ordering is not allowed, but we otherwise use stan-
dard Moses settings.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999398">
3.1 German to English
</subsectionHeader>
<bodyText confidence="0.999694419354839">
We trained our German to English system on the
constrained parallel data. The English data was
processed using character normalization. The Ger-
man data was first processed using character and
word (writing reform) normalization. We then
parsed the German data using BitPar and applied
the modified reordering rules. After this the split-
ting and stemming process was applied. Finally,
we lowercased the data.
Word alignments were generated using Model
4 (Brown et al., 1993) using the multi-threaded
implementation of GIZA++ (Och and Ney, 2003;
Gao and Vogel, 2008). We first trained Model 4
with English as the source language, and then with
German as the source language, resulting in two
Viterbi alignments3. The resulting Viterbi align-
ments were combined using the Grow Diag Final
And symmetrization heuristic (Koehn et al., 2003).
We estimated a standard Moses system using de-
fault settings. MERT was run until convergence
using dev-2009a (separately for each experiment).
One limitation of our German to English system
is that we were unable to scale to the full language
modeling data using SRILM (Stolcke, 2002), 5-
grams and modified Kneser-Ney with no single-
ton deletion4. The language model in our sub-
mitted system is based on all of the available En-
glish data, but news-train08 is truncated to the first
10193376 lines, meaning that we did not train on
the remaining 11038787 lines, so we used a little
less than half of the data. We converted the lan-
</bodyText>
<footnote confidence="0.9995755">
3We used 5 iterations of Model 1, 4 iterations of HMM
(Vogel et al., 1996) and 4 iterations of Model 4.
4SRILM failed when trained on the full data, even when a
machine with 32 GB RAM and 48 GB swap was used.
</footnote>
<page confidence="0.997734">
117
</page>
<bodyText confidence="0.999886567567568">
guage model trained using SRILM to the binary
format using IRSTLM.
Experiments are presented in table 1, using
BLEU (Papineni et al., 2001) and METEOR5
(Banerjee and Lavie, 2005), and we also show
the length ratio (ratio of hypothesized tokens to
reference tokens). For translation into English
METEOR had superior correlation with human
rankings to BLEU at WMT 2008 (Callison-Burch
et al., 2008). Our submitted system had a bug
where the environment variable LC ALL was set
to en US when creating the binarized filtered lex-
icalized reordering table for the test set (and for
the blindtest set, but not for the dev set used for
MERT). This caused minor degradation, see the
system marked (*) for the system with the bug cor-
rected.
Each system increases in both BLEU and ME-
TEOR as improvements are added. An exception
is that splitting/stemming decreases BLEU some-
what. However, we trust the METEOR results
more due to their better correlation with human
judgements.
We also compared using a different language
model instead of the SRILM model (the bottom
half of table 1). These used either the reduced
English language modeling data or the full data
(21.2 M segments, marked 21.2 in the results).
RANDLM (Talbot and Osborne, 2007) performs
well and scaled to the full data with improvement
(resulting in our best overall system). IRSTLM
(Federico and Cettolo, 2007) also performs well,
but the quantized model on the 21.2 data did
not improve over the smaller quantized model6.
IRSTLM uses an approximation of Witten-Bell
smoothing, our results support that this is compet-
itive.
</bodyText>
<subsectionHeader confidence="0.999748">
3.2 English to German
</subsectionHeader>
<bodyText confidence="0.999497888888889">
We trained our English to German system on the
constrained parallel data. The first SMT system
translates from lowercased English to lowercased
simplified German, which is then recased. The
syntactic reordering process is not used, but other-
wise the German data is processed identically. The
alignment from simplified German to English is
generated as described in the previous section. We
used all of the German data to train the language
</bodyText>
<footnote confidence="0.9907715">
5METEOR used default weights, stemming and Wordnet
synsets.
6After speaking with the authors, we plan to try IRSTLM
on the full data using memory mapping for binarization.
</footnote>
<bodyText confidence="0.9994523">
model on simplified German. The second SMT
system translates mixed case simplified German to
mixed case unsimplified German. The translation
model is built only on the simplified German from
the parallel text, and the language model is trained
on all German data.
We present the results in table 2. METEOR7 did
not correlate as well as BLEU for translation out of
English in WMT 2008. The BLEU score of our fi-
nal system is worse than the baseline. We had cho-
sen to submit this system as we found it more in-
teresting than submitting a vanilla system. In addi-
tion, the system of Stymne et al. (2008) received a
good human evaluation despite having a relatively
low BLEU score, and we hoped we were perform-
ing similar morphological generalization. We ex-
pect to be able to improve this system through er-
ror analysis. In an initial inspection we found case
mismatching problems between step one and step
two.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987133333333">
We presented our German to English system
which employed character normalization, com-
pensated for problems caused by the German writ-
ing reform, used modified syntactic reordering
rules (in combination with morphologically aware
parsing), and employed substring-based morpho-
logical analysis. Our best system improves by
2.46 METEOR and 1.12 BLEU over a standard
Moses system. Our English to German sys-
tem used the same two normalizations and the
substring-based morphological analysis, and addi-
tionally implemented a second translation step for
recreating compound words and generating case
and gender inflection. We will improve this sys-
tem in future work.
</bodyText>
<sectionHeader confidence="0.994749" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.871084">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization
at the 43th Annual Meeting of the Association of
Computational Linguistics (ACL-2005), Ann Arbor,
Michigan.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
</reference>
<footnote confidence="0.758385">
7METEOR for this task is calculated using default
weights but no Wordnet synsets.
</footnote>
<page confidence="0.987319">
118
</page>
<reference confidence="0.998280705882353">
estimation. Computational Linguistics, 19(2):263–
311.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
ACL Third Workshop on Statistical Machine Trans-
lation, Columbus, Ohio.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In ACL, pages 531–540, Ann Arbor, MI.
Elliott F. Dr´abek and David Yarowsky. 2004. Improv-
ing bitext word alignments via syntax-based reorder-
ing of English. In The Companion Volume to the
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 146–
149, Barcelona, Spain.
Marcello Federico and Mauro Cettolo. 2007. Efficient
handling of n-gram language models for statistical
machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
88–95, Prague, Czech Republic.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49–57.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL, pages
187–193, Morristown, NJ.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127–133, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Program, Prague, Czech Re-
public.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntactic information. Computational Lin-
guistics, 30(2):181–204.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY.
Maja Popovi´c and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation.
In LREC, pages 1278–1283, Genoa, Italy.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: a German computational morphology
covering derivation, composition, and inflection. In
LREC, pages 1263–1266, Lisbon, Portugal.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING, Geneva, Switzerland.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135–138, Columbus, Ohio.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In ACL, pages 512–519, Prague, Czech Re-
public.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING, pages 836–841, Copen-
hagen, Denmark.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In EMNLP-CONLL, pages
737–745.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING.
</reference>
<page confidence="0.999091">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.513554">
<title confidence="0.770839">Experiments in morphosyntactic processing for translating to and from German</title>
<author confidence="0.928891">Alexander</author>
<affiliation confidence="0.999213">Institute for Natural Language University of</affiliation>
<email confidence="0.985511">fraser@ims.uni-stuttgart.de</email>
<abstract confidence="0.995407888888889">We describe two shared task systems and associated experiments. The German to English system used reordering rules applied to parses and morphological splitting and stemming. The English to German system used an additional translation step which recreated compound words and generated morphological inflection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005),</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="12724" citStr="Banerjee and Lavie, 2005" startWordPosition="2122" endWordPosition="2125">based on all of the available English data, but news-train08 is truncated to the first 10193376 lines, meaning that we did not train on the remaining 11038787 lines, so we used a little less than half of the data. We converted the lan3We used 5 iterations of Model 1, 4 iterations of HMM (Vogel et al., 1996) and 4 iterations of Model 4. 4SRILM failed when trained on the full data, even when a machine with 32 GB RAM and 48 GB swap was used. 117 guage model trained using SRILM to the binary format using IRSTLM. Experiments are presented in table 1, using BLEU (Papineni et al., 2001) and METEOR5 (Banerjee and Lavie, 2005), and we also show the length ratio (ratio of hypothesized tokens to reference tokens). For translation into English METEOR had superior correlation with human rankings to BLEU at WMT 2008 (Callison-Burch et al., 2008). Our submitted system had a bug where the environment variable LC ALL was set to en US when creating the binarized filtered lexicalized reordering table for the test set (and for the blindtest set, but not for the dev set used for MERT). This caused minor degradation, see the system marked (*) for the system with the bug corrected. Each system increases in both BLEU and METEOR a</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005), Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="11344" citStr="Brown et al., 1993" startWordPosition="1883" endWordPosition="1886">he previous section. In translation, reordering is not allowed, but we otherwise use standard Moses settings. 3 Experiments 3.1 German to English We trained our German to English system on the constrained parallel data. The English data was processed using character normalization. The German data was first processed using character and word (writing reform) normalization. We then parsed the German data using BitPar and applied the modified reordering rules. After this the splitting and stemming process was applied. Finally, we lowercased the data. Word alignments were generated using Model 4 (Brown et al., 1993) using the multi-threaded implementation of GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). We first trained Model 4 with English as the source language, and then with German as the source language, resulting in two Viterbi alignments3. The resulting Viterbi alignments were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). We estimated a standard Moses system using default settings. MERT was run until convergence using dev-2009a (separately for each experiment). One limitation of our German to English system is that we were unable to scale to the full langu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In ACL Third Workshop on Statistical Machine Translation,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="12942" citStr="Callison-Burch et al., 2008" startWordPosition="2156" endWordPosition="2159">e converted the lan3We used 5 iterations of Model 1, 4 iterations of HMM (Vogel et al., 1996) and 4 iterations of Model 4. 4SRILM failed when trained on the full data, even when a machine with 32 GB RAM and 48 GB swap was used. 117 guage model trained using SRILM to the binary format using IRSTLM. Experiments are presented in table 1, using BLEU (Papineni et al., 2001) and METEOR5 (Banerjee and Lavie, 2005), and we also show the length ratio (ratio of hypothesized tokens to reference tokens). For translation into English METEOR had superior correlation with human rankings to BLEU at WMT 2008 (Callison-Burch et al., 2008). Our submitted system had a bug where the environment variable LC ALL was set to en US when creating the binarized filtered lexicalized reordering table for the test set (and for the blindtest set, but not for the dev set used for MERT). This caused minor degradation, see the system marked (*) for the system with the bug corrected. Each system increases in both BLEU and METEOR as improvements are added. An exception is that splitting/stemming decreases BLEU somewhat. However, we trust the METEOR results more due to their better correlation with human judgements. We also compared using a diffe</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In ACL Third Workshop on Statistical Machine Translation, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, MI.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In ACL, pages 531–540, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliott F Dr´abek</author>
<author>David Yarowsky</author>
</authors>
<title>Improving bitext word alignments via syntax-based reordering of English.</title>
<date>2004</date>
<booktitle>In The Companion Volume to the Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>146--149</pages>
<location>Barcelona,</location>
<marker>Dr´abek, Yarowsky, 2004</marker>
<rawString>Elliott F. Dr´abek and David Yarowsky. 2004. Improving bitext word alignments via syntax-based reordering of English. In The Companion Volume to the Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 146– 149, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Mauro Cettolo</author>
</authors>
<title>Efficient handling of n-gram language models for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>88--95</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="13912" citStr="Federico and Cettolo, 2007" startWordPosition="2320" endWordPosition="2323">tem increases in both BLEU and METEOR as improvements are added. An exception is that splitting/stemming decreases BLEU somewhat. However, we trust the METEOR results more due to their better correlation with human judgements. We also compared using a different language model instead of the SRILM model (the bottom half of table 1). These used either the reduced English language modeling data or the full data (21.2 M segments, marked 21.2 in the results). RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). IRSTLM (Federico and Cettolo, 2007) also performs well, but the quantized model on the 21.2 data did not improve over the smaller quantized model6. IRSTLM uses an approximation of Witten-Bell smoothing, our results support that this is competitive. 3.2 English to German We trained our English to German system on the constrained parallel data. The first SMT system translates from lowercased English to lowercased simplified German, which is then recased. The syntactic reordering process is not used, but otherwise the German data is processed identically. The alignment from simplified German to English is generated as described in</context>
</contexts>
<marker>Federico, Cettolo, 2007</marker>
<rawString>Marcello Federico and Mauro Cettolo. 2007. Efficient handling of n-gram language models for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 88–95, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="11435" citStr="Gao and Vogel, 2008" startWordPosition="1897" endWordPosition="1900">ard Moses settings. 3 Experiments 3.1 German to English We trained our German to English system on the constrained parallel data. The English data was processed using character normalization. The German data was first processed using character and word (writing reform) normalization. We then parsed the German data using BitPar and applied the modified reordering rules. After this the splitting and stemming process was applied. Finally, we lowercased the data. Word alignments were generated using Model 4 (Brown et al., 1993) using the multi-threaded implementation of GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). We first trained Model 4 with English as the source language, and then with German as the source language, resulting in two Viterbi alignments3. The resulting Viterbi alignments were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). We estimated a standard Moses system using default settings. MERT was run until convergence using dev-2009a (separately for each experiment). One limitation of our German to English system is that we were unable to scale to the full language modeling data using SRILM (Stolcke, 2002), 5- grams and modified Kneser-Ney with no sin</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting. In</title>
<date>2003</date>
<booktitle>EACL,</booktitle>
<pages>187--193</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="7376" citStr="Koehn and Knight (2003)" startWordPosition="1208" endWordPosition="1211">itive), c+w = char+word normalization, s/s = splitting/stemming The second error that we handled was that S-RC constituents which do not have a complementizer are reordered incorrectly. We modified the original verb 2nd rule, so that if there is no complementizer in a S-RC constituent, then the head is moved to the second position, see the second part of table 3 for an example. Using the original rules, the verb 2nd rule fails to fire, incorrectly leaving haben (gloss: have) at the end of the clause. 2.4 Morphological Decomposition We implemented the frequency-based word splitting approach of Koehn and Knight (2003), and made modifications, including some similar to those described by Stymne et al. (2008). This well-known technique splits compound words. In addition, we performed simple suffix elimination, aimed at removing inflection marking features such as gender and case that are not necessary for translation to English. We took the stem combination with the highest geometric mean of the frequencies of the stems, but following Stymne et al. (2008), we restricted stems to minimum length 4, and we allowed an extended list of infixes: s, n, en, nen, es, er and ien. For suffixes, we allowed: e, en, n, es</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In EACL, pages 187–193, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="11704" citStr="Koehn et al., 2003" startWordPosition="1940" endWordPosition="1943">normalization. We then parsed the German data using BitPar and applied the modified reordering rules. After this the splitting and stemming process was applied. Finally, we lowercased the data. Word alignments were generated using Model 4 (Brown et al., 1993) using the multi-threaded implementation of GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). We first trained Model 4 with English as the source language, and then with German as the source language, resulting in two Viterbi alignments3. The resulting Viterbi alignments were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). We estimated a standard Moses system using default settings. MERT was run until convergence using dev-2009a (separately for each experiment). One limitation of our German to English system is that we were unable to scale to the full language modeling data using SRILM (Stolcke, 2002), 5- grams and modified Kneser-Ney with no singleton deletion4. The language model in our submitted system is based on all of the available English data, but news-train08 is truncated to the first 10193376 lines, meaning that we did not train on the remaining 11038787 lines, so we used a little less than half of t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, Demonstration Program,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="854" citStr="Koehn et al., 2007" startWordPosition="114" endWordPosition="117">d associated experiments. The German to English system used reordering rules applied to parses and morphological splitting and stemming. The English to German system used an additional translation step which recreated compound words and generated morphological inflection. 1 Introduction The Institute for Natural Language Processing (IfNLP), Stuttgart, participated in the WMT-2009 shared tasks for German to English and English to German translation with constrained systems which employed morphological and syntactic processing techniques. The systems were based on the open source Moses docoder (Koehn et al., 2007). We combined IfNLP tools for syntactic and morphological analysis (which are publicly available and widely used) with preprocessing techniques that were successfully used by other groups in WMT-2008, and extended these. For English to German translation, we additionally performed a step which recreated compound words and generated morphological inflection. 1.1 Baseline The baseline is the standard system supplied for the shared task. We used the default parameters of the Moses toolkit, except for a small difference in the generation of the word alignments, see section 3. 2 Improvements 2.1 Ch</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Demonstration Program, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morpho-syntactic information.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="3847" citStr="Niessen and Ney (2004)" startWordPosition="607" endWordPosition="610">oding which has the common accented characters used in Western European languages. A reviewer pointed out that ISO 8859-15 has superseded ISO 8859-1. Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 115 2.3 Reordering German German word order differs from English substantially. Preprocessing approaches involving the use of a syntactic parse of the source sentence to change the word order to more closely match the word order of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Dr´abek and Yarowsky (2004), Collins et al. (2005), Popovi´c and Ney (2006), Wang et al. (2007) and many others. To obtain a parse of each German sentence in the training, dev and test corpora, we employed the IfNLP BitPar probabilistic parser (Schmid, 2004), using models learned from the Tiger Treebank for German. Dealing with morphological productivity is important in the syntactic parsing of German. BitPar has been designed with this in mind. IfNLP’s SMOR analyzer is used for morphological analysis (Schmid et al., 2004). SMOR is run over a list of types in each Germ</context>
</contexts>
<marker>Niessen, Ney, 2004</marker>
<rawString>Sonja Niessen and Hermann Ney. 2004. Statistical machine translation with scarce resources using morpho-syntactic information. Computational Linguistics, 30(2):181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11413" citStr="Och and Ney, 2003" startWordPosition="1893" endWordPosition="1896">otherwise use standard Moses settings. 3 Experiments 3.1 German to English We trained our German to English system on the constrained parallel data. The English data was processed using character normalization. The German data was first processed using character and word (writing reform) normalization. We then parsed the German data using BitPar and applied the modified reordering rules. After this the splitting and stemming process was applied. Finally, we lowercased the data. Word alignments were generated using Model 4 (Brown et al., 1993) using the multi-threaded implementation of GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). We first trained Model 4 with English as the source language, and then with German as the source language, resulting in two Viterbi alignments3. The resulting Viterbi alignments were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). We estimated a standard Moses system using default settings. MERT was run until convergence using dev-2009a (separately for each experiment). One limitation of our German to English system is that we were unable to scale to the full language modeling data using SRILM (Stolcke, 2002), 5- grams and modified </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center, Yorktown Heights, NY.</location>
<contexts>
<context position="12685" citStr="Papineni et al., 2001" startWordPosition="2116" endWordPosition="2119">ge model in our submitted system is based on all of the available English data, but news-train08 is truncated to the first 10193376 lines, meaning that we did not train on the remaining 11038787 lines, so we used a little less than half of the data. We converted the lan3We used 5 iterations of Model 1, 4 iterations of HMM (Vogel et al., 1996) and 4 iterations of Model 4. 4SRILM failed when trained on the full data, even when a machine with 32 GB RAM and 48 GB swap was used. 117 guage model trained using SRILM to the binary format using IRSTLM. Experiments are presented in table 1, using BLEU (Papineni et al., 2001) and METEOR5 (Banerjee and Lavie, 2005), and we also show the length ratio (ratio of hypothesized tokens to reference tokens). For translation into English METEOR had superior correlation with human rankings to BLEU at WMT 2008 (Callison-Burch et al., 2008). Our submitted system had a bug where the environment variable LC ALL was set to en US when creating the binarized filtered lexicalized reordering table for the test set (and for the blindtest set, but not for the dev set used for MERT). This caused minor degradation, see the system marked (*) for the system with the bug corrected. Each sys</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>POS-based word reorderings for statistical machine translation.</title>
<date>2006</date>
<booktitle>In LREC,</booktitle>
<pages>1278--1283</pages>
<location>Genoa, Italy.</location>
<marker>Popovi´c, Ney, 2006</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2006. POS-based word reorderings for statistical machine translation. In LREC, pages 1278–1283, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Arne Fitschen</author>
<author>Ulrich Heid</author>
</authors>
<title>SMOR: a German computational morphology covering derivation, composition, and inflection.</title>
<date>2004</date>
<booktitle>In LREC,</booktitle>
<pages>1263--1266</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="4400" citStr="Schmid et al., 2004" startWordPosition="698" endWordPosition="701"> of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Dr´abek and Yarowsky (2004), Collins et al. (2005), Popovi´c and Ney (2006), Wang et al. (2007) and many others. To obtain a parse of each German sentence in the training, dev and test corpora, we employed the IfNLP BitPar probabilistic parser (Schmid, 2004), using models learned from the Tiger Treebank for German. Dealing with morphological productivity is important in the syntactic parsing of German. BitPar has been designed with this in mind. IfNLP’s SMOR analyzer is used for morphological analysis (Schmid et al., 2004). SMOR is run over a list of types in each German sentence, and outputs a list of analyses for each type, each of which corresponds to a POS tag. BitPar is limited to choosing one of these POS tags for this type. Words which SMOR fails to analyze are allowed to occur with any POS tag. We reimplemented the syntactic preprocessing approach of Collins et al. (2005), with modifications. Reordering rules are applied to a German parse tree (generated by BitPar), and focus on reordering the words in the German clause structure to more closely resemble English clause structure. The rules are applied t</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. SMOR: a German computational morphology covering derivation, composition, and inflection. In LREC, pages 1263–1266, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="4130" citStr="Schmid, 2004" startWordPosition="656" endWordPosition="657">tion for Computational Linguistics 115 2.3 Reordering German German word order differs from English substantially. Preprocessing approaches involving the use of a syntactic parse of the source sentence to change the word order to more closely match the word order of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Dr´abek and Yarowsky (2004), Collins et al. (2005), Popovi´c and Ney (2006), Wang et al. (2007) and many others. To obtain a parse of each German sentence in the training, dev and test corpora, we employed the IfNLP BitPar probabilistic parser (Schmid, 2004), using models learned from the Tiger Treebank for German. Dealing with morphological productivity is important in the syntactic parsing of German. BitPar has been designed with this in mind. IfNLP’s SMOR analyzer is used for morphological analysis (Schmid et al., 2004). SMOR is run over a list of types in each German sentence, and outputs a list of analyses for each type, each of which corresponds to a POS tag. BitPar is limited to choosing one of these POS tags for this type. Words which SMOR fails to analyze are allowed to occur with any POS tag. We reimplemented the syntactic preprocessing</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="11989" citStr="Stolcke, 2002" startWordPosition="1988" endWordPosition="1989">mentation of GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). We first trained Model 4 with English as the source language, and then with German as the source language, resulting in two Viterbi alignments3. The resulting Viterbi alignments were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). We estimated a standard Moses system using default settings. MERT was run until convergence using dev-2009a (separately for each experiment). One limitation of our German to English system is that we were unable to scale to the full language modeling data using SRILM (Stolcke, 2002), 5- grams and modified Kneser-Ney with no singleton deletion4. The language model in our submitted system is based on all of the available English data, but news-train08 is truncated to the first 10193376 lines, meaning that we did not train on the remaining 11038787 lines, so we used a little less than half of the data. We converted the lan3We used 5 iterations of Model 1, 4 iterations of HMM (Vogel et al., 1996) and 4 iterations of Model 4. 4SRILM failed when trained on the full data, even when a machine with 32 GB RAM and 48 GB swap was used. 117 guage model trained using SRILM to the bina</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Intl. Conf. Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Maria Holmqvist</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Effects of morphological analysis in translation between German and English.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>135--138</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="7467" citStr="Stymne et al. (2008)" startWordPosition="1222" endWordPosition="1225">ed was that S-RC constituents which do not have a complementizer are reordered incorrectly. We modified the original verb 2nd rule, so that if there is no complementizer in a S-RC constituent, then the head is moved to the second position, see the second part of table 3 for an example. Using the original rules, the verb 2nd rule fails to fire, incorrectly leaving haben (gloss: have) at the end of the clause. 2.4 Morphological Decomposition We implemented the frequency-based word splitting approach of Koehn and Knight (2003), and made modifications, including some similar to those described by Stymne et al. (2008). This well-known technique splits compound words. In addition, we performed simple suffix elimination, aimed at removing inflection marking features such as gender and case that are not necessary for translation to English. We took the stem combination with the highest geometric mean of the frequencies of the stems, but following Stymne et al. (2008), we restricted stems to minimum length 4, and we allowed an extended list of infixes: s, n, en, nen, es, er and ien. For suffixes, we allowed: e, en, n, es, s, em and er, which is more aggressive 116 INPUT Mir ist bewusst , dass der Balkan kein g</context>
<context position="15357" citStr="Stymne et al. (2008)" startWordPosition="2563" endWordPosition="2566">ping for binarization. model on simplified German. The second SMT system translates mixed case simplified German to mixed case unsimplified German. The translation model is built only on the simplified German from the parallel text, and the language model is trained on all German data. We present the results in table 2. METEOR7 did not correlate as well as BLEU for translation out of English in WMT 2008. The BLEU score of our final system is worse than the baseline. We had chosen to submit this system as we found it more interesting than submitting a vanilla system. In addition, the system of Stymne et al. (2008) received a good human evaluation despite having a relatively low BLEU score, and we hoped we were performing similar morphological generalization. We expect to be able to improve this system through error analysis. In an initial inspection we found case mismatching problems between step one and step two. 4 Conclusion We presented our German to English system which employed character normalization, compensated for problems caused by the German writing reform, used modified syntactic reordering rules (in combination with morphologically aware parsing), and employed substring-based morphological</context>
</contexts>
<marker>Stymne, Holmqvist, Ahrenberg, 2008</marker>
<rawString>Sara Stymne, Maria Holmqvist, and Lars Ahrenberg. 2008. Effects of morphological analysis in translation between German and English. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 135–138, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>512--519</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="13777" citStr="Talbot and Osborne, 2007" startWordPosition="2299" endWordPosition="2302"> the dev set used for MERT). This caused minor degradation, see the system marked (*) for the system with the bug corrected. Each system increases in both BLEU and METEOR as improvements are added. An exception is that splitting/stemming decreases BLEU somewhat. However, we trust the METEOR results more due to their better correlation with human judgements. We also compared using a different language model instead of the SRILM model (the bottom half of table 1). These used either the reduced English language modeling data or the full data (21.2 M segments, marked 21.2 in the results). RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). IRSTLM (Federico and Cettolo, 2007) also performs well, but the quantized model on the 21.2 data did not improve over the smaller quantized model6. IRSTLM uses an approximation of Witten-Bell smoothing, our results support that this is competitive. 3.2 English to German We trained our English to German system on the constrained parallel data. The first SMT system translates from lowercased English to lowercased simplified German, which is then recased. The syntactic reordering process is not use</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In ACL, pages 512–519, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="12407" citStr="Vogel et al., 1996" startWordPosition="2064" endWordPosition="2067">nvergence using dev-2009a (separately for each experiment). One limitation of our German to English system is that we were unable to scale to the full language modeling data using SRILM (Stolcke, 2002), 5- grams and modified Kneser-Ney with no singleton deletion4. The language model in our submitted system is based on all of the available English data, but news-train08 is truncated to the first 10193376 lines, meaning that we did not train on the remaining 11038787 lines, so we used a little less than half of the data. We converted the lan3We used 5 iterations of Model 1, 4 iterations of HMM (Vogel et al., 1996) and 4 iterations of Model 4. 4SRILM failed when trained on the full data, even when a machine with 32 GB RAM and 48 GB swap was used. 117 guage model trained using SRILM to the binary format using IRSTLM. Experiments are presented in table 1, using BLEU (Papineni et al., 2001) and METEOR5 (Banerjee and Lavie, 2005), and we also show the length ratio (ratio of hypothesized tokens to reference tokens). For translation into English METEOR had superior correlation with human rankings to BLEU at WMT 2008 (Callison-Burch et al., 2008). Our submitted system had a bug where the environment variable L</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP-CONLL,</booktitle>
<pages>737--745</pages>
<contexts>
<context position="3967" citStr="Wang et al. (2007)" startWordPosition="627" endWordPosition="630">as superseded ISO 8859-1. Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 115 2.3 Reordering German German word order differs from English substantially. Preprocessing approaches involving the use of a syntactic parse of the source sentence to change the word order to more closely match the word order of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Dr´abek and Yarowsky (2004), Collins et al. (2005), Popovi´c and Ney (2006), Wang et al. (2007) and many others. To obtain a parse of each German sentence in the training, dev and test corpora, we employed the IfNLP BitPar probabilistic parser (Schmid, 2004), using models learned from the Tiger Treebank for German. Dealing with morphological productivity is important in the syntactic parsing of German. BitPar has been designed with this in mind. IfNLP’s SMOR analyzer is used for morphological analysis (Schmid et al., 2004). SMOR is run over a list of types in each German sentence, and outputs a list of analyses for each type, each of which corresponds to a POS tag. BitPar is limited to </context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In EMNLP-CONLL, pages 737–745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="3870" citStr="Xia and McCord (2004)" startWordPosition="611" endWordPosition="614">on accented characters used in Western European languages. A reviewer pointed out that ISO 8859-15 has superseded ISO 8859-1. Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 115 2.3 Reordering German German word order differs from English substantially. Preprocessing approaches involving the use of a syntactic parse of the source sentence to change the word order to more closely match the word order of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Dr´abek and Yarowsky (2004), Collins et al. (2005), Popovi´c and Ney (2006), Wang et al. (2007) and many others. To obtain a parse of each German sentence in the training, dev and test corpora, we employed the IfNLP BitPar probabilistic parser (Schmid, 2004), using models learned from the Tiger Treebank for German. Dealing with morphological productivity is important in the syntactic parsing of German. BitPar has been designed with this in mind. IfNLP’s SMOR analyzer is used for morphological analysis (Schmid et al., 2004). SMOR is run over a list of types in each German sentence, and output</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>