<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003211">
<title confidence="0.6920185">
AVAYA: Sentiment Analysis on Twitter with Self-Training
and Polarity Lexicon Expansion
</title>
<author confidence="0.982755">
Lee Becker, George Erhart, David Skiba and Valentine Matula
</author>
<affiliation confidence="0.981769">
Avaya Labs Research
</affiliation>
<address confidence="0.920838">
1300 West 120th Avenue
Westminster, CO 80234, USA
</address>
<email confidence="0.999444">
{beckerl,gerhart,dskiba,matula}@avaya.com
</email>
<sectionHeader confidence="0.996643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994008666666667">
This paper describes the systems submitted by
Avaya Labs (AVAYA) to SemEval-2013 Task
2 - Sentiment Analysis in Twitter. For the
constrained conditions of both the message
polarity classification and contextual polarity
disambiguation subtasks, our approach cen-
ters on training high-dimensional, linear clas-
sifiers with a combination of lexical and syn-
tactic features. The constrained message po-
larity model is then used to tag nearly half
a million unlabeled tweets. These automati-
cally labeled data are used for two purposes:
</bodyText>
<listItem confidence="0.7426617">
1) to discover prior polarities of words and
2) to provide additional training examples for
self-training. Our systems performed compet-
itively, placing in the top five for all subtasks
and data conditions. More importantly, these
results show that expanding the polarity lexi-
con and augmenting the training data with un-
labeled tweets can yield improvements in pre-
cision and recall in classifying the polarity of
non-neutral messages and contexts.
</listItem>
<sectionHeader confidence="0.99713" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.943918604651163">
The past decade has witnessed a massive expansion
in communication from long-form delivery such
as e-mail to short-form mechanisms such as mi-
croblogging and short messaging service (SMS) text
messages. Simultaneously businesses, media out-
lets, and investors are increasingly relying on these
messages as sources of real-time information and
are increasingly turning to sentiment analysis to dis-
cover product trends, identify customer preferences,
and categorize users. While a variety of corpora ex-
ist for developing and evaluating sentiment classi-
fiers for long-form texts such as product reviews,
there are few such resources for evaluating senti-
ment algorithms on microblogs and SMS texts.
The organizers of SemEval-2013 task 2, have be-
gun to address this resource deficiency by coordi-
nating a shared evaluation task for Twitter sentiment
analysis. In doing so they have assembled corpora
in support of the following two subtasks:
Task A - Contextual Polarity Disambiguation
“Given a message containing a marked in-
stance of a word or phrase, determine whether
that instance is positive, negative or neutral in
that context.”
Task B - Message Polarity Classification “Given
a message, classify whether the message is
of positive, negative, or neutral sentiment.
For messages conveying both a positive and
negative sentiment, whichever is the stronger
sentiment should be chosen.”
This paper describes the systems submitted by
Avaya Labs for participation in subtasks A and B.
Our goal for this evaluation was to investigate the
usefulness of dependency parses, polarity lexicons,
and unlabeled tweets for sentiment classification on
short messages. In total we built four systems for
SemEval-2013 task 2. For task B we developed a
constrained model using supervised learning, and
an unconstrained model that used semi-supervised
learning in the form of self-training and polarity lex-
icon expansion. For task A the constrained sys-
tem utilized supervised learning, while the uncon-
strained model made use of the expanded lexicon
</bodyText>
<page confidence="0.989629">
333
</page>
<bodyText confidence="0.883130375">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 333–340, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
from task B. Output from these systems were sub-
mitted to all eight evaluation conditions. For a com-
plete description of the data, tasks, and conditions,
please refer to Wilson et al. (2013). The remainder
of this paper details the approaches, experiments and
results associated with each of these models.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999916666666667">
Over the past few years sentiment analysis has
grown from a nascent topic in natural language pro-
cessing to a broad research area targeting a wide
range of text genres and applications. There is
now a significant body of work that spans topics
as diverse as document level sentiment classifica-
tion (Pang and Lee, 2008), induction of word polar-
ity lexicons (Hatzivassiloglou and McKeown, 1997;
Turney, 2002; Esuli and Sebastiani, 2006; Moham-
mad and Turney, 2011) and even election prediction
(Tumasjan et al., 2010).
Efforts to train sentiment classifiers for Twitter
messages have largely relied on using emoticons
and hashtags as proxies of the true polarity (Bar-
bosa and Feng, 2010; Davidov et al., 2010b; Pak and
Paroubek, 2010; Agarwal et al., 2011; Kouloumpis
et al., 2011; Mohammad, 2012). Classification of
word and phrase sentiment with respect to surround-
ing context (Wilson et al., 2005) has yet to be ex-
plored for the less formal language often found in
microblog and SMS text. Semi-supervised learn-
ing has been applied to polarity lexicon induction
(Rao and Ravichandran, 2009), and sentiment clas-
sification at the sentence level (T¨ackstr¨om and Mc-
Donald, 2011) and document level (Sindhwani and
Melville, 2008; He and Zhou, 2011); however to
the best of our knowledge self-training and other
semi-supervised learning has seen only minimal use
in classifying Twitter texts (Davidov et al., 2010a;
Zhang et al., 2012).
</bodyText>
<sectionHeader confidence="0.978952" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.988740272727273">
Given our overarching goal of combining polarity
lexicons, syntactic information and unlabeled data,
our approach centered on first building strong con-
strained models and then improving performance
by adding additional data and resources. For
both tasks, our data-constrained approach com-
bined standard features for document classification
conj → conj hconjunctioni
pobj → prep hprepositioni
pcomp → prepc hprepositioni
prep|punct|cc → ∅
</bodyText>
<tableCaption confidence="0.997836">
Table 1: Collapsed Dependency Transformation Rules
</tableCaption>
<bodyText confidence="0.998998">
with dependency parse and word polarity features
into a weighted linear classifier. For our data-
unconstrained models we used pointwise mutual in-
formation for lexicon expansion in conjunction with
self-training to increase the size of the feature space.
</bodyText>
<sectionHeader confidence="0.940571" genericHeader="method">
4 Preprocessing and Text Normalization
</sectionHeader>
<bodyText confidence="0.999957">
Our systems were built with ClearTK (Ogren et
al., 2008) a framework for developing NLP com-
ponents built on top of Apache UIMA. Our pre-
processing pipeline utilized ClearTK’s wrappers for
ClearNLP’s (Choi and McCallum, 2013) tokenizer,
lemmatizer, part-of-speech (POS) tagger, and de-
pendency parser. ClearNLP’s ability to retain emoti-
cons and emoji as individual tokens made it espe-
cially attractive for sentiment analysis. POS tags
were mapped from Penn Treebank-style tags to the
simplified, Twitter-oriented tags introduced by Gim-
pel et al. (2011). Dependency graphs output by
ClearNLP were also transformed to the Stanford
Collapsed dependencies representation (de Marneffe
and Manning, 2012) using our own transformation
rules (table 1). Input normalization consisted solely
of replacing all usernames and URLs with common
placeholders.
</bodyText>
<sectionHeader confidence="0.929882" genericHeader="method">
5 Sentiment Resources
</sectionHeader>
<bodyText confidence="0.999930583333333">
A variety of our classifier features rely on manually
tagged sentiment lexicons and word lists. In partic-
ular we make use of the MPQA Subjectivity Lexi-
con (Wiebe et al., 2005) as well as manually-created
negation and emoticon dictionaries1. The negation
word list consisting of negation words such as no
and not. Because tokenization splits contractions,
the list includes the sub-word token n’t as well as
the apostrophe-less version of 12 contractions (e.g.
cant, wont, etc ... ). To support emoticon-specific
features we created a dictionary, which paired 183
emoticons with either a positive or negative polarity.
</bodyText>
<footnote confidence="0.972952">
1http://leebecker.com/resources/semeval-2013
</footnote>
<page confidence="0.998185">
334
</page>
<sectionHeader confidence="0.944513" genericHeader="method">
6 Message Polarity Classification
</sectionHeader>
<subsectionHeader confidence="0.917366">
6.1 Features
</subsectionHeader>
<bodyText confidence="0.99883092">
Polarized Bag-of-Words Features: Instead of ex-
tracting raw bag-of words (BOW), we opted to in-
tegrate negation directly into the word representa-
tions following the approaches used by Das and
Chen (2001) and Pang et al. (2002). All words
between a negation word and the first punctuation
mark after the negation word were suffixed with
a NOT tag – essentially doubling the number of
BOW features. We extended this polarized BOW
paradigm to include not only the raw word forms
but all of the following combinations: raw word, raw
word+PTB POS tag, raw word+simplified POS tag,
lemma+simplified POS tag.
Word Polarity Features: Using a subjectivity lex-
icon, we extracted features for the number of posi-
tive, negative, and neutral words as well as the net
polarity based on these counts. Individual word po-
larities were inverted if the word had a child depen-
dency relation with a negation (neg) label. Con-
strained models use the MPQA lexicon, while un-
constrained models use an expanded lexicon that is
described in section 6.2.
Emoticon Features: Similar to the word polarity
features, we computed features for the number of
positive, negative, and neutral emoticons, and the
net emoticon polarity score.
Microblogging Features: As noted by Kouloumpis
et al. (2011), the emotional intensity of words in so-
cial media messages is often emphasized by changes
to the word form such as capitalization, charac-
ter repetition, and emphasis characters (asterisks,
dashes). To capture this intuition we compute fea-
tures for the number of fully-capitalized words,
words with characters repeated more than 3 times
(e.g. booooo), and words surround by asterisks or
dashes (e.g. *yay*). We also created a binary fea-
ture to indicate the presence of a winning score or
winning record within the target span (e.g. Oh yeah
#Nuggets 15-0).
Part-of-Speech Tag Features: Counts of the Penn
Treebank POS tags provide a rough measure of the
content of the message.
Syntactic Dependency Features: We extracted
dependency pair features using both standard and
collapsed dependency parse graphs. Extracted
head/child relations include: raw word/raw word,
lemma/lemma, lemma/simplified POS tag, simpli-
fied POS tag/lemma. If the head node of the relation
has a child negation dependency, the pair’s relation
label is prefixed with a NEG tag.
</bodyText>
<subsectionHeader confidence="0.999743">
6.2 Expanding the Polarity Lexicon
</subsectionHeader>
<bodyText confidence="0.999983342105263">
Unseen words pose a recurring challenge for both
machine learning and dictionary-based approaches
to sentiment analysis. This problem is even more
prevalent in social media and SMS messages where
text lengths are often limited to 140 characters or
less. To expand our word polarity lexicon we adopt
a framework similar to the one introduced by Turney
(2002). Turney’s unsupervised approach centered on
computing pointwise mutual information (PMI) be-
tween highly polar seed words and bigram phrases
extracted from a corpus of product reviews.
Instead of relying solely on seed words for po-
larity, we use the constrained version of the mes-
sage polarity classifier to tag a corpus of approxi-
mately 475,000 unlabeled, English language tweets.
These tweets were collected over the period from
November 2012 to February 2013. To reduce the
number of noisy instances and to obtain a more bal-
anced distribution of sentiment labels, we eliminated
all tweets with classifier confidence scores below
0.9, 0.7, and 0.8 for positive, negative and neutral
instances respectively. Applying the threshold, re-
duced the tweet count to 180,419 tweets (50,789
positive, 59,029 negative, 70,601 neutral). This fil-
tered set of automatically labeled tweets was used
to accumulate co-occurrence statistics between the
words in the tweets and their corresponding senti-
ment labels. These statistics are then used to com-
pute word-sentiment PMI (equation 1), which is
the joint probability of a word and sentiment co-
occurring divided by the probability of each of the
events occurring independently. A word’s net po-
larity is computed as the signum (sgn) of the differ-
ence between a its positive and negative PMI values
(equation 2). It should be noted that polarities were
deliberately limited to values of {-1, 0, +11 to ensure
consistency with the existing MPQA lexicon, and to
dampen the bias of any single word.
</bodyText>
<page confidence="0.929165">
335
</page>
<equation confidence="0.9934364">
p(word, sentiment)
p(word)p(sentiment)
(1)
polarity(word) = sgn(PMI(word, positive)− (2)
PMI (word, negative) )
</equation>
<bodyText confidence="0.9987776">
Words with fewer than 10 occurrences, words
with neutral polarities, numbers, single characters,
and punctuation were then removed from this PMI-
derived polarity dictionary. Lastly, this dictionary
was merged with the dictionary created from the
MPQA lexicon yielding a final polarity dictionary
with 11,740 entries. In cases where an entry existed
in both dictionaries, the MPQA polarity value was
retained. This final polarity dictionary was used by
the unconstrained models for task A and B.
</bodyText>
<subsectionHeader confidence="0.999533">
6.3 Model Parameters and Training
</subsectionHeader>
<bodyText confidence="0.999633585365854">
Constrained Model: Models were trained us-
ing the LIBLINEAR classification library (Fan et
al., 2008). L2 regularized logistic regression was
chosen over other LIBLINEAR loss functions be-
cause it not only gave improved performance on
the development set but also produced calibrated
outcomes for confidence thresholding. Training
data for the constrained model consisted of all
9829 examples from the training (8175 exam-
ples) and development (1654 examples) set re-
leased for SemEval 2013. Cost and label-specific
cost weight parameters were selected via exper-
imentation on the development set to maximize
the average positive and negative F1 values. The
c values ranged over 10.1, 0.5,1, 2, 5, 10, 20,100}
and the label weights wpolarity ranged over
10.1,1, 2, 5, 10, 20, 25, 50, 100}. Final parameters
for the constrained model were cost c = 1 and
weights wpositive = 1, wnegative = 25, and
wneutral = 1.
Unconstrained Model: In addition to using the ex-
panded polarity dictionary described in 6.2 for fea-
ture extraction, the unconstrained model also makes
use of automatically labeled tweets for self-training
(Scudder, 1965). In contrast to preparation of the ex-
panded polarity dictionary, the self-training placed
no threshold on the examples. Combining the self-
labeled tweets, with the official training and devel-
opment set yielded a new training set consisting
of 485,112 examples. Because the self-labeled in-
stances were predominantly tagged neutral, the LI-
BLINEAR cost parameters were adjusted to heav-
ily discount neutral while emphasizing positive and
neutral instances. The size and cost of training
this model prevented extensive parameter tuning and
instead were chosen based on experience with the
constrained model and to maximize recall on pos-
itive and negative items. Final parameters for the
unconstrained model were cost c = 1 and cate-
gory weights wpositive = 2, wnegative = 5, and
wneutral = 0.1.
</bodyText>
<sectionHeader confidence="0.789493" genericHeader="method">
7 Contextual Polarity Disambiguation
7.1 Features
</sectionHeader>
<bodyText confidence="0.99985278125">
The same base set of features used for message po-
larity classification were used for the contextual po-
larity classification, with the exception of the syn-
tactic dependency features. To better express the in-
context and out-of-context relation these additional
feature classes were added:
Scoped Dependency Features: Because this task
focuses on a smaller context within the message,
collapsed dependencies are less useful as the com-
pression may cross over context boundaries. In-
stead the standard syntactic dependency features de-
scribed above were modified to account for their re-
lation to the context. All governing relations for the
words contained within the contact were extracted.
Relations wholly contained within the boundaries of
the context were prefixed with an IN tag, whereas
those that crossed outside of the context were pre-
fixed with an OUT tag. Additionally counts of IN
and OUT relations were included as features.
Dependency Path Features: Like the single de-
pendency arcs, a dependency path can provide addi-
tional information about the syntactic and semantic
role of the context in the sentence. Our path fea-
tures consisted of two varieties: 1) POS-path and
2) Sentiment-POS-path. The POS-path consisted of
the PTB POS tags and dependency relation labels
for all nodes between the head of the context and the
root node of the parent sentence. The Sentiment-
POS-path follows the same path but omits the de-
pendency relation labels, uses the simplified POS
tags and appends word polarities (POS/NEG/NTR)
to the POS tags along the path.
</bodyText>
<equation confidence="0.715712">
PMI(word, sentiment) = log2
</equation>
<page confidence="0.995316">
336
</page>
<table confidence="0.9996361">
System P Positive F P Negative F P Neutral F Fav9 Rank
R R R +/-
Tweet NRC-Canada (top) 0.814 0.667 0.733 0.697 0.604 0.647 0.677 0.826 0.744 0.690 1
AVAYA-Unconstrained 0.751 0.655 0.700 0.608 0.557 0.582 0.665 0.768 0.713 0.641 5
AVAYA-Constrained 0.791 0.580 0.669 0.593 0.509 0.548 0.636 0.832 0.721 0.608 12
Mean of submissions 0.687 0.591 0.626 0.491 0.456 0.450 0.612 0.663 0.615 0.538 -
SMS NRC-Canada (top) 0.731 0.730 0.730 0.554 0.754 0.639 0.852 0.753 0.799 0.685 1
AVAYA-Constrained 0.630 0.667 0.648 0.526 0.581 0.553 0.802 0.756 0.778 0.600 4
AVAYA-Unconstrained 0.609 0.659 0.633 0.494 0.637 0.557 0.814 0.710 0.759 0.595 5
Mean of submissions 0.512 0.620 0.546 0.462 0.518 0.456 0.754 0.578 0.627 0.501 -
</table>
<tableCaption confidence="0.960136">
Table 2: Message Polarity Classification (Task B) Results
</tableCaption>
<table confidence="0.999778">
System P Positive F P Negative F P Neutral F Fav9 Rank
R R R +/-
Tweet NRC-Canada (top) 0.889 0.932 0.910 0.866 0.871 0.869 0.455 0.063 0.110 0.889 1
AVAYA-Unconstrained 0.892 0.905 0.898 0.834 0.865 0.849 0.539 0.219 0.311 0.874 2
AVAYA-Constrained 0.882 0.911 0.896 0.844 0.843 0.843 0.493 0.225 0.309 0.870 3
Mean of submissions 0.837 0.745 0.773 0.745 0.656 0.677 0.159 0.240 0.115 0.725 -
SMS GUMLTLT (top) 0.814 0.924 0.865 0.908 0.896 0.902 0.286 0.050 0.086 0.884 1
AVAYA-Unconstrained 0.815 0.871 0.842 0.853 0.896 0.874 0.448 0.082 0.138 0.858 3
AVAYA-Constrained 0.777 0.875 0.823 0.859 0.852 0.856 0.364 0.076 0.125 0.839 4
Mean of submissions 0.734 0.722 0.710 0.807 0.663 0.698 0.144 0.184 0.099 0.704 -
</table>
<tableCaption confidence="0.999706">
Table 3: Contextual Polarity Disambiguation (Task A) Results
</tableCaption>
<bodyText confidence="0.956391133333333">
For example given the bold-faced context in the
sentence:
@User Criminals killed Sadat, and in the
process they killed Egypt... they destroyed
the future of young &amp; old Egyptians..
the extracted POS-path feature would be:
{NNP} dobj &lt;{VBD} conj &lt;{VBD}
ccomp &lt;{VBD} root &lt;{TOP}
while the Sentiment-POS path would be:
{ˆ/pos}{V/neg}{V/neg}{V/neg}{TOP}.
Paths with depth greater than 4 dependency rela-
tions were truncated to reduce feature sparsity. In
addition to these detailed path features, we include
two binary features to indicate if any part of the path
contains subject or object relations.
</bodyText>
<subsectionHeader confidence="0.99963">
7.2 Model Parameters and Training
</subsectionHeader>
<bodyText confidence="0.999786923076923">
Like with message polarity classification, the con-
textual polarity disambiguation systems rely on LI-
BLINEAR’s L2 regularized logistic regression for
model training. Both constrained and unconstrained
models use identical parameters of cost c = 1
and weights wpositive = 1, wnegative = 2, and
wneutral = 1. They vary only in the choice of polar-
ity lexicon. The constrained model uses the MPQA
subjectivity lexicon, while the unconstrained model
uses the expanded dictionary derived via computa-
tion of PMI, which ultimately differentiates these
models through the variation in the sentiment path
and word polarity features.
</bodyText>
<sectionHeader confidence="0.980091" genericHeader="method">
8 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999904066666667">
In this section we report results for the series of Sen-
timent Analysis in Twitter tasks at SemEval 2013.
Please refer to refer to Wilson et al. (2013) for the
exact details about the corpora, evaluation condi-
tions, and methodology.
We submitted polarity output for the Message Po-
larity Classification (task B) and the Contextual Po-
larity Disambiguation (task A). For each task we
submitted system output from our constrained and
unconstrained models. As stated above, the con-
strained models made use of only the training data
released for the task, whereas the unconstrained
models trained on additional tweets. Each subtask
had two test sets one comprised of tweets and the
other comprised of SMS messages. Final task 2
</bodyText>
<page confidence="0.988521">
337
</page>
<figure confidence="0.916835142857143">
S G Message / Context
1 + /
2 / +
3 - /
4 - +
5 + -
6 + -
Going to Helsinki tomorrow or on the day after tomorrow,yay!
Eric Decker catches his second TD pass from Manning. This puts Broncos up 31-7 with 14:54 left in the 4th.
So, crashed a wedding reception and Andy Lee’s bro was in the bridal party. How’d you spend your Saturday
night? #longstory
Aiyo... Dun worry la, they’ll let u change one... Anyway, sleep early, nite nite...
Sori I haven’t done anything for today’s meeting.. pls pardon me. Cya guys later at 10am.
these PSSA’s are just gonna be the icing to another horrible monday. #fmlll #ihateschool
</figure>
<tableCaption confidence="0.9368815">
Table 4: Example Classification Errors: S=System, G=Gold, +=positive, −=negative, /=neutral. Bold-faced text
indicates the span for contextual polarities.
</tableCaption>
<bodyText confidence="0.9954875">
evaluation is based on the average positive and neg-
ative F-score. Task B results are listed in table 2,
and task A results are shown in table 3. For compar-
ison these tables also include the top-ranked system
in each category as well as the mean scores across
all submissions.
</bodyText>
<sectionHeader confidence="0.991029" genericHeader="method">
9 Error Analysis
</sectionHeader>
<bodyText confidence="0.9998419375">
To better understand our systems’ limitations we
manually inspected misclassified output. Table 4
lists errors representative of the common issues un-
covered in our error analysis.
Though some degree of noise is expected in senti-
ment analysis, we found several instances of annota-
tion error or ambiguity where it could be argued that
the system was actually correct. The message in #1
was annotated as neutral, whereas the presence of
the word “yay” suggests an overall positive polarity.
The text in #2 could be interpreted as positive, nega-
tive or neutral depending on the author’s disposition.
Unseen vocabulary and unexpected usages were
the largest category of error. For example in #3
“crashed” means to attend without an invitation in-
stead of the more negative meaning associated with
car accidents and airplane failures. Although POS
features can disambiguate word senses, in this case
more sophisticated features for word sense disam-
biguation could help. While the degradation in
performance between the Tweet and SMS test sets
might be explained by differences in medium, er-
rors like those found in #4 and #5 suggest that this
may have more to do with the dialectal differences
between the predominantly American and British
English found in the Tweet test set and the Collo-
quial Singaporean English (aka Singlish) found in
the SMS test set. Error #6 illustrates both how hash-
tags composed of common words can easily become
a problem when assigning a polarity to a short con-
text. Hashtag segmentation presents one possible
path to reducing this source of error.
</bodyText>
<sectionHeader confidence="0.951539" genericHeader="conclusions">
10 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999988117647059">
The results and rankings reported in section 8 sug-
gest that our systems were competitive in assign-
ing sentiment across the varied tasks and data con-
ditions. We performed particularly well in dis-
ambiguating contextual polarities finishing second
overall on the Tweet test set. We hypothesize this
performance is largely due to the expanded vocabu-
lary obtained via unlabeled data and the richer syn-
tactic context captured with dependency path repre-
sentations.
Looking forward, we expect that term recall and
unseen vocabulary will continue to be key chal-
lenges for sentiment analysis on social media. While
larger amounts of data should assist in that pursuit,
we would like to explore how a more iterative ap-
proach to self-training and lexicon expansion may
provide a less noisy path to attaining such recall.
</bodyText>
<sectionHeader confidence="0.976381" genericHeader="acknowledgments">
11 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999854">
We would like to thank the organizers of SemEval
2013 and the Sentiment Analysis in Twitter task for
their time and energy. We also would like to ex-
press our appreciation to the anonymous reviewers
for their helpful feedback and suggestions.
</bodyText>
<sectionHeader confidence="0.997653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.941688833333333">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
</reference>
<page confidence="0.99385">
338
</page>
<reference confidence="0.998912738317758">
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ’10, pages
36–44, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jinho D. Choi and Andrew McCallum. 2013. Transition-
based dependency parsing with selectional branching.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (ACL’13).
Sanjiv Das and Mike Chen. 2001. Yahoo! for ama-
zon: extracting market sentiment from stock message
boards. In Proceedings of the 8th Asia Pacific Finance
Association Annual Conference.
Dmitry Davidov, Oren Tsur, and Ari Rappaport. 2010a.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.
Enhanced sentiment learning using twitter hashtags
and smileys. In Coling 2010, pages 241–249.
Marie-Catherine de Marneffe and Christopher D. Man-
ning, 2012. Stanford typed dependencies manual.
Stanford University, v2.0.4 edition, November.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC’06).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classication. Journal of Ma-
chine Learning Research, 9:1871–1874.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies ACL:HLT 2011.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics (ACL
1997).
Yulan He and Deyu Zhou. 2011. Self-training from
labeled features for sentiment analysis. Information
Processing and Management, 47(4):606–616.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The Good
the Bad and the OMG! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM 2011).
Saif M. Mohammad and Peter D. Turney. 2011. Crowd-
sourcing a word-emotion association lexicon. Compu-
tational Intelligence, 59(000).
Saif M. Mohammad. 2012. #emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics (*SEM).
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC ’08), 5.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC’10).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification Using
Machine Learning Techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002).
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009).
H. J. Scudder. 1965. Probability of error of some adap-
tive pattern-recognition machine. IEEE Transactions
on Information Theory, 11:363–371.
Vikas Sindhwani and Prem Melville. 2008. Document-
word co-regularization for semi-supervised sentiment
analysis. In Proceedings of the 2008 Eighth IEEE In-
ternational Conference on Data Mining, ICDM ’08,
pages 1025–1030.
Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2011).
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with twitter what 140 characters reveal about politi-
cal sentiment. In Proceedings of the Fourth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM 2010).
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
</reference>
<page confidence="0.98945">
339
</page>
<reference confidence="0.998626583333333">
Meeting of the Association for Computational Linguis-
tics (ACL 2002).
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39:165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter. In
Proceedings of the 7th International Workshop on Se-
mantic Evaluation. Association for Computation Lin-
guistics.
Xiuzhen Zhang, Yun Zhou, James Bailey, and Kota-
giri Ramamohanarao. 2012. Sentiment analysis
by augmenting expectation maximisation with lexi-
cal knowledge. Proceedings of the 13th International
Conference on Web Information Systems Engineering
(WISE2012).
</reference>
<page confidence="0.998202">
340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.436803">
<title confidence="0.9848455">AVAYA: Sentiment Analysis on Twitter with Self-Training and Polarity Lexicon Expansion</title>
<author confidence="0.7597595">Lee Becker</author>
<author confidence="0.7597595">George Erhart</author>
<author confidence="0.7597595">David Skiba</author>
<author confidence="0.7597595">Valentine Avaya Labs</author>
<address confidence="0.990051">1300 West 120th Westminster, CO 80234,</address>
<abstract confidence="0.989255739130435">This paper describes the systems submitted by Avaya Labs (AVAYA) to SemEval-2013 Task 2 - Sentiment Analysis in Twitter. For the constrained conditions of both the message polarity classification and contextual polarity disambiguation subtasks, our approach centers on training high-dimensional, linear classifiers with a combination of lexical and syntactic features. The constrained message polarity model is then used to tag nearly half a million unlabeled tweets. These automatically labeled data are used for two purposes: 1) to discover prior polarities of words and 2) to provide additional training examples for self-training. Our systems performed competitively, placing in the top five for all subtasks and data conditions. More importantly, these results show that expanding the polarity lexicon and augmenting the training data with unlabeled tweets can yield improvements in precision and recall in classifying the polarity of non-neutral messages and contexts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media (LSM</booktitle>
<contexts>
<context position="4613" citStr="Agarwal et al., 2011" startWordPosition="697" endWordPosition="700">rgeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Language in Social Media (LSM 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>36--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4544" citStr="Barbosa and Feng, 2010" startWordPosition="684" endWordPosition="688">ascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge s</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 36–44, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Transitionbased dependency parsing with selectional branching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13).</booktitle>
<contexts>
<context position="6314" citStr="Choi and McCallum, 2013" startWordPosition="953" endWordPosition="956">repositioni pcomp → prepc hprepositioni prep|punct|cc → ∅ Table 1: Collapsed Dependency Transformation Rules with dependency parse and word polarity features into a weighted linear classifier. For our dataunconstrained models we used pointwise mutual information for lexicon expansion in conjunction with self-training to increase the size of the feature space. 4 Preprocessing and Text Normalization Our systems were built with ClearTK (Ogren et al., 2008) a framework for developing NLP components built on top of Apache UIMA. Our preprocessing pipeline utilized ClearTK’s wrappers for ClearNLP’s (Choi and McCallum, 2013) tokenizer, lemmatizer, part-of-speech (POS) tagger, and dependency parser. ClearNLP’s ability to retain emoticons and emoji as individual tokens made it especially attractive for sentiment analysis. POS tags were mapped from Penn Treebank-style tags to the simplified, Twitter-oriented tags introduced by Gimpel et al. (2011). Dependency graphs output by ClearNLP were also transformed to the Stanford Collapsed dependencies representation (de Marneffe and Manning, 2012) using our own transformation rules (table 1). Input normalization consisted solely of replacing all usernames and URLs with com</context>
</contexts>
<marker>Choi, McCallum, 2013</marker>
<rawString>Jinho D. Choi and Andrew McCallum. 2013. Transitionbased dependency parsing with selectional branching. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv Das</author>
<author>Mike Chen</author>
</authors>
<title>Yahoo! for amazon: extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th Asia Pacific Finance Association Annual Conference.</booktitle>
<contexts>
<context position="7869" citStr="Das and Chen (2001)" startWordPosition="1180" endWordPosition="1183">h as no and not. Because tokenization splits contractions, the list includes the sub-word token n’t as well as the apostrophe-less version of 12 contractions (e.g. cant, wont, etc ... ). To support emoticon-specific features we created a dictionary, which paired 183 emoticons with either a positive or negative polarity. 1http://leebecker.com/resources/semeval-2013 334 6 Message Polarity Classification 6.1 Features Polarized Bag-of-Words Features: Instead of extracting raw bag-of words (BOW), we opted to integrate negation directly into the word representations following the approaches used by Das and Chen (2001) and Pang et al. (2002). All words between a negation word and the first punctuation mark after the negation word were suffixed with a NOT tag – essentially doubling the number of BOW features. We extended this polarized BOW paradigm to include not only the raw word forms but all of the following combinations: raw word, raw word+PTB POS tag, raw word+simplified POS tag, lemma+simplified POS tag. Word Polarity Features: Using a subjectivity lexicon, we extracted features for the number of positive, negative, and neutral words as well as the net polarity based on these counts. Individual word po</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>Sanjiv Das and Mike Chen. 2001. Yahoo! for amazon: extracting market sentiment from stock message boards. In Proceedings of the 8th Asia Pacific Finance Association Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappaport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="4566" citStr="Davidov et al., 2010" startWordPosition="689" endWordPosition="692">language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other</context>
</contexts>
<marker>Davidov, Tsur, Rappaport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappaport. 2010a. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys. In Coling</title>
<date>2010</date>
<pages>241--249</pages>
<contexts>
<context position="4566" citStr="Davidov et al., 2010" startWordPosition="689" endWordPosition="692">language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b. Enhanced sentiment learning using twitter hashtags and smileys. In Coling 2010, pages 241–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford typed dependencies manual. Stanford University,</title>
<date>2012</date>
<tech>v2.0.4 edition,</tech>
<marker>de Marneffe, Manning, 2012</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning, 2012. Stanford typed dependencies manual. Stanford University, v2.0.4 edition, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC’06).</booktitle>
<contexts>
<context position="4294" citStr="Esuli and Sebastiani, 2006" startWordPosition="645" endWordPosition="648">a, tasks, and conditions, please refer to Wilson et al. (2013). The remainder of this paper details the approaches, experiments and results associated with each of these models. 2 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classication.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="12644" citStr="Fan et al., 2008" startWordPosition="1933" endWordPosition="1936">fewer than 10 occurrences, words with neutral polarities, numbers, single characters, and punctuation were then removed from this PMIderived polarity dictionary. Lastly, this dictionary was merged with the dictionary created from the MPQA lexicon yielding a final polarity dictionary with 11,740 entries. In cases where an entry existed in both dictionaries, the MPQA polarity value was retained. This final polarity dictionary was used by the unconstrained models for task A and B. 6.3 Model Parameters and Training Constrained Model: Models were trained using the LIBLINEAR classification library (Fan et al., 2008). L2 regularized logistic regression was chosen over other LIBLINEAR loss functions because it not only gave improved performance on the development set but also produced calibrated outcomes for confidence thresholding. Training data for the constrained model consisted of all 9829 examples from the training (8175 examples) and development (1654 examples) set released for SemEval 2013. Cost and label-specific cost weight parameters were selected via experimentation on the development set to maximize the average positive and negative F1 values. The c values ranged over 10.1, 0.5,1, 2, 5, 10, 20,</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classication. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies ACL:HLT</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies ACL:HLT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="4252" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="639" endWordPosition="642"> conditions. For a complete description of the data, tasks, and conditions, please refer to Wilson et al. (2013). The remainder of this paper details the approaches, experiments and results associated with each of these models. 2 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Se</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL 1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Deyu Zhou</author>
</authors>
<title>Self-training from labeled features for sentiment analysis.</title>
<date>2011</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>47--4</pages>
<contexts>
<context position="5104" citStr="He and Zhou, 2011" startWordPosition="775" endWordPosition="778">gs as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitter texts (Davidov et al., 2010a; Zhang et al., 2012). 3 System Overview Given our overarching goal of combining polarity lexicons, syntactic information and unlabeled data, our approach centered on first building strong constrained models and then improving performance by adding additional data and resources. For both tasks, our data-constrained approach combined standard features for document classification conj → conj hconjunctioni pobj → prep hprepositioni pc</context>
</contexts>
<marker>He, Zhou, 2011</marker>
<rawString>Yulan He and Deyu Zhou. 2011. Self-training from labeled features for sentiment analysis. Information Processing and Management, 47(4):606–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter Sentiment Analysis: The Good the Bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM</booktitle>
<contexts>
<context position="4638" citStr="Kouloumpis et al., 2011" startWordPosition="701" endWordPosition="704">f text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitt</context>
<context position="8928" citStr="Kouloumpis et al. (2011)" startWordPosition="1354" endWordPosition="1357">ectivity lexicon, we extracted features for the number of positive, negative, and neutral words as well as the net polarity based on these counts. Individual word polarities were inverted if the word had a child dependency relation with a negation (neg) label. Constrained models use the MPQA lexicon, while unconstrained models use an expanded lexicon that is described in section 6.2. Emoticon Features: Similar to the word polarity features, we computed features for the number of positive, negative, and neutral emoticons, and the net emoticon polarity score. Microblogging Features: As noted by Kouloumpis et al. (2011), the emotional intensity of words in social media messages is often emphasized by changes to the word form such as capitalization, character repetition, and emphasis characters (asterisks, dashes). To capture this intuition we compute features for the number of fully-capitalized words, words with characters repeated more than 3 times (e.g. booooo), and words surround by asterisks or dashes (e.g. *yay*). We also created a binary feature to indicate the presence of a winning score or winning record within the target span (e.g. Oh yeah #Nuggets 15-0). Part-of-Speech Tag Features: Counts of the P</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter Sentiment Analysis: The Good the Bad and the OMG! In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word-emotion association lexicon.</title>
<date>2011</date>
<journal>Computational Intelligence,</journal>
<volume>59</volume>
<issue>000</issue>
<contexts>
<context position="4322" citStr="Mohammad and Turney, 2011" startWordPosition="649" endWordPosition="653">ease refer to Wilson et al. (2013). The remainder of this paper details the approaches, experiments and results associated with each of these models. 2 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction </context>
</contexts>
<marker>Mohammad, Turney, 2011</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2011. Crowdsourcing a word-emotion association lexicon. Computational Intelligence, 59(000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
</authors>
<title>emotional tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="4655" citStr="Mohammad, 2012" startWordPosition="705" endWordPosition="706">tions. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitter texts (Davidov</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>Saif M. Mohammad. 2012. #emotional tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
<author>Philipp G Wetzler</author>
<author>Steven Bethard</author>
</authors>
<title>ClearTK: A UIMA toolkit for statistical natural language processing.</title>
<date>2008</date>
<booktitle>In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC ’08),</booktitle>
<pages>5</pages>
<contexts>
<context position="6147" citStr="Ogren et al., 2008" startWordPosition="927" endWordPosition="930"> data and resources. For both tasks, our data-constrained approach combined standard features for document classification conj → conj hconjunctioni pobj → prep hprepositioni pcomp → prepc hprepositioni prep|punct|cc → ∅ Table 1: Collapsed Dependency Transformation Rules with dependency parse and word polarity features into a weighted linear classifier. For our dataunconstrained models we used pointwise mutual information for lexicon expansion in conjunction with self-training to increase the size of the feature space. 4 Preprocessing and Text Normalization Our systems were built with ClearTK (Ogren et al., 2008) a framework for developing NLP components built on top of Apache UIMA. Our preprocessing pipeline utilized ClearTK’s wrappers for ClearNLP’s (Choi and McCallum, 2013) tokenizer, lemmatizer, part-of-speech (POS) tagger, and dependency parser. ClearNLP’s ability to retain emoticons and emoji as individual tokens made it especially attractive for sentiment analysis. POS tags were mapped from Penn Treebank-style tags to the simplified, Twitter-oriented tags introduced by Gimpel et al. (2011). Dependency graphs output by ClearNLP were also transformed to the Stanford Collapsed dependencies represe</context>
</contexts>
<marker>Ogren, Wetzler, Bethard, 2008</marker>
<rawString>Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard. 2008. ClearTK: A UIMA toolkit for statistical natural language processing. In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC ’08), 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<contexts>
<context position="4591" citStr="Pak and Paroubek, 2010" startWordPosition="693" endWordPosition="696">a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="4179" citStr="Pang and Lee, 2008" startWordPosition="629" endWordPosition="632"> from these systems were submitted to all eight evaluation conditions. For a complete description of the data, tasks, and conditions, please refer to Wilson et al. (2013). The remainder of this paper details the approaches, experiments and results associated with each of these models. 2 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explor</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification Using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="7892" citStr="Pang et al. (2002)" startWordPosition="1185" endWordPosition="1188"> tokenization splits contractions, the list includes the sub-word token n’t as well as the apostrophe-less version of 12 contractions (e.g. cant, wont, etc ... ). To support emoticon-specific features we created a dictionary, which paired 183 emoticons with either a positive or negative polarity. 1http://leebecker.com/resources/semeval-2013 334 6 Message Polarity Classification 6.1 Features Polarized Bag-of-Words Features: Instead of extracting raw bag-of words (BOW), we opted to integrate negation directly into the word representations following the approaches used by Das and Chen (2001) and Pang et al. (2002). All words between a negation word and the first punctuation mark after the negation word were suffixed with a NOT tag – essentially doubling the number of BOW features. We extended this polarized BOW paradigm to include not only the raw word forms but all of the following combinations: raw word, raw word+PTB POS tag, raw word+simplified POS tag, lemma+simplified POS tag. Word Polarity Features: Using a subjectivity lexicon, we extracted features for the number of positive, negative, and neutral words as well as the net polarity based on these counts. Individual word polarities were inverted </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification Using Machine Learning Techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<contexts>
<context position="4950" citStr="Rao and Ravichandran, 2009" startWordPosition="751" endWordPosition="754">and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitter texts (Davidov et al., 2010a; Zhang et al., 2012). 3 System Overview Given our overarching goal of combining polarity lexicons, syntactic information and unlabeled data, our approach centered on first building strong constrained models and then improving performance by adding additional data and resources. F</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Scudder</author>
</authors>
<title>Probability of error of some adaptive pattern-recognition machine.</title>
<date>1965</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>11--363</pages>
<contexts>
<context position="13669" citStr="Scudder, 1965" startWordPosition="2098" endWordPosition="2099"> cost weight parameters were selected via experimentation on the development set to maximize the average positive and negative F1 values. The c values ranged over 10.1, 0.5,1, 2, 5, 10, 20,100} and the label weights wpolarity ranged over 10.1,1, 2, 5, 10, 20, 25, 50, 100}. Final parameters for the constrained model were cost c = 1 and weights wpositive = 1, wnegative = 25, and wneutral = 1. Unconstrained Model: In addition to using the expanded polarity dictionary described in 6.2 for feature extraction, the unconstrained model also makes use of automatically labeled tweets for self-training (Scudder, 1965). In contrast to preparation of the expanded polarity dictionary, the self-training placed no threshold on the examples. Combining the selflabeled tweets, with the official training and development set yielded a new training set consisting of 485,112 examples. Because the self-labeled instances were predominantly tagged neutral, the LIBLINEAR cost parameters were adjusted to heavily discount neutral while emphasizing positive and neutral instances. The size and cost of training this model prevented extensive parameter tuning and instead were chosen based on experience with the constrained mode</context>
</contexts>
<marker>Scudder, 1965</marker>
<rawString>H. J. Scudder. 1965. Probability of error of some adaptive pattern-recognition machine. IEEE Transactions on Information Theory, 11:363–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Prem Melville</author>
</authors>
<title>Documentword co-regularization for semi-supervised sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM ’08,</booktitle>
<pages>1025--1030</pages>
<contexts>
<context position="5084" citStr="Sindhwani and Melville, 2008" startWordPosition="771" endWordPosition="774"> on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitter texts (Davidov et al., 2010a; Zhang et al., 2012). 3 System Overview Given our overarching goal of combining polarity lexicons, syntactic information and unlabeled data, our approach centered on first building strong constrained models and then improving performance by adding additional data and resources. For both tasks, our data-constrained approach combined standard features for document classification conj → conj hconjunctioni pobj → p</context>
</contexts>
<marker>Sindhwani, Melville, 2008</marker>
<rawString>Vikas Sindhwani and Prem Melville. 2008. Documentword co-regularization for semi-supervised sentiment analysis. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM ’08, pages 1025–1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Semisupervised latent variable models for sentence-level sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semisupervised latent variable models for sentence-level sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Predicting elections with twitter what 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (ICWSM</booktitle>
<contexts>
<context position="4375" citStr="Tumasjan et al., 2010" startWordPosition="658" endWordPosition="661"> paper details the approaches, experiments and results associated with each of these models. 2 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classific</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe. 2010. Predicting elections with twitter what 140 characters reveal about political sentiment. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (ICWSM 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="4266" citStr="Turney, 2002" startWordPosition="643" endWordPosition="644">ion of the data, tasks, and conditions, please refer to Wilson et al. (2013). The remainder of this paper details the approaches, experiments and results associated with each of these models. 2 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised </context>
<context position="10369" citStr="Turney (2002)" startWordPosition="1581" endWordPosition="1582">lations include: raw word/raw word, lemma/lemma, lemma/simplified POS tag, simplified POS tag/lemma. If the head node of the relation has a child negation dependency, the pair’s relation label is prefixed with a NEG tag. 6.2 Expanding the Polarity Lexicon Unseen words pose a recurring challenge for both machine learning and dictionary-based approaches to sentiment analysis. This problem is even more prevalent in social media and SMS messages where text lengths are often limited to 140 characters or less. To expand our word polarity lexicon we adopt a framework similar to the one introduced by Turney (2002). Turney’s unsupervised approach centered on computing pointwise mutual information (PMI) between highly polar seed words and bigram phrases extracted from a corpus of product reviews. Instead of relying solely on seed words for polarity, we use the constrained version of the message polarity classifier to tag a corpus of approximately 475,000 unlabeled, English language tweets. These tweets were collected over the period from November 2012 to February 2013. To reduce the number of noisy instances and to obtain a more balanced distribution of sentiment labels, we eliminated all tweets with cla</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--165</pages>
<contexts>
<context position="7129" citStr="Wiebe et al., 2005" startWordPosition="1074" endWordPosition="1077">is. POS tags were mapped from Penn Treebank-style tags to the simplified, Twitter-oriented tags introduced by Gimpel et al. (2011). Dependency graphs output by ClearNLP were also transformed to the Stanford Collapsed dependencies representation (de Marneffe and Manning, 2012) using our own transformation rules (table 1). Input normalization consisted solely of replacing all usernames and URLs with common placeholders. 5 Sentiment Resources A variety of our classifier features rely on manually tagged sentiment lexicons and word lists. In particular we make use of the MPQA Subjectivity Lexicon (Wiebe et al., 2005) as well as manually-created negation and emoticon dictionaries1. The negation word list consisting of negation words such as no and not. Because tokenization splits contractions, the list includes the sub-word token n’t as well as the apostrophe-less version of 12 contractions (e.g. cant, wont, etc ... ). To support emoticon-specific features we created a dictionary, which paired 183 emoticons with either a positive or negative polarity. 1http://leebecker.com/resources/semeval-2013 334 6 Message Polarity Classification 6.1 Features Polarized Bag-of-Words Features: Instead of extracting raw ba</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39:165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffman</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<contexts>
<context position="4758" citStr="Wilson et al., 2005" startWordPosition="719" endWordPosition="722">iment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and even election prediction (Tumasjan et al., 2010). Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012). Classification of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitter texts (Davidov et al., 2010a; Zhang et al., 2012). 3 System Overview Given our overarching goal of combining polarity</context>
</contexts>
<marker>Wilson, Wiebe, Hoffman, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffman. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computation Linguistics.</booktitle>
<contexts>
<context position="3730" citStr="Wilson et al. (2013)" startWordPosition="554" endWordPosition="557">of self-training and polarity lexicon expansion. For task A the constrained system utilized supervised learning, while the unconstrained model made use of the expanded lexicon 333 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 333–340, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics from task B. Output from these systems were submitted to all eight evaluation conditions. For a complete description of the data, tasks, and conditions, please refer to Wilson et al. (2013). The remainder of this paper details the approaches, experiments and results associated with each of these models. 2 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification (Pang and Lee, 2008), induction of word polarity lexicons (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011) and eve</context>
<context position="19077" citStr="Wilson et al. (2013)" startWordPosition="2959" endWordPosition="2962">ned and unconstrained models use identical parameters of cost c = 1 and weights wpositive = 1, wnegative = 2, and wneutral = 1. They vary only in the choice of polarity lexicon. The constrained model uses the MPQA subjectivity lexicon, while the unconstrained model uses the expanded dictionary derived via computation of PMI, which ultimately differentiates these models through the variation in the sentiment path and word polarity features. 8 Experiments and Results In this section we report results for the series of Sentiment Analysis in Twitter tasks at SemEval 2013. Please refer to refer to Wilson et al. (2013) for the exact details about the corpora, evaluation conditions, and methodology. We submitted polarity output for the Message Polarity Classification (task B) and the Contextual Polarity Disambiguation (task A). For each task we submitted system output from our constrained and unconstrained models. As stated above, the constrained models made use of only the training data released for the task, whereas the unconstrained models trained on additional tweets. Each subtask had two test sets one comprised of tweets and the other comprised of SMS messages. Final task 2 337 S G Message / Context 1 +</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computation Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiuzhen Zhang</author>
<author>Yun Zhou</author>
<author>James Bailey</author>
<author>Kotagiri Ramamohanarao</author>
</authors>
<title>Sentiment analysis by augmenting expectation maximisation with lexical knowledge.</title>
<date>2012</date>
<booktitle>Proceedings of the 13th International Conference on Web Information Systems Engineering (WISE2012).</booktitle>
<contexts>
<context position="5290" citStr="Zhang et al., 2012" startWordPosition="804" endWordPosition="807"> of word and phrase sentiment with respect to surrounding context (Wilson et al., 2005) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction (Rao and Ravichandran, 2009), and sentiment classification at the sentence level (T¨ackstr¨om and McDonald, 2011) and document level (Sindhwani and Melville, 2008; He and Zhou, 2011); however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitter texts (Davidov et al., 2010a; Zhang et al., 2012). 3 System Overview Given our overarching goal of combining polarity lexicons, syntactic information and unlabeled data, our approach centered on first building strong constrained models and then improving performance by adding additional data and resources. For both tasks, our data-constrained approach combined standard features for document classification conj → conj hconjunctioni pobj → prep hprepositioni pcomp → prepc hprepositioni prep|punct|cc → ∅ Table 1: Collapsed Dependency Transformation Rules with dependency parse and word polarity features into a weighted linear classifier. For our</context>
</contexts>
<marker>Zhang, Zhou, Bailey, Ramamohanarao, 2012</marker>
<rawString>Xiuzhen Zhang, Yun Zhou, James Bailey, and Kotagiri Ramamohanarao. 2012. Sentiment analysis by augmenting expectation maximisation with lexical knowledge. Proceedings of the 13th International Conference on Web Information Systems Engineering (WISE2012).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>