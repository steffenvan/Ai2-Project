<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.9297345">
Anchors Regularized: Adding Robustness and Extensibility
to Scalable Topic-Modeling Algorithms
</title>
<author confidence="0.997497">
Thang Nguyen
</author>
<affiliation confidence="0.98107925">
iSchool and UMIACS,
University of Maryland
and National Library of Medicine,
National Institutes of Health
</affiliation>
<email confidence="0.981937">
daithang@umiacs.umd.edu
</email>
<author confidence="0.995484">
Yuening Hu
</author>
<affiliation confidence="0.99633">
Computer Science
University of Maryland
</affiliation>
<email confidence="0.987581">
ynhu@cs.umd.edu
</email>
<author confidence="0.966916">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.9788835">
iSchool and UMIACS
University of Maryland
</affiliation>
<email confidence="0.991734">
jbg@umiacs.umd.edu
</email>
<sectionHeader confidence="0.984597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836642857143">
Spectral methods offer scalable alternatives
to Markov chain Monte Carlo and expec-
tation maximization. However, these new
methods lack the rich priors associated with
probabilistic models. We examine Arora et
al.’s anchor words algorithm for topic mod-
eling and develop new, regularized algo-
rithms that not only mathematically resem-
ble Gaussian and Dirichlet priors but also
improve the interpretability of topic models.
Our new regularization approaches make
these efficient algorithms more flexible; we
also show that these methods can be com-
bined with informed priors.
</bodyText>
<sectionHeader confidence="0.99253" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999598">
Topic models are of practical and theoretical inter-
est. Practically, they have been used to understand
political perspective (Paul and Girju, 2010), im-
prove machine translation (Eidelman et al., 2012),
reveal literary trends (Jockers, 2013), and under-
stand scientific discourse (Hall et al., 2008). The-
oretically, their latent variable formulation has
served as a foundation for more robust models
of other linguistic phenomena (Brody and Lapata,
2009).
Modern topic models are formulated as a la-
tent variable model. Like hidden Markov mod-
els (Rabiner, 1989, HMM), each token comes from
one of K unknown distributions. Unlike a HMM,
topic models assume that each document is an ad-
mixture of these hidden components called topics.
Posterior inference discovers the hidden variables
that best explain a dataset. Typical solutions use
MCMC (Griffiths and Steyvers, 2004) or variational
EM (Blei et al., 2003), which can be viewed as local
optimization: searching for the latent variables that
maximize the data likelihood.
An exciting vein of new research provides
provable polynomial-time alternatives. These ap-
proaches provide solutions to hidden Markov mod-
els (Anandkumar et al., 2012), mixture mod-
els (Kannan et al., 2005), and latent variable gram-
mars (Cohen et al., 2013). The key insight is not to
directly optimize observation likelihood but to in-
stead discover latent variables that can reconstruct
statistics of the assumed generative model. Unlike
search-based methods, which can be caught in lo-
cal minima, these techniques are often guaranteed
to find global optima.
These general techniques can be improved by
making reasonable assumptions about the models.
For example, Arora et al. (2012b)’s approach for in-
ference in topic models assume that each topic has
a unique “anchor” word (thus, we call this approach
anchor). This approach is fast and effective; be-
cause it only uses word co-occurrence information,
it can scale to much larger datasets than MCMC or
EM alternatives. We review the anchor method in
Section 2.
Despite their advantages, these techniques are
not a panacea. They do not accommodate the
rich priors that modelers have come to expect.
Priors can improve performance (Wallach et al.,
2009), provide domain adaptation (Daum´e III,
2007; Finkel and Manning, 2009), and guide mod-
els to reflect users’ needs (Hu et al., 2013). In
Section 3, we regularize the anchor method to
trade-off the reconstruction fidelity with the penalty
terms that mimic Gaussian and Dirichlet priors.
Another shortcoming is that these models have
not been scrutinized using standard NLP evalua-
tions. Because these approaches emerged from
the theory community, anchor’s evaluations, when
present, typically use training reconstruction. In
Section 4, we show that our regularized models can
generalize to previously unseen data—as measured
by held-out likelihood (Blei et al., 2003)—and are
more interpretable (Chang et al., 2009; Newman
et al., 2010). We also show that our extension to
the anchor method enables new applications: for
</bodyText>
<page confidence="0.457765">
359
</page>
<note confidence="0.9849535">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359–369,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<tableCaption confidence="0.961474">
Table 1: Notation used. Matrices are in bold
(Q, C), sets are in script S
</tableCaption>
<bodyText confidence="0.997650714285714">
example, using an informed priors to discover con-
cepts of interest.
Having shown that regularization does improve
performance, in Section 5 we explore why. We
discuss the trade-off of training data reconstruction
with sparsity and why regularized topics are more
interpretable.
</bodyText>
<sectionHeader confidence="0.898564" genericHeader="method">
2 Anchor Words: Scalable Topic Models
</sectionHeader>
<bodyText confidence="0.9998514">
In this section, we briefly review the anchor
method and place it in the context of topic model
inference. Once we have established the anchor
objective function, in the next section we regularize
the objective function.
</bodyText>
<note confidence="0.468934">
Rethinking Data: Word Co-occurrence Infer-
</note>
<bodyText confidence="0.990733595744681">
ence in topic models can be viewed as a black box:
given a set of documents, discover the topics that
best explain the data. The difference between an-
chor and conventional inference is that while con-
ventional methods take a collection of documents
as input, anchor takes word co-occurrence statis-
tics. Given a vocabulary of size V , we represent
this joint distribution as Qi,j = p(w1 = i, w2 = j),
each cell represents the probability of words appear-
ing together in a document.
Like other topic modeling algorithms, the output
of the anchor method is the topic word distribu-
tions A with size V * K, where K is the total
number of topics desired, a parameter of the al-
gorithm. The kth column of A will be the topic
distribution over all words for topic k, and Aw,k is
the probability of observing type w given topic k.
Anchors: Topic Representatives The anchor
method (Arora et al., 2012a) is based on the sepa-
rability assumption (Donoho and Stodden, 2003),
which assumes that each topic contains at least one
namesake “anchor word” that has non-zero proba-
bility only in that topic. Intuitively, this means that
each topic has unique, specific word that, when
used, identifies that topic. For example, while
“run”, “base”, “fly”, and “shortstop” are associated
with a topic about baseball, only “shortstop” is un-
ambiguous, so it could serve as this topic’s anchor
word.
Let’s assume that we knew what the anchor
words were: a set S that indexes rows in Q. Now
consider the conditional distribution of word i,
the probability of the rest of the vocabulary given
an observation of word i; we represent this as Qi,·,
as we can construct this by normalizing the rows of
Q. For an anchor word sa E S, this will look like
a topic;¯Q“shortstop”,· will have high probability
for words associated with baseball.
The key insight of the anchor algorithm is that
the conditional distribution of polysemous non-
anchor words can be reconstructed as a linear com-
bination of the conditional distributions of anchor
words. For example,
structed by combining the anchor words “insecta”,
Q“fly”,· could be recon-
“boeing”, and “shortshop”. We represent the coeffi-
cients of this reconstruction as a matrix C, where
</bodyText>
<equation confidence="0.966504666666667">
Ci,k = p(z = k  |w = i). Thus, for any word i,
�¯Qi,· � Ci,k Qsk,·. (1)
skES
</equation>
<bodyText confidence="0.9987766">
The coefficient matrix is not the usual output of a
topic modeling algorithm. The usual output is the
probability of a word given a topic. The coefficient
matrix C is the probability of a topic given a word.
We use Bayes rule to recover the topic distribution
</bodyText>
<equation confidence="0.978831">
p(w = i|z = k) -
Ai,k a p(z = k|w = i)p(w = i)
�=Ci,k ¯Qi,j (2)
j
</equation>
<bodyText confidence="0.9988867">
where p(w) is the normalizer of Q to obtain Qw,·.
The geometric argument for finding the anchor
words is one of the key contributions of Arora et
al. (2012a) and is beyond the scope of this paper.
The algorithms in Section 3 use the anchor selec-
tion subroutine unchanged. The difference in our
approach is in how we discover the anchor coeffi-
cients C.
From Anchors to Topics After we have the an-
chor words, we need to find the coefficients that
</bodyText>
<figure confidence="0.8932704">
K number of topics
V vocabulary size
M document frequency: minimum documents an an-
chor word candidate must appear in
Q word co-occurrence matrix
</figure>
<equation confidence="0.619659916666667">
Qi,j = p(w1 = i, w2 = j)
Q¯ conditional distribution of Q
¯Qi,j = p(w1 = j  |w2 = i)
¯Qi,· row i of ¯
A topic matrix, of size V x K
Aj,k = p(w = j  |z = k)
C anchor coefficient of size K x V
Cj,k = p(z = k  |w = j)
S set of anchor word indexes {s1, ... sx}
λ regularization weight
Q
360
</equation>
<bodyText confidence="0.9806834">
best reconstruct the data Q (Equation 1). Arora
et al. (2012a) chose the C that minimizes the KL
divergence between ¯Qi,· and the reconstruction
based on the anchor word’s conditional word vec-
tors EskES Ci,k Qsk,·,
</bodyText>
<equation confidence="0.99508375">
⎛ ⎞
�Ci,· = argminCi,·DKL ⎝ ¯Qi,·  ||Ci,k¯Qsk,· ⎠ .
skES
(3)
</equation>
<bodyText confidence="0.999763166666667">
The anchor method is fast, as it only de-
pends on the size of the vocabulary once the co-
occurrence statistics Q are obtained. However, it
does not support rich priors for topic models, while
MCMC (Griffiths and Steyvers, 2004) and varia-
tional EM (Blei et al., 2003) methods can. This
prevents models from using priors to guide the
models to discover particular themes (Zhai et al.,
2012), or to encourage sparsity in the models (Yao
et al., 2009). In the rest of this paper, we correct
this lacuna by adding regularization inspired by
Bayesian priors to the anchor algorithm.
</bodyText>
<sectionHeader confidence="0.861481" genericHeader="method">
3 Adding Regularization
</sectionHeader>
<bodyText confidence="0.99990775">
In this section, we add regularizers to the anchor
objective (Equation 3). In this section, we briefly
review regularizers and then add two regularizers,
inspired by Gaussian (L2, Section 3.1) and Dirich-
let priors (Beta, Section 3.2), to the anchor objec-
tive function (Equation 3).
Regularization terms are ubiquitous. They typ-
ically appear as an additional term in an opti-
mization problem. Instead of optimizing a func-
tion just of the data x and parameters β, f(x, β),
one optimizes an objective function that includes
a regularizer that is only a function of parame-
ters: f(w, β) + r(β). Regularizers are critical in
staid methods like linear regression (Ng, 2004),
in workhorse methods such as maximum entropy
modeling (Dud´ık et al., 2004), and also in emerging
fields such as deep learning (Wager et al., 2013).
In addition to being useful, regularization terms
are appealing theoretically because they often corre-
spond to probabilistic interpretations of parameters.
For example, if we are seeking the MLE of a proba-
bilistic model parameterized by β, p(x|β), adding
a regularization term r(β) = EL i=1 β2i corresponds
to adding a Gaussian prior
</bodyText>
<equation confidence="0.991971333333333">
1β2
f (βi) = √ 2πσ2exp �− 2σ
361 (4)
</equation>
<table confidence="0.94627375">
Corpus Train Dev Test Vocab
NIPS 1231 247 262 12182
20NEWS 11243 3760 3726 81604
NYT 9255 2012 1959 34940
</table>
<tableCaption confidence="0.963656">
Table 2: The number of documents in the train,
development, and test folds in our three datasets.
</tableCaption>
<bodyText confidence="0.9888985">
and maximizing log probability of the posterior
(ignoring constant terms) (Rennie, 2003).
</bodyText>
<subsectionHeader confidence="0.954885">
3.1 L2 Regularization
</subsectionHeader>
<bodyText confidence="0.999186272727273">
The simplest form of regularization we can add is
L2 regularization. This is similar to assuming that
probability of a word given a topic comes from a
Gaussian distribution. While the distribution over
topics is typically Dirichlet, Dirichlet distributions
have been replaced by logistic normals in topic
modeling applications (Blei and Lafferty, 2005)
and for probabilistic grammars of language (Cohen
and Smith, 2009).
Augmenting the anchor objective with an L2
penalty yields
</bodyText>
<equation confidence="0.9917818">
⎛ ⎞
�
Ci,· =argminCi,·DKL ⎝ ¯Qi,·  ||Ci,k¯Qsk,· ⎠
skES
+ λkCi,· − µi,·k22, (5)
</equation>
<bodyText confidence="0.999981625">
where regularization weight λ balances the impor-
tance of a high-fidelity reconstruction against the
regularization, which encourages the anchor coeffi-
cients to be close to the vector µ. When the mean
vector µ is zero, this encourages the topic coeffi-
cients to be zero. In Section 4.3, we use a non-zero
mean µ to encode an informed prior to encourage
topics to discover specific concepts.
</bodyText>
<subsectionHeader confidence="0.999715">
3.2 Beta Regularization
</subsectionHeader>
<bodyText confidence="0.999768952380952">
The more common prior for topic models is a
Dirichlet prior (Minka, 2000). However, we cannot
apply this directly because the optimization is done
on a row-by-row basis of the anchor coefficient
matrix C, optimizing C for a fixed word w for and
all topics. If we want to model the probability of
a word, it must be the probability of word w in a
topic versus all other words.
Modeling this dichotomy (one versus all others
in a topic) is possible. The constructive definition
of the Dirichlet distribution (Sethuraman, 1994)
states that if one has a V-dimensional multinomial
θ ∼ Dir(α1 ... αV ), then the marginal distribution
of θw follows θw — Beta(αw, Ei6=w αi). This is
the tool we need to consider the distribution of a
single word’s probability.
This requires including the topic matrix as part
of the objective function. The topic matrix is a lin-
ear transformation of the coefficient matrix (Equa-
tion 2). The objective for beta regularization be-
comes
</bodyText>
<equation confidence="0.98595275">
Ci,· =argminCi,·DKL I ¯Qi,· ||1: Ci,k ¯Qsk,·
sk∈S
� λ 1: log (Beta(Ai,k; a, b)), (6)
sk∈S
</equation>
<bodyText confidence="0.999676125">
where λ again balances reconstruction against the
regularization. To ensure the tractability of this
algorithm, we enforce a convex regularization func-
tion, which requires that a &gt; 1 and b &gt; 1. If we
enforce a uniform prior—]EBeta(a,b) [Ai,k] = 1V—
and that the mode of the distribution is also V1,1
this gives us the following parametric form for a
and b:
</bodyText>
<equation confidence="0.9913085">
x (V � 1)x
V + 1, and b = V + 1 (7)
</equation>
<bodyText confidence="0.949085">
for real x greater than zero.
</bodyText>
<subsectionHeader confidence="0.997608">
3.3 Initialization and Convergence
</subsectionHeader>
<bodyText confidence="0.999986230769231">
Equation 5 and Equation 6 are optimized using L-
BFGS gradient optimization (Galassi et al., 2003).
We initialize C randomly from Dir(α) with α =
60V (Wallach et al., 2009). We update C after opti-
mizing all V rows. The newly updated C replaces
the old topic coefficients. We track how much
the topic coefficients C change between two con-
secutive iterations i and i + 1 and represent it as
AC - 11Ci+1—Ci112. We stop optimization when
AC &lt; δ. When δ = 0.1, the L2 and unregularized
anchor algorithm converges after a single iteration,
while beta regularization typically converges after
fewer than ten iterations (Figure 4).
</bodyText>
<sectionHeader confidence="0.983418" genericHeader="method">
4 Regularization Improves Topic Models
</sectionHeader>
<bodyText confidence="0.999232689655173">
In this section, we measure the performance of
our proposed regularized anchor word algorithms.
We will refer to specific algorithms in bold. For
example, the original anchor algorithm is an-
chor. Our L2 regularized variant is anchor-L2,
1For a, b &lt; 1, the expected value is still the uniform
distribution but the mode lies at the boundaries of the simplex.
This corresponds to a sparse Dirichlet distribution, which our
optimization cannot at present model.
and our beta regularized variant is anchor-beta.
To provide conventional baselines, we also com-
pare our methods against topic models from varia-
tional inference (Blei et al., 2003, variational) and
MCMC (Griffiths and Steyvers, 2004; McCallum,
2002, MCMC).
We apply these inference strategies on three di-
verse corpora: scientific articles from the Neural
Information Processing Society (NIPS),2 Internet
newsgroups postings (20NEWS),3 and New York
Times editorials (Sandhaus, 2008, NYT). Statistics
for the datasets are summarized in Table 2. We split
each dataset into a training fold (70%), develop-
ment fold (15%), and a test fold (15%): the training
data are used to fit models; the development set are
used to select parameters (anchor threshold M, doc-
ument prior α, regularization weight λ); and final
results are reported on the test fold.
We use two evaluation measures, held-out likeli-
hood (Blei et al., 2003, HL) and topic interpretabil-
ity (Chang et al., 2009; Newman et al., 2010, TI).
Held-out likelihood measures how well the model
can reconstruct held-out documents that the model
has never seen before. This is the typical evaluation
for probabilistic models. Topic interpretability is a
more recent metric to capture how useful the topics
can be to human users attempting to make sense of
a large datasets.
Held-out likelihood cannot be computed with
existing anchor algorithms, so we use the topic
distributions learned from anchor as input to a ref-
erence variational inference implementation (Blei
et al., 2003) to compute HL. This requires an ad-
ditional parameter, the Dirichlet prior α for the
per-document distribution over topics. We select α
using grid search on the development set.
To compute TI and evaluate topic coherence,
we use normalized pairwise mutual informa-
tion (NPMI) (Lau et al., 2014) over topics’ twenty
most probable words. Topic coherence is com-
puted against the NPMI of a reference corpus. For
coherence evaluations, we use both intrinsic and
extrinsic text collections to compute NPMI. Intrin-
sic coherence (TI-i) is computed on training and
development data at development time and on train-
ing and test data at test time. Extrinsic coherence
(TI-e) is computed from English Wikipedia articles,
with disjoint halves (1.1 million pages each) for
distinct development and testing TI-e evaluation.
</bodyText>
<figure confidence="0.849004">
2http://cs.nyu.edu/-roweis/data.html
3http://qwone.com/-jason/20Newsgroups/
a =
362
</figure>
<figureCaption confidence="0.919896666666667">
Figure 1: Grid search for document frequency M for our datasets with 20 topics (other configurations not
shown) on development data. The performance on both HL and TI score indicate that the unregularized
anchor algorithm is very sensitive to M. The M selected here is applied to subsequent models.
</figureCaption>
<figure confidence="0.999802763888888">
100 300 500 700 900
Document Frequency M
●
● ● ●
20NEWS
−388
●
−390
●
●
−392
●
TI−i Score
●
HL Score
NIPS
●
−4680
●
−4690
●
●
−4700
●
●
−4710
−4720
●
● ● ●
● ●
−882.5
● ●
NYT
−885.0
●
−887.5
−890.0
●
0.07
0.06
0.05
0.04
0.03
0.02
0.065
0.060
0.055
0.10
0.09
0.08
0.07
0.06
100 300 500 700 900
Document Frequency M
●
20NEWS
●
●
●
● ● ●
●
●
●
●
●
NIPS
●
●
●
●
●
●
●
NYT
● ●
●
●
Topics ● 20 40 60 80
Beta
L2
●
● ● ●●●●●●
● ●
● ● ●●●●●
● ●
●
●
●●●
20NEWS
NIPS
00.01 0.1 0.5 1 00.01 0.1 0.5 1
Regularization Weight λ
NYT
●
● ● ● ● ● ●●●●●●
Topics ● 20 40 60 80
00.01 0.1 0.5 1 00.01 0.1 0.5 1
Regularization Weight λ
−390
−395
−400
−405
−410
0.10
0.08
0.06
0.04
TI−i Score
0.02
0.08
HL Score
−4650
0.06
4700
0.04
4750
0.02
4800
0.15
NYT
−880
−890
−900
−910
−920
0.12
0.09
●
● ● ● ●●●●●
0.06
● ● ● ● ● ●●●●●● ● ● ● ● ●●●●●
L2
● ● ● ● ● ●●●●●●
●
● ● ● ●●●●
●
●
● ●
● ●●●●●
●
● ● ● ●●●●●
20NEWS
NIPS
Beta
</figure>
<figureCaption confidence="0.979639333333333">
Figure 2: Selection of A based on HL and TI scores on the development set. The value of A = 0 is
equivalent to the original anchor algorithm; regularized versions find better solutions as the regularization
weight A becomes non-zero.
</figureCaption>
<table confidence="0.93706225">
4.1 Grid Search for Parameters on Regularization Weight Once we select a cutoff
Development Set M for each combination of dataset, number of top-
Anchor Threshold A good anchor word must ics K and a evaluation measure, we select a reg-
have a unique, specific context but also explain ularization weight A on the development set. Fig-
</table>
<bodyText confidence="0.9803711875">
other words well. A word that appears only once ure 2 shows that beta regularization framework im-
will have a very specific cooccurence pattern but proves topic interpretability TI-i on all datasets and
will explain other words’ coocurrence poorly be- improved the held-out likelihood HL on 20NEWS.
cause the observations are so sparse. As discussed The L2 regularization also improves held-out like-
in Section 2, the anchor method uses document lihood HL for the 20NEWS corpus (Figure 2).
frequency M as a threshold to only consider words In the interests of space, we do not show the
with robust counts. figures for selecting M and A using TI-e, which is
Because all regularizations benefit equally similar to TI-i: anchor-beta improves TI-e score on
from higher-quality anchor words, we use cross- all datasets, anchor-L2 improves TI-e on 20NEWS
validation to select the document frequency cut- and NIPS with 20 topics and NYT with 40 topics.
off M using the unregularized anchor algorithm. 4.2 Evaluating Regularization
Figure 1 shows the performance of anchor with With document frequency M and regularization
different M on our three datasets with 20 topics for weight A selected from the development set, we
our two measures HL and TI-i.
363
compare the performance of those models on the
test set. We also compare with standard implemen-
tations of Latent Dirichlet Allocation: Blei’s LDAC
(variational) and Mallet (mcmc). We run 100 iter-
ations for LDAC and 5000 iterations for Mallet.
Each result is averaged over three random runs
and appears in Figure 3. The highly-tuned, widely-
used implementations uniformly have better held-
out likelihood than anchor-based methods, but the
much faster anchor methods are often comparable.
Within anchor-based methods, L2-regularization
offers comparable held-out likelihood as unregular-
ized anchor, while anchor-beta often has better
interpretability. Because of the mismatch between
the specialized vocabulary of NIPS and the general-
purpose language of Wikipedia, TI-e has a high
variance.
</bodyText>
<subsectionHeader confidence="0.990965">
4.3 Informed Regularization
</subsectionHeader>
<bodyText confidence="0.99998514893617">
A frequent use of priors is to add information to a
model. This is not possible with the existing an-
chor method. An informed prior for topic models
seeds a topic with words that describe a topic of in-
terest. In a topic model, these seeds will serve as a
“magnet”, attracting similar words to the topic (Zhai
et al., 2012).
We can achieve a similar goal with anchor-L2.
Instead of encouraging anchor coefficients to be
zero in Equation 5, we can instead encourage word
probabilities to close to an arbitrary mean Ai,k.
This vector can reflect expert knowledge.
One example of a source of expert knowledge
is Linguistic Inquiry and Word Count (Pennebaker
and Francis, 1999, LIWC), a dictionary of key-
words related to sixty-eight psychological concepts
such as positive emotions, negative emotions, and
death. For example, it associates “excessive, estate,
money, cheap, expensive, living, profit, live, rich,
income, poor, etc.” for the concept materialism.
We associate each anchor word with its closest
LIWC category based on the cooccurrence matrix
Q. This is computed by greedily finding the an-
chor word that has the highest cooccurrence score
for any LIWC category: we define the score of a
category to anchor word wsk as Ei Qsk,i, where i
ranges over words in this category; we compute the
scores of all categories to all anchor words; then
we find the highest score and assign the category to
that anchor word; we greedily repeat this process
until all anchor words have a category.
Given these associations, we create a goal mean
Ai,k. If there are Li anchor words associated with
LIWC word i, Ai,k = Li1 if this keyword i is associ-
ated with anchor word wsk and zero otherwise.
We apply anchor-L2 with informed priors on
NYT with twenty topics and compared the topics
against the original topics from anchor. Table 3
shows that the topic with anchor word “soviet”,
when combined with LIWC, draws in the new words
“bush” and “nuclear”; reflecting the threats of force
during the cold war. For the topic with topic word
“arms”, when associated with the LIWC category
with the terms “agree” and “agreement”, draws
in “clinton”, who represented a more conciliatory
foreign policy compared to his republican prede-
cessors.
</bodyText>
<sectionHeader confidence="0.998305" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.998469833333334">
Having shown that regularization can improve the
anchor topic modeling algorithm, in this section
we discuss why these regularizations can improve
the model and the implications for practitioners.
Efficiency Efficiency is a function of the number
of iterations and the cost of each iteration. Both
anchor and anchor-L2 require a single iteration,
although the latter’s iteration is slightly more ex-
pensive. For beta, as described in Section 3.2,
we update anchor coefficients C row by row, and
then repeat the process over several iterations until
it converges. However, it often converges within
ten iterations (Figure 4) on all three datasets: this
requires much fewer iterations than MCMC or vari-
ational inference, and the iterations are less expen-
sive. In addition, since we optimize each row Ci,·
independently, the algorithm can be easily paral-
lelized.
Sensitivity to Document Frequency While the
original anchor is sensitive to the document fre-
quency M (Figure 1), adding regularization makes
this less critical. Both anchor-L2 and anchor-beta
are less sensitive to M than anchor.
To highlight this, we compare the topics of an-
chor and anchor-beta when M = 100. As Table 4
shows, the words “article”, “write”, “don” and
“doe” appear in most of anchor’s topics. While
anchor-L2 also has some bad topics, it still can find
reasonable topics, demonstrating anchor-beta’s
greater robustness to suboptimal M.
</bodyText>
<note confidence="0.40058">
L2 (Sometimes) Improves Generalization As
</note>
<figureCaption confidence="0.6266805">
Figure 2 shows, anchor-L2 sometimes improves
held-out development likelihood for the smaller
</figureCaption>
<figure confidence="0.998314329113924">
364
Algorithm
● anchor
MCMC
variational
anchor−beta anchor−L2
20 40 60 80
topic number
20 40 60 80
topic number
20 40 60 80
topic number
20NEWS
HL
● ● ● ●
TI−e
● ● ● ●
TI−i
●
● ● ●
TI−e
● ●
●
●
NIPS
● ●
HL
●
●
TI−i
● ● ●
●
NYT
● ● ●
HL
●
TI−e
●
● ● ●
TI−i
● ● ● ●
−390
−395
−400
−405
−410
0.07
0.06
0.05
0.04
0.03
0.10
0.08
0.06
−4460
−4480
−4500
−4520
−4540
−4560
−4580
0.11
0.10
0.09
0.08
0.09
0.08
0.07
0.06
−860
−870
−880
0.09
0.08
0.07
0.14
0.12
0.10
0.08
</figure>
<figureCaption confidence="0.976001">
Figure 3: Comparing anchor-beta and anchor-L2 against the original anchor and the traditional vari-
ational and MCMC on HL score and TI score. variational and mcmc provide the best held-out gener-
alization. anchor-beta sometimes gives the best TI score and is consistently better than anchor. The
specialized vocabulary of NIPS causes high variance for the extrinsic interpretability evaluation (TI-e).
</figureCaption>
<table confidence="0.99936355">
Topic Shared Words Original (Top, green) vs. Informed L2 (Bottom, orange)
soviet american make president soviet union gorbachev moscow russian force economic world europe politi-
war years cal communist lead reform germany country
military state service washington bush army unite chief troops
officer nuclear time week
district assembly board city county district representative manhattan brooklyn queens election bronx council
member state york island local incumbent housing municipal
people party group social republican year make years friend
vote compromise million
peace american force government israel peace war military country minister leaders nation world palestinian
political president state unite israeli election
washington
offer justice aid deserve make bush years fair clinton hand
arms arms bush congress force iraq make north administration treaty missile defense war military korea
nuclear president state washington weapon reagan
agree agreement american accept unite share clinton
years
trade administration america american country world market japan foreign china policy price political
economic government make president state business economy congress year years clinton bush
trade unite washington buy
</table>
<tableCaption confidence="0.995707">
Table 3: Examples of topic comparison between anchor and informed anchor-L2. A topic is labeled
</tableCaption>
<bodyText confidence="0.98086845">
with the anchor word for that topic. The bold words are the informed prior from LIWC. With an informed
prior, relevant words appear in the top words of a topic; this also draws in other related terms (red).
20NEWS corpus. However, the A selected on devel-
opment data does not always improve test set per-
formance. This, in Figure 3, anchor-beta closely
tracks anchor. Thus, L2 regularization does not
hurt generalization while imparting expressiveness
and robustness to parameter settings.
Beta Improves Interpretability Figure 3 shows
that anchor-beta improves topic interpretability
(TI) compared to unregularized anchor methods. In
this section, we try to understand why.
We first compare the topics from the original
anchor against anchor-beta to analyze the topics
qualitatively. Table 5 shows that beta regulariza-
tion promotes rarer words within a topic and de-
motes common words. For example, in the topic
about hockey with the anchor word game, “run”
and “good”—ambiguous, polysemous words—in
the unregularized topic are replaced by “playoff”
</bodyText>
<page confidence="0.644493">
365
</page>
<table confidence="0.828605090909091">
Topic anchor anchor-beta
frequently article write don doe make time people good article write don doe make people time good
file question email file
debate write article people make don doe god key gov- people make god article write don doe key
ernment time point government
wings game team write wings article win red play game team wings win red hockey play season
hockey year player fan
stats player team write game article stats year good stats player season league baseball fan team in-
play doe dividual playoff nhl
compile program file write email doe windows call prob- compile program code file ftp advance package
lem run don error windows sun
</table>
<tableCaption confidence="0.917515">
Table 4: Topics from anchor and anchor-beta with M = 100 on 20NEWS with 20 topics. Each topic is
identified with its associated anchor word. When M = 100, the topics of anchor suffer: the four colored
words appear in almost every topic. anchor-beta, in contrast, is less sensitive to suboptimal M.
</tableCaption>
<figureCaption confidence="0.872068">
Figure 4: Convergence of anchor coefficient C for
</figureCaption>
<bodyText confidence="0.951166538461539">
anchor-beta. AC is the difference of current C
from the C at the previous iteration. C is converged
within ten iterations for all three datasets.
and “trade” in the regularized topic. These words
are less ambiguous and more likely to make sense
to a consumer of topic models.
Figure 5 shows why this happens. Compared
to the unregularized topics from anchor, the beta
regularized topic steals from the rich and creates a
more uniform distribution. Thus, highly frequent
words do not as easily climb to the top of the distri-
bution, and the topics reflect topical, relevant words
rather than globally frequent terms.
</bodyText>
<sectionHeader confidence="0.998209" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993838709677">
A topic model is a popular tool for quickly get-
ting the gist of large corpora. However, running
such an analysis on these large corpora entail a
substantial computational cost. While techniques
such as anchor algorithms offer faster solutions, it
comes at the cost of the expressive priors common
in Bayesian formulations.
This paper introduces two different regulariza-
tions that offer users more interpretable models
and the ability to inject prior knowledge without
sacrificing the speed and generalizability of the
underlying approach. However, one sacrifice that
this approach does make is the beautiful theoretical
guarantees of previous work. An important piece
of future work is a theoretical understanding of
generalizability in extensible, regularized models.
Incorporating other regularizations could further
improve performance or unlock new applications.
Our regularizations do not explicitly encourage
sparsity; applying other regularizations such as Li
could encourage true sparsity (Tibshirani, 1994),
and structured priors (Andrzejewski et al., 2009)
could efficiently incorporate constraints on topic
models.
These regularizations could improve spectral al-
gorithms for latent variables models, improving the
performance for other NLP tasks such as latent vari-
able PCFGs (Cohen et al., 2013) and HMMs (Anand-
kumar et al., 2012), combining the flexibility and
robustness offered by priors with the speed and
accuracy of new, scalable algorithms.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99990675">
We would like to thank the anonymous reviewers,
Hal Daum´e III, Ke Wu, and Ke Zhai for their help-
ful comments. This work was supported by NSF
Grant IIS-1320538. Boyd-Graber is also supported
by NSF Grant CCF-1018625. Any opinions, find-
ings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.
</bodyText>
<figure confidence="0.9934393125">
0 5 10 15 20
Iteration
ΔC
40
30
20
10
0
●
●
Dataset ● 20NEWS NIPS NYT
●
● ● ● ● ● ● ●
366
log p(word  |topic)
−10
−15
−20
−10
−15
−20
−5
−5
0
0
computer
drive
game
god
power
anchor anchor−beta
Rank of word in topic (topic shown by anchor word)
</figure>
<figureCaption confidence="0.748629">
Figure 5: How beta regularization influences the topic distribution. Each topic is identified with its
</figureCaption>
<bodyText confidence="0.985600882352941">
associated anchor word. Compared to the unregularized anchor method, anchor-beta steals probability
mass from the “rich” and prefers a smoother distribution of probability mass. These words often tend to
be unimportant, polysemous words common across topics.
Topic Shared Words anchor (Top, green) vs. anchor-beta (Bottom, orange)
computer computer means science screen system phone university problem doe work windows internet
software chip mac set fax technology information data
quote mhz pro processor ship remote print devices complex cpu
electrical transfer ray engineering serial reduce
power power play period supply car good make high problem work back turn control current
ground light battery engine small time
circuit oil wire unit water heat hot ranger input total joe plug
god god jesus christian bible faith church life christ belief people make things true doe
religion hell word lord truth love sin christianity atheist peace heaven
game game team player play win fan hockey season baseball run good
red wings score division league goal leaf cup toronto playoff trade
drive drive disk hard scsi controller card floppy ide mac bus problem work
speed monitor switch apple cable internal port meg ram pin
</bodyText>
<tableCaption confidence="0.756701666666667">
Table 5: Comparing topics—labeled by their anchor word—from anchor and anchor-beta. With beta
regularization, relevant words are promoted, while more general words are suppressed, improving topic
coherence.
</tableCaption>
<sectionHeader confidence="0.939163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998841972972973">
Animashree Anandkumar, Daniel Hsu, and Sham M.
Kakade. 2012. A method of moments for mixture
models and hidden markov models. In Proceedings
of Conference on Learning Theory.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings
of the International Conference of Machine Learn-
ing.
Sanjeev Arora, Rong Ge, Yoni Halpern, David M.
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2012a. A practical algorithm
for topic modeling with provable guarantees. CoRR,
abs/1212.4777.
Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012b.
Learning topic models - going beyond svd. CoRR,
abs/1204.1956.
David M. Blei and John D. Lafferty. 2005. Correlated
topic models. In Proceedings ofAdvances in Neural
Information Processing Systems.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics, Athens, Greece.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Proceedings ofAdvances in Neural Information Pro-
cessing Systems.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Conference
of the North American Chapter of the Association
for Computational Linguistics.
</reference>
<page confidence="0.531619">
367
</page>
<reference confidence="0.999598617647059">
Shay Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the Association for Com-
putational Linguistics.
David Donoho and Victoria Stodden. 2003. When
does non-negative matrix factorization give correct
decomposition into parts? page 2004. MIT Press.
Miroslav Dud´ık, Steven J. Phillips, and Robert E.
Schapire. 2004. Performance guarantees for reg-
ularized maximum entropy density estimation. In
Proceedings of Conference on Learning Theory.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the Association
for Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Morristown, NJ,
USA.
Mark Galassi, Jim Davies, James Theiler, Brian Gough,
Gerard Jungman, Michael Booth, and Fabrice Rossi.
2003. Gnu Scientific Library: Reference Manual.
Network Theory Ltd.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl 1):5228–5235.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of Emperical Methods
in Natural Language Processing.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2013. Interactive topic modeling.
Machine Learning Journal.
Matt L. Jockers. 2013. Macroanalysis: Digital Meth-
ods and Literary History. Topics in the Digital Hu-
manities. University of Illinois Press.
Ravindran Kannan, Hadi Salmasian, and Santosh Vem-
pala. 2005. The spectral method for general mixture
models. In Proceedings of Conference on Learning
Theory.
Ken Lang. 2007. 20 newsgroups data set.
Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In Proceedings of the European Chapter of the Asso-
ciation for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Thomas P. Minka. 2000. Estimating a
dirichlet distribution. Technical report, Mi-
crosoft. http://research.microsoft.com/en-
us/um/people/minka/papers/dirichlet/.
David Newman, Jey Han Lau, Karl Grieser, and Timo-
thy Baldwin. 2010. Automatic evaluation of topic
coherence. In Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regu-
larization, and rotational invariance. In Proceedings
of the International Conference of Machine Learn-
ing.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering
multi-faceted topics. In Association for the Advance-
ment of Artificial Intelligence.
James W. Pennebaker and Martha E. Francis. 1999.
Linguistic Inquiry and Word Count. Lawrence Erl-
baum, 1 edition, August.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–
286.
Jason Rennie. 2003. On l2-norm regularization and
the Gaussian prior.
Sam Roweis. 2002. NIPS 1-12 Dataset.
Evan Sandhaus. 2008. The New
York Times annotated corpus.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2008T19.
Jayaram Sethuraman. 1994. A constructive definition
of Dirichlet priors. Statistica Sinica, 4:639–650.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267–288.
Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. In Pro-
ceedings of Advances in Neural Information Pro-
cessing Systems, pages 351–359.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter.
In Proceedings of Advances in Neural Information
Processing Systems.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Knowledge
Discovery and Data Mining.
</reference>
<page confidence="0.59946">
368
</page>
<reference confidence="0.9967526">
Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-
hamad Alkhouja. 2012. Mr. LDA: A flexible large
scale topic modeling package using variational infer-
ence in mapreduce. In Proceedings of World Wide
Web Conference.
</reference>
<page confidence="0.953236">
369
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.301297">
<title confidence="0.9933365">Anchors Regularized: Adding Robustness and to Scalable Topic-Modeling Algorithms</title>
<author confidence="0.960935">Thang</author>
<affiliation confidence="0.97248075">and University of and National Library of National Institutes of Health</affiliation>
<email confidence="0.998699">daithang@umiacs.umd.edu</email>
<author confidence="0.582572">Yuening</author>
<affiliation confidence="0.9802125">Computer University of Maryland</affiliation>
<email confidence="0.999548">ynhu@cs.umd.edu</email>
<author confidence="0.989207">Jordan</author>
<affiliation confidence="0.831746">and University of Maryland</affiliation>
<email confidence="0.999784">jbg@umiacs.umd.edu</email>
<abstract confidence="0.993718266666667">Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.’s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Animashree Anandkumar</author>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
</authors>
<title>A method of moments for mixture models and hidden markov models.</title>
<date>2012</date>
<booktitle>In Proceedings of Conference on Learning Theory.</booktitle>
<contexts>
<context position="2166" citStr="Anandkumar et al., 2012" startWordPosition="310" endWordPosition="313">M), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research provides provable polynomial-time alternatives. These approaches provide solutions to hidden Markov models (Anandkumar et al., 2012), mixture models (Kannan et al., 2005), and latent variable grammars (Cohen et al., 2013). The key insight is not to directly optimize observation likelihood but to instead discover latent variables that can reconstruct statistics of the assumed generative model. Unlike search-based methods, which can be caught in local minima, these techniques are often guaranteed to find global optima. These general techniques can be improved by making reasonable assumptions about the models. For example, Arora et al. (2012b)’s approach for inference in topic models assume that each topic has a unique “ancho</context>
<context position="30542" citStr="Anandkumar et al., 2012" startWordPosition="5055" endWordPosition="5059">eneralizability in extensible, regularized models. Incorporating other regularizations could further improve performance or unlock new applications. Our regularizations do not explicitly encourage sparsity; applying other regularizations such as Li could encourage true sparsity (Tibshirani, 1994), and structured priors (Andrzejewski et al., 2009) could efficiently incorporate constraints on topic models. These regularizations could improve spectral algorithms for latent variables models, improving the performance for other NLP tasks such as latent variable PCFGs (Cohen et al., 2013) and HMMs (Anandkumar et al., 2012), combining the flexibility and robustness offered by priors with the speed and accuracy of new, scalable algorithms. Acknowledgments We would like to thank the anonymous reviewers, Hal Daum´e III, Ke Wu, and Ke Zhai for their helpful comments. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also supported by NSF Grant CCF-1018625. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor. 0 5 10 15 20 Iteration ΔC 40 30 20 10 0 ● ● Dataset ● 20NEWS NIPS NYT ● ● ● ● ● ● ● ● 366 log p(w</context>
</contexts>
<marker>Anandkumar, Hsu, Kakade, 2012</marker>
<rawString>Animashree Anandkumar, Daniel Hsu, and Sham M. Kakade. 2012. A method of moments for mixture models and hidden markov models. In Proceedings of Conference on Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via Dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="30266" citStr="Andrzejewski et al., 2009" startWordPosition="5014" endWordPosition="5017">ect prior knowledge without sacrificing the speed and generalizability of the underlying approach. However, one sacrifice that this approach does make is the beautiful theoretical guarantees of previous work. An important piece of future work is a theoretical understanding of generalizability in extensible, regularized models. Incorporating other regularizations could further improve performance or unlock new applications. Our regularizations do not explicitly encourage sparsity; applying other regularizations such as Li could encourage true sparsity (Tibshirani, 1994), and structured priors (Andrzejewski et al., 2009) could efficiently incorporate constraints on topic models. These regularizations could improve spectral algorithms for latent variables models, improving the performance for other NLP tasks such as latent variable PCFGs (Cohen et al., 2013) and HMMs (Anandkumar et al., 2012), combining the flexibility and robustness offered by priors with the speed and accuracy of new, scalable algorithms. Acknowledgments We would like to thank the anonymous reviewers, Hal Daum´e III, Ke Wu, and Ke Zhai for their helpful comments. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also supported</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating domain knowledge into topic modeling via Dirichlet forest priors. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Arora</author>
<author>Rong Ge</author>
<author>Yoni Halpern</author>
<author>David M Mimno</author>
<author>Ankur Moitra</author>
<author>David Sontag</author>
<author>Yichen Wu</author>
<author>Michael Zhu</author>
</authors>
<title>A practical algorithm for topic modeling with provable guarantees.</title>
<date>2012</date>
<location>CoRR, abs/1212.4777.</location>
<contexts>
<context position="2680" citStr="Arora et al. (2012" startWordPosition="391" endWordPosition="394">-time alternatives. These approaches provide solutions to hidden Markov models (Anandkumar et al., 2012), mixture models (Kannan et al., 2005), and latent variable grammars (Cohen et al., 2013). The key insight is not to directly optimize observation likelihood but to instead discover latent variables that can reconstruct statistics of the assumed generative model. Unlike search-based methods, which can be caught in local minima, these techniques are often guaranteed to find global optima. These general techniques can be improved by making reasonable assumptions about the models. For example, Arora et al. (2012b)’s approach for inference in topic models assume that each topic has a unique “anchor” word (thus, we call this approach anchor). This approach is fast and effective; because it only uses word co-occurrence information, it can scale to much larger datasets than MCMC or EM alternatives. We review the anchor method in Section 2. Despite their advantages, these techniques are not a panacea. They do not accommodate the rich priors that modelers have come to expect. Priors can improve performance (Wallach et al., 2009), provide domain adaptation (Daum´e III, 2007; Finkel and Manning, 2009), and g</context>
<context position="5759" citStr="Arora et al., 2012" startWordPosition="891" endWordPosition="894">s word co-occurrence statistics. Given a vocabulary of size V , we represent this joint distribution as Qi,j = p(w1 = i, w2 = j), each cell represents the probability of words appearing together in a document. Like other topic modeling algorithms, the output of the anchor method is the topic word distributions A with size V * K, where K is the total number of topics desired, a parameter of the algorithm. The kth column of A will be the topic distribution over all words for topic k, and Aw,k is the probability of observing type w given topic k. Anchors: Topic Representatives The anchor method (Arora et al., 2012a) is based on the separability assumption (Donoho and Stodden, 2003), which assumes that each topic contains at least one namesake “anchor word” that has non-zero probability only in that topic. Intuitively, this means that each topic has unique, specific word that, when used, identifies that topic. For example, while “run”, “base”, “fly”, and “shortstop” are associated with a topic about baseball, only “shortstop” is unambiguous, so it could serve as this topic’s anchor word. Let’s assume that we knew what the anchor words were: a set S that indexes rows in Q. Now consider the conditional di</context>
<context position="7621" citStr="Arora et al. (2012" startWordPosition="1221" endWordPosition="1224">the coefficients of this reconstruction as a matrix C, where Ci,k = p(z = k |w = i). Thus, for any word i, �¯Qi,· � Ci,k Qsk,·. (1) skES The coefficient matrix is not the usual output of a topic modeling algorithm. The usual output is the probability of a word given a topic. The coefficient matrix C is the probability of a topic given a word. We use Bayes rule to recover the topic distribution p(w = i|z = k) - Ai,k a p(z = k|w = i)p(w = i) �=Ci,k ¯Qi,j (2) j where p(w) is the normalizer of Q to obtain Qw,·. The geometric argument for finding the anchor words is one of the key contributions of Arora et al. (2012a) and is beyond the scope of this paper. The algorithms in Section 3 use the anchor selection subroutine unchanged. The difference in our approach is in how we discover the anchor coefficients C. From Anchors to Topics After we have the anchor words, we need to find the coefficients that K number of topics V vocabulary size M document frequency: minimum documents an anchor word candidate must appear in Q word co-occurrence matrix Qi,j = p(w1 = i, w2 = j) Q¯ conditional distribution of Q ¯Qi,j = p(w1 = j |w2 = i) ¯Qi,· row i of ¯ A topic matrix, of size V x K Aj,k = p(w = j |z = k) C anchor co</context>
</contexts>
<marker>Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, Zhu, 2012</marker>
<rawString>Sanjeev Arora, Rong Ge, Yoni Halpern, David M. Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. 2012a. A practical algorithm for topic modeling with provable guarantees. CoRR, abs/1212.4777.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sanjeev Arora</author>
</authors>
<title>Rong Ge, and Ankur Moitra. 2012b. Learning topic models - going beyond svd.</title>
<location>CoRR, abs/1204.1956.</location>
<marker>Arora, </marker>
<rawString>Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012b. Learning topic models - going beyond svd. CoRR, abs/1204.1956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2005</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="11069" citStr="Blei and Lafferty, 2005" startWordPosition="1823" endWordPosition="1826">2 12182 20NEWS 11243 3760 3726 81604 NYT 9255 2012 1959 34940 Table 2: The number of documents in the train, development, and test folds in our three datasets. and maximizing log probability of the posterior (ignoring constant terms) (Rennie, 2003). 3.1 L2 Regularization The simplest form of regularization we can add is L2 regularization. This is similar to assuming that probability of a word given a topic comes from a Gaussian distribution. While the distribution over topics is typically Dirichlet, Dirichlet distributions have been replaced by logistic normals in topic modeling applications (Blei and Lafferty, 2005) and for probabilistic grammars of language (Cohen and Smith, 2009). Augmenting the anchor objective with an L2 penalty yields ⎛ ⎞ � Ci,· =argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ skES + λkCi,· − µi,·k22, (5) where regularization weight λ balances the importance of a high-fidelity reconstruction against the regularization, which encourages the anchor coefficients to be close to the vector µ. When the mean vector µ is zero, this encourages the topic coefficients to be zero. In Section 4.3, we use a non-zero mean µ to encode an informed prior to encourage topics to discover specific concepts. 3.2 Be</context>
</contexts>
<marker>Blei, Lafferty, 2005</marker>
<rawString>David M. Blei and John D. Lafferty. 2005. Correlated topic models. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="1886" citStr="Blei et al., 2003" startWordPosition="269" endWordPosition="272">al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research provides provable polynomial-time alternatives. These approaches provide solutions to hidden Markov models (Anandkumar et al., 2012), mixture models (Kannan et al., 2005), and latent variable grammars (Cohen et al., 2013). The key insight is not to directly optimize observation likelihood but to instead discover latent variables that can reconstruct statistics of the assumed generative model. Unlike search-based methods, which can be caught in loca</context>
<context position="3866" citStr="Blei et al., 2003" startWordPosition="576" endWordPosition="579">Finkel and Manning, 2009), and guide models to reflect users’ needs (Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data—as measured by held-out likelihood (Blei et al., 2003)—and are more interpretable (Chang et al., 2009; Newman et al., 2010). We also show that our extension to the anchor method enables new applications: for 359 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359–369, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Table 1: Notation used. Matrices are in bold (Q, C), sets are in script S example, using an informed priors to discover concepts of interest. Having shown that regularization does improve performance, in Section 5 we explore why. We discuss the t</context>
<context position="8879" citStr="Blei et al., 2003" startWordPosition="1466" endWordPosition="1469">w = j) S set of anchor word indexes {s1, ... sx} λ regularization weight Q 360 best reconstruct the data Q (Equation 1). Arora et al. (2012a) chose the C that minimizes the KL divergence between ¯Qi,· and the reconstruction based on the anchor word’s conditional word vectors EskES Ci,k Qsk,·, ⎛ ⎞ �Ci,· = argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ . skES (3) The anchor method is fast, as it only depends on the size of the vocabulary once the cooccurrence statistics Q are obtained. However, it does not support rich priors for topic models, while MCMC (Griffiths and Steyvers, 2004) and variational EM (Blei et al., 2003) methods can. This prevents models from using priors to guide the models to discover particular themes (Zhai et al., 2012), or to encourage sparsity in the models (Yao et al., 2009). In the rest of this paper, we correct this lacuna by adding regularization inspired by Bayesian priors to the anchor algorithm. 3 Adding Regularization In this section, we add regularizers to the anchor objective (Equation 3). In this section, we briefly review regularizers and then add two regularizers, inspired by Gaussian (L2, Section 3.1) and Dirichlet priors (Beta, Section 3.2), to the anchor objective functi</context>
<context position="14491" citStr="Blei et al., 2003" startWordPosition="2409" endWordPosition="2412"> measure the performance of our proposed regularized anchor word algorithms. We will refer to specific algorithms in bold. For example, the original anchor algorithm is anchor. Our L2 regularized variant is anchor-L2, 1For a, b &lt; 1, the expected value is still the uniform distribution but the mode lies at the boundaries of the simplex. This corresponds to a sparse Dirichlet distribution, which our optimization cannot at present model. and our beta regularized variant is anchor-beta. To provide conventional baselines, we also compare our methods against topic models from variational inference (Blei et al., 2003, variational) and MCMC (Griffiths and Steyvers, 2004; McCallum, 2002, MCMC). We apply these inference strategies on three diverse corpora: scientific articles from the Neural Information Processing Society (NIPS),2 Internet newsgroups postings (20NEWS),3 and New York Times editorials (Sandhaus, 2008, NYT). Statistics for the datasets are summarized in Table 2. We split each dataset into a training fold (70%), development fold (15%), and a test fold (15%): the training data are used to fit models; the development set are used to select parameters (anchor threshold M, document prior α, regulari</context>
<context position="15840" citStr="Blei et al., 2003" startWordPosition="2621" endWordPosition="2624">, 2003, HL) and topic interpretability (Chang et al., 2009; Newman et al., 2010, TI). Held-out likelihood measures how well the model can reconstruct held-out documents that the model has never seen before. This is the typical evaluation for probabilistic models. Topic interpretability is a more recent metric to capture how useful the topics can be to human users attempting to make sense of a large datasets. Held-out likelihood cannot be computed with existing anchor algorithms, so we use the topic distributions learned from anchor as input to a reference variational inference implementation (Blei et al., 2003) to compute HL. This requires an additional parameter, the Dirichlet prior α for the per-document distribution over topics. We select α using grid search on the development set. To compute TI and evaluate topic coherence, we use normalized pairwise mutual information (NPMI) (Lau et al., 2014) over topics’ twenty most probable words. Topic coherence is computed against the NPMI of a reference corpus. For coherence evaluations, we use both intrinsic and extrinsic text collections to compute NPMI. Intrinsic coherence (TI-i) is computed on training and development data at development time and on t</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1433" citStr="Brody and Lapata, 2009" startWordPosition="196" endWordPosition="199">els. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the European Chapter of the Association for Computational Linguistics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Chong Wang</author>
<author>Sean Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="3913" citStr="Chang et al., 2009" startWordPosition="583" endWordPosition="586">reflect users’ needs (Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data—as measured by held-out likelihood (Blei et al., 2003)—and are more interpretable (Chang et al., 2009; Newman et al., 2010). We also show that our extension to the anchor method enables new applications: for 359 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359–369, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Table 1: Notation used. Matrices are in bold (Q, C), sets are in script S example, using an informed priors to discover concepts of interest. Having shown that regularization does improve performance, in Section 5 we explore why. We discuss the trade-off of training data reconstruction with s</context>
<context position="15280" citStr="Chang et al., 2009" startWordPosition="2533" endWordPosition="2536">ural Information Processing Society (NIPS),2 Internet newsgroups postings (20NEWS),3 and New York Times editorials (Sandhaus, 2008, NYT). Statistics for the datasets are summarized in Table 2. We split each dataset into a training fold (70%), development fold (15%), and a test fold (15%): the training data are used to fit models; the development set are used to select parameters (anchor threshold M, document prior α, regularization weight λ); and final results are reported on the test fold. We use two evaluation measures, held-out likelihood (Blei et al., 2003, HL) and topic interpretability (Chang et al., 2009; Newman et al., 2010, TI). Held-out likelihood measures how well the model can reconstruct held-out documents that the model has never seen before. This is the typical evaluation for probabilistic models. Topic interpretability is a more recent metric to capture how useful the topics can be to human users attempting to make sense of a large datasets. Held-out likelihood cannot be computed with existing anchor algorithms, so we use the topic distributions learned from anchor as input to a reference variational inference implementation (Blei et al., 2003) to compute HL. This requires an additio</context>
</contexts>
<marker>Chang, Boyd-Graber, Wang, Gerrish, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11136" citStr="Cohen and Smith, 2009" startWordPosition="1833" endWordPosition="1836">: The number of documents in the train, development, and test folds in our three datasets. and maximizing log probability of the posterior (ignoring constant terms) (Rennie, 2003). 3.1 L2 Regularization The simplest form of regularization we can add is L2 regularization. This is similar to assuming that probability of a word given a topic comes from a Gaussian distribution. While the distribution over topics is typically Dirichlet, Dirichlet distributions have been replaced by logistic normals in topic modeling applications (Blei and Lafferty, 2005) and for probabilistic grammars of language (Cohen and Smith, 2009). Augmenting the anchor objective with an L2 penalty yields ⎛ ⎞ � Ci,· =argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ skES + λkCi,· − µi,·k22, (5) where regularization weight λ balances the importance of a high-fidelity reconstruction against the regularization, which encourages the anchor coefficients to be close to the vector µ. When the mean vector µ is zero, this encourages the topic coefficients to be zero. In Section 4.3, we use a non-zero mean µ to encode an informed prior to encourage topics to discover specific concepts. 3.2 Beta Regularization The more common prior for topic models is a Diric</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable PCFGs.</title>
<date>2013</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2255" citStr="Cohen et al., 2013" startWordPosition="326" endWordPosition="329">at each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research provides provable polynomial-time alternatives. These approaches provide solutions to hidden Markov models (Anandkumar et al., 2012), mixture models (Kannan et al., 2005), and latent variable grammars (Cohen et al., 2013). The key insight is not to directly optimize observation likelihood but to instead discover latent variables that can reconstruct statistics of the assumed generative model. Unlike search-based methods, which can be caught in local minima, these techniques are often guaranteed to find global optima. These general techniques can be improved by making reasonable assumptions about the models. For example, Arora et al. (2012b)’s approach for inference in topic models assume that each topic has a unique “anchor” word (thus, we call this approach anchor). This approach is fast and effective; becaus</context>
<context position="30507" citStr="Cohen et al., 2013" startWordPosition="5049" endWordPosition="5052">theoretical understanding of generalizability in extensible, regularized models. Incorporating other regularizations could further improve performance or unlock new applications. Our regularizations do not explicitly encourage sparsity; applying other regularizations such as Li could encourage true sparsity (Tibshirani, 1994), and structured priors (Andrzejewski et al., 2009) could efficiently incorporate constraints on topic models. These regularizations could improve spectral algorithms for latent variables models, improving the performance for other NLP tasks such as latent variable PCFGs (Cohen et al., 2013) and HMMs (Anandkumar et al., 2012), combining the flexibility and robustness offered by priors with the speed and accuracy of new, scalable algorithms. Acknowledgments We would like to thank the anonymous reviewers, Hal Daum´e III, Ke Wu, and Ke Zhai for their helpful comments. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also supported by NSF Grant CCF-1018625. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor. 0 5 10 15 20 Iteration ΔC 40 30 20 10 0 ● ● Dataset ● 20NEWS N</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>Shay Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2013. Experiments with spectral learning of latent-variable PCFGs. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Donoho</author>
<author>Victoria Stodden</author>
</authors>
<title>When does non-negative matrix factorization give correct decomposition into parts?</title>
<date>2003</date>
<pages>page</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5828" citStr="Donoho and Stodden, 2003" startWordPosition="902" endWordPosition="905">, we represent this joint distribution as Qi,j = p(w1 = i, w2 = j), each cell represents the probability of words appearing together in a document. Like other topic modeling algorithms, the output of the anchor method is the topic word distributions A with size V * K, where K is the total number of topics desired, a parameter of the algorithm. The kth column of A will be the topic distribution over all words for topic k, and Aw,k is the probability of observing type w given topic k. Anchors: Topic Representatives The anchor method (Arora et al., 2012a) is based on the separability assumption (Donoho and Stodden, 2003), which assumes that each topic contains at least one namesake “anchor word” that has non-zero probability only in that topic. Intuitively, this means that each topic has unique, specific word that, when used, identifies that topic. For example, while “run”, “base”, “fly”, and “shortstop” are associated with a topic about baseball, only “shortstop” is unambiguous, so it could serve as this topic’s anchor word. Let’s assume that we knew what the anchor words were: a set S that indexes rows in Q. Now consider the conditional distribution of word i, the probability of the rest of the vocabulary g</context>
</contexts>
<marker>Donoho, Stodden, 2003</marker>
<rawString>David Donoho and Victoria Stodden. 2003. When does non-negative matrix factorization give correct decomposition into parts? page 2004. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miroslav Dud´ık</author>
<author>Steven J Phillips</author>
<author>Robert E Schapire</author>
</authors>
<title>Performance guarantees for regularized maximum entropy density estimation.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Learning Theory.</booktitle>
<marker>Dud´ık, Phillips, Schapire, 2004</marker>
<rawString>Miroslav Dud´ık, Steven J. Phillips, and Robert E. Schapire. 2004. Performance guarantees for regularized maximum entropy density estimation. In Proceedings of Conference on Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1181" citStr="Eidelman et al., 2012" startWordPosition="159" endWordPosition="162">th probabilistic models. We examine Arora et al.’s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3273" citStr="Finkel and Manning, 2009" startWordPosition="487" endWordPosition="490"> For example, Arora et al. (2012b)’s approach for inference in topic models assume that each topic has a unique “anchor” word (thus, we call this approach anchor). This approach is fast and effective; because it only uses word co-occurrence information, it can scale to much larger datasets than MCMC or EM alternatives. We review the anchor method in Section 2. Despite their advantages, these techniques are not a panacea. They do not accommodate the rich priors that modelers have come to expect. Priors can improve performance (Wallach et al., 2009), provide domain adaptation (Daum´e III, 2007; Finkel and Manning, 2009), and guide models to reflect users’ needs (Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data—as measured by held-out likelihood (Blei et al., 2003)—and ar</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Hierarchical bayesian domain adaptation. In Conference of the North American Chapter of the Association for Computational Linguistics, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Galassi</author>
<author>Jim Davies</author>
<author>James Theiler</author>
<author>Brian Gough</author>
<author>Gerard Jungman</author>
<author>Michael Booth</author>
<author>Fabrice Rossi</author>
</authors>
<title>Gnu Scientific Library: Reference Manual. Network Theory Ltd.</title>
<date>2003</date>
<contexts>
<context position="13290" citStr="Galassi et al., 2003" startWordPosition="2210" endWordPosition="2213">Qi,· ||1: Ci,k ¯Qsk,· sk∈S � λ 1: log (Beta(Ai,k; a, b)), (6) sk∈S where λ again balances reconstruction against the regularization. To ensure the tractability of this algorithm, we enforce a convex regularization function, which requires that a &gt; 1 and b &gt; 1. If we enforce a uniform prior—]EBeta(a,b) [Ai,k] = 1V— and that the mode of the distribution is also V1,1 this gives us the following parametric form for a and b: x (V � 1)x V + 1, and b = V + 1 (7) for real x greater than zero. 3.3 Initialization and Convergence Equation 5 and Equation 6 are optimized using LBFGS gradient optimization (Galassi et al., 2003). We initialize C randomly from Dir(α) with α = 60V (Wallach et al., 2009). We update C after optimizing all V rows. The newly updated C replaces the old topic coefficients. We track how much the topic coefficients C change between two consecutive iterations i and i + 1 and represent it as AC - 11Ci+1—Ci112. We stop optimization when AC &lt; δ. When δ = 0.1, the L2 and unregularized anchor algorithm converges after a single iteration, while beta regularization typically converges after fewer than ten iterations (Figure 4). 4 Regularization Improves Topic Models In this section, we measure the per</context>
</contexts>
<marker>Galassi, Davies, Theiler, Gough, Jungman, Booth, Rossi, 2003</marker>
<rawString>Mark Galassi, Jim Davies, James Theiler, Brian Gough, Gerard Jungman, Michael Booth, and Fabrice Rossi. 2003. Gnu Scientific Library: Reference Manual. Network Theory Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="1848" citStr="Griffiths and Steyvers, 2004" startWordPosition="262" endWordPosition="265">3), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research provides provable polynomial-time alternatives. These approaches provide solutions to hidden Markov models (Anandkumar et al., 2012), mixture models (Kannan et al., 2005), and latent variable grammars (Cohen et al., 2013). The key insight is not to directly optimize observation likelihood but to instead discover latent variables that can reconstruct statistics of the assumed generative model. Unlike search-base</context>
<context position="8840" citStr="Griffiths and Steyvers, 2004" startWordPosition="1458" endWordPosition="1461"> anchor coefficient of size K x V Cj,k = p(z = k |w = j) S set of anchor word indexes {s1, ... sx} λ regularization weight Q 360 best reconstruct the data Q (Equation 1). Arora et al. (2012a) chose the C that minimizes the KL divergence between ¯Qi,· and the reconstruction based on the anchor word’s conditional word vectors EskES Ci,k Qsk,·, ⎛ ⎞ �Ci,· = argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ . skES (3) The anchor method is fast, as it only depends on the size of the vocabulary once the cooccurrence statistics Q are obtained. However, it does not support rich priors for topic models, while MCMC (Griffiths and Steyvers, 2004) and variational EM (Blei et al., 2003) methods can. This prevents models from using priors to guide the models to discover particular themes (Zhai et al., 2012), or to encourage sparsity in the models (Yao et al., 2009). In the rest of this paper, we correct this lacuna by adding regularization inspired by Bayesian priors to the anchor algorithm. 3 Adding Regularization In this section, we add regularizers to the anchor objective (Equation 3). In this section, we briefly review regularizers and then add two regularizers, inspired by Gaussian (L2, Section 3.1) and Dirichlet priors (Beta, Secti</context>
<context position="14544" citStr="Griffiths and Steyvers, 2004" startWordPosition="2416" endWordPosition="2419">egularized anchor word algorithms. We will refer to specific algorithms in bold. For example, the original anchor algorithm is anchor. Our L2 regularized variant is anchor-L2, 1For a, b &lt; 1, the expected value is still the uniform distribution but the mode lies at the boundaries of the simplex. This corresponds to a sparse Dirichlet distribution, which our optimization cannot at present model. and our beta regularized variant is anchor-beta. To provide conventional baselines, we also compare our methods against topic models from variational inference (Blei et al., 2003, variational) and MCMC (Griffiths and Steyvers, 2004; McCallum, 2002, MCMC). We apply these inference strategies on three diverse corpora: scientific articles from the Neural Information Processing Society (NIPS),2 Internet newsgroups postings (20NEWS),3 and New York Times editorials (Sandhaus, 2008, NYT). Statistics for the datasets are summarized in Table 2. We split each dataset into a training fold (70%), development fold (15%), and a test fold (15%): the training data are used to fit models; the development set are used to select parameters (anchor threshold M, document prior α, regularization weight λ); and final results are reported on t</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Studying the history of ideas using topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1278" citStr="Hall et al., 2008" startWordPosition="173" endWordPosition="176">lop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al</context>
</contexts>
<marker>Hall, Jurafsky, Manning, 2008</marker>
<rawString>David Hall, Daniel Jurafsky, and Christopher D. Manning. 2008. Studying the history of ideas using topic models. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Jordan Boyd-Graber</author>
<author>Brianna Satinoff</author>
<author>Alison Smith</author>
</authors>
<title>Interactive topic modeling.</title>
<date>2013</date>
<journal>Machine Learning Journal.</journal>
<contexts>
<context position="3333" citStr="Hu et al., 2013" startWordPosition="499" endWordPosition="502">models assume that each topic has a unique “anchor” word (thus, we call this approach anchor). This approach is fast and effective; because it only uses word co-occurrence information, it can scale to much larger datasets than MCMC or EM alternatives. We review the anchor method in Section 2. Despite their advantages, these techniques are not a panacea. They do not accommodate the rich priors that modelers have come to expect. Priors can improve performance (Wallach et al., 2009), provide domain adaptation (Daum´e III, 2007; Finkel and Manning, 2009), and guide models to reflect users’ needs (Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data—as measured by held-out likelihood (Blei et al., 2003)—and are more interpretable (Chang et al., 2009; Newman et al., 201</context>
</contexts>
<marker>Hu, Boyd-Graber, Satinoff, Smith, 2013</marker>
<rawString>Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2013. Interactive topic modeling. Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt L Jockers</author>
</authors>
<title>Macroanalysis: Digital Methods and Literary History. Topics in the Digital Humanities.</title>
<date>2013</date>
<publisher>University of Illinois Press.</publisher>
<contexts>
<context position="1221" citStr="Jockers, 2013" startWordPosition="166" endWordPosition="167">’s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Gr</context>
</contexts>
<marker>Jockers, 2013</marker>
<rawString>Matt L. Jockers. 2013. Macroanalysis: Digital Methods and Literary History. Topics in the Digital Humanities. University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravindran Kannan</author>
<author>Hadi Salmasian</author>
<author>Santosh Vempala</author>
</authors>
<title>The spectral method for general mixture models.</title>
<date>2005</date>
<booktitle>In Proceedings of Conference on Learning Theory.</booktitle>
<contexts>
<context position="2204" citStr="Kannan et al., 2005" startWordPosition="317" endWordPosition="320"> distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research provides provable polynomial-time alternatives. These approaches provide solutions to hidden Markov models (Anandkumar et al., 2012), mixture models (Kannan et al., 2005), and latent variable grammars (Cohen et al., 2013). The key insight is not to directly optimize observation likelihood but to instead discover latent variables that can reconstruct statistics of the assumed generative model. Unlike search-based methods, which can be caught in local minima, these techniques are often guaranteed to find global optima. These general techniques can be improved by making reasonable assumptions about the models. For example, Arora et al. (2012b)’s approach for inference in topic models assume that each topic has a unique “anchor” word (thus, we call this approach a</context>
</contexts>
<marker>Kannan, Salmasian, Vempala, 2005</marker>
<rawString>Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. 2005. The spectral method for general mixture models. In Proceedings of Conference on Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Lang</author>
</authors>
<date>2007</date>
<note>20 newsgroups data set.</note>
<marker>Lang, 2007</marker>
<rawString>Ken Lang. 2007. 20 newsgroups data set.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality.</title>
<date>2014</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16133" citStr="Lau et al., 2014" startWordPosition="2669" endWordPosition="2672">re recent metric to capture how useful the topics can be to human users attempting to make sense of a large datasets. Held-out likelihood cannot be computed with existing anchor algorithms, so we use the topic distributions learned from anchor as input to a reference variational inference implementation (Blei et al., 2003) to compute HL. This requires an additional parameter, the Dirichlet prior α for the per-document distribution over topics. We select α using grid search on the development set. To compute TI and evaluate topic coherence, we use normalized pairwise mutual information (NPMI) (Lau et al., 2014) over topics’ twenty most probable words. Topic coherence is computed against the NPMI of a reference corpus. For coherence evaluations, we use both intrinsic and extrinsic text collections to compute NPMI. Intrinsic coherence (TI-i) is computed on training and development data at development time and on training and test data at test time. Extrinsic coherence (TI-e) is computed from English Wikipedia articles, with disjoint halves (1.1 million pages each) for distinct development and testing TI-e evaluation. 2http://cs.nyu.edu/-roweis/data.html 3http://qwone.com/-jason/20Newsgroups/ a = 362 F</context>
</contexts>
<marker>Lau, Newman, Baldwin, 2014</marker>
<rawString>Jey Han Lau, David Newman, and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://www.cs.umass.edu/ mccallum/mallet.</note>
<contexts>
<context position="14560" citStr="McCallum, 2002" startWordPosition="2420" endWordPosition="2421">hms. We will refer to specific algorithms in bold. For example, the original anchor algorithm is anchor. Our L2 regularized variant is anchor-L2, 1For a, b &lt; 1, the expected value is still the uniform distribution but the mode lies at the boundaries of the simplex. This corresponds to a sparse Dirichlet distribution, which our optimization cannot at present model. and our beta regularized variant is anchor-beta. To provide conventional baselines, we also compare our methods against topic models from variational inference (Blei et al., 2003, variational) and MCMC (Griffiths and Steyvers, 2004; McCallum, 2002, MCMC). We apply these inference strategies on three diverse corpora: scientific articles from the Neural Information Processing Society (NIPS),2 Internet newsgroups postings (20NEWS),3 and New York Times editorials (Sandhaus, 2008, NYT). Statistics for the datasets are summarized in Table 2. We split each dataset into a training fold (70%), development fold (15%), and a test fold (15%): the training data are used to fit models; the development set are used to select parameters (anchor threshold M, document prior α, regularization weight λ); and final results are reported on the test fold. We</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://www.cs.umass.edu/ mccallum/mallet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Estimating a dirichlet distribution.</title>
<date>2000</date>
<tech>Technical report, Microsoft. http://research.microsoft.com/enus/um/people/minka/papers/dirichlet/.</tech>
<contexts>
<context position="11760" citStr="Minka, 2000" startWordPosition="1940" endWordPosition="1941"> the anchor objective with an L2 penalty yields ⎛ ⎞ � Ci,· =argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ skES + λkCi,· − µi,·k22, (5) where regularization weight λ balances the importance of a high-fidelity reconstruction against the regularization, which encourages the anchor coefficients to be close to the vector µ. When the mean vector µ is zero, this encourages the topic coefficients to be zero. In Section 4.3, we use a non-zero mean µ to encode an informed prior to encourage topics to discover specific concepts. 3.2 Beta Regularization The more common prior for topic models is a Dirichlet prior (Minka, 2000). However, we cannot apply this directly because the optimization is done on a row-by-row basis of the anchor coefficient matrix C, optimizing C for a fixed word w for and all topics. If we want to model the probability of a word, it must be the probability of word w in a topic versus all other words. Modeling this dichotomy (one versus all others in a topic) is possible. The constructive definition of the Dirichlet distribution (Sethuraman, 1994) states that if one has a V-dimensional multinomial θ ∼ Dir(α1 ... αV ), then the marginal distribution of θw follows θw — Beta(αw, Ei6=w αi). This i</context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Thomas P. Minka. 2000. Estimating a dirichlet distribution. Technical report, Microsoft. http://research.microsoft.com/enus/um/people/minka/papers/dirichlet/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3935" citStr="Newman et al., 2010" startWordPosition="587" endWordPosition="590"> (Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data—as measured by held-out likelihood (Blei et al., 2003)—and are more interpretable (Chang et al., 2009; Newman et al., 2010). We also show that our extension to the anchor method enables new applications: for 359 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359–369, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Table 1: Notation used. Matrices are in bold (Q, C), sets are in script S example, using an informed priors to discover concepts of interest. Having shown that regularization does improve performance, in Section 5 we explore why. We discuss the trade-off of training data reconstruction with sparsity and why regula</context>
<context position="15301" citStr="Newman et al., 2010" startWordPosition="2537" endWordPosition="2540">cessing Society (NIPS),2 Internet newsgroups postings (20NEWS),3 and New York Times editorials (Sandhaus, 2008, NYT). Statistics for the datasets are summarized in Table 2. We split each dataset into a training fold (70%), development fold (15%), and a test fold (15%): the training data are used to fit models; the development set are used to select parameters (anchor threshold M, document prior α, regularization weight λ); and final results are reported on the test fold. We use two evaluation measures, held-out likelihood (Blei et al., 2003, HL) and topic interpretability (Chang et al., 2009; Newman et al., 2010, TI). Held-out likelihood measures how well the model can reconstruct held-out documents that the model has never seen before. This is the typical evaluation for probabilistic models. Topic interpretability is a more recent metric to capture how useful the topics can be to human users attempting to make sense of a large datasets. Held-out likelihood cannot be computed with existing anchor algorithms, so we use the topic distributions learned from anchor as input to a reference variational inference implementation (Blei et al., 2003) to compute HL. This requires an additional parameter, the Di</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
</authors>
<title>Feature selection, l1 vs. l2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="9879" citStr="Ng, 2004" startWordPosition="1633" endWordPosition="1634">tive (Equation 3). In this section, we briefly review regularizers and then add two regularizers, inspired by Gaussian (L2, Section 3.1) and Dirichlet priors (Beta, Section 3.2), to the anchor objective function (Equation 3). Regularization terms are ubiquitous. They typically appear as an additional term in an optimization problem. Instead of optimizing a function just of the data x and parameters β, f(x, β), one optimizes an objective function that includes a regularizer that is only a function of parameters: f(w, β) + r(β). Regularizers are critical in staid methods like linear regression (Ng, 2004), in workhorse methods such as maximum entropy modeling (Dud´ık et al., 2004), and also in emerging fields such as deep learning (Wager et al., 2013). In addition to being useful, regularization terms are appealing theoretically because they often correspond to probabilistic interpretations of parameters. For example, if we are seeking the MLE of a probabilistic model parameterized by β, p(x|β), adding a regularization term r(β) = EL i=1 β2i corresponds to adding a Gaussian prior 1β2 f (βi) = √ 2πσ2exp �− 2σ 361 (4) Corpus Train Dev Test Vocab NIPS 1231 247 262 12182 20NEWS 11243 3760 3726 816</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regularization, and rotational invariance. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Roxana Girju</author>
</authors>
<title>A twodimensional topic-aspect model for discovering multi-faceted topics.</title>
<date>2010</date>
<booktitle>In Association for the Advancement of Artificial Intelligence.</booktitle>
<contexts>
<context position="1128" citStr="Paul and Girju, 2010" startWordPosition="151" endWordPosition="154">these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.’s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inferen</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>Michael Paul and Roxana Girju. 2010. A twodimensional topic-aspect model for discovering multi-faceted topics. In Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Martha E Francis</author>
</authors>
<title>Linguistic Inquiry and Word Count.</title>
<date>1999</date>
<journal>Lawrence Erlbaum,</journal>
<volume>1</volume>
<pages>edition,</pages>
<contexts>
<context position="21223" citStr="Pennebaker and Francis, 1999" startWordPosition="3571" endWordPosition="3574">tion to a model. This is not possible with the existing anchor method. An informed prior for topic models seeds a topic with words that describe a topic of interest. In a topic model, these seeds will serve as a “magnet”, attracting similar words to the topic (Zhai et al., 2012). We can achieve a similar goal with anchor-L2. Instead of encouraging anchor coefficients to be zero in Equation 5, we can instead encourage word probabilities to close to an arbitrary mean Ai,k. This vector can reflect expert knowledge. One example of a source of expert knowledge is Linguistic Inquiry and Word Count (Pennebaker and Francis, 1999, LIWC), a dictionary of keywords related to sixty-eight psychological concepts such as positive emotions, negative emotions, and death. For example, it associates “excessive, estate, money, cheap, expensive, living, profit, live, rich, income, poor, etc.” for the concept materialism. We associate each anchor word with its closest LIWC category based on the cooccurrence matrix Q. This is computed by greedily finding the anchor word that has the highest cooccurrence score for any LIWC category: we define the score of a category to anchor word wsk as Ei Qsk,i, where i ranges over words in this c</context>
</contexts>
<marker>Pennebaker, Francis, 1999</marker>
<rawString>James W. Pennebaker and Martha E. Francis. 1999. Linguistic Inquiry and Word Count. Lawrence Erlbaum, 1 edition, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>286</pages>
<contexts>
<context position="1538" citStr="Rabiner, 1989" startWordPosition="216" endWordPosition="217">ds can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research provides provable polynomial-time alternatives. These approaches provide solutions to hidden Markov mode</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257– 286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Rennie</author>
</authors>
<title>On l2-norm regularization and the Gaussian prior.</title>
<date>2003</date>
<contexts>
<context position="10693" citStr="Rennie, 2003" startWordPosition="1769" endWordPosition="1770">are appealing theoretically because they often correspond to probabilistic interpretations of parameters. For example, if we are seeking the MLE of a probabilistic model parameterized by β, p(x|β), adding a regularization term r(β) = EL i=1 β2i corresponds to adding a Gaussian prior 1β2 f (βi) = √ 2πσ2exp �− 2σ 361 (4) Corpus Train Dev Test Vocab NIPS 1231 247 262 12182 20NEWS 11243 3760 3726 81604 NYT 9255 2012 1959 34940 Table 2: The number of documents in the train, development, and test folds in our three datasets. and maximizing log probability of the posterior (ignoring constant terms) (Rennie, 2003). 3.1 L2 Regularization The simplest form of regularization we can add is L2 regularization. This is similar to assuming that probability of a word given a topic comes from a Gaussian distribution. While the distribution over topics is typically Dirichlet, Dirichlet distributions have been replaced by logistic normals in topic modeling applications (Blei and Lafferty, 2005) and for probabilistic grammars of language (Cohen and Smith, 2009). Augmenting the anchor objective with an L2 penalty yields ⎛ ⎞ � Ci,· =argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ skES + λkCi,· − µi,·k22, (5) where regularizatio</context>
</contexts>
<marker>Rennie, 2003</marker>
<rawString>Jason Rennie. 2003. On l2-norm regularization and the Gaussian prior.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Roweis</author>
</authors>
<date>2002</date>
<tech>NIPS 1-12 Dataset.</tech>
<marker>Roweis, 2002</marker>
<rawString>Sam Roweis. 2002. NIPS 1-12 Dataset.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<date>2008</date>
<publisher>The</publisher>
<location>New York Times</location>
<note>annotated corpus. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2008T19.</note>
<contexts>
<context position="14792" citStr="Sandhaus, 2008" startWordPosition="2451" endWordPosition="2452">at the boundaries of the simplex. This corresponds to a sparse Dirichlet distribution, which our optimization cannot at present model. and our beta regularized variant is anchor-beta. To provide conventional baselines, we also compare our methods against topic models from variational inference (Blei et al., 2003, variational) and MCMC (Griffiths and Steyvers, 2004; McCallum, 2002, MCMC). We apply these inference strategies on three diverse corpora: scientific articles from the Neural Information Processing Society (NIPS),2 Internet newsgroups postings (20NEWS),3 and New York Times editorials (Sandhaus, 2008, NYT). Statistics for the datasets are summarized in Table 2. We split each dataset into a training fold (70%), development fold (15%), and a test fold (15%): the training data are used to fit models; the development set are used to select parameters (anchor threshold M, document prior α, regularization weight λ); and final results are reported on the test fold. We use two evaluation measures, held-out likelihood (Blei et al., 2003, HL) and topic interpretability (Chang et al., 2009; Newman et al., 2010, TI). Held-out likelihood measures how well the model can reconstruct held-out documents t</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times annotated corpus. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2008T19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayaram Sethuraman</author>
</authors>
<title>A constructive definition of Dirichlet priors.</title>
<date>1994</date>
<journal>Statistica Sinica,</journal>
<pages>4--639</pages>
<contexts>
<context position="12211" citStr="Sethuraman, 1994" startWordPosition="2018" endWordPosition="2019">an informed prior to encourage topics to discover specific concepts. 3.2 Beta Regularization The more common prior for topic models is a Dirichlet prior (Minka, 2000). However, we cannot apply this directly because the optimization is done on a row-by-row basis of the anchor coefficient matrix C, optimizing C for a fixed word w for and all topics. If we want to model the probability of a word, it must be the probability of word w in a topic versus all other words. Modeling this dichotomy (one versus all others in a topic) is possible. The constructive definition of the Dirichlet distribution (Sethuraman, 1994) states that if one has a V-dimensional multinomial θ ∼ Dir(α1 ... αV ), then the marginal distribution of θw follows θw — Beta(αw, Ei6=w αi). This is the tool we need to consider the distribution of a single word’s probability. This requires including the topic matrix as part of the objective function. The topic matrix is a linear transformation of the coefficient matrix (Equation 2). The objective for beta regularization becomes Ci,· =argminCi,·DKL I ¯Qi,· ||1: Ci,k ¯Qsk,· sk∈S � λ 1: log (Beta(Ai,k; a, b)), (6) sk∈S where λ again balances reconstruction against the regularization. To ensure</context>
</contexts>
<marker>Sethuraman, 1994</marker>
<rawString>Jayaram Sethuraman. 1994. A constructive definition of Dirichlet priors. Statistica Sinica, 4:639–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1994</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>58--267</pages>
<contexts>
<context position="30215" citStr="Tibshirani, 1994" startWordPosition="5009" endWordPosition="5010">nterpretable models and the ability to inject prior knowledge without sacrificing the speed and generalizability of the underlying approach. However, one sacrifice that this approach does make is the beautiful theoretical guarantees of previous work. An important piece of future work is a theoretical understanding of generalizability in extensible, regularized models. Incorporating other regularizations could further improve performance or unlock new applications. Our regularizations do not explicitly encourage sparsity; applying other regularizations such as Li could encourage true sparsity (Tibshirani, 1994), and structured priors (Andrzejewski et al., 2009) could efficiently incorporate constraints on topic models. These regularizations could improve spectral algorithms for latent variables models, improving the performance for other NLP tasks such as latent variable PCFGs (Cohen et al., 2013) and HMMs (Anandkumar et al., 2012), combining the flexibility and robustness offered by priors with the speed and accuracy of new, scalable algorithms. Acknowledgments We would like to thank the anonymous reviewers, Hal Daum´e III, Ke Wu, and Ke Zhai for their helpful comments. This work was supported by N</context>
</contexts>
<marker>Tibshirani, 1994</marker>
<rawString>Robert Tibshirani. 1994. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Wager</author>
<author>Sida Wang</author>
<author>Percy Liang</author>
</authors>
<title>Dropout training as adaptive regularization.</title>
<date>2013</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems,</booktitle>
<pages>351--359</pages>
<contexts>
<context position="10028" citStr="Wager et al., 2013" startWordPosition="1656" endWordPosition="1659">d Dirichlet priors (Beta, Section 3.2), to the anchor objective function (Equation 3). Regularization terms are ubiquitous. They typically appear as an additional term in an optimization problem. Instead of optimizing a function just of the data x and parameters β, f(x, β), one optimizes an objective function that includes a regularizer that is only a function of parameters: f(w, β) + r(β). Regularizers are critical in staid methods like linear regression (Ng, 2004), in workhorse methods such as maximum entropy modeling (Dud´ık et al., 2004), and also in emerging fields such as deep learning (Wager et al., 2013). In addition to being useful, regularization terms are appealing theoretically because they often correspond to probabilistic interpretations of parameters. For example, if we are seeking the MLE of a probabilistic model parameterized by β, p(x|β), adding a regularization term r(β) = EL i=1 β2i corresponds to adding a Gaussian prior 1β2 f (βi) = √ 2πσ2exp �− 2σ 361 (4) Corpus Train Dev Test Vocab NIPS 1231 247 262 12182 20NEWS 11243 3760 3726 81604 NYT 9255 2012 1959 34940 Table 2: The number of documents in the train, development, and test folds in our three datasets. and maximizing log prob</context>
</contexts>
<marker>Wager, Wang, Liang, 2013</marker>
<rawString>Stefan Wager, Sida Wang, and Percy Liang. 2013. Dropout training as adaptive regularization. In Proceedings of Advances in Neural Information Processing Systems, pages 351–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="3201" citStr="Wallach et al., 2009" startWordPosition="477" endWordPosition="480">s can be improved by making reasonable assumptions about the models. For example, Arora et al. (2012b)’s approach for inference in topic models assume that each topic has a unique “anchor” word (thus, we call this approach anchor). This approach is fast and effective; because it only uses word co-occurrence information, it can scale to much larger datasets than MCMC or EM alternatives. We review the anchor method in Section 2. Despite their advantages, these techniques are not a panacea. They do not accommodate the rich priors that modelers have come to expect. Priors can improve performance (Wallach et al., 2009), provide domain adaptation (Daum´e III, 2007; Finkel and Manning, 2009), and guide models to reflect users’ needs (Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously u</context>
<context position="13364" citStr="Wallach et al., 2009" startWordPosition="2224" endWordPosition="2227"> again balances reconstruction against the regularization. To ensure the tractability of this algorithm, we enforce a convex regularization function, which requires that a &gt; 1 and b &gt; 1. If we enforce a uniform prior—]EBeta(a,b) [Ai,k] = 1V— and that the mode of the distribution is also V1,1 this gives us the following parametric form for a and b: x (V � 1)x V + 1, and b = V + 1 (7) for real x greater than zero. 3.3 Initialization and Convergence Equation 5 and Equation 6 are optimized using LBFGS gradient optimization (Galassi et al., 2003). We initialize C randomly from Dir(α) with α = 60V (Wallach et al., 2009). We update C after optimizing all V rows. The newly updated C replaces the old topic coefficients. We track how much the topic coefficients C change between two consecutive iterations i and i + 1 and represent it as AC - 11Ci+1—Ci112. We stop optimization when AC &lt; δ. When δ = 0.1, the L2 and unregularized anchor algorithm converges after a single iteration, while beta regularization typically converges after fewer than ten iterations (Figure 4). 4 Regularization Improves Topic Models In this section, we measure the performance of our proposed regularized anchor word algorithms. We will refer</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA: Why priors matter. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<booktitle>In Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="9060" citStr="Yao et al., 2009" startWordPosition="1497" endWordPosition="1500">ence between ¯Qi,· and the reconstruction based on the anchor word’s conditional word vectors EskES Ci,k Qsk,·, ⎛ ⎞ �Ci,· = argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ . skES (3) The anchor method is fast, as it only depends on the size of the vocabulary once the cooccurrence statistics Q are obtained. However, it does not support rich priors for topic models, while MCMC (Griffiths and Steyvers, 2004) and variational EM (Blei et al., 2003) methods can. This prevents models from using priors to guide the models to discover particular themes (Zhai et al., 2012), or to encourage sparsity in the models (Yao et al., 2009). In the rest of this paper, we correct this lacuna by adding regularization inspired by Bayesian priors to the anchor algorithm. 3 Adding Regularization In this section, we add regularizers to the anchor objective (Equation 3). In this section, we briefly review regularizers and then add two regularizers, inspired by Gaussian (L2, Section 3.1) and Dirichlet priors (Beta, Section 3.2), to the anchor objective function (Equation 3). Regularization terms are ubiquitous. They typically appear as an additional term in an optimization problem. Instead of optimizing a function just of the data x and</context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections. In Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ke Zhai</author>
<author>Jordan Boyd-Graber</author>
<author>Nima Asadi</author>
<author>Mohamad Alkhouja</author>
</authors>
<title>Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce.</title>
<date>2012</date>
<booktitle>In Proceedings of World Wide Web Conference.</booktitle>
<contexts>
<context position="9001" citStr="Zhai et al., 2012" startWordPosition="1486" endWordPosition="1489">rora et al. (2012a) chose the C that minimizes the KL divergence between ¯Qi,· and the reconstruction based on the anchor word’s conditional word vectors EskES Ci,k Qsk,·, ⎛ ⎞ �Ci,· = argminCi,·DKL ⎝ ¯Qi,· ||Ci,k¯Qsk,· ⎠ . skES (3) The anchor method is fast, as it only depends on the size of the vocabulary once the cooccurrence statistics Q are obtained. However, it does not support rich priors for topic models, while MCMC (Griffiths and Steyvers, 2004) and variational EM (Blei et al., 2003) methods can. This prevents models from using priors to guide the models to discover particular themes (Zhai et al., 2012), or to encourage sparsity in the models (Yao et al., 2009). In the rest of this paper, we correct this lacuna by adding regularization inspired by Bayesian priors to the anchor algorithm. 3 Adding Regularization In this section, we add regularizers to the anchor objective (Equation 3). In this section, we briefly review regularizers and then add two regularizers, inspired by Gaussian (L2, Section 3.1) and Dirichlet priors (Beta, Section 3.2), to the anchor objective function (Equation 3). Regularization terms are ubiquitous. They typically appear as an additional term in an optimization probl</context>
<context position="20874" citStr="Zhai et al., 2012" startWordPosition="3515" endWordPosition="3518">egularization offers comparable held-out likelihood as unregularized anchor, while anchor-beta often has better interpretability. Because of the mismatch between the specialized vocabulary of NIPS and the generalpurpose language of Wikipedia, TI-e has a high variance. 4.3 Informed Regularization A frequent use of priors is to add information to a model. This is not possible with the existing anchor method. An informed prior for topic models seeds a topic with words that describe a topic of interest. In a topic model, these seeds will serve as a “magnet”, attracting similar words to the topic (Zhai et al., 2012). We can achieve a similar goal with anchor-L2. Instead of encouraging anchor coefficients to be zero in Equation 5, we can instead encourage word probabilities to close to an arbitrary mean Ai,k. This vector can reflect expert knowledge. One example of a source of expert knowledge is Linguistic Inquiry and Word Count (Pennebaker and Francis, 1999, LIWC), a dictionary of keywords related to sixty-eight psychological concepts such as positive emotions, negative emotions, and death. For example, it associates “excessive, estate, money, cheap, expensive, living, profit, live, rich, income, poor, </context>
</contexts>
<marker>Zhai, Boyd-Graber, Asadi, Alkhouja, 2012</marker>
<rawString>Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mohamad Alkhouja. 2012. Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce. In Proceedings of World Wide Web Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>