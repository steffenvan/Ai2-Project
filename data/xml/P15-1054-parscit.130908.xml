<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.515492">
Summarization of Multi-Document Topic Hierarchies using Submodular
Mixtures
</title>
<author confidence="0.517535">
Ramakrishna B Bairi
</author>
<affiliation confidence="0.530296">
IITB-Monash Research Academy
IIT Bombay
Mumbai, 40076, India
</affiliation>
<email confidence="0.981165">
bairi@cse.iitb.ac.in
</email>
<author confidence="0.966829">
Ganesh Ramakrishnan
</author>
<affiliation confidence="0.821276">
IIT Bombay
Mumbai, 40076, India
</affiliation>
<email confidence="0.982235">
ganesh@cse.iitb.ac.in
</email>
<author confidence="0.930774">
Rishabh Iyer
</author>
<affiliation confidence="0.970911">
University of Washington
</affiliation>
<address confidence="0.789449">
Seattle, WA-98175, USA
</address>
<email confidence="0.989404">
rkiyer@u.washington.edu
</email>
<author confidence="0.996088">
Jeff Bilmes
</author>
<affiliation confidence="0.99926">
University of Washington
</affiliation>
<address confidence="0.832971">
Seattle, WA-98175, USA
</address>
<email confidence="0.998779">
bilmes@uw.edu
</email>
<sectionHeader confidence="0.996661" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955135135135">
We study the problem of summarizing
DAG-structured topic hierarchies over a
given set of documents. Example appli-
cations include automatically generating
Wikipedia disambiguation pages for a
set of articles, and generating candidate
multi-labels for preparing machine learn-
ing datasets (e.g., for text classification,
functional genomics, and image classi-
fication). Unlike previous work, which
focuses on clustering the set of documents
using the topic hierarchy as features, we
directly pose the problem as a submodular
optimization problem on a topic hierarchy
using the documents as features. Desirable
properties of the chosen topics include
document coverage, specificity, topic
diversity, and topic homogeneity, each of
which, we show, is naturally modeled by
a submodular function. Other information,
provided say by unsupervised approaches
such as LDA and its variants, can also be
utilized by defining a submodular function
that expresses coherence between the
chosen topics and this information. We use
a large-margin framework to learn convex
mixtures over the set of submodular
components. We empirically evaluate our
method on the problem of automatically
generating Wikipedia disambiguation
pages using human generated clusterings
as ground truth. We find that our frame-
work improves upon several baselines
according to a variety of standard evalua-
tion metrics including the Jaccard Index,
F1 score and NMI, and moreover, can be
scaled to extremely large scale problems.
</bodyText>
<sectionHeader confidence="0.999058" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937333333333">
Several real world machine learning applications
involve hierarchy based categorization of topics
for a set of objects. Objects could be, e.g., a
set of documents for text classification, a set of
genes in functional genomics, or a set of images
in computer vision. One can often define a natural
topic hierarchy to categorize these objects. For
example, in text and image classification problems,
each document or image is assigned a hierarchy
of labels — a baseball page is assigned the labels
“baseball” and “sports.” Moreover, many of these
applications, naturally have an existing topic
hierarchy generated on the entire set of objects
(Rousu et al., 2006; Barutcuoglu et al., 2006; ling
Zhang and hua Zhou, 2007; Silla and Freitas, 2011;
Tsoumakas et al., 2010).
Given a DAG-structured topic hierarchy and a
subset of objects, we investigate the problem of
finding a subset of DAG-structured topics that are
induced by that subset (of objects). This problem
arises naturally in several real world applications.
For example, consider the problem of identifying
appropriate label sets for a collection of articles.
Several existing text collection datasets such as 20
Newsgroup1, Reuters-215782 work with a prede-
fined set of topics. We observe that these topic
names are highly abstract3 for the articles catego-
rized under them. On the other hand, techniques
proposed by systems such as Wikipedia Miner
(Milne, 2009) and TAGME (Ferragina and Scaiella,
2010) generate several labels for each article in the
dataset that are highly specific to the article. Col-
lating all labels from all articles to create a label
</bodyText>
<footnote confidence="0.9995666">
1http://qwone.com/˜jason/20Newsgroups/
2http://www.daviddlewis.com/resources/
testcollections/reuters21578/
3Topic Concept is more abstract than the topic Science
which is more abstract than the topicChemistry
</footnote>
<page confidence="0.946357">
553
</page>
<note confidence="0.986092333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 553–563,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.421339666666667">
Input documents on &apos;Apple&apos; with fine grained
Output Disambiguation page for &apos;Apple&apos; with
(near leaf level) topic assignment Topic DAG documents griped under summary topics
</figure>
<figureCaption confidence="0.8611236">
Figure 1: Topic Summarization overview. On the left, we show many documents related to Apple. In the
middle, a Wikipedia category hierarchy shown as a topic DAG, links these documents at the leaf level. On
the right, we show the output of our summarization process, which creates a set of summary topics (Plants,
Technology, Companies, Films, Music and Places in this example) with the input documents classified
under them.
</figureCaption>
<figure confidence="0.99384185123967">
The Apple (1980 films,
English language
,
Apple Novel (2007
novels, Novels of
Document Name Fine-grained topics
Apple Band (English
rock music groups,
London)
Malus(Eudicot genera,
Plants and Pollinators,... )
Cashew Apple (Edible
nuts,Trees of Brazil,...)
Apple Albums (1990
debut Albums, English
au a y
Hedge Apple (Trees of
cords)US, Maclura,..)
Apple Corps(Companies
of UK, Companies
Apple Inc(Companies
in California, Companies
eslied 1
Apple Bank
Hard C
New Your, Banks of
(Banks in
i)
Apple Records
(Scotish music groups)
Apple Store (Electronic
companies of Us, Video
g
Apple Oklahoma
(Unincorporated
Apple Key (Mac OS,
Computer keys)
Apple Computer
(Apple hardware,
Moecncs,)
Apple Card Game
(Point trick games)
HP Apple (HP
microprocessors, HP
Apple Daily (Next
media, Publications
Apple Valley (Cities
in California)
Apple River (Villages
in Illions)
Topic ... ... ... Summary Topic Parent-Child relation Ancestor-Descendant relation Topic-Object association
HP Microprocessors
Apple Hardware
Operating Systems HP Products
... ...
Technology Plants Companies Places Films Music
... ... ... ... ... ... ... ... ... ...
Trees by country
Plants and Pollinators Edible Nuts
Trees of US
Documents associated with fine-grained (near leaf level) topics
.. ...
... ... ... ... ...
...
Computer Hardware
Tropical Trees
Trees of Brazil
... ...
Root
... ... ... ...
... ...
Companies by year Books Films by country
... ... ...
Companies of UK
Banks in New York
... ...
... ...
... ... ... ... ... ... ... ... ... ... ... ... ... ...
Companies ofCalifornia
Populated place
Retail Companies
... ...
Novels
English Albums
Publications
Albums by language
... ...
Cities in California
... ... ... ... ...
1980 films
Companies
Apple corps
Apple Inc.
Apple bank
Films
Music
Technology
Apple computer
Apple Key
Apple Store
HP Apple
Other
Apple Card game
Apple Daily
Apple Novel
Apple Albums
Apple Band
Apple Records
Places
Apple Oklahoma
Apple River
Apple Valley
Plants
Malus
Cashew Apple
Hedge Apple
The Apple
Documents not
grouped under
any summary
topic
</figure>
<bodyText confidence="0.999355444444444">
set for the dataset can result in a large number of
labels and become unmanageable. Our proposed
techniques can summarize such large sets of labels
into a smaller and more meaningful label sets using
a DAG-structured topic hierarchy. This also holds
for image classification problems and datasets like
ImageNet (Deng et al., 2009). We use the term
summarize to highlight the fact that the smaller la-
bel set semantically covers the larger label set. For
example, the topics Physics, Chemistry, and Math-
ematics can be summarized into a topic Science.
A particularly important application of our work
(and the one we use for our evaluations in Section 4)
is the following: Given a collection of articles span-
ning different topics, but with similar titles, auto-
matically generate a disambiguation page for those
titles using the Wikipedia category hierarchy4 as a
topic DAG. Disambiguation pages5 on Wikipedia
are used to resolve conflicts in article titles that oc-
cur when a title is naturally associated with multi-
ple articles on distinct topics. Each disambiguation
page organizes articles into several groups, where
the articles in each group pertain only to a specific
topic. Disambiguations may be seen as paths in a
hierarchy leading to different articles that arguably
could have the same title. For example, the title
Apple6 can refer to a plant, a company, a film, a
</bodyText>
<footnote confidence="0.972625">
4http://en.wikipedia.org/wiki/Help:Categories
5http://en.wikipedia.org/wiki/Wikipedia:Disambiguation
6http://en.wikipedia.org/wiki/Apple_
(disambiguation)
</footnote>
<bodyText confidence="0.999620451612903">
television show, a place, a technology, an album, a
record label, and a newspaper daily. The problem
then, is to organize the articles into multiple groups
where each group contains articles of similar nature
(topics) and has an appropriately discerned group
heading. Figure 1 describes the topic summariza-
tion process for creation of the disambiguation page
for “Apple”.
All the above mentioned problems can be mod-
eled as the problem of finding the most representa-
tive subset of topic nodes from a DAG-Structured
topic hierarchy. We argue that many formulations
of this problem are natural instances of submodular
maximization, and provide a learning framework
to create submodular mixtures to solve this prob-
lem. A set function f (.) is said to be submodular
if for any element v and sets A C_ B C_ V \ {v},
where V represents the ground set of elements,
f (A U {v})−f (A) &gt; f (B U {v})−f (B). This is
called the diminishing returns property and states,
informally, that adding an element to a smaller
set increases the function value more than adding
that element to a larger set. Submodular func-
tions naturally model notions of coverage and di-
versity in applications, and therefore, a number
of machine learning problems can be modeled as
forms of submodular optimization (Kempe et al.,
2003; Krause and Guestrin, 2005; Narasimhan and
Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes,
2012; Lin and Bilmes, 2010). In this paper, we
investigate structured prediction methods for learn-
</bodyText>
<page confidence="0.997458">
554
</page>
<bodyText confidence="0.9999404">
ing weighted mixtures of submodular functions to
summarize topics for a collection of objects us-
ing DAG-structured topic hierarchies. Throughout
this paper we use the terms “topic” and “category”
interchangeably.
</bodyText>
<subsectionHeader confidence="0.834861">
1.1 Related Work
</subsectionHeader>
<bodyText confidence="0.999980393258428">
To the best of our knowledge, the specific problem
we consider here is new. Previous work on identi-
fying topics can be broadly categorized into one of
the following types: a) cluster the objects and then
identify names for the clusters; or b) dynamically
identify topics (including hierarchical) for a set of
objects. LDA (Blei et al., 2003) clusters the docu-
ments and simultaneously produces a set of topics
into which the documents are clustered. In LDA,
each document may be viewed as a mixture of var-
ious topics and the topic distribution is assumed
to have a Dirichlet prior. LDA associates a group
of high probability words to each identified topic.
A name can be assigned to a topic by manually
inspecting the words or using additional algorithms
like (Mei et al., 2007; Maiya et al., 2013). LDA
does not make use of existing topic hierarchies and
correlation between topics. The Correlated Topic
Model (Blei and Lafferty, 2006) induces a correla-
tion structure between topics by using the logistic
normal distribution instead of the Dirichlet. An-
other extension is the hierarchical LDA (Blei et
al., 2004), where topics are joined together in a
hierarchy by using the nested Chinese restaurant
process. Nonparametric extensions of LDA include
the Hierarchical Dirichlet Process (Teh et al., 2006)
mixture model, which allows the number of top-
ics to be unbounded and learnt from data and the
Nested Chinese Restaurant Process which allows
topics to be arranged in a hierarchy whose structure
is learnt from data. In each of these approaches,
unlike our proposed approach, an existing topic
hierarchy is not used, nor is any additional object-
topic information leveraged.
The pachinko allocation model (PAM)(Li and
McCallum, 2006) captures arbitrary, nested, and
possibly sparse correlations between topics using a
DAG. The leaves of the DAG represent individual
words in the vocabulary, while each interior node
represents a correlation among its children, which
may be words or other interior nodes (topics). PAM
learns the probability distributions of words in a
topic, subtopics in a topic, and topics in a document.
We cannot, however, generate a subset of topics
from a large existing topic DAG that can act as
summary topics, using PAM.
HSLDA (Perotte et al., 2011) introduces a hierar-
chically supervised LDA model to infer hierarchi-
cal labels for a document. It assumes an existing
label hierarchy in the form of a tree. The model
infers one or more labels such that, if a label l is
inferred as relevant to a document, then all the la-
bels from l to the root of the tree are also inferred
as relevant to the document. Our approach differs
from HSLDA since: (1) we use the label hierarchy
to infer a set of labels for a group of documents; (2)
we do not enforce the label hierarchy to be a tree
as it can be a DAG; and (3) generalizing HSLDA
to use a DAG structured hierarchy and infer labels
for a group of documents (e.g., combining into one
big document) also may not help in solving our
problem. HSLDA will apply all the relevant labels
to the documents as per the classifier that it learns
for every label. Moreover, the “root” label is al-
ways applied and it is very likely that many labels
near the top level of the label hierarchy are also
classified as relevant to the group of documents.
Wei and James (Bi and Kwok, 2011) present
a hierarchical multi-label classification algorithm
that can be used on both tree and DAG structured
hierarchies. They formulate a search for the opti-
mal consistent multi-label as the finding of the best
subgraph in a tree/DAG. In our approach, we as-
sume, individual documents are already associated
with one or more topics and we find a consistent
label set for a group of documents using the DAG
structured topic hierarchy.
Medelyan et al. (Medelyan et al., 2008) and
Ferragina et al. (Ferragina and Scaiella, 2010) de-
tect topics for a document using Wikipedia article
names and category names as the topic vocabulary.
These systems are able to extract signals from a text
document and identify Wikipedia articles and/or
categories that optimally match the document and
assign those article/category names as topics for the
document. When run on a large collection of docu-
ments, these approaches generate enormous num-
bers of topics, a problem our proposed approach
addresses.
</bodyText>
<subsectionHeader confidence="0.9869">
1.2 Our Contributions
</subsectionHeader>
<bodyText confidence="0.9999485">
While most prior work discussed above focuses
on the underlying set of documents, (e.g., by
clustering documents), we focus directly on the
topics. In particular, we formulate the problem
as subset selection on the set of topics within
a DAG while simultaneously considering the
documents to be categorized. Our method can
scale to the colossal size of the DAG (1 million
topics and 3 million correlation links between
topics in Wikipedia). Moreover, our approach can
</bodyText>
<page confidence="0.9906">
555
</page>
<bodyText confidence="0.99997594117647">
naturally incorporate outputs from many of the
aforementioned algorithms. Our approach is based
on submodular maximization and mixture learning,
which has been successfully used in applications
such as document summarization (Lin, 2012) and
image summarization (Tschiatschek et al., 2014),
but has never been applied to topic identification
tasks or, more generally, DAG summarization.
We introduce a family of submodular functions
to identify an appropriate set of topics from a DAG
structured hierarchy of topics for a group of docu-
ments. We characterize this topic appropriateness
through a set of desirable properties such as cov-
erage, diversity, specificity, clarity, and relevance.
Each of the submodular function components we
consider are monotone, thereby ensuring a near op-
timal performance obtainable via a simple greedy
algorithm for optimization.7. We also show how
our technique naturally embodies outputs of other
algorithms such as LDA, clustering, and classifi-
cations. Finally, we utilize a large margin formu-
lation for learning mixtures of these submodular
functions, and show how we can optimally learn
them from training data.
Our approach demonstrates how to utilize the
features collectively in the document space and the
topic space to infer a set of topics. From an em-
pirical perspective, we introduce and evaluate our
approach on a dataset of around 8000 disambigua-
tions that was extracted from Wikipedia and subse-
quently cleaned using the methods described in the
experimentation section. We show that our learn-
ing framework outperforms many of the baselines,
and is practical enough to be used on large corpora.
</bodyText>
<sectionHeader confidence="0.9166" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.991987133333333">
Let G (V, E) be the DAG structured topic hierarchy
with V topics. These topics are observed to have a
parent child (isa) relationship forming a DAG. Let
D be the set of documents that are associated with
one or more of these topics. The middle portion
of Figure 1 depicts a topic hierarchy with associ-
ated documents. The association links between the
documents and topics can be hard or soft. In case
of a hard link, a document is attached to a set of
topics. Examples include multi-labeled documents.
In case of a soft link, a document is associated with
a topic with some degree of confidence (or prob-
ability). Furthermore, if a document is attached
to a topic t, we assume that all the ancestor top-
ics of t are also relevant for that document. This
7A simple greedy algorithm (Nemhauser et al., 1978) ob-
tains a 1 − 1/e approximation guarantee for monotone sub-
modular function maximization
assumption has been employed in earlier works
(Blei et al., 2004; Bi and Kwok, 2011; Rousu et
al., 2006) as well. Given a budget of K, our objec-
tive is to choose a set of K topics from V , which
best describe the documents in D. The notion of
best describing topics is characterized through a set
of desirable properties - coverage, diversity, speci-
ficity, clarity, relevance and fidelity - that K topics
have to satisfy. The submodular functions that we
introduce in the next section ensure these proper-
ties are satisfied. Formally, we solve the following
discrete optimization problem:
</bodyText>
<equation confidence="0.969640666666667">
�
S* E argmax wifi(S) (1)
ScV :|S|&lt;K i
</equation>
<bodyText confidence="0.985852564102564">
where, fi are monotone submodular mixture com-
ponents and wi ≥ 0 are the weights associated with
those mixture components. Set S* is the summary
topics scored best.
It is easy to find massive (i.e., size in the order of
million) DAG structured topic hierarchies in prac-
tice. Wikipedia’s category hierarchy consists of
more than 1M categories (topics) arranged hierar-
chically. In fact, they form a cyclic graph (Zesch
and Gurevych, 2007). However, we can convert it
to a DAG by eliminating the cycles as described
in the supplementary material. YAGO (Suchanek
et al., 2007) and Freebase (Bollacker et al., 2008)
are other instances of massive topic hierarchies.
The association of the documents with the existing
topic hierarchy is also well studied. Systems such
as WikipediaMiner (Milne, 2009), TAGME (Fer-
ragina and Scaiella, 2010) and several annotation
systems such as (Dill et al., 2003; Mihalcea and
Csomai, 2007; Bunescu and Pasca, 2006) attach
topics from Wikipedia (and other catalogs) to the
documents by establishing the hard or soft links
mentioned above.
Our goal is the following: Given a (ground set)
collection V of topics organized in a pre-existing
hierarchical DAG structure, and a collection D of
documents, chose a size K E Z+ representative
subset of topics. Our approach is distinct from
earlier work (e.g., (Kanungo et al., 2002; Blei et
al., 2003)) where typically only a set of documents
is classified and categorized in some way. We next
provide a few definitions needed later in the paper.
Definition 1: Transitive Cover F): A topic t is
said to cover a set of documents F(t), called the
transitive cover of the topic t, if for all documents
i E F(t), either i is associated directly with topic
t or with any of the descendant topics of t in the
topic DAG. A natural extension of this definition to
a set of topics T is defined as F(T) = UtETF(t).
</bodyText>
<page confidence="0.992592">
556
</page>
<bodyText confidence="0.988238575757576">
Definition 2: Truncated Transitive Cover (Γα):
This is a transitive cover of topic t, but with the
limitation that the path length between a docu-
ment and the topic t is not more than α. Hence,
|Γα(t) |≤ |Γ(t)|.
While our problem is closely related to cluster-
ing approaches, which consider the set of docu-
ments directly, there are some crucial differences.
In particular, we focus on producing a clustering of
documents where clusters are encouraged to honor
a pre-defined DAG structured topic hierarchy. Ex-
isting agglomerative clustering algorithms focusing
on the coverage of documents may not produce the
desired clustering. To understand this, consider six
documents d1, d2 ... d6 to be grouped into three
clusters. There may be multiple ways to do this de-
pending upon multiple aggregation paths present in
the topic DAG: ((d1, d2), (d3, d4), (d5, d6)) or ((d1,
d2, d3), (d4, d5), (d6)) or ((d1, d2, d3, d4), (d5),
(d6)) or something else. Hence, we need more
stringent measures to prefer one clustering over
the others. Our work addresses this with a variety
of quality criteria (coverage, diversity, specificity,
clarity, relevance and fidelity, which are explained
later in this paper) that are organically derived from
well established submodular functions. And, most
importantly, we learn the right mixture of these
qualities to be enforced from the data itself. Fur-
thermore, our approach also generalizes these clus-
tering approaches, since one of the components in
our mixture of submodular functions is defined via
these unsupervised approaches, and maps a given
clustering to a set of topics in the hierarchy.
</bodyText>
<sectionHeader confidence="0.766201" genericHeader="method">
3 Submodular Components and
Learning
</sectionHeader>
<bodyText confidence="0.99566741025641">
Summarization is the task of extracting information
from a source that is both small in size but still
representative. Our problem is different from
traditional summarization tasks since we have an
underlying DAG as a topic hierarchy that we wish
to summarize in response to a subset of documents.
Thus, a critical part of our problem is to take the
graph structure into account while creating the
summaries. Below, we identify properties we wish
our summaries to posses.
Coverage: A summary set of topics should
cover most of the documents. A document is said
to be covered by a topic if there exists a path from
the topic, going through intermediary descendant
topics, to the document, i.e., the document is within
the transitive cover of the topic.
Diversity: Summaries should be as diverse as
possible, i.e., each summary topic should cover
a unique set of documents. When a document is
covered by more than one topic, that document is
redundantly covered, e.g., “Finance” and “Banking”
would be unlikely members of the same summary.
Summary qualities also involve “quality”
notions, including:
Specificity/Clarity/Relevance/Coherence:
These quality measures help us choose a set of
topics that are neither too abstract nor overly
specific. They ensure that the topics are clear
and relevant to the documents that they represent.
When additional information such as clustering
(from LDA or other sources) and tagging (manual)
documents is available, these quality criteria
encourage the chosen topics to show resemblance
(coherence) to those clustering/tagging in terms of
transitive cover of documents they produce.
In the below, we define a variety of submodular
functions that capture the above properties, and we
then describe a large margin learning framework
for learning convex mixtures of such components.
</bodyText>
<subsectionHeader confidence="0.991887">
3.1 Submodular Components
3.1.1 Coverage Based Functions
</subsectionHeader>
<bodyText confidence="0.998431931034483">
Coverage components capture “coverage” of a set
of documents.
Weighted Set Cover Function: Given a set of
categories, S ⊆ V , define Γ(S) as the set of docu-
ments covered — for each topic s ∈ S, Γ(s) ⊆ D
represents the documents covered by topic s and
Γ(S) = ∪s∈SΓ(s). The weighted set cover func-
tion, defined as f(S) = Ed∈Γ(S) wd = w(Γ(S)),
assigns weights to the documents based on
their relative importance (e.g., in Wikipedia
disambiguation, the different documents could be
ranked based on their priority).
Feature-based Functions: This class of
function represents coverage in feature space.
Given a set of categories S ⊆ V , and a set of
features U, define mu(S) as the score associated
with the set of categories S for feature u ∈ U.
The feature set could represent, for example, the
documents, in which case mu(S) represents the
number of times document u is covered by the
set S. U could also represent more complicated
features. For example, in the context of Wikipedia
disambiguation, U could represent TFIDF features
over the documents. Feature based are then
defined as f(S) = Eu∈U 0(mu(S)), where 0 is
a concave (e.g., the square root) function. This
function class has been successfully used in several
applications (Kirchhoff and Bilmes, 2014; Wei et
al., 2014a; Wei et al., 2014b).
</bodyText>
<page confidence="0.992268">
557
</page>
<subsectionHeader confidence="0.695733">
3.1.2 Similarity based Functions
</subsectionHeader>
<bodyText confidence="0.997989772727273">
Similarity functions are defined through a simi-
larity matrix S = {sij}i,jEV . Given categories
i, j ∈ V , similarity sij in our case can be defined
as sij = |Γ(i)∩Γ(j)|, i.e the number of documents
commonly covered by both i and j.
Facility Location: The facility location func-
tion, defined as f(S) = EiEV maxjES sij, is a
natural model for k-medoids and exemplar based
clustering, and has been used in several summariza-
tion problems (Tschiatschek et al., 2014; Wei et al.,
2014a).
Penalty based diversity: A similarity ma-
trix may be used to express a form of coverage
of a set S but that is then penalized with a re-
dundancy term, as in the following difference:
f(S) = EiEV,jES sij − λ EiES EjES, si,j (Lin
and Bilmes, 2011)). Here λ ∈ [0, 1]. This function
is submodular, but is not in general monotone, and
has been used in document summarization (Lin and
Bilmes, 2011), as a dispersion function (Borodin
et al., 2012), and in image summarization (Tschi-
atschek et al., 2014).
</bodyText>
<subsectionHeader confidence="0.56791">
3.1.3 Quality Control (QC) Functions
</subsectionHeader>
<bodyText confidence="0.9981335">
QC functions ensure a quality criteria is met by a
set S of topics. We define the quality score of the
set S as Fq (S) = EsES fq (s), where fq (s) is
the quality score of topic s for quality q. Therefore,
Fq (S) is a modular function in S. We investigate
three types of quality control functions: Topic
Specificity, Topic Clarity, and Topic Relevance.
Topic Specificity: The farther a topic is from
the root of the DAG, the more specific it becomes.
Topics higher up in the hierarchy are abstract and
less specific. We therefore prefer topics low in the
DAG, but lower topics also have less coverage. We
define fspecificity (s) = sh where sh is the height of
topic s in the DAG. The root topic has height zero
and the “height” increases as we move down the
DAG in Figure 1.
Topic Clarity: Topic clarity is the fraction of
descendant topics that cover one or more docu-
ments. If a topic has many descendant topics that
do not cover any documents, it has less clarity. For-
</bodyText>
<equation confidence="0.9047315">
mally, s = Et∈descendants(s) QΓ(t)&gt;01 where
fclarity() I descendants (s)
</equation>
<bodyText confidence="0.705111">
is the indicator function.
</bodyText>
<listItem confidence="0.998583666666667">
Topic Relevance: A topic is considered to be
better related to a document if the number of hops
needed to reach the document from that topic is
lower. Given any set A ⊆ D of document, and
any topic s ∈ V , we can define frelevance (s|A) =
argminα{α : A ⊆ Γα(s)}.
</listItem>
<bodyText confidence="0.823084">
QC Functions As Barrier Modular Mixtures:
We introduce a modular function for every QC
function as follows
</bodyText>
<equation confidence="0.9764405">
1 if the height of topic s is at least α
fαspecificity (s) = 0 otherwise
</equation>
<bodyText confidence="0.993249142857143">
for every possible value of α. This creates a sub-
modular mixture with as many components as the
number of possible values of α. In our experiments
with Wikipedia, we had α varying from 1 to 120
stepping by 1, adding 120 modular mixture compo-
nents. Similarly, we define,
ffa O 1 if the clarity of topic s is at least β
Lty s = 0 otherwise
for every possible (discretized to make it count-
ably finite) value of Q. And,
fγrelevance (s) = fcov (s|Γγ (s)), where fcov (.) is
the coverage submodular function and s|X indi-
cates coverage of a topic s over a set of documents
X. All these functions (modular and submodular
terms) are added as mixture components in our
learning framework to learn suitable weights for
them. We then use these weights in our inference
procedure to obtain a subset of topics as described
in 3.2. We show from our experiments that this
approach performs better than all other approaches
and baselines.
</bodyText>
<subsectionHeader confidence="0.895705">
3.1.4 Fidelity Functions
</subsectionHeader>
<bodyText confidence="0.985845269230769">
A function representing the fidelity of a set S to
another reference set R is one that gets a large value
when the set S represents the set R. Such a function
scores inferred topics high when it resembles a
reference set of topics and/or item clusters. The
reference set in this case can be produced from
other algorithms such as k-means, LDA and its
variants or from a manually tagged corpus. Next
we describe one such fidelity function.
Topic Coherence: This function scores a set
of topics S high when the transitive cover (Def-
inition 1) produced by the topics in S resembles
the clusters of documents produced by an external
source (k-means, LDA or manual). Given an exter-
nal source that clusters the documents, producing T
clusters L1, L2, ..., LT (for T topics), topic coher-
ence is defined as: f(S) = EtET maxkES wk,t
where wk,t = harmonic mean(wpk,t, wrk,t) and
wk,t = |Γ(k)n|Γ(k)|t |and wk t = |Γ( k)n|Lt |. Note that,
wpk,t ≥ 0 and wrk,t ≥ 0 are the precision on recall
of the resemblance and wk,t is the F1 measure. If
the transitive cover of topics in S resembles the
reference clusters Lt exactly, we attain maximum
coherence (or fidelity). As the resemblance dimin-
ishes, the score decreases. The above function f(S)
is monotone submodular.
</bodyText>
<page confidence="0.993967">
558
</page>
<subsectionHeader confidence="0.963124">
3.1.5 Mixture of Submodular Components:
</subsectionHeader>
<bodyText confidence="0.9953748">
Given the different classes of submodular functions
above, we construct our submodular scoring func-
tions Fw(·) as a convex combinations of these dif-
ferent submodular functions f1, f2, ... , fm, above.
In other words,
</bodyText>
<equation confidence="0.999177">
Fw(S) = �m wifi(S), (2)
i=1
</equation>
<bodyText confidence="0.9876341">
where w = (w1, ... , wm), wi ≥ 0, Ei wi = 1.
The components fi are submodular and assumed to
be normalized: i.e., fi(∅) = 0, and fi(V ) = 1 for
monotone functions and maxACV fi(A) ≤ 1 for
non-monotone functions. A simple way to normal-
ize a monotone submodular function is to define
the component as fi(S)/fi(V ). This ensures that
the components are compatible with each other.
Obviously, the merit of the scoring function Fw(·)
depends on the selection of the components.
</bodyText>
<subsectionHeader confidence="0.999016">
3.2 Large Margin Learning
</subsectionHeader>
<bodyText confidence="0.999176538461538">
We optimize the weights w of the scoring func-
tion Fw(·) in a large-margin structured prediction
framework. In this setting, we assume we have
training data in the form of pairs of a set of docu-
ments, and a human generated summary as a set
of topics. For example, in the case of Wikipedia
disambiguation, we use the human generated dis-
ambiguation pages as the ground truth summary.
We represent the set of ground-truth summaries as
S = {S1, S2, · · · , SN}. In large margin training,
the weights are optimized such that ground-truth
summaries S are separated from competitor sum-
maries by a loss-dependent margin:
</bodyText>
<equation confidence="0.985961">
Fw(S) &gt; Fw(S0) + G(S0), `dS E S, S0 E Y \ S, (3)
</equation>
<bodyText confidence="0.999623363636364">
where L(·) is the loss function, and where Y
is a structured output space (for example Y
is the set of summaries that satisfy a certain
budget B, i.e., Y = {S&apos; ⊆ V : |S&apos; |≤ B}).
We assume the loss to be normalized,
0 ≤ L(S&apos;) ≤ 1, ∀S&apos; ⊆ V , to ensure that mixture
and loss are calibrated. Equation (3) can be stated
as Fw(S) ≥ maxS0EY [Fw(S&apos;) + L(S&apos;)] , ∀S ∈ S
which is called loss-augmented inference. We
introduce slack variables and minimize the
regularized sum of slacks (Lin and Bilmes, 2012):
</bodyText>
<equation confidence="0.96780175">
min [max [Fw (S0) + G(S0)] − Fw(S)
≥ J
w0,11w111=1 S∈S SPEY
+ λ2 I1wI122, (4)
</equation>
<bodyText confidence="0.999833222222222">
where the non-negative orthant constraint, w ≥ 0,
ensures that the final mixture is submodular. Note
a 2-norm regularizer is used on top of a 1-norm
constraint kwk1 = 1 which we interpret as a prior
to encourage higher entropy, and thus more diverse
mixture distributions. Tractability depends on the
choice of the loss function. The parameters w
are learnt using stochastic gradient descent as in
(Tschiatschek et al., 2014).
</bodyText>
<subsectionHeader confidence="0.999795">
3.3 Loss Functions
</subsectionHeader>
<bodyText confidence="0.999989777777778">
A natural choice of loss functions for our case can
be derived from cluster evaluation metrics. Every
inferred topic s induces a subset of documents,
namely the transitive cover Γ (s) of s. We compare
these clusters with the clusters induced from the
true topics in the training set and compute the loss.
In this paper, we use the Jaccard Index (JI) as a
loss function. Let S be the inferred topics and T
be the true topics. The Jaccard loss is defined as
</bodyText>
<equation confidence="0.814503">
Ljaccard(S, T) = 1 − k EsES maxtET jΓ(sjur(t)|
</equation>
<bodyText confidence="0.9999712">
where k = |S |= |T |is the number of topics.
When the clustering produced by the inferred and
the true topics are similar, Jaccard loss is 0. When
they are completely dissimilar, the loss is maxi-
mum, i.e., 1. Jaccard loss is a modular function.
</bodyText>
<subsectionHeader confidence="0.77062">
3.4 Inference Algorithm: Greedy
</subsectionHeader>
<bodyText confidence="0.999888">
Having learnt the weights for the mixture
components, the resulting function Fw(S) =
Emi=1 wifi(S) is a submodular function. In the
case when the individual components are them-
selves monotone (all our functions in fact are),
Fw(S) can be optimized by the accelerated greedy
algorithm (Minoux, 1978). Thanks to submodu-
larity, we can obtain near optimal solutions very
efficiently. In case the functions are all monotone
submodular, we can guarantee that the solution is
within 1 − 1/e factor from the optimal solution.
</bodyText>
<sectionHeader confidence="0.990842" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999910235294118">
To validate our approach, we make use of
Wikipedia category structure as a topic DAG and
apply our technique to the task of automatic
generation of Wikipedia disambiguation pages.
We pre-processed the category graph to elimi-
nate the cycles in order to make it a DAG. Each
Wikipedia disambiguation page is manually created
by Wikipedia editors by grouping a collection of
Wikipedia articles into several groups. Each group
is then assigned a name, which serves as a topic for
the group. Typically, a disambiguation page segre-
gates around 20-30 articles into 5-6 groups. Our
goal is to measure how accurately we can recre-
ate the groups for a disambiguation page and label
them, given only the collection of articles men-
tioned in that disambiguation page (when actual
groupings and labels are hidden).
</bodyText>
<page confidence="0.995504">
559
</page>
<subsectionHeader confidence="0.948356">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999968111111111">
We parsed the contents of Wikipedia disambigua-
tion pages and extracted disambiguation page
names, article groups and group names. We col-
lected about 8000 disambiguation pages that had
at least four groups on them. Wikipedia category
structure is used as the topic DAG. We eliminated
few administrative categories such as “Hidden Cat-
egories”, “Articles needing cleanup”, and the like.
The final DAG had about 1M topics and 3M links.
</bodyText>
<subsectionHeader confidence="0.992542">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999728">
Every group of articles on the Wikipedia disam-
biguation page is assigned a name by the editors.
Unfortunately, these names may not correspond to
the Wikipedia category (topic) names. For exam-
ple, one of the groups on the “Matrix” disambigua-
tion page has a name “Business and government”
and there is no Wikipedia category by that name.
However, the group names generated by our (and
baseline) method are from the Wikipedia categories
(which forms our topic DAG). In addition, there
can be multiple relevant names for a group. For
example, a group on a disambiguation page may
be called “Calculus”, but an algorithm may rightly
generate “Vector Calculus”. Hence we cannot eval-
uate the accuracy of an algorithm just by matching
the generated group names to those on the disam-
biguation page. To alleviate this problem, we adopt
cluster-based evaluation metrics. We treat every
group of articles generated by an algorithm under a
topic for a disambiguation page as a cluster of arti-
cles. These are considered as inferred clusters for a
disambiguation page. We compare them against the
actual grouping of articles on the Wikipedia disam-
biguation page by treating those groups as true clus-
ters. We can now adopt Jaccard Index, F1-measure,
and NMI (Normalized Mutual Information) based
cluster evaluation metrics described in (Manning
et al., 2008). For each disambiguation page in the
test set, we compute every metric score and then
average it over all the disambiguation pages.
</bodyText>
<subsectionHeader confidence="0.999758">
4.3 Methods Compared
</subsectionHeader>
<bodyText confidence="0.999595777777778">
We validated our approach by comparing against
several baselines described below. We also com-
pared two variations of our approach as described
next. For each of these cases (baselines and two
variations) we generated and compared the metrics
(Jaccard Index, F1-measure and NMI) as described
in the previous section.
KMdocs: K-Means algorithm run on articles as
TF-IDF vectors of words. The number of clus-
ters K is set to the number of true clusters on the
Wikipedia disambiguation page.
KMeddocs: K-Medoids algorithm with articles
as TF-IDF vectors of words. The number of clus-
ters are set as in KMdocs.
KMedtopics: K-Medoids run on topics as TF-
IDF vectors of words. The words for each topic
is taken from the articles that are in the transitive
cover of the topic.
LDAdocs: LDA algorithm with the number of
topics set to the number of true clusters on the
Wikipedia disambiguation page. Each article is
then grouped under the highest probability topic.
SMMLcov: This is the submodular mixture
learning case explained in section 3.1.5. Here we
consider a mixture of all the submodular functions
governing coverage, diversity, fidelity and QC func-
tions. However, we exclude the similarity based
functions described in section 3.1.2. Coverage
based functions have a time complexity of O (n)
whereas similarity based functions are O (n2). By
excluding similarity based functions, we can com-
pare the quality of the results with and without
O(n2) functions. We learn the mixture weights
from the training set and use them during infer-
ence on the test set to subset K topics through the
submodular maximization (Equation 1).
SMMLcov+sim: This case is similar to SMMLcov
except that, we include similarity based submodu-
lar mixture components. This makes the inference
time complexity O (n2).
We do not compare against HSLDA, PAM and
few other techniques cited in the related work sec-
tions because they do not produce a subset of K
summary topics — these are not directly compara-
ble with our work.
</bodyText>
<subsectionHeader confidence="0.992798">
4.4 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999917529411765">
We show that the submodular mixture learning
and maximization approaches, i.e., SMMLcov and
SMMLcov+sim outperform other approaches in vari-
ous metrics. In all these experiments, we performed
5 fold cross validation to learn the parameters from
80% of the disambiguation pages and evaluated on
the rest of the 20%, in each fold.
In Figure 2a we summarize the results of the
comparison of the methods mentioned above on
Jaccard Index, F1 measure and NMI. Our pro-
posed techniques SMMLcov and SMMLcov+sim out-
perform other techniques consistently.
In Figures 2b and 2c we measure the number
of test instances (i.e., disambiguation queries) in
which each of the algorithms dominate (win) in
evaluation metrics. In 60% of the disambiguation
queries, SMMLcov and SMMLcov+sim approaches
</bodyText>
<page confidence="0.977538">
560
</page>
<figure confidence="0.9987530625">
KMdocs
KMeddocs
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
70
60
50
40
30
20
10
0
JI
JI
(b) Winni
70
60
50
40
30
20
10
0
JI
(c) Winning
M
</figure>
<figureCaption confidence="0.99997">
Figure 2: Comparison of techniques
</figureCaption>
<bodyText confidence="0.999559176470588">
produce higher JI, F1 and NMI than all other meth-
ods. This indicates that the clusters of articles pro-
duced by our technique resembles the clusters of
articles present on the disambiguation page better
than other techniques.
From Figures 2b and 2c it is clear that O (n) time
complexity based submodular mixture functions
(SMMLcov) perform on par with O (n2) based
functions (SMMLcov+sim), but at a greatly reduced
execution time, demonstrating the sufficiency of
O (n) functions for our task. On the average, for
each disambiguation query, SMMLcov took around
40 seconds (over 1M topics and 3M edges DAG) to
infer the topics, whereas SMMLcov+sim took around
35 minutes. Both these experiments were carried
on a machine with 32 GB RAM, Eight-Core AMD
Opteron(tm) Processor 2427.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.994501259259259">
We investigated a problem of summarizing topics
over a massive topic DAG such that the summary
set of topics produced represents the objects in
the collection. This representation is characterized
through various classes of submodular (and mono-
tone) functions that captured coverage, similarity,
diversity, specificity, clarity, relevance and fidelity
of the topics. Currently we assume that the number
of topics K is given as an input to our algorithm. It
would be an interesting future problem to estimate
the value of K automatically in our setting. As fu-
i%Wn i%Wn
ture work, we also plan to extend our techniques to
produce a hierarchical summary of topics and scale
it across heterogeneous collection of objects (from
different domains) to bring all of them under the
same topic DAG and investigate interesting cases
thereon.
Acknowledgements: This material is based
upon work supported by the National Science Foun-
dation under Grant No. IIS-1162606, and by a
Google, a Microsoft, and an Intel research award.
Rishabh Iyer acknowledges support from the Mi-
crosoft Research Ph.D Fellowship.
ci
aril
cSi
</bodyText>
<page confidence="0.68027">
,6
</page>
<sectionHeader confidence="0.997646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.974944142857143">
Zafer Barutcuoglu, Robert E. Schapire, and Olga G.
Troyanskaya. 2006. Hierarchical multi-label pre-
diction of gene function. Bioinforniatics, 22(7):830–
836, April.
W. Bi and J. T. Kwok. 2011. Multi-label classification
on tree-and DAG-structured hierarchies. In ICML.
ICML.
</reference>
<equation confidence="0.749278">
t e
n F r
Win%
</equation>
<page confidence="0.990618">
561
</page>
<reference confidence="0.999567542056075">
David M. Blei and John D. Lafferty. 2006. Correlated
topic models. In In Proceedings of the 23rd Interna-
tional Conference on Machine Learning, pages 113–
120. MIT Press.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process. In
Advances in Neural Information Processing Systems,
page 2003. MIT Press.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A
collaboratively created graph database for structur-
ing human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ’08, pages 1247–1250,
New York, NY, USA. ACM.
Allan Borodin, Hyun Chul Lee, and Yuli Ye. 2012.
Max-sum diversification, monotone submodular
functions and dynamic updates. In Proceedings
of Principles of Database Systems, pages 155–166.
ACM.
Razvan Bunescu and Marius Pasca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of the 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-06), Trento, Italy, pages 9–
16, April.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248–255. IEEE.
Stephen Dill, Nadav Eiron, David Gibson, Daniel
Gruhl, R. Guha, Anant Jhingran, Tapas Kanungo,
Sridhar Rajagopalan, Andrew Tomkins, John A.
Tomlin, and Jason Y. Zien. 2003. Semtag and
seeker: Bootstrapping the semantic web via auto-
mated semantic annotation. In Proceedings of the
12th International Conference on World Wide Web,
WWW ’03, pages 178–186, New York, NY, USA.
ACM.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
On-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of the 19th
ACM International Conference on Information and
Knowledge Management, CIKM ’10, pages 1625–
1628, New York, NY, USA.
R. Iyer, S. Jegelka, and J. Bilmes. 2013. Fast
semidifferential-based submodular function opti-
mization. ICML.
Tapas Kanungo, David M. Mount, Nathan S. Ne-
tanyahu, Christine D. Piatko, Ruth Silverman, An-
gela Y. Wu, Senior Member, and Senior Mem-
ber. 2002. An efficient k-means clustering algo-
rithm: Analysis and implementation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
24:881–892.
D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maxi-
mizing the spread of influence through a social net-
work. In SIGKDD.
Katrin Kirchhoff and Jeff Bilmes. 2014. Submodular-
ity for data selection in machine translation. Octo-
ber.
A. Krause and C. Guestrin. 2005. Near-optimal non-
myopic value of information in graphical models.
In Proceedings of Uncertainity in Artificial Intelli-
gence. UAI.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the 23rd International
Conference on Machine Learning, ICML ’06, pages
577–584, New York, NY, USA. ACM.
H. Lin and J. Bilmes. 2010. Multi-document summa-
rization via budgeted maximization of submodular
functions. In NAACL.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In The 49th
Meeting of the Assoc. for Comp. Ling. Human Lang.
Technologies (ACL/HLT-2011), Portland, OR, June.
H. Lin and J. Bilmes. 2012. Learning mixtures of sub-
modular shells with application to document summa-
rization. In Conference on Uncertainty in Artificial
Intelligence (UAI), page 479490.
Hui Lin. 2012. Submodularity in Natural Language
Processing: Algorithms and Applications. Ph.D.
thesis, University of Washington, Dept. of EE.
Min ling Zhang and Zhi hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. PAT-
TERN RECOGNITION, 40:2007.
Arun S. Maiya, John P. Thompson, Francisco Loaiza-
Lemos, and Robert M. Rolfe. 2013. Exploratory
analysis of highly heterogeneous document collec-
tions. In Proceedings of the 19th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, KDD ’13, pages 1375–1383, New
York, NY, USA. ACM.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Olena Medelyan, Ian H. Witten, and David Milne.
2008. Topic indexing with Wikipedia. In Proceed-
ings of the Wikipedia and AI workshop at AAAI-08.
AAAI.
</reference>
<page confidence="0.971117">
562
</page>
<reference confidence="0.999879040540541">
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, KDD ’07, pages 490–499, New York,
NY, USA. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
Proceedings of the Sixteenth ACM Conference on
Conference on Information and Knowledge Manage-
ment, CIKM ’07, pages 233–242, New York, NY,
USA. ACM.
David Milne. 2009. An open-source toolkit for min-
ing wikipedia. In In Proc. New Zealand Computer
Science Research Student Conf, page 2009.
Michel Minoux. 1978. Accelerated greedy algo-
rithms for maximizing submodular set functions. In
J. Stoer, editor, Optimization Techniques, volume 7
of Lecture Notes in Control and Information Sci-
ences, chapter 27, pages 234–243. Springer Berlin
Heidelberg, Berlin/Heidelberg.
Mukund Narasimhan and Jeff Bilmes. 2004. PAC-
learning bounded tree-width graphical models. In
Uncertainty in Artificial Intelligence: Proceedings
of the Twentieth Conference (UAI-2004). Morgan
Kaufmann Publishers, July.
George L Nemhauser, Laurence A Wolsey, and Mar-
shall L Fisher. 1978. An analysis of approximations
for maximizing submodular set functionsi. Mathe-
matical Programming, 14(1):265–294.
Adler J. Perotte, Frank Wood, Noemie Elhadad, and
Nicholas Bartlett. 2011. Hierarchically supervised
latent dirichlet allocation. In John Shawe-Taylor,
Richard S. Zemel, Peter L. Bartlett, Fernando C. N.
Pereira, and Kilian Q. Weinberger, editors, NIPS,
pages 2609–2617.
Juho Rousu, Craig Saunders, Sndor Szedmk, and John
Shawe-Taylor. 2006. Kernel-based learning of hier-
archical multilabel classification models. Journal of
Machine Learning Research, 7:1601–1626.
Jr. Silla, CarlosN. and AlexA. Freitas. 2011. A survey
of hierarchical classification across different applica-
tion domains. Data Mining and Knowledge Discov-
ery, 22(1-2):31–72.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, WWW ’07, pages 697–
706, New York, NY, USA. ACM.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associ-
ation, 101(476):1566–1581.
Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei,
and Jeff Bilmes. 2014. Learning Mixtures of Sub-
modular Functions for Image Collection Summariza-
tion. In Neural Information Processing Systems
(NIPS).
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis
Vlahavas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining and
Knowledge Discovery Handbook, pages 667–685.
Springer US.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2014a. Fast
multi-stage submodular maximization. In ICML.
Kai Wei, Yuzong Liu, Katrin Kirchhoff, Chris Bartels,
and Jeff Bilmes. 2014b. Submodular subset selec-
tion for large-scale speech training data. Proceed-
ings of ICASSP, Florence, Italy.
Torsten Zesch and Iryna Gurevych. 2007. Analy-
sis of the wikipedia category graph for nlp applica-
tions. In Proceedings of the TextGraphs-2 Workshop
(NAACL-HLT), pages 1–8, Rochester, April. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.998942">
563
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.531471">
<title confidence="0.996536">Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures</title>
<author confidence="0.951263">B Ramakrishna</author>
<affiliation confidence="0.984612">IITB-Monash Research IIT</affiliation>
<address confidence="0.99944">Mumbai, 40076,</address>
<email confidence="0.948714">bairi@cse.iitb.ac.in</email>
<author confidence="0.957337">Ganesh</author>
<affiliation confidence="0.994973">IIT</affiliation>
<address confidence="0.999477">Mumbai, 40076,</address>
<email confidence="0.971686">ganesh@cse.iitb.ac.in</email>
<author confidence="0.703028">Rishabh</author>
<affiliation confidence="0.986662">University of</affiliation>
<address confidence="0.971816">Seattle, WA-98175,</address>
<email confidence="0.999834">rkiyer@u.washington.edu</email>
<author confidence="0.98445">Jeff</author>
<affiliation confidence="0.99921">University of</affiliation>
<address confidence="0.981574">Seattle, WA-98175,</address>
<email confidence="0.999827">bilmes@uw.edu</email>
<abstract confidence="0.999360210526316">We study the problem of summarizing DAG-structured topic hierarchies over a given set of documents. Example applications include automatically generating Wikipedia disambiguation pages for a set of articles, and generating candidate multi-labels for preparing machine learning datasets (e.g., for text classification, functional genomics, and image classification). Unlike previous work, which focuses on clustering the set of documents using the topic hierarchy as features, we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features. Desirable properties of the chosen topics include document coverage, specificity, topic diversity, and topic homogeneity, each of which, we show, is naturally modeled by a submodular function. Other information, provided say by unsupervised approaches such as LDA and its variants, can also be utilized by defining a submodular function that expresses coherence between the chosen topics and this information. We use a large-margin framework to learn convex mixtures over the set of submodular components. We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth. We find that our framework improves upon several baselines according to a variety of standard evaluation metrics including the Jaccard Index, F1 score and NMI, and moreover, can be scaled to extremely large scale problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Zafer Barutcuoglu</author>
<author>Robert E Schapire</author>
<author>Olga G Troyanskaya</author>
</authors>
<title>Hierarchical multi-label prediction of gene function.</title>
<date>2006</date>
<journal>Bioinforniatics,</journal>
<volume>22</volume>
<issue>7</issue>
<pages>836</pages>
<contexts>
<context position="2606" citStr="Barutcuoglu et al., 2006" startWordPosition="372" endWordPosition="375">rchy based categorization of topics for a set of objects. Objects could be, e.g., a set of documents for text classification, a set of genes in functional genomics, or a set of images in computer vision. One can often define a natural topic hierarchy to categorize these objects. For example, in text and image classification problems, each document or image is assigned a hierarchy of labels — a baseball page is assigned the labels “baseball” and “sports.” Moreover, many of these applications, naturally have an existing topic hierarchy generated on the entire set of objects (Rousu et al., 2006; Barutcuoglu et al., 2006; ling Zhang and hua Zhou, 2007; Silla and Freitas, 2011; Tsoumakas et al., 2010). Given a DAG-structured topic hierarchy and a subset of objects, we investigate the problem of finding a subset of DAG-structured topics that are induced by that subset (of objects). This problem arises naturally in several real world applications. For example, consider the problem of identifying appropriate label sets for a collection of articles. Several existing text collection datasets such as 20 Newsgroup1, Reuters-215782 work with a predefined set of topics. We observe that these topic names are highly abst</context>
</contexts>
<marker>Barutcuoglu, Schapire, Troyanskaya, 2006</marker>
<rawString>Zafer Barutcuoglu, Robert E. Schapire, and Olga G. Troyanskaya. 2006. Hierarchical multi-label prediction of gene function. Bioinforniatics, 22(7):830– 836, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bi</author>
<author>J T Kwok</author>
</authors>
<title>Multi-label classification on tree-and DAG-structured hierarchies.</title>
<date>2011</date>
<booktitle>In ICML. ICML.</booktitle>
<contexts>
<context position="13246" citStr="Bi and Kwok, 2011" startWordPosition="2079" endWordPosition="2082">s for a group of documents; (2) we do not enforce the label hierarchy to be a tree as it can be a DAG; and (3) generalizing HSLDA to use a DAG structured hierarchy and infer labels for a group of documents (e.g., combining into one big document) also may not help in solving our problem. HSLDA will apply all the relevant labels to the documents as per the classifier that it learns for every label. Moreover, the “root” label is always applied and it is very likely that many labels near the top level of the label hierarchy are also classified as relevant to the group of documents. Wei and James (Bi and Kwok, 2011) present a hierarchical multi-label classification algorithm that can be used on both tree and DAG structured hierarchies. They formulate a search for the optimal consistent multi-label as the finding of the best subgraph in a tree/DAG. In our approach, we assume, individual documents are already associated with one or more topics and we find a consistent label set for a group of documents using the DAG structured topic hierarchy. Medelyan et al. (Medelyan et al., 2008) and Ferragina et al. (Ferragina and Scaiella, 2010) detect topics for a document using Wikipedia article names and category n</context>
<context position="17363" citStr="Bi and Kwok, 2011" startWordPosition="2745" endWordPosition="2748">ts and topics can be hard or soft. In case of a hard link, a document is attached to a set of topics. Examples include multi-labeled documents. In case of a soft link, a document is associated with a topic with some degree of confidence (or probability). Furthermore, if a document is attached to a topic t, we assume that all the ancestor topics of t are also relevant for that document. This 7A simple greedy algorithm (Nemhauser et al., 1978) obtains a 1 − 1/e approximation guarantee for monotone submodular function maximization assumption has been employed in earlier works (Blei et al., 2004; Bi and Kwok, 2011; Rousu et al., 2006) as well. Given a budget of K, our objective is to choose a set of K topics from V , which best describe the documents in D. The notion of best describing topics is characterized through a set of desirable properties - coverage, diversity, specificity, clarity, relevance and fidelity - that K topics have to satisfy. The submodular functions that we introduce in the next section ensure these properties are satisfied. Formally, we solve the following discrete optimization problem: � S* E argmax wifi(S) (1) ScV :|S|&lt;K i where, fi are monotone submodular mixture components and</context>
</contexts>
<marker>Bi, Kwok, 2011</marker>
<rawString>W. Bi and J. T. Kwok. 2011. Multi-label classification on tree-and DAG-structured hierarchies. In ICML. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Correlated topic models. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning,</booktitle>
<pages>113--120</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10840" citStr="Blei and Lafferty, 2006" startWordPosition="1664" endWordPosition="1667">jects. LDA (Blei et al., 2003) clusters the documents and simultaneously produces a set of topics into which the documents are clustered. In LDA, each document may be viewed as a mixture of various topics and the topic distribution is assumed to have a Dirichlet prior. LDA associates a group of high probability words to each identified topic. A name can be assigned to a topic by manually inspecting the words or using additional algorithms like (Mei et al., 2007; Maiya et al., 2013). LDA does not make use of existing topic hierarchies and correlation between topics. The Correlated Topic Model (Blei and Lafferty, 2006) induces a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (Blei et al., 2004), where topics are joined together in a hierarchy by using the nested Chinese restaurant process. Nonparametric extensions of LDA include the Hierarchical Dirichlet Process (Teh et al., 2006) mixture model, which allows the number of topics to be unbounded and learnt from data and the Nested Chinese Restaurant Process which allows topics to be arranged in a hierarchy whose structure is learnt from data. In each of these</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David M. Blei and John D. Lafferty. 2006. Correlated topic models. In In Proceedings of the 23rd International Conference on Machine Learning, pages 113– 120. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="10246" citStr="Blei et al., 2003" startWordPosition="1563" endWordPosition="1566">ate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) cluster the objects and then identify names for the clusters; or b) dynamically identify topics (including hierarchical) for a set of objects. LDA (Blei et al., 2003) clusters the documents and simultaneously produces a set of topics into which the documents are clustered. In LDA, each document may be viewed as a mixture of various topics and the topic distribution is assumed to have a Dirichlet prior. LDA associates a group of high probability words to each identified topic. A name can be assigned to a topic by manually inspecting the words or using additional algorithms like (Mei et al., 2007; Maiya et al., 2013). LDA does not make use of existing topic hierarchies and correlation between topics. The Correlated Topic Model (Blei and Lafferty, 2006) induc</context>
<context position="19277" citStr="Blei et al., 2003" startWordPosition="3063" endWordPosition="3066">tems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K E Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only a set of documents is classified and categorized in some way. We next provide a few definitions needed later in the paper. Definition 1: Transitive Cover F): A topic t is said to cover a set of documents F(t), called the transitive cover of the topic t, if for all documents i E F(t), either i is associated directly with topic t or with any of the descendant topics of t in the topic DAG. A natural extension of this definition to a set of topics T is defined as F(T) = UtETF(t). 556 Definition 2: Truncated Transitive Cover (Γα): This is a transitive cover of topic t, but wi</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>page</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11017" citStr="Blei et al., 2004" startWordPosition="1692" endWordPosition="1695">ure of various topics and the topic distribution is assumed to have a Dirichlet prior. LDA associates a group of high probability words to each identified topic. A name can be assigned to a topic by manually inspecting the words or using additional algorithms like (Mei et al., 2007; Maiya et al., 2013). LDA does not make use of existing topic hierarchies and correlation between topics. The Correlated Topic Model (Blei and Lafferty, 2006) induces a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (Blei et al., 2004), where topics are joined together in a hierarchy by using the nested Chinese restaurant process. Nonparametric extensions of LDA include the Hierarchical Dirichlet Process (Teh et al., 2006) mixture model, which allows the number of topics to be unbounded and learnt from data and the Nested Chinese Restaurant Process which allows topics to be arranged in a hierarchy whose structure is learnt from data. In each of these approaches, unlike our proposed approach, an existing topic hierarchy is not used, nor is any additional objecttopic information leveraged. The pachinko allocation model (PAM)(</context>
<context position="17344" citStr="Blei et al., 2004" startWordPosition="2741" endWordPosition="2744">between the documents and topics can be hard or soft. In case of a hard link, a document is attached to a set of topics. Examples include multi-labeled documents. In case of a soft link, a document is associated with a topic with some degree of confidence (or probability). Furthermore, if a document is attached to a topic t, we assume that all the ancestor topics of t are also relevant for that document. This 7A simple greedy algorithm (Nemhauser et al., 1978) obtains a 1 − 1/e approximation guarantee for monotone submodular function maximization assumption has been employed in earlier works (Blei et al., 2004; Bi and Kwok, 2011; Rousu et al., 2006) as well. Given a budget of K, our objective is to choose a set of K topics from V , which best describe the documents in D. The notion of best describing topics is characterized through a set of desirable properties - coverage, diversity, specificity, clarity, relevance and fidelity - that K topics have to satisfy. The submodular functions that we introduce in the next section ensure these properties are satisfied. Formally, we solve the following discrete optimization problem: � S* E argmax wifi(S) (1) ScV :|S|&lt;K i where, fi are monotone submodular mix</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. In Advances in Neural Information Processing Systems, page 2003. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18516" citStr="Bollacker et al., 2008" startWordPosition="2941" endWordPosition="2944"> ScV :|S|&lt;K i where, fi are monotone submodular mixture components and wi ≥ 0 are the weights associated with those mixture components. Set S* is the summary topics scored best. It is easy to find massive (i.e., size in the order of million) DAG structured topic hierarchies in practice. Wikipedia’s category hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08, pages 1247–1250, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan Borodin</author>
<author>Hyun Chul Lee</author>
<author>Yuli Ye</author>
</authors>
<title>Max-sum diversification, monotone submodular functions and dynamic updates.</title>
<date>2012</date>
<booktitle>In Proceedings of Principles of Database Systems,</booktitle>
<pages>155--166</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="25575" citStr="Borodin et al., 2012" startWordPosition="4116" endWordPosition="4119"> EiEV maxjES sij, is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity matrix may be used to express a form of coverage of a set S but that is then penalized with a redundancy term, as in the following difference: f(S) = EiEV,jES sij − λ EiES EjES, si,j (Lin and Bilmes, 2011)). Here λ ∈ [0, 1]. This function is submodular, but is not in general monotone, and has been used in document summarization (Lin and Bilmes, 2011), as a dispersion function (Borodin et al., 2012), and in image summarization (Tschiatschek et al., 2014). 3.1.3 Quality Control (QC) Functions QC functions ensure a quality criteria is met by a set S of topics. We define the quality score of the set S as Fq (S) = EsES fq (s), where fq (s) is the quality score of topic s for quality q. Therefore, Fq (S) is a modular function in S. We investigate three types of quality control functions: Topic Specificity, Topic Clarity, and Topic Relevance. Topic Specificity: The farther a topic is from the root of the DAG, the more specific it becomes. Topics higher up in the hierarchy are abstract and less</context>
</contexts>
<marker>Borodin, Lee, Ye, 2012</marker>
<rawString>Allan Borodin, Hyun Chul Lee, and Yuli Ye. 2012. Max-sum diversification, monotone submodular functions and dynamic updates. In Proceedings of Principles of Database Systems, pages 155–166. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06),</booktitle>
<pages>9--16</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="18849" citStr="Bunescu and Pasca, 2006" startWordPosition="2992" endWordPosition="2995">ore than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K E Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only a set of documents is classified and categorized in some way. We next provide a few definitions needed later in the paper. Definition 1: Transitive C</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), Trento, Italy, pages 9– 16, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>248--255</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6987" citStr="Deng et al., 2009" startWordPosition="1045" endWordPosition="1048">Music Technology Apple computer Apple Key Apple Store HP Apple Other Apple Card game Apple Daily Apple Novel Apple Albums Apple Band Apple Records Places Apple Oklahoma Apple River Apple Valley Plants Malus Cashew Apple Hedge Apple The Apple Documents not grouped under any summary topic set for the dataset can result in a large number of labels and become unmanageable. Our proposed techniques can summarize such large sets of labels into a smaller and more meaningful label sets using a DAG-structured topic hierarchy. This also holds for image classification problems and datasets like ImageNet (Deng et al., 2009). We use the term summarize to highlight the fact that the smaller label set semantically covers the larger label set. For example, the topics Physics, Chemistry, and Mathematics can be summarized into a topic Science. A particularly important application of our work (and the one we use for our evaluations in Section 4) is the following: Given a collection of articles spanning different topics, but with similar titles, automatically generate a disambiguation page for those titles using the Wikipedia category hierarchy4 as a topic DAG. Disambiguation pages5 on Wikipedia are used to resolve conf</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Dill</author>
<author>Nadav Eiron</author>
<author>David Gibson</author>
<author>Daniel Gruhl</author>
<author>R Guha</author>
<author>Anant Jhingran</author>
<author>Tapas Kanungo</author>
<author>Sridhar Rajagopalan</author>
<author>Andrew Tomkins</author>
<author>John A Tomlin</author>
<author>Jason Y Zien</author>
</authors>
<title>Semtag and seeker: Bootstrapping the semantic web via automated semantic annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on World Wide Web, WWW ’03,</booktitle>
<pages>178--186</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18796" citStr="Dill et al., 2003" startWordPosition="2984" endWordPosition="2987">. Wikipedia’s category hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K E Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only a set of documents is classified and categorized in some way. We next provide a few definitions </context>
</contexts>
<marker>Dill, Eiron, Gibson, Gruhl, Guha, Jhingran, Kanungo, Rajagopalan, Tomkins, Tomlin, Zien, 2003</marker>
<rawString>Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl, R. Guha, Anant Jhingran, Tapas Kanungo, Sridhar Rajagopalan, Andrew Tomkins, John A. Tomlin, and Jason Y. Zien. 2003. Semtag and seeker: Bootstrapping the semantic web via automated semantic annotation. In Proceedings of the 12th International Conference on World Wide Web, WWW ’03, pages 178–186, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Ferragina</author>
<author>Ugo Scaiella</author>
</authors>
<title>Tagme: On-the-fly annotation of short text fragments (by wikipedia entities).</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM ’10,</booktitle>
<pages>1625--1628</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="3381" citStr="Ferragina and Scaiella, 2010" startWordPosition="493" endWordPosition="496"> we investigate the problem of finding a subset of DAG-structured topics that are induced by that subset (of objects). This problem arises naturally in several real world applications. For example, consider the problem of identifying appropriate label sets for a collection of articles. Several existing text collection datasets such as 20 Newsgroup1, Reuters-215782 work with a predefined set of topics. We observe that these topic names are highly abstract3 for the articles categorized under them. On the other hand, techniques proposed by systems such as Wikipedia Miner (Milne, 2009) and TAGME (Ferragina and Scaiella, 2010) generate several labels for each article in the dataset that are highly specific to the article. Collating all labels from all articles to create a label 1http://qwone.com/˜jason/20Newsgroups/ 2http://www.daviddlewis.com/resources/ testcollections/reuters21578/ 3Topic Concept is more abstract than the topic Science which is more abstract than the topicChemistry 553 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 553–563, Beijing, China, July 26-31, 2015. c�2015 Association </context>
<context position="13772" citStr="Ferragina and Scaiella, 2010" startWordPosition="2165" endWordPosition="2168"> hierarchy are also classified as relevant to the group of documents. Wei and James (Bi and Kwok, 2011) present a hierarchical multi-label classification algorithm that can be used on both tree and DAG structured hierarchies. They formulate a search for the optimal consistent multi-label as the finding of the best subgraph in a tree/DAG. In our approach, we assume, individual documents are already associated with one or more topics and we find a consistent label set for a group of documents using the DAG structured topic hierarchy. Medelyan et al. (Medelyan et al., 2008) and Ferragina et al. (Ferragina and Scaiella, 2010) detect topics for a document using Wikipedia article names and category names as the topic vocabulary. These systems are able to extract signals from a text document and identify Wikipedia articles and/or categories that optimally match the document and assign those article/category names as topics for the document. When run on a large collection of documents, these approaches generate enormous numbers of topics, a problem our proposed approach addresses. 1.2 Our Contributions While most prior work discussed above focuses on the underlying set of documents, (e.g., by clustering documents), we</context>
<context position="18738" citStr="Ferragina and Scaiella, 2010" startWordPosition="2973" endWordPosition="2977"> in the order of million) DAG structured topic hierarchies in practice. Wikipedia’s category hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K E Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only a set of documents is classified and c</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>Paolo Ferragina and Ugo Scaiella. 2010. Tagme: On-the-fly annotation of short text fragments (by wikipedia entities). In Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM ’10, pages 1625– 1628, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>S Jegelka</author>
<author>J Bilmes</author>
</authors>
<title>Fast semidifferential-based submodular function optimization.</title>
<date>2013</date>
<publisher>ICML.</publisher>
<contexts>
<context position="9555" citStr="Iyer et al., 2013" startWordPosition="1453" endWordPosition="1456">modular if for any element v and sets A C_ B C_ V \ {v}, where V represents the ground set of elements, f (A U {v})−f (A) &gt; f (B U {v})−f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (Kempe et al., 2003; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes, 2012; Lin and Bilmes, 2010). In this paper, we investigate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) cluster the objects and then identify names for the clusters; or b) dynamic</context>
</contexts>
<marker>Iyer, Jegelka, Bilmes, 2013</marker>
<rawString>R. Iyer, S. Jegelka, and J. Bilmes. 2013. Fast semidifferential-based submodular function optimization. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tapas Kanungo</author>
<author>David M Mount</author>
<author>Nathan S Netanyahu</author>
<author>Christine D Piatko</author>
<author>Ruth Silverman</author>
<author>Angela Y Wu</author>
<author>Senior Member</author>
<author>Senior Member</author>
</authors>
<title>An efficient k-means clustering algorithm: Analysis and implementation.</title>
<date>2002</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>24--881</pages>
<contexts>
<context position="19257" citStr="Kanungo et al., 2002" startWordPosition="3059" endWordPosition="3062">also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K E Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only a set of documents is classified and categorized in some way. We next provide a few definitions needed later in the paper. Definition 1: Transitive Cover F): A topic t is said to cover a set of documents F(t), called the transitive cover of the topic t, if for all documents i E F(t), either i is associated directly with topic t or with any of the descendant topics of t in the topic DAG. A natural extension of this definition to a set of topics T is defined as F(T) = UtETF(t). 556 Definition 2: Truncated Transitive Cover (Γα): This is a transitive cove</context>
</contexts>
<marker>Kanungo, Mount, Netanyahu, Piatko, Silverman, Wu, Member, Member, 2002</marker>
<rawString>Tapas Kanungo, David M. Mount, Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman, Angela Y. Wu, Senior Member, and Senior Member. 2002. An efficient k-means clustering algorithm: Analysis and implementation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24:881–892.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kempe</author>
<author>J Kleinberg</author>
<author>E Tardos</author>
</authors>
<title>Maximizing the spread of influence through a social network. In</title>
<date>2003</date>
<booktitle>SIGKDD.</booktitle>
<contexts>
<context position="9480" citStr="Kempe et al., 2003" startWordPosition="1441" endWordPosition="1444">dular mixtures to solve this problem. A set function f (.) is said to be submodular if for any element v and sets A C_ B C_ V \ {v}, where V represents the ground set of elements, f (A U {v})−f (A) &gt; f (B U {v})−f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (Kempe et al., 2003; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes, 2012; Lin and Bilmes, 2010). In this paper, we investigate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) </context>
</contexts>
<marker>Kempe, Kleinberg, Tardos, 2003</marker>
<rawString>D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximizing the spread of influence through a social network. In SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Jeff Bilmes</author>
</authors>
<title>Submodularity for data selection in machine translation.</title>
<date>2014</date>
<contexts>
<context position="24576" citStr="Kirchhoff and Bilmes, 2014" startWordPosition="3936" endWordPosition="3939">nd a set of features U, define mu(S) as the score associated with the set of categories S for feature u ∈ U. The feature set could represent, for example, the documents, in which case mu(S) represents the number of times document u is covered by the set S. U could also represent more complicated features. For example, in the context of Wikipedia disambiguation, U could represent TFIDF features over the documents. Feature based are then defined as f(S) = Eu∈U 0(mu(S)), where 0 is a concave (e.g., the square root) function. This function class has been successfully used in several applications (Kirchhoff and Bilmes, 2014; Wei et al., 2014a; Wei et al., 2014b). 557 3.1.2 Similarity based Functions Similarity functions are defined through a similarity matrix S = {sij}i,jEV . Given categories i, j ∈ V , similarity sij in our case can be defined as sij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j. Facility Location: The facility location function, defined as f(S) = EiEV maxjES sij, is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity ma</context>
</contexts>
<marker>Kirchhoff, Bilmes, 2014</marker>
<rawString>Katrin Kirchhoff and Jeff Bilmes. 2014. Submodularity for data selection in machine translation. October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krause</author>
<author>C Guestrin</author>
</authors>
<title>Near-optimal nonmyopic value of information in graphical models.</title>
<date>2005</date>
<booktitle>In Proceedings of Uncertainity in Artificial Intelligence. UAI.</booktitle>
<contexts>
<context position="9507" citStr="Krause and Guestrin, 2005" startWordPosition="1445" endWordPosition="1448">lve this problem. A set function f (.) is said to be submodular if for any element v and sets A C_ B C_ V \ {v}, where V represents the ground set of elements, f (A U {v})−f (A) &gt; f (B U {v})−f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (Kempe et al., 2003; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes, 2012; Lin and Bilmes, 2010). In this paper, we investigate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) cluster the objects and the</context>
</contexts>
<marker>Krause, Guestrin, 2005</marker>
<rawString>A. Krause and C. Guestrin. 2005. Near-optimal nonmyopic value of information in graphical models. In Proceedings of Uncertainity in Artificial Intelligence. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: Dag-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning, ICML ’06,</booktitle>
<pages>577--584</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11639" citStr="Li and McCallum, 2006" startWordPosition="1789" endWordPosition="1792">, where topics are joined together in a hierarchy by using the nested Chinese restaurant process. Nonparametric extensions of LDA include the Hierarchical Dirichlet Process (Teh et al., 2006) mixture model, which allows the number of topics to be unbounded and learnt from data and the Nested Chinese Restaurant Process which allows topics to be arranged in a hierarchy whose structure is learnt from data. In each of these approaches, unlike our proposed approach, an existing topic hierarchy is not used, nor is any additional objecttopic information leveraged. The pachinko allocation model (PAM)(Li and McCallum, 2006) captures arbitrary, nested, and possibly sparse correlations between topics using a DAG. The leaves of the DAG represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). PAM learns the probability distributions of words in a topic, subtopics in a topic, and topics in a document. We cannot, however, generate a subset of topics from a large existing topic DAG that can act as summary topics, using PAM. HSLDA (Perotte et al., 2011) introduces a hierarchically supervised LDA model to infer</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: Dag-structured mixture models of topic correlations. In Proceedings of the 23rd International Conference on Machine Learning, ICML ’06, pages 577–584, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="9600" citStr="Lin and Bilmes, 2010" startWordPosition="1461" endWordPosition="1464">_ B C_ V \ {v}, where V represents the ground set of elements, f (A U {v})−f (A) &gt; f (B U {v})−f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (Kempe et al., 2003; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes, 2012; Lin and Bilmes, 2010). In this paper, we investigate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) cluster the objects and then identify names for the clusters; or b) dynamically identify topics (including hierarchical)</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>H. Lin and J. Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In The 49th Meeting of the Assoc. for Comp. Ling. Human Lang. Technologies (ACL/HLT-2011),</booktitle>
<location>Portland, OR,</location>
<contexts>
<context position="25379" citStr="Lin and Bilmes, 2011" startWordPosition="4082" endWordPosition="4085">, similarity sij in our case can be defined as sij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j. Facility Location: The facility location function, defined as f(S) = EiEV maxjES sij, is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity matrix may be used to express a form of coverage of a set S but that is then penalized with a redundancy term, as in the following difference: f(S) = EiEV,jES sij − λ EiES EjES, si,j (Lin and Bilmes, 2011)). Here λ ∈ [0, 1]. This function is submodular, but is not in general monotone, and has been used in document summarization (Lin and Bilmes, 2011), as a dispersion function (Borodin et al., 2012), and in image summarization (Tschiatschek et al., 2014). 3.1.3 Quality Control (QC) Functions QC functions ensure a quality criteria is met by a set S of topics. We define the quality score of the set S as Fq (S) = EsES fq (s), where fq (s) is the quality score of topic s for quality q. Therefore, Fq (S) is a modular function in S. We investigate three types of quality control functions: Topic Specif</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In The 49th Meeting of the Assoc. for Comp. Ling. Human Lang. Technologies (ACL/HLT-2011), Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>Learning mixtures of submodular shells with application to document summarization.</title>
<date>2012</date>
<booktitle>In Conference on Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>479490</pages>
<contexts>
<context position="9577" citStr="Lin and Bilmes, 2012" startWordPosition="1457" endWordPosition="1460">element v and sets A C_ B C_ V \ {v}, where V represents the ground set of elements, f (A U {v})−f (A) &gt; f (B U {v})−f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (Kempe et al., 2003; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes, 2012; Lin and Bilmes, 2010). In this paper, we investigate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) cluster the objects and then identify names for the clusters; or b) dynamically identify topics (</context>
<context position="31327" citStr="Lin and Bilmes, 2012" startWordPosition="5163" endWordPosition="5166">aries S are separated from competitor summaries by a loss-dependent margin: Fw(S) &gt; Fw(S0) + G(S0), `dS E S, S0 E Y \ S, (3) where L(·) is the loss function, and where Y is a structured output space (for example Y is the set of summaries that satisfy a certain budget B, i.e., Y = {S&apos; ⊆ V : |S&apos; |≤ B}). We assume the loss to be normalized, 0 ≤ L(S&apos;) ≤ 1, ∀S&apos; ⊆ V , to ensure that mixture and loss are calibrated. Equation (3) can be stated as Fw(S) ≥ maxS0EY [Fw(S&apos;) + L(S&apos;)] , ∀S ∈ S which is called loss-augmented inference. We introduce slack variables and minimize the regularized sum of slacks (Lin and Bilmes, 2012): min [max [Fw (S0) + G(S0)] − Fw(S) ≥ J w0,11w111=1 S∈S SPEY + λ2 I1wI122, (4) where the non-negative orthant constraint, w ≥ 0, ensures that the final mixture is submodular. Note a 2-norm regularizer is used on top of a 1-norm constraint kwk1 = 1 which we interpret as a prior to encourage higher entropy, and thus more diverse mixture distributions. Tractability depends on the choice of the loss function. The parameters w are learnt using stochastic gradient descent as in (Tschiatschek et al., 2014). 3.3 Loss Functions A natural choice of loss functions for our case can be derived from cluste</context>
</contexts>
<marker>Lin, Bilmes, 2012</marker>
<rawString>H. Lin and J. Bilmes. 2012. Learning mixtures of submodular shells with application to document summarization. In Conference on Uncertainty in Artificial Intelligence (UAI), page 479490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
</authors>
<title>Submodularity in Natural Language Processing: Algorithms and Applications.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington, Dept. of EE.</institution>
<contexts>
<context position="14963" citStr="Lin, 2012" startWordPosition="2350" endWordPosition="2351">ring documents), we focus directly on the topics. In particular, we formulate the problem as subset selection on the set of topics within a DAG while simultaneously considering the documents to be categorized. Our method can scale to the colossal size of the DAG (1 million topics and 3 million correlation links between topics in Wikipedia). Moreover, our approach can 555 naturally incorporate outputs from many of the aforementioned algorithms. Our approach is based on submodular maximization and mixture learning, which has been successfully used in applications such as document summarization (Lin, 2012) and image summarization (Tschiatschek et al., 2014), but has never been applied to topic identification tasks or, more generally, DAG summarization. We introduce a family of submodular functions to identify an appropriate set of topics from a DAG structured hierarchy of topics for a group of documents. We characterize this topic appropriateness through a set of desirable properties such as coverage, diversity, specificity, clarity, and relevance. Each of the submodular function components we consider are monotone, thereby ensuring a near optimal performance obtainable via a simple greedy algo</context>
</contexts>
<marker>Lin, 2012</marker>
<rawString>Hui Lin. 2012. Submodularity in Natural Language Processing: Algorithms and Applications. Ph.D. thesis, University of Washington, Dept. of EE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min ling Zhang</author>
<author>Zhi hua Zhou</author>
</authors>
<title>Ml-knn: A lazy learning approach to multi-label learning.</title>
<date>2007</date>
<journal>PATTERN RECOGNITION,</journal>
<pages>40--2007</pages>
<marker>Zhang, Zhou, 2007</marker>
<rawString>Min ling Zhang and Zhi hua Zhou. 2007. Ml-knn: A lazy learning approach to multi-label learning. PATTERN RECOGNITION, 40:2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arun S Maiya</author>
<author>John P Thompson</author>
<author>Francisco LoaizaLemos</author>
<author>Robert M Rolfe</author>
</authors>
<title>Exploratory analysis of highly heterogeneous document collections.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’13,</booktitle>
<pages>1375--1383</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10702" citStr="Maiya et al., 2013" startWordPosition="1643" endWordPosition="1646">ster the objects and then identify names for the clusters; or b) dynamically identify topics (including hierarchical) for a set of objects. LDA (Blei et al., 2003) clusters the documents and simultaneously produces a set of topics into which the documents are clustered. In LDA, each document may be viewed as a mixture of various topics and the topic distribution is assumed to have a Dirichlet prior. LDA associates a group of high probability words to each identified topic. A name can be assigned to a topic by manually inspecting the words or using additional algorithms like (Mei et al., 2007; Maiya et al., 2013). LDA does not make use of existing topic hierarchies and correlation between topics. The Correlated Topic Model (Blei and Lafferty, 2006) induces a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (Blei et al., 2004), where topics are joined together in a hierarchy by using the nested Chinese restaurant process. Nonparametric extensions of LDA include the Hierarchical Dirichlet Process (Teh et al., 2006) mixture model, which allows the number of topics to be unbounded and learnt from data and the</context>
</contexts>
<marker>Maiya, Thompson, LoaizaLemos, Rolfe, 2013</marker>
<rawString>Arun S. Maiya, John P. Thompson, Francisco LoaizaLemos, and Robert M. Rolfe. 2013. Exploratory analysis of highly heterogeneous document collections. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’13, pages 1375–1383, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>Ian H Witten</author>
<author>David Milne</author>
</authors>
<title>Topic indexing with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the Wikipedia and AI workshop at AAAI-08.</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="13720" citStr="Medelyan et al., 2008" startWordPosition="2157" endWordPosition="2160">t many labels near the top level of the label hierarchy are also classified as relevant to the group of documents. Wei and James (Bi and Kwok, 2011) present a hierarchical multi-label classification algorithm that can be used on both tree and DAG structured hierarchies. They formulate a search for the optimal consistent multi-label as the finding of the best subgraph in a tree/DAG. In our approach, we assume, individual documents are already associated with one or more topics and we find a consistent label set for a group of documents using the DAG structured topic hierarchy. Medelyan et al. (Medelyan et al., 2008) and Ferragina et al. (Ferragina and Scaiella, 2010) detect topics for a document using Wikipedia article names and category names as the topic vocabulary. These systems are able to extract signals from a text document and identify Wikipedia articles and/or categories that optimally match the document and assign those article/category names as topics for the document. When run on a large collection of documents, these approaches generate enormous numbers of topics, a problem our proposed approach addresses. 1.2 Our Contributions While most prior work discussed above focuses on the underlying s</context>
</contexts>
<marker>Medelyan, Witten, Milne, 2008</marker>
<rawString>Olena Medelyan, Ian H. Witten, and David Milne. 2008. Topic indexing with Wikipedia. In Proceedings of the Wikipedia and AI workshop at AAAI-08. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07,</booktitle>
<pages>490--499</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10681" citStr="Mei et al., 2007" startWordPosition="1639" endWordPosition="1642">wing types: a) cluster the objects and then identify names for the clusters; or b) dynamically identify topics (including hierarchical) for a set of objects. LDA (Blei et al., 2003) clusters the documents and simultaneously produces a set of topics into which the documents are clustered. In LDA, each document may be viewed as a mixture of various topics and the topic distribution is assumed to have a Dirichlet prior. LDA associates a group of high probability words to each identified topic. A name can be assigned to a topic by manually inspecting the words or using additional algorithms like (Mei et al., 2007; Maiya et al., 2013). LDA does not make use of existing topic hierarchies and correlation between topics. The Correlated Topic Model (Blei and Lafferty, 2006) induces a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (Blei et al., 2004), where topics are joined together in a hierarchy by using the nested Chinese restaurant process. Nonparametric extensions of LDA include the Hierarchical Dirichlet Process (Teh et al., 2006) mixture model, which allows the number of topics to be unbounded and lea</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07, pages 490–499, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: Linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM ’07,</booktitle>
<pages>233--242</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18823" citStr="Mihalcea and Csomai, 2007" startWordPosition="2988" endWordPosition="2991">ory hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K E Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only a set of documents is classified and categorized in some way. We next provide a few definitions needed later in the paper. </context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: Linking documents to encyclopedic knowledge. In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM ’07, pages 233–242, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
</authors>
<title>An open-source toolkit for mining wikipedia. In</title>
<date>2009</date>
<booktitle>In Proc. New Zealand Computer Science Research Student Conf,</booktitle>
<pages>page</pages>
<contexts>
<context position="3340" citStr="Milne, 2009" startWordPosition="489" endWordPosition="490">and a subset of objects, we investigate the problem of finding a subset of DAG-structured topics that are induced by that subset (of objects). This problem arises naturally in several real world applications. For example, consider the problem of identifying appropriate label sets for a collection of articles. Several existing text collection datasets such as 20 Newsgroup1, Reuters-215782 work with a predefined set of topics. We observe that these topic names are highly abstract3 for the articles categorized under them. On the other hand, techniques proposed by systems such as Wikipedia Miner (Milne, 2009) and TAGME (Ferragina and Scaiella, 2010) generate several labels for each article in the dataset that are highly specific to the article. Collating all labels from all articles to create a label 1http://qwone.com/˜jason/20Newsgroups/ 2http://www.daviddlewis.com/resources/ testcollections/reuters21578/ 3Topic Concept is more abstract than the topic Science which is more abstract than the topicChemistry 553 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 553–563, Beijing, Chi</context>
<context position="18700" citStr="Milne, 2009" startWordPosition="2970" endWordPosition="2971">d massive (i.e., size in the order of million) DAG structured topic hierarchies in practice. Wikipedia’s category hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K E Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only </context>
</contexts>
<marker>Milne, 2009</marker>
<rawString>David Milne. 2009. An open-source toolkit for mining wikipedia. In In Proc. New Zealand Computer Science Research Student Conf, page 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Minoux</author>
</authors>
<title>Accelerated greedy algorithms for maximizing submodular set functions.</title>
<date>1978</date>
<booktitle>Optimization Techniques,</booktitle>
<volume>7</volume>
<pages>234--243</pages>
<editor>In J. Stoer, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg, Berlin/Heidelberg.</location>
<contexts>
<context position="32931" citStr="Minoux, 1978" startWordPosition="5445" endWordPosition="5446">S, T) = 1 − k EsES maxtET jΓ(sjur(t)| where k = |S |= |T |is the number of topics. When the clustering produced by the inferred and the true topics are similar, Jaccard loss is 0. When they are completely dissimilar, the loss is maximum, i.e., 1. Jaccard loss is a modular function. 3.4 Inference Algorithm: Greedy Having learnt the weights for the mixture components, the resulting function Fw(S) = Emi=1 wifi(S) is a submodular function. In the case when the individual components are themselves monotone (all our functions in fact are), Fw(S) can be optimized by the accelerated greedy algorithm (Minoux, 1978). Thanks to submodularity, we can obtain near optimal solutions very efficiently. In case the functions are all monotone submodular, we can guarantee that the solution is within 1 − 1/e factor from the optimal solution. 4 Experimental Results To validate our approach, we make use of Wikipedia category structure as a topic DAG and apply our technique to the task of automatic generation of Wikipedia disambiguation pages. We pre-processed the category graph to eliminate the cycles in order to make it a DAG. Each Wikipedia disambiguation page is manually created by Wikipedia editors by grouping a </context>
</contexts>
<marker>Minoux, 1978</marker>
<rawString>Michel Minoux. 1978. Accelerated greedy algorithms for maximizing submodular set functions. In J. Stoer, editor, Optimization Techniques, volume 7 of Lecture Notes in Control and Information Sciences, chapter 27, pages 234–243. Springer Berlin Heidelberg, Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mukund Narasimhan</author>
<author>Jeff Bilmes</author>
</authors>
<title>PAClearning bounded tree-width graphical models.</title>
<date>2004</date>
<booktitle>In Uncertainty in Artificial Intelligence: Proceedings of the Twentieth Conference (UAI-2004).</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<contexts>
<context position="9536" citStr="Narasimhan and Bilmes, 2004" startWordPosition="1449" endWordPosition="1452">ction f (.) is said to be submodular if for any element v and sets A C_ B C_ V \ {v}, where V represents the ground set of elements, f (A U {v})−f (A) &gt; f (B U {v})−f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (Kempe et al., 2003; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes, 2012; Lin and Bilmes, 2010). In this paper, we investigate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) cluster the objects and then identify names for the clus</context>
</contexts>
<marker>Narasimhan, Bilmes, 2004</marker>
<rawString>Mukund Narasimhan and Jeff Bilmes. 2004. PAClearning bounded tree-width graphical models. In Uncertainty in Artificial Intelligence: Proceedings of the Twentieth Conference (UAI-2004). Morgan Kaufmann Publishers, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George L Nemhauser</author>
<author>Laurence A Wolsey</author>
<author>Marshall L Fisher</author>
</authors>
<title>An analysis of approximations for maximizing submodular set functionsi.</title>
<date>1978</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="17191" citStr="Nemhauser et al., 1978" startWordPosition="2716" endWordPosition="2719">hat are associated with one or more of these topics. The middle portion of Figure 1 depicts a topic hierarchy with associated documents. The association links between the documents and topics can be hard or soft. In case of a hard link, a document is attached to a set of topics. Examples include multi-labeled documents. In case of a soft link, a document is associated with a topic with some degree of confidence (or probability). Furthermore, if a document is attached to a topic t, we assume that all the ancestor topics of t are also relevant for that document. This 7A simple greedy algorithm (Nemhauser et al., 1978) obtains a 1 − 1/e approximation guarantee for monotone submodular function maximization assumption has been employed in earlier works (Blei et al., 2004; Bi and Kwok, 2011; Rousu et al., 2006) as well. Given a budget of K, our objective is to choose a set of K topics from V , which best describe the documents in D. The notion of best describing topics is characterized through a set of desirable properties - coverage, diversity, specificity, clarity, relevance and fidelity - that K topics have to satisfy. The submodular functions that we introduce in the next section ensure these properties ar</context>
</contexts>
<marker>Nemhauser, Wolsey, Fisher, 1978</marker>
<rawString>George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. 1978. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adler J Perotte</author>
<author>Frank Wood</author>
<author>Noemie Elhadad</author>
<author>Nicholas Bartlett</author>
</authors>
<title>Hierarchically supervised latent dirichlet allocation. In</title>
<date>2011</date>
<pages>2609--2617</pages>
<editor>John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, NIPS,</editor>
<contexts>
<context position="12181" citStr="Perotte et al., 2011" startWordPosition="1877" endWordPosition="1880">formation leveraged. The pachinko allocation model (PAM)(Li and McCallum, 2006) captures arbitrary, nested, and possibly sparse correlations between topics using a DAG. The leaves of the DAG represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). PAM learns the probability distributions of words in a topic, subtopics in a topic, and topics in a document. We cannot, however, generate a subset of topics from a large existing topic DAG that can act as summary topics, using PAM. HSLDA (Perotte et al., 2011) introduces a hierarchically supervised LDA model to infer hierarchical labels for a document. It assumes an existing label hierarchy in the form of a tree. The model infers one or more labels such that, if a label l is inferred as relevant to a document, then all the labels from l to the root of the tree are also inferred as relevant to the document. Our approach differs from HSLDA since: (1) we use the label hierarchy to infer a set of labels for a group of documents; (2) we do not enforce the label hierarchy to be a tree as it can be a DAG; and (3) generalizing HSLDA to use a DAG structured</context>
</contexts>
<marker>Perotte, Wood, Elhadad, Bartlett, 2011</marker>
<rawString>Adler J. Perotte, Frank Wood, Noemie Elhadad, and Nicholas Bartlett. 2011. Hierarchically supervised latent dirichlet allocation. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, NIPS, pages 2609–2617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juho Rousu</author>
<author>Craig Saunders</author>
<author>Sndor Szedmk</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Kernel-based learning of hierarchical multilabel classification models.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1601</pages>
<contexts>
<context position="2580" citStr="Rousu et al., 2006" startWordPosition="368" endWordPosition="371">ations involve hierarchy based categorization of topics for a set of objects. Objects could be, e.g., a set of documents for text classification, a set of genes in functional genomics, or a set of images in computer vision. One can often define a natural topic hierarchy to categorize these objects. For example, in text and image classification problems, each document or image is assigned a hierarchy of labels — a baseball page is assigned the labels “baseball” and “sports.” Moreover, many of these applications, naturally have an existing topic hierarchy generated on the entire set of objects (Rousu et al., 2006; Barutcuoglu et al., 2006; ling Zhang and hua Zhou, 2007; Silla and Freitas, 2011; Tsoumakas et al., 2010). Given a DAG-structured topic hierarchy and a subset of objects, we investigate the problem of finding a subset of DAG-structured topics that are induced by that subset (of objects). This problem arises naturally in several real world applications. For example, consider the problem of identifying appropriate label sets for a collection of articles. Several existing text collection datasets such as 20 Newsgroup1, Reuters-215782 work with a predefined set of topics. We observe that these t</context>
<context position="17384" citStr="Rousu et al., 2006" startWordPosition="2749" endWordPosition="2752">e hard or soft. In case of a hard link, a document is attached to a set of topics. Examples include multi-labeled documents. In case of a soft link, a document is associated with a topic with some degree of confidence (or probability). Furthermore, if a document is attached to a topic t, we assume that all the ancestor topics of t are also relevant for that document. This 7A simple greedy algorithm (Nemhauser et al., 1978) obtains a 1 − 1/e approximation guarantee for monotone submodular function maximization assumption has been employed in earlier works (Blei et al., 2004; Bi and Kwok, 2011; Rousu et al., 2006) as well. Given a budget of K, our objective is to choose a set of K topics from V , which best describe the documents in D. The notion of best describing topics is characterized through a set of desirable properties - coverage, diversity, specificity, clarity, relevance and fidelity - that K topics have to satisfy. The submodular functions that we introduce in the next section ensure these properties are satisfied. Formally, we solve the following discrete optimization problem: � S* E argmax wifi(S) (1) ScV :|S|&lt;K i where, fi are monotone submodular mixture components and wi ≥ 0 are the weigh</context>
</contexts>
<marker>Rousu, Saunders, Szedmk, Shawe-Taylor, 2006</marker>
<rawString>Juho Rousu, Craig Saunders, Sndor Szedmk, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. Journal of Machine Learning Research, 7:1601–1626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CarlosN Silla</author>
<author>AlexA Freitas</author>
</authors>
<title>A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery,</title>
<date>2011</date>
<pages>22--1</pages>
<contexts>
<context position="2662" citStr="Silla and Freitas, 2011" startWordPosition="382" endWordPosition="385"> Objects could be, e.g., a set of documents for text classification, a set of genes in functional genomics, or a set of images in computer vision. One can often define a natural topic hierarchy to categorize these objects. For example, in text and image classification problems, each document or image is assigned a hierarchy of labels — a baseball page is assigned the labels “baseball” and “sports.” Moreover, many of these applications, naturally have an existing topic hierarchy generated on the entire set of objects (Rousu et al., 2006; Barutcuoglu et al., 2006; ling Zhang and hua Zhou, 2007; Silla and Freitas, 2011; Tsoumakas et al., 2010). Given a DAG-structured topic hierarchy and a subset of objects, we investigate the problem of finding a subset of DAG-structured topics that are induced by that subset (of objects). This problem arises naturally in several real world applications. For example, consider the problem of identifying appropriate label sets for a collection of articles. Several existing text collection datasets such as 20 Newsgroup1, Reuters-215782 work with a predefined set of topics. We observe that these topic names are highly abstract3 for the articles categorized under them. On the ot</context>
</contexts>
<marker>Silla, Freitas, 2011</marker>
<rawString>Jr. Silla, CarlosN. and AlexA. Freitas. 2011. A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery, 22(1-2):31–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web, WWW ’07,</booktitle>
<pages>697--706</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18478" citStr="Suchanek et al., 2007" startWordPosition="2935" endWordPosition="2938">on problem: � S* E argmax wifi(S) (1) ScV :|S|&lt;K i where, fi are monotone submodular mixture components and wi ≥ 0 are the weights associated with those mixture components. Set S* is the summary topics scored best. It is easy to find massive (i.e., size in the order of million) DAG structured topic hierarchies in practice. Wikipedia’s category hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierar</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web, WWW ’07, pages 697– 706, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="11208" citStr="Teh et al., 2006" startWordPosition="1720" endWordPosition="1723">pic by manually inspecting the words or using additional algorithms like (Mei et al., 2007; Maiya et al., 2013). LDA does not make use of existing topic hierarchies and correlation between topics. The Correlated Topic Model (Blei and Lafferty, 2006) induces a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (Blei et al., 2004), where topics are joined together in a hierarchy by using the nested Chinese restaurant process. Nonparametric extensions of LDA include the Hierarchical Dirichlet Process (Teh et al., 2006) mixture model, which allows the number of topics to be unbounded and learnt from data and the Nested Chinese Restaurant Process which allows topics to be arranged in a hierarchy whose structure is learnt from data. In each of these approaches, unlike our proposed approach, an existing topic hierarchy is not used, nor is any additional objecttopic information leveraged. The pachinko allocation model (PAM)(Li and McCallum, 2006) captures arbitrary, nested, and possibly sparse correlations between topics using a DAG. The leaves of the DAG represent individual words in the vocabulary, while each </context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Tschiatschek</author>
<author>Rishabh Iyer</author>
<author>Hoachen Wei</author>
<author>Jeff Bilmes</author>
</authors>
<title>Learning Mixtures of Submodular Functions for Image Collection Summarization.</title>
<date>2014</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="15015" citStr="Tschiatschek et al., 2014" startWordPosition="2355" endWordPosition="2358">n the topics. In particular, we formulate the problem as subset selection on the set of topics within a DAG while simultaneously considering the documents to be categorized. Our method can scale to the colossal size of the DAG (1 million topics and 3 million correlation links between topics in Wikipedia). Moreover, our approach can 555 naturally incorporate outputs from many of the aforementioned algorithms. Our approach is based on submodular maximization and mixture learning, which has been successfully used in applications such as document summarization (Lin, 2012) and image summarization (Tschiatschek et al., 2014), but has never been applied to topic identification tasks or, more generally, DAG summarization. We introduce a family of submodular functions to identify an appropriate set of topics from a DAG structured hierarchy of topics for a group of documents. We characterize this topic appropriateness through a set of desirable properties such as coverage, diversity, specificity, clarity, and relevance. Each of the submodular function components we consider are monotone, thereby ensuring a near optimal performance obtainable via a simple greedy algorithm for optimization.7. We also show how our techn</context>
<context position="25114" citStr="Tschiatschek et al., 2014" startWordPosition="4030" endWordPosition="4033">on class has been successfully used in several applications (Kirchhoff and Bilmes, 2014; Wei et al., 2014a; Wei et al., 2014b). 557 3.1.2 Similarity based Functions Similarity functions are defined through a similarity matrix S = {sij}i,jEV . Given categories i, j ∈ V , similarity sij in our case can be defined as sij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j. Facility Location: The facility location function, defined as f(S) = EiEV maxjES sij, is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity matrix may be used to express a form of coverage of a set S but that is then penalized with a redundancy term, as in the following difference: f(S) = EiEV,jES sij − λ EiES EjES, si,j (Lin and Bilmes, 2011)). Here λ ∈ [0, 1]. This function is submodular, but is not in general monotone, and has been used in document summarization (Lin and Bilmes, 2011), as a dispersion function (Borodin et al., 2012), and in image summarization (Tschiatschek et al., 2014). 3.1.3 Quality Control (QC) Functions QC functions ensure a quality criteria is me</context>
<context position="31832" citStr="Tschiatschek et al., 2014" startWordPosition="5250" endWordPosition="5253"> loss-augmented inference. We introduce slack variables and minimize the regularized sum of slacks (Lin and Bilmes, 2012): min [max [Fw (S0) + G(S0)] − Fw(S) ≥ J w0,11w111=1 S∈S SPEY + λ2 I1wI122, (4) where the non-negative orthant constraint, w ≥ 0, ensures that the final mixture is submodular. Note a 2-norm regularizer is used on top of a 1-norm constraint kwk1 = 1 which we interpret as a prior to encourage higher entropy, and thus more diverse mixture distributions. Tractability depends on the choice of the loss function. The parameters w are learnt using stochastic gradient descent as in (Tschiatschek et al., 2014). 3.3 Loss Functions A natural choice of loss functions for our case can be derived from cluster evaluation metrics. Every inferred topic s induces a subset of documents, namely the transitive cover Γ (s) of s. We compare these clusters with the clusters induced from the true topics in the training set and compute the loss. In this paper, we use the Jaccard Index (JI) as a loss function. Let S be the inferred topics and T be the true topics. The Jaccard loss is defined as Ljaccard(S, T) = 1 − k EsES maxtET jΓ(sjur(t)| where k = |S |= |T |is the number of topics. When the clustering produced by</context>
</contexts>
<marker>Tschiatschek, Iyer, Wei, Bilmes, 2014</marker>
<rawString>Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei, and Jeff Bilmes. 2014. Learning Mixtures of Submodular Functions for Image Collection Summarization. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
</authors>
<title>Ioannis Katakis, and Ioannis Vlahavas.</title>
<date>2010</date>
<booktitle>In Oded Maimon and Lior Rokach, editors, Data Mining and Knowledge Discovery Handbook,</booktitle>
<pages>667--685</pages>
<publisher>Springer US.</publisher>
<marker>Tsoumakas, 2010</marker>
<rawString>Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2010. Mining multi-label data. In Oded Maimon and Lior Rokach, editors, Data Mining and Knowledge Discovery Handbook, pages 667–685. Springer US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wei</author>
<author>Rishabh Iyer</author>
<author>Jeff Bilmes</author>
</authors>
<title>Fast multi-stage submodular maximization.</title>
<date>2014</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="24594" citStr="Wei et al., 2014" startWordPosition="3940" endWordPosition="3943">ne mu(S) as the score associated with the set of categories S for feature u ∈ U. The feature set could represent, for example, the documents, in which case mu(S) represents the number of times document u is covered by the set S. U could also represent more complicated features. For example, in the context of Wikipedia disambiguation, U could represent TFIDF features over the documents. Feature based are then defined as f(S) = Eu∈U 0(mu(S)), where 0 is a concave (e.g., the square root) function. This function class has been successfully used in several applications (Kirchhoff and Bilmes, 2014; Wei et al., 2014a; Wei et al., 2014b). 557 3.1.2 Similarity based Functions Similarity functions are defined through a similarity matrix S = {sij}i,jEV . Given categories i, j ∈ V , similarity sij in our case can be defined as sij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j. Facility Location: The facility location function, defined as f(S) = EiEV maxjES sij, is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity matrix may be used t</context>
</contexts>
<marker>Wei, Iyer, Bilmes, 2014</marker>
<rawString>Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2014a. Fast multi-stage submodular maximization. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wei</author>
<author>Yuzong Liu</author>
<author>Katrin Kirchhoff</author>
<author>Chris Bartels</author>
<author>Jeff Bilmes</author>
</authors>
<title>Submodular subset selection for large-scale speech training data.</title>
<date>2014</date>
<booktitle>Proceedings of ICASSP,</booktitle>
<location>Florence, Italy.</location>
<contexts>
<context position="24594" citStr="Wei et al., 2014" startWordPosition="3940" endWordPosition="3943">ne mu(S) as the score associated with the set of categories S for feature u ∈ U. The feature set could represent, for example, the documents, in which case mu(S) represents the number of times document u is covered by the set S. U could also represent more complicated features. For example, in the context of Wikipedia disambiguation, U could represent TFIDF features over the documents. Feature based are then defined as f(S) = Eu∈U 0(mu(S)), where 0 is a concave (e.g., the square root) function. This function class has been successfully used in several applications (Kirchhoff and Bilmes, 2014; Wei et al., 2014a; Wei et al., 2014b). 557 3.1.2 Similarity based Functions Similarity functions are defined through a similarity matrix S = {sij}i,jEV . Given categories i, j ∈ V , similarity sij in our case can be defined as sij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j. Facility Location: The facility location function, defined as f(S) = EiEV maxjES sij, is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity matrix may be used t</context>
</contexts>
<marker>Wei, Liu, Kirchhoff, Bartels, Bilmes, 2014</marker>
<rawString>Kai Wei, Yuzong Liu, Katrin Kirchhoff, Chris Bartels, and Jeff Bilmes. 2014b. Submodular subset selection for large-scale speech training data. Proceedings of ICASSP, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Analysis of the wikipedia category graph for nlp applications.</title>
<date>2007</date>
<booktitle>In Proceedings of the TextGraphs-2 Workshop (NAACL-HLT),</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester,</location>
<contexts>
<context position="18342" citStr="Zesch and Gurevych, 2007" startWordPosition="2912" endWordPosition="2915">functions that we introduce in the next section ensure these properties are satisfied. Formally, we solve the following discrete optimization problem: � S* E argmax wifi(S) (1) ScV :|S|&lt;K i where, fi are monotone submodular mixture components and wi ≥ 0 are the weights associated with those mixture components. Set S* is the summary topics scored best. It is easy to find massive (i.e., size in the order of million) DAG structured topic hierarchies in practice. Wikipedia’s category hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard </context>
</contexts>
<marker>Zesch, Gurevych, 2007</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In Proceedings of the TextGraphs-2 Workshop (NAACL-HLT), pages 1–8, Rochester, April. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>