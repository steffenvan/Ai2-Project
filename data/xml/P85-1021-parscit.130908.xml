<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000047">
<note confidence="0.6498724">
PARSING HEAD-DRIVEN PHRASE STRUCTURE GRAMMAR
Derek Proudian and Carl Pollard
Hewlett-Packard Laboratories
1501 Page Mill Road
Palo Alto, CA. 94303, USA
</note>
<sectionHeader confidence="0.874713" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999415055555556">
The Head-driven Phrase Structure Grammar project
(HPSG) is an English language database query system
under development at Hewlett-Packard Laboratories.
Unlike other product-oriented efforts in the natural lan-
guage understanding field, the HPSG system was de-
signed and implemented by linguists on the basis of
recent theoretical developments. But, unlike other im-
plementations of linguistic theories, this system is not a
toy, as it deals with a variety of practical problems not
covered in the theoretical literature. We believe that
this makes the HPSG system unique in its combination
of linguistic theory and practical application.
The HPSG system differs from its predecessor
GPSG, reported on at the 1982 ACL meeting (Gawron
et al. 19821), in four significant respects: syntax, lexi-
cal representation, parsing, and semantics. The paper
focuses on parsing issues, but also gives a synopsis of
the underlying syntactic formalism.
</bodyText>
<sectionHeader confidence="0.992939" genericHeader="keywords">
1 Syntax
</sectionHeader>
<bodyText confidence="0.994975640625">
HPSG is a lexically based theory of phrase struc-
ture, so called because of the central role played by
grammatical heads and their associated complements.&apos;
Roughly speaking, heads are linguistic forms (words
and phrases) that exert syntactic and semantic restric-
tions on the phrases, called complements, that charac-
teristically combine with them to form larger phrases.
Verbs are the heads of verb phrases (and sentences),
nouns are the heads of noun phrases, and so forth.
As in most current syntactic theories, categories
are represented as complexes of feature specifications.
But the IIPSG treatment of lexical subcategorization
obviates the need in the theory of categories for the no-
tion of bar-level (in the sense of X-bar theory, prevalent
in much current linguistic research). In addition, the
augmentation of the system of categories with stack-
valued features - features whose values are sequences
of categories - unifies the theory of lexical subcatego-
rizatiott with the theory of binding phenomena. By
binding phenomena we mean essentially non-clause-
bounded dependencies, such as those involving dislo-
cated constituents, relative and interrogative pronouns,
and reflexive and reciprocal pronouns 114
iliPSG is a refinement and extension of the closely relateu Generalised Phrase
Structure Grammar T. The details of the theory of IIPSG are set forth in Ill!.
More precisely, the subcategorization of a head is
encoded as the value of a stack-valued feature called
&amp;quot;SUBCAT&amp;quot;. For example, the SUBCAT value of the
verb persuade is the sequence of three categories (VP,
NP, NEI, corresponding to the grammatical relations
(GR&apos;s): controlled complement, direct object, and sub-
ject respectively. We are adopting a modified version of
Dowty&apos;s 119821 terminology for GR&apos;s, where subject is
last, direct object second-to-last, etc. For semantic rea-
sons we call the GR following a controlled complement
the controller.
One of the key differences between HPSG and its
preclecesor GPSG is the massive relocation of linguistic
information from phrase structure rules into the lexi-
con [51. This wholesale lexicalization of linguistic infor-
mation in HPSG results in a drastic reduction in the
number of phrase structure rules. Since rules no longer
handle subcategorization, their sole remaining function
is to encode a small number of language-specific prin-
ciples for projecting from lexical entries I surface con-
stituent order.
The schematic nature of the grammar rules allows
the system to parse a large fragment of English with
only a small number of rules (the system currently uses
sixteen), since each rule can be used in many different
situations. The constituents of each rule are sparsely
annotated with features, but are fleshed out when taken
together with constituents looked for and constituents
found.
For example the sentence The manager works can
be parsed using the single rule RI below. The rule
is applied to build the noun phrase The manager by
identifying the head H with the lexical element man-
ager and the complement CI with the lexical element
the. The entire sentence is built by identifying the H
with works and the Cl with the noun phrase described
above. Thus the single rule 121 functions as both the S
NP VI&apos;, and NP Del IV rules of familiar context
free grammars.
</bodyText>
<figureCaption confidence="0.989038">
RI.. x -&gt; cl h( (CONTROL INTRANS) ] a*
Figure 1. A Grammar Rule.
</figureCaption>
<page confidence="0.992729">
167
</page>
<subsectionHeader confidence="0.951475">
Feature Passing
</subsectionHeader>
<bodyText confidence="0.98878675">
The theory of HPSG embodies a number of sub-
stantive hypotheses about universal grammatical prin-
ciples. Such principles as the Head Feature Princi-
ple, the Binding Inheritance Principle, and the Con-
trol Agreement Principle, require that certain syntac-
tic features specified on daughters in syntactic trees are
inherited by the mothers. Highly abstract phrase struc-
ture rules thus give rise to fully specified grammatical
structures in a recursive process driven by syntactic in-
formation encoded on lexical heads. Thus HPSG, un-
like similar &amp;quot;unification-based&amp;quot; syntactic theories, em-
bodies a strong hypothesis about the flow of relevant
information in the derivation of complex structures.
Unification
Another important difference between HPSG and
other unification based syntactic theories concerns the
form of the expressions which are actually unified.
In HPSG, the structures which get unified are (with
limited exceptions to be discussed below) not general
graph structures as in Lexical Functional Grammar
or Functional Unification Grammar 1101, but rather flat
atomic valued feature matrices, such as those shown
below.
[(CONTROL 0 INTRANS) (MAJ N A)
</bodyText>
<figure confidence="0.5016275">
(AGR 3RDSG) (PRD MINUS) (TOP MINUS)]
[(CONTROL 0) (MAJ N V) (INV PLUS)]
</figure>
<figureCaption confidence="0.996365">
Figure 2. Two feature matrices.
</figureCaption>
<bodyText confidence="0.999902444444444">
In the implementation of EIPSC; we have been able
to use this restriction on the form of feature matrices
to good advantage. Since for any given version of the
system the range of atomic features and feature values
is fixed, we are able to represent flat feature matrices,
such as the ones above, as vectors of integers, where
each cell in the vector represents a feature, and he in-
teger in each cell represents a disjunction of the possible
values for that feature.
</bodyText>
<figure confidence="0.488086333333333">
CON MAJ AGR PRD INV TOP ...
117110121113111
1 1 1 12 1 7 1 3 ! 1 1 3
</figure>
<figureCaption confidence="0.98802">
Figure 3: Two transduced feature matrices.
</figureCaption>
<bodyText confidence="0.999759538461538">
For example, if the possible values of the MAJ fea-
ture are N, V, A, and 13 then we can uniquely represent
any combination of these features with an integer in
the range 0..15. This is accomplished simply by assign-
ing each possible value an index which is an integral
power of 2 in this range and then adding up the indices
so derived for each disjunction of values encountered.
Unification in such cases is thus reduced to the &amp;quot;logical
and&amp;quot; of the integers in each cell of the vector represent-
ing the feature matrix. In this way unification of these
flat structures can be done in constant time, and since
&amp;quot;logical and&amp;quot; is generally a single machine instruction
the overhead is very low.
</bodyText>
<table confidence="0.9876126">
1 1 NV AP
to 1 1 (0 = 10 = (MAJ N A)
11 11 10 1 01= 12 (MAJ N V)
Unification
1 1 1Â° 1 0 I 0 1= 8= (MAJ N)
</table>
<figureCaption confidence="0.998844">
Figure 4: Closeup of the MAJ feature.
</figureCaption>
<bodyText confidence="0.999993466666667">
There are, however, certain cases when the values
of features are not atomic, but are instead themselves
feature matrices. The unification of such structures
could, in theory, involve arbitrary recursion on the gen-
eral unification algorithm, and it would seem that we
had not progressed very far from the problem of uni-
fying general graph structures. Happily, the features
for which this property of embedding holds, constitute
a small finite set (basically the so called &amp;quot;binding fea-
tures&amp;quot;). Thus we are able to segregate such features
from the rest, and recurse only when such a &amp;quot;category
valued&amp;quot; feature is present. In practice. therefore, the
time performance of the general unification algorithm
is very good, essentially the same as that of the flat
structure unification algorithm described above.
</bodyText>
<sectionHeader confidence="0.960068" genericHeader="introduction">
2 Parsing
</sectionHeader>
<bodyText confidence="0.999965076923077">
As in the earlier GPSG system, the primary job
of the parser in the HPSG system is to produce a se-
mantics for the input sentence. This is done composi-
tionally as the phrase structure is built, and uses only
locally available information. Thus every constituent
which is built syntactically has a corresponding seman-
tics built for it at the same time, using only information
available in the phrasal subtree which it immediately
dominates. This locality constraint in computing the
semantics for constituents is an essential characteristic
of HPSG. For a more complete description of the se-
mantic treatment used in the HPSG system see Creary
and Pollard 121.
</bodyText>
<subsectionHeader confidence="0.577689">
Head-driven Active Chart Parser
</subsectionHeader>
<bodyText confidence="0.9999472">
A crucial difference between the HPSG system and
its predecessor GPSG is the importance placed on the
head constituent in HPSG. In HPSG it is the head con-
stituent of a rule which carries the subcategorization
information needed to build the other constituents of
</bodyText>
<page confidence="0.99308">
168
</page>
<bodyText confidence="0.998239818181818">
the rule. Thus parsing proceeds head first through the
phrase structure of a sentence, rather than left to right
through the sentence string.
The parser itself is a variation of an active chart
parser [4,9,8,131, modified to permit the construction of
constituents head first, instead of in left-to-right order.
In order to successfully parse &amp;quot;head first&amp;quot;, an edge&apos;
must be augmented to include information about its
span (i.e. its position in the string). This is necessary
because head can appear as a middle constituent of
a rule with other constituents (e.g. complements or
adjuncts) on either side. Thus it is not possible to
record all the requisite boundary information simply
by moving a dot through the rule (as in Earley), or by
keeping track of just those constituents which remain
to be built (as in Winograd). An example should make
this clear.
Suppose as before we are confronted with the task
of parsing the sentence The manager works, and again
we have available the grammar rule RI. Since we are
parsing in a &amp;quot;head first&amp;quot; manner we must match the
H constituent against some substring of the sentence.
But which substring? In more conventional chart pars-
ing algorithms which proceed left to right this is not
a serious problem, since we are always guaranteed to
have an anchor to the left. We simply try building the
leftmost constituent of the rule starting at the leftmost
position of the string, and if this succeeds we try to
build the next leftmost constituent starting at one po-
sition to the right of wherever the previous constituent
ended. However in our case we cannot assume any such
anchoring to the left, since as the example illustrates.
the H is not always leftmost.
The solution we have adopted in the HPSG system
is to annotate each edge with information about the
span of substring which it covers. In the example be-
low the inactive edge El is matched against the head of
rule RI, and since they unify the new active edge E2 is
created with its head constituent instantiated with the
feature specifications which resulted from the unifica-
tion. This new edge E2 is annotated with the span of
the inactive edge El. Some time later the inactive edge
E3 is matched against the &amp;quot;np&amp;quot; constituent of our ac-
tive edge E2, resulting in the new active edge E4. The
span of E4 is obtained by combining the starting posi-
tion of E3 (i.e. l) with the finishing postion of E2 (i.e.
3). The point is that edges are constructed from the
head out, so that at any given time in the life cycle of
an edge the spanning information on the edge records
the span of contiguous substring which it covers.
Note that in the transition From rule RI to edge
E2 we have relabeled the constituent markers A ci,
and h with the symbols s, up, and VP respectively.
This is done merely as a mnemonic: device to reflect
the Fact that once the head of the edge is found, the
subcategorization information on that head (i.e. the
values of the &amp;quot;SUBCAT&amp;quot; feature of the verb works) is
An edge is, loosely speaking, an instantiation of a rule with sons. of Me
features on constituents made more specific.
propagated to the other elements of the edge, thereby
restricting the types of constituents with which they
can be satisfied. Writing a constituent marker in upper
case indicates that an inactive edge has been found
to instantiate it, while a lower case (not yet found)
constituent in bold face indicates that this is the next
constituent which will try to be instantiated.
</bodyText>
<figure confidence="0.988956666666667">
El. V&lt;3.3&gt;
Rl. x -&gt; cl ha*
E2. s&lt;3.3&gt; -&gt; np VP a*
E3. NP&lt;1.2&gt;.
E2. 5&lt;3.3&gt; -&gt; np VP a*
E4. s&lt;1.3&gt; -&gt; HP VP a*&apos;
</figure>
<figureCaption confidence="0.999603">
Figure 6: Combining edges and rules.
</figureCaption>
<subsectionHeader confidence="0.915581">
Using Semantics Restrictions
</subsectionHeader>
<bodyText confidence="0.999404965517242">
Parsing &amp;quot;head first&amp;quot; offers both practical and theo-
retical advantages. As mentioned above, the categories
of the grammatical relations subcategorized for by a
particular head are encoded as the SUBCAT value of
the head. Now GR&apos;s are of two distinct types: those
which are &amp;quot;saturated&amp;quot; (i.e. do not subcategorize for
anything themselves), such as subject and objects, and
those which subcategorize for a subject (i.e. controlled
complements). One of the language-universal gram-
matical principles (the Control Agreement Principle)
requires that the semantic controller of a controlled
complement always be the next grammatical relation
(in the order specified by the value of the SUBCAT
feature of the head) after the controlled complement
to combine with the head. But since the liPSG parser
always finds the head of a clause first, the gramrnati-
cal order of its complements, as well as their semantic
roles, are always specified before the complements are
found. As a consequence, semantic processing of con-
stituents can be done on the fly as the constituents
are found, rather than waiting until an edge has been
completed. Thus semantic processing can be done ex-
tremely locally (constituent-to-constituent in the edge,
rather than merely node-to-node in the parse tree as in
Montague semantics), and therefore a parse path ,-.tn
be abandoned on semantic grounds (e.g. sortal incon-
sistency) in the middle of constructing an edge. In this
way semantics, as well as syntax, can be used to control
the parsing process.
</bodyText>
<subsectionHeader confidence="0.742421">
Anaphora in HPSG
</subsectionHeader>
<bodyText confidence="0.9995571">
Another example of how parsing &amp;quot;head first&amp;quot; pays
off is illustrated by the elegant technique this strat-
egy makes possible for the binding of intrasentential
anaphors. This method allows us to assimilate cases of
bound anaphora to the same general binding method
used in the EIPSG system to handle other non-lexically-
governed dependencies such as gaps, interrogative pro-
nouns, and relative pronouns. Roughly, the unbound
dependencies of each type on every constituent are en- â¢
coded as values of an appropriate stack-valued feature
</bodyText>
<page confidence="0.997453">
169
</page>
<bodyText confidence="0.977066055555555">
(&amp;quot;binding feature&amp;quot;). In particular, unbound anaphors
are kept track of by two binding features, REFL (for
reflexive pronouns) and BPRO for personal pronouns
available to serve as bound anaphors. According to
the Binding Inheritance Principle, all categories on
binding-feature stacks which do not get bound under a
particular node are inherited onto that node. Just how
binding is effected depends on the type of dependency.
In the case of bound anaphora, this is accomplished
by merging the relevant agreement information (stored
in the REFL or BPRO stack of the constituent contain-
ing the anaphor) with one of the later GR&apos;s subcatego-
rized for by the head which governs that constituent.
This has the effect of forcing the node that ultimately
unifies with that GR (if any) to be the sought-after
antecedent. The difference between reflexives and per-
sonal pronouns is this. The binding feature REFL
is not allowed to inherit onto nodes of certain types
(those with CONTROL value INTRANS), thus forc-
ing the reflexive pronoun to become locally bound. In
the case of non-reflexive pronouns, the class of possible
antecedents is determined by modifying the subcatego-
rization information on the head governing the pronoun
so that all the subcategorized-for GR&apos;s later in gram-
matical order than the pronoun are &amp;quot;contra-indexed&amp;quot;
with the pronoun (and thereby prohibited from being
its antecedent). Binding then takes place precisely as
with reflexives, but somewhere higher in the tree.
We illustrate this distinction with two examples.
In sentence 51 below told subcategorizes for three con-
stituents: the subject NP Pullurn, the direct object
Gazdur, and the oblique object PP about himself.&apos;
Thus either Pullum or Gazdur are possible antecedents
of himself, but not Wasow.
Si. Wasow was convinced that Pullurn told
Gazdar about himself.
</bodyText>
<subsectionHeader confidence="0.879163">
S2. Wasow persuaded Pullum to shave him.
</subsectionHeader>
<bodyText confidence="0.999631857142857">
In sentence S2 shave subcategorizes for the direct
object NP him and an NP subject eventually filled by
the constituent Pullurn via control. Since the subject
position is contra-indexed with the pronoun, Pullum is
blocked from serving as the antecedent. The pronoun
is eventually bound by the NP Wasow higher up in the
tree.
</bodyText>
<subsectionHeader confidence="0.959285">
Heuristics to Optimize Search
</subsectionHeader>
<bodyText confidence="0.999839">
&apos;rhe IIPSC system, based US it is upon a care-
fully developed linguistic theory, has broad expressive
power. In practice, however, much of this power is often
not necessary. To exploit this fact the 11.1)5G system.
Uses heuristics to help reduce the search space implic-
itly defined by the grammar. These heuristics allow
the parser to produce an optimally ordered agenda of
edges to try based on words used in the sentence, and
on constituents it has found so far.
</bodyText>
<listItem confidence="0.382973">
â¢ The prepointion is treated essentially as a ease marking.
</listItem>
<bodyText confidence="0.999324833333333">
One type of heuristic involves additional syntactic
information which can be attached to rules to deter-
mine their likelihood. Such a heuristic is based on the
currently intended use for the rule to which it is at-
tached, and on the edges already available in the chart.
An example of this type of heuristic is sketched below.
</bodyText>
<figure confidence="0.399942">
Ri. x -&gt; cl h a*
</figure>
<figureCaption confidence="0.719594">
Heuristic-1: Are the features of cl *WE?
Figure 8: A rule with an attached heuristic.
</figureCaption>
<subsectionHeader confidence="0.484117">
Heuristic-1 encodes the fact that rule RI, when
</subsectionHeader>
<bodyText confidence="0.976683885714286">
used in its incarnation as the S NP VP rule, is pri-
marily intended to handle declarative sentences rather
than questions. Thus if the answer to Heuristic-1 is
&amp;quot;no&amp;quot; then this edge is given a higher ranking than if
the answer is &amp;quot;yes&amp;quot;. This heuristic, taken together with
others, determines the rank of the edge instantiated
from this rule, which in turn determines the order in
which edges will be tried. The result in this case is that
for a sentence such as 53 below, the system will pre-
fer the reading for which an appropriate answer is &amp;quot;a
character in a play by Shakespeare&amp;quot;, over the reading
which has as a felicitous answer &amp;quot;Richard Burton&amp;quot;.
S3. Who is Hamlet?
It should be emphasized, however, that heuristics
are not an essential part of the system, as are the fea-
ture passing principles, but rather are used only for
reasons of efficiency. In theory all possible constituents
permitted by the grammar will be found eventually
with or without heuristics. The heuristics simply help
a linguist tell the parser which readings are most likely,
and which parsing strategies are usually most fruitful,
thereby allowing the parser to construct the most likely
reading first. We believe that this clearly differenti-
ates IIPSG from &amp;quot;ad hoc&amp;quot; systems which do not make
sharp the distinction between theoretical principle and
heuristic guideline, and that this distinction is an im-
portant one if the natural language understanding pro-
grams of today are to be of any use to the natural
language programs and theories of the future.
ACKOWLEDGEMENTS
We would like to acknowledge the valuable assi-
tance of Thomas Wasow and Ivan Sag in the writing
of this paper. We would also like to thank Martin Kay
and Stuart Shieher for their helpful commentary on an
earlier draft.
</bodyText>
<page confidence="0.994673">
170
</page>
<sectionHeader confidence="0.999" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997916733333333">
[l1 Bresnan, J. (ed). (1982)
The Mental Representation of Grammatical Rela-
tions, The MIT Press, Cambridge, Mass.
21 Creary, L. and C. Pollard (1985)
&amp;quot;A Computational Semantics for Natural Lan-
guage&amp;quot;, Proceedings of the 23rd Annual Meeting
of the Association for Computational Linguistics.
31 Dowty, D.R. (1982)
&amp;quot;Grammatical Relations and Montague Grammar&amp;quot;,
In P. Jacobson and G.K. PuIlum (eds.), The Nature
of Syntactic Representation D. Reidel Publishing
Co., Dordrecht, Holland.
[ 41 Earley, J. (1970)
&amp;quot;An efficient context-free parsing algorithm&amp;quot;,
CACM 6:8, 1970.
51 Flickinger, D., C. Pollard. T. Wasow (1985)
&amp;quot;Structure-Sharing in Lexical Representation&amp;quot;,
Proceedings of the 23rd Annual Meeting of the
Association for Computational Linguistics.
i 61 Gawron, J. et al. (1982)
&amp;quot;Processing English with a Generalized Phrase
Structure Grammar&amp;quot;, ACI, Proceedings ea
Gazdar, G. et al. (in press)
Generalized Phrase Structure Grammar,
Blackwell and Harvard University Press.
[ 8! Kaplan, R. (i9i..)
&amp;quot;A General Syntactic Processor&amp;quot;, In Rustin (ed.)
Natural Language Processing. Algorithinics Press,
N.Y.
1, 91 Kay, M. (t973)
&amp;quot;The MIND System&amp;quot;, In Rustin (ed.) Natural
Language Processing. Algorithmics Press, N.Y.
1101 Kay, M. (forthcoming)
&amp;quot;Parsing in Functional Unification Grammar&amp;quot;.
111 Pollard. C. (1981),
Generalized Context -Free Grammars, Head Gram-
mars, and Natural Language, l&apos;h.D. Dissertation,
Stanford.
[121 Pollard, C. (forthcoming)
&amp;quot;A Semantic Approach to Binding in a Monos-
tratal Theory&amp;quot;, To appear in Linguistics and
Philosophy.
1131 Winograd, T. ( WHO)
Language as a Cognitive Process.
Addison-Wesley, Reading, Mass.
</reference>
<page confidence="0.998192">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001056">
<title confidence="0.998446">PARSING HEAD-DRIVEN PHRASE STRUCTURE GRAMMAR</title>
<author confidence="0.997767">Derek Proudian</author>
<author confidence="0.997767">Carl Pollard</author>
<affiliation confidence="0.998178">Hewlett-Packard Laboratories</affiliation>
<address confidence="0.9980345">1501 Page Mill Road Palo Alto, CA. 94303, USA</address>
<abstract confidence="0.998305325301205">The Head-driven Phrase Structure Grammar project (HPSG) is an English language database query system under development at Hewlett-Packard Laboratories. Unlike other product-oriented efforts in the natural language understanding field, the HPSG system was designed and implemented by linguists on the basis of recent theoretical developments. But, unlike other implementations of linguistic theories, this system is not a toy, as it deals with a variety of practical problems not covered in the theoretical literature. We believe that this makes the HPSG system unique in its combination of linguistic theory and practical application. The HPSG system differs from its predecessor GPSG, reported on at the 1982 ACL meeting (Gawron et al. 19821), in four significant respects: syntax, lexirepresentation, and semantics. The paper focuses on parsing issues, but also gives a synopsis of the underlying syntactic formalism. 1 Syntax HPSG is a lexically based theory of phrase structure, so called because of the central role played by grammatical heads and their associated complements.&apos; Roughly speaking, heads are linguistic forms (words and phrases) that exert syntactic and semantic restrictions on the phrases, called complements, that characteristically combine with them to form larger phrases. Verbs are the heads of verb phrases (and sentences), nouns are the heads of noun phrases, and so forth. As in most current syntactic theories, categories represented complexes of feature specifications. But the IIPSG treatment of lexical subcategorization the in the theory of categories for the notion of bar-level (in the sense of X-bar theory, prevalent in much current linguistic research). In addition, the augmentation of the system of categories with stackfeatures whose values are sequences of categories unifies the theory of lexical subcategorizatiott with the theory of binding phenomena. By binding phenomena we mean essentially non-clausebounded dependencies, such as those involving dislocated constituents, relative and interrogative pronouns, and reflexive and reciprocal pronouns 114 iliPSG is a refinement and extension of the closely relateu Generalised Phrase Structure Grammar T. The details of the theory of IIPSG are set forth in Ill!. More precisely, the subcategorization of a head is the value of a stack-valued feature called &amp;quot;SUBCAT&amp;quot;. For example, the SUBCAT value of the verb persuade is the sequence of three categories (VP, NP, NEI, corresponding to the grammatical relations (GR&apos;s): controlled complement, direct object, and subject respectively. We are adopting a modified version of 119821 terminology for GR&apos;s, where is direct object etc. For semantic reasons we call the GR following a controlled complement One of the key differences between HPSG and its GPSG is the massive relocation linguistic information from phrase structure rules into the lexi- This wholesale lexicalization linguistic information in HPSG results in a drastic reduction in the number of phrase structure rules. Since rules no longer handle subcategorization, their sole remaining function is to encode a small number of language-specific principles for projecting from lexical entries I surface constituent order. The schematic nature of the grammar rules allows the system to parse a large fragment of English with only a small number of rules (the system currently uses sixteen), since each rule can be used in many different situations. The constituents of each rule are sparsely annotated with features, but are fleshed out when taken together with constituents looked for and constituents found. example the sentence manager works be parsed using the single rule RI below. The rule applied to build the noun phrase by the head the lexical element manand the complement the lexical element entire sentence is built by identifying the the the noun described Thus the single rule 121 functions as both the VI&apos;, NP IV of familiar context free grammars.</abstract>
<note confidence="0.61642">x -&gt; cl h( (CONTROL INTRANS) ] Figure 1. A Grammar Rule. 167</note>
<title confidence="0.646327">Feature Passing</title>
<abstract confidence="0.976919018867925">The theory of HPSG embodies a number of substantive hypotheses about universal grammatical principles. Such principles as the Head Feature Principle, the Binding Inheritance Principle, and the Control Agreement Principle, require that certain syntactic features specified on daughters in syntactic trees are inherited by the mothers. Highly abstract phrase structure rules thus give rise to fully specified grammatical structures in a recursive process driven by syntactic information encoded on lexical heads. Thus HPSG, unlike similar &amp;quot;unification-based&amp;quot; syntactic theories, embodies a strong hypothesis about the flow of relevant information in the derivation of complex structures. Unification Another important difference between HPSG and other unification based syntactic theories concerns the form of the expressions which are actually unified. In HPSG, the structures which get unified are (with limited exceptions to be discussed below) not general graph structures as in Lexical Functional Grammar or Functional Unification Grammar 1101, but rather flat atomic valued feature matrices, such as those shown below. [(CONTROL 0 INTRANS) (MAJ N A) (AGR 3RDSG) (PRD MINUS) (TOP MINUS)] 0) (MAJ V) (INV PLUS)] Figure 2. Two feature matrices. In the implementation of EIPSC; we have been able to use this restriction on the form of feature matrices to good advantage. Since for any given version of the system the range of atomic features and feature values is fixed, we are able to represent flat feature matrices, such as the ones above, as vectors of integers, where in the vector represents a feature, and he integer in each cell represents a disjunction of the possible values for that feature. CON MAJ AGR PRD INV TOP ... 117110121113111 1 1 1 12 1 7 1 3 ! 1 1 3 Figure 3: Two transduced feature matrices. For example, if the possible values of the MAJ feaare N, V, A, and then uniquely represent any combination of these features with an integer in the range 0..15. This is accomplished simply by assigning each possible value an index which is an integral of this range and then adding up the indices so derived for each disjunction of values encountered. Unification in such cases is thus reduced to the &amp;quot;logical and&amp;quot; of the integers in each cell of the vector representing the feature matrix. In this way unification of these structures can be done in constant time, since &amp;quot;logical and&amp;quot; is generally a single machine instruction the overhead is very low.</abstract>
<note confidence="0.9771434">1 1 NV AP to1 1 (MAJ N A) 11 11 10 (MAJ N V) Unification 1Â° 1 0 0 1=8= (MAJ N)</note>
<abstract confidence="0.996277356">4: Closeup of the There are, however, certain cases when the values of features are not atomic, but are instead themselves feature matrices. The unification of such structures could, in theory, involve arbitrary recursion on the general unification algorithm, and it would seem that we had not progressed very far from the problem of unifying general graph structures. Happily, the features which this property of embedding constitute a small finite set (basically the so called &amp;quot;binding features&amp;quot;). Thus we are able to segregate such features from the rest, and recurse only when such a &amp;quot;category valued&amp;quot; feature is present. In practice. therefore, the time performance of the general unification algorithm is very good, essentially the same as that of the flat structure unification algorithm described above. As in the earlier GPSG system, the primary job of the parser in the HPSG system is to produce a semantics for the input sentence. This is done compositionally as the phrase structure is built, and uses only locally available information. Thus every constituent which is built syntactically has a corresponding semantics built for it at the same time, using only information available in the phrasal subtree which it immediately dominates. This locality constraint in computing the semantics for constituents is an essential characteristic of HPSG. For a more complete description of the semantic treatment used in the HPSG system see Creary and Pollard 121. Head-driven Active Chart Parser crucial between the HPSG system and its predecessor GPSG is the importance placed on the head constituent in HPSG. In HPSG it is the head cona rule which carries the subcategorization information needed to build the other constituents of 168 the rule. Thus parsing proceeds head first through the phrase structure of a sentence, rather than left to right through the sentence string. The parser itself is a variation of an active chart parser [4,9,8,131, modified to permit the construction of constituents head first, instead of in left-to-right order. In order to successfully parse &amp;quot;head first&amp;quot;, an edge&apos; must be augmented to include information about its span (i.e. its position in the string). This is necessary because head can appear as a middle constituent of a rule with other constituents (e.g. complements or adjuncts) on either side. Thus it is not possible to record all the requisite boundary information simply by moving a dot through the rule (as in Earley), or by keeping track of just those constituents which remain to be built (as in Winograd). An example should make this clear. Suppose as before we are confronted with the task parsing the sentence manager works, again we have available the grammar rule RI. Since we are parsing in a &amp;quot;head first&amp;quot; manner we must match the against some substring of the sentence. But which substring? In more conventional chart parsing algorithms which proceed left to right this is not a serious problem, since we are always guaranteed to have an anchor to the left. We simply try building the leftmost constituent of the rule starting at the leftmost position of the string, and if this succeeds we try to build the next leftmost constituent starting at one position to the right of wherever the previous constituent ended. However in our case we cannot assume any such anchoring to the left, since as the example illustrates. not always leftmost. The solution we have adopted in the HPSG system to each edge with information about of which it In the example below the inactive edge El is matched against the head of rule RI, and since they unify the new active edge E2 is created with its head constituent instantiated with the feature specifications which resulted from the unification. This new edge E2 is annotated with the span of the inactive edge El. Some time later the inactive edge E3 is matched against the &amp;quot;np&amp;quot; constituent of our active edge E2, resulting in the new active edge E4. The span of E4 is obtained by combining the starting position of E3 (i.e. l) with the finishing postion of E2 (i.e. point is that edges are constructed from the out, so that at any given time in cycle of an edge the spanning information on the edge records the span of contiguous substring which it covers. Note that in the transition From rule RI to edge E2 we have relabeled the constituent markers A ci, h with the symbols s, up, and This is done merely as a mnemonic: device to reflect the Fact that once the head of the edge is found, the subcategorization information on that head (i.e. the of the &amp;quot;SUBCAT&amp;quot; feature of the verb An edge is, loosely speaking, an instantiation of a rule with sons. of Me features on constituents made more specific. propagated to the other elements of the edge, thereby restricting the types of constituents with which they can be satisfied. Writing a constituent marker in upper case indicates that an inactive edge has been found to instantiate it, while a lower case (not yet found) in bold face indicates that this the next constituent which will try to be instantiated. El. V&lt;3.3&gt; x -&gt; cl E2. s&lt;3.3&gt; -&gt; np VP a* E3. E2. 5&lt;3.3&gt; -&gt; np VP a* E4. s&lt;1.3&gt; -&gt; HP VP a*&apos; Figure 6: Combining edges and rules. Using Semantics Restrictions first&amp;quot; offers both practical theoretical advantages. As mentioned above, the categories of the grammatical relations subcategorized for by a particular head are encoded as the SUBCAT value of the head. Now GR&apos;s are of two distinct types: those which are &amp;quot;saturated&amp;quot; (i.e. do not subcategorize for themselves), such as subject and and those which subcategorize for a subject (i.e. controlled One of the grammatical principles (the Control Agreement Principle) requires that the semantic controller of a controlled complement always be the next grammatical relation (in the order specified by the value of the SUBCAT feature of the head) after the controlled complement to combine with the head. But since the liPSG parser always finds the head of a clause first, the gramrnatical order of its complements, as well as their semantic roles, are always specified before the complements are found. As a consequence, semantic processing of concan be done on the the constituents are found, rather than waiting until an edge has been completed. Thus semantic processing can be done extremely locally (constituent-to-constituent in the edge, rather than merely node-to-node in the parse tree as in semantics), and therefore a parse path be abandoned on semantic grounds (e.g. sortal inconsistency) in the middle of constructing an edge. In this way semantics, as well as syntax, can be used to control the parsing process. Anaphora in HPSG Another example of how parsing &amp;quot;head first&amp;quot; pays off is illustrated by the elegant technique this strategy makes possible for the binding of intrasentential anaphors. This method allows us to assimilate cases of bound anaphora to the same general binding method used in the EIPSG system to handle other non-lexicallydependencies such gaps, interrogative pronouns, and relative pronouns. Roughly, the unbound dependencies of each type on every constituent are en- â¢ coded as values of an appropriate stack-valued feature 169 (&amp;quot;binding feature&amp;quot;). In particular, unbound anaphors are kept track of by two binding features, REFL (for reflexive pronouns) and BPRO for personal pronouns available to serve as bound anaphors. According to the Binding Inheritance Principle, all categories on binding-feature stacks which do not get bound under a particular node are inherited onto that node. Just how binding is effected depends on the type of dependency. In the case of bound anaphora, this is accomplished by merging the relevant agreement information (stored in the REFL or BPRO stack of the constituent containing the anaphor) with one of the later GR&apos;s subcategorized for by the head which governs that constituent. This has the effect of forcing the node that ultimately unifies with that GR (if any) to be the sought-after antecedent. The difference between reflexives and personal pronouns is this. The binding feature REFL is not allowed to inherit onto nodes of certain types (those with CONTROL value INTRANS), thus forcing the reflexive pronoun to become locally bound. In the case of non-reflexive pronouns, the class of possible is determined by modifying subcategorization information on the head governing the pronoun so that all the subcategorized-for GR&apos;s later in grammatical order than the pronoun are &amp;quot;contra-indexed&amp;quot; with the pronoun (and thereby prohibited from being its antecedent). Binding then takes place precisely as with reflexives, but somewhere higher in the tree. illustrate this distinction with two 51 below for three constituents: the subject NP Pullurn, the direct object the oblique object PP himself.&apos; either Pullum or Gazdur possible antecedents not Wasow was Pullum to shave sentence S2 for the direct NP an NP subject eventually filled by the constituent Pullurn via control. Since the subject is contra-indexed with the pronoun, blocked from serving as the antecedent. The pronoun is eventually bound by the NP Wasow higher up in the tree. Heuristics to Optimize Search IIPSC based upon a carefully developed linguistic theory, has broad expressive In practice, however, much of this is often necessary. exploit this fact the 11.1)5G to help reduce the search space implicitly defined by the grammar. These heuristics allow the parser to produce an optimally ordered agenda of to based on used in the sentence, on constituents it has found so far. â¢ The prepointion is treated essentially as a ease marking. One type of heuristic involves additional syntactic information which can be attached to rules to determine their likelihood. Such a heuristic is based on the intended use for the rule to which it attached, and on the edges already available in the chart. An example of this type of heuristic is sketched below. -&gt; cl h a* Heuristic-1: Are the features of cl *WE? Figure 8: A rule with an attached heuristic. Heuristic-1 encodes the fact that rule RI, when in its incarnation as the VP is marily intended to handle declarative sentences rather than questions. Thus if the answer to Heuristic-1 is &amp;quot;no&amp;quot; then this edge is given a higher ranking than if the answer is &amp;quot;yes&amp;quot;. This heuristic, taken together with determines the rank of the instantiated from this rule, which in turn determines the order in which edges will be tried. The result in this case is that for a sentence such as 53 below, the system will prefer the reading for which an appropriate answer is &amp;quot;a character in a play by Shakespeare&amp;quot;, over the reading which has as a felicitous answer &amp;quot;Richard Burton&amp;quot;. S3. Who is Hamlet? It should be emphasized, however, that heuristics are not an essential part of the system, as are the feature passing principles, but rather are used only for reasons of efficiency. In theory all possible constituents permitted by the grammar will be found eventually with or without heuristics. The heuristics simply help a linguist tell the parser which readings are most likely, and which parsing strategies are usually most fruitful, thereby allowing the parser to construct the most likely reading first. We believe that this clearly differentiates IIPSG from &amp;quot;ad hoc&amp;quot; systems which do not make sharp the distinction between theoretical principle and heuristic guideline, and that this distinction is an important one if the natural language understanding programs of today are to be of any use to the natural language programs and theories of the future. ACKOWLEDGEMENTS We would like to acknowledge the valuable assitance of Thomas Wasow and Ivan Sag in the writing of this paper. We would also like to thank Martin Kay Stuart Shieher their helpful commentary on an earlier draft.</abstract>
<note confidence="0.7602699">170 REFERENCES Bresnan, J. (1982) The Mental Representation of Grammatical Rela- The MIT Cambridge, Mass. L. and C. Pollard &amp;quot;A Computational Semantics for Natural Lanof the 23rd Annual Meeting of the Association for Computational Linguistics. Dowty, D.R.</note>
<title confidence="0.470906666666667">amp;quot;Grammatical Relations and Montague Grammar&amp;quot;, P. Jacobson and G.K. PuIlum (eds.), Nature Syntactic Representation Reidel Publishing</title>
<address confidence="0.8569575">Co., Dordrecht, Holland. 41 Earley, J.</address>
<note confidence="0.869855060606061">amp;quot;An efficient context-free parsing algorithm&amp;quot;, CACM 6:8, 1970. D., C. Pollard. T. Wasow &amp;quot;Structure-Sharing in Lexical Representation&amp;quot;, Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics. Gawron, J. et al. &amp;quot;Processing English with a Generalized Phrase Grammar&amp;quot;, Proceedings G. et al. press) Phrase Structure Blackwell and Harvard University Press. [ 8! Kaplan, R. (i9i..) &amp;quot;A General Syntactic Processor&amp;quot;, In Rustin (ed.) Language Processing. Press, N.Y. 91 M. MIND System&amp;quot;, In Rustin (ed.) Processing. Press, N.Y. 1101 Kay, M. (forthcoming) &amp;quot;Parsing in Functional Unification Grammar&amp;quot;. Pollard. C. -Free Grammars, Head Gramand Natural Language, l&apos;h.D. Stanford. Pollard, (forthcoming) &amp;quot;A Semantic Approach to Binding in a Monos- Theory&amp;quot;, To appear in and Philosophy. Winograd, T. ( a Process. Addison-Wesley, Reading, Mass. 171</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Mental Representation of Grammatical Relations,</title>
<date>1982</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1982</marker>
<rawString>[l1 Bresnan, J. (ed). (1982) The Mental Representation of Grammatical Relations, The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Creary</author>
<author>C Pollard</author>
</authors>
<title>A Computational Semantics for Natural Language&amp;quot;,</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Creary, Pollard, 1985</marker>
<rawString>21 Creary, L. and C. Pollard (1985) &amp;quot;A Computational Semantics for Natural Language&amp;quot;, Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
</authors>
<title>Grammatical Relations and Montague Grammar&amp;quot;,</title>
<date>1982</date>
<booktitle>The Nature of Syntactic Representation D. Reidel Publishing Co.,</booktitle>
<editor>In P. Jacobson and G.K. PuIlum (eds.),</editor>
<location>Dordrecht, Holland.</location>
<marker>Dowty, 1982</marker>
<rawString>31 Dowty, D.R. (1982) &amp;quot;Grammatical Relations and Montague Grammar&amp;quot;, In P. Jacobson and G.K. PuIlum (eds.), The Nature of Syntactic Representation D. Reidel Publishing Co., Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<title>An efficient context-free parsing algorithm&amp;quot;,</title>
<date>1970</date>
<journal>CACM</journal>
<volume>6</volume>
<marker>1970</marker>
<rawString>[ 41 Earley, J. (1970) &amp;quot;An efficient context-free parsing algorithm&amp;quot;, CACM 6:8, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wasow</author>
</authors>
<title>Structure-Sharing in Lexical Representation&amp;quot;,</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics. i</booktitle>
<volume>61</volume>
<marker>Wasow, 1985</marker>
<rawString>51 Flickinger, D., C. Pollard. T. Wasow (1985) &amp;quot;Structure-Sharing in Lexical Representation&amp;quot;, Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics. i 61 Gawron, J. et al. (1982) &amp;quot;Processing English with a Generalized Phrase Structure Grammar&amp;quot;, ACI, Proceedings ea</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>(in press) Generalized Phrase Structure Grammar, Blackwell and</title>
<date></date>
<booktitle>8!</booktitle>
<publisher>Harvard University Press.</publisher>
<location>Kaplan, R.</location>
<marker>Gazdar, </marker>
<rawString>Gazdar, G. et al. (in press) Generalized Phrase Structure Grammar, Blackwell and Harvard University Press. [ 8! Kaplan, R. (i9i..)</rawString>
</citation>
<citation valid="false">
<title>A General Syntactic Processor&amp;quot;,</title>
<booktitle>Natural Language Processing.</booktitle>
<editor>In Rustin (ed.)</editor>
<publisher>Algorithinics Press, N.Y.</publisher>
<marker></marker>
<rawString>&amp;quot;A General Syntactic Processor&amp;quot;, In Rustin (ed.) Natural Language Processing. Algorithinics Press, N.Y.</rawString>
</citation>
<citation valid="false">
<title>The MIND System&amp;quot;,</title>
<booktitle>Natural Language Processing.</booktitle>
<editor>1, 91 Kay, M. (t973)</editor>
<publisher>Algorithmics Press, N.Y.</publisher>
<marker></marker>
<rawString>1, 91 Kay, M. (t973) &amp;quot;The MIND System&amp;quot;, In Rustin (ed.) Natural Language Processing. Algorithmics Press, N.Y.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Kay</author>
</authors>
<title>(forthcoming) &amp;quot;Parsing in Functional Unification Grammar&amp;quot;.</title>
<marker>Kay, </marker>
<rawString>1101 Kay, M. (forthcoming) &amp;quot;Parsing in Functional Unification Grammar&amp;quot;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C</author>
</authors>
<title>Generalized Context -Free Grammars, Head Grammars, and Natural Language,</title>
<date>1981</date>
<location>l&apos;h.D. Dissertation, Stanford.</location>
<marker>C, 1981</marker>
<rawString>111 Pollard. C. (1981), Generalized Context -Free Grammars, Head Grammars, and Natural Language, l&apos;h.D. Dissertation, Stanford.</rawString>
</citation>
<citation valid="false">
<title>A Semantic Approach to Binding in a Monostratal Theory&amp;quot;,</title>
<note>To appear in Linguistics and Philosophy.</note>
<marker></marker>
<rawString>[121 Pollard, C. (forthcoming) &amp;quot;A Semantic Approach to Binding in a Monostratal Theory&amp;quot;, To appear in Linguistics and Philosophy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Winograd</author>
</authors>
<title>( WHO) Language as a Cognitive Process.</title>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Mass.</location>
<marker>Winograd, </marker>
<rawString>1131 Winograd, T. ( WHO) Language as a Cognitive Process. Addison-Wesley, Reading, Mass.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>