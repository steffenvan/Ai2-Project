<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.057738">
<title confidence="0.997579">
Concrete Assignments for Teaching NLP in an M.S. Program
</title>
<author confidence="0.996617">
Reva Freedman
</author>
<affiliation confidence="0.982269">
Department of Computer Science
Northern Illinois University
</affiliation>
<address confidence="0.938809">
DeKalb, IL 60115
</address>
<email confidence="0.99939">
freedman@cs.niu.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962739130435">
The professionally oriented computer
science M.S. students at Northern Illinois
University are intelligent, interested in
new ideas, and have good programming
skills and a good math background.
However, they have no linguistics
background, find traditional academic
prose difficult and uninteresting, and have
had no exposure to research. Given this
population, the assignments I have found
most successful in teaching Introduction to
NLP involve concrete projects where
students could see for themselves the
phenomena discussed in class. This paper
describes three of my most successful
assignments: duplicating Kernighan et
al.’s Bayesian approach to spelling
correction, a study of Greenberg’s
universals in the student’s native language,
and a dialogue generation project. For
each assignment I discuss what the
students learned and why the assignment
was successful.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9956254">
Northern Illinois University is a large public
university (25,000 students) located in the farm-
oriented exurbs of Chicago, about 60 miles west of
the city. Most of the undergraduate computer
science majors and about a third of the M.S.
</bodyText>
<page confidence="0.990583">
37
</page>
<bodyText confidence="0.99983903125">
students come from the hi-tech corridor west of
Chicago or small towns near the university. The
remaining M.S. students are international students,
currently mostly from India.
This paper discusses my experiences in two
semesters of teaching Introduction to NLP and
three semesters of teaching an NLP unit in an
Introduction to Artificial Intelligence course.
Because the students have no background in
linguistics and are not used to reading the type of
academic prose found in the textbook (Jurafsky
and Martin, 2000), the most successful units I have
taught involved concrete assignments where
students could see for themselves the phenomena
discussed in class. Successful assignments also did
not assume any background in linguistics, even
such basic notions as part of speech.
To provide an overview of the field, each year
the NLP course contains three segments: one on a
statistical approach to NLP, one on syntax, and
one on a logic-based approach. The three segments
are also chosen to include topics from phonology
and morphology, syntax, and pragmatics. The
specific content changes from year to year in an
effort to find topics that both represent current
issues in the field and capture the students’
imagination.
This paper describes three of the most
successful assignments. For each one, I describe
the assignment, the topics the students learned, and
why it was successful. The three assignments are:
duplicating Kernighan et al.’s Bayesian approach
</bodyText>
<note confidence="0.7459185">
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 37–42,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996716">
to spelling correction, a study of Greenberg’s
universals in a language other than English
(usually the student’s native language), and a
dialogue generation project using my research
software.
</bodyText>
<sectionHeader confidence="0.986723" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999738">
2.1 Student demographics
</subsectionHeader>
<bodyText confidence="0.99997185">
Most of the students taking Introduction to NLP
are graduate students, although undergraduates are
eligible if they have had three semesters of C++
programming. Graduate students in cognitive
science-related fields, such as psychology or
linguistics, are eligible if they have taken one
semester of programming and are willing to teach
themselves about trees. I actively recruit non-
computer science students because it makes the
course more interesting. In addition to providing a
broader spectrum of interests, they tend to be more
outgoing. They tend to be more willing to answer
questions in class, and also to ask questions in
class, which many of the computer science
students will not do.
The preferred career path among the students is
to obtain a programming job in local industry,
preferably in a hi-tech area. However, among both
undergraduates and graduate students, a few
continue their education. One minority student
with no previous experience in research became
interested and is now planning to apply to a PhD
program. In general, students take the course out
of a desire to do something different from the
normal operating systems, networking and
database courses. An occasional student also takes
the course because it fits in their schedule or
because it doesn’t have onerous prerequisites.
In general, the international students have good
to near-native competence in spoken English,
although a few cannot not follow my lectures, and
some do not have sufficient writing skills for an
essay exam. All could read my lecture notes
without difficulty. Both among the international
students and local students, many do not have
sufficient experience with formal academic prose
to understand the textbook (Jurafsky and Martin,
2000). Students’ first languages have included
Telugu, Hindi/Urdu, Nepali, Chinese (Mandarin),
and Bulgarian.
</bodyText>
<subsectionHeader confidence="0.999653">
2.2 Student background
</subsectionHeader>
<bodyText confidence="0.9993235">
Koedinger (2001), in his research on tutoring
systems for high school mathematics, gives the
following as his fundamental principle: “the
student is not like me.” In particular, student
background frequently did not include the
following items:
</bodyText>
<listItem confidence="0.9268071">
1) Parts of speech
2) Basic English grammar
3) Relationships between languages and language
families
4) Practical issues, such as the importance of
transliteration and glossing
5) Philosophical issues, such as the fact that there
is no single authoritative grammar of a natural
language or that one language is not more
difficult than another in an absolute sense
</listItem>
<bodyText confidence="0.999974428571429">
However, students were talented at and enjoyed
programming. Most students also had a good math
background. Finally, they were enthusiastic about
learning new things, as long as it involved concrete
examples that they could work out and a sample
problem with a solution that they could use as a
model.
</bodyText>
<sectionHeader confidence="0.991733" genericHeader="method">
3 Spelling correction
</sectionHeader>
<subsectionHeader confidence="0.999499">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.999927882352941">
The goal of the first section of the course was to
show the students the power of statistical methods
in NLP. In this section, students were asked to
duplicate the calculations used in Kernighan et
al.’s (1990) Bayesian approach to spelling
correction, as explained in Section 5.5 of the
textbook.
Kernighan et al. choose as the preferred
correction the one that maximizes P(t|c)P(c),
where t is the typo and c is a candidate correction.
Candidate corrections are generated by assuming
that errors involve only one letter or the
transposition of two adjacent letters. To reproduce
this calculation, students need the confusion
matrices provided in the original paper, a source of
unigram and bigram data, and a source for word
frequencies.
</bodyText>
<page confidence="0.994754">
38
</page>
<subsectionHeader confidence="0.993574">
3.2 Assignment
</subsectionHeader>
<bodyText confidence="0.891698470588235">
Students are given some misspelled words and
possible corrections, such as the following
examples from Kernighan et al:
misspelled word possible corrections
ambitios ambitious
ambitions
ambition
For each of these misspelled words, students are
asked to do the following:
a) Use the method described by Kernighan et al.,
or equivalently in section 5.5 of the text, to find
the probability of each possible correction.
b) Use their preferred spell checker (Microsoft
Word, Unix ispell, etc.) to generate possible
corrections for the same misspelled words.
The following questions are asked for each
misspelled word:
</bodyText>
<listItem confidence="0.980823">
• Is the most probable correction according to
Kernighan’s algorithm the same as the one
suggested by your program?
• Which additional possible corrections (i.e., non-
single-error corrections or non-single word
corrections) does your program generate?
• Which of Kernighan’s possible corrections does
your program omit?
</listItem>
<bodyText confidence="0.934626666666667">
Since Kernighan’s original paper omits the
unigram and bigram count matrices, I provide a
file with this information. Students are encouraged
to find a source for word frequencies on the Web.
As one option, I suggest they use any search
engine (e.g., Google), after class discussion about
the approximations involved in this approach.
Students are also given two summary questions
to answer:
</bodyText>
<listItem confidence="0.950074">
• A former student, Mr. I. M. Kluless, says: I
don’t see the point of using the frequency of
potential corrections in the corpus (i.e., the prior
probability) as part of Kernighan’s algorithm. I
would just use the likelihood of a given error. How
would you answer Mr. Kluless? (One way to think
about this question is: what would happen if you
left it out?)
• Another former student, Ms. U. R. Useless, says:
I don’t see the point of using the likelihood of a
given error as part of Kernighan’s algorithm. I
would just use the prior probability. How would
you answer Ms. Useless?
</listItem>
<subsectionHeader confidence="0.756796">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999816545454546">
Students enjoyed this assignment because it was
straightforward and used mathematics they were
familiar with. They were uniformly surprised to
discover that spelling correction is generally done
today using Bayesian concepts rather than by
dictionary lookup alone. They were also surprised
to learn that learn that results were largely
independent of the corpus chosen. Students who
already knew Bayes’ theorem learned about an
application completely different from the ones
they had used in other courses.
The majority of students used my suggestion to
approximate word frequencies in a corpus by page
counts in Google. They were surprised to learn
that in spite of the number of ways in which the
web differs from an ideal corpus, the volume of
data available ensures that accurate results are still
obtained. The better students searched the web for
corpora they preferred, including the works of
Shakespeare and an online interface to the British
National Corpus
(http://sara.natcorp.ox.ac.uk/lookup.html).
</bodyText>
<sectionHeader confidence="0.98188" genericHeader="method">
4 Syntax and language universals
</sectionHeader>
<subsectionHeader confidence="0.99763">
4.1 Background
</subsectionHeader>
<bodyText confidence="0.999314266666667">
The second section of the course had as its goal to
teach the students some basic aspects of syntax. I
started with parts of speech and basic concepts of
context-free grammars. I then introduced
unification grammars as a way of obtaining more
power with fewer rules.
As a way of showing the syntactic variation
among languages, I also introduced some of
Greenberg’s word order universals (Greenberg,
1966), following the exposition in Baker (2001).
Although identifying the most probable underlying
word order (SVO, etc.) of an unknown language
can involve significant linguistic intuition, I did
not expect students to achieve that goal. Rather, I
used Greenberg’s ideas to make students think
</bodyText>
<page confidence="0.998673">
39
</page>
<bodyText confidence="0.999661833333333">
about the rules they were generating instead of
generating S --&gt; NP VP by rote. Additionally, the
use of multiple languages contributed to the
university’s goal of introducing ideas of
internationalization and diversity in classes where
feasible.
</bodyText>
<subsectionHeader confidence="0.987646">
4.2 Assignment
</subsectionHeader>
<bodyText confidence="0.999137333333333">
The students were asked to prepare a 15-minute
class presentation showing two or three interesting
phenomena of one of the languages of the world.
Most students used their native language.
They were asked to include the following
information:
</bodyText>
<listItem confidence="0.972466142857143">
• Where the language fits in Greenberg’s
classification (SVO, etc.)
• One or more syntactic phenomena that make
the language interesting
• A grammar fragment (a set of CFG rules,
possibly with unification-based features)
illustrating one of the chosen phenomena
</listItem>
<bodyText confidence="0.981864517241379">
They could show several interesting phenomena
with a short implementation of one, a complex
phenomenon and a longer fragment of grammar, or
one interesting phenomenon and multiple ways to
implement it.
For each example they used, they were required
to show the original transliterated into the Roman
alphabet, a morpheme-level analysis, and a
translation into English.
As a template, I gave a presentation using a
language none of them had been exposed to,
modern Hebrew. The four sample phenomena I
presented were: a) there is no indefinite article,
b) nouns and adjectives must agree in gender and
number, c) adjectives follow the noun, and d) the
definite article is attached to every adjective in an
NP as well as to the noun.
In addition to providing an example of the scope
required, the presentation also introduced the
students to conventions of linguistic presentation,
including interlinear display of transliteration,
morpheme analysis, and translation. One slide
from my presentation is shown below:
he- khatul ha- gadol
DET cat-M-S DET big-M-S
“the big cat”
he- khatulim ha- g’dolim
DET cat-M-PL DET big-M-PL
“the big cats”
</bodyText>
<subsectionHeader confidence="0.948692">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999973181818182">
This assignment was useful for ensuring that
students had a basic grasp of many elements of
syntax covered in Section II of the textbook,
including parts of speech, context-free grammars,
and unification grammars. Second, the class
presentations provided students concrete examples
of some major syntactic concepts that all
languages share, as well as some of the
differences. Finally, this assignment enabled
students to learn about and present some of the
core linguistic features of their native language.
</bodyText>
<sectionHeader confidence="0.996434" genericHeader="method">
5 Dialogue generation
</sectionHeader>
<subsectionHeader confidence="0.998685">
5.1 Background
</subsectionHeader>
<bodyText confidence="0.999982107142857">
The third segment of the course had as its goal to
show how a logic-based approach is useful in
NLP. Since some of my previous work involves
implementing dialogue software using a logic-
based approach, dialogue systems was a natural
choice for this segment.
Phenomena discussed in lecture included the
concepts of speech act and discourse intention, the
relationship between syntactic form and intention,
direct and indirect speech acts, and a short
introduction to dialogue act classification.
As a counterbalance to the more theoretical
material from Greenberg, this section included
some information about current commercial uses
of NLP. Students were asked to read an article
from the popular press (Mount, 2005) describing
experiences with currently available commercial
systems.
I used my own software, APE (Freedman,
2000), a domain-independent dialogue plan
interpreter based on reactive planning concepts.
APE uses a rule-based macro language
implemented in Common Lisp. It is a hierarchical
task network (HTN) style planner, achieving each
goal via a series of subgoals. APE’s high-level
planning loop alternates between waiting for user
input and planning responses. It executes plan
operators until a primitive, non-decomposable one
</bodyText>
<page confidence="0.951933">
40
</page>
<bodyText confidence="0.9991545625">
is obtained. In addition to elicit and inform, plans different from other algorithms they had learned,
can also include primitives to query and update they had no trouble learning the unification
APE’s internal knowledge base, thus giving the algorithm, both iterative and recursive versions,
system a “mind.” Primitives are added to a buffer because they were experienced in learning
until a primitive requiring a response from the user algorithms. For most students in our program, this
is received. At that point the operators in the project will be their only experience with a non-
buffer are used to build the output text. Goals are imperative programming language.
represented using first-order logic without Students were not bothered by the fact that the
quantifiers, with full unification used for sample software provided included some features
matching. not discussed in class. In fact, some of the better
APE provides two ways to change a plan in students studied these features and used them in
progress. The author can instruct the system either their own programs.
to switch from one method of satisfying a goal to Every student mastered the basics of logic
another or to add new goals at the top of the programming, including how to choose between
agenda, possibly replacing existing goals. The alternatives, establish a default, implement multi-
latter facility is particularly useful in dialogue step and hierarchical procedures, interact with the
generation, since it allows the system to prompt user, and access an external database. They also
the user after an error. This feature makes APE learned how to use unification along with multiple
more powerful than the pushdown automaton one first-order relations to access and update a
might use to implement a context-free grammar. database. The weaker students simply used the
In addition, APE is obviously more powerful than sample software as a guide, while the stronger
the finite-state machines often used in dialogue ones mastered the underlying concepts and wrote
generation. more creative code.
Use of APE allows students to generate realistic Student projects ranged the gamut, including a
hierarchically structured conversations with a system for running a car dealership, a game
reasonable number of rules. identifying movie directors, and an interactive
5.2 Assignment system providing health information.
Sample code presented in class involved looking 6 Conclusions
up data in a database of presidents’ names. The Teaching NLP to students for whom this will be
sample system prompted the user for input, then the only exposure to the topic, and possibly the
provided answers, error messages, and re-prompts only exposure to a research-oriented topic, can be
as appropriate. As an illustration of the power of a successful and enjoyable experience for both
the approach, I also demonstrated some of my students and teacher. With good organization,
research software, which showed conversations students can do useful projects even in one
embedded in a variety of front-end GUIs. semester.
For the assignment, students were asked to One factor that has increased student
choose their own topic. They were asked to choose satisfaction as well as their mastery of the material
a problem, then provide a database layout and is the use of concrete assignments where students
draw a graph showing the possible conversations can see for themselves concepts described in class.
their system could generate. Finally, they were Three such assignments I have successfully used
asked to implement the code. At the end of the involve duplicating Kernighan et al.’s Bayesian
semester, students made a five-minute presentation approach to spelling correction, a study of
to the class showing their application. Greenberg’s universals in the student’s native
5.3 Results language, and a dialogue generation project using
Students greatly enjoyed this assignment because my research software. Each of these assignments is
it involved the activity they enjoyed most, namely used in one of the three segments of the course:
programming. Even though it was qualitatively statistical approaches to language, introduction to
41 syntax, and logic-based approaches to NLP.
</bodyText>
<sectionHeader confidence="0.996658" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98619375">
Michael Glass of Valparaiso University graciously
supplied the unigram and bigram counts needed to
implement Kernighan et al.’s (1990) spelling
correction algorithm.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999812695652174">
Baker, M. (2001). Atoms of Language: The Mind’s
Hidden Rules of Grammar. New York: Basic Books.
Freedman, R. (2000). Using a Reactive Planner as the
Basis for a Dialogue Agent. In Proceedings of the
Thirteenth Florida Artificial Intelligence Research
Symposium (FLAIRS 2000), Orlando.
Greenberg, J. (1966). Some universals of grammar with
particular reference to the order of meaningful
elements. In Universals of language, ed.
J. Greenberg, pp. 73–113. Cambridge, MA: MIT
Press. 2nd ed.
Kernighan, M., Church, K., and Gale, W. (1990). A
spelling correction program based on a noisy channel
model. In COLING ’90 (Helsinki), v. 2, pp. 205–211.
Available online from the ACL archive at
http://acl.ldc.upenn.edu/C/C90/C90-2036.pdf.
Koedinger, K. (2001). The Student is Not Like Me. In
Tenth International Conference on Artificial
Intelligence in Education (AI-ED 2001). San Antonio,
TX. Keynote address. Slides available online at
http://www.itsconference.org/content/seminars.htm.
Mount, I. (2005). Cranky Consumer: Testing Online
Service Reps. Wall Street Journal, Feb. 1, 2005.
</reference>
<page confidence="0.999296">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.318145">
<title confidence="0.915662">Concrete Assignments for Teaching NLP in an M.S. Program</title>
<author confidence="0.827587">Reva</author>
<affiliation confidence="0.854118">Department of Computer Northern Illinois</affiliation>
<address confidence="0.66075">DeKalb, IL</address>
<email confidence="0.999395">freedman@cs.niu.edu</email>
<abstract confidence="0.989776818181818">The professionally oriented computer science M.S. students at Northern Illinois University are intelligent, interested in new ideas, and have good programming skills and a good math background. However, they have no linguistics background, find traditional academic prose difficult and uninteresting, and have had no exposure to research. Given this population, the assignments I have found</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baker</author>
</authors>
<title>Atoms of Language: The Mind’s Hidden Rules of Grammar.</title>
<date>2001</date>
<publisher>Basic Books.</publisher>
<location>New York:</location>
<contexts>
<context position="10190" citStr="Baker (2001)" startWordPosition="1579" endWordPosition="1580">ks of Shakespeare and an online interface to the British National Corpus (http://sara.natcorp.ox.ac.uk/lookup.html). 4 Syntax and language universals 4.1 Background The second section of the course had as its goal to teach the students some basic aspects of syntax. I started with parts of speech and basic concepts of context-free grammars. I then introduced unification grammars as a way of obtaining more power with fewer rules. As a way of showing the syntactic variation among languages, I also introduced some of Greenberg’s word order universals (Greenberg, 1966), following the exposition in Baker (2001). Although identifying the most probable underlying word order (SVO, etc.) of an unknown language can involve significant linguistic intuition, I did not expect students to achieve that goal. Rather, I used Greenberg’s ideas to make students think 39 about the rules they were generating instead of generating S --&gt; NP VP by rote. Additionally, the use of multiple languages contributed to the university’s goal of introducing ideas of internationalization and diversity in classes where feasible. 4.2 Assignment The students were asked to prepare a 15-minute class presentation showing two or three </context>
</contexts>
<marker>Baker, 2001</marker>
<rawString>Baker, M. (2001). Atoms of Language: The Mind’s Hidden Rules of Grammar. New York: Basic Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Freedman</author>
</authors>
<title>Using a Reactive Planner as the Basis for a Dialogue Agent.</title>
<date>2000</date>
<booktitle>In Proceedings of the Thirteenth Florida Artificial Intelligence Research Symposium (FLAIRS 2000),</booktitle>
<location>Orlando.</location>
<contexts>
<context position="13731" citStr="Freedman, 2000" startWordPosition="2126" endWordPosition="2127">tems was a natural choice for this segment. Phenomena discussed in lecture included the concepts of speech act and discourse intention, the relationship between syntactic form and intention, direct and indirect speech acts, and a short introduction to dialogue act classification. As a counterbalance to the more theoretical material from Greenberg, this section included some information about current commercial uses of NLP. Students were asked to read an article from the popular press (Mount, 2005) describing experiences with currently available commercial systems. I used my own software, APE (Freedman, 2000), a domain-independent dialogue plan interpreter based on reactive planning concepts. APE uses a rule-based macro language implemented in Common Lisp. It is a hierarchical task network (HTN) style planner, achieving each goal via a series of subgoals. APE’s high-level planning loop alternates between waiting for user input and planning responses. It executes plan operators until a primitive, non-decomposable one 40 is obtained. In addition to elicit and inform, plans different from other algorithms they had learned, can also include primitives to query and update they had no trouble learning t</context>
</contexts>
<marker>Freedman, 2000</marker>
<rawString>Freedman, R. (2000). Using a Reactive Planner as the Basis for a Dialogue Agent. In Proceedings of the Thirteenth Florida Artificial Intelligence Research Symposium (FLAIRS 2000), Orlando.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Greenberg</author>
</authors>
<title>Some universals of grammar with particular reference to the order of meaningful elements.</title>
<date>1966</date>
<booktitle>In Universals of language,</booktitle>
<pages>73--113</pages>
<editor>ed. J. Greenberg,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<note>2nd ed.</note>
<contexts>
<context position="10148" citStr="Greenberg, 1966" startWordPosition="1573" endWordPosition="1574"> for corpora they preferred, including the works of Shakespeare and an online interface to the British National Corpus (http://sara.natcorp.ox.ac.uk/lookup.html). 4 Syntax and language universals 4.1 Background The second section of the course had as its goal to teach the students some basic aspects of syntax. I started with parts of speech and basic concepts of context-free grammars. I then introduced unification grammars as a way of obtaining more power with fewer rules. As a way of showing the syntactic variation among languages, I also introduced some of Greenberg’s word order universals (Greenberg, 1966), following the exposition in Baker (2001). Although identifying the most probable underlying word order (SVO, etc.) of an unknown language can involve significant linguistic intuition, I did not expect students to achieve that goal. Rather, I used Greenberg’s ideas to make students think 39 about the rules they were generating instead of generating S --&gt; NP VP by rote. Additionally, the use of multiple languages contributed to the university’s goal of introducing ideas of internationalization and diversity in classes where feasible. 4.2 Assignment The students were asked to prepare a 15-minut</context>
</contexts>
<marker>Greenberg, 1966</marker>
<rawString>Greenberg, J. (1966). Some universals of grammar with particular reference to the order of meaningful elements. In Universals of language, ed. J. Greenberg, pp. 73–113. Cambridge, MA: MIT Press. 2nd ed.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kernighan</author>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>In COLING ’90 (Helsinki),</booktitle>
<volume>2</volume>
<pages>205--211</pages>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>Kernighan, M., Church, K., and Gale, W. (1990). A spelling correction program based on a noisy channel model. In COLING ’90 (Helsinki), v. 2, pp. 205–211. Available online from the ACL archive at http://acl.ldc.upenn.edu/C/C90/C90-2036.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koedinger</author>
</authors>
<title>The Student is Not Like Me.</title>
<date>2001</date>
<booktitle>In Tenth International Conference on Artificial Intelligence in Education (AI-ED</booktitle>
<contexts>
<context position="5078" citStr="Koedinger (2001)" startWordPosition="768" endWordPosition="769">doesn’t have onerous prerequisites. In general, the international students have good to near-native competence in spoken English, although a few cannot not follow my lectures, and some do not have sufficient writing skills for an essay exam. All could read my lecture notes without difficulty. Both among the international students and local students, many do not have sufficient experience with formal academic prose to understand the textbook (Jurafsky and Martin, 2000). Students’ first languages have included Telugu, Hindi/Urdu, Nepali, Chinese (Mandarin), and Bulgarian. 2.2 Student background Koedinger (2001), in his research on tutoring systems for high school mathematics, gives the following as his fundamental principle: “the student is not like me.” In particular, student background frequently did not include the following items: 1) Parts of speech 2) Basic English grammar 3) Relationships between languages and language families 4) Practical issues, such as the importance of transliteration and glossing 5) Philosophical issues, such as the fact that there is no single authoritative grammar of a natural language or that one language is not more difficult than another in an absolute sense However</context>
</contexts>
<marker>Koedinger, 2001</marker>
<rawString>Koedinger, K. (2001). The Student is Not Like Me. In Tenth International Conference on Artificial Intelligence in Education (AI-ED 2001). San Antonio, TX. Keynote address. Slides available online at http://www.itsconference.org/content/seminars.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mount</author>
</authors>
<title>Cranky Consumer: Testing Online Service Reps.</title>
<date>2005</date>
<journal>Wall Street Journal, Feb.</journal>
<volume>1</volume>
<contexts>
<context position="13618" citStr="Mount, 2005" startWordPosition="2111" endWordPosition="2112">nce some of my previous work involves implementing dialogue software using a logicbased approach, dialogue systems was a natural choice for this segment. Phenomena discussed in lecture included the concepts of speech act and discourse intention, the relationship between syntactic form and intention, direct and indirect speech acts, and a short introduction to dialogue act classification. As a counterbalance to the more theoretical material from Greenberg, this section included some information about current commercial uses of NLP. Students were asked to read an article from the popular press (Mount, 2005) describing experiences with currently available commercial systems. I used my own software, APE (Freedman, 2000), a domain-independent dialogue plan interpreter based on reactive planning concepts. APE uses a rule-based macro language implemented in Common Lisp. It is a hierarchical task network (HTN) style planner, achieving each goal via a series of subgoals. APE’s high-level planning loop alternates between waiting for user input and planning responses. It executes plan operators until a primitive, non-decomposable one 40 is obtained. In addition to elicit and inform, plans different from </context>
</contexts>
<marker>Mount, 2005</marker>
<rawString>Mount, I. (2005). Cranky Consumer: Testing Online Service Reps. Wall Street Journal, Feb. 1, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>