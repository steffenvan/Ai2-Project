<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000328">
<title confidence="0.995239">
Probabilistic Soft Logic for Semantic Textual Similarity
</title>
<author confidence="0.997735">
Islam Beltagy§ Katrin Erk† Raymond Mooney§
</author>
<affiliation confidence="0.998808333333333">
§Department of Computer Science
†Department of Linguistics
The University of Texas at Austin
</affiliation>
<address confidence="0.755536">
Austin, Texas 78712
</address>
<email confidence="0.9958535">
§{beltagy,mooney}@cs.utexas.edu
†katrin.erk@mail.utexas.edu
</email>
<sectionHeader confidence="0.993579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996758">
Probabilistic Soft Logic (PSL) is a re-
cently developed framework for proba-
bilistic logic. We use PSL to combine
logical and distributional representations
of natural-language meaning, where distri-
butional information is represented in the
form of weighted inference rules. We ap-
ply this framework to the task of Seman-
tic Textual Similarity (STS) (i.e. judg-
ing the semantic similarity of natural-
language sentences), and show that PSL
gives improved results compared to a pre-
vious approach based on Markov Logic
Networks (MLNs) and a purely distribu-
tional approach.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997180234375">
When will people say that two sentences are sim-
ilar? This question is at the heart of the Semantic
Textual Similarity task (STS)(Agirre et al., 2012).
Certainly, if two sentences contain many of the
same words, or many similar words, that is a good
indication of sentence similarity. But that can be
misleading. A better characterization would be to
say that if two sentences use the same or similar
words in the same or similar relations, then those
two sentences will be judged similar.1 Interest-
ingly, this characterization echoes the principle of
compositionality, which states that the meaning of
a phrase is uniquely determined by the meaning of
its parts and the rules that connect those parts.
Beltagy et al. (2013) proposed a hybrid ap-
proach to sentence similarity: They use a very
1Mitchell and Lapata (2008) give an amusing example of
two sentences that consist of all the same words, but are very
different in their meaning: (a) It was not the sales manager
who hit the bottle that day, but the office worker with the
serious drinking problem. (b) That day the office manager,
who was drinking, hit the problem sales worker with a bottle,
but it was not serious.
deep representation of sentence meaning, ex-
pressed in first-order logic, to capture sentence
structure, but combine it with distributional sim-
ilarity ratings at the word and phrase level. Sen-
tence similarity is then modelled as mutual entail-
ment in a probabilistic logic. This approach is in-
teresting in that it uses a very deep and precise
representation of meaning, which can then be re-
laxed in a controlled fashion using distributional
similarity. But the approach faces large hurdles
in practice, stemming from efficiency issues with
the Markov Logic Networks (MLN) (Richardson
and Domingos, 2006) that they use for performing
probabilistic logical inference.
In this paper, we use the same combined logic-
based and distributional framework as Beltagy et
al., (2013) but replace Markov Logic Networks
with Probabilistic Soft Logic (PSL) (Kimmig et
al., 2012; Bach et al., 2013). PSL is a proba-
bilistic logic framework designed to have efficient
inference. Inference in MLNs is theoretically in-
tractable in the general case, and existing approxi-
mate inference algorithms are computationally ex-
pensive and sometimes inaccurate. Consequently,
the MLN approach of Beltagy et al. (2013) was
unable to scale to long sentences and was only
tested on the relatively short sentences in the Mi-
crosoft video description corpus used for STS
(Agirre et al., 2012). On the other hand, inference
in PSL reduces to a linear programming problem,
which is theoretically and practically much more
efficient. Empirical results on a range of prob-
lems have confirmed that inference in PSL is much
more efficient than in MLNs, and frequently more
accurate (Kimmig et al., 2012; Bach et al., 2013).
We show how to use PSL for STS, and describe
changes to the PSL framework that make it more
effective for STS. For evaluation, we test on three
STS datasets, and compare our PSL system with
the MLN approach of Beltagy et al., (2013) and
with distributional-only baselines. Experimental
</bodyText>
<page confidence="0.892794">
1210
</page>
<note confidence="0.828349">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1210–1219,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998023">
results demonstrate that, overall, PSL models hu-
man similarity judgements more accurately than
these alternative approaches, and is significantly
faster than MLNs.
The rest of the paper is organized as follows:
section 2 presents relevant background material,
section 3 explains how we adapted PSL for the
STS task, section 4 presents the evaluation, and
sections 5 and 6 discuss future work and conclu-
sions, respectively.
</bodyText>
<sectionHeader confidence="0.996287" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.99807">
2.1 Logical Semantics
</subsectionHeader>
<bodyText confidence="0.999981352941177">
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, their binary
nature prevents them from capturing the “graded”
aspects of meaning in language. Also, it is difficult
to construct formal ontologies of properties and re-
lations that have broad coverage, and semantically
parsing sentences into logical expressions utilizing
such an ontology is very difficult. Consequently,
current semantic parsers are mostly restricted to
quite limited domains, such as querying a specific
database (Kwiatkowski et al., 2013; Berant et al.,
2013). In contrast, our system is not limited to any
formal ontology and can use a wide-coverage tool
for semantic analysis, as discussed below.
</bodyText>
<subsectionHeader confidence="0.998734">
2.2 Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.999952419354839">
Distributional models (Turney and Pantel, 2010),
on the other hand, use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Landauer and Du-
mais, 1997; Mitchell and Lapata, 2010). They are
relatively easier to build than logical representa-
tions, automatically acquire knowledge from “big
data,” and capture the “graded” nature of linguis-
tic meaning, but do not adequately capture logical
structure (Grefenstette, 2013).
Distributional models are motivated by the ob-
servation that semantically similar words occur in
similar contexts, so words can be represented as
vectors in high dimensional spaces generated from
the contexts in which they occur (Landauer and
Dumais, 1997; Lund and Burgess, 1996). Such
models have also been extended to compute vec-
tor representations for larger phrases, e.g. by
adding the vectors for the individual words (Lan-
dauer and Dumais, 1997) or by a component-wise
product of word vectors (Mitchell and Lapata,
2008; Mitchell and Lapata, 2010), or more com-
plex methods that compute phrase vectors from
word vectors and tensors (Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). We use
vector addition (Landauer and Dumais, 1997), and
component-wise product (Mitchell and Lapata,
2008) as baselines for STS. Vector addition was
previously found to be the best performing sim-
ple distributional method for STS (Beltagy et al.,
2013).
</bodyText>
<subsectionHeader confidence="0.999633">
2.3 Markov Logic Networks
</subsectionHeader>
<bodyText confidence="0.999863619047619">
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints
and thereby allowing situations in which not all
clauses are satisfied. MLNs define a probability
distribution over possible worlds, where a world’s
probability increases exponentially with the to-
tal weight of the logical clauses that it satisfies.
A variety of inference methods for MLNs have
been developed, however, developing a scalable,
general-purpose, accurate inference method for
complex MLNs is an open problem. Beltagy et
al. (2013) use MLNs to represent the meaning of
natural language sentences and judge textual en-
tailment and semantic similarity, but they were un-
able to scale the approach beyond short sentences
due to the complexity of MLN inference.
</bodyText>
<subsectionHeader confidence="0.998826">
2.4 Probabilistic Soft Logic
</subsectionHeader>
<bodyText confidence="0.999389333333333">
Probabilistic Soft Logic (PSL) is a recently pro-
posed alternative framework for probabilistic logic
(Kimmig et al., 2012; Bach et al., 2013). It uses
logical representations to compactly define large
graphical models with continuous variables, and
includes methods for performing efficient proba-
bilistic inference for the resulting models. A key
distinguishing feature of PSL is that ground atoms
have soft, continuous truth values in the interval
[0, 1] rather than binary truth values as used in
MLNs and most other probabilistic logics. Given
a set of weighted logical formulas, PSL builds a
graphical model defining a probability distribution
over the continuous space of values of the random
variables in the model.
</bodyText>
<page confidence="0.98203">
1211
</page>
<bodyText confidence="0.998306857142857">
A PSL model is defined using a set of weighted
if-then rules in first-order logic, as in the following
example:
Using distance to satisfaction, PSL defines a
probability distribution over all possible interpre-
tations I of all ground atoms. The pdf is defined
as follows:
</bodyText>
<equation confidence="0.9461975">
∀x, y, z. friend(x, y) ∧ votesFor(y, z) → 1 p(I) = Z exp [− λr(d(r))p]; (3)
votesFor(x, z)  |0.3 (1) r∈R λr(d(r))p]
∀x, y, z. spouse(x, y) ∧ votesFor(y, z) → Z = Jexp [−
votesFor(x, z)  |0.8 (2) I r∈R
</equation>
<bodyText confidence="0.998892333333333">
In our notation, we use lower case letters like
x, y, z to represent variables and upper case let-
ters for constants. The first rule states that a per-
son is likely to vote for the same person as his/her
friend. The second rule encodes the same regular-
ity for a person’s spouse. The weights encode the
knowledge that a spouse’s influence is greater than
a friend’s in this regard.
In addition, PSL includes similarity functions.
Similarity functions take two strings or two sets as
input and return a truth value in the interval [0, 1]
denoting the similarity of the inputs. For example,
in our application, we generate inference rules that
incorporate the similarity of two predicates. This
can be represented in PSL as:
</bodyText>
<equation confidence="0.9987895">
∀x. similarity(“predicate1”, “predicate2”) ∧
predicate1(x) → predicate2(x)
</equation>
<bodyText confidence="0.988421166666667">
As mentioned above, each ground atom, a,
has a soft truth value in the interval [0, 1],
which is denoted by I(a). To compute soft truth
values for logical formulas, Lukasiewicz’s re-
laxation of conjunctions(∧), disjunctions(∨) and
negations(¬) are used:
</bodyText>
<equation confidence="0.999995333333333">
I(l1 ∧ l1) = max{0,I(l1) + I(l2) − 1}
I(l1 ∨ l1) = min{I(l1) + I(l2),1}
I(¬l1) = 1 − I(l1)
</equation>
<bodyText confidence="0.867014911764706">
Then, a given rule r ≡ rbody → rhead, is said to be
satisfied (i.e. I(r) = 1) iff I(rbody) ≤ I(rhead).
Otherwise, PSL defines a distance to satisfaction
d(r) which captures how far a rule r is from being
satisfied: d(r) = max{0, I(rbody) − I(rhead)}.
For example, assume we have the set of evidence:
I(spouse(B,A)) = 1, I(votesFor(A,P)) =
0.9, I(votesFor(B, P)) = 0.3, and that r
is the resulting ground instance of rule (2).
Then I(spouse(B, A) ∧ votesFor(A, P)) =
max{0,1 + 0.9 − 1} = 0.9, and d(r) =
max{0, 0.9 − 0.3} = 0.6.
where Z is the normalization constant, λr is the
weight of rule r, R is the set of all rules, and p ∈
{1, 2} provides two different loss functions. For
our application, we always use p = 1
PSL is primarily designed to support MPE in-
ference (Most Probable Explanation). MPE infer-
ence is the task of finding the overall interpretation
with the maximum probability given a set of evi-
dence. Intuitively, the interpretation with the high-
est probability is the interpretation with the lowest
distance to satisfaction. In other words, it is the
interpretation that tries to satisfy all rules as much
as possible. Formally, from equation 3, the most
probable interpretation, is the one that minimizes
Er∈R λr(d(r))p. In case of p = 1, and given
that all d(r) are linear equations, then minimizing
the sum requires solving a linear program, which,
compared to inference in other probabilistic logics
such as MLNs, can be done relatively efficiently
using well-established techniques. In case p = 2,
MPE inference can be shown to be a second-order
cone program (SOCP) (Kimmig et al., 2012).
</bodyText>
<subsectionHeader confidence="0.996467">
2.5 Semantic Textual Similarity
</subsectionHeader>
<bodyText confidence="0.999890222222222">
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 0 to 5, and was recently introduced
as a SemEval task (Agirre et al., 2012). Gold
standard scores are averaged over multiple hu-
man annotations and systems are evaluated using
the Pearson correlation between a system’s out-
put and gold standard scores. The best perform-
ing system in 2012’s competition was by B¨ar et
al. (2012), a complex ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics. Two of the
datasets we use for evaluation are from the 2012
competition. We did not utilize the new datasets
added in the 2013 competition since they did not
contain naturally-occurring, full sentences, which
is the focus of our work.
</bodyText>
<page confidence="0.965095">
1212
</page>
<subsectionHeader confidence="0.972384">
2.6 Combining logical and distributional
methods using probabilistic logic
</subsectionHeader>
<bodyText confidence="0.999948307692308">
There are a few recent attempts to combine log-
ical and distributional representations in order to
obtain the advantages of both. Lewis and Steed-
man (2013) use distributional information to deter-
mine word senses, but still produce a strictly log-
ical semantic representation that does not address
the “graded” nature of linguistic meaning that is
important to measuring semantic similarity.
Garrette et al. (2011) introduced a framework
for combining logic and distributional models us-
ing probabilistic logic. Distributional similarity
between pairs of words is converted into weighted
inference rules that are added to the logical repre-
sentation, and Markov Logic Networks are used to
perform probabilistic logical inference.
Beltagy et al. (2013) extended this framework
by generating distributional inference rules from
phrase similarity and tailoring the system to the
STS task. STS is treated as computing the prob-
ability of two textual entailments T |= H and
H |= T, where T and H are the two sentences
whose similarity is being judged. These two en-
tailment probabilities are averaged to produce a
measure of similarity. The MLN constructed to
determine the probability of a given entailment
includes the logical forms for both T and H as
well as soft inference rules that are constructed
from distributional information. Given a similar-
ity score for all pairs of sentences in the dataset,
a regressor is trained on the training set to map
the system’s output to the gold standard scores.
The trained regressor is applied to the scores in
the test set before calculating Pearson correlation.
The regression algorithm used is Additive Regres-
sion (Friedman, 2002).
To determine an entailment probability, first,
the two sentences are mapped to logical repre-
sentations using Boxer (Bos, 2008), a tool for
wide-coverage semantic analysis that maps a CCG
(Combinatory Categorial Grammar) parse into a
lexically-based logical form. Boxer uses C&amp;C for
CCG parsing (Clark and Curran, 2004).
Distributional semantic knowledge is then en-
coded as weighted inference rules in the MLN.
A rule’s weight (w) is a function of the cosine
similarity (sim) between its antecedent and con-
sequent. Rules are generated on the fly for each
T and H. Let t and h be the lists of all words
and phrases in T and H respectively. For all
pairs (a, b), where a ∈ t, b ∈ h, it generates
an inference rule: a → b  |w, where w =
f(sim(−→a , →−b )). Both a and b can be words or
phrases. Phrases are defined in terms of Boxer’s
output. A phrase is more than one unary atom
sharing the same variable like “a little kid” which
in logic is little(K) ∧ kid(K). A phrase also can
be two unary atoms connected by a relation like
“a man is driving” which in logic is man(M) ∧
agent(D, M) ∧ drive(D). The similarity func-
tion sim takes two vectors as input. Phrasal vec-
tors are constructed using Vector Addition (Lan-
dauer and Dumais, 1997). The set of generated
inference rules can be regarded as the knowledge
base KB.
Beltagy et al. (2013) found that the logical con-
junction in H is very restrictive for the STS task,
so they relaxed the conjunction by using an aver-
age evidence combiner (Natarajan et al., 2010).
The average combiner results in computationally
complex inference and only works for short sen-
tences. In case inference breaks or times-out, they
back off to a simpler combiner that leads to much
faster inference but loses most of the structure of
the sentence and is therefore less accurate.
Given T, KB and H from the previous
steps, MLN inference is then used to compute
p(H|T, KB), which is then used as a measure of
the degree to which T entails H.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="method">
3 PSL for STS
</sectionHeader>
<bodyText confidence="0.99988765">
For several reasons, we believe PSL is a more ap-
propriate probabilistic logic for STS than MLNs.
First, it is explicitly designed to support efficient
inference, therefore it scales better to longer sen-
tences with more complex logical forms. Sec-
ond, it was also specifically designed for com-
puting similarity between complex structured ob-
jects rather than determining probabilistic logical
entailment. In fact, the initial version of PSL
(Broecheler et al., 2010) was called Probabilis-
tic Similarity Logic, based on its use of similar-
ity functions. This initial version was shown to be
very effective for measuring the similarity of noisy
database records and performing record linkage
(i.e. identifying database entries referring to the
same entity, such as bibliographic citations refer-
ring to the same paper). Therefore, we have devel-
oped an approach that follows that of Beltagy et
al. (2013), but replaces Markov Logic with PSL.
This section explains how we formulate the STS
</bodyText>
<page confidence="0.930589">
1213
</page>
<bodyText confidence="0.999959571428571">
task as a PSL program. PSL does not work very
well “out of the box” for STS, mainly because
Lukasiewicz’s equation for the conjunction is very
restrictive. Therefore, we use a different interpre-
tation for conjunction that uses averaging, which
requires corresponding changes to the optimiza-
tion problem and the grounding technique.
</bodyText>
<subsectionHeader confidence="0.992816">
3.1 Representation
</subsectionHeader>
<bodyText confidence="0.994791909090909">
Given the logical forms for a pair of sentences,
a text T and a hypothesis H, and given a set of
weighted rules derived from the distributional se-
mantics (as explained in section 2.6) composing
the knowledge base KB, we build a PSL model
that supports determining the truth value of H in
the most probable interpretation (i.e. MPE) given
T and KB.
Consider the pair of sentences is “A man is driv-
ing”, and “A guy is walking”. Parsing into logical
form gives:
</bodyText>
<equation confidence="0.948373333333333">
T : Ix, y. man(x) n agent(y, x) n drive(y)
H : Ix, y. guy(x) n agent(y, x) n walk(y)
The PSL program is constructed as follows:
</equation>
<bodyText confidence="0.969962205882353">
T: The text is represented in the evidence set. For
the example, after Skolemizing the existential
quantifiers, this contains the ground atoms:
{man(A), agent(B, A), drive(B)}
KB: The knowledge base is a set of lexical and
phrasal rules generated from distributional
semantics, along with a similarity score for
each rule (section 2.6). For the exam-
ple, we generate the rules: bx. man(x) n
vs sim(“man”, “guy”) —* guy(x) ,
bx.drive(x)nvs sim(“drive”, “walk”) —*
walk(x)
where vs sim is a similarity function that
calculates the distributional similarity score
between the two lexical predicates. All rules
are assigned the same weight because all
rules are equally important.
H: The hypothesis is represented as H —*
result(), and then PSL is queried for the
truth value of the atom result(). For
our example, the rule is: bx, y. guy(x) n
agent(y, x) n walk(y) —* result().
Priors: A low prior is given to all predicates. This
encourages the truth values of ground atoms
to be zero, unless there is evidence to the con-
trary.
For each STS pair of sentences 51, 52, we run
PSL twice, once where T = 51, H = 52 and
another where T = 52, H = 51, and output the
two scores. To produce a final similarity score, we
train a regressor to learn the mapping between the
two PSL scores and the overall similarity score.
As in Beltagy et al., (2013) we use Additive Re-
gression (Friedman, 2002).
</bodyText>
<subsectionHeader confidence="0.999527">
3.2 Changing Conjunction
</subsectionHeader>
<bodyText confidence="0.99884447826087">
As mentioned above, Lukasiewicz’s formula for
conjunction is very restrictive and does not work
well for STS. For example, for T: “A man is driv-
ing” and H: “A man is driving a car”, if we use the
standard PSL formula for conjunction, the output
value is zero because there is no evidence for a car
and max(0, X + 0 − 1) = 0 for any truth value
0 &lt; X &lt; 1. However, humans find these sen-
tences to be quite similar.
Therefore, we introduce a new averaging inter-
pretation of conjunction that we use for the hy-
pothesis H. The truth value for a conjunction
is defined as I(p1 n .... n pn) = n En 1i=(pi).
This averaging function is linear, and the result is
a valid truth value in the interval [0, 1], therefore
this change is easily incorporated into PSL with-
out changing the complexity of inference which
remains a linear-programming problem.
It would perhaps be even better to use a
weighted average, where weights for different
components are learned from a supervised train-
ing set. This is an important direction for future
work.
</bodyText>
<subsectionHeader confidence="0.994199">
3.3 Grounding Process
</subsectionHeader>
<bodyText confidence="0.999971071428572">
Grounding is the process of instantiating the vari-
ables in the quantified rules with concrete con-
stants in order to construct the nodes and links in
the final graphical model. In principle, ground-
ing requires instantiating each rule in all possible
ways, substituting every possible constant for each
variable in the rule. However, this is a combinato-
rial process that can easily result in an explosion in
the size of the final network. Therefore, PSL em-
ploys a “lazy” approach to grounding that avoids
the construction of irrelevant groundings. If there
is no evidence for one of the antecedents in a par-
ticular grounding of a rule, then the normal PSL
formula for conjunction guarantees that the rule is
</bodyText>
<page confidence="0.988165">
1214
</page>
<bodyText confidence="0.945241428571429">
Algorithm 1 Heuristic Grounding
Input: rbody = a1 n .... n an: antecedent of a rule
with average interpretation of conjunction
Input: V : set of variables used in rbody
Input: Ant(vi): subset of antecedents aj con-
taining variable vi
Input: Const(vi): list of possible constants of
variable vi
Input: Gnd(ai): set of ground atoms of ai.
Input: GndConst(a, g, v): takes an atom a,
grounding g for a, and variable v, and returns
the constant that substitutes v in g
Input: gnd limit: limit on the number of
groundings
</bodyText>
<listItem confidence="0.920178181818182">
1: for all vi E V do
2: for all C E Const(vi) do
score(C) = �
3: a∈Ant(vi)(max I(g))
for g E Gnd(a) n GndConst(a, g, vi) = C
4: end for
5: sort Const(vi) on scores, descending
6: end for
7: return For all vi E V , take the Cartesian-
product of the sorted Const(vi) and return the
top gnd limit results
</listItem>
<bodyText confidence="0.981022">
trivially satisfied (I(r) = 1) since the truth value
of the antecedent is zero. Therefore, its distance to
satisfaction is also zero, and it can be omitted from
the ground network without impacting the result of
MPE inference.
However, this technique does not work once
we switch to using averaging to interpret conjunc-
tions. For example, given the rule bx. p(x) n
q(x) —* t() and only one piece of evidence p(C)
there are no relevant groundings because there is
no evidence for q(C), and therefore, for normal
PSL, I(p(C) n q(C)) = 0 which does not affect
I(t()). However, when using averaging with the
same evidence, we need to generate the grounding
p(C)nq(C) because I(p(C)nq(C)) = 0.5 which
does affect I(t()).
One way to solve this problem is to eliminate
lazy grounding and generate all possible ground-
ings. However, this produces an intractably large
network. Therefore, we developed a heuristic ap-
proximate grounding technique that generates a
subset of the most impactful groundings.
Pseudocode for this heuristic approach is shown
in algorithm 1. Its goal is to find constants that
participate in ground propositions with high truth
value and preferentially use them to construct a
limited number of groundings of each rule.
The algorithm takes the antecedents of a rule
employing averaging conjunction as input. It also
takes the grounding limit which is a threshold on
the number of groundings to be returned. The al-
gorithm uses several subroutines, they are:
</bodyText>
<listItem confidence="0.986336133333333">
• Ant(vi): given a variable vi, it returns the set
of rule antecedent atoms containing vi. E.g,
for the rule: a(x) n b(y) n c(x), Ant(x) re-
turns the set of atoms {a(x), c(x)1.
• Const(vi): given a variable vi, it returns the
list of possible constants that can be used to
instantiate the variable vi.
• Gnd(ai): given an atom ai, it returns the set
of all possible ground atoms generated for ai.
• GndConst(a, g, v): given an atom a and
grounding g for a, and a variable v, it finds
the constant that substitutes for v in g. E.g,
assume there is an atom a = ai(v1, v2), and
the ground atom g = ai(A, B) is one of its
groundings. GndConst(a, g, v2) would re-
</listItem>
<bodyText confidence="0.996552185185185">
turn the constant B since it is the substitution
for the variable v2 in g.
Lines 1-6 loop over all variables in the rule. For
each variable, lines 2-5 construct a list of constants
for that variable and sort it based on a heuristic
score. In line 3, each constant is assigned a score
that indicates the importance of this constant in
terms of its impact on the truth value of the overall
grounding. A constant’s score is the sum, over all
antedents that contain the variable in question, of
the maximum truth value of any grounding of that
antecedent that contains that constant.
Pushing constants with high scores to the top
of each variable’s list will tend to make the over-
all truth value of the top groundings high. Line
7 computes a subset of the Cartesian product of
the sorted lists of constants, selecting constants in
ranked order and limiting the number of results to
the grounding limit.
One point that needs to be clarified about this
approach is how it relies on the truth values of
ground atoms when the goal of inference is to ac-
tually find these values. PSL’s inference is ac-
tually an iterative process where in each itera-
tion a grounding phase is followed by an opti-
mization phase (solving the linear program). This
loop repeats until convergence, i.e. until the truth
</bodyText>
<page confidence="0.977734">
1215
</page>
<bodyText confidence="0.9998822">
values stop changing. The truth values used in
each grounding phase come from the previous op-
timization phase. The first grounding phase as-
sumes only the propositions in the evidence pro-
vided have non-zero truth values.
</bodyText>
<sectionHeader confidence="0.997619" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999077">
This section evaluates the performance of PSL on
the STS task.
</bodyText>
<subsectionHeader confidence="0.94818">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999211">
We evaluate our system on three STS datasets.
</bodyText>
<listItem confidence="0.998748555555556">
• msr-vid: Microsoft Video Paraphrase Cor-
pus from STS 2012. The dataset consists
of 1,500 pairs of short video descriptions
collected using crowdsourcing (Chen and
Dolan, 2011) and subsequently annotated for
the STS task (Agirre et al., 2012). Half of
the dataset is for training, and the second half
is for testing.
• msr-par: Microsoft Paraphrase Corpus from
</listItem>
<bodyText confidence="0.998161142857143">
STS 2012 task. The dataset is 5,801
pairs of sentences collected from news
sources (Dolan et al., 2004). Then, for STS
2012, 1,500 pairs were selected and anno-
tated with similarity scores. Half of the
dataset is for training, and the second half is
for testing.
</bodyText>
<listItem confidence="0.980932">
• SICK: Sentences Involving Compositional
</listItem>
<bodyText confidence="0.907851571428571">
Knowledge is a dataset collected for SemEval
2014. Only the training set is available at this
point, which consists of 5,000 pairs of sen-
tences. Pairs are annotated for RTE and STS,
but we only use the STS data. Training and
testing was done using 10-fold cross valida-
tion.
</bodyText>
<subsectionHeader confidence="0.998865">
4.2 Systems Compared
</subsectionHeader>
<bodyText confidence="0.9994625">
We compare our PSL system with several others.
In all cases, we use the distributional word vec-
tors employed by Beltagy et al. (2013) based on
context windows from Gigaword.
</bodyText>
<listItem confidence="0.91235495">
• vec-add: Vector Addition (Landauer and
Dumais, 1997). We compute a vector rep-
resentation for each sentence by adding the
distributional vectors of all of its words and
measure similarity using cosine. This is a
simple yet powerful baseline that uses only
distributional information.
• vec-mul: Component-wise Vector Multipli-
cation (Mitchell and Lapata, 2008). The
same as vec-add except uses component-
wise multiplication to combine word vectors.
• MLN: The system of Beltagy et al. (2013),
which uses Markov logic instead of PSL for
probabilistic inference. MLN inference is
very slow in some cases, so we use a 10
minute timeout. When MLN times out, it
backs off to a simpler sentence representation
as explained in section 2.6.
• PSL: Our proposed PSL system for combin-
ing logical and distributional information.
• PSL-no-DIR: Our PSL system without dis-
tributional inference rules(empty knowledge
base). This system uses PSL to compute sim-
ilarity of logical forms but does not use dis-
tributional information on lexical or phrasal
similarity. It tests the impact of the proba-
bilistic logic only
• PSL+vec-add: PSL ensembled with vec-
add. Ensembling the MLN approach with a
purely distributional approach was found to
improve results (Beltagy et al., 2013), so we
also tried this with PSL. The methods are en-
sembled by using both entailment scores of
both systems as input features to the regres-
sion step that learns to map entailment scores
to STS similarity ratings. This way, the train-
ing data is used to learn how to weight the
contribution of the different components.
• PSL+MLN: PSL ensembled with MLN in
the same manner.
</listItem>
<subsectionHeader confidence="0.993357">
4.3 Experiments
</subsectionHeader>
<bodyText confidence="0.999340333333333">
Systems are evaluated on two metrics, Pearson
correlation and average CPU time per pair of sen-
tences.
</bodyText>
<listItem confidence="0.9643626">
• Pearson correlation: The Pearson correlation
between the system’s similarity scores and
the human gold-standards.
• CPU time: This metric only applies to MLN
and PSL. The CPU time taken by the infer-
ence step is recorded and averaged over all
pairs in each of the test datasets. In many
cases, MLN inference is very slow, so we
timeout after 10 minutes and report the num-
ber of timed-out pairs on each dataset.
</listItem>
<page confidence="0.93475">
1216
</page>
<table confidence="0.9996127">
msr-vid msr-par SICK
vec-add 0.78 0.24 0.65
vec-mul 0.76 0.12 0.62
MLN 0.63 0.16 0.47
PSL-no-DIR 0.74 0.46 0.68
PSL 0.79 0.53 0.70
PSL+vec-add 0.83 0.49 0.71
PSL+MLN 0.79 0.51 0.70
Best Score (B¨ar 0.87 0.68 n/a
et al., 2012)
</table>
<tableCaption confidence="0.927546">
Table 1: STS Pearson Correlations
</tableCaption>
<table confidence="0.9995766">
PSL MLN
time time timeouts/total
msr-vid 8s 1m 31s 132/1500
msr-par 30s 11m 49s 1457/1500
SICK 10s 4m 24s 1791/5000
</table>
<tableCaption confidence="0.994348">
Table 2: Average CPU time per STS pair, and
</tableCaption>
<bodyText confidence="0.992258428571429">
number of timed-out pairs in MLN with a 10
minute time limit. PSL’s grounding limit is set to
10,000 groundings.
We also evaluated the effect of changing the
grounding limit on both Pearson correlation and
CPU time for the msr-par dataset. Most of the
sentences in msr-par are long, which results is
large number of groundings, and limiting the num-
ber of groundings has a visible effect on the over-
all performance. In the other two datasets, the
sentences are fairly short, and the full number of
groundings is not large; therefore, changing the
grounding limit does not significantly affect the re-
sults.
</bodyText>
<subsectionHeader confidence="0.976922">
4.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999476857142857">
Table 1 shows the results for Pearson correlation.
PSL out-performs the purely distributional base-
lines (vec-add and vec-mul) because PSL is able
to combine the information available to vec-add
and vec-mul in a better way that takes sentence
structure into account. PSL also outperforms
the unaided probabilistic-logic baseline that does
not use distributional information (PSL-no-DIR).
PSL-no-DIR works fairly well because there is
significant overlap in the exact words and struc-
ture of the paired sentences in the test data, and
PSL combines the evidence from these similari-
ties effectively. In addition, PSL always does sig-
nificantly better than MLN, because of the large
</bodyText>
<figureCaption confidence="0.9773755">
Figure 1: Effect of PSL’s grounding limit on the
correlation score for the msr-par dataset
</figureCaption>
<bodyText confidence="0.99996840625">
number of timeouts, and because the conjunction-
averaging in PSL is combining evidence bet-
ter than MLN’s average-combiner, whose perfor-
mance is sensitive to various parameters. These
results further support the claim that using prob-
abilistic logic to integrate logical and distribu-
tional information is a promising approach to
natural-language semantics. More specifically,
they strongly indicate that PSL is a more effective
probabilistic logic for judging semantic similarity
than MLNs.
Like for MLNs (Beltagy et al., 2013), en-
sembling PSL with vector addition improved the
scores a bit, except for msr-par where vec-add’s
performance is particularly low. However, this en-
semble still does not beat the state of the art (B¨ar et
al., 2012) which is a large ensemble of many dif-
ferent systems. It would be informative to add our
system to their ensemble to see if it could improve
it even further.
Table 2 shows the CPU time for PSL and MLN.
The results clearly demonstrate that PSL is an or-
der of magnitude faster than MLN.
Figures 1 and 2 show the effect of changing the
grounding limit on Pearson correlation and CPU
time. As expected, as the grounding limit is in-
creased, accuracy improves but CPU time also
increases. However, note that the difference in
scores between the smallest and largest grounding
limit tested is not large, suggesting that the heuris-
tic approach to limiting grounding is quite effec-
tive.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.9962505">
As mentioned in Section 3.2, it would be good
to use a weighted average to compute the truth
</bodyText>
<page confidence="0.989595">
1217
</page>
<sectionHeader confidence="0.866123" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<figureCaption confidence="0.745867">
Figure 2: Effect of PSL’s grounding limit on CPU
time for the msr-par dataset
</figureCaption>
<bodyText confidence="0.99992738">
values for conjunctions, weighting some predi-
cates more than others rather than treating them
all equally. Appropriate weights for different com-
ponents could be learned from training data. For
example, such an approach could learn that the
type of an object determined by a noun should be
weighted more than a property specified by an ad-
jective. As a result, “black dog” would be appro-
priately judged more similar to “white dog” than
to “black cat.”
One of the advantages of using a probabilis-
tic logic is that additional sources of knowledge
can easily be incorporated by adding additional
soft inference rules. To complement the soft in-
ference rules capturing distributional lexical and
phrasal similarities, PSL rules could be added that
encode explicit paraphrase rules, such as those
mined from monolingual text (Berant et al., 2011)
or multi-lingual parallel text (Ganitkevitch et al.,
2013).
This paper has focused on STS; however, as
shown by Beltagy et al. (2013), probabilistic logic
is also an effective approach to recognizing tex-
tual entailment (RTE). By using the appropriate
functions to combine truth values for various log-
ical connectives, PSL could also be adapted for
RTE. Although we have shown that PSL outper-
forms MLNs on STS, we hypothesize that MLNs
may still be a better approach for RTE. However, it
would be good to experimentally confirm this in-
tuition. In any case, the high computational com-
plexity of MLN inference could mean that PSL is
still a more practical choice for RTE.
This paper has presented an approach that uses
Probabilistic Soft Logic (PSL) to determine Se-
mantic Textual Similarity (STS). The approach
uses PSL to effectively combine logical seman-
tic representations of sentences with soft infer-
ence rules for lexical and phrasal similarities com-
puted from distributional information. The ap-
proach builds upon a previous method that uses
Markov Logic (MLNs) for STS, but replaces the
probabilistic logic with PSL in order to improve
the efficiency and accuracy of probabilistic infer-
ence. The PSL approach was experimentally eval-
uated on three STS datasets and was shown to out-
perform purely distributional baselines as well as
the MLN approach. The PSL approach was also
shown to be much more scalable and efficient than
using MLNs
</bodyText>
<sectionHeader confidence="0.998567" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99558075">
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
</bodyText>
<sectionHeader confidence="0.998409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998169571428571">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Stephen H. Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss Markov random fields:
Convex inference for structured prediction. In Pro-
ceedings of Uncertainty in Artificial Intelligence
(UAI-13).
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of Semantic
Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
</reference>
<page confidence="0.780233">
1218
</page>
<reference confidence="0.999814689320389">
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Matthias Broecheler, Lilyana Mihalkova, and Lise
Getoor. 2010. Probabilistic Similarity Logic. In
Proceedings of Uncertainty in Artificial Intelligence
(UAI-20).
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the International Conference on
Computational Linguistics (COLING-04).
Jerome H Friedman. 2002. Stochastic gradient boost-
ing. Journal of Computational Statistics &amp; Data
Analysis (CSDA-02).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Dan Garrette, Katrin Erk, and Raymond Mooney.
2011. Integrating logical representations with prob-
abilistic information using Markov logic. In Pro-
ceedings of International Conference on Computa-
tional Semantics (IWCS-11).
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato’s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics
(TACL-13).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373–398.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik.
2010. Exploiting causal independence in Markov
logic networks: Combining undirected and directed
models. In Proceedings of European Conference in
Machine Learning (ECML-10).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107–136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
</reference>
<page confidence="0.994895">
1219
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.177418">
<title confidence="0.934647">Probabilistic Soft Logic for Semantic Textual Similarity</title>
<degree confidence="0.434543">of Computer of The University of Texas at Austin, Texas</degree>
<abstract confidence="0.9775720625">Probabilistic Soft Logic (PSL) is a recently developed framework for probabilistic logic. We use PSL to combine logical and distributional representations of natural-language meaning, where distributional information is represented in the form of weighted inference rules. We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of naturallanguage sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distributional approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of Semantic Evaluation (SemEval-12).</booktitle>
<contexts>
<context position="1010" citStr="Agirre et al., 2012" startWordPosition="144" endWordPosition="147">ine logical and distributional representations of natural-language meaning, where distributional information is represented in the form of weighted inference rules. We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of naturallanguage sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distributional approach. 1 Introduction When will people say that two sentences are similar? This question is at the heart of the Semantic Textual Similarity task (STS)(Agirre et al., 2012). Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approac</context>
<context position="3389" citStr="Agirre et al., 2012" startWordPosition="533" endWordPosition="536"> framework as Beltagy et al., (2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al., 2012; Bach et al., 2013). PSL is a probabilistic logic framework designed to have efficient inference. Inference in MLNs is theoretically intractable in the general case, and existing approximate inference algorithms are computationally expensive and sometimes inaccurate. Consequently, the MLN approach of Beltagy et al. (2013) was unable to scale to long sentences and was only tested on the relatively short sentences in the Microsoft video description corpus used for STS (Agirre et al., 2012). On the other hand, inference in PSL reduces to a linear programming problem, which is theoretically and practically much more efficient. Empirical results on a range of problems have confirmed that inference in PSL is much more efficient than in MLNs, and frequently more accurate (Kimmig et al., 2012; Bach et al., 2013). We show how to use PSL for STS, and describe changes to the PSL framework that make it more effective for STS. For evaluation, we test on three STS datasets, and compare our PSL system with the MLN approach of Beltagy et al., (2013) and with distributional-only baselines. Ex</context>
<context position="12168" citStr="Agirre et al., 2012" startWordPosition="1952" endWordPosition="1955">hat minimizes Er∈R λr(d(r))p. In case of p = 1, and given that all d(r) are linear equations, then minimizing the sum requires solving a linear program, which, compared to inference in other probabilistic logics such as MLNs, can be done relatively efficiently using well-established techniques. In case p = 2, MPE inference can be shown to be a second-order cone program (SOCP) (Kimmig et al., 2012). 2.5 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 0 to 5, and was recently introduced as a SemEval task (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. The best performing system in 2012’s competition was by B¨ar et al. (2012), a complex ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Two of the datasets we use for evaluation are from the 2012 competition. We did not utilize the new datasets added in the 2013 competition since they did not contain naturall</context>
<context position="26480" citStr="Agirre et al., 2012" startWordPosition="4423" endWordPosition="4426">. until the truth 1215 values stop changing. The truth values used in each grounding phase come from the previous optimization phase. The first grounding phase assumes only the propositions in the evidence provided have non-zero truth values. 4 Evaluation This section evaluates the performance of PSL on the STS task. 4.1 Datasets We evaluate our system on three STS datasets. • msr-vid: Microsoft Video Paraphrase Corpus from STS 2012. The dataset consists of 1,500 pairs of short video descriptions collected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al., 2012). Half of the dataset is for training, and the second half is for testing. • msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al., 2004). Then, for STS 2012, 1,500 pairs were selected and annotated with similarity scores. Half of the dataset is for training, and the second half is for testing. • SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which consists of 5,000 pairs of sentences. Pairs are annotated for RTE and </context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of Semantic Evaluation (SemEval-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen H Bach</author>
<author>Bert Huang</author>
<author>Ben London</author>
<author>Lise Getoor</author>
</authors>
<title>Hinge-loss Markov random fields: Convex inference for structured prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence (UAI-13).</booktitle>
<contexts>
<context position="2916" citStr="Bach et al., 2013" startWordPosition="458" endWordPosition="461">ic logic. This approach is interesting in that it uses a very deep and precise representation of meaning, which can then be relaxed in a controlled fashion using distributional similarity. But the approach faces large hurdles in practice, stemming from efficiency issues with the Markov Logic Networks (MLN) (Richardson and Domingos, 2006) that they use for performing probabilistic logical inference. In this paper, we use the same combined logicbased and distributional framework as Beltagy et al., (2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al., 2012; Bach et al., 2013). PSL is a probabilistic logic framework designed to have efficient inference. Inference in MLNs is theoretically intractable in the general case, and existing approximate inference algorithms are computationally expensive and sometimes inaccurate. Consequently, the MLN approach of Beltagy et al. (2013) was unable to scale to long sentences and was only tested on the relatively short sentences in the Microsoft video description corpus used for STS (Agirre et al., 2012). On the other hand, inference in PSL reduces to a linear programming problem, which is theoretically and practically much more</context>
<context position="8146" citStr="Bach et al., 2013" startWordPosition="1259" endWordPosition="1262">al clauses that it satisfies. A variety of inference methods for MLNs have been developed, however, developing a scalable, general-purpose, accurate inference method for complex MLNs is an open problem. Beltagy et al. (2013) use MLNs to represent the meaning of natural language sentences and judge textual entailment and semantic similarity, but they were unable to scale the approach beyond short sentences due to the complexity of MLN inference. 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is a recently proposed alternative framework for probabilistic logic (Kimmig et al., 2012; Bach et al., 2013). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing feature of PSL is that ground atoms have soft, continuous truth values in the interval [0, 1] rather than binary truth values as used in MLNs and most other probabilistic logics. Given a set of weighted logical formulas, PSL builds a graphical model defining a probability distribution over the continuous space of values of the random variables in the model. 1211 A PSL model is</context>
</contexts>
<marker>Bach, Huang, London, Getoor, 2013</marker>
<rawString>Stephen H. Bach, Bert Huang, Ben London, and Lise Getoor. 2013. Hinge-loss Markov random fields: Convex inference for structured prediction. In Proceedings of Uncertainty in Artificial Intelligence (UAI-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of Semantic Evaluation (SemEval-12).</booktitle>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of Semantic Evaluation (SemEval-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-10).</booktitle>
<contexts>
<context position="6664" citStr="Baroni and Zamparelli, 2010" startWordPosition="1033" endWordPosition="1036">y the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). 2.3 Markov Logic Networks Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softeni</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<date>2013</date>
<contexts>
<context position="1584" citStr="Beltagy et al. (2013)" startWordPosition="239" endWordPosition="242">tual Similarity task (STS)(Agirre et al., 2012). Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approach to sentence similarity: They use a very 1Mitchell and Lapata (2008) give an amusing example of two sentences that consist of all the same words, but are very different in their meaning: (a) It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. (b) That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similarit</context>
<context position="3220" citStr="Beltagy et al. (2013)" startWordPosition="503" endWordPosition="506"> (MLN) (Richardson and Domingos, 2006) that they use for performing probabilistic logical inference. In this paper, we use the same combined logicbased and distributional framework as Beltagy et al., (2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al., 2012; Bach et al., 2013). PSL is a probabilistic logic framework designed to have efficient inference. Inference in MLNs is theoretically intractable in the general case, and existing approximate inference algorithms are computationally expensive and sometimes inaccurate. Consequently, the MLN approach of Beltagy et al. (2013) was unable to scale to long sentences and was only tested on the relatively short sentences in the Microsoft video description corpus used for STS (Agirre et al., 2012). On the other hand, inference in PSL reduces to a linear programming problem, which is theoretically and practically much more efficient. Empirical results on a range of problems have confirmed that inference in PSL is much more efficient than in MLNs, and frequently more accurate (Kimmig et al., 2012; Bach et al., 2013). We show how to use PSL for STS, and describe changes to the PSL framework that make it more effective for </context>
<context position="6952" citStr="Beltagy et al., 2013" startWordPosition="1076" endWordPosition="1079">tor representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). 2.3 Markov Logic Networks Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints and thereby allowing situations in which not all clauses are satisfied. MLNs define a probability distribution over possible worlds, where a world’s probability increases exponentially with the total weight of the logical clauses that it satis</context>
<context position="13655" citStr="Beltagy et al. (2013)" startWordPosition="2178" endWordPosition="2181">th. Lewis and Steedman (2013) use distributional information to determine word senses, but still produce a strictly logical semantic representation that does not address the “graded” nature of linguistic meaning that is important to measuring semantic similarity. Garrette et al. (2011) introduced a framework for combining logic and distributional models using probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical representation, and Markov Logic Networks are used to perform probabilistic logical inference. Beltagy et al. (2013) extended this framework by generating distributional inference rules from phrase similarity and tailoring the system to the STS task. STS is treated as computing the probability of two textual entailments T |= H and H |= T, where T and H are the two sentences whose similarity is being judged. These two entailment probabilities are averaged to produce a measure of similarity. The MLN constructed to determine the probability of a given entailment includes the logical forms for both T and H as well as soft inference rules that are constructed from distributional information. Given a similarity s</context>
<context position="15910" citStr="Beltagy et al. (2013)" startWordPosition="2575" endWordPosition="2578"> → b |w, where w = f(sim(−→a , →−b )). Both a and b can be words or phrases. Phrases are defined in terms of Boxer’s output. A phrase is more than one unary atom sharing the same variable like “a little kid” which in logic is little(K) ∧ kid(K). A phrase also can be two unary atoms connected by a relation like “a man is driving” which in logic is man(M) ∧ agent(D, M) ∧ drive(D). The similarity function sim takes two vectors as input. Phrasal vectors are constructed using Vector Addition (Landauer and Dumais, 1997). The set of generated inference rules can be regarded as the knowledge base KB. Beltagy et al. (2013) found that the logical conjunction in H is very restrictive for the STS task, so they relaxed the conjunction by using an average evidence combiner (Natarajan et al., 2010). The average combiner results in computationally complex inference and only works for short sentences. In case inference breaks or times-out, they back off to a simpler combiner that leads to much faster inference but loses most of the structure of the sentence and is therefore less accurate. Given T, KB and H from the previous steps, MLN inference is then used to compute p(H|T, KB), which is then used as a measure of the </context>
<context position="17449" citStr="Beltagy et al. (2013)" startWordPosition="2832" endWordPosition="2835">designed for computing similarity between complex structured objects rather than determining probabilistic logical entailment. In fact, the initial version of PSL (Broecheler et al., 2010) was called Probabilistic Similarity Logic, based on its use of similarity functions. This initial version was shown to be very effective for measuring the similarity of noisy database records and performing record linkage (i.e. identifying database entries referring to the same entity, such as bibliographic citations referring to the same paper). Therefore, we have developed an approach that follows that of Beltagy et al. (2013), but replaces Markov Logic with PSL. This section explains how we formulate the STS 1213 task as a PSL program. PSL does not work very well “out of the box” for STS, mainly because Lukasiewicz’s equation for the conjunction is very restrictive. Therefore, we use a different interpretation for conjunction that uses averaging, which requires corresponding changes to the optimization problem and the grounding technique. 3.1 Representation Given the logical forms for a pair of sentences, a text T and a hypothesis H, and given a set of weighted rules derived from the distributional semantics (as e</context>
<context position="19814" citStr="Beltagy et al., (2013)" startWordPosition="3247" endWordPosition="3250">* result(), and then PSL is queried for the truth value of the atom result(). For our example, the rule is: bx, y. guy(x) n agent(y, x) n walk(y) —* result(). Priors: A low prior is given to all predicates. This encourages the truth values of ground atoms to be zero, unless there is evidence to the contrary. For each STS pair of sentences 51, 52, we run PSL twice, once where T = 51, H = 52 and another where T = 52, H = 51, and output the two scores. To produce a final similarity score, we train a regressor to learn the mapping between the two PSL scores and the overall similarity score. As in Beltagy et al., (2013) we use Additive Regression (Friedman, 2002). 3.2 Changing Conjunction As mentioned above, Lukasiewicz’s formula for conjunction is very restrictive and does not work well for STS. For example, for T: “A man is driving” and H: “A man is driving a car”, if we use the standard PSL formula for conjunction, the output value is zero because there is no evidence for a car and max(0, X + 0 − 1) = 0 for any truth value 0 &lt; X &lt; 1. However, humans find these sentences to be quite similar. Therefore, we introduce a new averaging interpretation of conjunction that we use for the hypothesis H. The truth va</context>
<context position="27331" citStr="Beltagy et al. (2013)" startWordPosition="4570" endWordPosition="4573">for STS 2012, 1,500 pairs were selected and annotated with similarity scores. Half of the dataset is for training, and the second half is for testing. • SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which consists of 5,000 pairs of sentences. Pairs are annotated for RTE and STS, but we only use the STS data. Training and testing was done using 10-fold cross validation. 4.2 Systems Compared We compare our PSL system with several others. In all cases, we use the distributional word vectors employed by Beltagy et al. (2013) based on context windows from Gigaword. • vec-add: Vector Addition (Landauer and Dumais, 1997). We compute a vector representation for each sentence by adding the distributional vectors of all of its words and measure similarity using cosine. This is a simple yet powerful baseline that uses only distributional information. • vec-mul: Component-wise Vector Multiplication (Mitchell and Lapata, 2008). The same as vec-add except uses componentwise multiplication to combine word vectors. • MLN: The system of Beltagy et al. (2013), which uses Markov logic instead of PSL for probabilistic inference.</context>
<context position="28629" citStr="Beltagy et al., 2013" startWordPosition="4777" endWordPosition="4780">n MLN times out, it backs off to a simpler sentence representation as explained in section 2.6. • PSL: Our proposed PSL system for combining logical and distributional information. • PSL-no-DIR: Our PSL system without distributional inference rules(empty knowledge base). This system uses PSL to compute similarity of logical forms but does not use distributional information on lexical or phrasal similarity. It tests the impact of the probabilistic logic only • PSL+vec-add: PSL ensembled with vecadd. Ensembling the MLN approach with a purely distributional approach was found to improve results (Beltagy et al., 2013), so we also tried this with PSL. The methods are ensembled by using both entailment scores of both systems as input features to the regression step that learns to map entailment scores to STS similarity ratings. This way, the training data is used to learn how to weight the contribution of the different components. • PSL+MLN: PSL ensembled with MLN in the same manner. 4.3 Experiments Systems are evaluated on two metrics, Pearson correlation and average CPU time per pair of sentences. • Pearson correlation: The Pearson correlation between the system’s similarity scores and the human gold-stand</context>
<context position="31879" citStr="Beltagy et al., 2013" startWordPosition="5313" endWordPosition="5316">large Figure 1: Effect of PSL’s grounding limit on the correlation score for the msr-par dataset number of timeouts, and because the conjunctionaveraging in PSL is combining evidence better than MLN’s average-combiner, whose performance is sensitive to various parameters. These results further support the claim that using probabilistic logic to integrate logical and distributional information is a promising approach to natural-language semantics. More specifically, they strongly indicate that PSL is a more effective probabilistic logic for judging semantic similarity than MLNs. Like for MLNs (Beltagy et al., 2013), ensembling PSL with vector addition improved the scores a bit, except for msr-par where vec-add’s performance is particularly low. However, this ensemble still does not beat the state of the art (B¨ar et al., 2012) which is a large ensemble of many different systems. It would be informative to add our system to their ensemble to see if it could improve it even further. Table 2 shows the CPU time for PSL and MLN. The results clearly demonstrate that PSL is an order of magnitude faster than MLN. Figures 1 and 2 show the effect of changing the grounding limit on Pearson correlation and CPU time</context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Montague meets</author>
</authors>
<title>Markov: Deep semantics with probabilistic logical form.</title>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM-13).</booktitle>
<marker>meets, </marker>
<rawString>Montague meets Markov: Deep semantics with probabilistic logical form. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL-11).</booktitle>
<contexts>
<context position="33814" citStr="Berant et al., 2011" startWordPosition="5644" endWordPosition="5647"> could learn that the type of an object determined by a noun should be weighted more than a property specified by an adjective. As a result, “black dog” would be appropriately judged more similar to “white dog” than to “black cat.” One of the advantages of using a probabilistic logic is that additional sources of knowledge can easily be incorporated by adding additional soft inference rules. To complement the soft inference rules capturing distributional lexical and phrasal similarities, PSL rules could be added that encode explicit paraphrase rules, such as those mined from monolingual text (Berant et al., 2011) or multi-lingual parallel text (Ganitkevitch et al., 2013). This paper has focused on STS; however, as shown by Beltagy et al. (2013), probabilistic logic is also an effective approach to recognizing textual entailment (RTE). By using the appropriate functions to combine truth values for various logical connectives, PSL could also be adapted for RTE. Although we have shown that PSL outperforms MLNs on STS, we hypothesize that MLNs may still be a better approach for RTE. However, it would be good to experimentally confirm this intuition. In any case, the high computational complexity of MLN in</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of Association for Computational Linguistics (ACL-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</booktitle>
<contexts>
<context position="5362" citStr="Berant et al., 2013" startWordPosition="832" endWordPosition="835">p and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture lo</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer.</title>
<date>2008</date>
<booktitle>In Proceedings of Semantics in Text Processing (STEP-08).</booktitle>
<contexts>
<context position="14702" citStr="Bos, 2008" startWordPosition="2352" endWordPosition="2353">n entailment includes the logical forms for both T and H as well as soft inference rules that are constructed from distributional information. Given a similarity score for all pairs of sentences in the dataset, a regressor is trained on the training set to map the system’s output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regression (Friedman, 2002). To determine an entailment probability, first, the two sentences are mapped to logical representations using Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&amp;C for CCG parsing (Clark and Curran, 2004). Distributional semantic knowledge is then encoded as weighted inference rules in the MLN. A rule’s weight (w) is a function of the cosine similarity (sim) between its antecedent and consequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all words and phrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule: a → b |w, wher</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with Boxer. In Proceedings of Semantics in Text Processing (STEP-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Broecheler</author>
<author>Lilyana Mihalkova</author>
<author>Lise Getoor</author>
</authors>
<title>Probabilistic Similarity Logic.</title>
<date>2010</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence (UAI-20).</booktitle>
<contexts>
<context position="17016" citStr="Broecheler et al., 2010" startWordPosition="2763" endWordPosition="2766">d H from the previous steps, MLN inference is then used to compute p(H|T, KB), which is then used as a measure of the degree to which T entails H. 3 PSL for STS For several reasons, we believe PSL is a more appropriate probabilistic logic for STS than MLNs. First, it is explicitly designed to support efficient inference, therefore it scales better to longer sentences with more complex logical forms. Second, it was also specifically designed for computing similarity between complex structured objects rather than determining probabilistic logical entailment. In fact, the initial version of PSL (Broecheler et al., 2010) was called Probabilistic Similarity Logic, based on its use of similarity functions. This initial version was shown to be very effective for measuring the similarity of noisy database records and performing record linkage (i.e. identifying database entries referring to the same entity, such as bibliographic citations referring to the same paper). Therefore, we have developed an approach that follows that of Beltagy et al. (2013), but replaces Markov Logic with PSL. This section explains how we formulate the STS 1213 task as a PSL program. PSL does not work very well “out of the box” for STS, </context>
</contexts>
<marker>Broecheler, Mihalkova, Getoor, 2010</marker>
<rawString>Matthias Broecheler, Lilyana Mihalkova, and Lise Getoor. 2010. Probabilistic Similarity Logic. In Proceedings of Uncertainty in Artificial Intelligence (UAI-20).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL-11).</booktitle>
<contexts>
<context position="26414" citStr="Chen and Dolan, 2011" startWordPosition="4412" endWordPosition="4415">lving the linear program). This loop repeats until convergence, i.e. until the truth 1215 values stop changing. The truth values used in each grounding phase come from the previous optimization phase. The first grounding phase assumes only the propositions in the evidence provided have non-zero truth values. 4 Evaluation This section evaluates the performance of PSL on the STS task. 4.1 Datasets We evaluate our system on three STS datasets. • msr-vid: Microsoft Video Paraphrase Corpus from STS 2012. The dataset consists of 1,500 pairs of short video descriptions collected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al., 2012). Half of the dataset is for training, and the second half is for testing. • msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al., 2004). Then, for STS 2012, 1,500 pairs were selected and annotated with similarity scores. Half of the dataset is for training, and the second half is for testing. • SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which cons</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of Association for Computational Linguistics (ACL-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL-04).</booktitle>
<contexts>
<context position="14894" citStr="Clark and Curran, 2004" startWordPosition="2379" endWordPosition="2382">rs of sentences in the dataset, a regressor is trained on the training set to map the system’s output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regression (Friedman, 2002). To determine an entailment probability, first, the two sentences are mapped to logical representations using Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&amp;C for CCG parsing (Clark and Curran, 2004). Distributional semantic knowledge is then encoded as weighted inference rules in the MLN. A rule’s weight (w) is a function of the cosine similarity (sim) between its antecedent and consequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all words and phrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule: a → b |w, where w = f(sim(−→a , →−b )). Both a and b can be words or phrases. Phrases are defined in terms of Boxer’s output. A phrase is more than one unary atom sharing the same variable like “a little ki</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proceedings of Association for Computational Linguistics (ACL-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING-04).</booktitle>
<contexts>
<context position="26702" citStr="Dolan et al., 2004" startWordPosition="4461" endWordPosition="4464">-zero truth values. 4 Evaluation This section evaluates the performance of PSL on the STS task. 4.1 Datasets We evaluate our system on three STS datasets. • msr-vid: Microsoft Video Paraphrase Corpus from STS 2012. The dataset consists of 1,500 pairs of short video descriptions collected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al., 2012). Half of the dataset is for training, and the second half is for testing. • msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al., 2004). Then, for STS 2012, 1,500 pairs were selected and annotated with similarity scores. Half of the dataset is for training, and the second half is for testing. • SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which consists of 5,000 pairs of sentences. Pairs are annotated for RTE and STS, but we only use the STS data. Training and testing was done using 10-fold cross validation. 4.2 Systems Compared We compare our PSL system with several others. In all cases, we use the distributional word vectors empl</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the International Conference on Computational Linguistics (COLING-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Stochastic gradient boosting.</title>
<date>2002</date>
<journal>Journal of Computational Statistics &amp; Data Analysis</journal>
<contexts>
<context position="14574" citStr="Friedman, 2002" startWordPosition="2333" endWordPosition="2334"> entailment probabilities are averaged to produce a measure of similarity. The MLN constructed to determine the probability of a given entailment includes the logical forms for both T and H as well as soft inference rules that are constructed from distributional information. Given a similarity score for all pairs of sentences in the dataset, a regressor is trained on the training set to map the system’s output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regression (Friedman, 2002). To determine an entailment probability, first, the two sentences are mapped to logical representations using Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&amp;C for CCG parsing (Clark and Curran, 2004). Distributional semantic knowledge is then encoded as weighted inference rules in the MLN. A rule’s weight (w) is a function of the cosine similarity (sim) between its antecedent and consequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all wor</context>
<context position="19858" citStr="Friedman, 2002" startWordPosition="3256" endWordPosition="3257">alue of the atom result(). For our example, the rule is: bx, y. guy(x) n agent(y, x) n walk(y) —* result(). Priors: A low prior is given to all predicates. This encourages the truth values of ground atoms to be zero, unless there is evidence to the contrary. For each STS pair of sentences 51, 52, we run PSL twice, once where T = 51, H = 52 and another where T = 52, H = 51, and output the two scores. To produce a final similarity score, we train a regressor to learn the mapping between the two PSL scores and the overall similarity score. As in Beltagy et al., (2013) we use Additive Regression (Friedman, 2002). 3.2 Changing Conjunction As mentioned above, Lukasiewicz’s formula for conjunction is very restrictive and does not work well for STS. For example, for T: “A man is driving” and H: “A man is driving a car”, if we use the standard PSL formula for conjunction, the output value is zero because there is no evidence for a car and max(0, X + 0 − 1) = 0 for any truth value 0 &lt; X &lt; 1. However, humans find these sentences to be quite similar. Therefore, we introduce a new averaging interpretation of conjunction that we use for the hypothesis H. The truth value for a conjunction is defined as I(p1 n .</context>
</contexts>
<marker>Friedman, 2002</marker>
<rawString>Jerome H Friedman. 2002. Stochastic gradient boosting. Journal of Computational Statistics &amp; Data Analysis (CSDA-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13).</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Integrating logical representations with probabilistic information using Markov logic.</title>
<date>2011</date>
<booktitle>In Proceedings of International Conference on Computational Semantics (IWCS-11).</booktitle>
<contexts>
<context position="13320" citStr="Garrette et al. (2011)" startWordPosition="2130" endWordPosition="2133">sets added in the 2013 competition since they did not contain naturally-occurring, full sentences, which is the focus of our work. 1212 2.6 Combining logical and distributional methods using probabilistic logic There are a few recent attempts to combine logical and distributional representations in order to obtain the advantages of both. Lewis and Steedman (2013) use distributional information to determine word senses, but still produce a strictly logical semantic representation that does not address the “graded” nature of linguistic meaning that is important to measuring semantic similarity. Garrette et al. (2011) introduced a framework for combining logic and distributional models using probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical representation, and Markov Logic Networks are used to perform probabilistic logical inference. Beltagy et al. (2013) extended this framework by generating distributional inference rules from phrase similarity and tailoring the system to the STS task. STS is treated as computing the probability of two textual entailments T |= H and H |= T, where T and H are the two sentences whos</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2011</marker>
<rawString>Dan Garrette, Katrin Erk, and Raymond Mooney. 2011. Integrating logical representations with probabilistic information using Markov logic. In Proceedings of International Conference on Computational Semantics (IWCS-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-11).</booktitle>
<contexts>
<context position="6699" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1037" endWordPosition="1040">ically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). 2.3 Markov Logic Networks Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical co</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
</authors>
<title>Towards a formal distributional semantics: Simulating logical calculi with tensors.</title>
<date>2013</date>
<booktitle>In Proceedings of Second Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="5998" citStr="Grefenstette, 2013" startWordPosition="930" endWordPosition="931">r system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase </context>
</contexts>
<marker>Grefenstette, 2013</marker>
<rawString>Edward Grefenstette. 2013. Towards a formal distributional semantics: Simulating logical calculi with tensors. In Proceedings of Second Joint Conference on Lexical and Computational Semantics (*SEM 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<date>1993</date>
<booktitle>From Discourse to Logic.</booktitle>
<publisher>Kluwer.</publisher>
<contexts>
<context position="4760" citStr="Kamp and Reyle, 1993" startWordPosition="746" endWordPosition="749">ne 23-25 2014. c�2014 Association for Computational Linguistics results demonstrate that, overall, PSL models human similarity judgements more accurately than these alternative approaches, and is significantly faster than MLNs. The rest of the paper is organized as follows: section 2 presents relevant background material, section 3 explains how we adapted PSL for the STS task, section 4 presents the evaluation, and sections 5 and 6 discuss future work and conclusions, respectively. 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 201</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Kimmig</author>
<author>Stephen H Bach</author>
<author>Matthias Broecheler</author>
<author>Bert Huang</author>
<author>Lise Getoor</author>
</authors>
<title>A short introduction to Probabilistic Soft Logic.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12).</booktitle>
<contexts>
<context position="2896" citStr="Kimmig et al., 2012" startWordPosition="454" endWordPosition="457">ment in a probabilistic logic. This approach is interesting in that it uses a very deep and precise representation of meaning, which can then be relaxed in a controlled fashion using distributional similarity. But the approach faces large hurdles in practice, stemming from efficiency issues with the Markov Logic Networks (MLN) (Richardson and Domingos, 2006) that they use for performing probabilistic logical inference. In this paper, we use the same combined logicbased and distributional framework as Beltagy et al., (2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al., 2012; Bach et al., 2013). PSL is a probabilistic logic framework designed to have efficient inference. Inference in MLNs is theoretically intractable in the general case, and existing approximate inference algorithms are computationally expensive and sometimes inaccurate. Consequently, the MLN approach of Beltagy et al. (2013) was unable to scale to long sentences and was only tested on the relatively short sentences in the Microsoft video description corpus used for STS (Agirre et al., 2012). On the other hand, inference in PSL reduces to a linear programming problem, which is theoretically and p</context>
<context position="8126" citStr="Kimmig et al., 2012" startWordPosition="1255" endWordPosition="1258">l weight of the logical clauses that it satisfies. A variety of inference methods for MLNs have been developed, however, developing a scalable, general-purpose, accurate inference method for complex MLNs is an open problem. Beltagy et al. (2013) use MLNs to represent the meaning of natural language sentences and judge textual entailment and semantic similarity, but they were unable to scale the approach beyond short sentences due to the complexity of MLN inference. 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is a recently proposed alternative framework for probabilistic logic (Kimmig et al., 2012; Bach et al., 2013). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing feature of PSL is that ground atoms have soft, continuous truth values in the interval [0, 1] rather than binary truth values as used in MLNs and most other probabilistic logics. Given a set of weighted logical formulas, PSL builds a graphical model defining a probability distribution over the continuous space of values of the random variables in the model.</context>
<context position="11948" citStr="Kimmig et al., 2012" startWordPosition="1913" endWordPosition="1916">rpretation with the lowest distance to satisfaction. In other words, it is the interpretation that tries to satisfy all rules as much as possible. Formally, from equation 3, the most probable interpretation, is the one that minimizes Er∈R λr(d(r))p. In case of p = 1, and given that all d(r) are linear equations, then minimizing the sum requires solving a linear program, which, compared to inference in other probabilistic logics such as MLNs, can be done relatively efficiently using well-established techniques. In case p = 2, MPE inference can be shown to be a second-order cone program (SOCP) (Kimmig et al., 2012). 2.5 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 0 to 5, and was recently introduced as a SemEval task (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. The best performing system in 2012’s competition was by B¨ar et al. (2012), a complex ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vecto</context>
</contexts>
<marker>Kimmig, Bach, Broecheler, Huang, Getoor, 2012</marker>
<rawString>Angelika Kimmig, Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. 2012. A short introduction to Probabilistic Soft Logic. In Proceedings of NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</booktitle>
<contexts>
<context position="5340" citStr="Kwiatkowski et al., 2013" startWordPosition="828" endWordPosition="831">ition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review.</journal>
<contexts>
<context position="5730" citStr="Landauer and Dumais, 1997" startWordPosition="889" endWordPosition="893">sing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute ve</context>
<context position="15808" citStr="Landauer and Dumais, 1997" startWordPosition="2556" endWordPosition="2560">hrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule: a → b |w, where w = f(sim(−→a , →−b )). Both a and b can be words or phrases. Phrases are defined in terms of Boxer’s output. A phrase is more than one unary atom sharing the same variable like “a little kid” which in logic is little(K) ∧ kid(K). A phrase also can be two unary atoms connected by a relation like “a man is driving” which in logic is man(M) ∧ agent(D, M) ∧ drive(D). The similarity function sim takes two vectors as input. Phrasal vectors are constructed using Vector Addition (Landauer and Dumais, 1997). The set of generated inference rules can be regarded as the knowledge base KB. Beltagy et al. (2013) found that the logical conjunction in H is very restrictive for the STS task, so they relaxed the conjunction by using an average evidence combiner (Natarajan et al., 2010). The average combiner results in computationally complex inference and only works for short sentences. In case inference breaks or times-out, they back off to a simpler combiner that leads to much faster inference but loses most of the structure of the sentence and is therefore less accurate. Given T, KB and H from the pre</context>
<context position="27426" citStr="Landauer and Dumais, 1997" startWordPosition="4584" endWordPosition="4587">dataset is for training, and the second half is for testing. • SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which consists of 5,000 pairs of sentences. Pairs are annotated for RTE and STS, but we only use the STS data. Training and testing was done using 10-fold cross validation. 4.2 Systems Compared We compare our PSL system with several others. In all cases, we use the distributional word vectors employed by Beltagy et al. (2013) based on context windows from Gigaword. • vec-add: Vector Addition (Landauer and Dumais, 1997). We compute a vector representation for each sentence by adding the distributional vectors of all of its words and measure similarity using cosine. This is a simple yet powerful baseline that uses only distributional information. • vec-mul: Component-wise Vector Multiplication (Mitchell and Lapata, 2008). The same as vec-add except uses componentwise multiplication to combine word vectors. • MLN: The system of Beltagy et al. (2013), which uses Markov logic instead of PSL for probabilistic inference. MLN inference is very slow in some cases, so we use a 10 minute timeout. When MLN times out, i</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combined distributional and logical semantics.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics</journal>
<contexts>
<context position="13063" citStr="Lewis and Steedman (2013)" startWordPosition="2091" endWordPosition="2095"> system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Two of the datasets we use for evaluation are from the 2012 competition. We did not utilize the new datasets added in the 2013 competition since they did not contain naturally-occurring, full sentences, which is the focus of our work. 1212 2.6 Combining logical and distributional methods using probabilistic logic There are a few recent attempts to combine logical and distributional representations in order to obtain the advantages of both. Lewis and Steedman (2013) use distributional information to determine word senses, but still produce a strictly logical semantic representation that does not address the “graded” nature of linguistic meaning that is important to measuring semantic similarity. Garrette et al. (2011) introduced a framework for combining logic and distributional models using probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical representation, and Markov Logic Networks are used to perform probabilistic logical inference. Beltagy et al. (2013) extende</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined distributional and logical semantics. Transactions of the Association for Computational Linguistics (TACL-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers.</title>
<date>1996</date>
<contexts>
<context position="6279" citStr="Lund and Burgess, 1996" startWordPosition="971" endWordPosition="974">redict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the be</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL08).</booktitle>
<contexts>
<context position="1679" citStr="Mitchell and Lapata (2008)" startWordPosition="255" endWordPosition="258">of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approach to sentence similarity: They use a very 1Mitchell and Lapata (2008) give an amusing example of two sentences that consist of all the same words, but are very different in their meaning: (a) It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. (b) That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similarity ratings at the word and phrase level. Sentence similarity is then modelled as mutual entailme</context>
<context position="6524" citStr="Mitchell and Lapata, 2008" startWordPosition="1011" endWordPosition="1014"> nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). 2.3 Markov Logic Networks Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder </context>
<context position="27732" citStr="Mitchell and Lapata, 2008" startWordPosition="4630" endWordPosition="4633">TS data. Training and testing was done using 10-fold cross validation. 4.2 Systems Compared We compare our PSL system with several others. In all cases, we use the distributional word vectors employed by Beltagy et al. (2013) based on context windows from Gigaword. • vec-add: Vector Addition (Landauer and Dumais, 1997). We compute a vector representation for each sentence by adding the distributional vectors of all of its words and measure similarity using cosine. This is a simple yet powerful baseline that uses only distributional information. • vec-mul: Component-wise Vector Multiplication (Mitchell and Lapata, 2008). The same as vec-add except uses componentwise multiplication to combine word vectors. • MLN: The system of Beltagy et al. (2013), which uses Markov logic instead of PSL for probabilistic inference. MLN inference is very slow in some cases, so we use a 10 minute timeout. When MLN times out, it backs off to a simpler sentence representation as explained in section 2.6. • PSL: Our proposed PSL system for combining logical and distributional information. • PSL-no-DIR: Our PSL system without distributional inference rules(empty knowledge base). This system uses PSL to compute similarity of logica</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of Association for Computational Linguistics (ACL08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Journal of Cognitive Science.</journal>
<contexts>
<context position="5758" citStr="Mitchell and Lapata, 2010" startWordPosition="894" endWordPosition="897"> expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for lar</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Journal of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<date>1970</date>
<booktitle>Universal grammar. Theoria,</booktitle>
<pages>36--373</pages>
<contexts>
<context position="4737" citStr="Montague, 1970" startWordPosition="744" endWordPosition="745">aryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics results demonstrate that, overall, PSL models human similarity judgements more accurately than these alternative approaches, and is significantly faster than MLNs. The rest of the paper is organized as follows: section 2 presents relevant background material, section 3 explains how we adapted PSL for the STS task, section 4 presents the evaluation, and sections 5 and 6 discuss future work and conclusions, respectively. 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Richard Montague. 1970. Universal grammar. Theoria, 36:373–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriraam Natarajan</author>
<author>Tushar Khot</author>
<author>Daniel Lowd</author>
<author>Prasad Tadepalli</author>
<author>Kristian Kersting</author>
<author>Jude Shavlik</author>
</authors>
<title>Exploiting causal independence in Markov logic networks: Combining undirected and directed models.</title>
<date>2010</date>
<booktitle>In Proceedings of European Conference in Machine Learning (ECML-10).</booktitle>
<contexts>
<context position="16083" citStr="Natarajan et al., 2010" startWordPosition="2606" endWordPosition="2609">he same variable like “a little kid” which in logic is little(K) ∧ kid(K). A phrase also can be two unary atoms connected by a relation like “a man is driving” which in logic is man(M) ∧ agent(D, M) ∧ drive(D). The similarity function sim takes two vectors as input. Phrasal vectors are constructed using Vector Addition (Landauer and Dumais, 1997). The set of generated inference rules can be regarded as the knowledge base KB. Beltagy et al. (2013) found that the logical conjunction in H is very restrictive for the STS task, so they relaxed the conjunction by using an average evidence combiner (Natarajan et al., 2010). The average combiner results in computationally complex inference and only works for short sentences. In case inference breaks or times-out, they back off to a simpler combiner that leads to much faster inference but loses most of the structure of the sentence and is therefore less accurate. Given T, KB and H from the previous steps, MLN inference is then used to compute p(H|T, KB), which is then used as a measure of the degree to which T entails H. 3 PSL for STS For several reasons, we believe PSL is a more appropriate probabilistic logic for STS than MLNs. First, it is explicitly designed </context>
</contexts>
<marker>Natarajan, Khot, Lowd, Tadepalli, Kersting, Shavlik, 2010</marker>
<rawString>Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad Tadepalli, Kristian Kersting, and Jude Shavlik. 2010. Exploiting causal independence in Markov logic networks: Combining undirected and directed models. In Proceedings of European Conference in Machine Learning (ECML-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="2637" citStr="Richardson and Domingos, 2006" startWordPosition="414" endWordPosition="417">tle, but it was not serious. deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similarity ratings at the word and phrase level. Sentence similarity is then modelled as mutual entailment in a probabilistic logic. This approach is interesting in that it uses a very deep and precise representation of meaning, which can then be relaxed in a controlled fashion using distributional similarity. But the approach faces large hurdles in practice, stemming from efficiency issues with the Markov Logic Networks (MLN) (Richardson and Domingos, 2006) that they use for performing probabilistic logical inference. In this paper, we use the same combined logicbased and distributional framework as Beltagy et al., (2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al., 2012; Bach et al., 2013). PSL is a probabilistic logic framework designed to have efficient inference. Inference in MLNs is theoretically intractable in the general case, and existing approximate inference algorithms are computationally expensive and sometimes inaccurate. Consequently, the MLN approach of Beltagy et al. (2013) was unable to sc</context>
<context position="7039" citStr="Richardson and Domingos, 2006" startWordPosition="1088" endWordPosition="1091">ividual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). 2.3 Markov Logic Networks Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints and thereby allowing situations in which not all clauses are satisfied. MLNs define a probability distribution over possible worlds, where a world’s probability increases exponentially with the total weight of the logical clauses that it satisfies. A variety of inference methods for MLNs have been developed, however, developing </context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research</journal>
<contexts>
<context position="5578" citStr="Turney and Pantel, 2010" startWordPosition="864" endWordPosition="867">of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional space</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research (JAIR-10).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>