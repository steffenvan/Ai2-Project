<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017684">
<title confidence="0.9994595">
A Bayesian Method to Incorporate Background Knowledge during
Automatic Text Summarization
</title>
<author confidence="0.987666">
Annie Louis
</author>
<affiliation confidence="0.9955875">
ILCC, School of Informatics,
University of Edinburgh,
</affiliation>
<address confidence="0.946626">
Edinburgh EH8 9AB, UK
</address>
<email confidence="0.998448">
alouis@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998425">
In order to summarize a document, it is
often useful to have a background set
of documents from the domain to serve
as a reference for determining new and
important information in the input doc-
ument. We present a model based on
Bayesian surprise which provides an in-
tuitive way to identify surprising informa-
tion from a summarization input with re-
spect to a background corpus. Specifically,
the method quantifies the degree to which
pieces of information in the input change
one’s beliefs’ about the world represented
in the background. We develop sys-
tems for generic and update summariza-
tion based on this idea. Our method pro-
vides competitive content selection perfor-
mance with particular advantages in the
update task where systems are given a
small and topical background corpus.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999739396551725">
Important facts in a new text are those which devi-
ate from previous knowledge on the topic. When
people create summaries, they use their knowl-
edge about the world to decide what content in an
input document is informative to include in a sum-
mary. Understandably in automatic summariza-
tion as well, it is useful to keep a background set
of documents to represent general facts and their
frequency in the domain.
For example, in the simplest setting of multi-
document summarization of news, systems are
asked to summarize an input set of topically-
related news documents to reflect its central con-
tent. In this GENERIC task, some of the best re-
ported results were obtained by a system (Conroy
et al., 2006) which computed importance scores
for words in the input by examining if the word
occurs with significantly higher probability in the
input compared to a large background collection
of news articles. Other specialized summarization
tasks explicitly require the use of background in-
formation. In the UPDATE summarization task, a
system is given two sets of news documents on the
same topic; the second contains articles published
later in time. The system should summarize the
important updates from the second set assuming a
user has already read the first set of articles.
In this work, we present a Bayesian model for
assessing the novelty of a sentence taken from a
summarization input with respect to a background
corpus of documents.
Our model is based on the idea of Bayesian Sur-
prise (Itti and Baldi, 2006). For illustration, as-
sume that a user’s background knowledge com-
prises of multiple hypotheses about the current
state of the world and a probability distribution
over these hypotheses indicates his degree of be-
lief in each hypothesis. For example, one hypoth-
esis may be that the political situation in Ukraine
is peaceful, another where it is not. Apriori as-
sume the user favors the hypothesis about a peace-
ful Ukraine, i.e. the hypothesis has higher prob-
ability in the prior distribution. Given new data,
the evidence can be incorporated using Bayes Rule
to compute the posterior distribution over the hy-
potheses. For example, upon viewing news reports
about riots in the country, a user would update his
beliefs and the posterior distribution of the user’s
knowledge would have a higher probability for a
riotous Ukraine. Bayesian surprise is the differ-
ence between the prior and posterior distributions
over the hypotheses which quantifies the extent to
which the new data (the news report) has changed
a user’s prior beliefs about the world.
In this work, we exemplify how Bayesian sur-
prise can be used to do content selection for text
summarization. Here a user’s prior knowledge
is approximated by a background corpus and we
</bodyText>
<page confidence="0.989863">
333
</page>
<bodyText confidence="0.978959529411765">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 333–338,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
show how to identify sentences from the input
set which are most surprising with respect to this
background. We use the method to do two types
of summarization tasks: a) GENERIC news sum-
marization which uses a large random collection
of news articles as the background, and b) UP-
DATE summarization where the background is a
smaller but specific set of news documents on
the same topic as the input set. We find that
our method performs competitively with a previ-
ous log-likelihood ratio approach which identifies
words with significantly higher probability in the
input compared to the background. The Bayesian
approach is more advantageous in the update task,
where the background corpus is smaller in size.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.989070368421053">
Computing new information is useful in many ap-
plications. The TREC novelty tasks (Allan et al.,
2003; Soboroff and Harman, 2005; Schiffman,
2005) tested the ability of systems to find novel
information in an IR setting. Systems were given
a list of documents ranked according to relevance
to a query. The goal is to find sentences in each
document which are relevant to the query, and at
the same time is new information given the content
of documents higher in the relevance list.
For update summarization of news, methods
range from textual entailment techniques (Ben-
tivogli et al., 2010) to find facts in the input which
are not entailed by the background, to Bayesian
topic models (Delort and Alfonseca, 2012) which
aim to learn and use topics discussed only in back-
ground, those only in the update input and those
that overlap across the two sets.
Even for generic summarization, some of the
best results were obtained by Conroy et al. (2006)
by using a large random corpus of news articles
as the background while summarizing a new arti-
cle, an idea first proposed by Lin and Hovy (2000).
Central to this approach is the use of a likelihood
ratio test to compute topic words, words that have
significantly higher probability in the input com-
pared to the background corpus, and are hence
descriptive of the input’s topic. In this work,
we compare our system to topic word based ones
since the latter is also a general method to find sur-
prising new words in a set of input documents but
is not a bayesian approach. We briefly explain the
topic words based approach below.
Computing topic words: Let us call the input
set I and the background B. The log-likelihood
ratio test compares two hypotheses:
H1: A word t is not a topic word and occurs
with equal probability in I and B, i.e. p(t|I) =
</bodyText>
<equation confidence="0.976342666666667">
p(t|B) = p
H2: t is a topic word, hence p(t|I) = p1 and
p(t|B) = p2 and p1 &gt; p2
</equation>
<bodyText confidence="0.999954">
A set of documents D containing N tokens is
viewed as a sequence of words w1w2...wN. The
word in each position i is assumed to be generated
by a Bernoulli trial which succeeds when the gen-
erated word wz = t and fails when wz is not t.
Suppose that the probability of success is p. Then
the probability of a word t appearing k times in a
dataset of N tokens is/ the
</bodyText>
<equation confidence="0.878869">
\binomial probability:
b(k,N,p) = I k Ipk(1 − p)N−k (1)
</equation>
<bodyText confidence="0.974374">
The likelihood ratio \compares the likelihood of
the data D = {B, I} under the two hypotheses.
</bodyText>
<equation confidence="0.9768505">
b(ct, N, p) (2)
b(cI, NI, p1) b(cB, NB, p2)
</equation>
<bodyText confidence="0.999846823529412">
p, p1 and p2 are estimated by maximum likeli-
hood. p = ct/N where ct is the number of times
word t appears in the total set of tokens compris-
ing {B, I}. p1 = cIt/NI and p2 = ctB/NB are the
probabilities of t estimated only from the input and
only from the background respectively.
A convenient aspect of this approach is that
−2 log λ is asymptotically χ2 distributed. So for a
resulting −2 log λ value, we can use the χ2 table to
find the significance level with which the null hy-
pothesis H1 can be rejected. For example, a value
of 10 corresponds to a significance level of 0.001
and is standardly used as the cutoff. Words with
−2 log λ &gt; 10 are considered topic words. Con-
roy et al. (2006)’s system gives a weight of 1 to the
topic words and scores sentences using the number
of topic words normalized by sentence length.
</bodyText>
<sectionHeader confidence="0.967911" genericHeader="method">
3 Bayesian Surprise
</sectionHeader>
<bodyText confidence="0.999907555555555">
First we present the formal definition of Bayesian
surprise given by Itti and Baldi (2006) without ref-
erence to the summarization task.
Let H be the space of all hypotheses represent-
ing the background knowledge of a user. The user
has a probability P(H) associated with each hy-
pothesis H E H. Let D be a new observation. The
posterior probability of a single hypothesis H can
be computed as:
</bodyText>
<equation confidence="0.94551">
P(H|D) = P( |H))(H) (3)
P(D|H1)
λ =
P(D|H2)
334
fH P(H |D) log P(H) (5)
</equation>
<bodyText confidence="0.999818888888889">
Note that since KL-divergence is not symmet-
ric, we could also compute KL(P(H), P(H|D))
as the surprise value. In some cases, surprise can
be computed analytically, in particular when the
prior distribution is conjugate to the form of the
hypothesis, and so the posterior has the same func-
tional form as the prior. (See Baldi and Itti (2010)
for the surprise computation for different families
of probability distributions).
</bodyText>
<sectionHeader confidence="0.867179" genericHeader="method">
4 Summarization with Bayesian Surprise
</sectionHeader>
<bodyText confidence="0.999990571428572">
We consider the hypothesis space H as the set of
all the hypotheses encoding background knowl-
edge. A single hypothesis about the background
takes the form of a multinomial distribution over
word unigrams. For example, one multinomial
may have higher word probabilities for ‘Ukraine’
and ‘peaceful’ and another multinomial has higher
probabilities for ‘Ukraine’ and ‘riots’. P(H) gives
a prior probability to each hypothesis based on
the information in the background corpus. In our
case, P(H) is a Dirichlet distribution, the conju-
gate prior for multinomials. Suppose that the vo-
cabulary size of the background corpus is V and
we label the word types as (w1, w2, ... wV ). Then,
</bodyText>
<equation confidence="0.989101">
P(H) = Dir(α1, α2, ...αV ) (6)
</equation>
<bodyText confidence="0.999910428571429">
where α1:V are the concentration parameters of
the Dirichlet distribution (and will be set using the
background corpus as explained in Section 4.2).
Now consider a new observation I (a text, sen-
tence, or paragraph from the summarization input)
and the word counts in I given by (c1, c2, ..., cV ).
Then the posterior over H is the dirichlet:
</bodyText>
<equation confidence="0.9888645">
P(H|I) = Dir(α1 + c1, α2 + c2, ..., αV + cV )
(7)
</equation>
<bodyText confidence="0.999974666666666">
The surprise due to observing I, S(I, H) is the
KL divergence between the two dirichlet distribu-
tions. (Details about computing KL divergence
between two dirichlet distributions can be found
in Penny (2001) and Baldi and Itti (2010)).
Below we propose a general algorithm for sum-
marization using surprise computation. Then we
define the prior distribution P(H) for each of our
two tasks, GENERIC and UPDATE summarization.
</bodyText>
<subsectionHeader confidence="0.989792">
4.1 Extractive summarization algorithm
</subsectionHeader>
<bodyText confidence="0.9785602">
We first compute a surprise value for each word
type in the summarization input. Word scores are
aggregated to obtain a score for each sentence.
Step 1: Word score. Suppose that word type
wi appears ci times in the summarization input
I. We obtain the posterior distribution after see-
ing all instances of this word (wi) as P(H|wi) =
Dir(α1, α2, ...αi + ci, ...αV ). The score for wi is
the surprise computed as KL divergence between
P(H|wi) and the prior P(H) (eqn. 6).
</bodyText>
<figureCaption confidence="0.906505666666667">
Step 2: Sentence score. The composition
functions to obtain sentence scores from word
scores can impact content selection performance
(Nenkova et al., 2006). We experiment with sum
and average value of the word scores.1
Step 3: Sentence selection. The goal is to se-
</figureCaption>
<bodyText confidence="0.987115958333333">
lect a subset of sentences with high surprise val-
ues. We follow a greedy approach to optimize the
summary surprise by choosing the most surprising
sentence, the next most surprising and so on. At
the same time, we aim to avoid redundancy, i.e.
selecting sentences with similar content. After a
sentence is selected for the summary, the surprise
for words from this sentence are set to zero. We re-
compute the surprise for the remaining sentences
using step 2 and the selection process continues
until the summary length limit is reached.
The key differences between our Bayesian ap-
proach and a method such as topic words are: (i)
The Bayesian approach keeps multiple hypothe-
ses about the background rather than a single one.
Surprise is computed based on the changes in
probabilities of all of these hypotheses upon see-
ing the summarization input. (ii) The computation
of topic words is local, it assumes a binomial dis-
tribution and the occurrence of a word is indepen-
dent of others. In contrast, word surprise although
computed for each word type separately, quantifies
the surprise when incorporating the new counts of
this word into the background multinomials.
</bodyText>
<subsectionHeader confidence="0.921931">
4.2 Input and background
</subsectionHeader>
<bodyText confidence="0.964770714285715">
Here we describe the input sets and background
corpus used for the two summarization tasks and
1An alternative algorithm could directly compute the sur-
prise of a sentence by incorporating the words from the sen-
tence into the posterior. However, we found this specific
method to not work well probably because the few and un-
repeated content words from a sentence did not change the
posterior much. In future, we plan to use latent topic models
to assign a topic to a sentence so that the counts of all the
sentence’s words can be aggregated into one dimension.
The surprise S(D, H) created by D on hypoth-
esis space H is defined as the difference between
the prior and posterior distributions over the hy-
potheses, and is computed using KL divergence.
</bodyText>
<equation confidence="0.9836915">
S(D, H) = KL(P(H|D), P(H))) (4)
P(H|D)
</equation>
<page confidence="0.988988">
335
</page>
<bodyText confidence="0.999975514285714">
define the prior distribution for each. We use data
from the DUC2 and TAC3 summarization evalua-
tion workshops conducted by NIST.
Generic summarization. We use multidocument
inputs from DUC 2004. There were 50 inputs,
each contains around 10 documents on a common
topic. Each input is also provided with 4 manually
written summaries created by NIST assessors. We
use these manual summaries for evaluation.
The background corpus is a collection of 5000
randomly selected articles from the English Giga-
word corpus. We use a list of 571 stop words from
the SMART IR system (Buckley, 1985) and the re-
maining content word vocabulary has 59,497 word
types. The count of each word in the background
is calculated and used as the α parameters of the
prior Dirichlet distribution P(H) (eqn. 6).
Update summarization. This task uses data from
TAC 2009. An input has two sets of documents, A
and B, each containing 10 documents. Both A and
B are on same topic but documents in B were pub-
lished at a later time than A (background). There
were 44 inputs and 4 manual update summaries
are provided for each.
The prior parameters are the counts of words
in A for that input (using the same stoplist). The
vocabulary of these A sets is smaller, ranging from
400 to 3000 words for the different inputs.
In practice for both tasks, a new summarization
input can have words unseen in the background.
So new words in an input are added to the back-
ground corpus with a count of 1 and the counts of
existing words in the background are incremented
by 1 before computing the prior parameters. The
summary length limit is 100 words in both tasks.
</bodyText>
<sectionHeader confidence="0.99808" genericHeader="method">
5 Systems for comparison
</sectionHeader>
<bodyText confidence="0.99995475">
We compare against three types of systems, (i)
those which similarly to surprise, use a back-
ground corpus to identify important sentences, (ii)
a system that uses information from the input set
only and no background, and (iii) systems that
combine scores from the input and background.
KLback: represents a simple baseline for sur-
prise computation from a background corpus. A
single unigram probability distribution B is cre-
ated from the background using maximum like-
lihood. The summary is created by greedily
adding sentences which maximize KL divergence
</bodyText>
<footnote confidence="0.997872666666667">
2http://www-nlpir.nist.gov/projects/
duc/index.html
3http://www.nist.gov/tac/
</footnote>
<bodyText confidence="0.998732456521739">
between B and the current summary. Suppose
the set of sentences currently chosen in the sum-
mary is S. The next step chooses the sentence
sl = argmaxsz KL({S ∪ si}||B) .
TSsum, TSavg: use topic words computed as de-
scribed in Section 2 and utilizing the same back-
ground corpus for the generic and update tasks
as the surprise-based methods. For the generic
task, we use a critical value of 10 (0.001 signif-
icance level) for the x2 distribution during topic
word computation. In the update task however, the
background corpus A is smaller and for most in-
puts, no words exceeded this cutoff. We lower the
significance level to the generally accepted value
of 0.05 and take words scoring above this as topic
words. The number of topic words is still small
(ranging from 1 to 30) for different inputs.
The TSsum system selects sentences with greater
counts of topic words and TSavg computes the
number of topic words normalized by sentence
length. A greedy selection procedure is used. To
reduce redundancy, once a sentence is added, the
topic words contained in it are removed from the
topic word list before the next sentence selection.
KLinp: represents the system that does not use
background information. Rather the method cre-
ates a summary by optimizing for high similarity
of the summary with the input word distribution.
Suppose the input unigram distribution is I and
the current summary is S, the method chooses the
sentence sl = arg mins, KL({S ∪ si}||I) at each
iteration. Since {S ∪ si} is used to compute diver-
gence, redundancy is implicitly controlled in this
approach. Such a KL objective was used in com-
petitive systems in the past (Daum´e III and Marcu,
2006; Haghighi and Vanderwende, 2009).
Input + background: These systems com-
bine (i) a score based on the background (KLback,
TS or SR) with (ii) the score based on the input
only (KLinp). For example, to combine TSsum and
KLinp: for each sentence, we compute its scores
based on the two methods. Then we normalize the
two sets of scores for candidate sentences using z-
scores and compute the best sentence as arg maxs,
(TSsum(si) - KLinp(si)). Redundancy control is
done similarly to the TS only systems.
</bodyText>
<sectionHeader confidence="0.924192" genericHeader="method">
6 Content selection results
</sectionHeader>
<bodyText confidence="0.99426">
For evaluation, we compare each summary to the
four manual summaries using ROUGE (Lin and
Hovy, 2003; Lin, 2004). All summaries were trun-
cated to 100 words, stemming was performed and
</bodyText>
<page confidence="0.99665">
336
</page>
<table confidence="0.976948818181818">
0.0477 (SRsum, SRavg)
0.2909 (SRsum, SRavg)
0.0635
0.3010 (KLinp+SRsum, avg)
KLback ROUGE-1 ROUGE-2
TSsum 0.2276 (TS, SR) 0.0250 (TS, SR)
TSavg 0.3078 0.0616
0.2841 (TSsum, SRsum) 0.0493 (TSsum)
SRsum 0.3120 0.0580
SRavg 0.3003 0.0549
KLinp 0.3075 (KLinp+TSavg) 0.0684
KLinp+TSsum 0.3250 0.0725
KLinp+TSavg 0.3410 0.0795
KLinp+SRsum 0.3187 (KLinp+TSavg) 0.0660 (KLinp+TSavg)
KLinp+SRavg 0.3220 (KLinp+TSavg) 0.0696
ROUGE-1 ROUGE-2
0.2246 (TS, SR) 0.0213 (TS, SR)
0.3037 (SRavg) 0.0563
0.3201 0.0640
0.3226 0.0639
0.3098 (KLinp+SRavg) 0.0710
0.3021 (KLinp+SRsum, avg) 0.0543 (KLinp,
</table>
<figure confidence="0.918899846153846">
KLinp+SRsum, avg)
0.3292 0.0721
0.3379 0.0767
KLback
TSsum
TSavg
SRsum
SRavg
KLinp
KLinp+TSsum
KLinp+TSavg
KLinp+SRsum
KLinp+SRavg
</figure>
<tableCaption confidence="0.9923725">
Table 1: Evaluation results for generic summaries.
Systems in parentheses are significantly better.
</tableCaption>
<bodyText confidence="0.999787425">
stop words were not removed, as is standard in
TAC evaluations. We report the ROUGE-1 and
ROUGE-2 recall scores (average over the inputs)
for each system. We use the Wilcoxon signed-rank
test to check for significant differences in mean
scores. Table 1 shows the scores for generic sum-
maries and 2 for the update task. For each system,
the peer systems with significantly better scores
(p-value &lt; 0.05) are indicated within parentheses.
We refer to the surprise-based summaries as
SRsum and SRavg depending on the type of com-
position function (Section 4.1).
First, consider GENERIC summarization and the
systems which use the background corpus only
(those above the horizontal line). The KLback
baseline performs significantly worse than topic
words and surprise summaries. Numerically,
SRsum has the highest ROUGE-1 score and TSsum
tops according to ROUGE-2. As per the Wilcoxon
test, TSsum, SRsum and SRavg scores are statisti-
cally indistinguishable at 95% confidence level.
Systems below the horizontal line in Table 1
use an objective which combines both similarity
with the input and difference from the background.
The first line here shows that a system optimiz-
ing only for input similarity, KLinp, by itself has
higher scores (though not significant) than those
using background information only. This result is
not surprising for generic summarization where all
the topical content is present in the input and the
background is a non-focused random collection.
At the same time, adding either TS or SR scores
to KLinp almost always leads to better results with
KLinp + TSavg giving the best score.
In UPDATE summarization, the surprise-based
methods have an advantage over the topic word
ones. SRavg is significantly better than TSavg
for both ROUGE-1 and ROUGE-2 scores and
better than TSsum according to ROUGE-1. In
fact, the surprise methods have numerically higher
</bodyText>
<tableCaption confidence="0.900247">
Table 2: Evaluation results for update summaries.
Systems in parentheses are significantly better.
</tableCaption>
<bodyText confidence="0.996660052631579">
ROUGE-1 scores compared to input similarity
(KLinp) in contrast to generic summarization.
When combined with KLinp, the surprise meth-
ods provide improved results, significantly better
in terms of ROUGE-1 scores. The TS methods do
not lead to any improvement, and KLinp + TSavg
is significantly worse than KLinp only. The limi-
tation of the TS approach arises from the paucity
of topic words that exceed the significance cutoff
applied on the log-likelihood ratio. But Bayesian
surprise is robust on the small background corpus
and does not need any tuning for cutoff values de-
pending on the size of the background set.
Note that these models do not perform on par
with summarization systems that use multiple in-
dicators of content importance, involve supervised
training and which perform sentence compression.
Rather our goal in this work is to demonstrate a
simple and intuitive unsupervised model.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999926">
We have introduced a Bayesian summarization
method that strongly aligns with intuitions about
how people use existing knowledge to identify im-
portant events or content in new observations.
Our method is especially valuable when a sys-
tem must utilize a small background corpus.
While the update task datasets we have used were
carefully selected and grouped by NIST assesors
into initial and background sets, for systems on
the web, there is little control over the number of
background documents on a particular topic. A
system should be able to use smaller amounts of
background information and as new data arrives,
be able to incorporate the evidence. Our Bayesian
approach is a natural fit in such a setting.
</bodyText>
<sectionHeader confidence="0.997514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999591333333333">
The author was supported by a Newton Interna-
tional Fellowship (NF120479) from the Royal So-
ciety and the British Academy.
</bodyText>
<page confidence="0.997547">
337
</page>
<sectionHeader confidence="0.995868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999683901960784">
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and
novelty detection at the sentence level. In Proceed-
ings of SIGIR, pages 314–321.
P. Baldi and L. Itti. 2010. Of bits and wows: a
bayesian theory of surprise with applications to at-
tention. Neural Networks, 23(5):649–666.
L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo.
2010. The sixth PASCAL recognizing textual en-
tailment challenge. Proceedings of TAC.
C. Buckley. 1985. Implementation of the SMART in-
formation retrieval system. Technical report, Cor-
nell University.
J. Conroy, J. Schlesinger, and D. O’Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
COLING-ACL, pages 152–159.
H. Daum´e III and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL,
pages 305–312.
J. Delort and E. Alfonseca. 2012. DualSum: A topic-
model based approach for update summarization. In
Proceedings of EACL, pages 214–223.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of NAACL-HLT, pages 362–370.
L. Itti and P. F. Baldi. 2006. Bayesian surprise attracts
human attention. In Proceedings of NIPS, pages
547–554.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING, pages 495–501.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 1085–1090.
C. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Proceedings of Text Sum-
marization Branches Out Workshop, ACL, pages 74–
81.
A. Nenkova, L. Vanderwende, and K. McKeown.
2006. A compositional context sensitive multi-
document summarizer: exploring the factors that in-
fluence summarization. In Proceedings of SIGIR,
pages 573–580.
W. D Penny. 2001. Kullback-liebler divergences
of normal, gamma, dirichlet and wishart densities.
Wellcome Department of Cognitive Neurology.
B. Schiffman. 2005. Learning to Identify New Infor-
mation. Ph.D. thesis, Columbia University.
I. Soboroff and D. Harman. 2005. Novelty detection:
the trec experience. In Proceedings of HLT-EMNLP,
pages 105–112.
</reference>
<page confidence="0.99837">
338
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960621">
<title confidence="0.9972505">A Bayesian Method to Incorporate Background Knowledge Automatic Text Summarization</title>
<author confidence="0.976901">Annie Louis</author>
<affiliation confidence="0.999931">ILCC, School of Informatics, University of Edinburgh,</affiliation>
<address confidence="0.99971">Edinburgh EH8 9AB, UK</address>
<email confidence="0.996743">alouis@inf.ed.ac.uk</email>
<abstract confidence="0.999596666666667">In order to summarize a document, it is useful to have a of documents from the domain to serve as a reference for determining new and important information in the input document. We present a model based on Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus. Specifically, the method quantifies the degree to which pieces of information in the input change one’s beliefs’ about the world represented in the background. We develop systems for generic and update summarization based on this idea. Our method provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>C Wade</author>
<author>A Bolivar</author>
</authors>
<title>Retrieval and novelty detection at the sentence level.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>314--321</pages>
<contexts>
<context position="4792" citStr="Allan et al., 2003" startWordPosition="779" endWordPosition="782">s a large random collection of news articles as the background, and b) UPDATE summarization where the background is a smaller but specific set of news documents on the same topic as the input set. We find that our method performs competitively with a previous log-likelihood ratio approach which identifies words with significantly higher probability in the input compared to the background. The Bayesian approach is more advantageous in the update task, where the background corpus is smaller in size. 2 Related work Computing new information is useful in many applications. The TREC novelty tasks (Allan et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alf</context>
</contexts>
<marker>Allan, Wade, Bolivar, 2003</marker>
<rawString>J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and novelty detection at the sentence level. In Proceedings of SIGIR, pages 314–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Baldi</author>
<author>L Itti</author>
</authors>
<title>Of bits and wows: a bayesian theory of surprise with applications to attention.</title>
<date>2010</date>
<journal>Neural Networks,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="8777" citStr="Baldi and Itti (2010)" startWordPosition="1517" endWordPosition="1520">ng the background knowledge of a user. The user has a probability P(H) associated with each hypothesis H E H. Let D be a new observation. The posterior probability of a single hypothesis H can be computed as: P(H|D) = P( |H))(H) (3) P(D|H1) λ = P(D|H2) 334 fH P(H |D) log P(H) (5) Note that since KL-divergence is not symmetric, we could also compute KL(P(H), P(H|D)) as the surprise value. In some cases, surprise can be computed analytically, in particular when the prior distribution is conjugate to the form of the hypothesis, and so the posterior has the same functional form as the prior. (See Baldi and Itti (2010) for the surprise computation for different families of probability distributions). 4 Summarization with Bayesian Surprise We consider the hypothesis space H as the set of all the hypotheses encoding background knowledge. A single hypothesis about the background takes the form of a multinomial distribution over word unigrams. For example, one multinomial may have higher word probabilities for ‘Ukraine’ and ‘peaceful’ and another multinomial has higher probabilities for ‘Ukraine’ and ‘riots’. P(H) gives a prior probability to each hypothesis based on the information in the background corpus. In</context>
<context position="10234" citStr="Baldi and Itti (2010)" startWordPosition="1762" endWordPosition="1765">re α1:V are the concentration parameters of the Dirichlet distribution (and will be set using the background corpus as explained in Section 4.2). Now consider a new observation I (a text, sentence, or paragraph from the summarization input) and the word counts in I given by (c1, c2, ..., cV ). Then the posterior over H is the dirichlet: P(H|I) = Dir(α1 + c1, α2 + c2, ..., αV + cV ) (7) The surprise due to observing I, S(I, H) is the KL divergence between the two dirichlet distributions. (Details about computing KL divergence between two dirichlet distributions can be found in Penny (2001) and Baldi and Itti (2010)). Below we propose a general algorithm for summarization using surprise computation. Then we define the prior distribution P(H) for each of our two tasks, GENERIC and UPDATE summarization. 4.1 Extractive summarization algorithm We first compute a surprise value for each word type in the summarization input. Word scores are aggregated to obtain a score for each sentence. Step 1: Word score. Suppose that word type wi appears ci times in the summarization input I. We obtain the posterior distribution after seeing all instances of this word (wi) as P(H|wi) = Dir(α1, α2, ...αi + ci, ...αV ). The s</context>
</contexts>
<marker>Baldi, Itti, 2010</marker>
<rawString>P. Baldi and L. Itti. 2010. Of bits and wows: a bayesian theory of surprise with applications to attention. Neural Networks, 23(5):649–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>P Clark</author>
<author>I Dagan</author>
<author>D Giampiccolo</author>
</authors>
<title>The sixth PASCAL recognizing textual entailment challenge.</title>
<date>2010</date>
<booktitle>Proceedings of TAC.</booktitle>
<contexts>
<context position="5282" citStr="Bentivogli et al., 2010" startWordPosition="860" endWordPosition="864">s smaller in size. 2 Related work Computing new information is useful in many applications. The TREC novelty tasks (Allan et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to learn and use topics discussed only in background, those only in the update input and those that overlap across the two sets. Even for generic summarization, some of the best results were obtained by Conroy et al. (2006) by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by Lin and Hovy (2000). Central to this approach is the use of a likelihood ratio test to compute topic words, words tha</context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Giampiccolo, 2010</marker>
<rawString>L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo. 2010. The sixth PASCAL recognizing textual entailment challenge. Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
</authors>
<title>Implementation of the SMART information retrieval system.</title>
<date>1985</date>
<tech>Technical report,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="13763" citStr="Buckley, 1985" startWordPosition="2358" endWordPosition="2359">P(H|D), P(H))) (4) P(H|D) 335 define the prior distribution for each. We use data from the DUC2 and TAC3 summarization evaluation workshops conducted by NIST. Generic summarization. We use multidocument inputs from DUC 2004. There were 50 inputs, each contains around 10 documents on a common topic. Each input is also provided with 4 manually written summaries created by NIST assessors. We use these manual summaries for evaluation. The background corpus is a collection of 5000 randomly selected articles from the English Gigaword corpus. We use a list of 571 stop words from the SMART IR system (Buckley, 1985) and the remaining content word vocabulary has 59,497 word types. The count of each word in the background is calculated and used as the α parameters of the prior Dirichlet distribution P(H) (eqn. 6). Update summarization. This task uses data from TAC 2009. An input has two sets of documents, A and B, each containing 10 documents. Both A and B are on same topic but documents in B were published at a later time than A (background). There were 44 inputs and 4 manual update summaries are provided for each. The prior parameters are the counts of words in A for that input (using the same stoplist).</context>
</contexts>
<marker>Buckley, 1985</marker>
<rawString>C. Buckley. 1985. Implementation of the SMART information retrieval system. Technical report, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Conroy</author>
<author>J Schlesinger</author>
<author>D O’Leary</author>
</authors>
<title>Topic-focused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>152--159</pages>
<marker>Conroy, Schlesinger, O’Leary, 2006</marker>
<rawString>J. Conroy, J. Schlesinger, and D. O’Leary. 2006. Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of COLING-ACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Bayesian queryfocused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>305--312</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>H. Daum´e III and D. Marcu. 2006. Bayesian queryfocused summarization. In Proceedings of ACL, pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Delort</author>
<author>E Alfonseca</author>
</authors>
<title>DualSum: A topicmodel based approach for update summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>214--223</pages>
<contexts>
<context position="5405" citStr="Delort and Alfonseca, 2012" startWordPosition="882" endWordPosition="885">n et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to learn and use topics discussed only in background, those only in the update input and those that overlap across the two sets. Even for generic summarization, some of the best results were obtained by Conroy et al. (2006) by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by Lin and Hovy (2000). Central to this approach is the use of a likelihood ratio test to compute topic words, words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the in</context>
</contexts>
<marker>Delort, Alfonseca, 2012</marker>
<rawString>J. Delort and E. Alfonseca. 2012. DualSum: A topicmodel based approach for update summarization. In Proceedings of EACL, pages 214–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>362--370</pages>
<contexts>
<context position="17164" citStr="Haghighi and Vanderwende, 2009" startWordPosition="2935" endWordPosition="2938">from the topic word list before the next sentence selection. KLinp: represents the system that does not use background information. Rather the method creates a summary by optimizing for high similarity of the summary with the input word distribution. Suppose the input unigram distribution is I and the current summary is S, the method chooses the sentence sl = arg mins, KL({S ∪ si}||I) at each iteration. Since {S ∪ si} is used to compute divergence, redundancy is implicitly controlled in this approach. Such a KL objective was used in competitive systems in the past (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Input + background: These systems combine (i) a score based on the background (KLback, TS or SR) with (ii) the score based on the input only (KLinp). For example, to combine TSsum and KLinp: for each sentence, we compute its scores based on the two methods. Then we normalize the two sets of scores for candidate sentences using zscores and compute the best sentence as arg maxs, (TSsum(si) - KLinp(si)). Redundancy control is done similarly to the TS only systems. 6 Content selection results For evaluation, we compare each summary to the four manual summaries using ROUGE (Lin and Hovy, 2003; Li</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of NAACL-HLT, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Itti</author>
<author>P F Baldi</author>
</authors>
<title>Bayesian surprise attracts human attention.</title>
<date>2006</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>547--554</pages>
<contexts>
<context position="2521" citStr="Itti and Baldi, 2006" startWordPosition="411" endWordPosition="414">ticles. Other specialized summarization tasks explicitly require the use of background information. In the UPDATE summarization task, a system is given two sets of news documents on the same topic; the second contains articles published later in time. The system should summarize the important updates from the second set assuming a user has already read the first set of articles. In this work, we present a Bayesian model for assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents. Our model is based on the idea of Bayesian Surprise (Itti and Baldi, 2006). For illustration, assume that a user’s background knowledge comprises of multiple hypotheses about the current state of the world and a probability distribution over these hypotheses indicates his degree of belief in each hypothesis. For example, one hypothesis may be that the political situation in Ukraine is peaceful, another where it is not. Apriori assume the user favors the hypothesis about a peaceful Ukraine, i.e. the hypothesis has higher probability in the prior distribution. Given new data, the evidence can be incorporated using Bayes Rule to compute the posterior distribution over </context>
<context position="8063" citStr="Itti and Baldi (2006)" startWordPosition="1388" endWordPosition="1391">ch is that −2 log λ is asymptotically χ2 distributed. So for a resulting −2 log λ value, we can use the χ2 table to find the significance level with which the null hypothesis H1 can be rejected. For example, a value of 10 corresponds to a significance level of 0.001 and is standardly used as the cutoff. Words with −2 log λ &gt; 10 are considered topic words. Conroy et al. (2006)’s system gives a weight of 1 to the topic words and scores sentences using the number of topic words normalized by sentence length. 3 Bayesian Surprise First we present the formal definition of Bayesian surprise given by Itti and Baldi (2006) without reference to the summarization task. Let H be the space of all hypotheses representing the background knowledge of a user. The user has a probability P(H) associated with each hypothesis H E H. Let D be a new observation. The posterior probability of a single hypothesis H can be computed as: P(H|D) = P( |H))(H) (3) P(D|H1) λ = P(D|H2) 334 fH P(H |D) log P(H) (5) Note that since KL-divergence is not symmetric, we could also compute KL(P(H), P(H|D)) as the surprise value. In some cases, surprise can be computed analytically, in particular when the prior distribution is conjugate to the </context>
</contexts>
<marker>Itti, Baldi, 2006</marker>
<rawString>L. Itti and P. F. Baldi. 2006. Bayesian surprise attracts human attention. In Proceedings of NIPS, pages 547–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="5784" citStr="Lin and Hovy (2000)" startWordPosition="951" endWordPosition="954">ce list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to learn and use topics discussed only in background, those only in the update input and those that overlap across the two sets. Even for generic summarization, some of the best results were obtained by Conroy et al. (2006) by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by Lin and Hovy (2000). Central to this approach is the use of a likelihood ratio test to compute topic words, words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the input’s topic. In this work, we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach. We briefly explain the topic words based approach below. Computing topic words: Let us call the input set I and the background B. The log-likelihood ratio test compares two h</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of COLING, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>1085--1090</pages>
<contexts>
<context position="17760" citStr="Lin and Hovy, 2003" startWordPosition="3038" endWordPosition="3041">and Vanderwende, 2009). Input + background: These systems combine (i) a score based on the background (KLback, TS or SR) with (ii) the score based on the input only (KLinp). For example, to combine TSsum and KLinp: for each sentence, we compute its scores based on the two methods. Then we normalize the two sets of scores for candidate sentences using zscores and compute the best sentence as arg maxs, (TSsum(si) - KLinp(si)). Redundancy control is done similarly to the TS only systems. 6 Content selection results For evaluation, we compare each summary to the four manual summaries using ROUGE (Lin and Hovy, 2003; Lin, 2004). All summaries were truncated to 100 words, stemming was performed and 336 0.0477 (SRsum, SRavg) 0.2909 (SRsum, SRavg) 0.0635 0.3010 (KLinp+SRsum, avg) KLback ROUGE-1 ROUGE-2 TSsum 0.2276 (TS, SR) 0.0250 (TS, SR) TSavg 0.3078 0.0616 0.2841 (TSsum, SRsum) 0.0493 (TSsum) SRsum 0.3120 0.0580 SRavg 0.3003 0.0549 KLinp 0.3075 (KLinp+TSavg) 0.0684 KLinp+TSsum 0.3250 0.0725 KLinp+TSavg 0.3410 0.0795 KLinp+SRsum 0.3187 (KLinp+TSavg) 0.0660 (KLinp+TSavg) KLinp+SRavg 0.3220 (KLinp+TSavg) 0.0696 ROUGE-1 ROUGE-2 0.2246 (TS, SR) 0.0213 (TS, SR) 0.3037 (SRavg) 0.0563 0.3201 0.0640 0.3226 0.0639</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT-NAACL, pages 1085–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of Text Summarization Branches Out Workshop, ACL,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="17772" citStr="Lin, 2004" startWordPosition="3042" endWordPosition="3043">9). Input + background: These systems combine (i) a score based on the background (KLback, TS or SR) with (ii) the score based on the input only (KLinp). For example, to combine TSsum and KLinp: for each sentence, we compute its scores based on the two methods. Then we normalize the two sets of scores for candidate sentences using zscores and compute the best sentence as arg maxs, (TSsum(si) - KLinp(si)). Redundancy control is done similarly to the TS only systems. 6 Content selection results For evaluation, we compare each summary to the four manual summaries using ROUGE (Lin and Hovy, 2003; Lin, 2004). All summaries were truncated to 100 words, stemming was performed and 336 0.0477 (SRsum, SRavg) 0.2909 (SRsum, SRavg) 0.0635 0.3010 (KLinp+SRsum, avg) KLback ROUGE-1 ROUGE-2 TSsum 0.2276 (TS, SR) 0.0250 (TS, SR) TSavg 0.3078 0.0616 0.2841 (TSsum, SRsum) 0.0493 (TSsum) SRsum 0.3120 0.0580 SRavg 0.3003 0.0549 KLinp 0.3075 (KLinp+TSavg) 0.0684 KLinp+TSsum 0.3250 0.0725 KLinp+TSavg 0.3410 0.0795 KLinp+SRsum 0.3187 (KLinp+TSavg) 0.0660 (KLinp+TSavg) KLinp+SRavg 0.3220 (KLinp+TSavg) 0.0696 ROUGE-1 ROUGE-2 0.2246 (TS, SR) 0.0213 (TS, SR) 0.3037 (SRavg) 0.0563 0.3201 0.0640 0.3226 0.0639 0.3098 (KLi</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of Text Summarization Branches Out Workshop, ACL, pages 74– 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
<author>K McKeown</author>
</authors>
<title>A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>573--580</pages>
<contexts>
<context position="11089" citStr="Nenkova et al., 2006" startWordPosition="1903" endWordPosition="1906">rst compute a surprise value for each word type in the summarization input. Word scores are aggregated to obtain a score for each sentence. Step 1: Word score. Suppose that word type wi appears ci times in the summarization input I. We obtain the posterior distribution after seeing all instances of this word (wi) as P(H|wi) = Dir(α1, α2, ...αi + ci, ...αV ). The score for wi is the surprise computed as KL divergence between P(H|wi) and the prior P(H) (eqn. 6). Step 2: Sentence score. The composition functions to obtain sentence scores from word scores can impact content selection performance (Nenkova et al., 2006). We experiment with sum and average value of the word scores.1 Step 3: Sentence selection. The goal is to select a subset of sentences with high surprise values. We follow a greedy approach to optimize the summary surprise by choosing the most surprising sentence, the next most surprising and so on. At the same time, we aim to avoid redundancy, i.e. selecting sentences with similar content. After a sentence is selected for the summary, the surprise for words from this sentence are set to zero. We recompute the surprise for the remaining sentences using step 2 and the selection process continu</context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization. In Proceedings of SIGIR, pages 573–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W D Penny</author>
</authors>
<title>Kullback-liebler divergences of normal, gamma, dirichlet and wishart densities. Wellcome Department of Cognitive Neurology.</title>
<date>2001</date>
<contexts>
<context position="10208" citStr="Penny (2001)" startWordPosition="1759" endWordPosition="1760">, ...αV ) (6) where α1:V are the concentration parameters of the Dirichlet distribution (and will be set using the background corpus as explained in Section 4.2). Now consider a new observation I (a text, sentence, or paragraph from the summarization input) and the word counts in I given by (c1, c2, ..., cV ). Then the posterior over H is the dirichlet: P(H|I) = Dir(α1 + c1, α2 + c2, ..., αV + cV ) (7) The surprise due to observing I, S(I, H) is the KL divergence between the two dirichlet distributions. (Details about computing KL divergence between two dirichlet distributions can be found in Penny (2001) and Baldi and Itti (2010)). Below we propose a general algorithm for summarization using surprise computation. Then we define the prior distribution P(H) for each of our two tasks, GENERIC and UPDATE summarization. 4.1 Extractive summarization algorithm We first compute a surprise value for each word type in the summarization input. Word scores are aggregated to obtain a score for each sentence. Step 1: Word score. Suppose that word type wi appears ci times in the summarization input I. We obtain the posterior distribution after seeing all instances of this word (wi) as P(H|wi) = Dir(α1, α2, </context>
</contexts>
<marker>Penny, 2001</marker>
<rawString>W. D Penny. 2001. Kullback-liebler divergences of normal, gamma, dirichlet and wishart densities. Wellcome Department of Cognitive Neurology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schiffman</author>
</authors>
<title>Learning to Identify New Information.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="4837" citStr="Schiffman, 2005" startWordPosition="787" endWordPosition="788"> the background, and b) UPDATE summarization where the background is a smaller but specific set of news documents on the same topic as the input set. We find that our method performs competitively with a previous log-likelihood ratio approach which identifies words with significantly higher probability in the input compared to the background. The Bayesian approach is more advantageous in the update task, where the background corpus is smaller in size. 2 Related work Computing new information is useful in many applications. The TREC novelty tasks (Allan et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to learn and use topi</context>
</contexts>
<marker>Schiffman, 2005</marker>
<rawString>B. Schiffman. 2005. Learning to Identify New Information. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Soboroff</author>
<author>D Harman</author>
</authors>
<title>Novelty detection: the trec experience.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="4819" citStr="Soboroff and Harman, 2005" startWordPosition="783" endWordPosition="786">lection of news articles as the background, and b) UPDATE summarization where the background is a smaller but specific set of news documents on the same topic as the input set. We find that our method performs competitively with a previous log-likelihood ratio approach which identifies words with significantly higher probability in the input compared to the background. The Bayesian approach is more advantageous in the update task, where the background corpus is smaller in size. 2 Related work Computing new information is useful in many applications. The TREC novelty tasks (Allan et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to </context>
</contexts>
<marker>Soboroff, Harman, 2005</marker>
<rawString>I. Soboroff and D. Harman. 2005. Novelty detection: the trec experience. In Proceedings of HLT-EMNLP, pages 105–112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>