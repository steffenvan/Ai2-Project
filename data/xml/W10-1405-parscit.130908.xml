<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998663">
Modeling Morphosyntactic Agreement
in Constituency-Based Parsing of Modern Hebrew
</title>
<author confidence="0.996685">
Reut Tsarfaty∗ and Khalil Sima’an
</author>
<affiliation confidence="0.9987275">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<email confidence="0.992047">
{r.tsarfaty,k.simaan}@uva.nl
</email>
<sectionHeader confidence="0.997329" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911733333333">
We show that naive modeling of morphosyn-
tactic agreement in a Constituency-Based
(CB) statistical parsing model is worse than
none, whereas a linguistically adequate way
of modeling inflectional morphology in CB
parsing leads to improved performance. In
particular, we show that an extension of the
Relational-Realizational (RR) model that in-
corporates agreement features is superior to
CB models that treat morphosyntax as state-
splits (SP), and that the RR model benefits
more from inflectional features. We focus on
parsing Hebrew and report the best result to
date, F184.13 for parsing off of gold-tagged
text, 5% error reduction from previous results.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984511944444444">
Agreement is defined by linguists as the system-
atic covariance of the grammatical properties of one
linguistic element to reflect the semantic or formal
properties of another (Corbett, 2001). Morpholog-
ically marked agreement features such as gender,
number and person are used to realize grammat-
ical relations between syntactic constituents, and
such patterns are abundantly found in (less- or) non-
configurational languages (Hale, 1983) where the
order of words is known to be (relatively) free.
Agreement features encompass information con-
cerning the functional relations between constituents
in the syntactic structure, but whether incorporat-
ing agreement features in a statistical parsing model
leads to improved performance has so far remained
an open question and saw contradictory results.
∗The first author is currently a researcher at the department
of Linguistics and Philology at Uppsala University.
</bodyText>
<page confidence="0.977824">
40
</page>
<bodyText confidence="0.999964138888889">
Taking Semitic languages as an example, it was
shown that an SVM-based shallow parser (Gold-
berg et al., 2006) does not benefit from includ-
ing agreement features for NP chunking in Hebrew.
Phrase-structure based parsers for Arabic system-
atically discard morphological features from their
label-set and never parametrize agreement explic-
itly (Maamouri et al., 2008). Models based on deep
grammars such as CCG (Hockenmaier and Steed-
man, 2003) and HPSG (Miyao and Tsujii, 2008)
could in principle use inflectional morphology, but
they currently rely on functional information mainly.
For formalisms that do incorporate morphology,
generative models are may leak probability due to
unification failures (Abney, 1997). Even results
from dependency parsing remain inconclusive. It
was shown for dependency parsing that case, defi-
niteness and animacy features are useful to enhance
parsing (e.g., (Øvrelid and Nivre, 2007)), agreement
patterns are often excluded. When agreement fea-
tures were included as features in dependency parser
for Hebrew in (Goldberg and Elhadad, 2009) for He-
brew they obtained tiny-to-no improvement.
A question thus emerges whether there are any
benefits in explicitly incorporating morphosyntactic
agreement patterns into our models. This question is
a manifestation of a greater issue, namely, whether
it is beneficial to represent complex patterns of mor-
phology in the statistical parsing model, or whether
configurational information subsume the relevant
patterns, as it is commonly assumed in constituency-
based parsing. Here we claim that agreement fea-
tures are useful for statistical parsing provided that
they are represented and parametrized in a way that
reflects their linguistic substance; to express func-
tional information orthogonal to configuration.
</bodyText>
<note confidence="0.9845165">
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 40–48,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998475">
We do so by extending the Relational-
Realizational (RR) model we presented in (Tsarfaty
and Sima’an, 2008) to explicitly encode agreement
features in its native representation (RR-AGR). In
the RR model, a joint distribution over grammatical
relations is firstly articulated in the projection phase.
The grammatical relations may be spelled out by
positioning them with respect to one another in the
configuration phase, through the use of morphology
in the realization phase, or both. This paper shows
that, for Hebrew, this RR-AGR strategy signifi-
cantly outperforms a constituency-based model that
treats agreement features as internally structured
non-terminal state-splits (SP-AGR). As we accumu-
late morphological features, the performance gap
between the RR and SP models becomes larger.
The best result we report for the RR-AGR model,
F184.13, is the best result reported for Hebrew to
date for parsing gold PoS-tagged segments, with
5% error reduction from previous results. This
result is also significantly higher than all parsing
results reported so far for Arabic, a Semitic lan-
guage with similar morphosyntactic phenomena.1
The RR approach is shown to be an adequate way
to model complex morphosyntactic patterns for im-
proving constituency-based parsing of a morpholog-
ically rich, free word order language. Because the
RR model is also proper and generative, it may also
embed as a language model to enhance more com-
plex NLP tasks, e.g., statistical Machine Translation.
</bodyText>
<sectionHeader confidence="0.990428" genericHeader="method">
2 The Data
</sectionHeader>
<bodyText confidence="0.999644">
The grammar of nonconfigurational languages al-
lows for freedom in word ordering and discontinu-
ities of syntactic constituents (Hale, 1983). Such
languages do not rely on configurational information
such as position and adjacency in marking grammat-
ical relations such as subject and object, but instead
they use word-level morphology. One way to encode
grammatical relations in the form of words is by us-
ing morphological case, that is, explicitly marking
an argument (e.g. nominative, accusative) with re-
spect to its grammatical function. In (Tsarfaty et
al., 2009) we showed that incorporating case indeed
leads to improved performance for constituency-
based, Relational-Realizational parsing of Hebrew.
</bodyText>
<footnote confidence="0.500572">
1In (Maamouri et al., 2008), F178.1 for gold standard input.
</footnote>
<bodyText confidence="0.999412086956522">
A more involved way to morphologically encode
grammatical relations is by making explicit refer-
ence to the properties of multiple linguistic ele-
ments. This is the general pattern of agreement, i.e.,
“[A] systematic covariance between a se-
mantic or a formal property of one ele-
ment and a formal property of another.”
(Steele, adapted from (Corbett, 2001))
Describing agreement patterns involves explicit
reference to the following four components; the el-
ement which determines the agreement properties
is the Controller of the agreement, the element
whose properties are determined by agreement is
the Target, the syntactic environment in which the
agreement occurs is the Domain of agreement, and
the properties with respect to which they agree are
agreement Features (Corbett, 2001). Agreement is
an inherently asymmetrical relation. Combination
of features displayed by controllers has to be ac-
commodated by the inflectional features of the tar-
get, but there is no opposite requirement. Let us il-
lustrate the formal description of agreement through
Subject-Verb agreement familiar from English (1).
</bodyText>
<listItem confidence="0.90387075">
(1) a. Subject-Verb Agreement in English:
Controller: NP
Target: V
Domain: S
Features: number, person
b. Example:
i. They like the paper
ii. *They likes the paper
</listItem>
<bodyText confidence="0.999904642857143">
The agreement target (the verb) in English has a
rich enough inflectional paradigm that reflects the
person and number features inherent in controllers
— the nouns that realize subjects. (But nouns in En-
glish need not reflect, say, tense.) Had the subject
been an NP, e.g., the phrase “the committee”, the
agreement pattern would have had to be determined
by the features of the entire NP, and in English the
features of the phrase would be determined by the
lexical head “committee”. The controller of the
agreement (noun) does not coincide with the head of
the lexical dependency (the verb), which means that
the direction of morphological dependencies need
not coincide with that of lexical dependencies.
</bodyText>
<page confidence="0.998897">
41
</page>
<bodyText confidence="0.994954">
The Semitic Language Modern Hebrew Modern
Hebrew, (henceforth, Hebrew) is a Semitic language
with a flexible word order and rich morphological
structure. Hebrew nouns morphologically reflect
their inherent gender and number. Pronouns also
reflect person features. Hebrew verbs are inflected
to reflect gender, number, person and tense. Adjec-
tives are inflected to reflect the inherent properties of
nouns, and both nouns and adjectives are inflected
for definiteness. The Hebrew grammar uses this ar-
senal of properties to implement a wide variety of
agreement patterns realizing grammatical relations.
Agreement in Hebrew S Domains Hebrew man-
ifests different patterns of agreement in its S do-
main. Verbal predicates (the target) in matrix sen-
tences (the domain) agree with their nominal sub-
jects (the controller) on the agreement features gen-
der, number and person. This occurs regardless of
their configurational positions, as illustrated in (2b).
</bodyText>
<listItem confidence="0.6511165">
(2) a. Agreement in Verbal Sentences:
Controller: NP
Target: V
Domain: S
Features: number, person, gender
b. i. הנידל הנתמ ןתנ ינד
</listItem>
<bodyText confidence="0.9986769">
which (Doron, 1986) calls Pron, are optionally
placed after the subject. The position of Pron el-
ement with respect to the subject and predicate is
fixed.2 The role of these Pron elements is to indicate
the argument-structure of a nominal sentence that is
not projected by a verb. In the Hebrew treebank they
are subsumed under predicative phrases (PREDPs).
If a PREDP head is of type NP or ADJP it must be
inflected to reflect the features of the subject con-
troller, as is illustrated in examples (3b-i)–(3b-ii).
</bodyText>
<listItem confidence="0.82987125">
(3) a. Agreement in Nominal Sentences:
Controller: NP
Target: Pron
Domain: S
</listItem>
<equation confidence="0.847453095238095">
Features: number, gender,person
b. i. תרייצ (איה) הניד
dina (hi) cayeret
Dina.FS (Pron.3FS) painter.FS
Dina is a painter
ii. תרשכומ (איה) הניד
Dina (hi) muchsheret
Dina.FS (Pron.3FS) talented.FS
Dina is talented
iii. תיבב (איה) הניד
dani natan matana Dina (hi) babayit
Dani.3MS gave.3MS present Dina.FS (Pron.3FS) in-the-house
ledina
to-Dina
Dani gave a present to Dina (SVO)
ii. הנידל ינד ןתנ הנתמ
matana natan dani
present gave.3MS Dani.3MS
ledina
to-Dina
Dani gave a present to Dina (VI)
</equation>
<listItem confidence="0.600331833333334">
Subject-Predicate agreement relations are not
only independent of surface positions, but are also
orthogonal to the syntactic distributional type of
the constituent realizing the predicate. Semitic lan-
guages allow for predicates to be realized as an
NP, an ADJP or a PP clause (3b) lacking a verb
altogether. (In the Hebrew treebank, such pred-
icates are marked as PREDP). In all such cases,
agreement feature-bundles realized as pronominals,
Dina is at home
c. i. תרייצ הניד *(איה) (hi)* dina cayeret
(Pron.3FS)* Dina.FS painter.FS
</listItem>
<bodyText confidence="0.999917818181818">
The pronominal features gender, number, person
are also a part the inflectional paradigm of the verb
היה (be), which is extended to include tense features.
These inflected elements are used as AUX which
function as co-heads together with the main (nom-
inal or verbal) predicate. AUX elements that take a
nominal predicate as in (4b) agree with their subject,
and so do auxiliaries that take a verbal complement,
e.g., the modal verb in (4c). The nominal predicate
in (4b) also agrees with the subject – and so does
the modal verb in (4c). Agreement of AUX with the
</bodyText>
<footnote confidence="0.9955355">
2Doron (1986) shows that these Pron elements can not be
considered the present tense supplements of AUX elements in
Hebrew since their position with respect to the subject and pred-
icate is fixed, whereas AUX can change position, see (4) below.
</footnote>
<page confidence="0.999435">
42
</page>
<bodyText confidence="0.919395">
verbal or nominal predicates is again independent of
their surface positions.
</bodyText>
<figure confidence="0.6044927">
(4) a. Subject-AUX Agreement in Hebrew:
Controller: NP
Target: AUX
Domain: S
Features: number, person, gender
b. i. תרייצ רבעב התיה איה
hi hayta be’avar
she.3FS was.3FS in-past
cayeret
painter.FS
</figure>
<bodyText confidence="0.868562571428572">
She was a painter in the past
ii. תרייצ איה התיה רבעב
be’avar hayta hi
in-past was.3FS she.3FS
cayeret
painter.FS
She was a painter in the past”
</bodyText>
<equation confidence="0.68500375">
c. i. עיגהל הרומא התיה איה
hi hayta amura
She.3FS was.3FS supposed.FS
lehagi’a
</equation>
<bodyText confidence="0.937160285714286">
to-arrive
She was supposed to arrive
ii. עיגהל התיה הרומא איה
hi amura hayta
She.3FS supposed.FS was.3FS
lehagi’a
to-arrive
She was supposed to arrive
Agreement in Construct State Nouns Semitic
languages allow for the creation of noun compounds
by phonologically marking their lexical head and
adding a genitive complement. These constructions
are called Construct-State Nouns (CSN) (Danon,
2008) and an example of a CSN is provided in (5a).3
</bodyText>
<listItem confidence="0.414856666666667">
(5) a. רייצה תב
bat ha-cayar
child.FS.CSN Def-painter.MS
</listItem>
<bodyText confidence="0.851461">
The painter’s daughter
</bodyText>
<footnote confidence="0.457907">
3Also known as iDaFa constructions in Arabic.
</footnote>
<bodyText confidence="0.998386666666667">
In such cases, all the agreement features are taken
from the head of the CSN, the noun ‘daughter’ in (5).
Since CSNs may be embedded in other CSNs, the
constructions may be arbitrarily long. When short
or long, CSNs themselves may be modified by ad-
jectives that agree with the CSN as a whole. This
gives rise to multiple patterns of agreement within
a single complex CSN. Consider, for instance, the
modified CSN in (6a).
</bodyText>
<equation confidence="0.3453444">
(6) a. תרשכומה רייצה תב
bat ha-cayar
child.FS.CSN Def-painter.MS
ha-muchsheret
Def-talented.FS
</equation>
<bodyText confidence="0.9970416">
The talented daughter of the painter
The features Def, F, S of the adjective ‘talented’
agree with the inherent properties of the CSN head
‘child.FS’ and with the definiteness status of the em-
bedded genitive Def-painter. This phenomenon is
called by Danon (2008) definiteness-spreading, and
what is important about such spreading is to observe
that it is not always the case that all agreement fea-
tures of a phrase are contributed by its lexical head.4
Interim Summary The challenges of model-
ing agreement inside constituency-based statistical
models can be summarized as follows. The models
are required to assign probability mass to alternating
sequences of constituents while retaining equivalent
feature distributions that capture agreement. Agree-
ment is (i) orthogonal to the position of constituents
(ii), orthogonal to their distributional types, and (iii)
orthogonal to features’ distributions among domi-
nated subconstituents. Yet, from a functional point
of view their contribution is entirely systematic.
</bodyText>
<sectionHeader confidence="0.995887" genericHeader="method">
3 The Models
</sectionHeader>
<bodyText confidence="0.999768125">
The strong version of the well-known Lexicalist
Hypothesis (LH) states that “syntactic rules cannot
make reference to any aspect of word internal struc-
ture” (Chomsky, 1970). Anderson (1982) argues
that syntactic processes operating within configura-
tional structures can often manipulate, or have ac-
cess to, formal and inherent properties of individ-
ual words. Anderson (1982) argues that a model
</bodyText>
<footnote confidence="0.659695">
4Examples for non-overlapping contribution of features by
multiple dependencies can be found in (Guthmann et al., 2009).
</footnote>
<page confidence="0.999528">
43
</page>
<bodyText confidence="0.95236296875">
that is well-equipped to capture such phenomena is
one that retains a relaxed version of the LH, that is,
one in which syntactic processes do not make refer-
ence to aspects of word-internal structure other than
morphologically marked inflectional features. What
kind of parsing model would allow us to implement
this relaxed version of the Lexicalist Hypothesis?
The Morphosyntatctic State-Splits (SP) Model
One way to maintain a relaxed version of the LH
in syntax is to assume a constituency-based rep-
resentation in which the morphological features of
words are percolated to the level of constituency
in which they are syntactically relevant. This ap-
proach is characteristic of feature-based grammars
(e.g., GPSG (Gazdar et al., 1985) and follow-up
studies). These grammars assume a feature geom-
etry that defines the internal structure of node labels
in phrase-structure trees.5
Category-label state-splits can reflect the different
morphosyntactic behavior of different non-terminals
of the same type. Using such supervised, linguis-
tically motivated, state-splits, based on the phrase-
level marking of morphological information is one
may build an efficient implementation of a PCFG-
based parsing model that takes into account mor-
phological features. State-split models were shown
to obtain state-of-the-art performance with little
computational effort. Supervised state-splits for
constituency-based unlexicalized parsing in (Klein
and Manning, 2003) in an accurate English parser.
For the pair of Hebrew sentences (2b), the morpho-
logical state-split context-free representation of the
domain S is as described at the top of figure 1.6
The Relational-Realizational (RR) Model A dif-
ferent way to implement a syntactic model that con-
form to the relaxed LH is by separating the inflec-
tional features of surface words from their grammat-
ical functions in the syntactic representation and let-
5While agreement patterns in feature-rich grammars give
rise to re-entrancies that break context-freeness, GPSG shows
that using feature-percolation we can get quite far in modeling
morphosyntactic dependencies and retaining context-freeness.
6Horizontal markovization a` la (Klein and Manning, 2003)
would be self-defeating here. Markovization of constituents
conditions inflectional features on configurational positions,
which is inadequate for free word-order languages as Hebrew.
This is already conjectured in the PhD thesis of Collins, and it
is verified empirically for Hebrew in (Tsarfaty et al., 2009).
ting the model learn systematic form-function corre-
spondence patterns between them.
The Relational-Realizational (RR) model (Tsar-
faty and Sima’an, 2008) takes such a ‘separational-
ist’ approach which is constituent-based. Grammat-
ical relations are separated from their morphologi-
cal or syntactic means of realization, which are in
turn also distinguished. The easiest way to describe
the RR model is via a three-phase generative process
encompassing the projection, configuration and re-
alization phases. In the projection phase, a clause-
level syntactic category generates a Relational Net-
work (RN), i.e., a set of grammatical function-labels
representing the argument-structure of the clause. In
the configuration phase, linear ordering is generated
for the function-labels and optional realization slots
are reserved for elements such as punctuation, auxil-
iaries and adjuncts. The realization phase spells out
a rich morphosyntactic representation (MSR) — a
syntactic label plus morphological features — real-
izing each grammatical function and each of the re-
served slots. The process repeats as necessary until
MSRs of pre-terminals are mapped to lexical items.
In (Tsarfaty et al., 2009) we have shown that
the RR model makes beneficial use of morpholog-
ical patterns involving case marking, but did not
study the incorporation of inflectional agreement
features such as gender. Since agreement features
such as gender, number and case-related informa-
tion such accusativity, definiteness are determined
by non-overlapping subconstituents, it remains an
open question whether an addition of agreement fea-
tures into the model can be down in a linguistically
adequate and statistically sound way, and whether or
not they further improve performance.
We claim that the Relational-Realizational model
of (Tsarfaty et al., 2009) has all the necessary ingre-
dients to seamlessly migrate RR representations to
ones that encode agreement explicitely. In order to
explain how we do so let us recapitulate the empir-
ical facts. Agreement is an asymmetric relation de-
fined for a certain domain, in which the agreement
properties of a target co-vary with the inherent prop-
erties of the controller. Consider the two sentences
in (2b) in which the formal means to differentiate the
subject from the object is by the pattern of an agree-
ing predicate. The RR representations of the domain
S are given at the bottom of figure 1.
</bodyText>
<page confidence="0.996026">
44
</page>
<bodyText confidence="0.999476068181818">
The agreement targets and agreement controllers
are easy to recognize; controllers are the syntac-
tic constituents that realize subjects, parametrized
as Prealization(V B|PRD@S), and targets are the
ones that realize predicates, parametrized as
Prealization(NP|SBJ@S). Now, if we take the
predicted labels of controllers and targets to in-
clude reference to inflectional features, we get
the following parameterization of the realization
parameters Prealization(V B(FEATSZ)|PRD@S) and
Prealization(NP(FEATSj)|SBJ@S) with (FEATSZ),
(FEATSj) the inflectional features indicated in their
morphosyntactic representation. Now, we only need
to make sure that (FEATSZ), (FEATSj) indeed agree,
regardless of their position under S.
We do so by explicitly marking the domain
of agreement, the S category, with the features
of the syntactically most prominent participant in
the situation, the subject (this is where the non-
symmetrical nature of agreement comes into play).
The realization distributions take the following
forms Prealization(V B(FEATSj)|PRD@S(FEATSZ))
and Prealization(NP(FEATSZ)|SBJ@S(FEATSZ)). In
the former, NP(FEATSZ) reflects the inherent fea-
tures of the SBJ and in the latter V B(FEATSj) re-
flects the agreement features of the PRD. Now, re-
gardless of word order, and regardless of the inter-
nal structure of NPs, the parameters capturing agree-
ment would be the same for examples (2b i-ii). The
only parameters that differ are the configuration pa-
rameters (boxed), reflecting word-order alternation.
For the sake of completeness we include here also
the SP vs. RR representation of S domains involv-
ing auxiliaries in figure 2. Here the sentences vary
only in the position of the AUX element relative to
the subject with which it agrees. Subjects, predi-
cates, and slots that have been reserved for AUX
elements, all reflect the same pattern of agreement
through their conditioning on the rich representa-
tion of the domain.7 More parameters that vary here
(boxed) are AUX placement and realization param-
eters. Since Pron elements endow PREDPs with
agreement features, agreement with verbless (nomi-
nal) predicates under S analogously follows.
</bodyText>
<footnote confidence="0.989419">
7In Hebrew, even some adverbial modifiers reflect pat-
terns of agreement, e.g., ינדוY (literally, ”I am still”, glossed
‘still.1S’). This solution caters for all such patterns in which
non-obligatory elements exhibit agreement.
</footnote>
<sectionHeader confidence="0.99856" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999983888888889">
We aim to examine whether the explicit incorpora-
tion of agreement features helps Hebrew parsing,
and if so, which of the two modeling strategies is
better for utilizing the disambiguation cues provided
by morphosyntactic agreement.
Data We use the Hebrew treebank v2.0 with the
extended annotation of (Guthmann et al., 2009),
which adds inflectional properties to non-terminal
categories such as NP and VP. We head-annotate
the corpus and systematically add the agreement fea-
tures of Domains throughout the treebank. We fur-
ther distinguish finite from non-finite verb forms,
and cliticized from non-cliticized nouns, as in
(Goldberg and Tsarfaty, 2008; Tsarfaty et al., 2009).
On top of the treebank labels SBJ subject, OBJ ob-
ject, COM complement and CNJ conjunction we
add PRD predicates and IC infinitival complements.
Procedure We devised a procedure to read-off
treebank grammars based on (i) GPSG-like, states-
plit context-free parameters (SP-AGR), and (ii) RR-
AGR parameters in which context-free rules capture
the projection, configuration and realization phases.
In each model the multiplication provides the prob-
ability of the generation. We use relative frequency
estimates and exhaustively parse gold pos-tagged in-
put8 using a general-purpose CKY parser. We use
the same data split as in (Goldberg and Tsarfaty,
2008; Tsarfaty et al., 2009) (training on sentences
501-6000 and parsing sentences 1-500) and we con-
vert all trees to the flat, coarse-grained, original tree-
bank representation for the purpose of evaluation.
Setup We experiment with bare constituent labels,
grand-parent decorated labels (gp), and labels deco-
rated with grand-parent and head-tag labels (gp,hd).
We use increasingly richer subsets of the {gender,
definiteness, accusativity} set.9
</bodyText>
<footnote confidence="0.6014005">
8This choice to parse gold-tagged sentences is meant to alle-
viate the differences in the model’s morphological disambigua-
tion capacity. We want to evaluate the contribution of morpho-
logical features for syntactic disambiguation, and if the models
will disambiguate the morphological analyses differently, the
syntactic analysis will be assigned to different yields and the
accuracy results would be strictly incomparable. But see (Gold-
berg and Tsarfaty, 2008) for a way to combine the two.
9We deliberately choose features that have non-overlapping
behavior, to see whether their contribution is accumulative.
</footnote>
<page confidence="0.995804">
45
</page>
<figure confidence="0.996673704301075">
SMSF SMS
NPMS-SBJ
hi N
dani
she
Dani
MS-SBJ
VPMS-RD
hayta NPMS
VP
BJ
i- -SBJ
SBJ
natan
was
i gave
J amura
-PRD
NP-OBJ
VPM
upposed
an matana
- -PRD n
an
e present g
OBJehagia
D PP-COM NP
J BOBJ
na arrive
ledian m
etato-dina
n ana pr
PRD
NP-OBJ
hi VPMSD
NS-SBJ
mura
VPMS-PRD
NPMS-SJ
PPCOM
hta NPMS-SBJ
PP-CO
leha
MS
PMS NPMS
PM PC
PP
she
nmataVPMna natan
ed NPM d dani
PP le
t a d ld le
epreset
g na gaveD aDani
to le
PP-COM
ledian
to-dina
P(NPMS-SBJ VPMS-PRDMNP-OBJ PP-COM I SMS )BJVMS-PRDNPMS-SBJ,P-COM PP-COM  |SMS )
-RDNPMSSBJ,P-COM  |SMS )  |SMS )
P(NP-OBJ,VPMS-PRD,NPMS-SBJ,  |)
PSBJNCOM
VPDNPJCO
VRNPBPPCM
SMS {SBJ,PRD SMS
CO ,OBJ,COM}@SMS
{SBJ,PRD,OBJ,COM} @BSMs }S
SMS
S ) M@
M@ @S
hayt
NPMS
M PP-
PCadani
PPPPle
ld le
ato
Dani l
SBJ@SMS
NP P(NP S
SB S
NPMS
she
dani
Dani
SBJV@SMS
PRD@SMS
AUXSP(NPMS
SBJ@ PR
PRD
PR
y
VPMS
S was N
S MSnatan
gave
B {SBJ
BJVPMS-PR
@SMS
OBJ@SMS
DNOBJ
S PRD@ O
PP OB O
MS NP-OBJ
supposed VP
MS matana
S n n
MS
e present
n an BJPRD g
M� �
COM}@
MS @SMS
@SMS
NOBJPP-C
COM  |SM
PREDPFS
OBJ@ CO
COM
CO
g
OBJ
PP-COM NP
J B toarrive
na BJ
ledian m
a na
aaM}@S
nt
to-dina pr
NPMSBJPCOM  |SMS )
@SMS
S OBJ@SMS
PSPRDNSBJPP-CM  |SM
PRD@SS
SB@SMS
PRD@SMS
SBJ@SS
CO@SMS
SBJ@ CO
@ @
S VPMS
ura NP
natanNPM
dPNd
}@S
gave D
MS COM@SMS
PP-COM
lehagia
oari
ledian
to-dina
D@ @S
MS NP-OBJ
i VP
VM
nmatanaVP
t a
epreset
BJPRDO g
COM}@S
na
Pptojection({SBJ,PRD,OBJ,COM} I SMs)BPprojection({SBJ,PRD,OBJ,COM} I SMS )
Pconfigumtion(S,P,O,C) I,{SBJ,PRD,OBJ,COM}@SMs)OPconfiguration((O,P,S,C) I {SBJ,PRD,OBJ,
fi OB OP
SMS)
COM}@SMS)
S
g g NP
Prrealiza
Prrealiza
Prealiza
Prealiza � SBJ@S ) P (NP
tlori MS MS YQQlizalion
tionNBMS P�OQ SMs) Prealizarion(
tions OBJ@ SMS) Prealizarion�
tion�P COM@ SMS) Prealizarion�P
MSSBJ@SMS )
VBMSPRDOaSMS )
OBJ@ SMS )
COW SMS )
re 1 e -(to) an reresenations of sentences (2) (left) an (2).
NP |
igure 1 The SP-AGR (top) and R-AGR represntaions o sentences (2b-i) (left) and (2b-ii).
SMS )
</figure>
<figureCaption confidence="0.999916">
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
</figureCaption>
<equation confidence="0.8862744">
realization(N  |)
MS ) realizaion ealzaton
aliz(PP COM@ SS )
PP  |COM@ S ) Pln(P  |COM@ SM )
Pli(PP  |COM@ SMS )
</equation>
<figure confidence="0.977958336842105">
: SFS
FS SFS FS 2b (e SF S
S
hayta DFS-PRD
DFS-PRD
MD-PRD
amura
SFS lehag
as mura
uppose
amura
amura
to
upposed
supposed
suppose
u d
amuraPINF-COM
PINF-
INF-
VPNCOM
lehaia
hagia
sed
lehagia
oarrive
a
lehagia
to-arrive
-arrive
to-arrve
i
MDFS-PRD
DFS-PR
a FS-
MDPR
hi
amur hay
amura
supposhe
amur
amu
supposed
suppose
suppos
su
VPINF-COM
a PINF-
VPFCOINF-
hay
lehagia
lehagia
wastoarri
lehag
lehag
to-arrive
to-arriv
to-arr i
NPFS-SBJ
PFS-SB
F
NPFS-S
hi S am
hi s h
shesh s
AUXFS
amura XF
AUX
hayta
leh
pposehaya
hay ay
h wa
to
wasww
hiUXFS
AXFS FS
AUX
hayt SFS am
ayta
e hayta
a was
hayta
sup
was
as was
PFS-SBJ
PFS-SBJ
F-
NPFSSBJ
hi hay
se
hih
heshese
h
</figure>
<table confidence="0.7352861">
( , (FS
FS ,
, , FS FS )
FS,   |) ( F,
(FS-, FS, , FS-  |SFS )
(NPFS-SBJ, AUXFS, PP REDPFS-PRD  |SFS )
FS , ,
P(P, AUXFS, NPFS-SBJ, PREDPFS-PRD  |SFS
(, FS, FS-, FS  |F
FS, FS
</table>
<figure confidence="0.95082558974359">
FS FS )
gi F
- -
hi hta amura lehaga hi hy
aa ,  |
hi hay amura lehagia amura ayta lehagia
P(NPFSSBJ AUXFS PP PREDPFSPRD SFS ) S S- S- F
P(PP AUXF NPFSBJ PREDPFPRD  |S
h d pped
h upposd
was toi ie
AU PRFSSFS
SFS
{}
} {,
SBJ,PRD,COM}@SFS
{SBJ,PRD,}SFS FS
{SBJPRDCOM}@S
S S
RD
RSFSSFS
{}
} {
{SBJ,PRD,COM}@SFS
{SBJ,PRD,}SF
{SBJPRDCOM}@S
S
BJ@SFS
SBJSFS
SBJ@SFS
@SFS
PFS PFS FS
NPF
hi
hesheseihi
S h
se
h
BJ:PRD@SFS
SBJ:PRDSFS
:FS
SBJPRD@S
S F
FS UXFS
@SFS
AXFS
AUXFS
hayta a
aytahayta
S ayta
ha was su
aswas
was
PRD SS+FS
muPP PR
PP
mura
uppose
amura
amura
to P
upposed
supposed
supose am le
upd
OM@SINF
SINF
INF
COM@S
REDPFS
F
ura
PREDPFS
COM
sedPREDPFS
lehagia
hagia
oarrive
lehagia
lhagieaa
PRED
to-arive
-arrive
to-arrve
lehag
i
SBJ@SFS
SBJSF
SBJ@S
J@SFS
RD@
NPFS PF
NP hi
hiP s h
FS
hi amu
she sh s
PRD@S+� �
PRDS+F
PRD@S
PP P
P UXFS
amura
amur
e suppo
amu r
murasupposed
hayta
suppose
suppos
su
PRD:COM@SFS
PRD:SF
PRD:COM@S
@SFS
AUXFS
SINF
ura AXF
AUX
hayta
PFSsed
hayta
hay wa ay
h
awaswaw
COM@SINF
SIN IN
COM@S
S S
aytaPREDPFS
F PREDPF
PREDP
lehagia
lehagia
lehag
as toarri
e
lehag
to-arrive
to-ariv
to-arr i
PprojectionQSBJ,PRD,COM} I SFs)OPprojection({SB(,PRD,COM} I SFs )
Pconfiguration(i(SBJ,SBJ:PRD, PRD, COM) |R{SBJ,PRD,COM}@SFs)OPconfiguration(o(SBJ, PRD, CRD:COM, COM) I {SBJ,PRD,COM}@SFs)
Prealization(NPFS I SBJ@SFs) Prealization(NPFS I SBJ@BFs )
AX RD COM@S
AUX
PRD CM P
atP
Fs SBJ:PRD@SFS) P (AUXFs I PRD•COM@SFS )
rPalizahon reakzahon
a (A
z
Prealization(MDFS PRD@SFs) Prealization(MDFS PRD@SFs )
Prealization(VP COM@SFs) Prealization(VP COM@SFs)
(VP  |C@
OM
</figure>
<figureCaption confidence="0.9997422">
re: he S- (top) and - representation of sentences (4c-i) (left) and (4c-i). .
igure 2: The SP-AGR (top) and R-AGR represntaion of sentences (4c-i) (left) and (4c-ii)
g g p ) p p ) ) )
- - - -
Figure 2: The SPAGR (top) and RRAGR representation of sentences (4ci) (left) and (4cii)
</figureCaption>
<page confidence="0.988063">
46
</page>
<table confidence="0.999866">
Model ∅ gender def+acc gender+def+acc
SP-AGR 79.77 79.55 80.13 80.26
(3942) (7594) (4980) (8933)
RR-AGR 80.23 81.09 81.48 82.64
(3292) (5686) (3772) (6516)
SP-AGR (gp) 83.06 82.18 79.53 80.89
(5914) (10765) (12700) (11028)
RR-AGR (gp) 83.49 83.70 83.66 84.13
(6688) (10063) (12383) (12497)
SP-AGR (gp,hd) 76.61 64.07 75.12 61.69
(10081) (16721) (11681) (18428)
RR-AGR (gp,hd) 83.40 81.19 83.33 80.45
(12497) (22979) (13828) (24934)
</table>
<tableCaption confidence="0.9881495">
Table 1: F-score (#params) measure for all models on
the Hebrew treebank dev-set for Sentences Length &lt; 40
</tableCaption>
<sectionHeader confidence="0.999849" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999968915254238">
Table 1 shows the standard F1 scores (and #param-
eters) for all models. Throughout, the RR-AGR
model outperforms the SP-AGR models that use the
same category set and the same morphological fea-
tures as state splits. For RR-AGR and RR-AGR (gp)
models, adding agreement features to case features
improves performance. The accumulative contribu-
tion is significant. For SP-AGR and SP-AGR (gp)
models, adding more features either remains at the
same level of performance or becomes detrimental.
Since the SP/RR-AGR and SP/RR-AGR (gp)
models are of comparable size for each feature-set,
it is unlikely that the differences in performance are
due to the lack of training data. A more reason-
able explanation if that the RR parameters repre-
sent functional generalizations orthogonal to config-
uration for which statistical evidence is more easily
found in the data. The robust realization distribu-
tions which cut across ordering alternatives can steer
the disambiguation in the right direction.
The RR-AGR (gp) +gen+def+acc model yields
the best result for parsing Hebrew to date (F1 84.13),
improving upon our best model in (Tsarfaty et al.,
2009) (F1 83.33, underlined) in a pos-tagged set-
ting. For this setting, Arabic parsing results are F1
78.1. Given the similar morphosyntactic phenomena
(agreement, MaSDaR, iDaFa) it would be interest-
ing to see if the model enhances parsing for Arabic.
For (gp,hd) models (a configuration which was
shown to give the best results in (Tsarfaty et al.,
2009)) there is a significant decrease in accuracy
with the gender feature, but there is a lesson to be
learned. Firstly, while the RR-AGR (gp,hd) model
shows moderate decrease with gender, the decrease
in performance of SP-AGR (gp,hd) for the same
feature-set is rather dramatic, which is consistent
with the observation that the RR model is less vul-
nerable to sparseness and that it makes better use of
the statistics of functional relations in the data.
Consulting the size of the different grammars, the
combination of RR-AGR (gp, hd) with gender fea-
tures indeed results in substantially larger grammars,
and it is possible that at this point we indeed need to
incorporate smoothing. At the same time there may
be an alternative explanation for the decreased per-
formance. It might be that the head-tag does not add
informative cues beyond the contribution of the fea-
tures which are spread inside the constituent, and are
already specified. This is a reasonable hypothesis
since gender in Hebrew always percolates through
the head as opposed to def/acc that percolate from
other forms. Incorporating head-tag in (Tsarfaty et
al., 2009) might have led to improvement only due
to the lack of agreement features which subsume
the relevant pattern. This suggests that incorporat-
ing all co-heads and functional elements that con-
tribute morphological features spread inside the con-
stituent, is more adequate for modeling morphosyn-
tax than focusing on the features of a single head.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999607882352941">
We show that morphologically marked agreement
features can significantly improve parsing perfor-
mance if they are represented and parametrized in
a way that reflects their linguistic substance: relat-
ing form-and-function in a non-linear fashion. We
have so far dealt with the adequacy of representa-
tion and we plan to test whether more sophisticated
estimation (e.g., split-merge-smooth estimation as in
(Petrov et al., 2006)) can obtain further improve-
ments from the explicit representation of agreement.
At the same time, the state-of-the-art results we
present render the RR model promising for further
exploration with morphologically rich languages.
Acknowledgements The work of the first author
has been funded by NWO, grant 017.001.271. We
wish to thank Joakim Nivre and three anonymous
reviewers for helpful comments on earlier drafts.
</bodyText>
<page confidence="0.999093">
47
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999444310344828">
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597–618.
Stephen R. Anderson. 1982. Where’s morphology? Lin-
guistic Inquiry.
Noam Chomsky. 1970. Remarks on nominalization. In
R. Jacobs and P. Rosenbaum, editors, Reading in En-
glish Transformational Grammar. Waltham: Ginn.
Greville G. Corbett. 2001. Agreement: Terms and
boundaries. In SMG conference papers.
Gabi Danon. 2008. Definiteness spreading in the hebrew
construct-state. Lingua, 118(7):872–906.
Edit Doron. 1986. The pronominal “copula” as agree-
ment clitic. Syntax and Semantics, (19):313–332.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan A. Sag. 1985. Generalised phrase structure
grammar. Blackwell, Oxford, England.
Yoav Goldberg and Michael Elhadad. 2009. Hebrew de-
pendency parsing: Initial results. In Proceedings of
IWPT.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2006.
Noun phrase chunking in hebrew: Influence of lex-
ical and morphological features. In Proceedings of
COLING-ACL.
Nomie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew treebank.
In Frank Van Eynde, Anette Frank, Koenraad De
Smedt, and Gertjan van Noord, editors, Proceedings
of TLT.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Julia Hockenmaier and Mark Steedman. 2003. Parsing
with generative models of predicate-argument struc-
ture. In Proceedings of ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the arabic tree-
bank. In Proceedings of INFOS.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature-forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35–80.
Lilja Øvrelid and Joakim Nivre. 2007. Swedish depen-
dency parsing with rich linguistic features. In Pro-
ceeding of RANLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Reut Tsarfaty and Khalil Sima’an. 2008. Relational-
realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Khalil Sima’an, and Remko Scha. 2009.
An alternative to head-driven approaches for parsing a
(relatively) free word order language. In Proceedings
of EMNLP.
</reference>
<page confidence="0.999349">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.859730">
<title confidence="0.985046">Modeling Morphosyntactic in Constituency-Based Parsing of Modern Hebrew</title>
<author confidence="0.971645">Khalil</author>
<affiliation confidence="0.994012">Institute for Logic, Language and University of</affiliation>
<abstract confidence="0.9941266875">We show that naive modeling of morphosyntactic agreement in a Constituency-Based (CB) statistical parsing model is worse than none, whereas a linguistically adequate way of modeling inflectional morphology in CB parsing leads to improved performance. In particular, we show that an extension of the Relational-Realizational (RR) model that incorporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional features. We focus on parsing Hebrew and report the best result to for parsing off of gold-tagged text, 5% error reduction from previous results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="2519" citStr="Abney, 1997" startWordPosition="365" endWordPosition="366">al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for Hebrew they obtained tiny-to-no improvement. A question thus emerges whether there are any benefits in explicitly incorporating morphosyntactic agreement patterns into our models. This question is a manifestation of a greater issue, name</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen R Anderson</author>
</authors>
<title>Where’s morphology? Linguistic Inquiry.</title>
<date>1982</date>
<contexts>
<context position="14333" citStr="Anderson (1982)" startWordPosition="2200" endWordPosition="2201">ed to assign probability mass to alternating sequences of constituents while retaining equivalent feature distributions that capture agreement. Agreement is (i) orthogonal to the position of constituents (ii), orthogonal to their distributional types, and (iii) orthogonal to features’ distributions among dominated subconstituents. Yet, from a functional point of view their contribution is entirely systematic. 3 The Models The strong version of the well-known Lexicalist Hypothesis (LH) states that “syntactic rules cannot make reference to any aspect of word internal structure” (Chomsky, 1970). Anderson (1982) argues that syntactic processes operating within configurational structures can often manipulate, or have access to, formal and inherent properties of individual words. Anderson (1982) argues that a model 4Examples for non-overlapping contribution of features by multiple dependencies can be found in (Guthmann et al., 2009). 43 that is well-equipped to capture such phenomena is one that retains a relaxed version of the LH, that is, one in which syntactic processes do not make reference to aspects of word-internal structure other than morphologically marked inflectional features. What kind of p</context>
</contexts>
<marker>Anderson, 1982</marker>
<rawString>Stephen R. Anderson. 1982. Where’s morphology? Linguistic Inquiry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Remarks on nominalization.</title>
<date>1970</date>
<booktitle>Reading in English Transformational Grammar.</booktitle>
<editor>In R. Jacobs and P. Rosenbaum, editors,</editor>
<location>Waltham: Ginn.</location>
<contexts>
<context position="14316" citStr="Chomsky, 1970" startWordPosition="2198" endWordPosition="2199">odels are required to assign probability mass to alternating sequences of constituents while retaining equivalent feature distributions that capture agreement. Agreement is (i) orthogonal to the position of constituents (ii), orthogonal to their distributional types, and (iii) orthogonal to features’ distributions among dominated subconstituents. Yet, from a functional point of view their contribution is entirely systematic. 3 The Models The strong version of the well-known Lexicalist Hypothesis (LH) states that “syntactic rules cannot make reference to any aspect of word internal structure” (Chomsky, 1970). Anderson (1982) argues that syntactic processes operating within configurational structures can often manipulate, or have access to, formal and inherent properties of individual words. Anderson (1982) argues that a model 4Examples for non-overlapping contribution of features by multiple dependencies can be found in (Guthmann et al., 2009). 43 that is well-equipped to capture such phenomena is one that retains a relaxed version of the LH, that is, one in which syntactic processes do not make reference to aspects of word-internal structure other than morphologically marked inflectional feature</context>
</contexts>
<marker>Chomsky, 1970</marker>
<rawString>Noam Chomsky. 1970. Remarks on nominalization. In R. Jacobs and P. Rosenbaum, editors, Reading in English Transformational Grammar. Waltham: Ginn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greville G Corbett</author>
</authors>
<title>Agreement: Terms and boundaries.</title>
<date>2001</date>
<booktitle>In SMG conference</booktitle>
<pages>papers.</pages>
<contexts>
<context position="1085" citStr="Corbett, 2001" startWordPosition="154" endWordPosition="155">ce. In particular, we show that an extension of the Relational-Realizational (RR) model that incorporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional features. We focus on parsing Hebrew and report the best result to date, F184.13 for parsing off of gold-tagged text, 5% error reduction from previous results. 1 Introduction Agreement is defined by linguists as the systematic covariance of the grammatical properties of one linguistic element to reflect the semantic or formal properties of another (Corbett, 2001). Morphologically marked agreement features such as gender, number and person are used to realize grammatical relations between syntactic constituents, and such patterns are abundantly found in (less- or) nonconfigurational languages (Hale, 1983) where the order of words is known to be (relatively) free. Agreement features encompass information concerning the functional relations between constituents in the syntactic structure, but whether incorporating agreement features in a statistical parsing model leads to improved performance has so far remained an open question and saw contradictory res</context>
<context position="6406" citStr="Corbett, 2001" startWordPosition="948" endWordPosition="949">spect to its grammatical function. In (Tsarfaty et al., 2009) we showed that incorporating case indeed leads to improved performance for constituencybased, Relational-Realizational parsing of Hebrew. 1In (Maamouri et al., 2008), F178.1 for gold standard input. A more involved way to morphologically encode grammatical relations is by making explicit reference to the properties of multiple linguistic elements. This is the general pattern of agreement, i.e., “[A] systematic covariance between a semantic or a formal property of one element and a formal property of another.” (Steele, adapted from (Corbett, 2001)) Describing agreement patterns involves explicit reference to the following four components; the element which determines the agreement properties is the Controller of the agreement, the element whose properties are determined by agreement is the Target, the syntactic environment in which the agreement occurs is the Domain of agreement, and the properties with respect to which they agree are agreement Features (Corbett, 2001). Agreement is an inherently asymmetrical relation. Combination of features displayed by controllers has to be accommodated by the inflectional features of the target, bu</context>
</contexts>
<marker>Corbett, 2001</marker>
<rawString>Greville G. Corbett. 2001. Agreement: Terms and boundaries. In SMG conference papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabi Danon</author>
</authors>
<title>Definiteness spreading in the hebrew construct-state.</title>
<date>2008</date>
<journal>Lingua,</journal>
<volume>118</volume>
<issue>7</issue>
<contexts>
<context position="12426" citStr="Danon, 2008" startWordPosition="1904" endWordPosition="1905">a painter in the past ii. תרייצ איה התיה רבעב be’avar hayta hi in-past was.3FS she.3FS cayeret painter.FS She was a painter in the past” c. i. עיגהל הרומא התיה איה hi hayta amura She.3FS was.3FS supposed.FS lehagi’a to-arrive She was supposed to arrive ii. עיגהל התיה הרומא איה hi amura hayta She.3FS supposed.FS was.3FS lehagi’a to-arrive She was supposed to arrive Agreement in Construct State Nouns Semitic languages allow for the creation of noun compounds by phonologically marking their lexical head and adding a genitive complement. These constructions are called Construct-State Nouns (CSN) (Danon, 2008) and an example of a CSN is provided in (5a).3 (5) a. רייצה תב bat ha-cayar child.FS.CSN Def-painter.MS The painter’s daughter 3Also known as iDaFa constructions in Arabic. In such cases, all the agreement features are taken from the head of the CSN, the noun ‘daughter’ in (5). Since CSNs may be embedded in other CSNs, the constructions may be arbitrarily long. When short or long, CSNs themselves may be modified by adjectives that agree with the CSN as a whole. This gives rise to multiple patterns of agreement within a single complex CSN. Consider, for instance, the modified CSN in (6a). (6) a</context>
</contexts>
<marker>Danon, 2008</marker>
<rawString>Gabi Danon. 2008. Definiteness spreading in the hebrew construct-state. Lingua, 118(7):872–906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edit Doron</author>
</authors>
<title>The pronominal “copula” as agreement clitic. Syntax and Semantics,</title>
<date>1986</date>
<contexts>
<context position="9127" citStr="Doron, 1986" startWordPosition="1369" endWordPosition="1370">uses this arsenal of properties to implement a wide variety of agreement patterns realizing grammatical relations. Agreement in Hebrew S Domains Hebrew manifests different patterns of agreement in its S domain. Verbal predicates (the target) in matrix sentences (the domain) agree with their nominal subjects (the controller) on the agreement features gender, number and person. This occurs regardless of their configurational positions, as illustrated in (2b). (2) a. Agreement in Verbal Sentences: Controller: NP Target: V Domain: S Features: number, person, gender b. i. הנידל הנתמ ןתנ ינד which (Doron, 1986) calls Pron, are optionally placed after the subject. The position of Pron element with respect to the subject and predicate is fixed.2 The role of these Pron elements is to indicate the argument-structure of a nominal sentence that is not projected by a verb. In the Hebrew treebank they are subsumed under predicative phrases (PREDPs). If a PREDP head is of type NP or ADJP it must be inflected to reflect the features of the subject controller, as is illustrated in examples (3b-i)–(3b-ii). (3) a. Agreement in Nominal Sentences: Controller: NP Target: Pron Domain: S Features: number, gender,pers</context>
<context position="11298" citStr="Doron (1986)" startWordPosition="1725" endWordPosition="1726">ret (Pron.3FS)* Dina.FS painter.FS The pronominal features gender, number, person are also a part the inflectional paradigm of the verb היה (be), which is extended to include tense features. These inflected elements are used as AUX which function as co-heads together with the main (nominal or verbal) predicate. AUX elements that take a nominal predicate as in (4b) agree with their subject, and so do auxiliaries that take a verbal complement, e.g., the modal verb in (4c). The nominal predicate in (4b) also agrees with the subject – and so does the modal verb in (4c). Agreement of AUX with the 2Doron (1986) shows that these Pron elements can not be considered the present tense supplements of AUX elements in Hebrew since their position with respect to the subject and predicate is fixed, whereas AUX can change position, see (4) below. 42 verbal or nominal predicates is again independent of their surface positions. (4) a. Subject-AUX Agreement in Hebrew: Controller: NP Target: AUX Domain: S Features: number, person, gender b. i. תרייצ רבעב התיה איה hi hayta be’avar she.3FS was.3FS in-past cayeret painter.FS She was a painter in the past ii. תרייצ איה התיה רבעב be’avar hayta hi in-past was.3FS she.3</context>
</contexts>
<marker>Doron, 1986</marker>
<rawString>Edit Doron. 1986. The pronominal “copula” as agreement clitic. Syntax and Semantics, (19):313–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan A Sag</author>
</authors>
<title>Generalised phrase structure grammar.</title>
<date>1985</date>
<publisher>Blackwell,</publisher>
<location>Oxford, England.</location>
<contexts>
<context position="15399" citStr="Gazdar et al., 1985" startWordPosition="2362" endWordPosition="2365">ch syntactic processes do not make reference to aspects of word-internal structure other than morphologically marked inflectional features. What kind of parsing model would allow us to implement this relaxed version of the Lexicalist Hypothesis? The Morphosyntatctic State-Splits (SP) Model One way to maintain a relaxed version of the LH in syntax is to assume a constituency-based representation in which the morphological features of words are percolated to the level of constituency in which they are syntactically relevant. This approach is characteristic of feature-based grammars (e.g., GPSG (Gazdar et al., 1985) and follow-up studies). These grammars assume a feature geometry that defines the internal structure of node labels in phrase-structure trees.5 Category-label state-splits can reflect the different morphosyntactic behavior of different non-terminals of the same type. Using such supervised, linguistically motivated, state-splits, based on the phraselevel marking of morphological information is one may build an efficient implementation of a PCFGbased parsing model that takes into account morphological features. State-split models were shown to obtain state-of-the-art performance with little com</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan A. Sag. 1985. Generalised phrase structure grammar. Blackwell, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Hebrew dependency parsing: Initial results.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="2877" citStr="Goldberg and Elhadad, 2009" startWordPosition="416" endWordPosition="419">d HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for Hebrew they obtained tiny-to-no improvement. A question thus emerges whether there are any benefits in explicitly incorporating morphosyntactic agreement patterns into our models. This question is a manifestation of a greater issue, namely, whether it is beneficial to represent complex patterns of morphology in the statistical parsing model, or whether configurational information subsume the relevant patterns, as it is commonly assumed in constituencybased parsing. Here we claim that agreement features are useful for statistical parsing provided that they are represented and parametrized </context>
</contexts>
<marker>Goldberg, Elhadad, 2009</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2009. Hebrew dependency parsing: Initial results. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single framework for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22617" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="3430" endWordPosition="3433">r the explicit incorporation of agreement features helps Hebrew parsing, and if so, which of the two modeling strategies is better for utilizing the disambiguation cues provided by morphosyntactic agreement. Data We use the Hebrew treebank v2.0 with the extended annotation of (Guthmann et al., 2009), which adds inflectional properties to non-terminal categories such as NP and VP. We head-annotate the corpus and systematically add the agreement features of Domains throughout the treebank. We further distinguish finite from non-finite verb forms, and cliticized from non-cliticized nouns, as in (Goldberg and Tsarfaty, 2008; Tsarfaty et al., 2009). On top of the treebank labels SBJ subject, OBJ object, COM complement and CNJ conjunction we add PRD predicates and IC infinitival complements. Procedure We devised a procedure to read-off treebank grammars based on (i) GPSG-like, statesplit context-free parameters (SP-AGR), and (ii) RRAGR parameters in which context-free rules capture the projection, configuration and realization phases. In each model the multiplication provides the probability of the generation. We use relative frequency estimates and exhaustively parse gold pos-tagged input8 using a general-purpose</context>
<context position="24193" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="3662" endWordPosition="3666">ed labels (gp), and labels decorated with grand-parent and head-tag labels (gp,hd). We use increasingly richer subsets of the {gender, definiteness, accusativity} set.9 8This choice to parse gold-tagged sentences is meant to alleviate the differences in the model’s morphological disambiguation capacity. We want to evaluate the contribution of morphological features for syntactic disambiguation, and if the models will disambiguate the morphological analyses differently, the syntactic analysis will be assigned to different yields and the accuracy results would be strictly incomparable. But see (Goldberg and Tsarfaty, 2008) for a way to combine the two. 9We deliberately choose features that have non-overlapping behavior, to see whether their contribution is accumulative. 45 SMSF SMS NPMS-SBJ hi N dani she Dani MS-SBJ VPMS-RD hayta NPMS VP BJ i- -SBJ SBJ natan was i gave J amura -PRD NP-OBJ VPM upposed an matana - -PRD n an e present g OBJehagia D PP-COM NP J BOBJ na arrive ledian m etato-dina n ana pr PRD NP-OBJ hi VPMSD NS-SBJ mura VPMS-PRD NPMS-SJ PPCOM hta NPMS-SBJ PP-CO leha MS PMS NPMS PM PC PP she nmataVPMna natan ed NPM d dani PP le t a d ld le epreset g na gaveD aDani to le PP-COM ledian to-dina P(NPMS-S</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single framework for joint morphological segmentation and syntactic parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Noun phrase chunking in hebrew: Influence of lexical and morphological features.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1917" citStr="Goldberg et al., 2006" startWordPosition="275" endWordPosition="279">nfigurational languages (Hale, 1983) where the order of words is known to be (relatively) free. Agreement features encompass information concerning the functional relations between constituents in the syntactic structure, but whether incorporating agreement features in a statistical parsing model leads to improved performance has so far remained an open question and saw contradictory results. ∗The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. 40 Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 199</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2006</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2006. Noun phrase chunking in hebrew: Influence of lexical and morphological features. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nomie Guthmann</author>
<author>Yuval Krymolowski</author>
<author>Adi Milea</author>
<author>Yoad Winter</author>
</authors>
<title>Automatic annotation of morphosyntactic dependencies in a Modern Hebrew treebank.</title>
<date>2009</date>
<booktitle>Proceedings of TLT.</booktitle>
<editor>In Frank Van Eynde, Anette Frank, Koenraad De Smedt, and Gertjan van Noord, editors,</editor>
<contexts>
<context position="14658" citStr="Guthmann et al., 2009" startWordPosition="2246" endWordPosition="2249">bconstituents. Yet, from a functional point of view their contribution is entirely systematic. 3 The Models The strong version of the well-known Lexicalist Hypothesis (LH) states that “syntactic rules cannot make reference to any aspect of word internal structure” (Chomsky, 1970). Anderson (1982) argues that syntactic processes operating within configurational structures can often manipulate, or have access to, formal and inherent properties of individual words. Anderson (1982) argues that a model 4Examples for non-overlapping contribution of features by multiple dependencies can be found in (Guthmann et al., 2009). 43 that is well-equipped to capture such phenomena is one that retains a relaxed version of the LH, that is, one in which syntactic processes do not make reference to aspects of word-internal structure other than morphologically marked inflectional features. What kind of parsing model would allow us to implement this relaxed version of the Lexicalist Hypothesis? The Morphosyntatctic State-Splits (SP) Model One way to maintain a relaxed version of the LH in syntax is to assume a constituency-based representation in which the morphological features of words are percolated to the level of const</context>
<context position="22290" citStr="Guthmann et al., 2009" startWordPosition="3382" endWordPosition="3385">rbless (nominal) predicates under S analogously follows. 7In Hebrew, even some adverbial modifiers reflect patterns of agreement, e.g., ינדוY (literally, ”I am still”, glossed ‘still.1S’). This solution caters for all such patterns in which non-obligatory elements exhibit agreement. 4 Experiments We aim to examine whether the explicit incorporation of agreement features helps Hebrew parsing, and if so, which of the two modeling strategies is better for utilizing the disambiguation cues provided by morphosyntactic agreement. Data We use the Hebrew treebank v2.0 with the extended annotation of (Guthmann et al., 2009), which adds inflectional properties to non-terminal categories such as NP and VP. We head-annotate the corpus and systematically add the agreement features of Domains throughout the treebank. We further distinguish finite from non-finite verb forms, and cliticized from non-cliticized nouns, as in (Goldberg and Tsarfaty, 2008; Tsarfaty et al., 2009). On top of the treebank labels SBJ subject, OBJ object, COM complement and CNJ conjunction we add PRD predicates and IC infinitival complements. Procedure We devised a procedure to read-off treebank grammars based on (i) GPSG-like, statesplit conte</context>
</contexts>
<marker>Guthmann, Krymolowski, Milea, Winter, 2009</marker>
<rawString>Nomie Guthmann, Yuval Krymolowski, Adi Milea, and Yoad Winter. 2009. Automatic annotation of morphosyntactic dependencies in a Modern Hebrew treebank. In Frank Van Eynde, Anette Frank, Koenraad De Smedt, and Gertjan van Noord, editors, Proceedings of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth L Hale</author>
</authors>
<title>Warlpiri and the grammar of non-configurational languages.</title>
<date>1983</date>
<journal>Natural Language and Linguistic Theory,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1331" citStr="Hale, 1983" startWordPosition="189" endWordPosition="190">features. We focus on parsing Hebrew and report the best result to date, F184.13 for parsing off of gold-tagged text, 5% error reduction from previous results. 1 Introduction Agreement is defined by linguists as the systematic covariance of the grammatical properties of one linguistic element to reflect the semantic or formal properties of another (Corbett, 2001). Morphologically marked agreement features such as gender, number and person are used to realize grammatical relations between syntactic constituents, and such patterns are abundantly found in (less- or) nonconfigurational languages (Hale, 1983) where the order of words is known to be (relatively) free. Agreement features encompass information concerning the functional relations between constituents in the syntactic structure, but whether incorporating agreement features in a statistical parsing model leads to improved performance has so far remained an open question and saw contradictory results. ∗The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. 40 Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not bene</context>
<context position="5426" citStr="Hale, 1983" startWordPosition="797" endWordPosition="798">r than all parsing results reported so far for Arabic, a Semitic language with similar morphosyntactic phenomena.1 The RR approach is shown to be an adequate way to model complex morphosyntactic patterns for improving constituency-based parsing of a morphologically rich, free word order language. Because the RR model is also proper and generative, it may also embed as a language model to enhance more complex NLP tasks, e.g., statistical Machine Translation. 2 The Data The grammar of nonconfigurational languages allows for freedom in word ordering and discontinuities of syntactic constituents (Hale, 1983). Such languages do not rely on configurational information such as position and adjacency in marking grammatical relations such as subject and object, but instead they use word-level morphology. One way to encode grammatical relations in the form of words is by using morphological case, that is, explicitly marking an argument (e.g. nominative, accusative) with respect to its grammatical function. In (Tsarfaty et al., 2009) we showed that incorporating case indeed leads to improved performance for constituencybased, Relational-Realizational parsing of Hebrew. 1In (Maamouri et al., 2008), F178.</context>
</contexts>
<marker>Hale, 1983</marker>
<rawString>Kenneth L. Hale. 1983. Warlpiri and the grammar of non-configurational languages. Natural Language and Linguistic Theory, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Parsing with generative models of predicate-argument structure.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2247" citStr="Hockenmaier and Steedman, 2003" startWordPosition="324" endWordPosition="328">rmance has so far remained an open question and saw contradictory results. ∗The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. 40 Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew i</context>
</contexts>
<marker>Hockenmaier, Steedman, 2003</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2003. Parsing with generative models of predicate-argument structure. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16115" citStr="Klein and Manning, 2003" startWordPosition="2457" endWordPosition="2460">structure of node labels in phrase-structure trees.5 Category-label state-splits can reflect the different morphosyntactic behavior of different non-terminals of the same type. Using such supervised, linguistically motivated, state-splits, based on the phraselevel marking of morphological information is one may build an efficient implementation of a PCFGbased parsing model that takes into account morphological features. State-split models were shown to obtain state-of-the-art performance with little computational effort. Supervised state-splits for constituency-based unlexicalized parsing in (Klein and Manning, 2003) in an accurate English parser. For the pair of Hebrew sentences (2b), the morphological state-split context-free representation of the domain S is as described at the top of figure 1.6 The Relational-Realizational (RR) Model A different way to implement a syntactic model that conform to the relaxed LH is by separating the inflectional features of surface words from their grammatical functions in the syntactic representation and let5While agreement patterns in feature-rich grammars give rise to re-entrancies that break context-freeness, GPSG shows that using feature-percolation we can get quit</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Seth Kulick</author>
</authors>
<title>Enhanced annotation and parsing of the arabic treebank.</title>
<date>2008</date>
<booktitle>In Proceedings of INFOS.</booktitle>
<contexts>
<context position="2171" citStr="Maamouri et al., 2008" startWordPosition="312" endWordPosition="315">ent features in a statistical parsing model leads to improved performance has so far remained an open question and saw contradictory results. ∗The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. 40 Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When ag</context>
<context position="6019" citStr="Maamouri et al., 2008" startWordPosition="884" endWordPosition="887">tic constituents (Hale, 1983). Such languages do not rely on configurational information such as position and adjacency in marking grammatical relations such as subject and object, but instead they use word-level morphology. One way to encode grammatical relations in the form of words is by using morphological case, that is, explicitly marking an argument (e.g. nominative, accusative) with respect to its grammatical function. In (Tsarfaty et al., 2009) we showed that incorporating case indeed leads to improved performance for constituencybased, Relational-Realizational parsing of Hebrew. 1In (Maamouri et al., 2008), F178.1 for gold standard input. A more involved way to morphologically encode grammatical relations is by making explicit reference to the properties of multiple linguistic elements. This is the general pattern of agreement, i.e., “[A] systematic covariance between a semantic or a formal property of one element and a formal property of another.” (Steele, adapted from (Corbett, 2001)) Describing agreement patterns involves explicit reference to the following four components; the element which determines the agreement properties is the Controller of the agreement, the element whose properties </context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2008</marker>
<rawString>Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008. Enhanced annotation and parsing of the arabic treebank. In Proceedings of INFOS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature-forest models for probabilistic hpsg parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2281" citStr="Miyao and Tsujii, 2008" startWordPosition="331" endWordPosition="334">n and saw contradictory results. ∗The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. 40 Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature-forest models for probabilistic hpsg parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja Øvrelid</author>
<author>Joakim Nivre</author>
</authors>
<title>Swedish dependency parsing with rich linguistic features.</title>
<date>2007</date>
<booktitle>In Proceeding of RANLP.</booktitle>
<contexts>
<context position="2722" citStr="Øvrelid and Nivre, 2007" startWordPosition="393" endWordPosition="396">el-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for Hebrew they obtained tiny-to-no improvement. A question thus emerges whether there are any benefits in explicitly incorporating morphosyntactic agreement patterns into our models. This question is a manifestation of a greater issue, namely, whether it is beneficial to represent complex patterns of morphology in the statistical parsing model, or whether configurational information subsume the relevant patterns, as it is commonly assumed </context>
</contexts>
<marker>Øvrelid, Nivre, 2007</marker>
<rawString>Lilja Øvrelid and Joakim Nivre. 2007. Swedish dependency parsing with rich linguistic features. In Proceeding of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
</authors>
<title>Relationalrealizational parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoLing.</booktitle>
<marker>Tsarfaty, Sima’an, 2008</marker>
<rawString>Reut Tsarfaty and Khalil Sima’an. 2008. Relationalrealizational parsing. In Proceedings of CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
<author>Remko Scha</author>
</authors>
<title>An alternative to head-driven approaches for parsing a (relatively) free word order language.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Tsarfaty, Sima’an, Scha, 2009</marker>
<rawString>Reut Tsarfaty, Khalil Sima’an, and Remko Scha. 2009. An alternative to head-driven approaches for parsing a (relatively) free word order language. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>