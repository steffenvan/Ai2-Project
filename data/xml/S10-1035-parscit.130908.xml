<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014414">
<title confidence="0.97483">
WINGNUS: Keyphrase Extraction Utilizing Document Logical Structure
</title>
<author confidence="0.99319">
Thuy Dung Nguyen
</author>
<affiliation confidence="0.996207666666667">
Department of Computer Science
School of Computing
National University of Singapore
</affiliation>
<email confidence="0.989842">
nguyen14@comp.nus.edu.sg
</email>
<author confidence="0.994473">
Minh-Thang Luong
</author>
<affiliation confidence="0.9966">
Department of Computer Science
School of Computing
National University of Singapore
</affiliation>
<email confidence="0.995536">
luongmin@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932176470588">
We present a system description of the
WINGNUS team work1 for the SemEval-
2010 task #5 Automatic Keyphrase Ex-
traction from Scientific Articles. A key
feature of our system is that it utilizes an
inferred document logical structure in our
candidate identification process, to limit
the number of phrases in the candidate list,
while maintaining its coverage of impor-
tant phrases. Our top performing system
achieves an F1 of 25.22% for the com-
bined keyphrases (author and reader as-
signed) in the final test data. We note that
the method we report here is novel and or-
thogonal from other systems, so it can be
combined with other techniques to poten-
tially achieve higher performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999825941176471">
Keyphrases are noun phrases (NPs) that capture
the primary topics of a document. While benefi-
cial for applications such as summarization, clus-
tering and indexing, only a minority of documents
have manually-assigned keyphrases, as it is a time-
consuming process. Automatic keyphrase genera-
tion is thus a focus for many researchers.
Most existing keyphrase extraction systems
view this task as a supervised classification task in
two stages: generating a list of candidates – can-
didate identification; and using answer keyphrases
to distinguish true keyphrases – candidate selec-
tion. The selection model uses a set of features that
capture the saliency of a phrase as a keyphrase.
A major challenge of the keyphrase extraction
task lies in the candidate identification process.
A narrow candidate list will overlook some true
</bodyText>
<footnote confidence="0.939594">
1This work was supported by a National Research Foun-
dation grant “Interactive Media Search” (grant # R-252-000-
325-279).
</footnote>
<bodyText confidence="0.997019391304348">
keyphrases (favoring precision), whereas a broad
list will produce more errors and require more pro-
cessing in latter selection stage (favoring recall).
In our previous system (Nguyen and Kan,
2007), we made use of the document logical struc-
ture in the proposed features. The premise of this
earlier work was that keyphrases are distributed
non-uniformly in different logical sections of a pa-
per, favoring sections such as introduction, and
related work. We introduced features indicating
which sections a candidate occurrs in. For our
fielded system in this task (Kim et al., 2010), we
further leverage the document logical structure for
both candidate identification and selection stages.
Our contributions are as follows: 1) We suggest
the use of Google Scholar-based crawler to auto-
matically find PDF files to enhance logical struc-
ture extraction; 2) We provide a keyphrase distri-
bution study with respect to different logical struc-
tures; and 3) From the study result, we propose a
candidate identification approach that uses logical
structures to effectively limit the number of candi-
dates considered while ensuring good coverage.
</bodyText>
<sectionHeader confidence="0.966268" genericHeader="introduction">
2 Preprocessing
</sectionHeader>
<bodyText confidence="0.9999741">
Although we have plain text for all test input, we
posit that logical structure recovery is much more
robust given the original richly-formatted docu-
ment (e.g., PDF), as font and formatting informa-
tion can be used for detection. As a bridge be-
tween plain text data provided by the organizer
and PDF input required to extract formatting fea-
tures, we first describe our Google Scholar-based
crawler to find PDFs given plain texts. We then
detail on the logical structure extraction process.
</bodyText>
<subsectionHeader confidence="0.991075">
Google Scholar-based Paper Crawler
</subsectionHeader>
<bodyText confidence="0.9291155">
Our crawler2 takes inputs as titles to query Google
Scholar (GS) by means of web scraping. It pro-
</bodyText>
<footnote confidence="0.982411">
2http://wing.comp.nus.edu.sg/˜lmthang/GS/
</footnote>
<page confidence="0.966227">
166
</page>
<bodyText confidence="0.9558789">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 166–169,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
cesses GS results and performs approximate ti-
tle matching using character-based Longest Com-
mon Subsequence similarity. Once a matching ti-
tle with high similarity score (&gt; 0.7 experimen-
tally) is found, the crawler retrieves the list of
available PDFs, and starts downloading until one
is correctly stored. We enforce that PDFs accepted
should have the OCR texts closely match the pro-
vided plain texts in terms of lines and tokens.
In the keyphrase task, we approximate the title
inputs to our crawler by considering the first two
lines of each plain text provided. For 140 train
and 100 test input documents, the crawler down-
loaded 117 and 80 PDFs, of which 116 and 76 files
are correct, respectively. This yields an accept-
able level of performance in terms of (Precision,
Recall) of (99.15%, 82.86%) for train and (95%,
76%) for test data.
</bodyText>
<subsectionHeader confidence="0.959182">
Logical Structure Extraction
</subsectionHeader>
<bodyText confidence="0.999989388888889">
Logical structure is defined as “a hierarchy of log-
ical components, for example, titles, authors, affil-
iations, abstracts, sections, etc.” in (Mao et al.,
2003). Operationalizing this definition, we em-
ploy an in-house software, called SectLabel (Lu-
ong et al., to appear), to obtain comprehensive
logical structure information for each document.
SectLabel classifies each text line in a scholarly
document with a semantic class (e.g., title, header,
bodyText). Header lines are furthered classified
into generic roles (e.g., abstract, intro, method).
A prominent feature of SectLabel is that it is
capable of utilizing rich information, such as font
format and spatial layout, from an optical char-
acter recognition (OCR) output if PDF files are
present3. In case PDFs are unavailable, SectLa-
bel still handles plain text based logical structure
discovery, but with degraded performance.
</bodyText>
<sectionHeader confidence="0.959972" genericHeader="method">
3 Candidate Phrase Identification
</sectionHeader>
<subsectionHeader confidence="0.971519">
Phrase Distribution Study
</subsectionHeader>
<bodyText confidence="0.983809388888889">
We perform a study of keyphrase distribution on
the training data over different logical structures
(LSs) to understand the importance of each sec-
tion within documents. These LSs include: ti-
tle, headers, abstract, introduction (intro), related
work (rw), conclusion, and body text4 (body).
3We note that the PDFs have author assigned keyphrases
of the document, but we filtered this information before pass-
ing to our keyphrases system to ensure a fair test.
4We utilize the comprehensive output of our logical struc-
ture system to filter out copyright, email, equation, figure,
We make a key observation that within a para-
graph, important phrases occur mostly in the first
n sentences. To validate our hypothesis, we con-
sider keyphrase distribution over bodyn, which is
the subset of all of the body LS, limited to the first
n sentences of each paragraph (n = 1, 2, 3 experi-
mentally).
</bodyText>
<table confidence="0.988816615384615">
Ath Rder Com Sent Den
title 142 175 251 122 2.06
headers 158 342 425 1,893 0.22
abstract 276 745 897 1,124 0.80
intro 335 984 1,166 4,338 0.27
rw 160 345 443 1,945 0.23
concl 227 488 616 1,869 0.33
body 398 1,175 1,411 39,179 0.04
full 465 1,720 1,994 50,512 0.04
body, 333 839 1,035 11,280 0.09
body2 366 980 1,197 20,024 0.06
body3 382 1,042 1,269 26,163 0.05
fulltext 480 1,773 2,059 166,471 0.01
</table>
<tableCaption confidence="0.998887">
Table 1: Keyphrase distribution over different log-
</tableCaption>
<bodyText confidence="0.990649695652174">
ical structures computed from the 144 training
documents. The type counts of author-assigned
(ath), reader-assigned (rder) and combined (comb)
keyphrases are shown. Sent indicates the number
of sentences in each LS. The Den column gives the
density of keyphrases for each LS.
Results in Table 1 show that individual LSs
(title, headers, abstract, intro, rw, concl) con-
tain a high concentration (i.e., density &gt; 0.2)
of keyphrases, with title and abstract having the
highest density, and intro being the most dominant
LS in terms of keyphrase count. With all these
LSs and body, we obtain the full setting, covering
1994/2059=96.84% of all keyphrases appearing in
the original text, fulltext, while effectively reduc-
ing the number of processed sentences by more
than two-thirds.
Considering only the first sentence of each para-
graph in the body text, bodyi, yields fair keyphrase
coverage of 1035/1411=73.35% relative to that of
fulltext. The number of lines to be processed is
much smaller, about a third, which validates our
aforementioned hypothesis.
</bodyText>
<subsectionHeader confidence="0.605888">
Keyphrase Extraction
</subsectionHeader>
<bodyText confidence="0.937445166666667">
Results from the keyphrase distribution study mo-
tivates us to further explore the use of logical
structures (LS). The idea is to limit the search
scope of our candidate identification system while
maintaining coverage. We propose a new ap-
caption, footnote, and reference lines.
</bodyText>
<page confidence="0.99186">
167
</page>
<bodyText confidence="0.9999602">
proach, which extracts candidates according to the
regular expression rules discussed in (Kim and
Kan, 2009). However, instead of using the whole
document text as input, we abridge the input text
at different levels from full to minimal.
</bodyText>
<subsectionHeader confidence="0.869052">
Input Description Cand Com Recall
</subsectionHeader>
<bodyText confidence="0.99419947826087">
Table 2: Different levels of abridged inputs com-
puted on the training data. Cand shows the
number of candidate keyphrases extracted for
each input type; Com gives the number of cor-
rect keyphrases appear as candidates; Recall is
computed with respect to the total number of
keyphrases in the original texts (2059).
Results in Table 2 show that we could gather
a recall of 63.72% when considering a signifi-
cantly abridged form of the input culled from ti-
tle, headers, abstract (abs) and introduction (in-
tro) – minimal. Further adding related work (rw)
and conclusion – medium – enhances the recall by
4.95%. When adding only the first line of each
paragraph in the body text, we achieve a good re-
call of 76.74% while effectively reducing the num-
ber of candidate phrases to be process by a half
with respect to the fulltext input. Even though
full2, full3, and full show further improvements in
terms of recall, we opt to use full1 in our experi-
mental runs, which trades off recall for less com-
putational complexity, which may influence down-
stream classification.
</bodyText>
<sectionHeader confidence="0.984255" genericHeader="method">
4 Candidate Phrase Selection
</sectionHeader>
<bodyText confidence="0.99967375">
Following (Nguyen and Kan, 2007), we use the
Naive Bayes model implemented in Weka (Hall et
al., 2009) for candidate phrase selection. As dif-
ferent learning models have been discussed much
previous work, we just list the different features
with which we experimented with. Our features5
are as follows (where n indicates a numeric fea-
ture; b, a boolean one):
</bodyText>
<footnote confidence="0.835487">
5Detailed feature definitions are described in (Nguyen and
Kan, 2007; Kim and Kan, 2009).
</footnote>
<listItem confidence="0.989143230769231">
F1-F3 (n): TFxIDF, term frequency, term fre-
quency of substrings.
F4-F5 (n): First and last occurrences (word off-
set).
F6 (n): Length of phrases in words.
F7 (b): Typeface attribute (available when PDF
is present) – Indicates if any part of the candidate
phrase has appeared in the document with bold
or italic format, a good hint for its relevance as
a keyphrase.
F8 (b): InTitle – shows whether a phrase is also
part of the document title.
F9 (n): TitleOverlap – the number of times
</listItem>
<bodyText confidence="0.976788909090909">
a phrase appears in the title of other scholarly
documents (obtained from a dump of the DBLP
database).
F10-F14 (b): Header, Abstract, Intro, RW,
Concl – indicate whether a phrase appears in head-
ers, abstract, introduction, related work or conclu-
sion sections, respectively.
F15-F19 (n): HeaderF, AbstractF, IntroF, RWF,
ConclF - indicate the frequency of a phrase in
the headers, abstract, introduction, related work or
conclusion sections, respectively.
</bodyText>
<sectionHeader confidence="0.99801" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.954537">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999965375">
For this task (Kim et al., 2010), we are given two
datasets: train (144 docs) and test (100 docs) with
detailed answers for train. To tune our system,
we split the train dataset into train and validation
subsets: trains (104 docs) and train„ (40 docs).
Once the best setting is derived from trains-train,,,
we obtain the final model trained on the full data,
and apply it to the test set for the final results.
</bodyText>
<subsectionHeader confidence="0.997925">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999926">
Our evaluation process is accomplished in two
stages: we first experiment different feature com-
binations by using the input types fulltext and full1.
We then fix the best feature set, and vary our dif-
ferent abridged inputs to find the optimal one.
</bodyText>
<subsectionHeader confidence="0.984414">
Feature Combination
</subsectionHeader>
<bodyText confidence="0.999226428571429">
To evaluate the performance of individual features,
we define a base feature set, as F1,4, and measure
the performance of each feature added separately
to the base. Results in Table 3 have highlighted
the set of positive features, which is F3,5,6,13,16.
From the positive set F3,5,6,13,16, we tried dif-
ferent combinations for the two input types shown
</bodyText>
<figure confidence="0.986451176470588">
minimal 30,702 1,312
title + headers
+ abs + intro
medium min + rw
+ conclusion 44,975 1,414
full1 med + body1 73,958 1,580
full2 med + body2 90,624 1,635
full3 med + body3 101,006 1,672
full med + body 121,378 1,737
fulltext original text 148,411 1,766
63.72%
68.67%
76.74%
79.41%
81.20%
84.36%
85.77%
</figure>
<page confidence="0.977371">
168
</page>
<table confidence="0.9987448">
System F Score System F Score
base 23.42% + F11 23.42%
+ F2 21.13% + F12 23.42%
+ F3 24.57% + F13 23.75%
+ F5 24.08% + F14 22.28%
+ F6 25.06% + F15 22.11%
+ F7 23.42% + F16 23.59%
+ F8 22.77% + F17 22.60%
+ F9 22.28% + F18 23.26%
+ F10 23.42% + F19 21.95%
</table>
<tableCaption confidence="0.9090165">
Table 3: Performance of individual features (on
fulltext) added separately to the base set F1,4.
</tableCaption>
<bodyText confidence="0.9964038">
in Table 4. The results indicate that while fulltext
obtains the best performance with F3,6,5 added, us-
ing full1 shows superior performance at 28.18% F
Score with F3,6 added. Hence, we have identified
our best feature set as F1,3,4,6.
</bodyText>
<table confidence="0.997772714285714">
fulltext full1
base (F1,4) 23.42% 22.60%
+ F3,6 25.88% 28.18%
+ F3,6,5 26.21% 26.21%
+ F3,6,5,13 24.90% 26.21%
+ F3,6,5,16 24.24% 26.70%
+ F3,6,5,13,16 23.42% 26.70%
</table>
<tableCaption confidence="0.995703">
Table 4: Performance (F1) over difference feature
combinations for fulltext and full1 inputs.
</tableCaption>
<sectionHeader confidence="0.760994" genericHeader="method">
Abridged Inputs
</sectionHeader>
<bodyText confidence="0.9998546">
Table 5 gives the performance for the abridged
inputs we tried with the best feature set F1,3,4,6.
All full1, full2, full3 and full show improved per-
formance compared to those on the fulltext. We
achieve our best performance with full1 at 28.18%
F Score. These results validate the effectiveness
of our approach in utilizing logical structure for
the candidate identification. We report our results
submitted in Table 6. These figures are achieved
using the best feature combination F1,3,4,6.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999839571428571">
We have described and evaluated our keyphrase
extraction system for the SemEval-2 Task #5.
With the use of logical structure in the candidate
identification, our system has demonstrated its su-
perior performance over systems that do not use
such information. Moreover, we have effectively
reduced the numbers of text lines and candidate
</bodyText>
<table confidence="0.979296375">
Input @5 @10 @15 Fscore
min 62 110 145 23.75%
med 79 130 158 25.88%
full1 84 135 172 28.18%
full2 90 132 164 26.86%
full3 89 134 162 26.54%
full 84 130 164 26.86%
fulltext 82 127 158 25.88%
</table>
<tableCaption confidence="0.911070333333333">
Table 5: Performance over different abridged in-
puts using the best feature set F1,3,4,6. “@N” indi-
cates the number of top N keyphrase matches.
</tableCaption>
<table confidence="0.999481666666667">
System Description F@5 F@10 F@15
WINGNUS1 full, F1,3,4,6 20.65% 24.66% 24.95%
WINGNUS2 full1, F1,3,4,6 20.45% 24.73% 25.22%
</table>
<tableCaption confidence="0.996709">
Table 6: Final results on the test data.
</tableCaption>
<bodyText confidence="0.999685333333333">
phrases to be processed in the candidate identifi-
cation and selection respectively by about half.
Our system takes advantage of the logical struc-
ture analysis but not to the extent we had hoped.
We had hypothesized that formatting features (F7)
such as bold and italics, would help discriminate
key phrases, but in our limited experiments for
this task did not validate this. Similarly, external
knowledge should help in the keyphrase task, but
the prior knowledge about keyphrase likelihood
(F9) in DBLP hurt performance in our tests. We
plan to further explore these issues for the future.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708047619047">
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. In MOVE ’09.
Su Nam Kim, Alyona Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Task 5: Automatic
keyphrase extraction from scientific articles. In Se-
mEval.
Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen
Kan. to appear. Logical structure recovery in schol-
arly articles with rich document features. IJDLS.
Forthcoming, accepted for publication.
Song Mao, Azriel Rosenfeld, and Tapas Kanungo.
2003. Document structure analysis algorithms: a lit-
erature survey. In Proc. SPIE Electronic Imaging.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
ICADL.
</reference>
<page confidence="0.998827">
169
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901762">
<title confidence="0.999538">WINGNUS: Keyphrase Extraction Utilizing Document Logical Structure</title>
<author confidence="0.999876">Thuy Dung Nguyen</author>
<affiliation confidence="0.999786">Department of Computer Science School of Computing National University of Singapore</affiliation>
<email confidence="0.937408">nguyen14@comp.nus.edu.sg</email>
<author confidence="0.997484">Minh-Thang Luong</author>
<affiliation confidence="0.999692666666667">Department of Computer Science School of Computing National University of Singapore</affiliation>
<email confidence="0.99336">luongmin@comp.nus.edu.sg</email>
<abstract confidence="0.99826">We present a system description of the team for the SemEval- 2010 task #5 Automatic Keyphrase Extraction from Scientific Articles. A key feature of our system is that it utilizes an inferred document logical structure in our candidate identification process, to limit the number of phrases in the candidate list, while maintaining its coverage of important phrases. Our top performing system an 25.22% for the combined keyphrases (author and reader assigned) in the final test data. We note that the method we report here is novel and orthogonal from other systems, so it can be combined with other techniques to potentially achieve higher performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="9932" citStr="Hall et al., 2009" startWordPosition="1577" endWordPosition="1580">– enhances the recall by 4.95%. When adding only the first line of each paragraph in the body text, we achieve a good recall of 76.74% while effectively reducing the number of candidate phrases to be process by a half with respect to the fulltext input. Even though full2, full3, and full show further improvements in terms of recall, we opt to use full1 in our experimental runs, which trades off recall for less computational complexity, which may influence downstream classification. 4 Candidate Phrase Selection Following (Nguyen and Kan, 2007), we use the Naive Bayes model implemented in Weka (Hall et al., 2009) for candidate phrase selection. As different learning models have been discussed much previous work, we just list the different features with which we experimented with. Our features5 are as follows (where n indicates a numeric feature; b, a boolean one): 5Detailed feature definitions are described in (Nguyen and Kan, 2007; Kim and Kan, 2009). F1-F3 (n): TFxIDF, term frequency, term frequency of substrings. F4-F5 (n): First and last occurrences (word offset). F6 (n): Length of phrases in words. F7 (b): Typeface attribute (available when PDF is present) – Indicates if any part of the candidate</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Min-Yen Kan</author>
</authors>
<title>Re-examining automatic keyphrase extraction approaches in scientific articles.</title>
<date>2009</date>
<booktitle>In MOVE ’09.</booktitle>
<contexts>
<context position="8576" citStr="Kim and Kan, 2009" startWordPosition="1345" endWordPosition="1348">xt, bodyi, yields fair keyphrase coverage of 1035/1411=73.35% relative to that of fulltext. The number of lines to be processed is much smaller, about a third, which validates our aforementioned hypothesis. Keyphrase Extraction Results from the keyphrase distribution study motivates us to further explore the use of logical structures (LS). The idea is to limit the search scope of our candidate identification system while maintaining coverage. We propose a new apcaption, footnote, and reference lines. 167 proach, which extracts candidates according to the regular expression rules discussed in (Kim and Kan, 2009). However, instead of using the whole document text as input, we abridge the input text at different levels from full to minimal. Input Description Cand Com Recall Table 2: Different levels of abridged inputs computed on the training data. Cand shows the number of candidate keyphrases extracted for each input type; Com gives the number of correct keyphrases appear as candidates; Recall is computed with respect to the total number of keyphrases in the original texts (2059). Results in Table 2 show that we could gather a recall of 63.72% when considering a significantly abridged form of the inpu</context>
<context position="10277" citStr="Kim and Kan, 2009" startWordPosition="1633" endWordPosition="1636">use full1 in our experimental runs, which trades off recall for less computational complexity, which may influence downstream classification. 4 Candidate Phrase Selection Following (Nguyen and Kan, 2007), we use the Naive Bayes model implemented in Weka (Hall et al., 2009) for candidate phrase selection. As different learning models have been discussed much previous work, we just list the different features with which we experimented with. Our features5 are as follows (where n indicates a numeric feature; b, a boolean one): 5Detailed feature definitions are described in (Nguyen and Kan, 2007; Kim and Kan, 2009). F1-F3 (n): TFxIDF, term frequency, term frequency of substrings. F4-F5 (n): First and last occurrences (word offset). F6 (n): Length of phrases in words. F7 (b): Typeface attribute (available when PDF is present) – Indicates if any part of the candidate phrase has appeared in the document with bold or italic format, a good hint for its relevance as a keyphrase. F8 (b): InTitle – shows whether a phrase is also part of the document title. F9 (n): TitleOverlap – the number of times a phrase appears in the title of other scholarly documents (obtained from a dump of the DBLP database). F10-F14 (b</context>
</contexts>
<marker>Kim, Kan, 2009</marker>
<rawString>Su Nam Kim and Min-Yen Kan. 2009. Re-examining automatic keyphrase extraction approaches in scientific articles. In MOVE ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Alyona Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Task 5: Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In SemEval.</booktitle>
<contexts>
<context position="2549" citStr="Kim et al., 2010" startWordPosition="388" endWordPosition="391">arch” (grant # R-252-000- 325-279). keyphrases (favoring precision), whereas a broad list will produce more errors and require more processing in latter selection stage (favoring recall). In our previous system (Nguyen and Kan, 2007), we made use of the document logical structure in the proposed features. The premise of this earlier work was that keyphrases are distributed non-uniformly in different logical sections of a paper, favoring sections such as introduction, and related work. We introduced features indicating which sections a candidate occurrs in. For our fielded system in this task (Kim et al., 2010), we further leverage the document logical structure for both candidate identification and selection stages. Our contributions are as follows: 1) We suggest the use of Google Scholar-based crawler to automatically find PDF files to enhance logical structure extraction; 2) We provide a keyphrase distribution study with respect to different logical structures; and 3) From the study result, we propose a candidate identification approach that uses logical structures to effectively limit the number of candidates considered while ensuring good coverage. 2 Preprocessing Although we have plain text fo</context>
<context position="11278" citStr="Kim et al., 2010" startWordPosition="1798" endWordPosition="1801">shows whether a phrase is also part of the document title. F9 (n): TitleOverlap – the number of times a phrase appears in the title of other scholarly documents (obtained from a dump of the DBLP database). F10-F14 (b): Header, Abstract, Intro, RW, Concl – indicate whether a phrase appears in headers, abstract, introduction, related work or conclusion sections, respectively. F15-F19 (n): HeaderF, AbstractF, IntroF, RWF, ConclF - indicate the frequency of a phrase in the headers, abstract, introduction, related work or conclusion sections, respectively. 5 Experiments 5.1 Datasets For this task (Kim et al., 2010), we are given two datasets: train (144 docs) and test (100 docs) with detailed answers for train. To tune our system, we split the train dataset into train and validation subsets: trains (104 docs) and train„ (40 docs). Once the best setting is derived from trains-train,,, we obtain the final model trained on the full data, and apply it to the test set for the final results. 5.2 Evaluation Our evaluation process is accomplished in two stages: we first experiment different feature combinations by using the input types fulltext and full1. We then fix the best feature set, and vary our different</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Alyona Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. Task 5: Automatic keyphrase extraction from scientific articles. In SemEval.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Minh-Thang Luong</author>
<author>Thuy Dung Nguyen</author>
<author>Min-Yen Kan</author>
</authors>
<title>to appear. Logical structure recovery in scholarly articles with rich document features. IJDLS. Forthcoming, accepted for publication.</title>
<marker>Luong, Nguyen, Kan, </marker>
<rawString>Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen Kan. to appear. Logical structure recovery in scholarly articles with rich document features. IJDLS. Forthcoming, accepted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Mao</author>
<author>Azriel Rosenfeld</author>
<author>Tapas Kanungo</author>
</authors>
<title>Document structure analysis algorithms: a literature survey.</title>
<date>2003</date>
<booktitle>In Proc. SPIE Electronic Imaging.</booktitle>
<contexts>
<context position="4993" citStr="Mao et al., 2003" startWordPosition="772" endWordPosition="775">lines and tokens. In the keyphrase task, we approximate the title inputs to our crawler by considering the first two lines of each plain text provided. For 140 train and 100 test input documents, the crawler downloaded 117 and 80 PDFs, of which 116 and 76 files are correct, respectively. This yields an acceptable level of performance in terms of (Precision, Recall) of (99.15%, 82.86%) for train and (95%, 76%) for test data. Logical Structure Extraction Logical structure is defined as “a hierarchy of logical components, for example, titles, authors, affiliations, abstracts, sections, etc.” in (Mao et al., 2003). Operationalizing this definition, we employ an in-house software, called SectLabel (Luong et al., to appear), to obtain comprehensive logical structure information for each document. SectLabel classifies each text line in a scholarly document with a semantic class (e.g., title, header, bodyText). Header lines are furthered classified into generic roles (e.g., abstract, intro, method). A prominent feature of SectLabel is that it is capable of utilizing rich information, such as font format and spatial layout, from an optical character recognition (OCR) output if PDF files are present3. In cas</context>
</contexts>
<marker>Mao, Rosenfeld, Kanungo, 2003</marker>
<rawString>Song Mao, Azriel Rosenfeld, and Tapas Kanungo. 2003. Document structure analysis algorithms: a literature survey. In Proc. SPIE Electronic Imaging.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Dung Nguyen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Keyphrase extraction in scientific publications.</title>
<date>2007</date>
<booktitle>In ICADL.</booktitle>
<contexts>
<context position="2165" citStr="Nguyen and Kan, 2007" startWordPosition="326" endWordPosition="329">rases to distinguish true keyphrases – candidate selection. The selection model uses a set of features that capture the saliency of a phrase as a keyphrase. A major challenge of the keyphrase extraction task lies in the candidate identification process. A narrow candidate list will overlook some true 1This work was supported by a National Research Foundation grant “Interactive Media Search” (grant # R-252-000- 325-279). keyphrases (favoring precision), whereas a broad list will produce more errors and require more processing in latter selection stage (favoring recall). In our previous system (Nguyen and Kan, 2007), we made use of the document logical structure in the proposed features. The premise of this earlier work was that keyphrases are distributed non-uniformly in different logical sections of a paper, favoring sections such as introduction, and related work. We introduced features indicating which sections a candidate occurrs in. For our fielded system in this task (Kim et al., 2010), we further leverage the document logical structure for both candidate identification and selection stages. Our contributions are as follows: 1) We suggest the use of Google Scholar-based crawler to automatically fi</context>
<context position="9862" citStr="Nguyen and Kan, 2007" startWordPosition="1564" endWordPosition="1567">tro) – minimal. Further adding related work (rw) and conclusion – medium – enhances the recall by 4.95%. When adding only the first line of each paragraph in the body text, we achieve a good recall of 76.74% while effectively reducing the number of candidate phrases to be process by a half with respect to the fulltext input. Even though full2, full3, and full show further improvements in terms of recall, we opt to use full1 in our experimental runs, which trades off recall for less computational complexity, which may influence downstream classification. 4 Candidate Phrase Selection Following (Nguyen and Kan, 2007), we use the Naive Bayes model implemented in Weka (Hall et al., 2009) for candidate phrase selection. As different learning models have been discussed much previous work, we just list the different features with which we experimented with. Our features5 are as follows (where n indicates a numeric feature; b, a boolean one): 5Detailed feature definitions are described in (Nguyen and Kan, 2007; Kim and Kan, 2009). F1-F3 (n): TFxIDF, term frequency, term frequency of substrings. F4-F5 (n): First and last occurrences (word offset). F6 (n): Length of phrases in words. F7 (b): Typeface attribute (a</context>
</contexts>
<marker>Nguyen, Kan, 2007</marker>
<rawString>Thuy Dung Nguyen and Min-Yen Kan. 2007. Keyphrase extraction in scientific publications. In ICADL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>