<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048146">
<title confidence="0.995261">
ATT1: Temporal Annotation Using Big Windows and Rich Syntactic and
Semantic Features
</title>
<author confidence="0.749389">
Hyuckchul Jung and Amanda Stent
</author>
<affiliation confidence="0.620765">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.8819705">
180 Park Ave
Florham Park, NJ 07932, USA
</address>
<email confidence="0.996595">
hjung, stent@research.att.com
</email>
<sectionHeader confidence="0.995835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990768333333333">
In this paper we present the results of exper-
iments comparing (a) rich syntactic and se-
mantic feature sets and (b) big context win-
dows, for the TempEval time expression and
event segmentation and classification tasks.
We show that it is possible for models using
only lexical features to approach the perfor-
mance of models using rich syntactic and se-
mantic feature sets.
</bodyText>
<sectionHeader confidence="0.998235" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.950515473684211">
TempEval-3 Temporal Annotation Task (UzZaman
et al., 2012) has three subtasks:
A Time expression extraction and classification -
extract time expressions from input text, and de-
termine the type and normalised value for each
extracted time expression.
B Event extraction and classification - extract event
mentions from input text, and determine the class,
tense and aspect features for each extracted event.
C Temporal link identification - identify and cate-
gorise temporal links between events in the same
or consecutive sentences, events and time expres-
sions in the same sentence, and events and the
document creation time of the input text.
Here we report results for the first two tasks.
Previous TempEval competitions have shown that
rich syntactic and semantic feature sets can lead to
good performance on event and time expression ex-
traction and classification tasks (e.g. (Llorens et al.,
</bodyText>
<page confidence="0.96479">
20
</page>
<table confidence="0.998321">
Type Files EVENT TIMEX
AQUAINT gold 73 4431 579
TimeBank gold 183 6698 1243
TE3-Silver silver 2452 81329 12739
</table>
<tableCaption confidence="0.9917135">
Table 1: Frequency of event and time expressions in the
text portions of the TempEval-3 data sets
</tableCaption>
<bodyText confidence="0.9995504">
2010; UzZaman and Allen, 2010)). In this work, we
show that with large windows of context, it is pos-
sible for models using only lexical features to ap-
proach the performance of models using rich syn-
tactic and semantic feature sets.
</bodyText>
<sectionHeader confidence="0.989443" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999812909090909">
Using the gold and silver data distributed by the
TempEval-3 task organizers (see Table 1), we pro-
cessed each input file with the Stanford CoreNLP
(Stanford Natural Language Processing Group,
2012) and SENNA (Collobert et al., 2011) open-
source NLP tools. From the Stanford CoreNLP
tools we obtained a tokenization of the input text,
the lemma and part of speech (POS) tag for each
token, and dependency and constituency parses for
each sentence. From SENNA, we obtained a seman-
tic role labelling for each sentence.
</bodyText>
<sectionHeader confidence="0.99629" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999964166666667">
We were curious to explore the tradeoff between ad-
ditional context on the one hand, and additional lay-
ers of representation on the other, for the event and
time expression extraction tasks. Researchers have
investigated the impacts of different sets of features
(Adafre and de Rijke, 2005; Angeli et al., 2012;
</bodyText>
<table confidence="0.895521307692308">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 20–24, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
Feature type Features Used in
Lexical 1 token ATT2, ATT3
Lexical 2 lemma ATT1, ATT2
Part of speech POS tag ATT1, ATT2
Dependency governing verb, governing verb POS, governing preposition, ATT1, ATT2
phrase tag, path to root of parse tree, head word, head word lemma,
head word POS
Constituency governing verb, governing verb POS, governing preposition, ATT1, ATT2
parse phrase tag, path to root of parse tree
Semantic role semantic role label, semantic role labels along path to root of parse ATT1
tree
</table>
<tableCaption confidence="0.863022">
Table 2: Features used in our models
</tableCaption>
<table confidence="0.998075857142857">
Tag type Tags
time expression extraction tags B DATE, B DURATION, B SET, B TIME, I DATE,
I DURATION, I SET, I TIME, O
Event expression extraction tags B ACTION, B ASPECTUAL, B ACTION, B OCCURRENCE,
B PERCEPTION, B REPORTING, B STATE, O
Event tense FUTURE, INFINITIVE, PAST, PASTPART, PRESENT, PRES-
PART, NONE, O
Event aspect PROGRESSIVE, PREFECTIVE PROGRESSIVE, PERFEC-
TIVE, NONE, O
Event polarity NEG, POS
Event modality ’D, CAN, CLOSE, COULD, DELETE, HAVE TO, HAVE TO,
LIKELIHOOD, MAY, MIGHT, MUST, NONE, O, POSSIBLE,
POTENTIAL, SHOULD, SHOULD HAVE TO, TO, UNLIKELY,
UNTIL, WOULD, WOULD HAVE TO
</table>
<tableCaption confidence="0.999946">
Table 3: Tags assigned by our classifiers for TempEval-3 tasks A and B
</tableCaption>
<bodyText confidence="0.997752875">
Rigo and Lavelli, 2011). In particular, (Rigo and
Lavelli, 2011) also examined performance based on
different sizes of n-grams in a small scale (n=1,3).
In this work, we intended to systematically inves-
tigate the performance of various models with differ-
ent layers of representation (based on much larger
sets of rich syntactic/semantic features) as well as
additional context. For each time expression/event
segmentation/classification task, we trained twelve
models exploring these two dimensions, three of
which we submitted for TempEval-3.
Additional layers of representation We
trained three types of model: (ATT1) STAN-
FORD+SENNA, (ATT2) STANFORD and (ATT3)
WORDS ONLY. The basic features used in each
type of model are given in Table 2: ATT1 models
include lexical, syntactic and semantic features,
ATT2 models include only lexical and syntactic
features, and ATT3 models include only lexical
features. For the ATT1 models we had 18 basic
features per token, for the ATT2 models we had 16
basic features per token, and for the ATT3 models
we had one basic feature per token.
Additional context We experimented with context
windows of 0, 1, 3, and 7 words preceding and fol-
lowing the token to be labeled (i.e. window sizes of
1, 3, 7, and 15). For each window size, we trained
ATT1, ATT2 and ATT3 models. The ATT1 mod-
els had 18 basic features per token in the context
window, for up to 15 tokens, so up to 270 basic fea-
tures for each token to be labeled. The ATT2 mod-
els had 16 basic features per token in the context
</bodyText>
<page confidence="0.99586">
21
</page>
<bodyText confidence="0.999866681818182">
window, so up to 240 basic features for each token
to be labeled. The ATT3 models had 1 basic feature
per token in the context window, so up to 15 basic
features for each token to be labeled.
Model training For event extraction and classifica-
tion, time expression extraction and classification,
and event feature classification, we used the machine
learning toolkit LLAMA (Haffner, 2006). LLAMA
encodes multiclass classification problems using bi-
nary MaxEnt classifiers to increase the speed of
training and to scale the method to large data sets.
We also used a front-end to LLAMA that builds un-
igram, bigram and trigram extended features from
basic features; for example, from the basic feature
“go there today”, it would build the features “go”,
“there”, “today”, “go there”, “there today”, and “go
there today”. We grouped our basic features (see Ta-
ble 2) by type rather than by token, and the LLAMA
front-end then produced ngram features. We chose
LLAMA primarily because of the proven power
of the ngram feature-extraction front-end for NLP
tasks.
</bodyText>
<sectionHeader confidence="0.943398" genericHeader="method">
4 Event and Time Expression Extraction
</sectionHeader>
<bodyText confidence="0.999214">
For event and time expression extraction, we trained
BIO classifiers. A BIO classifier tags each input to-
ken as either Beginning, In, or Out of an event/time
expression. Our classifier for events simultaneously
assigns a B, I or O to each token, and classifies the
class of the event for tokens that Begin or are In an
event. Our time expression classifier simultaneously
assigns a B, I, or O to each token, and classifies the
type of the time expression for tokens that Begin or
are In a time expression (see Table 3).
A BIO model may sometimes be inconsistent; for
example, a token may be labeled as Inside a segment
of a particular type, while the previous token may
be labeled as Out of any segment. We considered
the two most likely labels for each token (as long as
each had likelihood at least 0.9), choosing the one
most consistent with the context.
</bodyText>
<sectionHeader confidence="0.989137" genericHeader="method">
5 Event Feature Classification
</sectionHeader>
<bodyText confidence="0.999627733333334">
We determined the event features for each extracted
event using four additional classifiers, one each for
tense, aspect, polarity and modality. These classi-
fiers were trained only on tokens identified as part of
event expressions. Since the event expressions were
single words for all but a few (erroneous) cases in the
silver data, for determining the event features, we
used the same features as before, with the single ad-
dition of the event class (during testing, we used the
dynamically assigned event class from the event seg-
mentation classifier). As before, we experimented
with ATT1, ATT2, and ATT3 models. TempEval-
3 only includes evaluation of tense and aspect fea-
tures, so we only report for those. The tags assigned
by each classifier are listed in Table 3.
</bodyText>
<sectionHeader confidence="0.994401" genericHeader="method">
6 Time Normalization
</sectionHeader>
<bodyText confidence="0.999893">
To compute TIMEX3 standard based values for
extracted time expressions, we used the TIMEN
(Llorens et al., 2012) and TRIOS (UzZaman and
Allen, 2010) time normalizers. Values from the
normalizers were validated in post-processing (e.g.
“T2445” is invalid) and, when the normalizers re-
turned different non-nil values, TIMEN’s values
were selected without further reasoning. Time nor-
malization was out of scope in our research for this
evaluation, but it remains as part of our future work.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999927526315789">
Our results for event segmentation/classification on
the TempEval-3 test data are provided in Table 4.
The absence of semantic features causes only small
changes in F1. The absence of syntactic features
causes F1 to drop slightly (less than 2.5% for all
but the smallest window size), with recall decreasing
while precision improves somewhat. Attribute F1 is
also impacted minimally by the absence of semantic
features, and about 2-5% by the absence of syntactic
features for all but the smallest window size.1
Our results for time expression extraction and
classification on the TempEval-3 test data are pro-
vided in Table 5. Here, the performance drops more
in the absence of semantic and syntactic features;
however, there is an interaction between length of
time expression and performance drop which we
may be able to ameliorate in future work by han-
dling consistency issues in the BIO time expression
extraction model better.
</bodyText>
<footnote confidence="0.936936333333333">
1In Tables 4 and 5, we present results that are slightly dif-
ferent from our submission due to a minor fix in our models by
removing some redundant feature values used twice.
</footnote>
<page confidence="0.990455">
22
</page>
<table confidence="0.999259384615385">
Features Window size F1 P R Class Tense Aspect
STANFORD+SENNA 15 (ATT1) 81.16 81.49 80.83 71.60 59.62 73.76
7 81.08 81.74 80.43 71.49 59.05 73.78
3 80.35 81.23 79.49 71.41 58.67 73.17
1 80.94 80.77 81.10 72.37 58.06 73.71
STANFORD 15 (ATT2) 80.86 81.02 80.70 71.05 59.10 73.34
7 81.30 81.90 80.70 71.57 59.01 74.14
3 80.87 81.58 80.16 71.94 58.96 73.70
1 80.78 80.72 80.83 71.80 57.47 73.41
WORDS ONLY 15 (ATT3) 78.58 81.95 75.47 69.5 55.27 70.76
7 78.40 82.21 74.93 69.14 55.54 70.27
3 78.14 82.44 74.26 69.39 52.75 70.38
1 73.55 79.78 68.23 66.33 44.94 63.15
</table>
<tableCaption confidence="0.993457">
Table 4: Event extraction results (F1, P and R, strict match); feature classification results (attribute F1)
</tableCaption>
<table confidence="0.911151692307692">
Features Window size F1 P R Type Value
STANFORD+SENNA 15 (ATT1) 80.17 (85.95) 93.27 (100) 70.29 (75.36) 77.69 65.29
7 76.99 (83.68) 91.09 (99.01) 66.67 (72.46) 75.31 64.44
3 75.52 (83.82) 88.35 (98.06) 65.94 (73.19) 75.52 63.07
1 66.12 (83.27) 75.70 (95.33) 58.70 (73.91) 72.65 59.59
STANFORD 15 (ATT2) 78.69 (85.25) 90.57 (98.11) 69.57 (75.36) 76.23 65.57
7 78.51 (84.30) 91.35 (98.08) 68.84 (73.91) 76.03 63.64
3 78.19 (84.77) 90.48 (98.10) 68.84 (74.64) 75.72 64.20
1 67.48 (83.74) 76.85 (95.37) 60.14 (74.64) 73.17 59.35
WORDS ONLY 15 (ATT3) 72.34 (80.85) 87.63 (97.94) 61.59 (68.84) 74.04 60.43
7 72.34 (80.85) 87.63 (97.94) 61.59 (67.84) 74.04 59.57
3 74.48 (82.85) 88.12 (98.02) 64.49 (71.74) 75.31 61.09
1 44.62 (82.87) 49.56 (92.04) 40.58 (75.36) 70.92 39.84
</table>
<tableCaption confidence="0.8621335">
Table 5: Time expression extraction results (F1, P and R, strict match with relaxed match in parentheses); attribute F1
for type and value features
</tableCaption>
<bodyText confidence="0.999963272727273">
A somewhat surprising finding is that both event
and time expression extraction are subject to rela-
tively tight constraints from the lexical context. We
were surprised by how well the ATT3 (WORDS
ONLY) models performed, especially in terms of
precision. We were also surprised that the words
only models with window sizes of 3 and 7 performed
as well as the models with a window size of 15. We
think these results are promising for “big data” text
analytics, where there may not be time to do heavy
preprocessing of input text or to train large models.
</bodyText>
<sectionHeader confidence="0.99953" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999792444444444">
For us, participation in TempEval-3 is a first step
in developing a temporal understanding component
for text analytics and virtual agents. We now in-
tend to appy our best performing models to this task.
In future work, we plan to evaluate our initial re-
sults with larger data sets (e.g., cross validation on
the tempeval training data) and experiment with hy-
brid/ensemble methods for performing time expres-
sion and temporal link extraction.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999558666666667">
We thank Srinivas Bangalore, Patrick Haffner, and
Sumit Chopra for helpful discussions and for sup-
plying LLAMA and its front-end for our use.
</bodyText>
<page confidence="0.997442">
23
</page>
<sectionHeader confidence="0.982042" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987065804878049">
S. F. Adafre and M. de Rijke. 2005. Feature engineering
and post-processing for temporal expression recogni-
tion using conditional random fields. In Proceedings
of the ACL Workshop on Feature Engineering for Ma-
chine Learning in Natural Language Processing.
G. Angeli, C. D. Manning, and D. Jurafsky. 2012. Pars-
ing time: Learning to interpret time expressions. In
Proceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (HLT-NAACL).
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(3–4).
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem
(English and Spanish): Evaluating CRFs and semantic
roles in TempEval-2. In Proceedings of the Interna-
tional Workshop on Semantic Evaluation (SemEval).
H. Llorens, L. Derczynski, R. Gaizauskas, and E. Sa-
quete. 2012. Timen: An open temporal expression
normalisation resource. In Proceedings of the Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
S. Rigo and A. Lavelli. 2011. Multisex - a multi-
language timex sequential extractor. In Proceedings
of Temporal Representation and Reasoning (TIME).
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP. http://nlp.stanford.
edu/software/corenlp.shtml.
N. UzZaman and J. F. Allen. 2010. TRIPS and TRIOS
system for TempEval-2: Extracting temporal informa-
tion from text. In Proceedings of the International
Workshop on Semantic Evaluation (SemEval).
N. UzZaman, H. Llorens, J. Allen, L. Derczynski,
M. Verhagen, and J. Pustejovsky. 2012. Tempeval-
3: Evaluating events, time expressions, and tempo-
ral relations. http://arxiv.org/abs/1206.
5333v1.
</reference>
<page confidence="0.999168">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909432">
<title confidence="0.9795705">ATT1: Temporal Annotation Using Big Windows and Rich Syntactic and Semantic Features</title>
<author confidence="0.977051">Jung</author>
<affiliation confidence="0.975871">AT&amp;T Labs -</affiliation>
<address confidence="0.9976025">180 Park Florham Park, NJ 07932,</address>
<email confidence="0.998075">hjung,stent@research.att.com</email>
<abstract confidence="0.9991088">In this paper we present the results of experiments comparing (a) rich syntactic and semantic feature sets and (b) big context windows, for the TempEval time expression and event segmentation and classification tasks. We show that it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S F Adafre</author>
<author>M de Rijke</author>
</authors>
<title>Feature engineering and post-processing for temporal expression recognition using conditional random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing.</booktitle>
<marker>Adafre, de Rijke, 2005</marker>
<rawString>S. F. Adafre and M. de Rijke. 2005. Feature engineering and post-processing for temporal expression recognition using conditional random fields. In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Angeli</author>
<author>C D Manning</author>
<author>D Jurafsky</author>
</authors>
<title>Parsing time: Learning to interpret time expressions.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL).</booktitle>
<contexts>
<context position="2786" citStr="Angeli et al., 2012" startWordPosition="451" endWordPosition="454">rt et al., 2011) opensource NLP tools. From the Stanford CoreNLP tools we obtained a tokenization of the input text, the lemma and part of speech (POS) tag for each token, and dependency and constituency parses for each sentence. From SENNA, we obtained a semantic role labelling for each sentence. 3 Approach We were curious to explore the tradeoff between additional context on the one hand, and additional layers of representation on the other, for the event and time expression extraction tasks. Researchers have investigated the impacts of different sets of features (Adafre and de Rijke, 2005; Angeli et al., 2012; Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 20–24, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Feature type Features Used in Lexical 1 token ATT2, ATT3 Lexical 2 lemma ATT1, ATT2 Part of speech POS tag ATT1, ATT2 Dependency governing verb, governing verb POS, governing preposition, ATT1, ATT2 phrase tag, path to root of parse tree, head word, head word lemma, head word POS Constituency governing verb, governing verb POS, governing prepositi</context>
</contexts>
<marker>Angeli, Manning, Jurafsky, 2012</marker>
<rawString>G. Angeli, C. D. Manning, and D. Jurafsky. 2012. Parsing time: Learning to interpret time expressions. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<contexts>
<context position="2183" citStr="Collobert et al., 2011" startWordPosition="350" endWordPosition="353"> gold 183 6698 1243 TE3-Silver silver 2452 81329 12739 Table 1: Frequency of event and time expressions in the text portions of the TempEval-3 data sets 2010; UzZaman and Allen, 2010)). In this work, we show that with large windows of context, it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets. 2 Data Using the gold and silver data distributed by the TempEval-3 task organizers (see Table 1), we processed each input file with the Stanford CoreNLP (Stanford Natural Language Processing Group, 2012) and SENNA (Collobert et al., 2011) opensource NLP tools. From the Stanford CoreNLP tools we obtained a tokenization of the input text, the lemma and part of speech (POS) tag for each token, and dependency and constituency parses for each sentence. From SENNA, we obtained a semantic role labelling for each sentence. 3 Approach We were curious to explore the tradeoff between additional context on the one hand, and additional layers of representation on the other, for the event and time expression extraction tasks. Researchers have investigated the impacts of different sets of features (Adafre and de Rijke, 2005; Angeli et al., 2</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Haffner</author>
</authors>
<title>Scaling large margin classifiers for spoken language understanding.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<pages>48--3</pages>
<contexts>
<context position="6156" citStr="Haffner, 2006" startWordPosition="1006" endWordPosition="1007">T3 models. The ATT1 models had 18 basic features per token in the context window, for up to 15 tokens, so up to 270 basic features for each token to be labeled. The ATT2 models had 16 basic features per token in the context 21 window, so up to 240 basic features for each token to be labeled. The ATT3 models had 1 basic feature per token in the context window, so up to 15 basic features for each token to be labeled. Model training For event extraction and classification, time expression extraction and classification, and event feature classification, we used the machine learning toolkit LLAMA (Haffner, 2006). LLAMA encodes multiclass classification problems using binary MaxEnt classifiers to increase the speed of training and to scale the method to large data sets. We also used a front-end to LLAMA that builds unigram, bigram and trigram extended features from basic features; for example, from the basic feature “go there today”, it would build the features “go”, “there”, “today”, “go there”, “there today”, and “go there today”. We grouped our basic features (see Table 2) by type rather than by token, and the LLAMA front-end then produced ngram features. We chose LLAMA primarily because of the pro</context>
</contexts>
<marker>Haffner, 2006</marker>
<rawString>P. Haffner. 2006. Scaling large margin classifiers for spoken language understanding. Speech Communication, 48(3–4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Llorens</author>
<author>E Saquete</author>
<author>B Navarro</author>
</authors>
<title>TIPSem (English and Spanish): Evaluating CRFs and semantic roles in TempEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</booktitle>
<marker>Llorens, Saquete, Navarro, 2010</marker>
<rawString>H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem (English and Spanish): Evaluating CRFs and semantic roles in TempEval-2. In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Llorens</author>
<author>L Derczynski</author>
<author>R Gaizauskas</author>
<author>E Saquete</author>
</authors>
<title>Timen: An open temporal expression normalisation resource.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="8652" citStr="Llorens et al., 2012" startWordPosition="1425" endWordPosition="1428"> all but a few (erroneous) cases in the silver data, for determining the event features, we used the same features as before, with the single addition of the event class (during testing, we used the dynamically assigned event class from the event segmentation classifier). As before, we experimented with ATT1, ATT2, and ATT3 models. TempEval3 only includes evaluation of tense and aspect features, so we only report for those. The tags assigned by each classifier are listed in Table 3. 6 Time Normalization To compute TIMEX3 standard based values for extracted time expressions, we used the TIMEN (Llorens et al., 2012) and TRIOS (UzZaman and Allen, 2010) time normalizers. Values from the normalizers were validated in post-processing (e.g. “T2445” is invalid) and, when the normalizers returned different non-nil values, TIMEN’s values were selected without further reasoning. Time normalization was out of scope in our research for this evaluation, but it remains as part of our future work. 7 Results and Discussion Our results for event segmentation/classification on the TempEval-3 test data are provided in Table 4. The absence of semantic features causes only small changes in F1. The absence of syntactic featu</context>
</contexts>
<marker>Llorens, Derczynski, Gaizauskas, Saquete, 2012</marker>
<rawString>H. Llorens, L. Derczynski, R. Gaizauskas, and E. Saquete. 2012. Timen: An open temporal expression normalisation resource. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rigo</author>
<author>A Lavelli</author>
</authors>
<title>Multisex - a multilanguage timex sequential extractor.</title>
<date>2011</date>
<booktitle>In Proceedings of Temporal Representation and Reasoning (TIME).</booktitle>
<contexts>
<context position="4266" citStr="Rigo and Lavelli, 2011" startWordPosition="686" endWordPosition="689">B SET, B TIME, I DATE, I DURATION, I SET, I TIME, O Event expression extraction tags B ACTION, B ASPECTUAL, B ACTION, B OCCURRENCE, B PERCEPTION, B REPORTING, B STATE, O Event tense FUTURE, INFINITIVE, PAST, PASTPART, PRESENT, PRESPART, NONE, O Event aspect PROGRESSIVE, PREFECTIVE PROGRESSIVE, PERFECTIVE, NONE, O Event polarity NEG, POS Event modality ’D, CAN, CLOSE, COULD, DELETE, HAVE TO, HAVE TO, LIKELIHOOD, MAY, MIGHT, MUST, NONE, O, POSSIBLE, POTENTIAL, SHOULD, SHOULD HAVE TO, TO, UNLIKELY, UNTIL, WOULD, WOULD HAVE TO Table 3: Tags assigned by our classifiers for TempEval-3 tasks A and B Rigo and Lavelli, 2011). In particular, (Rigo and Lavelli, 2011) also examined performance based on different sizes of n-grams in a small scale (n=1,3). In this work, we intended to systematically investigate the performance of various models with different layers of representation (based on much larger sets of rich syntactic/semantic features) as well as additional context. For each time expression/event segmentation/classification task, we trained twelve models exploring these two dimensions, three of which we submitted for TempEval-3. Additional layers of representation We trained three types of model: (ATT1) STA</context>
</contexts>
<marker>Rigo, Lavelli, 2011</marker>
<rawString>S. Rigo and A. Lavelli. 2011. Multisex - a multilanguage timex sequential extractor. In Proceedings of Temporal Representation and Reasoning (TIME).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanford</author>
</authors>
<title>Natural Language Processing Group.</title>
<date>2012</date>
<note>Stanford CoreNLP. http://nlp.stanford. edu/software/corenlp.shtml.</note>
<marker>Stanford, 2012</marker>
<rawString>Stanford Natural Language Processing Group. 2012. Stanford CoreNLP. http://nlp.stanford. edu/software/corenlp.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N UzZaman</author>
<author>J F Allen</author>
</authors>
<title>TRIPS and TRIOS system for TempEval-2: Extracting temporal information from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="1743" citStr="UzZaman and Allen, 2010" startWordPosition="275" endWordPosition="278">cutive sentences, events and time expressions in the same sentence, and events and the document creation time of the input text. Here we report results for the first two tasks. Previous TempEval competitions have shown that rich syntactic and semantic feature sets can lead to good performance on event and time expression extraction and classification tasks (e.g. (Llorens et al., 20 Type Files EVENT TIMEX AQUAINT gold 73 4431 579 TimeBank gold 183 6698 1243 TE3-Silver silver 2452 81329 12739 Table 1: Frequency of event and time expressions in the text portions of the TempEval-3 data sets 2010; UzZaman and Allen, 2010)). In this work, we show that with large windows of context, it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets. 2 Data Using the gold and silver data distributed by the TempEval-3 task organizers (see Table 1), we processed each input file with the Stanford CoreNLP (Stanford Natural Language Processing Group, 2012) and SENNA (Collobert et al., 2011) opensource NLP tools. From the Stanford CoreNLP tools we obtained a tokenization of the input text, the lemma and part of speech (POS) tag for each token, and </context>
<context position="8688" citStr="UzZaman and Allen, 2010" startWordPosition="1431" endWordPosition="1434">in the silver data, for determining the event features, we used the same features as before, with the single addition of the event class (during testing, we used the dynamically assigned event class from the event segmentation classifier). As before, we experimented with ATT1, ATT2, and ATT3 models. TempEval3 only includes evaluation of tense and aspect features, so we only report for those. The tags assigned by each classifier are listed in Table 3. 6 Time Normalization To compute TIMEX3 standard based values for extracted time expressions, we used the TIMEN (Llorens et al., 2012) and TRIOS (UzZaman and Allen, 2010) time normalizers. Values from the normalizers were validated in post-processing (e.g. “T2445” is invalid) and, when the normalizers returned different non-nil values, TIMEN’s values were selected without further reasoning. Time normalization was out of scope in our research for this evaluation, but it remains as part of our future work. 7 Results and Discussion Our results for event segmentation/classification on the TempEval-3 test data are provided in Table 4. The absence of semantic features causes only small changes in F1. The absence of syntactic features causes F1 to drop slightly (less</context>
</contexts>
<marker>UzZaman, Allen, 2010</marker>
<rawString>N. UzZaman and J. F. Allen. 2010. TRIPS and TRIOS system for TempEval-2: Extracting temporal information from text. In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N UzZaman</author>
<author>H Llorens</author>
<author>J Allen</author>
<author>L Derczynski</author>
<author>M Verhagen</author>
<author>J Pustejovsky</author>
</authors>
<title>Tempeval3: Evaluating events, time expressions, and temporal relations.</title>
<date>2012</date>
<note>http://arxiv.org/abs/1206. 5333v1.</note>
<contexts>
<context position="662" citStr="UzZaman et al., 2012" startWordPosition="101" endWordPosition="104">ws and Rich Syntactic and Semantic Features Hyuckchul Jung and Amanda Stent AT&amp;T Labs - Research 180 Park Ave Florham Park, NJ 07932, USA hjung, stent@research.att.com Abstract In this paper we present the results of experiments comparing (a) rich syntactic and semantic feature sets and (b) big context windows, for the TempEval time expression and event segmentation and classification tasks. We show that it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets. 1 Introduction TempEval-3 Temporal Annotation Task (UzZaman et al., 2012) has three subtasks: A Time expression extraction and classification - extract time expressions from input text, and determine the type and normalised value for each extracted time expression. B Event extraction and classification - extract event mentions from input text, and determine the class, tense and aspect features for each extracted event. C Temporal link identification - identify and categorise temporal links between events in the same or consecutive sentences, events and time expressions in the same sentence, and events and the document creation time of the input text. Here we report</context>
</contexts>
<marker>UzZaman, Llorens, Allen, Derczynski, Verhagen, Pustejovsky, 2012</marker>
<rawString>N. UzZaman, H. Llorens, J. Allen, L. Derczynski, M. Verhagen, and J. Pustejovsky. 2012. Tempeval3: Evaluating events, time expressions, and temporal relations. http://arxiv.org/abs/1206. 5333v1.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>