<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004442">
<title confidence="0.997617">
UIR-PKU: Twitter-OpinMiner System for Sentiment
Analysis in Twitter at SemEval 2015
</title>
<author confidence="0.997491">
Xu Han2,4, Binyang Li1*, Jing Ma2, Yuxiao Zhang3, Gaoyan Ou3,
Tengjiao Wang3, Kam-fai Wong2,4,5
</author>
<affiliation confidence="0.99777425">
1School of Information and Technology, University of Information Relations, Beijing
2Dept. of Sys. Engineering&amp;Engineering Management, The Chinese University of Hong Kong
3School of Information Science, Peking University, Beijing
4Shenzhen Research Institute, The Chinese University of Hong Kong
</affiliation>
<address confidence="0.560419">
5MoE Key Laboratory of High Confidence Software Technologies, China
</address>
<email confidence="0.8973105">
{xhan, jma, kfwong}@cuhk.edu.hk; byli@uir.cn; {yxzhang, gyou,
tjwang}@pku.edu.cn
</email>
<sectionHeader confidence="0.994267" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99911395">
Microblogs are considered as We-Media infor-
mation with many real-time opinions. This paper
presents a Twitter-OpinMiner system for Twitter
sentiment analysis evaluation at SemEval 2015.
Our approach stems from two different angles:
topic detection for discovering the sentiment distri-
bution on different topics and sentiment analysis
based on a variety of features. Moreover, we also
implemented intra-sentence discourse relations for
polarity identification. We divided the discourse re-
lations into 4 predefined categories, including con-
tinuation, contrast, condition, and cause. These
relations could facilitate us to eliminate polarity
ambiguities in compound sentences where both
positive and negative sentiments are appearing.
Based on the SemEval 2014 and SemEval 2015
Twitter sentiment analysis task datasets, the exper-
imental results show that the performance of Twit-
ter-OpinMiner could effectively recognize
opinionated messages and identify the polarities.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.960376285714286">
This year comes the third edition of SemEval Twit-
ter sentiment analysis task consisting of new genres,
including topic-based polarity classification, trends
detection towards a topic, and the sentimental
strength of association of terms (Nakov et al.,
2013).
*Corresponding author
We only participated in the subtask of message
sentiment analysis and built up a system, named
Twitter-OpinMiner for the task. Twitter-
OpinMiner stems from two different angles: LDA-
based topic detection for discovering the opinion-
ated features of trending tweets’ topics and senti-
ment analysis based on a variety of features.
</bodyText>
<listItem confidence="0.873892">
• Topic detection
</listItem>
<bodyText confidence="0.999881555555556">
Recent studies show that people often search
Twitter to find temporally relevant information
(Teevan et al., 2011), such as emergent events,
trending topics. In fact, similar opinions were
likely to express on the same topic/event in Twitter.
For example, there are 20 tweets expressing similar
opinions on “Blood moon” in SemEval 2015 da-
taset. Therefore, it can facilitate us to discover the
sentiment distribution on different topics.
</bodyText>
<subsectionHeader confidence="0.76138">
• Sentiment analysis
</subsectionHeader>
<bodyText confidence="0.999430230769231">
Unlike traditional news content, tweets are spe-
cialists in short texts with long compound sen-
tences, and a number of irregular expressions,
including emoticon, hashtag, and special punctua-
tions. In order to better support tweets analysis, we
extract features from following aspects: textual
content, irregular expression, discourse relations,
and word embedding. Then we introduce above
features into a SVM classifier for sentiment analy-
sis.
This paper is organized as follows. Section 2 de-
scribes the framework of our system. Section 3 in-
troduces the details of our feature extraction. We
</bodyText>
<page confidence="0.976016">
664
</page>
<note confidence="0.7517525">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 664–668,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999581">
Figure 1. System architecture.
</figureCaption>
<bodyText confidence="0.995667">
present the evaluation results in Section 4. Finally,
Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.922047" genericHeader="method">
2 System Overview
</sectionHeader>
<subsectionHeader confidence="0.986444">
2.1 Architecture
</subsectionHeader>
<bodyText confidence="0.999385">
The architecture of Twitter-OpinMiner is described
in Figure 1. Twitter-OpinMiner system is com-
prised of three modules:
</bodyText>
<listItem confidence="0.916838">
(1) Pre-processing module: reads all data of training
data and test data. It performs, POS tagging, named
entity recognition, and semantic role labeling.
(2) Feature extraction module: extracts the features
including formal text features, tweet-specific fea-
tures, discourse features, sentiment distribution
among topics, and word embedding.
(3) Sentiment analysis module: creates a SVM clas-
sifier that incorporates the above features classify
the polarity of each tweet.
</listItem>
<bodyText confidence="0.888949">
Finally, Twitter-OpinMiner outputs the polarity
of each tweet.
</bodyText>
<subsectionHeader confidence="0.99994">
2.2 Development Data and Lexicon
</subsectionHeader>
<bodyText confidence="0.999841555555556">
The development data are necessary in our system.
We fully utilize the training tweets provided by
SemEval 2013. The dataset consists of 9,912 anno-
tated tweets.
Besides, for sentiment analysis, we also utilize
several sentiment lexicons, including Liu’s senti-
ment lexicon (Liu, 2012), MPQA subjectivity lexi-
con (Wilson et al., 2005), and the sentiment lexicon
generated from tweets (Mohammad et al., 2013).
</bodyText>
<tableCaption confidence="0.976742">
Table 1. Features of text in our system.
</tableCaption>
<bodyText confidence="0.968771">
Word-Level and entity-level features
The presence of sentiment word
The ratio of sentiment word in a sentence
The total number of positive words
The total number of negative words
The presence of negation words
The total number of the word in all-caps
</bodyText>
<table confidence="0.963196666666667">
Bi-gram features
Named entities + opinion operators
Pronouns + opinion operators
Nouns or named entities + opinion words
Pronouns + opinion words
Opinion words (adjective) + (noun)
</table>
<sectionHeader confidence="0.979611" genericHeader="method">
3 Feature Extraction
</sectionHeader>
<bodyText confidence="0.999945083333333">
The objective of this task is to determine whether a
given message is positive, negative, or neutral. We
train sentiment classifiers with LibLinear (Fan et
al., 2008) on the training set and dev set, and tune
parameter −c, −wi of SVM on the test set of
SemEval 2013. SVM is a popular machine learning
algorithm, the effectiveness of which has been
proved in sentiment analysis on formal texts in re-
lated work (Pang and Lee, 2002; Liu, 2012). Since
the performance of SVM classifier will be greatly
influenced by the features selection, we explore a
variety of features in the evaluation.
</bodyText>
<subsectionHeader confidence="0.998915">
3.1 Features of topical sentiment distribution
</subsectionHeader>
<bodyText confidence="0.999995833333333">
The advancement of Twitter is fast response to the
real world, so people often search Twitter to find
temporally relevant information, such as emergent
events, trending topics. In fact, tweets are likely to
converge on some opinions for a specific topic,
which will lead to different sentiment distributions
among topics.
In our system, we adopt LDA-based approach
for representing the typical sentiment distribution
features. We use the Mallet toolkit, set the topic
number as 50, and map each tweet into 50 dimen-
sions to extract those features.
</bodyText>
<subsectionHeader confidence="0.999713">
3.2 Features of formal text
</subsectionHeader>
<bodyText confidence="0.997749166666667">
Although the task is to analyze sentiment in Twitter,
much research proved the effectiveness of the clas-
sic features of formal texts on tweets. The features
we adopted in this task are partly the same with
(Zhou et al., 2010) and listed in Table 1, and two
types of features are incorporated in the classifier.
</bodyText>
<figure confidence="0.990878875">
Topical sentiment distribution
Preprocessing
Formal text
feature
Word em-
bedding
Tweets
Twitter-spe-
cific feature
Discourse
relations
Feature ex-
traction
Sentiment
analysis
Results
</figure>
<page confidence="0.986404">
665
</page>
<bodyText confidence="0.999612">
These features are also integrated into our SVM
classifier for training and treated as the baseline in
our experiment.
</bodyText>
<subsectionHeader confidence="0.990779">
3.3 Twitter specific feature
</subsectionHeader>
<bodyText confidence="0.999941571428572">
Unlike formal texts, tweet has its own characteris-
tics, including irregular expressions, emoticon,
hashtag, ill format, and special punctuations. In our
system, we combine the features proposed by Mo-
hammad et al. (2013) with some new features as
Twitter-specific features for supplementary to the
forma text.
</bodyText>
<listItem confidence="0.997469">
• Hashtags: the number of hashtags in one tweet;
• Ill format: the presence of ill format with some
characters replacing by *, for example, f**k;
• Punctuation: the number of contiguous se-
quences of exclamation marks, question marks,
and both exclamation and question marks;
whether the last token contains an exclamation
or question mark;
• Emoticons: the presence of positive and nega-
tive emoticons at any position in the tweet;
whether the last token is an emoticon;
• OOV: the ratio of words out of vocabulary;
• Elongated words: the presence of sentiment
words with one character repeated more than
two times, for example, ‘cooool’;
• URL: whether the tweet contains a URL.
• Reply or Retweet: Is the current tweet a re-
ply/retweet tweet
</listItem>
<subsectionHeader confidence="0.989132">
3.4 Word embedding
</subsectionHeader>
<bodyText confidence="0.999984666666667">
We also utilize word embedding technique for fea-
ture extraction. We adopt sentiment-specific word
embedding method (Tang et al., 2014) that could
encode sentiment information in the continuous
representation of words. In our approach, each term
is extended into a 150 dimensional vector.
</bodyText>
<subsectionHeader confidence="0.946392">
3.5 Discourse specific feature
</subsectionHeader>
<bodyText confidence="0.876238714285714">
Since tweets are usually expressed informally, there
are many compound sentences in a tweet, which al-
ways contain positive sentiment and negative senti-
ment with ambiguity. For example,
It may not be the biggest squad in the last 10yrs, but
Ancelotti is working for quality over quantity. Eve-
ryone... http://t.co/oCdPXQWggT.
</bodyText>
<tableCaption confidence="0.998316">
Table 2. Examples of cue-phrases.
</tableCaption>
<table confidence="0.934053">
Relation Cue Phrases
Contrast although, but, however, though
Condition if, despite, in case of
Continuation and, moreover, not only but
also
Cause because, so that, due to, in or-
der that
</table>
<bodyText confidence="0.999906375">
In this case, there are two segments in the tweet
that holds a Contrast discourse relation, and the po-
larity is determined by “but” segment. In our sys-
tem, we also take into consideration of intra-
sentence discourse relation features for processing
compound sentences.
Mann and Thompson (1988) defined a complete
discourse scheme Rhetorical Structure Theory
(RST). Since not all of the discourse relations in
RST would help eliminate polarity ambiguities, the
discourse relations were implemented in our sys-
tem was on a subset (Zhou et al., 2011).
In our system, we use cue-phrase based method for
discourse relation identification. We maintain a cue
phrase lexicon and the examples of the cue phrases
were shown in Table 2.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.99993056">
We trained a SVM classifier on 9,912 annotated
tweets (8,258 in the training set and 1,654 in the
development set). We used the same evaluation
metrics with SemEval 2013, including the macro-
averaged F-score of the positive and negative clas-
ses. The experimental results obtained by our sys-
tem on the training set (ten-fold cross validation),
development set, and test sets on Twitter 2013 were
shown in Table 3 where the baseline was achieved
by using the formal text features as well as twitter-
specific features. Since the effectiveness of these
two types of features were analyzed in (Moham-
mad et al., 2013), we mainly evaluated the effec-
tiveness of other features.
Table 3 showed that the most effective feature
on Twitter 2013 dataset turned out to be the word
embedding features: they provided gains of about
7%. For LDA, we set the numbers of topic from 10
to 100, and found it could achieve best perfor-
mance when equaling 50. We then constructed the
sentiment distribution among 50 topics for the fur-
ther evaluation.
Besides, we also investigated the effectiveness
of discourse features on compound sentences, and
the statistics were shown in Table 6.
</bodyText>
<page confidence="0.99905">
666
</page>
<tableCaption confidence="0.999332">
Table 3. Experimental results on Twitter 2013 dataset.
</tableCaption>
<table confidence="0.999826285714286">
Approaches Metrics
pos-P pos-R pos-F neg-P neg-R neg-F ave-F
Baseline (BL) 0.743 0.673 0.706 0.451 0.679 0.542 0.624
BL+LDA 0.752 0.679 0.714 0.465 0.707 0.561 0.634
BL+Word Embedding 0.772 0.685 0.724 0.561 0.798 0.659 0.692
BL+Discourse Relation 0.756 0.680 0.716 0.467 0.705 0.562 0.635
BL+All 0.791 0.704 0.745 0.563 0.809 0.664 0.704
</table>
<tableCaption confidence="0.998169">
Table 4. Experimental results of 2015 test.
</tableCaption>
<table confidence="0.99953925">
Method Metrics
pos-P pos-R pos-F neg-P neg-R neg-F ave-F
UIR-PKU 0.7518 0.6098 0.6734 0.4636 0.6110 0.5272 0.6003
Best run 0.7702 0.6975 0.7321 0.5171 0.6219 0.5647 0.6484
</table>
<tableCaption confidence="0.995074">
Table 5. Experimental results of progress test on Average F-value.
</tableCaption>
<table confidence="0.987330230769231">
Approaches Corpus
Live Journal 2014 SMS 2013 Twitter 2014 Twitter 2014
Sarcasm
UIR-PKU 0.7044 0.6741 0.6718 0.5258
Best run 0.7534 0.6716 0.7448 0.4286
Table 6. Distribution of discourse relations and
the contribution in the evaluation.
Discourse Occurrence Contribution
Relation
Cause 26.9% 33.9%
Condition 12.6% 22.1%
Contrast 18.2% 10.1%
Continuation 42.3% 33.9%
</table>
<bodyText confidence="0.99996425">
By adopting discourse features, around 59% sen-
tences with discourse relations were identified.
Among these four types of relations, better perfor-
mance were achieved on cause and condition rela-
tions. Especially for the sentences with condition
relation, they were all classified correctly. It is be-
cause that more cue-phrase of cause and condition
relations were used to explicitly denote the dis-
course relations in tweets, but more likely use con-
text to imply contrast and continuation relations.
Table 4 and Table 5 showed the evaluation re-
sults in SemEval 2015 Task 10. Compared with the
best run in Table 5, our system achieved comparable
results on Twitter sentiment analysis and better per-
formance on the evaluation of sarcasm. In fact,
many sarcasm are likely expressed in ironic, hence
most feature types are ineffective for this case. In
our system, we also used the features of topical sen-
timent distribution, which assumed the polarity of
sarcasm tweet the same with non-sarcasm tweets.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999997933333333">
We describe our Twitter-OpinMiner systems for
participating in SemEval 2015 sentiment analysis
in Twitter. Our approach stems the features from
two different aspects: topical sentiment distribution
and a variety of short text based features. In our pa-
per, we also implemented intra-sentence discourse
relations for polarity identification in compound
sentences where both positive and negative senti-
ments are appearing. In this way, the polarity ambi-
guities will be eliminated. Based on SemEval 2015
and SemEval 2014 datasets for Twitter sentiment
analysis task, we examined the performance of
Twitter-OpinMiner, which could achieved compa-
rable results on recognizing opinionated messages
and identifying the polarities.
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.482008142857143">
This research is partially supported by Fundamental Re-
search Funds for the Central Universities (3262014T75,
3262015T20), Shenzhen Fundamental Research Pro-
gram (JCYJ20130401172046450), General Research
Fund of Hong Kong (417112). We also thank Liyu Chen,
Jianxiong Wu, and anonymous reviewers for their help-
ful comments.
</bodyText>
<page confidence="0.996959">
667
</page>
<sectionHeader confidence="0.972229" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997424155172414">
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. Journal of
Machine Learning Research, 9: 1871-1874.
Bing Liu, Mingqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: Analyzing and Comparing Opin-
ions on the Web. In Proceedings of the 14th interna-
tional conference on World Wide Web, pages 342-351,
Chiba, Japan, ACM.
Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Lcanguage Tehnologies,
5(1): 1-167.
William C. Mann and Sandra A. Thompson. 1988. Rhe-
torical structure theory: Toward a functional theory of
text organization. Text-Interdisciplinary Journal for
the Study of Discourse, 8(3): 243-281.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-art
in sentiment analysis of tweets. In Proceedings of the
2nd Joint Conference on Lexical and Computational
Semantics.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Workshop
on Semantic Evaluation, volume 13.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification Using Ma-
chine Learning Techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79-86.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-spe-
cific word embedding for twitter sentiment classifi-
cation. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics,
pages 1555-1565.
Jaime Teevan, Daniel Ramage, and Merredith R.
Morris. 2011. #TwitteSearch: A Comparison of
Microblog Search and Web Search. In Proceedings
of the 4th ACM International Conference on Web
Search and Data Mining, pages 35-44.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the In-
ternational Conference on Human Language Tech-
nology and Empirical Methods in Natural
Language Processing, pages 347-354.
Lanjun Zhou, Yunqing Xia, Binyang Li, and Kam-
fai Wong. 2010. WIA-Opinmine System in
NTCIR-8 MOAT Evaluation. In the 8th NTCIR
Workshop Meeting, pages 286-292.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,
and Kam-fai Wong. 2011. Unsupervised discovery
of discourse relations for eliminating intra-sentence
polarity ambiguities. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 162-171.
</reference>
<page confidence="0.996535">
668
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.077769">
<title confidence="0.720466333333333">UIR-PKU: Twitter-OpinMiner System for Analysis in Twitter at SemEval 2015 Binyang Jing Yuxiao Gaoyan</title>
<author confidence="0.958325">Kam-fai</author>
<affiliation confidence="0.8191032">of Information and Technology, University of Information Relations, of Sys. Engineering&amp;Engineering Management, The Chinese University of Hong of Information Science, Peking University, Beijing Research Institute, The Chinese University of Hong Kong Key Laboratory of High Confidence Software Technologies,</affiliation>
<email confidence="0.571199">xhan@pku.edu.cn</email>
<email confidence="0.571199">jma@pku.edu.cn</email>
<email confidence="0.571199">kfwong}@cuhk.edu.hk;byli@uir.cn;{yxzhang@pku.edu.cn</email>
<email confidence="0.571199">tjwang@pku.edu.cn</email>
<abstract confidence="0.997252666666667">Microblogs are considered as We-Media information with many real-time opinions. This paper presents a Twitter-OpinMiner system for Twitter sentiment analysis evaluation at SemEval 2015. Our approach stems from two different angles: topic detection for discovering the sentiment distribution on different topics and sentiment analysis based on a variety of features. Moreover, we also implemented intra-sentence discourse relations for polarity identification. We divided the discourse reinto 4 predefined categories, including concontrast, condition, These relations could facilitate us to eliminate polarity ambiguities in compound sentences where both positive and negative sentiments are appearing. Based on the SemEval 2014 and SemEval 2015 Twitter sentiment analysis task datasets, the experimental results show that the performance of Twitter-OpinMiner could effectively recognize opinionated messages and identify the polarities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<contexts>
<context position="5381" citStr="Fan et al., 2008" startWordPosition="774" endWordPosition="777">d-Level and entity-level features The presence of sentiment word The ratio of sentiment word in a sentence The total number of positive words The total number of negative words The presence of negation words The total number of the word in all-caps Bi-gram features Named entities + opinion operators Pronouns + opinion operators Nouns or named entities + opinion words Pronouns + opinion words Opinion words (adjective) + (noun) 3 Feature Extraction The objective of this task is to determine whether a given message is positive, negative, or neutral. We train sentiment classifiers with LibLinear (Fan et al., 2008) on the training set and dev set, and tune parameter −c, −wi of SVM on the test set of SemEval 2013. SVM is a popular machine learning algorithm, the effectiveness of which has been proved in sentiment analysis on formal texts in related work (Pang and Lee, 2002; Liu, 2012). Since the performance of SVM classifier will be greatly influenced by the features selection, we explore a variety of features in the evaluation. 3.1 Features of topical sentiment distribution The advancement of Twitter is fast response to the real world, so people often search Twitter to find temporally relevant informati</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9: 1871-1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Mingqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion Observer: Analyzing and Comparing Opinions on the Web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>342--351</pages>
<publisher>ACM.</publisher>
<location>Chiba, Japan,</location>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Mingqing Hu, and Junsheng Cheng. 2005. Opinion Observer: Analyzing and Comparing Opinions on the Web. In Proceedings of the 14th international conference on World Wide Web, pages 342-351, Chiba, Japan, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining. Synthesis Lectures on Human Lcanguage Tehnologies,</title>
<date>2012</date>
<volume>5</volume>
<issue>1</issue>
<pages>1--167</pages>
<contexts>
<context position="4596" citStr="Liu, 2012" startWordPosition="650" endWordPosition="651">weet-specific features, discourse features, sentiment distribution among topics, and word embedding. (3) Sentiment analysis module: creates a SVM classifier that incorporates the above features classify the polarity of each tweet. Finally, Twitter-OpinMiner outputs the polarity of each tweet. 2.2 Development Data and Lexicon The development data are necessary in our system. We fully utilize the training tweets provided by SemEval 2013. The dataset consists of 9,912 annotated tweets. Besides, for sentiment analysis, we also utilize several sentiment lexicons, including Liu’s sentiment lexicon (Liu, 2012), MPQA subjectivity lexicon (Wilson et al., 2005), and the sentiment lexicon generated from tweets (Mohammad et al., 2013). Table 1. Features of text in our system. Word-Level and entity-level features The presence of sentiment word The ratio of sentiment word in a sentence The total number of positive words The total number of negative words The presence of negation words The total number of the word in all-caps Bi-gram features Named entities + opinion operators Pronouns + opinion operators Nouns or named entities + opinion words Pronouns + opinion words Opinion words (adjective) + (noun) 3 </context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Lcanguage Tehnologies, 5(1): 1-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text-Interdisciplinary Journal for the Study of Discourse,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<contexts>
<context position="9292" citStr="Mann and Thompson (1988)" startWordPosition="1403" endWordPosition="1406">uad in the last 10yrs, but Ancelotti is working for quality over quantity. Everyone... http://t.co/oCdPXQWggT. Table 2. Examples of cue-phrases. Relation Cue Phrases Contrast although, but, however, though Condition if, despite, in case of Continuation and, moreover, not only but also Cause because, so that, due to, in order that In this case, there are two segments in the tweet that holds a Contrast discourse relation, and the polarity is determined by “but” segment. In our system, we also take into consideration of intrasentence discourse relation features for processing compound sentences. Mann and Thompson (1988) defined a complete discourse scheme Rhetorical Structure Theory (RST). Since not all of the discourse relations in RST would help eliminate polarity ambiguities, the discourse relations were implemented in our system was on a subset (Zhou et al., 2011). In our system, we use cue-phrase based method for discourse relation identification. We maintain a cue phrase lexicon and the examples of the cue phrases were shown in Table 2. 4 Experiment We trained a SVM classifier on 9,912 annotated tweets (8,258 in the training set and 1,654 in the development set). We used the same evaluation metrics wit</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse, 8(3): 243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="4718" citStr="Mohammad et al., 2013" startWordPosition="667" endWordPosition="670">ment analysis module: creates a SVM classifier that incorporates the above features classify the polarity of each tweet. Finally, Twitter-OpinMiner outputs the polarity of each tweet. 2.2 Development Data and Lexicon The development data are necessary in our system. We fully utilize the training tweets provided by SemEval 2013. The dataset consists of 9,912 annotated tweets. Besides, for sentiment analysis, we also utilize several sentiment lexicons, including Liu’s sentiment lexicon (Liu, 2012), MPQA subjectivity lexicon (Wilson et al., 2005), and the sentiment lexicon generated from tweets (Mohammad et al., 2013). Table 1. Features of text in our system. Word-Level and entity-level features The presence of sentiment word The ratio of sentiment word in a sentence The total number of positive words The total number of negative words The presence of negation words The total number of the word in all-caps Bi-gram features Named entities + opinion operators Pronouns + opinion operators Nouns or named entities + opinion words Pronouns + opinion words Opinion words (adjective) + (noun) 3 Feature Extraction The objective of this task is to determine whether a given message is positive, negative, or neutral. W</context>
<context position="7285" citStr="Mohammad et al. (2013)" startWordPosition="1079" endWordPosition="1083">n Table 1, and two types of features are incorporated in the classifier. Topical sentiment distribution Preprocessing Formal text feature Word embedding Tweets Twitter-specific feature Discourse relations Feature extraction Sentiment analysis Results 665 These features are also integrated into our SVM classifier for training and treated as the baseline in our experiment. 3.3 Twitter specific feature Unlike formal texts, tweet has its own characteristics, including irregular expressions, emoticon, hashtag, ill format, and special punctuations. In our system, we combine the features proposed by Mohammad et al. (2013) with some new features as Twitter-specific features for supplementary to the forma text. • Hashtags: the number of hashtags in one tweet; • Ill format: the presence of ill format with some characters replacing by *, for example, f**k; • Punctuation: the number of contiguous sequences of exclamation marks, question marks, and both exclamation and question marks; whether the last token contains an exclamation or question mark; • Emoticons: the presence of positive and negative emoticons at any position in the tweet; whether the last token is an emoticon; • OOV: the ratio of words out of vocabul</context>
<context position="10346" citStr="Mohammad et al., 2013" startWordPosition="1577" endWordPosition="1581">4 Experiment We trained a SVM classifier on 9,912 annotated tweets (8,258 in the training set and 1,654 in the development set). We used the same evaluation metrics with SemEval 2013, including the macroaveraged F-score of the positive and negative classes. The experimental results obtained by our system on the training set (ten-fold cross validation), development set, and test sets on Twitter 2013 were shown in Table 3 where the baseline was achieved by using the formal text features as well as twitterspecific features. Since the effectiveness of these two types of features were analyzed in (Mohammad et al., 2013), we mainly evaluated the effectiveness of other features. Table 3 showed that the most effective feature on Twitter 2013 dataset turned out to be the word embedding features: they provided gains of about 7%. For LDA, we set the numbers of topic from 10 to 100, and found it could achieve best performance when equaling 50. We then constructed the sentiment distribution among 50 topics for the further evaluation. Besides, we also investigated the effectiveness of discourse features on compound sentences, and the statistics were shown in Table 6. 666 Table 3. Experimental results on Twitter 2013 </context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation,</booktitle>
<volume>13</volume>
<contexts>
<context position="1873" citStr="Nakov et al., 2013" startWordPosition="247" endWordPosition="250">eliminate polarity ambiguities in compound sentences where both positive and negative sentiments are appearing. Based on the SemEval 2014 and SemEval 2015 Twitter sentiment analysis task datasets, the experimental results show that the performance of Twitter-OpinMiner could effectively recognize opinionated messages and identify the polarities. 1 Introduction This year comes the third edition of SemEval Twitter sentiment analysis task consisting of new genres, including topic-based polarity classification, trends detection towards a topic, and the sentimental strength of association of terms (Nakov et al., 2013). *Corresponding author We only participated in the subtask of message sentiment analysis and built up a system, named Twitter-OpinMiner for the task. TwitterOpinMiner stems from two different angles: LDAbased topic detection for discovering the opinionated features of trending tweets’ topics and sentiment analysis based on a variety of features. • Topic detection Recent studies show that people often search Twitter to find temporally relevant information (Teevan et al., 2011), such as emergent events, trending topics. In fact, similar opinions were likely to express on the same topic/event in</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, volume 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification Using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification Using Machine Learning Techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentiment-specific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="8270" citStr="Tang et al., 2014" startWordPosition="1242" endWordPosition="1245">ether the last token contains an exclamation or question mark; • Emoticons: the presence of positive and negative emoticons at any position in the tweet; whether the last token is an emoticon; • OOV: the ratio of words out of vocabulary; • Elongated words: the presence of sentiment words with one character repeated more than two times, for example, ‘cooool’; • URL: whether the tweet contains a URL. • Reply or Retweet: Is the current tweet a reply/retweet tweet 3.4 Word embedding We also utilize word embedding technique for feature extraction. We adopt sentiment-specific word embedding method (Tang et al., 2014) that could encode sentiment information in the continuous representation of words. In our approach, each term is extended into a 150 dimensional vector. 3.5 Discourse specific feature Since tweets are usually expressed informally, there are many compound sentences in a tweet, which always contain positive sentiment and negative sentiment with ambiguity. For example, It may not be the biggest squad in the last 10yrs, but Ancelotti is working for quality over quantity. Everyone... http://t.co/oCdPXQWggT. Table 2. Examples of cue-phrases. Relation Cue Phrases Contrast although, but, however, tho</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word embedding for twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555-1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Teevan</author>
<author>Daniel Ramage</author>
<author>Merredith R Morris</author>
</authors>
<title>TwitteSearch: A Comparison of Microblog Search and Web Search.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>35--44</pages>
<contexts>
<context position="2354" citStr="Teevan et al., 2011" startWordPosition="320" endWordPosition="323">-based polarity classification, trends detection towards a topic, and the sentimental strength of association of terms (Nakov et al., 2013). *Corresponding author We only participated in the subtask of message sentiment analysis and built up a system, named Twitter-OpinMiner for the task. TwitterOpinMiner stems from two different angles: LDAbased topic detection for discovering the opinionated features of trending tweets’ topics and sentiment analysis based on a variety of features. • Topic detection Recent studies show that people often search Twitter to find temporally relevant information (Teevan et al., 2011), such as emergent events, trending topics. In fact, similar opinions were likely to express on the same topic/event in Twitter. For example, there are 20 tweets expressing similar opinions on “Blood moon” in SemEval 2015 dataset. Therefore, it can facilitate us to discover the sentiment distribution on different topics. • Sentiment analysis Unlike traditional news content, tweets are specialists in short texts with long compound sentences, and a number of irregular expressions, including emoticon, hashtag, and special punctuations. In order to better support tweets analysis, we extract featur</context>
</contexts>
<marker>Teevan, Ramage, Morris, 2011</marker>
<rawString>Jaime Teevan, Daniel Ramage, and Merredith R. Morris. 2011. #TwitteSearch: A Comparison of Microblog Search and Web Search. In Proceedings of the 4th ACM International Conference on Web Search and Data Mining, pages 35-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="4645" citStr="Wilson et al., 2005" startWordPosition="656" endWordPosition="659">res, sentiment distribution among topics, and word embedding. (3) Sentiment analysis module: creates a SVM classifier that incorporates the above features classify the polarity of each tweet. Finally, Twitter-OpinMiner outputs the polarity of each tweet. 2.2 Development Data and Lexicon The development data are necessary in our system. We fully utilize the training tweets provided by SemEval 2013. The dataset consists of 9,912 annotated tweets. Besides, for sentiment analysis, we also utilize several sentiment lexicons, including Liu’s sentiment lexicon (Liu, 2012), MPQA subjectivity lexicon (Wilson et al., 2005), and the sentiment lexicon generated from tweets (Mohammad et al., 2013). Table 1. Features of text in our system. Word-Level and entity-level features The presence of sentiment word The ratio of sentiment word in a sentence The total number of positive words The total number of negative words The presence of negation words The total number of the word in all-caps Bi-gram features Named entities + opinion operators Pronouns + opinion operators Nouns or named entities + opinion words Pronouns + opinion words Opinion words (adjective) + (noun) 3 Feature Extraction The objective of this task is </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the International Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanjun Zhou</author>
<author>Yunqing Xia</author>
<author>Binyang Li</author>
<author>Kamfai Wong</author>
</authors>
<date>2010</date>
<booktitle>WIA-Opinmine System in NTCIR-8 MOAT Evaluation. In the 8th NTCIR Workshop Meeting,</booktitle>
<pages>286--292</pages>
<contexts>
<context position="6650" citStr="Zhou et al., 2010" startWordPosition="986" endWordPosition="989">ct, tweets are likely to converge on some opinions for a specific topic, which will lead to different sentiment distributions among topics. In our system, we adopt LDA-based approach for representing the typical sentiment distribution features. We use the Mallet toolkit, set the topic number as 50, and map each tweet into 50 dimensions to extract those features. 3.2 Features of formal text Although the task is to analyze sentiment in Twitter, much research proved the effectiveness of the classic features of formal texts on tweets. The features we adopted in this task are partly the same with (Zhou et al., 2010) and listed in Table 1, and two types of features are incorporated in the classifier. Topical sentiment distribution Preprocessing Formal text feature Word embedding Tweets Twitter-specific feature Discourse relations Feature extraction Sentiment analysis Results 665 These features are also integrated into our SVM classifier for training and treated as the baseline in our experiment. 3.3 Twitter specific feature Unlike formal texts, tweet has its own characteristics, including irregular expressions, emoticon, hashtag, ill format, and special punctuations. In our system, we combine the features</context>
</contexts>
<marker>Zhou, Xia, Li, Wong, 2010</marker>
<rawString>Lanjun Zhou, Yunqing Xia, Binyang Li, and Kamfai Wong. 2010. WIA-Opinmine System in NTCIR-8 MOAT Evaluation. In the 8th NTCIR Workshop Meeting, pages 286-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanjun Zhou</author>
<author>Binyang Li</author>
<author>Wei Gao</author>
<author>Zhongyu Wei</author>
<author>Kam-fai Wong</author>
</authors>
<title>Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>162--171</pages>
<contexts>
<context position="9545" citStr="Zhou et al., 2011" startWordPosition="1443" endWordPosition="1446">oreover, not only but also Cause because, so that, due to, in order that In this case, there are two segments in the tweet that holds a Contrast discourse relation, and the polarity is determined by “but” segment. In our system, we also take into consideration of intrasentence discourse relation features for processing compound sentences. Mann and Thompson (1988) defined a complete discourse scheme Rhetorical Structure Theory (RST). Since not all of the discourse relations in RST would help eliminate polarity ambiguities, the discourse relations were implemented in our system was on a subset (Zhou et al., 2011). In our system, we use cue-phrase based method for discourse relation identification. We maintain a cue phrase lexicon and the examples of the cue phrases were shown in Table 2. 4 Experiment We trained a SVM classifier on 9,912 annotated tweets (8,258 in the training set and 1,654 in the development set). We used the same evaluation metrics with SemEval 2013, including the macroaveraged F-score of the positive and negative classes. The experimental results obtained by our system on the training set (ten-fold cross validation), development set, and test sets on Twitter 2013 were shown in Table</context>
</contexts>
<marker>Zhou, Li, Gao, Wei, Wong, 2011</marker>
<rawString>Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, and Kam-fai Wong. 2011. Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 162-171.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>