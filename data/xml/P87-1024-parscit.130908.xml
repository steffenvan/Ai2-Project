<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998791">
On the Acquisition of Lexical Entries:
The Perceptual Origin of Thematic Relations
</title>
<author confidence="0.992421">
James Pustejovsky
</author>
<affiliation confidence="0.9438">
Department of Computer Science
Brandeis University
</affiliation>
<address confidence="0.6872065">
Waltham, MA 02254
617-736-2709
</address>
<email confidence="0.984015">
jamesp@brandeis.csnet-relay
</email>
<sectionHeader confidence="0.993339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999673">
This paper describes a computational model of concept
acquisition for natural language. We develop a theory
of lexical semantics, the Extended Aspect Calculus, which
together with a &amp;quot;markedness theory&amp;quot; for thematic rela-
tions, constrains what a possible word meaning can be.
This is based on the supposition that predicates from the
perceptual domain are the primitives for more abstract
relations. We then describe an implementation of this
model, TULLY, which mirrors the stages of lexical acqui-
sition for children.
</bodyText>
<sectionHeader confidence="0.9979" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.998306634146342">
In this paper we describe a computational model of con-
cept acquisition for natural language making use of po-
sitive-only data, modelled on a theory of lexical seman-
tics. This theory, the Extended Aspect Calculus acts to-
gether with a markedness theory for thematic roles to
constrain what a possible word type is, just as a gram-
mar defines what a well-formed tree structure is in syntax.
We argue that linguistic specific knowledge and learning
principles are needed for concept acquisition from positive
evidence alone&amp;quot;. Furthermore, this model posits a close in-
teraction between the predicates of visual perception and
the early semantic interpretation of thematic roles as used
in linguistic expressions. In fact, we claim that these re-
lations act as constraints to the development of predicate
hierachies in language acquisition. Finally, we describe
TULLY, an implementation of this model in ZETALISP
and discuss its design in the context of machine learning
research.
There has been little work on the acquisition of
thematic relation and case roles, due to the absence of
any consensus on their formal properties. In this research
we begin to address what a theory of thematic relations
might look like, using learnability theory as a metric for
evaluating the model. We claim that there is an impor-
tant relationship between visual or imagistic perception
and the development of thematic relations in linguistic us-
age for a child. This has been argued recently by Jackend-
off (1983, 1985) and was an assumption in the pioneering
work of Miller and Johnson-Laird (1976). Here we argue
that the conceptual abstraction of thematic information
does not develop arbitrarily but along a given, predictable
path; namely, a developmental path that starts with tan-
gible perceptual predicates (e.g. spatial, causative) to
later form the more abstract mental and cognitive predi-
cates. In this view thematic relations are actually sets of
thematic properties, related by a partial ordering. This
effectively establishes a markedness theory for thematic
roles that a learning system must adhere to in the acqui-
sition of lexical entries for a language.
We will discuss two computational methods for
concept development in natural language:
</bodyText>
<listItem confidence="0.618373">
(1) Feature Relaxation of particular features of the ar-
</listItem>
<bodyText confidence="0.912437875">
guments to a verb. This is performed by a con-
straint propagation method.
(2) Thematic Decoupling of semantically incorporated
information from the verb.
When these two learning techniques are combined with
the model of lexical semantics adopted here, the stages
of development for verb acquisition are similar to those
acknowledged for child language acquisition.
</bodyText>
<sectionHeader confidence="0.848108" genericHeader="introduction">
2. Learnability Theory and Concept De-
velopment
</sectionHeader>
<bodyText confidence="0.99996575">
Work in machine learning has shown the useful-
ness to an inductive concept-learning system of inducing
&amp;quot;bias&amp;quot; in the learning process (cf. [Mitchell 1977, 1978],
[Michalski 1983]). An even more promising development
is the move to base the bias on domain-intensive models,
as seen in [Mitchell et al. 1985], [Utgoff 19851, and [Win-
ston et al. 1983]. This is an important direction for those
concerned with natural language acquisition, as it con-
verges with a long-held belief of many psychologists and
linguists that domain-specific information is necessary for
learning (cf. [Slobin 1982], [Pinker 19841, [Bowerman
1974], [Chomsky 1980]). Indeed, Berwick (1984) moves in
exactly this direction. Berwick describes a model for the
acquisition of syntactic knowledge based on a restricted
X-syntactic parser, a modification of the Marcus parser
((Marcus 1980]). The domain knowledge specified to the
system in this case is a parametric parser and learning
system that adapts to a particular linguistic environment,
given only positive data. This is just the sort of biasing
necessary to account for data on syntactic acquisition.
</bodyText>
<page confidence="0.99668">
172
</page>
<bodyText confidence="0.996241380952381">
One area of language acquisition that has not been
sufficiently addressed within computational models is the
acquisition of conceptual structure. For language acquisi-
tion, the problem can be stated as follows: How does the
child identify a particular thematic role with a specific
grammatical function in the sentence? This is the prob-
lem of mapping the semantic functions of a proposition
into specified syntactic positions in a sentence.
Pinker (1984) makes an interesting suggestion (due
originally to D. Lebeaux) in answer to this question. He
proposes that one of the strategies available to the lan-
guage learner involves a sort of &amp;quot;template matching&amp;quot; of
argument to syntactic position. There are canonical con-
figurations that are the default mappings and non-cano-
nical mappings for the exceptions. For example, the tem-
plate consists of two rows, one of thematic roles, and the
other of syntactic positions. A canonical mapping exists
if no lines joining the two rows cross. Figure 1 shows a
canonical mapping representing the sentence in (1), while
Figure 2 illustrates a noncanonical mapping representing
sentence (2).
</bodyText>
<figure confidence="0.924232666666667">
0-roles: A Th GISIL
NNN:NN
Syntactic roles: SUBJ OBJ OBL
</figure>
<figureCaption confidence="0.67188">
Figure 1
</figureCaption>
<figure confidence="0.7340735">
9-roles: A Th GISIL
Syntactic roles: SUBJ OBJ OBL
</figure>
<figureCaption confidence="0.941316">
Figure 2
</figureCaption>
<listItem confidence="0.9923525">
(1) Mary hit Bill.
(2) Bill was hit by Mary.
</listItem>
<bodyText confidence="0.999934275862069">
With this principle we can represent the productivity of
verb forms that are used but not heard by the child. We
will adopt a modified version of the canonical mapping
strategy for our system, and embed it within a theory of
how perceptual primitives help derive linguistic concepts.
As mentioned, one of the motivations for adopt-
ing the canonical mapping principle is the power it gives
a learning system in the face of positive-only data. In
terms of learnability theory, Berwick (1985) (following
[Angluin 1978)) notes that to ensure successful acquisi-
tion of the language after a finite number of positive ex-
amples, something like the Subset Principle is necessary.
We can compare this principle to a Version Space model
of inductive learning( [Mitchell 1977, 1978]), with no neg-
ative instances. Generalization proceeds in a conservative
fashion, taking only the narrowest concept that covers the
data.
How does this principle relate to lexical seman-
tics and the way thematic relations are mapped to syn-
tactic positions? We claim that the connection is very
direct. Concept learning begins with spatial, temporal,
and causal predicates being the most salient. This follows
from our supposition that these are innate structures, or
are learned very early. Following Miller and Johnson-
Laird (1976), [Miller 1985], and most psychologists, we
assume the prelinguistic child is already able to discern
spatial orientations, causation, and temporal dependen-
cies. We take this as a point of departure for our theory
of markedness, which is developed in the next section.
</bodyText>
<sectionHeader confidence="0.527564" genericHeader="method">
3.0 Theoretical Assumptions
</sectionHeader>
<subsectionHeader confidence="0.999976">
3.1 The Extended Aspect Calculus
</subsectionHeader>
<bodyText confidence="0.999930736842106">
In this section we outline the semantic framework
which defines our domain for lexical acquisition. In the
current linguistic literature on case roles or thematic re-
lations, there is little discussion on what logical connec-
tion exists between one 8-role and another. Besides being
the workhorse for motivating several principles of syn-
tax (cf. [Chomsky 1981], (Williams 1980]) the most that
is claimed is that Universal Grammar specifies a reper-
toire of thematic relations (or case roles), Agent, Theme,
Patient, Goal, Source, Instrument, and that every NP
must carry one and only one role. It should be remem-
bered, however, that thematic relations were originally
conceived in terms of the argument positions of seman-
tic predicates such as CAUSE and DO. 1 That is a verb
didn&apos;t simply have a list of labelled arguments 2 such as
Agent and Patient, but had an interpretation in terms of
more primitive predicates where the notions Agent and
Patient were defined. The causer of an event (following
Jackendoff (1976)) is defined as an Agent, for example,
</bodyText>
<subsectionHeader confidence="0.593702">
CAUSE(x,e)—■ Agent(x).
</subsectionHeader>
<bodyText confidence="0.94393095">
Similarly, the first argument position of the pred-
icate GO is interpreted as Theme, as in GO(x,y,z). The
second argument here is the SOURCE and the third is
called the GOAL.
The model we have in mind acts to constrain the
space of possible word meanings. In this sense it is similar
to Dowty&apos;s aspect calculus but goes beyond it in embed-
ding his model within a markedness theory for thematic
types. Our model is a first-order logic that employs sym-
bols acting as special operators over the standard logical
vocabulary. These are taken from three distinct semantic
fields. They are: causal, spatial, and aspectual.
The predicates associated with the causal field are
Causer (C1), Causee (C2), and Instrument (/). The spatial
field has only one predicate, Locative, which is predicated
of an object we term the Theme. Finally, the aspectual
Cf.Jackendoff (1972, 1976) for a detailed elaboration of
this theory.
2 This is now roughly the common assumption in GB,
GPSG, and LFG.
</bodyText>
<page confidence="0.995789">
173
</page>
<bodyText confidence="0.99998395">
field has three predicates, representing the three temporal
intervals t1, beginning, t2, middle, and t3, end. From the
interaction of these predicates all thematic types can be
derived. We call the lexical specification for this aspectual
and thematic information the Thematic Mapping Index.
As an example of how these components work to-
gether to define a thematic type, consider first the dis-
tinction between a state, an activity (or process), and an
accomplishment. A state can be thought of as reference
to an unbounded interval, which we will simply call ts;
that is, the state spans this interval.3 An activity or pro-
cess can be thought of as referring to a designated initial
point and the ensuing process; in other words, the situa-
tion spans the two intervals t1 and t2. Finally, an event
can be viewed as referring to both an activity and a des-
ignated terminating interval; that is, the event spans all
three intervals, t1, t2, and t3.
Now consider how these bindings interact with the
other semantic fields for the verb run in sentence (8) and
give in sentence (9).
</bodyText>
<listItem confidence="0.9947365">
(8) John ran yesterday.
(9) John gave the book to Mary.
</listItem>
<bodyText confidence="0.999741">
We associate with the verb run an argument structure of
simply run(x). For give we associate the argument struc-
ture give(x, y, z). The Thematic Mapping Index for each is
given below in (10) and (11).
</bodyText>
<equation confidence="0.9014352">
,c1
give = r
L Th
11 12 t3.
I ,
</equation>
<bodyText confidence="0.99057268">
The sentence in (8) represents a process with no logical
culmination, and the one argument is linked to the named
case role, Theme. The entire process is associated with
both the initial interval el and the middle interval t2. The
argument z is linked to CI as well, indicating that it is
an Actor as well as a moving object (i.e. Theme). This
represents one TMI for an activity verb.
The structure in (9) specifies that the meaning of
give carries with it the supposition that there is a logical
3 This is a simplication of our model, but for our
purposes the difference is moot. A state is actually inter-
preted as a primitive homogeneous event-sequence, with
downward closure. Cf. [Pustejovsky, 19871,
4 [Jackendoff 19851 develops a similar idea, but vide infra
for discussion.
culmination to the process of giving. This is captured by
reference to the final subinterval, t3. The linking between
z and the L associated with el is interpreted as Source,
while the other linked arguments, y and z are Theme (the
book) and Goal, respectively. Furthermore, z is specified
as a Causer and the object which is marked Theme is also
an affected object (i.e. Patient). This will be one of the
TMIs for an accomplishment.
In these examples the three subsystems are shown
as rows, and the configuration given is lexically specified.
</bodyText>
<subsectionHeader confidence="0.99721">
3.2 A Markedness Theory for Thematic Roles
</subsectionHeader>
<bodyText confidence="0.9862555625">
As mentioned above, the theory we are outlining
here is grounded on the supposition that all relations in
the language are suffiently described in terms of causal,
spatial and aspectual predicates. A thematic role in this
view is seen as a set of primitive properties relating to the
predicates mentioned above. The relationship between
these thematic roles is a partial ordering over the sets of
properties defining them. It is this partial ordering that
allows us to define a markedness theory for thematic roles.
Why is this important?
If thematic roles are assigned randomly to a verb,
then one would expect that there exist verbs that have
only Patient or Instrument, or two Agents or Themes, for
example. Yet this is not what we find. What appears to
be the case is that thematic roles are not assigned to a
verb independently of one another, but rather that some
thematic roles are fixed only after other roles have been
established. For example, a verb will not be assigned a
GOAL if there is not a THEME assigned first. Similarly,
a LOCATIVE is dependent on there being a THEME
present. This dependency can be viewed as an acquisition
strategy for learning the thematic relations of a verb.
Now let us outline the theory. We begin by estab-
lishing the most unmarked relation that an argument can
bear to its predicate. Let us call this role Themeu. The
only semantic information this carries is that of an exis-
tential quantifier. It is the only named role outside of the
three interpretive systems defined above. Normally, we
think of Theme as an object in motion. This is only half
correct, however, since statives carry a Theme readings as
well. It is in fact the feature [±motson] that distinguishes
the role of Mary in (1) and (2) below.
</bodyText>
<listItem confidence="0.9997885">
(1) Stative: [—motion) Mary sleeps.
(2) Active: [+motton] Mary fell.
</listItem>
<bodyText confidence="0.73832">
This gives us our first markedness convention:
</bodyText>
<listItem confidence="0.984053">
(3) Themeu —.ThemeAll+mottoni
(3) Themeu —.Themeslt—motion[
</listItem>
<equation confidence="0.97922325">
, I \
run =
Th
ti t2
</equation>
<page confidence="0.984899">
174
</page>
<bodyText confidence="0.999862166666667">
where ThemeA is an &amp;quot;activity&amp;quot; Theme, and Themes is a
stative.
Within the spatial subsystem, there is one variable
type, Location, and a finite set of them L1,1,2 ... Ln. The
most unmarked location is that carrying no specific aspec-
tual binding. That is, the named variables are LB and LE
and are commonly referred to as Source and Goal. Thus,
Lu is the unmarked role. The limitations on named loca-
tive variables is perhaps constrained only by the aspectual
system of the language (rich aspectual distinction, then
more named locative variables). The markedness conven-
tions here are:
</bodyText>
<figure confidence="0.838547">
(4) Lu SIB
(5) Lu GIE
</figure>
<bodyText confidence="0.991145076923077">
Within the causal subsystem there are three pred-
icates, C1, C2, and I. We call C2, (the traditional Patient
role) is less marked than C1, but is more marked than I.
These conventions give us the core of the primitive
semantic relations. To be able to perform predicate gen-
eralization over each relation, however, we define a set of
features that applies to each argument within the seman-
tic subsystems. These are the abstraction operators that
allow a perceptual-based semantics to generalize to non-
perceptual relations. These features also have marked
and unmarked values, as we will show below. There are
four features that contribute to the generalization process
in concept acquisition:
</bodyText>
<figure confidence="0.9357175">
(a) (*abstract] (b) [*direct]
(c) [±complete] (d) [±animate]
</figure>
<bodyText confidence="0.998234583333333">
The first feature, abstract, distinguishes tangible
objects from intangible ones. Direct will allow a gradi-
ence in the notion of causation and motion. The third
feature, complete, picks out the extension of an argument
as either an entire object or only part of it. Ansmacy has
the standard semantics of labeling an object as alive or
not.
Let us illustrate how these operators abstract over
primitive thematic roles. By changing the value of a fea-
ture, we can alter the description, and hence, the set of
objects in its extension. Assume, for example, that the
predicate C1 has as its unmarked value, [+Direct].
</bodyText>
<listItem confidence="0.3427345">
(6) CdUDirect] ki-Direct]
By changing the value of this feature we allow Ci, the
direct agent of an event, to refer to an indirect causer.
(7) Acent[+Direct] &lt;a Agent[—Direct(
</listItem>
<bodyText confidence="0.990129">
Similarly, we can change the value of the default setting
for the feature (+Compute] to refer to a subcausation (or
causation by part).
</bodyText>
<listItem confidence="0.506154">
(8) Agent[+COMplete] &lt;a Agent[—Complete]
</listItem>
<bodyText confidence="0.9999168">
These changes define a new concept, &amp;quot;effector&amp;quot;, which is
a superset of the previous concepts given in the system.
The same can be done with C2 to arrive at the concept of
an &amp;quot;effected object.&amp;quot; We see the difference in interpreta-
tion in the sentences below.
</bodyText>
<listItem confidence="0.97777325">
a. John intentionally broke the chair. (Agent-direct)
b. John accidentally broke that chair when he sat
down. (Agent-indirect)
c. John broke the chair when he fell. (Effector)
</listItem>
<bodyText confidence="0.999860380952381">
Given the manner in which the features of primi-
tive thematic roles are able to change their values, we are
defining a predictable generalization path that relations
incorporating these roles will take. In other words, two
concepts may be related thematically, but may have very
different extensional properties. For example, give and
take are clearly definable perceptual transfer relations.
But given the abstractions available from our marked-
ness theory, they are thematically related to something
as distant as &amp;quot;experiencer verbs&amp;quot;, e.g. please, as in &amp;quot;The
book pleased John.&amp;quot; This relation is a transfer verb with
an incorporated Theme; namely, the &amp;quot;pleasure.&amp;quot; 5
If we apply these features in the spatial subsystem,
we can arrive at generalized notions of location, as well
as abstracted interpretations for Theme, Goal and Source.
For example, given the thematic role Th— A with the fea-
ture [—Abstract] in the default setting, we can generalize
to allow for abstract relations such as like, where the ob-
ject is not affected, but is an abstract Theme. Similarly,
the Theme in a sentence such as (a) can be concrete and
direct, or abstract, as in (b).
</bodyText>
<listItem confidence="0.987161">
(a) have(L, Th) Mary has a book.
(b) have(L, Th) Mary has a problem with Bill.
</listItem>
<bodyText confidence="0.8607045">
In conclusion, we can give the following dependencies be-
tween thematic roles:
Cf. Pustejovsky (1987) for an explanation of this term
and a full discussion of the extended aspect calculus.
</bodyText>
<page confidence="0.99758">
175
</page>
<bodyText confidence="0.965946666666667">
The generaliztion features apply to this structure to build
hierarchical structures (Cf. [Keil 1979], [Kodratoff 1986]).
This partial ordering allows us to define a notion of cov-
ering, as with a semi-lattice, from which a strong princi-
ple of functional uniqueness is derivable (cf. [Jackendoff
1985]). The mapping of a thematic role to an argument
follows the following principle:
(9) Maximal Assignment Principle An argument
will receive the maximal interpretation consistent
with the data.
This says two things. First, it says that an Agent, for
example, will always have a location and theme role as-
sociated with it. Furthermore, an Agent may be affected
by its action, and hence be a Patient as well. Secondly,
this principle says that although an argument may bear
many thematic roles, the grammar picks out that function
which is maximally specific in its interpretation, accord-
ing to the markedness theory. Thus, the two arguments
might be Themes in &amp;quot;John chased Mary&amp;quot;, but the the-
matic roles which maximally characterize their functions
in the sentence are A and P, respectively.
</bodyText>
<sectionHeader confidence="0.995191" genericHeader="method">
4. The Learning Component
</sectionHeader>
<subsectionHeader confidence="0.999561">
4.1 The Form of the Input
</subsectionHeader>
<bodyText confidence="0.999886266666667">
The input is a data structure pair; an event se-
quence expression and a sentence describing the event.
The event-sequence is a simulated output from a middle-
level vision system where motion detection from the low-
level input has already been associated with particular
object types. °
The event-sequence consists of three instantaneous
descriptions (IDs) of a situation represented as intervals.
These correspond to the intervals el, t2, and t3 in the
aspect calculus. The predicates are perceptual primi-
tives, such as those described in Miller and Johnson-
Laird (1976) and Maddox and Pustejovsky (1987), such
as [AT(tl, e) &amp;e = [0 N(a, e) &amp; Animate(c) &amp; Moves(a) &amp;
The second object is a linguistic expression (i.e. a sen-
tence), parsed by a simple finite state transducer. 7
</bodyText>
<subsectionHeader confidence="0.989229">
4.2 The Acquisition Procedure
</subsectionHeader>
<bodyText confidence="0.999922">
We now turn to the design of the learning program
itself. TULLY can be characterized as a domain-intensive
inductive learning system, where the generalizations pos-
sible in the system are restricted by the architecture im-
posed by the semantic model. We can separate clearly
what is given from what is learned in the system, as shown
in Figure 1.
</bodyText>
<sectionHeader confidence="0.644863166666667" genericHeader="method">
GIVEN
Extended Aspect Calculus
0-Markedness Theory
Canonical Mapping
Rule Execution Loop
ACQUIRED
</sectionHeader>
<subsectionHeader confidence="0.589159">
Verbal Lexical semantics
Argument-function mapping
Predication Hierarchy
</subsectionHeader>
<bodyText confidence="0.8758658">
Figure 1
In order to better understand the learning mecha-
nism, we will step through an example run of the system.
First, however, we will give the rule execution loop which
the system follows.
</bodyText>
<subsectionHeader confidence="0.595921">
Rule Execution Loop
</subsectionHeader>
<listItem confidence="0.427938">
1. Instantiate Existing Thematic Indexes
</listItem>
<bodyText confidence="0.994685666666667">
INSTANTIATE: Attempt to do a semantic analy-
sis of word given using existing Thematic Mapping
Indexes. If the analysis fails then go to 2.
</bodyText>
<listItem confidence="0.5462986">
2. Concept-acquisition phase.
Note failure: Credit assignment.
Link arguments to roles according to Canonical
Mapping.
3. Build new Thematic Mapping Index
</listItem>
<bodyText confidence="0.81712475">
LINK and SHIFT: Constructs new index accord-
ing to the Extended Aspect Calculus using infor-
mation from credit assignment in (2). If this fails
then go to (4).
</bodyText>
<sectionHeader confidence="0.849405" genericHeader="method">
4. Invoke Noncanonical Mapping Principle.
</sectionHeader>
<bodyText confidence="0.716267181818182">
If (3) fails to build a mapping for the lexical item in
the input, then the rule INTERSECT is invoked.
This allows the lines to cross from any of the in-
terpretive levels to the argument tier.
6 For a detailed discussion of how the visual processing
and linguistic systems interact, d. Maddox and Pustejovsky
(1987).
7 We are not addressing any complex interaction between
syntactic and semantic acquisition in this system. Ideally, we
would like to integrate the concept acquisition mechanisms here
with a parser such as Berwick&apos;s, Cf. Berwick 1985.
</bodyText>
<sectionHeader confidence="0.840724" genericHeader="method">
5. Generalization Step.
</sectionHeader>
<bodyText confidence="0.99900425">
This is where the markedness theory is invoked.
Induction follows the restrictions in the theory,
where generalization is limited to one of the stated
types.
</bodyText>
<page confidence="0.997108">
176
</page>
<bodyText confidence="0.999736043478261">
Assume that the first input to the system is the
sentence &amp;quot;Mary hit the cat,&amp;quot; with its accompanying event
sequence expression, represented as a situation calculus
expression. INSTANTIATE attempts to map an exist-
ing Thematic Mapping Index onto the input, but fails.
Stage (2) is entered by the failure of (1), and credit as-
signment indicates where it failed. Heuristics will indicate
which thematic properties are associated with each argu-
ment, and stage (3) links the arguments with the proper
roles, according to Canonical Mapping. This links Mary
to Agent and the cat to Patient.
One important point to make here is that any
information from the perceptual expression that is not
grammatically expressed will automatically be assumed
to be part of the verb meaning itself. In this case, the
instrument of the hitting (e.g. Mary&apos;s arm) is covered by
the lexical semantics of hit.
There are two forms of generalization performed
by the system in step (5): constraint propagation and
thematic decoupling. In a propagation procedure (Cf.
[Waltz, 1975]), the computation is described as operat-
ing locally, since the change has local consistency. To
illustrate, consider the verb entry for have, as in (1),
</bodyText>
<listItem confidence="0.802513">
(1) John has a book. have(x = L, y = Th)
</listItem>
<bodyText confidence="0.999939333333333">
where the object carries the feature [—abstract]. Now, con-
sider how the sense of the verb changes with a feature
change to [-&apos;-abstract], as in (2).
</bodyText>
<listItem confidence="0.752753">
(2) John has an idea.
</listItem>
<bodyText confidence="0.978094727272727">
In other words, there is a propagation of this feature to
the subject, where the sense of locative becomes more
abstract, e.g. mental. These types of extensions give rise
to other verbs with the same thematic mapping, but with
&amp;quot;relaxed&amp;quot; interpretations. °
The other strategy employed here is that of the-
matic decoupling, where thematic information becomes
disassociated from the lexical semantics for a verb. °
The narrower interpretation of a verb&apos;s meaning will be
arrived at after enough training instances are given; for
example, from cut as meaning a particular action with a
knife, to cut as an action that results in a certain state.
It is interesting to speculate on how these strate-
gies facilitate the development from perceptual relations
to more abstract ones. The verb tell, for example, can be
viewed as a transfer verb with a (+abstract] Theme, and the
accompanying contraint propagation (Cf. [Pinker, 1984]
and [Jackendoff, 1983]). Similarly, experiencer verbs such
as please, upset, and anger can be seen as combining both
strategies: they are similar to transfer verbs, but with fea-
8 For further discussion of constraint propagation as
a learning strategy, cf. Pustejovsky (1987b).
° Results given in Nygren (1977) indicate that chil-
dren have fully incorporated instruments for verbs such
as hammer, cut, and saw, and only at a later-age do they
abstract to a verb sense without a particular and constant
instrument interpretation.
ture relaxation on the Theme, together with propagated
constraints to the Source and Goal (the subject and ob-
ject, respectively); the difference is that the Theme is
incorporated and is not grammatically expressed.
John pleased his mother.
please(x = 5, y = G,Th : incorporated)
</bodyText>
<sectionHeader confidence="0.558771" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.999154727272727">
In this paper we have outlined a theory of acquisi-
tion for the semantic roles associated, with verbs. Specifi-
cally, we argue that perceptual predicates form the foun-
dation for later conceptual development in language, and
propose a specific algorithm for learning employing a the-
ory of markedness for thematic types and the two strate-
gies of thematic decoupling and constraint relaxation and
propagation. The approach sketched above will doubtless
need revision and refinement on particular points, but is
claimed to offer a new perspective which can contribute to
the solution of some long-standing puzzles in acquisition.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999636">
I would like to thank Sabine Bergler who did the
first implementation of the algorithm, as well as Anthony
Maddox, John Brolio, Ken Wexler, Mellissa Bowerman,
and Edwin Williams for useful discussion. All faults and
errors are of course my own.
</bodyText>
<sectionHeader confidence="0.99819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986068739130435">
[1] Angluin, D. &amp;quot;Inductive Inference of formal Lan-
guages from positive data.&amp;quot; Information and Con-
trol 45:117-135.
[2] Berwick, Robert C. The Acquisition of Syntactic
Information, MIT Press, Cambridge, MA. 1985.
[3] Berwick, Robert C., &amp;quot;Learning from Positive-Only
Examples: The Subset Principle and Three Case
Studies,&amp;quot; in Michalski et al, 1986.
[4] Bowerman, Mellissa &amp;quot;Learning the Structure of Cau
sative Verbs,&amp;quot; in Clark (ed) Papers and reports on
child language development, No. 8, Stanford Uni-
versity Committee on Linguistics. 1974
[5] Chomsky, Noam Rules and Representation, Colum-
bia University Press, 1980
[6] Chomsky, Noam Lectures on Government and Bind-
ing, Foris, Holland, 1981.
[7] Dowty, David R., Word Meaning and Montague
Grammar, D. Reidel, Dordrecht, Holland, 1979.
[8] Jackendoff, Ray, Language and Cognition, MIT
Press, Cambridge, MA. 1983.
[9] Jackendoff, Ray, &amp;quot;The Role of Thematic Relations
in Linguistic Theory,&amp;quot;, ms. Brandeis University,
1985
</reference>
<page confidence="0.982009">
177
</page>
<reference confidence="0.999941892857143">
[10] Kodratoff, Yves, and J-G. Ganascia, &amp;quot;Improving
the Generalization Step in Learning&amp;quot;, in Michal-
skiet el (eds.), Machine Learning II, Morgan Kauf-
mann.
[11] Marcus, Mitch, A Theory of Syntactic Recogni-
tion for Natural Language, MIT Press, Cambridge,
1980
[12] Michalski, R.S., &amp;quot;A Theory and Methodology of
Inductive Learning,&amp;quot;, in Michalski et al (eds.), Ma-
chine Learning I.
[13] Miller, George, &amp;quot;Dictionaries of the Mind&amp;quot; in Pro-.
ceedings of the 23rd Annual Meeting of the As-
sociation for Computational Linguistics, Chicago,
1985.
[14] Miller, George and Philip Johnson-Laird, Language
and Perception, Belknap, Harvard University Press,
Cambridge, MA. 1976.
[15] Mitchell, Tom, &amp;quot;Version Spaces: A Candidate Elim-
ination Approach to Rule Learning,&amp;quot; in IJCAI-77,
1977
[16] Mitchell, Tom, Version Spaces: An Approach to
Concept Learning, Ph.D. thesis Stanford, 1978.
[17] Nygren, Carolyn, &amp;quot;Results of Experiments with In-
strumentals,&amp;quot; ms. UMASS, Amherst, MA.
[18] Pilato, Samuel F. and Robert C. Berwick, &amp;quot;Re-
versible Automata and Induction of the English
Auxiliary System&amp;quot;, in Proceedings of the 23rd An-
nual Meeting of the Association for Computational
Linguistics, Chicago, 1985.
[19] Pinker, Steven, Language Learnability and Lan-
guage Development, Harvard University Press, Cam
bridge, 1984
[20] Pustejovsky, James, &amp;quot;A Theory of Lexical Seman-
tics for Concept Acqusition in Natural Language&amp;quot;,
to appear in International Journal of Intelligent Systems
[21] Pustejovsky, James and Sabine Bergler, &amp;quot;On the
Acquisition of the Conceptual Lexicon&amp;quot;, paper sub-
mitted to AAAI-1987, Seattle, WA.
[22] Slobin , D. &amp;quot;Universals and Particulars in Lan-
guage Acqusition&amp;quot; , in Gleitmann, Language Ac-
quisition, Cambridge, 1982
[23] Waltz, David &amp;quot;Understanding line drawings of sce-
nces with shadows,&amp;quot; in The Psychology of Com-
puter Vision, P. Winston ed. New York, McGraw-
Hill, pp. 19-92.
[24] Waltz, David &amp;quot;Event Space Descriptions,&amp;quot; Pro-
ceedings of the AAAI-82, 1982
[25] Williams, Edwin, &amp;quot;Predication&amp;quot;, Linguistic Inquiry,
1980
[26] Winston, Patrick H., &amp;quot;Learning by Augmenting
Rules and Accumulating Censors,&amp;quot; in Michalski et
al, 1986.
[27] Winston, Patrick, Binford, Katz, and Lowry, &amp;quot;Learn
ing Physical Descriptions from Functional Defini-
tions, Examples, and Precedents, Proceedings of
AAAI, Washington, 1983
</reference>
<page confidence="0.997186">
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803895">
<title confidence="0.9997805">On the Acquisition of Lexical Entries: The Perceptual Origin of Thematic Relations</title>
<author confidence="0.999759">James Pustejovsky</author>
<affiliation confidence="0.999752">Department of Computer Science Brandeis University</affiliation>
<address confidence="0.999986">Waltham, MA 02254</address>
<phone confidence="0.980695">617-736-2709</phone>
<email confidence="0.985393">jamesp@brandeis.csnet-relay</email>
<abstract confidence="0.984544818181818">This paper describes a computational model of concept acquisition for natural language. We develop a theory lexical semantics, the Aspect Calculus, together with a &amp;quot;markedness theory&amp;quot; for thematic relations, constrains what a possible word meaning can be. This is based on the supposition that predicates from the perceptual domain are the primitives for more abstract relations. We then describe an implementation of this model, TULLY, which mirrors the stages of lexical acquisition for children.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>D Angluin</author>
</authors>
<title>Inductive Inference of formal Languages from positive data.&amp;quot;</title>
<journal>Information and Control</journal>
<pages>45--117</pages>
<marker>[1]</marker>
<rawString>Angluin, D. &amp;quot;Inductive Inference of formal Languages from positive data.&amp;quot; Information and Control 45:117-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
</authors>
<title>The Acquisition of Syntactic Information,</title>
<date>1985</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>[2]</marker>
<rawString>Berwick, Robert C. The Acquisition of Syntactic Information, MIT Press, Cambridge, MA. 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
</authors>
<title>Learning from Positive-Only Examples: The Subset Principle and Three Case Studies,&amp;quot;</title>
<date>1986</date>
<note>in Michalski et al,</note>
<marker>[3]</marker>
<rawString>Berwick, Robert C., &amp;quot;Learning from Positive-Only Examples: The Subset Principle and Three Case Studies,&amp;quot; in Michalski et al, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mellissa Bowerman</author>
</authors>
<title>Learning the Structure of Cau sative Verbs,&amp;quot; in Clark (ed) Papers and reports on child language development,</title>
<date>1974</date>
<tech>No. 8,</tech>
<institution>Stanford University</institution>
<marker>[4]</marker>
<rawString>Bowerman, Mellissa &amp;quot;Learning the Structure of Cau sative Verbs,&amp;quot; in Clark (ed) Papers and reports on child language development, No. 8, Stanford University Committee on Linguistics. 1974</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Rules and Representation,</title>
<date>1980</date>
<publisher>University Press,</publisher>
<location>Columbia</location>
<marker>[5]</marker>
<rawString>Chomsky, Noam Rules and Representation, Columbia University Press, 1980</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Lectures on Government and Binding, Foris,</title>
<date>1981</date>
<location>Holland,</location>
<marker>[6]</marker>
<rawString>Chomsky, Noam Lectures on Government and Binding, Foris, Holland, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
<author>Word Meaning</author>
<author>Montague Grammar</author>
<author>D Reidel</author>
</authors>
<date>1979</date>
<location>Dordrecht, Holland,</location>
<marker>[7]</marker>
<rawString>Dowty, David R., Word Meaning and Montague Grammar, D. Reidel, Dordrecht, Holland, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Language and Cognition,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>[8]</marker>
<rawString>Jackendoff, Ray, Language and Cognition, MIT Press, Cambridge, MA. 1983.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>The Role of Thematic Relations in Linguistic Theory,&amp;quot;, ms.</title>
<institution>Brandeis University,</institution>
<marker>[9]</marker>
<rawString>Jackendoff, Ray, &amp;quot;The Role of Thematic Relations in Linguistic Theory,&amp;quot;, ms. Brandeis University,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yves Kodratoff</author>
<author>J-G Ganascia</author>
</authors>
<title>Improving the Generalization Step in Learning&amp;quot;,</title>
<booktitle>Machine Learning II,</booktitle>
<editor>in Michalskiet el (eds.),</editor>
<publisher>Morgan Kaufmann.</publisher>
<marker>[10]</marker>
<rawString>Kodratoff, Yves, and J-G. Ganascia, &amp;quot;Improving the Generalization Step in Learning&amp;quot;, in Michalskiet el (eds.), Machine Learning II, Morgan Kaufmann.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mitch Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language,</title>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<marker>[11]</marker>
<rawString>Marcus, Mitch, A Theory of Syntactic Recognition for Natural Language, MIT Press, Cambridge,</rawString>
</citation>
<citation valid="false">
<authors>
<author>R S Michalski</author>
</authors>
<title>A Theory and Methodology of Inductive Learning,&amp;quot;,</title>
<booktitle>Machine Learning I.</booktitle>
<editor>in Michalski et al (eds.),</editor>
<marker>[12]</marker>
<rawString>Michalski, R.S., &amp;quot;A Theory and Methodology of Inductive Learning,&amp;quot;, in Michalski et al (eds.), Machine Learning I.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>Dictionaries of the Mind&amp;quot;</title>
<date>1985</date>
<booktitle>in Pro-. ceedings of the 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Chicago,</location>
<marker>[13]</marker>
<rawString>Miller, George, &amp;quot;Dictionaries of the Mind&amp;quot; in Pro-. ceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Philip Johnson-Laird</author>
<author>Language</author>
<author>Belknap Perception</author>
</authors>
<date>1976</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<marker>[14]</marker>
<rawString>Miller, George and Philip Johnson-Laird, Language and Perception, Belknap, Harvard University Press, Cambridge, MA. 1976.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tom Mitchell</author>
</authors>
<title>Version Spaces: A Candidate Elimination Approach to Rule Learning,&amp;quot;</title>
<note>in IJCAI-77,</note>
<marker>[15]</marker>
<rawString>Mitchell, Tom, &amp;quot;Version Spaces: A Candidate Elimination Approach to Rule Learning,&amp;quot; in IJCAI-77,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<title>Version Spaces: An Approach to Concept Learning, Ph.D. thesis Stanford,</title>
<date>1978</date>
<marker>[16]</marker>
<rawString>Mitchell, Tom, Version Spaces: An Approach to Concept Learning, Ph.D. thesis Stanford, 1978.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Carolyn Nygren</author>
</authors>
<title>Results of Experiments with Instrumentals,&amp;quot; ms.</title>
<publisher>UMASS,</publisher>
<location>Amherst, MA.</location>
<marker>[17]</marker>
<rawString>Nygren, Carolyn, &amp;quot;Results of Experiments with Instrumentals,&amp;quot; ms. UMASS, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel F Pilato</author>
<author>Robert C Berwick</author>
</authors>
<title>Reversible Automata and Induction of the English Auxiliary System&amp;quot;,</title>
<date>1985</date>
<booktitle>in Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Chicago,</location>
<marker>[18]</marker>
<rawString>Pilato, Samuel F. and Robert C. Berwick, &amp;quot;Reversible Automata and Induction of the English Auxiliary System&amp;quot;, in Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>Language Learnability and Language Development,</title>
<date>1984</date>
<publisher>Harvard University Press,</publisher>
<location>Cam bridge,</location>
<marker>[19]</marker>
<rawString>Pinker, Steven, Language Learnability and Language Development, Harvard University Press, Cam bridge, 1984</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>A Theory of Lexical Semantics for Concept Acqusition in Natural Language&amp;quot;, to appear in</title>
<journal>International Journal of Intelligent Systems</journal>
<marker>[20]</marker>
<rawString>Pustejovsky, James, &amp;quot;A Theory of Lexical Semantics for Concept Acqusition in Natural Language&amp;quot;, to appear in International Journal of Intelligent Systems</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Pustejovsky</author>
<author>Sabine Bergler</author>
</authors>
<title>On the Acquisition of the Conceptual Lexicon&amp;quot;, paper submitted to AAAI-1987,</title>
<location>Seattle, WA.</location>
<marker>[21]</marker>
<rawString>Pustejovsky, James and Sabine Bergler, &amp;quot;On the Acquisition of the Conceptual Lexicon&amp;quot;, paper submitted to AAAI-1987, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slobin</author>
</authors>
<title>Universals and Particulars in Language Acqusition&amp;quot; , in Gleitmann, Language Acquisition,</title>
<date>1982</date>
<location>Cambridge,</location>
<marker>[22]</marker>
<rawString>Slobin , D. &amp;quot;Universals and Particulars in Language Acqusition&amp;quot; , in Gleitmann, Language Acquisition, Cambridge, 1982</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Waltz</author>
</authors>
<title>Understanding line drawings of scences with shadows,&amp;quot;</title>
<booktitle>in The Psychology of Computer Vision,</booktitle>
<pages>19--92</pages>
<editor>P. Winston ed.</editor>
<location>New York, McGrawHill,</location>
<marker>[23]</marker>
<rawString>Waltz, David &amp;quot;Understanding line drawings of scences with shadows,&amp;quot; in The Psychology of Computer Vision, P. Winston ed. New York, McGrawHill, pp. 19-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Waltz</author>
</authors>
<title>Event Space Descriptions,&amp;quot;</title>
<date>1982</date>
<booktitle>Proceedings of the AAAI-82,</booktitle>
<marker>[24]</marker>
<rawString>Waltz, David &amp;quot;Event Space Descriptions,&amp;quot; Proceedings of the AAAI-82, 1982</rawString>
</citation>
<citation valid="false">
<authors>
<author>Edwin Williams</author>
</authors>
<title>Predication&amp;quot;, Linguistic Inquiry,</title>
<marker>[25]</marker>
<rawString>Williams, Edwin, &amp;quot;Predication&amp;quot;, Linguistic Inquiry,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick H Winston</author>
</authors>
<title>Learning by Augmenting Rules and Accumulating Censors,&amp;quot;</title>
<date>1986</date>
<note>in Michalski et al,</note>
<marker>[26]</marker>
<rawString>Winston, Patrick H., &amp;quot;Learning by Augmenting Rules and Accumulating Censors,&amp;quot; in Michalski et al, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Winston</author>
<author>Katz Binford</author>
<author>Lowry</author>
</authors>
<title>Learn ing Physical Descriptions from Functional Definitions, Examples, and Precedents,</title>
<date>1983</date>
<booktitle>Proceedings of AAAI,</booktitle>
<location>Washington,</location>
<marker>[27]</marker>
<rawString>Winston, Patrick, Binford, Katz, and Lowry, &amp;quot;Learn ing Physical Descriptions from Functional Definitions, Examples, and Precedents, Proceedings of AAAI, Washington, 1983</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>