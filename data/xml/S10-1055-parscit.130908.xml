<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009091">
<title confidence="0.9935265">
HUMB: Automatic Key Term Extraction from Scientific Articles
in GROBID
</title>
<author confidence="0.903343">
Patrice Lopez
</author>
<affiliation confidence="0.74961">
INRIA
</affiliation>
<address confidence="0.795833">
Berlin, Germany
</address>
<email confidence="0.992032">
patrice lopez@hotmail.com
</email>
<sectionHeader confidence="0.993582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999218523809524">
The Semeval task 5 was an opportunity
for experimenting with the key term ex-
traction module of GROBID, a system for
extracting and generating bibliographical
information from technical and scientific
documents. The tool first uses GROBID’s
facilities for analyzing the structure of sci-
entific articles, resulting in a first set of
structural features. A second set of fea-
tures captures content properties based on
phraseness, informativeness and keyword-
ness measures. Two knowledge bases,
GRISP and Wikipedia, are then exploited
for producing a last set of lexical/semantic
features. Bagged decision trees appeared
to be the most efficient machine learning
algorithm for generating a list of ranked
key term candidates. Finally a post rank-
ing was realized based on statistics of co-
usage of keywords in HAL, a large Open
Access publication repository.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.931075307692308">
Key terms (or keyphrases or keywords) are meta-
data providing general information about the con-
tent of a document. Their selection by authors
or readers is, to a large extent, subjective which
makes automatic extraction difficult. This is, how-
ever, a valuable exercise, because such key terms
constitute good topic descriptions of documents
which can be used in particular for information
retrieval, automatic document clustering and clas-
sification. Used as subject headings, better key-
words can lead to higher retrieval rates of an arti-
cle in a digital library.
We view automatic key term extraction as a sub-
task of the general problem of extraction of tech-
nical terms which is crucial in technical and scien-
tific documents (Ahmad and Collingham, 1996).
Laurent Romary
INRIA &amp; HUB-IDSL
Berlin, Germany
laurent.romary@inria.fr
Among the extracted terms for a given scientific
document in a given collection, which key terms
best characterize this document?
This article describes the system realized for
the Semeval 2010 task 5, based on GROBID’s
(GeneRation Of BIbilographic Data) module ded-
icated to key term extraction. GROBID is a tool
for analyzing technical and scientific documents,
focusing on automatic bibliographical data extrac-
tion (header, citations, etc.) (Lopez, 2009) and
structure recognition (section titles, figures, etc).
As the space for the system description is very
limited, this presentation focuses on key aspects.
We present first an overview of our approach, then
our selection of features (section 3), the different
tested machine learning models (section 4) and the
final post-ranking (section 5). We briefly describe
our unsuccessful experiments (section 6) and we
conclude by discussing future works.
</bodyText>
<sectionHeader confidence="0.980837" genericHeader="introduction">
2 Bases
</sectionHeader>
<bodyText confidence="0.99449625">
Principle As most of the successful works for
keyphrase extraction, our approach relies on Ma-
chine Learning (ML). The following steps are ap-
plied to each document to be processed:
</bodyText>
<listItem confidence="0.995452857142857">
1. Analysis of the structure of the article.
2. Selection of candidate terms.
3. Calculation of features.
4. Application of a ML model for evaluating
each candidate term independently.
5. Final re-ranking for capturing relationships
between the term candidates.
</listItem>
<bodyText confidence="0.999741833333333">
For creating the ML model, steps 1-3 are applied
to the articles of the training set. We view steps 1
and 5 as our main novel contributions. The struc-
ture analysis permits the usage of reliable features
in relation to the logical composition of the arti-
cle to be processed. The final re-ranking exploits
</bodyText>
<page confidence="0.959907">
248
</page>
<bodyText confidence="0.888456625">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 248–251,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
general relationships between the set of candidates
which cannot be captured by the ML models.
Candidate term selection In the following,
word should be understood as similar to token in
the sense of MAF1. Step 2 has been implemented
in a standard manner, as follows:
</bodyText>
<listItem confidence="0.997347857142857">
1. Extract all n-grams up to 5 words,
2. Remove all candidate n-grams starting or
ending with a stop word,
3. Filter from these candidates terms having
mathematical symbols,
4. Normalize each candidate by lowercasing
and by stemming using the Porter stemmer.
</listItem>
<bodyText confidence="0.840725333333333">
Training data The task’s collection consists of
articles from the ACM (Association for Computa-
tional Machinery) in four narrow domains (C.2.4
Distributed Systems, H.3.3 Information Search
and Retrieval, I.2.6 Learning and J.4 Social and
Behavioral Sciences). As training data, we used
this task’s training resources (144 articles from
ACM) and the National University of Singapore
(NUS) corpus2 (156 ACM articles from all com-
puting domains). Adding the additional NUS
training data improved our final results (+7.4%
for the F-score at top 15, i.e. from 25.6 to 27.5).
</bodyText>
<sectionHeader confidence="0.99986" genericHeader="method">
3 Features
</sectionHeader>
<subsectionHeader confidence="0.999885">
3.1 Structural features
</subsectionHeader>
<bodyText confidence="0.999983058823529">
One of the goals of GROBID is to realize reli-
able conversions of technical and scientific docu-
ments in PDF to fully compliant TEI3 documents.
This conversion implies first the recognition of
the different sections of the document, then the
extraction of all header metadata and references.
The analysis is realized in GROBID with Condi-
tional Random Fields (CRF) (Peng and McCal-
lum, 2004) exploiting a large amount of training
data. We added to this training set a few ACM doc-
uments manually annotated and obtained a very
high performance for field recognitions, between
97% (section titles, reference titles) and 99% (ti-
tle, abstract) accuracy for the task’s collection.
Authors commonly introduce the main concepts
of a written communication in the header (title,
abstract, table of contents), the introduction, the
</bodyText>
<footnote confidence="0.9996905">
1Morpho-syntactic Annotation Framework, see
http://pauillac.inria.fr/ clerger/MAF/
2http://wing.comp.nus.edu.sg/downloads/keyphraseCorpus
3Text Encoding Initiative (TEI), http://www.tei-c.org.
</footnote>
<bodyText confidence="0.999933533333333">
section titles, the conclusion and the reference list.
Similarly human readers/annotators typically fo-
cus their attention on the same document parts.
We introduced thus the following 6 binary fea-
tures characterizing the position of a term with re-
spect to the document structure for each candidate:
present in the title, in the abstract, in the introduc-
tion, in at least one section titles, in the conclusion,
in at least one reference or book title.
In addition, we used the following standard fea-
ture: the position of the first occurrence, calcu-
lated as the number of words which precede the
first occurrence of the term divided by the num-
ber of words in the document, similarly as, for in-
stance, (Witten et al., 1999).
</bodyText>
<subsectionHeader confidence="0.998958">
3.2 Content features
</subsectionHeader>
<bodyText confidence="0.999963944444444">
The second set of features used in this work tries
to captures distributional properties of a term rel-
atively to the overall textual content of the docu-
ment where the term appears or the collection.
Phraseness The phraseness measures the lexical
cohesion of a sequence of words in a given docu-
ment, i.e. the degree to which it can be consid-
ered as a phrase. This measure is classically used
for term extraction and can rely on different tech-
niques, usually evaluating the ability of a sequence
of words to appear as a stable phrase more often
than just by chance. We applied here the Gen-
eralized Dice Coeficient (GDC) as introduced by
(Park et al., 2002), applicable to any arbitrary n-
gram of words (n ≥ 2). For a given term T,  |T |
being the number of words in T, freq(T) the fre-
quency of occurrence of T and freq(wi) the fre-
quency of occurrence of the word wi, we have:
</bodyText>
<equation confidence="0.99935275">
GDC(T) =  |T  |log10(freq(T))freq(T)
�
freq(wi)
wi∈T
</equation>
<bodyText confidence="0.9998953">
We used a default value for a single word, because,
in this case, the association measure is not mean-
ingful as it depends only on the frequency.
Informativeness The informativeness of a term
is the degree to which the term is representative of
a document given a collection of documents. Once
again many measures can be relevant, and we opt
for the standard TF-IDF value which is used in
most of the keyphrase extraction systems, see for
instance (Witten et al., 1999) or (Medelyan and
</bodyText>
<page confidence="0.991927">
249
</page>
<bodyText confidence="0.6995435">
Witten, 2008). The TF-IDF score for a Term T in
document D is given by:
</bodyText>
<equation confidence="0.972007333333333">
TF-IDF(T, D) = freq(T, D)
 |D  |x −log2count(T)
N
</equation>
<bodyText confidence="0.999984071428571">
where  |D  |is the number of words in D,
count(T) is the number of occurrence of the term
T in the global corpus, and N is the number of doc-
uments in the corpus.
Keywordness Introduced by (Witten et al.,
1999), the keywordness reflects the degree to
which a term is selected as a keyword. In prac-
tice, it is simply the frequency of the keyword in
the global corpus. The efficiency of this feature
depends, however, on the amount of training data
available and the variety of technical domains con-
sidered. As the training set of documents for this
task is relatively large and narrow in term of tech-
nical domains, this feature was relevant.
</bodyText>
<subsectionHeader confidence="0.980613">
3.3 Lexical/Semantic features
</subsectionHeader>
<bodyText confidence="0.99996545">
GRISP is a large scale terminological database
for technical and scientific domains resulting from
the fusion of terminological resources (MeSH, the
Gene Ontology, etc.), linguistic resources (part of
WordNet) and part of Wikipedia. It has been cre-
ated for improving patent retrieval and classifica-
tion (Lopez and Romary, 2010). The assumption
is that a phrase which has been identified as con-
trolled term in these resources tend to be a more
important keyphrase. A binary feature is used to
indicate if the term is part of GRISP or not.
We use Wikipedia similarly as the Wikipedia
keyphraseness in Maui (Medelyan, 2009). The
Wikipedia keyphraseness of a term T is the prob-
ability of an appearance of T in a document being
an anchor (Medelyan, 2009). We use Wikipedia
Miner4 for obtaining this value.
Finally we introduced an additional feature
commonly used in keyword extraction, the length
of the term candidate, i.e. its number of words.
</bodyText>
<sectionHeader confidence="0.998734" genericHeader="method">
4 Machine learning model
</sectionHeader>
<bodyText confidence="0.999068166666667">
We experimented different ML models: Decision
tree (C4.5), Multi-Layer perceptron (MLP) and
Support Vector Machine (SVM). In addition, we
combined these models with boosting and bagging
techniques. We used WEKA (Witten and Frank,
2005) for all our experiments, except for SVM
</bodyText>
<footnote confidence="0.754996">
4http://wikipedia-miner.sourceforge.net
</footnote>
<bodyText confidence="0.999950153846154">
where LIBSVM (Chang and Lin, 2001) was used.
We failed to obtain reasonable results with SVM.
Our hypothesis is that SVM is sensitive to the very
large number of negative examples compared to
the positive ones and additional techniques should
be used for balancing the training data. Results
for decision tree and MLP were similar but the lat-
ter is approx. 57 times more time-consuming for
training. Bagged decision tree appeared to per-
form constantly better than boosting (+8,4% for
F-score). The selected model for the final run was,
therefore, bagged decision tree, similarly as, for
instance, in (Medelyan, 2009).
</bodyText>
<sectionHeader confidence="0.725323" genericHeader="method">
5 Post-ranking
</sectionHeader>
<bodyText confidence="0.999579833333333">
Post-ranking uses the selected candidates as a
whole for improving the results, while in the pre-
vious step, each candidate was selected indepen-
dently from the other. If we have a ranked list of
term T1−N, each having a score s(Ti), the new
score s&apos; for the term Ti is obtained as follow:
</bodyText>
<equation confidence="0.996031">
s&apos;(Ti) = s(Ti) + α−1 P(Tj|Ti)s(Tj)
j7�i
</equation>
<bodyText confidence="0.999983533333333">
where α is a constant in [0 − 1] for control-
ling the re-ranking factor. α has been set ex-
perimentally to 0.8. P(Tj|Ti) is the probability
that the keyword Tj is chosen by the author when
the keyword Ti has been selected. For obtain-
ing these probabilities, we use statistics for the
HAL5 research archive. HAL contains approx.
139,000 full texts articles described by a rich set of
metadata, often including author’s keywords. We
use the keywords appearing in English and in the
Computer Science domain (a subset of 29,000 ar-
ticles), corresponding to a total of 16,412 different
keywords. No smoothing was used. The usage of
open publication repository as a research resource
is in its infancy and very promising.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999264166666667">
Our system was ranked first of the competition
among 19 participants. Table 1 presents our offi-
cial results (Precision, Recall, F-score) for com-
bined keywords and reader keywords, together
with the scores of the systems ranked second
(WINGNUS and KX FBK).
</bodyText>
<footnote confidence="0.993336666666667">
5HAL (Hyper Article en Ligne) is the French Institutional
repository for research publications: http://hal.archives-
ouvertes.fr/index.php?langue=en
</footnote>
<page confidence="0.974804">
250
</page>
<table confidence="0.997161">
Set System top 5 top 10 top 15
Comb. HUMB P:39.0 R:13.3 F:19.8 F:32.0 R:21.8 F:25.9 P:27.2 R:27.8 F:27.5
WINGNUS P:40.2 R:13.7 F:20.5 P:30.5 R:20.8 F:24.7 P:24.9 R:25.5 F:25.2
Reader HUMB P:30.4 R:12.6 F:17.8 P:24.8 R:20.6 F:22.5 P:21.2 R:26.4 F:23.5
KX FBK P:29.2 R:12.1 F:17.1 P:23.2 R:19.3 F:21.1 P:20.3 R:25.3 F:22.6
</table>
<tableCaption confidence="0.999774">
Table 1: Performance of our system (HUMB) and of the systems ranked second.
</tableCaption>
<sectionHeader confidence="0.862779" genericHeader="method">
7 What did not work
</sectionHeader>
<bodyText confidence="0.99995008">
The previously described features were selected
because they all had a positive impact on the ex-
traction accuracy based on our experiments on the
task’s collection. The following intuitively perti-
nent ideas appeared, however, to deteriorate or to
be neutral for the results.
Noun phrase filtering We applied a filtering of
noun phrases based on a POS tagging and extrac-
tion of all possible NP based on typical patterns.
This filtering lowered both the recall and the pre-
cision (−7.6% for F-score at top 15).
Term variants We tried to apply a post-ranking
by conflating term variants using FASTR6, result-
ing in a disappointing −11.5% for the F-score.
Global keywordness We evaluated the key-
wordness using also the overall HAL keyword fre-
quencies rather than only the training corpus. It
had no impact on the results.
Language Model deviation We experimented
the usage of HMM deviation using LingPipe7 as
alternative informativeness measure, resulting in
−3.7% for the F-score at top 15.
Wikipedia term Relatedness Using Wikipedia
Miner, we tried to apply as post-ranking a boosting
of related terms, but saw no impact on the results.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="discussions">
8 Future work
</sectionHeader>
<bodyText confidence="0.999966777777778">
We think that automatic key term extraction can
be highly valuable for assisting self-archiving of
research papers by authors in scholarly reposito-
ries such as arXiv or HAL. We plan to experiment
keyword suggestions in HAL based on the present
system. Many archived research papers are cur-
rently not associated with any keyword.
We also plan to adapt our module to a large col-
lection of approx. 2.6 million patent documents in
</bodyText>
<footnote confidence="0.99978">
6http://perso.limsi.fr/jacquemi/FASTR
7http://alias-i.com/lingpipe
</footnote>
<bodyText confidence="0.998194">
the context of CLEF IP 2010. This will be the op-
portunity to evaluate the relevance of the extracted
key terms for large scale topic-based IR.
</bodyText>
<sectionHeader confidence="0.999182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99898215">
K. Ahmad and S. Collingham. 1996. Pointer project
final report. Technical report, University of Surrey.
http://www.computing.surrey.ac.uk/ai/pointer/report.
C.-C. Chang and C.-J. Lin. 2001. Libsvm: a library
for support vector machines. Technical report.
P. Lopez and L. Romary. 2010. GRISP: A Massive
Multilingual Terminological Database for Scientific
and Technical Domains. In Seventh international
conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
P. Lopez. 2009. GROBID: Combining Automatic
Bibliographic Data Recognition and Term Extrac-
tion for Scholarship Publications. In Proceedings of
ECDL 2009, 13th European Conference on Digital
Library, Corfu, Greece.
O. Medelyan and I.H. Witten. 2008. Domain-
independent automatic keyphrase indexing with
small training sets. Journal of the American
Society for Information Science and Technology,
59(7):1026–1040.
O. Medelyan. 2009. Human-competitive automatic
topic indexing. Ph.D. thesis.
Y. Park, R.J. Byrd, and B.K. Boguraev. 2002. Auto-
matic glossary extraction: beyond terminology iden-
tification. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1–7. Association for Computational Linguis-
tics.
F. Peng and A. McCallum. 2004. Accurate infor-
mation extraction from research papers using con-
ditional random fields. In Proceedings of HLT-
NAACL, Boston, USA.
I.H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition edition.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. KEA: Practical auto-
matic keyphrase extraction. In Proceedings of the
fourth ACM conference on Digital libraries, page
255. ACM.
</reference>
<page confidence="0.997756">
251
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.667350">
<title confidence="0.9933655">HUMB: Automatic Key Term Extraction from Scientific Articles in GROBID</title>
<author confidence="0.998898">Patrice Lopez</author>
<affiliation confidence="0.991025">INRIA</affiliation>
<address confidence="0.939696">Berlin, Germany</address>
<email confidence="0.987393">patricelopez@hotmail.com</email>
<abstract confidence="0.986379045454545">The Semeval task 5 was an opportunity for experimenting with the key term extraction module of GROBID, a system for extracting and generating bibliographical information from technical and scientific documents. The tool first uses GROBID’s facilities for analyzing the structure of scientific articles, resulting in a first set of structural features. A second set of features captures content properties based on phraseness, informativeness and keywordness measures. Two knowledge bases, GRISP and Wikipedia, are then exploited for producing a last set of lexical/semantic features. Bagged decision trees appeared to be the most efficient machine learning algorithm for generating a list of ranked key term candidates. Finally a post ranking was realized based on statistics of cousage of keywords in HAL, a large Open Access publication repository.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Ahmad</author>
<author>S Collingham</author>
</authors>
<title>Pointer project final report.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>University of Surrey.</institution>
<note>http://www.computing.surrey.ac.uk/ai/pointer/report.</note>
<contexts>
<context position="1760" citStr="Ahmad and Collingham, 1996" startWordPosition="269" endWordPosition="272">tion by authors or readers is, to a large extent, subjective which makes automatic extraction difficult. This is, however, a valuable exercise, because such key terms constitute good topic descriptions of documents which can be used in particular for information retrieval, automatic document clustering and classification. Used as subject headings, better keywords can lead to higher retrieval rates of an article in a digital library. We view automatic key term extraction as a subtask of the general problem of extraction of technical terms which is crucial in technical and scientific documents (Ahmad and Collingham, 1996). Laurent Romary INRIA &amp; HUB-IDSL Berlin, Germany laurent.romary@inria.fr Among the extracted terms for a given scientific document in a given collection, which key terms best characterize this document? This article describes the system realized for the Semeval 2010 task 5, based on GROBID’s (GeneRation Of BIbilographic Data) module dedicated to key term extraction. GROBID is a tool for analyzing technical and scientific documents, focusing on automatic bibliographical data extraction (header, citations, etc.) (Lopez, 2009) and structure recognition (section titles, figures, etc). As the spac</context>
</contexts>
<marker>Ahmad, Collingham, 1996</marker>
<rawString>K. Ahmad and S. Collingham. 1996. Pointer project final report. Technical report, University of Surrey. http://www.computing.surrey.ac.uk/ai/pointer/report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2001</date>
<tech>Technical report.</tech>
<contexts>
<context position="10076" citStr="Chang and Lin, 2001" startWordPosition="1614" endWordPosition="1617"> of T in a document being an anchor (Medelyan, 2009). We use Wikipedia Miner4 for obtaining this value. Finally we introduced an additional feature commonly used in keyword extraction, the length of the term candidate, i.e. its number of words. 4 Machine learning model We experimented different ML models: Decision tree (C4.5), Multi-Layer perceptron (MLP) and Support Vector Machine (SVM). In addition, we combined these models with boosting and bagging techniques. We used WEKA (Witten and Frank, 2005) for all our experiments, except for SVM 4http://wikipedia-miner.sourceforge.net where LIBSVM (Chang and Lin, 2001) was used. We failed to obtain reasonable results with SVM. Our hypothesis is that SVM is sensitive to the very large number of negative examples compared to the positive ones and additional techniques should be used for balancing the training data. Results for decision tree and MLP were similar but the latter is approx. 57 times more time-consuming for training. Bagged decision tree appeared to perform constantly better than boosting (+8,4% for F-score). The selected model for the final run was, therefore, bagged decision tree, similarly as, for instance, in (Medelyan, 2009). 5 Post-ranking P</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C. Chang and C.-J. Lin. 2001. Libsvm: a library for support vector machines. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lopez</author>
<author>L Romary</author>
</authors>
<title>GRISP: A Massive Multilingual Terminological Database for Scientific and Technical Domains.</title>
<date>2010</date>
<booktitle>In Seventh international conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="9086" citStr="Lopez and Romary, 2010" startWordPosition="1456" endWordPosition="1459">ciency of this feature depends, however, on the amount of training data available and the variety of technical domains considered. As the training set of documents for this task is relatively large and narrow in term of technical domains, this feature was relevant. 3.3 Lexical/Semantic features GRISP is a large scale terminological database for technical and scientific domains resulting from the fusion of terminological resources (MeSH, the Gene Ontology, etc.), linguistic resources (part of WordNet) and part of Wikipedia. It has been created for improving patent retrieval and classification (Lopez and Romary, 2010). The assumption is that a phrase which has been identified as controlled term in these resources tend to be a more important keyphrase. A binary feature is used to indicate if the term is part of GRISP or not. We use Wikipedia similarly as the Wikipedia keyphraseness in Maui (Medelyan, 2009). The Wikipedia keyphraseness of a term T is the probability of an appearance of T in a document being an anchor (Medelyan, 2009). We use Wikipedia Miner4 for obtaining this value. Finally we introduced an additional feature commonly used in keyword extraction, the length of the term candidate, i.e. its nu</context>
</contexts>
<marker>Lopez, Romary, 2010</marker>
<rawString>P. Lopez and L. Romary. 2010. GRISP: A Massive Multilingual Terminological Database for Scientific and Technical Domains. In Seventh international conference on Language Resources and Evaluation (LREC), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lopez</author>
</authors>
<title>GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications.</title>
<date>2009</date>
<booktitle>In Proceedings of ECDL 2009, 13th European Conference on Digital Library, Corfu,</booktitle>
<contexts>
<context position="2290" citStr="Lopez, 2009" startWordPosition="347" endWordPosition="348">hich is crucial in technical and scientific documents (Ahmad and Collingham, 1996). Laurent Romary INRIA &amp; HUB-IDSL Berlin, Germany laurent.romary@inria.fr Among the extracted terms for a given scientific document in a given collection, which key terms best characterize this document? This article describes the system realized for the Semeval 2010 task 5, based on GROBID’s (GeneRation Of BIbilographic Data) module dedicated to key term extraction. GROBID is a tool for analyzing technical and scientific documents, focusing on automatic bibliographical data extraction (header, citations, etc.) (Lopez, 2009) and structure recognition (section titles, figures, etc). As the space for the system description is very limited, this presentation focuses on key aspects. We present first an overview of our approach, then our selection of features (section 3), the different tested machine learning models (section 4) and the final post-ranking (section 5). We briefly describe our unsuccessful experiments (section 6) and we conclude by discussing future works. 2 Bases Principle As most of the successful works for keyphrase extraction, our approach relies on Machine Learning (ML). The following steps are appl</context>
</contexts>
<marker>Lopez, 2009</marker>
<rawString>P. Lopez. 2009. GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications. In Proceedings of ECDL 2009, 13th European Conference on Digital Library, Corfu, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
</authors>
<title>Domainindependent automatic keyphrase indexing with small training sets.</title>
<date>2008</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>59</volume>
<issue>7</issue>
<marker>Medelyan, Witten, 2008</marker>
<rawString>O. Medelyan and I.H. Witten. 2008. Domainindependent automatic keyphrase indexing with small training sets. Journal of the American Society for Information Science and Technology, 59(7):1026–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
</authors>
<title>Human-competitive automatic topic indexing.</title>
<date>2009</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="9379" citStr="Medelyan, 2009" startWordPosition="1510" endWordPosition="1511"> is a large scale terminological database for technical and scientific domains resulting from the fusion of terminological resources (MeSH, the Gene Ontology, etc.), linguistic resources (part of WordNet) and part of Wikipedia. It has been created for improving patent retrieval and classification (Lopez and Romary, 2010). The assumption is that a phrase which has been identified as controlled term in these resources tend to be a more important keyphrase. A binary feature is used to indicate if the term is part of GRISP or not. We use Wikipedia similarly as the Wikipedia keyphraseness in Maui (Medelyan, 2009). The Wikipedia keyphraseness of a term T is the probability of an appearance of T in a document being an anchor (Medelyan, 2009). We use Wikipedia Miner4 for obtaining this value. Finally we introduced an additional feature commonly used in keyword extraction, the length of the term candidate, i.e. its number of words. 4 Machine learning model We experimented different ML models: Decision tree (C4.5), Multi-Layer perceptron (MLP) and Support Vector Machine (SVM). In addition, we combined these models with boosting and bagging techniques. We used WEKA (Witten and Frank, 2005) for all our exper</context>
<context position="10658" citStr="Medelyan, 2009" startWordPosition="1710" endWordPosition="1711">here LIBSVM (Chang and Lin, 2001) was used. We failed to obtain reasonable results with SVM. Our hypothesis is that SVM is sensitive to the very large number of negative examples compared to the positive ones and additional techniques should be used for balancing the training data. Results for decision tree and MLP were similar but the latter is approx. 57 times more time-consuming for training. Bagged decision tree appeared to perform constantly better than boosting (+8,4% for F-score). The selected model for the final run was, therefore, bagged decision tree, similarly as, for instance, in (Medelyan, 2009). 5 Post-ranking Post-ranking uses the selected candidates as a whole for improving the results, while in the previous step, each candidate was selected independently from the other. If we have a ranked list of term T1−N, each having a score s(Ti), the new score s&apos; for the term Ti is obtained as follow: s&apos;(Ti) = s(Ti) + α−1 P(Tj|Ti)s(Tj) j7�i where α is a constant in [0 − 1] for controlling the re-ranking factor. α has been set experimentally to 0.8. P(Tj|Ti) is the probability that the keyword Tj is chosen by the author when the keyword Ti has been selected. For obtaining these probabilities,</context>
</contexts>
<marker>Medelyan, 2009</marker>
<rawString>O. Medelyan. 2009. Human-competitive automatic topic indexing. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Park</author>
<author>R J Byrd</author>
<author>B K Boguraev</author>
</authors>
<title>Automatic glossary extraction: beyond terminology identification.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7212" citStr="Park et al., 2002" startWordPosition="1125" endWordPosition="1128">his work tries to captures distributional properties of a term relatively to the overall textual content of the document where the term appears or the collection. Phraseness The phraseness measures the lexical cohesion of a sequence of words in a given document, i.e. the degree to which it can be considered as a phrase. This measure is classically used for term extraction and can rely on different techniques, usually evaluating the ability of a sequence of words to appear as a stable phrase more often than just by chance. We applied here the Generalized Dice Coeficient (GDC) as introduced by (Park et al., 2002), applicable to any arbitrary ngram of words (n ≥ 2). For a given term T, |T | being the number of words in T, freq(T) the frequency of occurrence of T and freq(wi) the frequency of occurrence of the word wi, we have: GDC(T) = |T |log10(freq(T))freq(T) � freq(wi) wi∈T We used a default value for a single word, because, in this case, the association measure is not meaningful as it depends only on the frequency. Informativeness The informativeness of a term is the degree to which the term is representative of a document given a collection of documents. Once again many measures can be relevant, a</context>
</contexts>
<marker>Park, Byrd, Boguraev, 2002</marker>
<rawString>Y. Park, R.J. Byrd, and B.K. Boguraev. 2002. Automatic glossary extraction: beyond terminology identification. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>A McCallum</author>
</authors>
<title>Accurate information extraction from research papers using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="5193" citStr="Peng and McCallum, 2004" startWordPosition="803" endWordPosition="807">y of Singapore (NUS) corpus2 (156 ACM articles from all computing domains). Adding the additional NUS training data improved our final results (+7.4% for the F-score at top 15, i.e. from 25.6 to 27.5). 3 Features 3.1 Structural features One of the goals of GROBID is to realize reliable conversions of technical and scientific documents in PDF to fully compliant TEI3 documents. This conversion implies first the recognition of the different sections of the document, then the extraction of all header metadata and references. The analysis is realized in GROBID with Conditional Random Fields (CRF) (Peng and McCallum, 2004) exploiting a large amount of training data. We added to this training set a few ACM documents manually annotated and obtained a very high performance for field recognitions, between 97% (section titles, reference titles) and 99% (title, abstract) accuracy for the task’s collection. Authors commonly introduce the main concepts of a written communication in the header (title, abstract, table of contents), the introduction, the 1Morpho-syntactic Annotation Framework, see http://pauillac.inria.fr/ clerger/MAF/ 2http://wing.comp.nus.edu.sg/downloads/keyphraseCorpus 3Text Encoding Initiative (TEI),</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>F. Peng and A. McCallum. 2004. Accurate information extraction from research papers using conditional random fields. In Proceedings of HLTNAACL, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San</location>
<contexts>
<context position="9961" citStr="Witten and Frank, 2005" startWordPosition="1600" endWordPosition="1603">ia keyphraseness in Maui (Medelyan, 2009). The Wikipedia keyphraseness of a term T is the probability of an appearance of T in a document being an anchor (Medelyan, 2009). We use Wikipedia Miner4 for obtaining this value. Finally we introduced an additional feature commonly used in keyword extraction, the length of the term candidate, i.e. its number of words. 4 Machine learning model We experimented different ML models: Decision tree (C4.5), Multi-Layer perceptron (MLP) and Support Vector Machine (SVM). In addition, we combined these models with boosting and bagging techniques. We used WEKA (Witten and Frank, 2005) for all our experiments, except for SVM 4http://wikipedia-miner.sourceforge.net where LIBSVM (Chang and Lin, 2001) was used. We failed to obtain reasonable results with SVM. Our hypothesis is that SVM is sensitive to the very large number of negative examples compared to the positive ones and additional techniques should be used for balancing the training data. Results for decision tree and MLP were similar but the latter is approx. 57 times more time-consuming for training. Bagged decision tree appeared to perform constantly better than boosting (+8,4% for F-score). The selected model for th</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I.H. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco, 2nd edition edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>G W Paynter</author>
<author>E Frank</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>KEA: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the fourth ACM conference on Digital libraries,</booktitle>
<pages>255</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6535" citStr="Witten et al., 1999" startWordPosition="1005" endWordPosition="1008"> focus their attention on the same document parts. We introduced thus the following 6 binary features characterizing the position of a term with respect to the document structure for each candidate: present in the title, in the abstract, in the introduction, in at least one section titles, in the conclusion, in at least one reference or book title. In addition, we used the following standard feature: the position of the first occurrence, calculated as the number of words which precede the first occurrence of the term divided by the number of words in the document, similarly as, for instance, (Witten et al., 1999). 3.2 Content features The second set of features used in this work tries to captures distributional properties of a term relatively to the overall textual content of the document where the term appears or the collection. Phraseness The phraseness measures the lexical cohesion of a sequence of words in a given document, i.e. the degree to which it can be considered as a phrase. This measure is classically used for term extraction and can rely on different techniques, usually evaluating the ability of a sequence of words to appear as a stable phrase more often than just by chance. We applied he</context>
<context position="7949" citStr="Witten et al., 1999" startWordPosition="1260" endWordPosition="1263">) the frequency of occurrence of T and freq(wi) the frequency of occurrence of the word wi, we have: GDC(T) = |T |log10(freq(T))freq(T) � freq(wi) wi∈T We used a default value for a single word, because, in this case, the association measure is not meaningful as it depends only on the frequency. Informativeness The informativeness of a term is the degree to which the term is representative of a document given a collection of documents. Once again many measures can be relevant, and we opt for the standard TF-IDF value which is used in most of the keyphrase extraction systems, see for instance (Witten et al., 1999) or (Medelyan and 249 Witten, 2008). The TF-IDF score for a Term T in document D is given by: TF-IDF(T, D) = freq(T, D) |D |x −log2count(T) N where |D |is the number of words in D, count(T) is the number of occurrence of the term T in the global corpus, and N is the number of documents in the corpus. Keywordness Introduced by (Witten et al., 1999), the keywordness reflects the degree to which a term is selected as a keyword. In practice, it is simply the frequency of the keyword in the global corpus. The efficiency of this feature depends, however, on the amount of training data available and </context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and C.G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. In Proceedings of the fourth ACM conference on Digital libraries, page 255. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>