<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000248">
<title confidence="0.991022">
Using Web Selectors for the Disambiguation of All Words
</title>
<author confidence="0.949944">
Hansen A. Schwartz and Fernando Gomez
</author>
<affiliation confidence="0.997878">
School of Electrical Engineering and Computer Science
University of Central Florida
</affiliation>
<address confidence="0.593635">
Orlando, FL 32816, USA
</address>
<email confidence="0.99833">
{hschwartz, gomez}@cs.ucf.edu
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954863636364">
This research examines a word sense dis-
ambiguation method using selectors acquired
from the Web. Selectors describe words which
may take the place of another given word
within its local context. Work in using Web se-
lectors for noun sense disambiguation is gen-
eralized into the disambiguation of verbs, ad-
verbs, and adjectives as well. Additionally,
this work incorporates previously ignored ad-
verb context selectors and explores the effec-
tiveness of each type of context selector ac-
cording to its part of speech. Overall results
for verb, adjective, and adverb disambigua-
tion are well above a random baseline and
slightly below the most frequent sense base-
line, a point which noun sense disambigua-
tion overcomes. Our experiments find that,
for noun and verb sense disambiguation tasks,
each type of context selector may assist target
selectors in disambiguation. Finally, these ex-
periments also help to draw insights about the
future direction of similar research.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865861111111">
The great amount of text on the Web has emerged
as an unprecedented electronic source of natural
language. Recently, word sense disambiguation
systems have fostered the size of the Web in or-
der to supplant the issue of limited annotated data
availability for supervised systems (Mihalcea, 2002;
Agirre and Martinez, 2004). Some unsupervised or
minimally supervised methods use the Web more di-
rectly in disambiguation algorithms that do not use
a training set for the specific target words.
One such minimally supervised method uses se-
lectors acquired from the Web for noun sense disam-
biguation by comparing the selectors of a given sen-
tence to a target noun within the sentence (Schwartz
and Gomez, 2008). Although this work found strong
results, many aspects of the use of selectors was left
unexplored. For one, the method was only applied
to noun sense disambiguation, focusing on the well-
developed noun hypernym hierarchy within Word-
Net (Miller et al., 1993). Additionally, the role of
different types of selectors was not extensively ex-
plored, and adverb selectors were not used at all. We
seek to address those issues.
In this paper, we extend our method of using se-
lectors from the Web for noun sense disambigua-
tion into a more robust method of disambiguating
words of all parts of speech. After a brief back-
ground on selectors and related work, we explain
the acquisition and empirical application of selec-
tors from nouns, verbs, adjectives, pronouns/proper
nouns, and adverbs. Finally, results are presented
from the SemEval-2007 coarse grained all-words
task (Navigli et al., 2007), and we explore the influ-
ence of various types of selectors on the algorithm
in order to draw insight for future improvement of
Web-based methods.
</bodyText>
<sectionHeader confidence="0.986527" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.987362833333333">
In this section we describe related research in selec-
tors and solving the problem of word sense disam-
biguation (WSD). Specifically, two types of WSD
research are examined: works that used the Web in
direct manner, and works which applied a similarity
or relatedness measure.
</bodyText>
<subsectionHeader confidence="0.952247">
2.1 Selectors
</subsectionHeader>
<bodyText confidence="0.999652">
The term selector comes from (Lin, 1997), and
refers to a word which can take the place of another
given word within the same local context. Although
</bodyText>
<page confidence="0.983119">
28
</page>
<note confidence="0.992146">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 28–36,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999573846153846">
Lin searched a dependency relationship database in
order to match local context, it is not yet possible to
parse dependency relationships of the entire Web. In
turn, one must search for text as local context. For
example, in the sentence below, the local context for
‘strikers’ would be composed of “he addressed the”
and “at the rally.”.
He addressed the strikers at the rally.
Previously, we introduced the idea of using selec-
tors of other words in a sentence in addition to se-
lectors of the target, the word being disambiguated
(Schwartz and Gomez, 2008). Words taking the
place of a target word are referred to as target selec-
tors and words which take the place of other words
in a sentence are referred to as context selectors.
Context selectors can be classified further based on
their part of speech. In our example, if ‘striker’ was
the target word, the verb context selectors would be
verbs replacing ‘addressed’ and the noun context se-
lectors would be nouns replacing ‘rally’.
Similarity is used to measure the relationship be-
tween a target word and its target selectors, while
relatedness measures the relationship between a tar-
get word and context selectors from other parts of
the sentence. Thus, the use of selectors in disam-
biguating words relies on a couple assumptions:
</bodyText>
<listItem confidence="0.993527333333333">
1. Concepts which appear in matching syntactic
constructions are similar.
2. Concepts which appear in the context of a given
</listItem>
<bodyText confidence="0.887010222222222">
target word are related to the correct sense of
the target word.
Note that ‘concept’ and ‘word sense’ are used in-
terchangeably throughout this paper. This idea of
distinguishing similarity and relatedness has an ex-
tensive history (Rada et al., 1989; Resnik, 1999; Pat-
wardhan et al., 2003; Budanitsky and Hirst, 2006),
but most algorithms only find a use for one or the
other.
</bodyText>
<subsectionHeader confidence="0.971981">
2.2 Related Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999988909090909">
A key aspect of using selectors for disambiguation is
the inclusion of context in the Web search queries.
This was done in works by (Martinez et al., 2006)
and (Yuret, 2007), which substituted relatives or
similar words in place of the target word within a
given context. The context, restricted with a win-
dow size, helped to limit the results from the Web.
These works followed (Mihalcea and Moldovan,
1999; Agirre et al., 2001) in that queries were con-
structed through the use of a knowledge-base, fill-
ing the queries with pre-chosen words. We also use
context in the web search, but we acquire words
matching a wildcard in the search rather than incor-
porate a knowledge-base to construct queries with
pre-chosen relatives. Consequently, the later half of
our algorithm uses a knowledge-base through simi-
larity and relatedness measures.
Some recent works have used similarity or relat-
edness measures to assist with WSD. Particuarly,
(Patwardhan et al., 2003) provide evaluations of var-
ious relatedness measures for word sense disam-
biguation based on words in context. These evalu-
ations helped us choose the similarity and related-
ness measures to use in this work. Other works,
such as (Sinha and Mihalcea, 2007), use similar-
ity or relatedness measures over graphs connecting
words within a sentence. Likewise, (Navigli and Ve-
lardi, 2005) analyze the connectivity of concepts in
a sentence among Structured Semantic Interconnec-
tions (SSI), graphs of relationships based on many
knowledge sources. These works do not use selec-
tors or the Web. Additionally, target selectors and
context selectors provide an application for the dis-
tinction between similarity and relatedness not used
in these other methods.
Several ideas distinguish this current work from
our research described in (Schwartz and Gomez,
2008). The most notable aspect is that we have gen-
eralized the overall method of using Web selectors
into disambiguating verbs, adverbs, and adjectives
in addition to nouns. Another difference is the in-
clusion of selectors for adverbs. Finally, we also ex-
plore the actual impact that each type of selector has
on the performance of the disambiguation algorithm.
</bodyText>
<sectionHeader confidence="0.996117" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9903768">
In this section we describe the Web Selector algo-
rithm such that verbs, adjectives, and adverbs are
disambiguated in addition to nouns. The algorithm
essentially runs in two steps: acquisition of selectors
and application of selectors.
</bodyText>
<page confidence="0.997238">
29
</page>
<subsectionHeader confidence="0.999831">
3.1 Acquisition of Selectors
</subsectionHeader>
<bodyText confidence="0.999577086956522">
Selectors are acquired for all appropriate parts of
speech. Whether the selectors are used as target
selectors or context selectors depends on the target
word with which they are being applied. Thus, one
process can be used to acquire all noun, verb, adjec-
tive, and adverb selectors. Additionally, noun selec-
tors can be acquired for pronouns and proper nouns
(referred to as “pro” selectors). These are regular
nouns found to replace a pronoun or proper noun
within their local context.
The first step in acquisition is to construct a query
with a wildcard in place of the target. In our ex-
ample, with ‘address’ as the target, the query is “he
* the strikers at the rally.” Yahoo! Web Services1
provides the functionality for searching the web for
phrases with wildcards. Selectors are extracted from
the samples returned from the web search by match-
ing the words which take the place of the wildcard.
All words not found in WordNet under the same
part of speech as the target are thrown out as well
as phrases longer than 4 words or those containing
punctuation.
The system enters a loop where it:
</bodyText>
<listItem confidence="0.996764">
• searches the web with a given query, and
• extracts selectors from the web samples.
</listItem>
<bodyText confidence="0.991867">
The query is truncated and the search is repeated un-
til a goal for the number of selectors was reached
or the query becomes too short. This approach, de-
tailed in (Schwartz and Gomez, 2008), removes se-
lect punctuation, determiners, and gradually short-
ens the query one word at a time. Selectors retrieved
from a larger query are removed from the results of
smaller queries as the smaller queries should sub-
sume the larger query results. Some selectors re-
trieved for the example, with their corresponding
web query are listed in Table 1.
</bodyText>
<subsectionHeader confidence="0.999774">
3.2 Similarity and Relatedness
</subsectionHeader>
<bodyText confidence="0.993617375">
To apply selectors in disambiguation, similarity and
relatedness measures are used to compare the selec-
tors with the target word. We incorporate the use
of a few previously defined measures over WordNet
(Miller et al., 1993). The WordNet::Similarity pack-
age provides a flexible implementation of many of
these measures (Pedersen et al., 2004). We config-
ured WordNet::Similarity for WordNet version 2.1,
</bodyText>
<footnote confidence="0.771436">
1http://developer.yahoo.com/search/
</footnote>
<equation confidence="0.963370095238095">
He addressed the * at the rally
crowd:1
He addressed * at the rally
student:1, supporter:2
He addressed * at the
Council:1, Muslim:1, Saturday:1, Ugandan:1,
analyst:2, attendee:20, audience:3, class:2,
consumer:1, council:1, delegate:64, diplomat:2,
employee:2, engineer:1, fan:1, farmer:1,
globalization:1, graduate:5, guest:2, hundred:3,
investor:1, issue:1, journalist:9, lawmaker:11,
legislator:1, member:6, midshipman:1,
mourner:1, official:2, parliamentarian:1,
participant:17, patient:1, physician:18,
reporter:8, sailor:1, secretary:1, soldier:3,
staff:3, student:20, supporter:8, thousand:3,
today:2, trader:1, troops:2, visitor:1, worker:1
He * the strikers at the
treat:2
He * the strikers at
get:1, keep:1, price:1, treat:1
</equation>
<tableCaption confidence="0.9588555">
Table 1: Lists of selectors for the target words ‘striker’
and ‘address’ returned by corresponding web queries.
</tableCaption>
<bodyText confidence="0.99994980952381">
the same version used to annotate our chosen exper-
imental corpus.
A relatedness measure was used with context se-
lectors, and we chose the adapted Lesk algorithm
(Banerjee and Pedersen, 2002). An important char-
acteristic of this measure is that it can handle multi-
ple parts of speech. For target selectors we sought
to use measures over the WordNet ontology in order
to most closely measure similarity. An information-
content (IC) measure (Resnik, 1999) was used for
target selectors of nouns and verbs. However, be-
cause IC measures do not work with all parts of
speech, we used the adapted Lesk algorithm as an
approximation of similarity for adjectives and ad-
verbs. Note that finding the best relatedness or sim-
ilarity measure was outside the scope of this paper.
The following function, based on Resnik’s word
similarity (Resnik, 1999), is used to find the max
similarity or relatedness between a concept and a
word (specifically between a sense of the target
word, ct and a selector, ws).
</bodyText>
<equation confidence="0.8668145">
maxsr(ct, ws) = max [meas(ct, cs)]
c3∈w3
</equation>
<bodyText confidence="0.9996665">
where cs is a sense of the selector and meas is a
similarity or relatedness measure.
</bodyText>
<page confidence="0.995765">
30
</page>
<figureCaption confidence="0.993897666666667">
Figure 1: General flow in applying selectors to word
sense disambiguation. Note that the target selectors may
be any part of speech.
</figureCaption>
<subsectionHeader confidence="0.999839">
3.3 Application of Selectors
</subsectionHeader>
<bodyText confidence="0.9999868">
Next, we briefly describe the empirical basis for
scoring senses of the target word. This step is out-
lined in Figure 1. The occurrences of selectors can
be converted to a probability of a selector, ws ap-
pearing in a web query, q:
</bodyText>
<equation confidence="0.658255">
p(ws, q)
</equation>
<bodyText confidence="0.99985075">
The senses of the target word are compared with
each selector. For a given sense of the target word,
ct, the similarity or relatedness from a selector and
query is computed as:
</bodyText>
<equation confidence="0.9936215">
SR(ct, ws, q) = p(ws, q) * maxsr(ct, ws)
senses(ws)
</equation>
<bodyText confidence="0.999515375">
where senses(ws) is the number of senses of the
selector.
As the queries get shorter, the accuracy of the se-
lectors becomes weaker. In turn, the SR value from
selectors is scaled by a ratio of the web query length,
wql, to the original sentence length, sl. This scaling
is applied when the SR values for one target word
sense are summed:
</bodyText>
<equation confidence="0.9880155">
�sum(ct, T) =
qEqs(T)
</equation>
<bodyText confidence="0.998700153846154">
where qs(T) represents the set of queries for a selec-
tor type, T, and ws ranges over all selectors found
with q, denoted sels(q).
The general approach of disambiguation is to find
the sense of a target word which is most similar to all
target selectors and most related to all context selec-
tors. This follows our assumptions about selectors
given in the background section. Thus, similarity
and relatedness values from different selector types
(represented as Types) must be combined. By ag-
gregating the normalized sums from all types of se-
lectors, we get a combined similarity/relatedness for
a given target word sense:
</bodyText>
<equation confidence="0.4788325">
� sum(ct,T)
CSR(ct) = scale(T) *
TETypes max [sum(cZ, T)]
ciEwt
</equation>
<bodyText confidence="0.999367833333333">
where wt represents the set of all senses belonging to
the target word, and scale(T) is a coefficient used to
weight each type of selector. This term is important
in this work, because our experiments explore the
impact of various selector types.
The top sense is then chosen by looking at the
CSR of all senses. For some situations, specifically
when other senses have a score within 5% of the
top CSR, the difference between concepts is very
small. In these cases, the concept with the lowest
sense number in WordNet is chosen from among the
top scoring senses.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999970333333333">
Our experiments are run over the SemEval2007 Task
7: coarse-grained English all-words. The sense in-
ventory was created by mapping senses in WordNet
2.1 to the Oxford Dictionary of English (Navigli et
al., 2007). The corpus was composed of five docu-
ments with differing domains resulting in 2269 an-
notated word instances. Our system runs on fine-
grained WordNet senses, but evaluation is done by
checking if the predicted fine-grained sense maps to
the correct coarse-grained sense. Many issues as-
sociated with fine-grained annotation, such as those
brought up in (Ide and Wilks, 2006) are avoided
through the use of this corpus.
First, we apply the generalized Web selector algo-
rithm in a straight-forward manner to the entire task.
Then, we delve into analyzing the acquired selectors
and the influence of each type of context selector in
order to gain insights into future related work.
</bodyText>
<equation confidence="0.70517875">
�
wsEsels(q)
SR(ct, ws, q)* sl
wql
</equation>
<page confidence="0.981271">
31
</page>
<table confidence="0.9984465">
BLRand MED WS BLMFS
53.43 70.21 76.02 78.89
</table>
<tableCaption confidence="0.98930425">
Table 2: Results as F1 Values of our system, WS,
compared with baselines: random, BLRand; most fre-
quent sense, BLMFS; median system performance at Se-
mEval07, MED.
</tableCaption>
<table confidence="0.9976115">
UPV-WSD NUS-PT SSI
78.63 82.50 83.21
</table>
<tableCaption confidence="0.92759125">
Table 3: Results as F1 Values of top performing systems
for the SemEval07 Task07 (UPV = (Buscaldi and Rosso,
2007), NUS-PT = (Chan et al., 2007), and SSI = a task
organizer’s system (Navigli and Velardi, 2005)).
</tableCaption>
<subsectionHeader confidence="0.954217">
4.1 Evaluating All Words
</subsectionHeader>
<bodyText confidence="0.9982275">
In this section, we seek to apply the algorithm to all
instances of the testing corpus in order to compare
with baselines and other disambiguation algorithms.
Unless stated otherwise, all results are presented as
F1 values, where F1 = 2∗ P ∗R
P +R. For SemEval2007,
all systems performed better than the random base-
line of 53.43%, but only 4 of 13 systems achieved
an F1 score higher than the MFS baseline of 78.89%
(Navigli et al., 2007).
Table 2 lists the results of applying the general-
ized Web selector algorithm described in this paper
in a straight-forward manner, such that all scale(T)
are set to 1. We see that this version of the system
performs better than the median system in the Se-
mEval07 task, but it is a little below the MFS base-
line. A comparison with top systems is seen in Table
3. Our overall results were just below that of the top
system not utilizing training data, (UPV-WSD (Bus-
caldi and Rosso, 2007)), and a little over 6 percent-
age points below the top supervised system (NUS-
PT (Chan et al., 2007)).
The results are broken down by part of speech
in Table 4. We see that adjective disambiguation
was the furthest above our median point of refer-
ence, and noun disambiguation results were above
the MFS baseline. On the other hand, our adverb
disambiguation results appear weakest compared to
the baselines. Note that we previously reported a
noun sense disambiguation F1 value of 80.20% on
the same corpus (Schwartz and Gomez, 2008). Cur-
rent results differ because the previous work used
</bodyText>
<table confidence="0.9992736">
N V A R
MED 70.76 62.10 71.55 74.04
WS 78.52 68.36 81.21 75.48
BLMFS 77.44 75.30 84.25 87.50
insts 1108 591 362 208
</table>
<tableCaption confidence="0.99752475">
Table 4: Results as F1 values (precision = recall) of our
system by parts of speech (N = noun, V = verb, A = ad-
jective, R = adverb). insts = disambiguation instances of
each part of speech. For other keys see Table 2.
</tableCaption>
<bodyText confidence="0.9703865">
different scale(T) values as well as a custom noun
similarity measure.
</bodyText>
<subsectionHeader confidence="0.969395">
4.2 Selector Acquisition Analysis
</subsectionHeader>
<bodyText confidence="0.9944936875">
We examine the occurrences of acquired selectors.
Listed as the column headings of Table 5, selectors
are acquired for five parts of speech (pro is actually
a combination of two parts of speech: pronoun and
proper noun). The data in Table 5 is based on re-
sults from acquiring selectors for our experimental
corpus. The information presented includes:
insts instances which the algorithm attempts
to acquire selectors
% w/ sels percentage of instances for which
selectors were acquired
sels/inst average number of selectors for an
instance (over all insts)
unique/inst average number of unique selectors for
an instance (over all insts)
insts/sent average instances in a sentence
</bodyText>
<table confidence="0.784208666666667">
noun verb adj. adverb pro
insts 1108 591 362 208 370
% w/sels 54.5 65.8 61.0 57.2 27.0
sels/inst 36.5 51.2 29.5 17.7 15.9
unique/inst 11.6 13.1 8.4 4.1 5.6
insts/sent 4.5 2.4 1.5 0.8 1.5
</table>
<tableCaption confidence="0.965030333333333">
Table 5: Various statistics on the acquired selectors for
the SemEval07 Task 7 broken down by part of speech.
Row descriptions are in the text.
</tableCaption>
<bodyText confidence="0.999643666666667">
The selector acquisition data provides useful in-
formation. In general, % w/ sels was low from be-
ing unable to find text on the Web matching local
context (even with truncated queries). The lowest
% w/ sels, found for pro, was expected consider-
ing only nouns which replace the original words are
</bodyText>
<page confidence="0.998346">
32
</page>
<bodyText confidence="0.999913">
used (pronouns acquired were thrown out since they
are not compatible with the relatedness measures).
There was quite a variation in the sels/inst depending
on the type, and all of these numbers are well below
the upper-bound of 200 selectors acquired before the
algorithm stops searching. It turned out that only
15.9% of the instances hit this mark. This means
that most instances stopped acquiring selectors be-
cause they hit the minimum query length (5 words).
In fact, the average web query to acquire at least one
selector had a length of 6.7 words, and the bulk of
selectors came from shorter queries (with less con-
text from shorter queries, the selectors returned are
not as strong). We refer to the combination of quan-
tity and quality issues presented above, in general,
as the quality selector sparsity problem.
Although quality and quantity were not ideal,
when one considers data from the sentence level,
things are more optimistic. The average sentence
had 10.7 instances (of any part of speech listed),
so when certain selector types were missing, oth-
ers were present. As explained previously, the tar-
get selector and context selector distinction is made
after the acquisition of selectors. Thus, each in-
stance is used as both (exception: pro instances were
never used as target selectors since they were not
disambiguated) . Employing this fact, more infor-
mation can be discovered. For example, the aver-
age noun was disambiguated with 36.5 target selec-
tors, 122.9 verb context selectors (51.2 sels/inst *
2.4 insts/sent), 44.3 adjective context selectors, 14.2
adverb context selectors, and 23.9 pro context se-
lectors. Still, with the bulk of those selectors com-
ing from short queries, the reliability of the selectors
was not strong.
</bodyText>
<subsectionHeader confidence="0.999513">
4.3 Exploring the Influence of Selector Types
</subsectionHeader>
<bodyText confidence="0.999782909090909">
This section explores the influence of each context
selector on the disambiguation algorithm, by chang-
ing the value of scale(T) in the previously listed
CSR function.
Examining Table 6 reveals precision results
when disambiguating instances with target selec-
tors, based only on the target word’s similarity with
target selectors. This serves as a bearing for inter-
preting results of context selector variation.
We tested how well each type of context selec-
tor complements the target selectors. Accordingly,
</bodyText>
<table confidence="0.9966794">
wsd prec. % insts.
N 64.08 348
V 52.86 227
A 77.36 106
R 58.39 56
</table>
<tableCaption confidence="0.978234">
Table 6: Precision when disambiguating with target se-
lectors only. All instances contain target selectors and
multiple senses in WordNet. (insts. = number of in-
stances disambiguated.)
</tableCaption>
<table confidence="0.9990372">
wsd noun verb adj. adverb pro
N 272 186 120 84 108
V 211 167 110 80 103
A 97 78 50 40 34
R 47 44 30 17 26
</table>
<tableCaption confidence="0.99528">
Table 7: Instance occurrences used for disambiguation
</tableCaption>
<bodyText confidence="0.95755616">
when experimenting with all types of context selectors
(listed as columns). The rows represent the four parts of
speech disambiguated.
scale(target) was set to 1, and scale(T) for all
other context types were set to 0. In order to limit ex-
ternal influences, we did not predict words with only
one sense in WordNet or instances where the CSR
was zero (indicating no selectors). Additionally, we
only tested on examples which had at least one tar-
get selector and at least one selector of the specific
type being examined. This restriction ensures we are
avoiding some of the quality selector sparsity prob-
lem described in the analysis. Nevertheless, results
are expected to be a little lower than our initial tests
as we are ignoring other types of selectors and not
including monosemous words according to Word-
Net. Table 7 lists the instance occurrences for each
of the four parts of speech that were disambiguated,
based on these restrictions.
Figures 2 through 5 show graphs of the precision
score while increasing the influence of each context
selector type. Each graph corresponds to the disam-
biguation of a different part of speech, and each line
in a graph represents one of the five types of context
selectors:
</bodyText>
<listItem confidence="0.9581548">
1. noun context
2. verb context
3. adjective context
4. adverb context
5. pro context
</listItem>
<page confidence="0.994066">
33
</page>
<figure confidence="0.9932155">
0.25 1 4 16
scale(T) value
</figure>
<figureCaption confidence="0.998649666666667">
Figure 2: The noun sense disambiguation precision when
varying the scale(T) value for each type of context selec-
tor. scale(target) is always 1.
</figureCaption>
<figure confidence="0.9915925">
0.25 1 4 16
scale(T) value
</figure>
<figureCaption confidence="0.994488333333333">
Figure 3: The verb sense disambiguation precision when
varying the scale(T) value for each type of context se-
lector. scale(target) is 1.
</figureCaption>
<figure confidence="0.99319644">
68
66
64
62
78
76
74
72
70
noun
verb
adjective
adverb
pro
65
60
45
40
55
50
noun
verb
adjective
adverb
pro
</figure>
<bodyText confidence="0.999615565217391">
The lines are formed with a Bezier curve algorithm2
on the precision data. The horizontal line represents
the precision of only using the target selectors to dis-
ambiguate instances with target selectors. Precision
either decreases or remains the same if any graph
line was extended past the right-most boundary.
When examining the figures, one should note
when the precision increases as the scale value in-
creases. This indicates that increases in influence of
the particular type of context selector improved the
results. The x-axis increases exponentially, since we
would like a ratio of scale(T) to scale(target), and
at x = 1 the context selector has the same influence
as the target selector.
We see that all types of context selectors improve
the results for noun and verb sense disambiguation.
Thus, our inclusion of adverb context selectors was
worthwhile. It is difficult to draw a similar conclu-
sion from the adverb and adjective disambiguation
graphs (Figures 4 and 5), although it still appears
that the noun context selectors are helpful for both
and the pro context selectors are helpful for the ad-
jective task. We also note that most selector types
</bodyText>
<footnote confidence="0.686179">
2http://www.gnuplot.info/docs/node124.html
</footnote>
<bodyText confidence="0.99991225">
achieve highest precision above a scale value of 1,
indicating that the context selector should have more
influence than the target selectors. This is proba-
bly due to the existence of more selectors from con-
text than those from the target word. The results of
adverb disambiguation should be taken lightly, be-
cause there were not many disambiguation instances
that fit the restrictions (see Table 7).
</bodyText>
<subsectionHeader confidence="0.994878">
4.4 Discussion of Future Work
</subsectionHeader>
<bodyText confidence="0.971272571428572">
Based on the results of our analysis and experiments,
we list two avenues of future improvement:
1. Automatic Alternative Query Construction:
This idea is concerned with the quality and
quantity of selectors acquired for which there
is currently a trade-off. As one shortens the
query to receive more quantity, the quality
goes down due to a less accurate local context.
One may be able to side-step this trade-off by
searching with alternative queries which cap-
ture just as much local context. For example,
the query “He * the strikers at the rally” can
be mapped into the passive transformation “the
strikers were * at the rally by him”. Query
</bodyText>
<page confidence="0.991351">
34
</page>
<figure confidence="0.992075">
0.25 1 4 16
scale(T) value
</figure>
<figureCaption confidence="0.997423">
Figure 4: The adjective sense disambiguation precision
when varying the scale(T) value for each type of context
selector. scale(target) is 1.
</figureCaption>
<figure confidence="0.9905105">
0.25 1 4 16
scale(T) value
</figure>
<figureCaption confidence="0.995657333333333">
Figure 5: The adverb sense disambiguation precision
when varying the scale(T) value for each type of con-
text selector. scale(target) is 1.
</figureCaption>
<figure confidence="0.996947571428571">
80
68
66
64
62
60
78
76
74
72
70
noun
verb
adjective
adverb
pro
65
60
45
40
35
55
50
noun
verb
adjective
adverb
pro
</figure>
<bodyText confidence="0.958953863636364">
reconstruction can be accomplished by using
a constituent-based parser, which will help to
produce syntactic alternations and other trans-
formations such as the dative.
2. Improving Similarity and Relatedness: Noun
sense disambiguation was the only subtask to
pass the MFS baseline. One reason we suspect
for this is that work in similarity and related-
ness has a longer history over nouns than over
other parts of speech (Budanitsky and Hirst,
2006). Additionally, the hypernym (is-a) re-
lationship of the noun ontology of WordNet
captures the notion of similarity more clearly
than the primary relationships of other parts of
speech in WordNet. Accordingly, future work
should look into specific measures of similarity
for each part of speech, and further improve-
ment to relatedness measures which function
accross different parts of speech. A subtle piece
of this type of work may find a way to effec-
tively incorporate pronouns in the measures, al-
lowing less selectors to be thrown out.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999977055555556">
We found the use of Web selectors to be a worth-
while approach to the disambiguation of other parts
of speech in addition to nouns. However, results
for verb, adjective, and adverb disambiguation were
slightly below the most frequent sense baseline, a
point which noun sense disambiguation overcomes.
The use of this type of algorithm is still rich with
avenues yet to be taken for improvement.
Future work may address aspects at all levels of
the algorithm. To deal with a quality selector spar-
sity problem, a system might automatically form
alternative web queries utilizing a syntactic parser.
Research may also look into defining similarity mea-
sures for adjectives and adverbs, and refining the
similarity measures for nouns and verbs. Neverthe-
less, without these promising future extensions the
system still performs well, only topped by one other
minimally supervised system.
</bodyText>
<sectionHeader confidence="0.99934" genericHeader="conclusions">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.993528">
This research was supported by the NASA Engi-
neering and Safety Center under Grant/Cooperative
Agreement NNX08AJ98A.
</bodyText>
<page confidence="0.998998">
35
</page>
<sectionHeader confidence="0.99589" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938625">
Eneko Agirre and David Martinez. 2004. Unsupervised
WSD based on automatically retrieved examples: The
importance of bias. In Proceedings of EMNLP 2004,
pages 25–32, Barcelona, Spain, July.
Eneko Agirre, Olatz Ansa, and David Martinez. 2001.
Enriching wordnet concepts with topic signatures. In
In Proceedings of the NAACL workshop on WordNet
and Other Lexical Resources: Applications, Exten-
sions and Customizations, Pittsburg, USA.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics. Mexico City, Mexico.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13–47.
Davide Buscaldi and Paolo Rosso. 2007. UPV-WSD :
Combining different WSD methods by means of fuzzy
borda voting. In Proceedings of SemEval-2007, pages
434–437, Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-PT: Exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of Proceedings of SemEval-2007, pages 253–
256, Prague, Czech Republic, June.
Nancy Ide and Yorick Wilks, 2006. Word Sense Dis-
ambiguation: Algorithms And Applications, chapter 3:
Making Sense About Sense. Springer.
Dekang Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In Pro-
ceedings of the 35th annual meeting on Association for
Computational Linguistics, pages 64–71.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42–50.
Rada Mihalcea and Dan I. Moldovan. 1999. An auto-
matic method for generating sense tagged corpora. In
Proceedings of AAAI-99, pages 461–466.
Rada Mihalcea. 2002. Bootstrapping large sense tagged
corpora. In Proceedings of the 3rd International
Conference on Languages Resources and Evaluations
LREC 2002, Las Palmas, Spain, May.
George Miller, R. Beckwith, Christiane Fellbaum,
D. Gross, and K. Miller. 1993. Five papers on word-
net. Technical report, Princeton University.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: A knowledge-based ap-
proach to word sense disambiguation. IEEE Trans.
Pattern Anal. Mach. Intell., 27(7):1075–1086.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-grained
english all-words task. In Proceedings of SemEval-
2007, pages 30–35, Prague, Czech Republic. Associa-
tion for Computational Linguistics.
Siddharth Patwardhan, S. Banerjee, and T. Pedersen.
2003. Using Measures of Semantic Relatedness
for Word Sense Disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics, pages
241–257, Mexico City, Mexico, February.
Ted Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Human Language Technology Confer-
ence of the NAACL Demonstrations, pages 38–41,
Boston, MA, May.
R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. In IEEE Transactions on Systems, Man and Cy-
bernetics, volume 19, pages 17–30.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95–130.
Hansen A. Schwartz and Fernando Gomez. 2008. Ac-
quiring knowledge from the web to be used as selec-
tors for noun sense disambiguation. In CoNLL 2008:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 105–112,
Manchester, England, August.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. Irvine, CA, Septem-
ber.
Deniz Yuret. 2007. KU: Word sense disambiguation by
substitution. In Proceedings of SemEval-2007, pages
207–214, Prague, Czech Republic, June.
</reference>
<page confidence="0.998944">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967633">
<title confidence="0.999951">Using Web Selectors for the Disambiguation of All Words</title>
<author confidence="0.999921">A Schwartz</author>
<affiliation confidence="0.9999325">School of Electrical Engineering and Computer University of Central</affiliation>
<address confidence="0.982947">Orlando, FL 32816,</address>
<abstract confidence="0.99926247826087">This research examines a word sense disambiguation method using selectors acquired from the Web. Selectors describe words which may take the place of another given word within its local context. Work in using Web selectors for noun sense disambiguation is generalized into the disambiguation of verbs, adverbs, and adjectives as well. Additionally, this work incorporates previously ignored adverb context selectors and explores the effectiveness of each type of context selector according to its part of speech. Overall results for verb, adjective, and adverb disambiguation are well above a random baseline and slightly below the most frequent sense baseline, a point which noun sense disambiguation overcomes. Our experiments find that, for noun and verb sense disambiguation tasks, each type of context selector may assist target selectors in disambiguation. Finally, these experiments also help to draw insights about the future direction of similar research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Unsupervised WSD based on automatically retrieved examples: The importance of bias.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>25--32</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1541" citStr="Agirre and Martinez, 2004" startWordPosition="235" endWordPosition="238"> noun sense disambiguation overcomes. Our experiments find that, for noun and verb sense disambiguation tasks, each type of context selector may assist target selectors in disambiguation. Finally, these experiments also help to draw insights about the future direction of similar research. 1 Introduction The great amount of text on the Web has emerged as an unprecedented electronic source of natural language. Recently, word sense disambiguation systems have fostered the size of the Web in order to supplant the issue of limited annotated data availability for supervised systems (Mihalcea, 2002; Agirre and Martinez, 2004). Some unsupervised or minimally supervised methods use the Web more directly in disambiguation algorithms that do not use a training set for the specific target words. One such minimally supervised method uses selectors acquired from the Web for noun sense disambiguation by comparing the selectors of a given sentence to a target noun within the sentence (Schwartz and Gomez, 2008). Although this work found strong results, many aspects of the use of selectors was left unexplored. For one, the method was only applied to noun sense disambiguation, focusing on the welldeveloped noun hypernym hiera</context>
</contexts>
<marker>Agirre, Martinez, 2004</marker>
<rawString>Eneko Agirre and David Martinez. 2004. Unsupervised WSD based on automatically retrieved examples: The importance of bias. In Proceedings of EMNLP 2004, pages 25–32, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>David Martinez</author>
</authors>
<title>Enriching wordnet concepts with topic signatures. In</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<location>Pittsburg, USA.</location>
<contexts>
<context position="5867" citStr="Agirre et al., 2001" startWordPosition="951" endWordPosition="954">istory (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in that queries were constructed through the use of a knowledge-base, filling the queries with pre-chosen words. We also use context in the web search, but we acquire words matching a wildcard in the search rather than incorporate a knowledge-base to construct queries with pre-chosen relatives. Consequently, the later half of our algorithm uses a knowledge-base through similarity and relatedness measures. Some recent works have used similarity or relatedness measures to assist with WSD. Particuarly, (Patwardhan et al., 2003) provide evaluations of various relatedness measures for word sense d</context>
</contexts>
<marker>Agirre, Ansa, Martinez, 2001</marker>
<rawString>Eneko Agirre, Olatz Ansa, and David Martinez. 2001. Enriching wordnet concepts with topic signatures. In In Proceedings of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics. Mexico City,</booktitle>
<location>Mexico.</location>
<contexts>
<context position="11111" citStr="Banerjee and Pedersen, 2002" startWordPosition="1764" endWordPosition="1767">, midshipman:1, mourner:1, official:2, parliamentarian:1, participant:17, patient:1, physician:18, reporter:8, sailor:1, secretary:1, soldier:3, staff:3, student:20, supporter:8, thousand:3, today:2, trader:1, troops:2, visitor:1, worker:1 He * the strikers at the treat:2 He * the strikers at get:1, keep:1, price:1, treat:1 Table 1: Lists of selectors for the target words ‘striker’ and ‘address’ returned by corresponding web queries. the same version used to annotate our chosen experimental corpus. A relatedness measure was used with context selectors, and we chose the adapted Lesk algorithm (Banerjee and Pedersen, 2002). An important characteristic of this measure is that it can handle multiple parts of speech. For target selectors we sought to use measures over the WordNet ontology in order to most closely measure similarity. An informationcontent (IC) measure (Resnik, 1999) was used for target selectors of nouns and verbs. However, because IC measures do not work with all parts of speech, we used the adapted Lesk algorithm as an approximation of similarity for adjectives and adverbs. Note that finding the best relatedness or similarity measure was outside the scope of this paper. The following function, ba</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics. Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="5340" citStr="Budanitsky and Hirst, 2006" startWordPosition="860" endWordPosition="863">easures the relationship between a target word and context selectors from other parts of the sentence. Thus, the use of selectors in disambiguating words relies on a couple assumptions: 1. Concepts which appear in matching syntactic constructions are similar. 2. Concepts which appear in the context of a given target word are related to the correct sense of the target word. Note that ‘concept’ and ‘word sense’ are used interchangeably throughout this paper. This idea of distinguishing similarity and relatedness has an extensive history (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in that queries were constructed through the use of a knowledge-base, fi</context>
<context position="26607" citStr="Budanitsky and Hirst, 2006" startWordPosition="4402" endWordPosition="4405">) value for each type of context selector. scale(target) is 1. 80 68 66 64 62 60 78 76 74 72 70 noun verb adjective adverb pro 65 60 45 40 35 55 50 noun verb adjective adverb pro reconstruction can be accomplished by using a constituent-based parser, which will help to produce syntactic alternations and other transformations such as the dative. 2. Improving Similarity and Relatedness: Noun sense disambiguation was the only subtask to pass the MFS baseline. One reason we suspect for this is that work in similarity and relatedness has a longer history over nouns than over other parts of speech (Budanitsky and Hirst, 2006). Additionally, the hypernym (is-a) relationship of the noun ontology of WordNet captures the notion of similarity more clearly than the primary relationships of other parts of speech in WordNet. Accordingly, future work should look into specific measures of similarity for each part of speech, and further improvement to relatedness measures which function accross different parts of speech. A subtle piece of this type of work may find a way to effectively incorporate pronouns in the measures, allowing less selectors to be thrown out. 5 Conclusion We found the use of Web selectors to be a worthw</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
</authors>
<title>UPV-WSD : Combining different WSD methods by means of fuzzy borda voting.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>434--437</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="15578" citStr="Buscaldi and Rosso, 2007" startWordPosition="2532" endWordPosition="2535"> Web selector algorithm in a straight-forward manner to the entire task. Then, we delve into analyzing the acquired selectors and the influence of each type of context selector in order to gain insights into future related work. � wsEsels(q) SR(ct, ws, q)* sl wql 31 BLRand MED WS BLMFS 53.43 70.21 76.02 78.89 Table 2: Results as F1 Values of our system, WS, compared with baselines: random, BLRand; most frequent sense, BLMFS; median system performance at SemEval07, MED. UPV-WSD NUS-PT SSI 78.63 82.50 83.21 Table 3: Results as F1 Values of top performing systems for the SemEval07 Task07 (UPV = (Buscaldi and Rosso, 2007), NUS-PT = (Chan et al., 2007), and SSI = a task organizer’s system (Navigli and Velardi, 2005)). 4.1 Evaluating All Words In this section, we seek to apply the algorithm to all instances of the testing corpus in order to compare with baselines and other disambiguation algorithms. Unless stated otherwise, all results are presented as F1 values, where F1 = 2∗ P ∗R P +R. For SemEval2007, all systems performed better than the random baseline of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al., 2007). Table 2 lists the results of applying</context>
</contexts>
<marker>Buscaldi, Rosso, 2007</marker>
<rawString>Davide Buscaldi and Paolo Rosso. 2007. UPV-WSD : Combining different WSD methods by means of fuzzy borda voting. In Proceedings of SemEval-2007, pages 434–437, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>Zhi Zhong</author>
</authors>
<title>NUS-PT: Exploiting parallel texts for word sense disambiguation in the english all-words tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of Proceedings of SemEval-2007,</booktitle>
<pages>253--256</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="15608" citStr="Chan et al., 2007" startWordPosition="2538" endWordPosition="2541">-forward manner to the entire task. Then, we delve into analyzing the acquired selectors and the influence of each type of context selector in order to gain insights into future related work. � wsEsels(q) SR(ct, ws, q)* sl wql 31 BLRand MED WS BLMFS 53.43 70.21 76.02 78.89 Table 2: Results as F1 Values of our system, WS, compared with baselines: random, BLRand; most frequent sense, BLMFS; median system performance at SemEval07, MED. UPV-WSD NUS-PT SSI 78.63 82.50 83.21 Table 3: Results as F1 Values of top performing systems for the SemEval07 Task07 (UPV = (Buscaldi and Rosso, 2007), NUS-PT = (Chan et al., 2007), and SSI = a task organizer’s system (Navigli and Velardi, 2005)). 4.1 Evaluating All Words In this section, we seek to apply the algorithm to all instances of the testing corpus in order to compare with baselines and other disambiguation algorithms. Unless stated otherwise, all results are presented as F1 values, where F1 = 2∗ P ∗R P +R. For SemEval2007, all systems performed better than the random baseline of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al., 2007). Table 2 lists the results of applying the generalized Web selector </context>
</contexts>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007. NUS-PT: Exploiting parallel texts for word sense disambiguation in the english all-words tasks. In Proceedings of Proceedings of SemEval-2007, pages 253– 256, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Yorick Wilks</author>
</authors>
<title>Word Sense Disambiguation: Algorithms And Applications, chapter 3: Making Sense About Sense.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="14877" citStr="Ide and Wilks, 2006" startWordPosition="2412" endWordPosition="2415">he top scoring senses. 4 Experiments Our experiments are run over the SemEval2007 Task 7: coarse-grained English all-words. The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). The corpus was composed of five documents with differing domains resulting in 2269 annotated word instances. Our system runs on finegrained WordNet senses, but evaluation is done by checking if the predicted fine-grained sense maps to the correct coarse-grained sense. Many issues associated with fine-grained annotation, such as those brought up in (Ide and Wilks, 2006) are avoided through the use of this corpus. First, we apply the generalized Web selector algorithm in a straight-forward manner to the entire task. Then, we delve into analyzing the acquired selectors and the influence of each type of context selector in order to gain insights into future related work. � wsEsels(q) SR(ct, ws, q)* sl wql 31 BLRand MED WS BLMFS 53.43 70.21 76.02 78.89 Table 2: Results as F1 Values of our system, WS, compared with baselines: random, BLRand; most frequent sense, BLMFS; median system performance at SemEval07, MED. UPV-WSD NUS-PT SSI 78.63 82.50 83.21 Table 3: Resu</context>
</contexts>
<marker>Ide, Wilks, 2006</marker>
<rawString>Nancy Ide and Yorick Wilks, 2006. Word Sense Disambiguation: Algorithms And Applications, chapter 3: Making Sense About Sense. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="3301" citStr="Lin, 1997" startWordPosition="526" endWordPosition="527">, and adverbs. Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al., 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods. 2 Background In this section we describe related research in selectors and solving the problem of word sense disambiguation (WSD). Specifically, two types of WSD research are examined: works that used the Web in direct manner, and works which applied a similarity or relatedness measure. 2.1 Selectors The term selector comes from (Lin, 1997), and refers to a word which can take the place of another given word within the same local context. Although 28 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 28–36, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Lin searched a dependency relationship database in order to match local context, it is not yet possible to parse dependency relationships of the entire Web. In turn, one must search for text as local context. For example, in the sentence below, the local context for ‘strikers’ would be co</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th annual meeting on Association for Computational Linguistics, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Eneko Agirre</author>
<author>Xinglong Wang</author>
</authors>
<title>Word relatives in context for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Australasian Language Technology Workshop,</booktitle>
<pages>42--50</pages>
<contexts>
<context position="5593" citStr="Martinez et al., 2006" startWordPosition="905" endWordPosition="908">. 2. Concepts which appear in the context of a given target word are related to the correct sense of the target word. Note that ‘concept’ and ‘word sense’ are used interchangeably throughout this paper. This idea of distinguishing similarity and relatedness has an extensive history (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in that queries were constructed through the use of a knowledge-base, filling the queries with pre-chosen words. We also use context in the web search, but we acquire words matching a wildcard in the search rather than incorporate a knowledge-base to construct queries with pre-chosen relatives. Consequently, the later half </context>
</contexts>
<marker>Martinez, Agirre, Wang, 2006</marker>
<rawString>David Martinez, Eneko Agirre, and Xinglong Wang. 2006. Word relatives in context for word sense disambiguation. In Proceedings of the 2006 Australasian Language Technology Workshop, pages 42–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>An automatic method for generating sense tagged corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI-99,</booktitle>
<pages>461--466</pages>
<contexts>
<context position="5845" citStr="Mihalcea and Moldovan, 1999" startWordPosition="947" endWordPosition="950">elatedness has an extensive history (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in that queries were constructed through the use of a knowledge-base, filling the queries with pre-chosen words. We also use context in the web search, but we acquire words matching a wildcard in the search rather than incorporate a knowledge-base to construct queries with pre-chosen relatives. Consequently, the later half of our algorithm uses a knowledge-base through similarity and relatedness measures. Some recent works have used similarity or relatedness measures to assist with WSD. Particuarly, (Patwardhan et al., 2003) provide evaluations of various relatedness mea</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 1999. An automatic method for generating sense tagged corpora. In Proceedings of AAAI-99, pages 461–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Bootstrapping large sense tagged corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Languages Resources and Evaluations LREC 2002,</booktitle>
<location>Las Palmas, Spain,</location>
<contexts>
<context position="1513" citStr="Mihalcea, 2002" startWordPosition="233" endWordPosition="234">e, a point which noun sense disambiguation overcomes. Our experiments find that, for noun and verb sense disambiguation tasks, each type of context selector may assist target selectors in disambiguation. Finally, these experiments also help to draw insights about the future direction of similar research. 1 Introduction The great amount of text on the Web has emerged as an unprecedented electronic source of natural language. Recently, word sense disambiguation systems have fostered the size of the Web in order to supplant the issue of limited annotated data availability for supervised systems (Mihalcea, 2002; Agirre and Martinez, 2004). Some unsupervised or minimally supervised methods use the Web more directly in disambiguation algorithms that do not use a training set for the specific target words. One such minimally supervised method uses selectors acquired from the Web for noun sense disambiguation by comparing the selectors of a given sentence to a target noun within the sentence (Schwartz and Gomez, 2008). Although this work found strong results, many aspects of the use of selectors was left unexplored. For one, the method was only applied to noun sense disambiguation, focusing on the welld</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Rada Mihalcea. 2002. Bootstrapping large sense tagged corpora. In Proceedings of the 3rd International Conference on Languages Resources and Evaluations LREC 2002, Las Palmas, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>R Beckwith</author>
<author>Christiane Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Princeton University.</institution>
<contexts>
<context position="2182" citStr="Miller et al., 1993" startWordPosition="341" endWordPosition="344">r minimally supervised methods use the Web more directly in disambiguation algorithms that do not use a training set for the specific target words. One such minimally supervised method uses selectors acquired from the Web for noun sense disambiguation by comparing the selectors of a given sentence to a target noun within the sentence (Schwartz and Gomez, 2008). Although this work found strong results, many aspects of the use of selectors was left unexplored. For one, the method was only applied to noun sense disambiguation, focusing on the welldeveloped noun hypernym hierarchy within WordNet (Miller et al., 1993). Additionally, the role of different types of selectors was not extensively explored, and adverb selectors were not used at all. We seek to address those issues. In this paper, we extend our method of using selectors from the Web for noun sense disambiguation into a more robust method of disambiguating words of all parts of speech. After a brief background on selectors and related work, we explain the acquisition and empirical application of selectors from nouns, verbs, adjectives, pronouns/proper nouns, and adverbs. Finally, results are presented from the SemEval-2007 coarse grained all-word</context>
<context position="9858" citStr="Miller et al., 1993" startWordPosition="1607" endWordPosition="1610"> and Gomez, 2008), removes select punctuation, determiners, and gradually shortens the query one word at a time. Selectors retrieved from a larger query are removed from the results of smaller queries as the smaller queries should subsume the larger query results. Some selectors retrieved for the example, with their corresponding web query are listed in Table 1. 3.2 Similarity and Relatedness To apply selectors in disambiguation, similarity and relatedness measures are used to compare the selectors with the target word. We incorporate the use of a few previously defined measures over WordNet (Miller et al., 1993). The WordNet::Similarity package provides a flexible implementation of many of these measures (Pedersen et al., 2004). We configured WordNet::Similarity for WordNet version 2.1, 1http://developer.yahoo.com/search/ He addressed the * at the rally crowd:1 He addressed * at the rally student:1, supporter:2 He addressed * at the Council:1, Muslim:1, Saturday:1, Ugandan:1, analyst:2, attendee:20, audience:3, class:2, consumer:1, council:1, delegate:64, diplomat:2, employee:2, engineer:1, fan:1, farmer:1, globalization:1, graduate:5, guest:2, hundred:3, investor:1, issue:1, journalist:9, lawmaker:1</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George Miller, R. Beckwith, Christiane Fellbaum, D. Gross, and K. Miller. 1993. Five papers on wordnet. Technical report, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: A knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="6777" citStr="Navigli and Velardi, 2005" startWordPosition="1095" endWordPosition="1099">n relatives. Consequently, the later half of our algorithm uses a knowledge-base through similarity and relatedness measures. Some recent works have used similarity or relatedness measures to assist with WSD. Particuarly, (Patwardhan et al., 2003) provide evaluations of various relatedness measures for word sense disambiguation based on words in context. These evaluations helped us choose the similarity and relatedness measures to use in this work. Other works, such as (Sinha and Mihalcea, 2007), use similarity or relatedness measures over graphs connecting words within a sentence. Likewise, (Navigli and Velardi, 2005) analyze the connectivity of concepts in a sentence among Structured Semantic Interconnections (SSI), graphs of relationships based on many knowledge sources. These works do not use selectors or the Web. Additionally, target selectors and context selectors provide an application for the distinction between similarity and relatedness not used in these other methods. Several ideas distinguish this current work from our research described in (Schwartz and Gomez, 2008). The most notable aspect is that we have generalized the overall method of using Web selectors into disambiguating verbs, adverbs,</context>
<context position="15673" citStr="Navigli and Velardi, 2005" startWordPosition="2549" endWordPosition="2552">nalyzing the acquired selectors and the influence of each type of context selector in order to gain insights into future related work. � wsEsels(q) SR(ct, ws, q)* sl wql 31 BLRand MED WS BLMFS 53.43 70.21 76.02 78.89 Table 2: Results as F1 Values of our system, WS, compared with baselines: random, BLRand; most frequent sense, BLMFS; median system performance at SemEval07, MED. UPV-WSD NUS-PT SSI 78.63 82.50 83.21 Table 3: Results as F1 Values of top performing systems for the SemEval07 Task07 (UPV = (Buscaldi and Rosso, 2007), NUS-PT = (Chan et al., 2007), and SSI = a task organizer’s system (Navigli and Velardi, 2005)). 4.1 Evaluating All Words In this section, we seek to apply the algorithm to all instances of the testing corpus in order to compare with baselines and other disambiguation algorithms. Unless stated otherwise, all results are presented as F1 values, where F1 = 2∗ P ∗R P +R. For SemEval2007, all systems performed better than the random baseline of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al., 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, s</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural semantic interconnections: A knowledge-based approach to word sense disambiguation. IEEE Trans. Pattern Anal. Mach. Intell., 27(7):1075–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Semeval-2007 task 07: Coarse-grained english all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval2007,</booktitle>
<pages>30--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2811" citStr="Navigli et al., 2007" startWordPosition="443" endWordPosition="446">onally, the role of different types of selectors was not extensively explored, and adverb selectors were not used at all. We seek to address those issues. In this paper, we extend our method of using selectors from the Web for noun sense disambiguation into a more robust method of disambiguating words of all parts of speech. After a brief background on selectors and related work, we explain the acquisition and empirical application of selectors from nouns, verbs, adjectives, pronouns/proper nouns, and adverbs. Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al., 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods. 2 Background In this section we describe related research in selectors and solving the problem of word sense disambiguation (WSD). Specifically, two types of WSD research are examined: works that used the Web in direct manner, and works which applied a similarity or relatedness measure. 2.1 Selectors The term selector comes from (Lin, 1997), and refers to a word which can take the place of another given word within the same local context. Although </context>
<context position="14504" citStr="Navigli et al., 2007" startWordPosition="2352" endWordPosition="2355">his work, because our experiments explore the impact of various selector types. The top sense is then chosen by looking at the CSR of all senses. For some situations, specifically when other senses have a score within 5% of the top CSR, the difference between concepts is very small. In these cases, the concept with the lowest sense number in WordNet is chosen from among the top scoring senses. 4 Experiments Our experiments are run over the SemEval2007 Task 7: coarse-grained English all-words. The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). The corpus was composed of five documents with differing domains resulting in 2269 annotated word instances. Our system runs on finegrained WordNet senses, but evaluation is done by checking if the predicted fine-grained sense maps to the correct coarse-grained sense. Many issues associated with fine-grained annotation, such as those brought up in (Ide and Wilks, 2006) are avoided through the use of this corpus. First, we apply the generalized Web selector algorithm in a straight-forward manner to the entire task. Then, we delve into analyzing the acquired selectors and the influence of each</context>
<context position="16139" citStr="Navigli et al., 2007" startWordPosition="2631" endWordPosition="2634">s for the SemEval07 Task07 (UPV = (Buscaldi and Rosso, 2007), NUS-PT = (Chan et al., 2007), and SSI = a task organizer’s system (Navigli and Velardi, 2005)). 4.1 Evaluating All Words In this section, we seek to apply the algorithm to all instances of the testing corpus in order to compare with baselines and other disambiguation algorithms. Unless stated otherwise, all results are presented as F1 values, where F1 = 2∗ P ∗R P +R. For SemEval2007, all systems performed better than the random baseline of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al., 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale(T) are set to 1. We see that this version of the system performs better than the median system in the SemEval07 task, but it is a little below the MFS baseline. A comparison with top systems is seen in Table 3. Our overall results were just below that of the top system not utilizing training data, (UPV-WSD (Buscaldi and Rosso, 2007)), and a little over 6 percentage points below the top supervised system (NUSPT (Chan et al., 2007)). The results</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. Semeval-2007 task 07: Coarse-grained english all-words task. In Proceedings of SemEval2007, pages 30–35, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>Using Measures of Semantic Relatedness for Word Sense Disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--257</pages>
<location>Mexico City, Mexico,</location>
<contexts>
<context position="5311" citStr="Patwardhan et al., 2003" startWordPosition="855" endWordPosition="859">tors, while relatedness measures the relationship between a target word and context selectors from other parts of the sentence. Thus, the use of selectors in disambiguating words relies on a couple assumptions: 1. Concepts which appear in matching syntactic constructions are similar. 2. Concepts which appear in the context of a given target word are related to the correct sense of the target word. Note that ‘concept’ and ‘word sense’ are used interchangeably throughout this paper. This idea of distinguishing similarity and relatedness has an extensive history (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in that queries were constructed through th</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, S. Banerjee, and T. Pedersen. 2003. Using Measures of Semantic Relatedness for Word Sense Disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pages 241–257, Mexico City, Mexico, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Human Language Technology Conference of the NAACL Demonstrations,</booktitle>
<pages>38--41</pages>
<location>Boston, MA,</location>
<contexts>
<context position="9976" citStr="Pedersen et al., 2004" startWordPosition="1624" endWordPosition="1627">ectors retrieved from a larger query are removed from the results of smaller queries as the smaller queries should subsume the larger query results. Some selectors retrieved for the example, with their corresponding web query are listed in Table 1. 3.2 Similarity and Relatedness To apply selectors in disambiguation, similarity and relatedness measures are used to compare the selectors with the target word. We incorporate the use of a few previously defined measures over WordNet (Miller et al., 1993). The WordNet::Similarity package provides a flexible implementation of many of these measures (Pedersen et al., 2004). We configured WordNet::Similarity for WordNet version 2.1, 1http://developer.yahoo.com/search/ He addressed the * at the rally crowd:1 He addressed * at the rally student:1, supporter:2 He addressed * at the Council:1, Muslim:1, Saturday:1, Ugandan:1, analyst:2, attendee:20, audience:3, class:2, consumer:1, council:1, delegate:64, diplomat:2, employee:2, engineer:1, fan:1, farmer:1, globalization:1, graduate:5, guest:2, hundred:3, investor:1, issue:1, journalist:9, lawmaker:11, legislator:1, member:6, midshipman:1, mourner:1, official:2, parliamentarian:1, participant:17, patient:1, physicia</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Human Language Technology Conference of the NAACL Demonstrations, pages 38–41, Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rada</author>
<author>H Mili</author>
<author>E Bicknell</author>
<author>M Blettner</author>
</authors>
<title>Development and application of a metric on semantic nets.</title>
<date>1989</date>
<journal>In IEEE Transactions on Systems, Man and Cybernetics,</journal>
<volume>19</volume>
<pages>17--30</pages>
<contexts>
<context position="5272" citStr="Rada et al., 1989" startWordPosition="849" endWordPosition="852"> target word and its target selectors, while relatedness measures the relationship between a target word and context selectors from other parts of the sentence. Thus, the use of selectors in disambiguating words relies on a couple assumptions: 1. Concepts which appear in matching syntactic constructions are similar. 2. Concepts which appear in the context of a given target word are related to the correct sense of the target word. Note that ‘concept’ and ‘word sense’ are used interchangeably throughout this paper. This idea of distinguishing similarity and relatedness has an extensive history (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in t</context>
</contexts>
<marker>Rada, Mili, Bicknell, Blettner, 1989</marker>
<rawString>R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989. Development and application of a metric on semantic nets. In IEEE Transactions on Systems, Man and Cybernetics, volume 19, pages 17–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--95</pages>
<contexts>
<context position="5286" citStr="Resnik, 1999" startWordPosition="853" endWordPosition="854">s target selectors, while relatedness measures the relationship between a target word and context selectors from other parts of the sentence. Thus, the use of selectors in disambiguating words relies on a couple assumptions: 1. Concepts which appear in matching syntactic constructions are similar. 2. Concepts which appear in the context of a given target word are related to the correct sense of the target word. Note that ‘concept’ and ‘word sense’ are used interchangeably throughout this paper. This idea of distinguishing similarity and relatedness has an extensive history (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in that queries we</context>
<context position="11372" citStr="Resnik, 1999" startWordPosition="1809" endWordPosition="1810">* the strikers at get:1, keep:1, price:1, treat:1 Table 1: Lists of selectors for the target words ‘striker’ and ‘address’ returned by corresponding web queries. the same version used to annotate our chosen experimental corpus. A relatedness measure was used with context selectors, and we chose the adapted Lesk algorithm (Banerjee and Pedersen, 2002). An important characteristic of this measure is that it can handle multiple parts of speech. For target selectors we sought to use measures over the WordNet ontology in order to most closely measure similarity. An informationcontent (IC) measure (Resnik, 1999) was used for target selectors of nouns and verbs. However, because IC measures do not work with all parts of speech, we used the adapted Lesk algorithm as an approximation of similarity for adjectives and adverbs. Note that finding the best relatedness or similarity measure was outside the scope of this paper. The following function, based on Resnik’s word similarity (Resnik, 1999), is used to find the max similarity or relatedness between a concept and a word (specifically between a sense of the target word, ct and a selector, ws). maxsr(ct, ws) = max [meas(ct, cs)] c3∈w3 where cs is a sense</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hansen A Schwartz</author>
<author>Fernando Gomez</author>
</authors>
<title>Acquiring knowledge from the web to be used as selectors for noun sense disambiguation.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>105--112</pages>
<location>Manchester, England,</location>
<contexts>
<context position="1924" citStr="Schwartz and Gomez, 2008" startWordPosition="299" endWordPosition="302">source of natural language. Recently, word sense disambiguation systems have fostered the size of the Web in order to supplant the issue of limited annotated data availability for supervised systems (Mihalcea, 2002; Agirre and Martinez, 2004). Some unsupervised or minimally supervised methods use the Web more directly in disambiguation algorithms that do not use a training set for the specific target words. One such minimally supervised method uses selectors acquired from the Web for noun sense disambiguation by comparing the selectors of a given sentence to a target noun within the sentence (Schwartz and Gomez, 2008). Although this work found strong results, many aspects of the use of selectors was left unexplored. For one, the method was only applied to noun sense disambiguation, focusing on the welldeveloped noun hypernym hierarchy within WordNet (Miller et al., 1993). Additionally, the role of different types of selectors was not extensively explored, and adverb selectors were not used at all. We seek to address those issues. In this paper, we extend our method of using selectors from the Web for noun sense disambiguation into a more robust method of disambiguating words of all parts of speech. After a</context>
<context position="4169" citStr="Schwartz and Gomez, 2008" startWordPosition="665" endWordPosition="668">lorado, June 2009. c�2009 Association for Computational Linguistics Lin searched a dependency relationship database in order to match local context, it is not yet possible to parse dependency relationships of the entire Web. In turn, one must search for text as local context. For example, in the sentence below, the local context for ‘strikers’ would be composed of “he addressed the” and “at the rally.”. He addressed the strikers at the rally. Previously, we introduced the idea of using selectors of other words in a sentence in addition to selectors of the target, the word being disambiguated (Schwartz and Gomez, 2008). Words taking the place of a target word are referred to as target selectors and words which take the place of other words in a sentence are referred to as context selectors. Context selectors can be classified further based on their part of speech. In our example, if ‘striker’ was the target word, the verb context selectors would be verbs replacing ‘addressed’ and the noun context selectors would be nouns replacing ‘rally’. Similarity is used to measure the relationship between a target word and its target selectors, while relatedness measures the relationship between a target word and conte</context>
<context position="7246" citStr="Schwartz and Gomez, 2008" startWordPosition="1166" endWordPosition="1169"> as (Sinha and Mihalcea, 2007), use similarity or relatedness measures over graphs connecting words within a sentence. Likewise, (Navigli and Velardi, 2005) analyze the connectivity of concepts in a sentence among Structured Semantic Interconnections (SSI), graphs of relationships based on many knowledge sources. These works do not use selectors or the Web. Additionally, target selectors and context selectors provide an application for the distinction between similarity and relatedness not used in these other methods. Several ideas distinguish this current work from our research described in (Schwartz and Gomez, 2008). The most notable aspect is that we have generalized the overall method of using Web selectors into disambiguating verbs, adverbs, and adjectives in addition to nouns. Another difference is the inclusion of selectors for adverbs. Finally, we also explore the actual impact that each type of selector has on the performance of the disambiguation algorithm. 3 Approach In this section we describe the Web Selector algorithm such that verbs, adjectives, and adverbs are disambiguated in addition to nouns. The algorithm essentially runs in two steps: acquisition of selectors and application of selecto</context>
<context position="9255" citStr="Schwartz and Gomez, 2008" startWordPosition="1509" endWordPosition="1512">phrases with wildcards. Selectors are extracted from the samples returned from the web search by matching the words which take the place of the wildcard. All words not found in WordNet under the same part of speech as the target are thrown out as well as phrases longer than 4 words or those containing punctuation. The system enters a loop where it: • searches the web with a given query, and • extracts selectors from the web samples. The query is truncated and the search is repeated until a goal for the number of selectors was reached or the query becomes too short. This approach, detailed in (Schwartz and Gomez, 2008), removes select punctuation, determiners, and gradually shortens the query one word at a time. Selectors retrieved from a larger query are removed from the results of smaller queries as the smaller queries should subsume the larger query results. Some selectors retrieved for the example, with their corresponding web query are listed in Table 1. 3.2 Similarity and Relatedness To apply selectors in disambiguation, similarity and relatedness measures are used to compare the selectors with the target word. We incorporate the use of a few previously defined measures over WordNet (Miller et al., 19</context>
<context position="17158" citStr="Schwartz and Gomez, 2008" startWordPosition="2810" endWordPosition="2813">below that of the top system not utilizing training data, (UPV-WSD (Buscaldi and Rosso, 2007)), and a little over 6 percentage points below the top supervised system (NUSPT (Chan et al., 2007)). The results are broken down by part of speech in Table 4. We see that adjective disambiguation was the furthest above our median point of reference, and noun disambiguation results were above the MFS baseline. On the other hand, our adverb disambiguation results appear weakest compared to the baselines. Note that we previously reported a noun sense disambiguation F1 value of 80.20% on the same corpus (Schwartz and Gomez, 2008). Current results differ because the previous work used N V A R MED 70.76 62.10 71.55 74.04 WS 78.52 68.36 81.21 75.48 BLMFS 77.44 75.30 84.25 87.50 insts 1108 591 362 208 Table 4: Results as F1 values (precision = recall) of our system by parts of speech (N = noun, V = verb, A = adjective, R = adverb). insts = disambiguation instances of each part of speech. For other keys see Table 2. different scale(T) values as well as a custom noun similarity measure. 4.2 Selector Acquisition Analysis We examine the occurrences of acquired selectors. Listed as the column headings of Table 5, selectors are</context>
</contexts>
<marker>Schwartz, Gomez, 2008</marker>
<rawString>Hansen A. Schwartz and Fernando Gomez. 2008. Acquiring knowledge from the web to be used as selectors for noun sense disambiguation. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 105–112, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised graph-based word sense disambiguation using measures of word semantic similarity.</title>
<date>2007</date>
<location>Irvine, CA,</location>
<contexts>
<context position="6651" citStr="Sinha and Mihalcea, 2007" startWordPosition="1077" endWordPosition="1080"> acquire words matching a wildcard in the search rather than incorporate a knowledge-base to construct queries with pre-chosen relatives. Consequently, the later half of our algorithm uses a knowledge-base through similarity and relatedness measures. Some recent works have used similarity or relatedness measures to assist with WSD. Particuarly, (Patwardhan et al., 2003) provide evaluations of various relatedness measures for word sense disambiguation based on words in context. These evaluations helped us choose the similarity and relatedness measures to use in this work. Other works, such as (Sinha and Mihalcea, 2007), use similarity or relatedness measures over graphs connecting words within a sentence. Likewise, (Navigli and Velardi, 2005) analyze the connectivity of concepts in a sentence among Structured Semantic Interconnections (SSI), graphs of relationships based on many knowledge sources. These works do not use selectors or the Web. Additionally, target selectors and context selectors provide an application for the distinction between similarity and relatedness not used in these other methods. Several ideas distinguish this current work from our research described in (Schwartz and Gomez, 2008). The</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2007. Unsupervised graph-based word sense disambiguation using measures of word semantic similarity. Irvine, CA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>KU: Word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>207--214</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5611" citStr="Yuret, 2007" startWordPosition="910" endWordPosition="911">n the context of a given target word are related to the correct sense of the target word. Note that ‘concept’ and ‘word sense’ are used interchangeably throughout this paper. This idea of distinguishing similarity and relatedness has an extensive history (Rada et al., 1989; Resnik, 1999; Patwardhan et al., 2003; Budanitsky and Hirst, 2006), but most algorithms only find a use for one or the other. 2.2 Related Word Sense Disambiguation A key aspect of using selectors for disambiguation is the inclusion of context in the Web search queries. This was done in works by (Martinez et al., 2006) and (Yuret, 2007), which substituted relatives or similar words in place of the target word within a given context. The context, restricted with a window size, helped to limit the results from the Web. These works followed (Mihalcea and Moldovan, 1999; Agirre et al., 2001) in that queries were constructed through the use of a knowledge-base, filling the queries with pre-chosen words. We also use context in the web search, but we acquire words matching a wildcard in the search rather than incorporate a knowledge-base to construct queries with pre-chosen relatives. Consequently, the later half of our algorithm u</context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. 2007. KU: Word sense disambiguation by substitution. In Proceedings of SemEval-2007, pages 207–214, Prague, Czech Republic, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>