<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000548">
<title confidence="0.7084745">
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages
Marta Recasens* Lluis M`arquez† Emili Sapena† M. Ant`onia Marti*
</title>
<author confidence="0.963241">
Mariona Taul´e* V´eronique Hoste‡ Massimo Poesio° Yannick Versley**
</author>
<affiliation confidence="0.9303012">
*: CLiC, University of Barcelona, {mrecasens,amarti,mtaule}@ub.edu
†: TALP, Technical University of Catalonia, {lluism,esapena}@lsi.upc.edu
‡: University College Ghent, veronique.hoste@hogent.be
o: University of Essex/University of Trento, poesio@essex.ac.uk
**: University of T¨ubingen, versley@sfs.uni-tuebingen.de
</affiliation>
<sectionHeader confidence="0.982535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953285714286">
This paper presents the SemEval-2010
task on Coreference Resolution in Multi-
ple Languages. The goal was to evaluate
and compare automatic coreference reso-
lution systems for six different languages
(Catalan, Dutch, English, German, Italian,
and Spanish) in four evaluation settings
and using four different metrics. Such a
rich scenario had the potential to provide
insight into key issues concerning corefer-
ence resolution: (i) the portability of sys-
tems across languages, (ii) the relevance of
different levels of linguistic information,
and (iii) the behavior of scoring metrics.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99979225">
The task of coreference resolution, defined as the
identification of the expressions in a text that re-
fer to the same discourse entity (1), has attracted
considerable attention within the NLP community.
</bodyText>
<listItem confidence="0.725313">
(1) Major League Baseball sent its head of se-
</listItem>
<bodyText confidence="0.9808606875">
curity to Chicago to review the second in-
cident of an on-field fan attack in the last
seven months. The league is reviewing se-
curity at all ballparks to crack down on
spectator violence.
Using coreference information has been shown to
be beneficial in a number of NLP applications
including Information Extraction (McCarthy and
Lehnert, 1995), Text Summarization (Steinberger
et al., 2007), Question Answering (Morton, 1999),
and Machine Translation. There have been a few
evaluation campaigns on coreference resolution in
the past, namely MUC (Hirschman and Chinchor,
1997), ACE (Doddington et al., 2004), and ARE
(Orasan et al., 2008), yet many questions remain
open:
</bodyText>
<listItem confidence="0.991409384615385">
• To what extent is it possible to imple-
ment a general coreference resolution system
portable to different languages? How much
language-specific tuning is necessary?
• How helpful are morphology, syntax and se-
mantics for solving coreference relations?
How much preprocessing is needed? Does its
quality (perfect linguistic input versus noisy
automatic input) really matter?
• How (dis)similar are different coreference
evaluation metrics—MUC, B-CUBED,
CEAF and BLANC? Do they all provide the
same ranking? Are they correlated?
</listItem>
<bodyText confidence="0.999962235294118">
Our goal was to address these questions in a
shared task. Given six datasets in Catalan, Dutch,
English, German, Italian, and Spanish, the task
we present involved automatically detecting full
coreference chains—composed of named entities
(NEs), pronouns, and full noun phrases—in four
different scenarios. For more information, the
reader is referred to the task website.1
The rest of the paper is organized as follows.
Section 2 presents the corpora from which the task
datasets were extracted, and the automatic tools
used to preprocess them. In Section 3, we describe
the task by providing information about the data
format, evaluation settings, and evaluation met-
rics. Participating systems are described in Sec-
tion 4, and their results are analyzed and compared
in Section 5. Finally, Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.987572" genericHeader="method">
2 Linguistic Resources
</sectionHeader>
<bodyText confidence="0.9258445">
In this section, we first present the sources of the
data used in the task. We then describe the auto-
matic tools that predicted input annotations for the
coreference resolution systems.
</bodyText>
<footnote confidence="0.990009">
1http://stel.ub.edu/semeval2010-coref
</footnote>
<page confidence="0.498529">
1
</page>
<note confidence="0.9809945">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.994057">
#docs Training #tokens #docs Development #docs Test #tokens
#sents #sents #tokens #sents
Catalan 829 8,709 253,513 142 1,445 42,072 167 1,698 49,260
Dutch 145 2,544 46,894 23 496 9,165 72 2,410 48,007
English 229 3,648 79,060 39 741 17,044 85 1,141 24,206
German 900 19,233 331,614 199 4,129 73,145 136 2,736 50,287
Italian 80 2,951 81,400 17 551 16,904 46 1,494 41,586
Spanish 875 9,022 284,179 140 1,419 44,460 168 1,705 51,040
</table>
<tableCaption confidence="0.999901">
Table 1: Size of the task datasets.
</tableCaption>
<subsectionHeader confidence="0.995985">
2.1 Source Corpora
</subsectionHeader>
<bodyText confidence="0.999640083333333">
Catalan and Spanish The AnCora corpora (Re-
casens and Mart´ı, 2009) consist of a Catalan and
a Spanish treebank of 500k words each, mainly
from newspapers and news agencies (El Peri´odico,
EFE, ACN). Manual annotation exists for ar-
guments and thematic roles, predicate semantic
classes, NEs, WordNet nominal senses, and coref-
erence relations. AnCora are freely available for
research purposes.
Dutch The KNACK-2002 corpus (Hoste and De
Pauw, 2006) contains 267 documents from the
Flemish weekly magazine Knack. They were
manually annotated with coreference information
on top of semi-automatically annotated PoS tags,
phrase chunks, and NEs.
English The OntoNotes Release 2.0 corpus
(Pradhan et al., 2007) covers newswire and broad-
cast news data: 300k words from The Wall Street
Journal, and 200k words from the TDT-4 col-
lection, respectively. OntoNotes builds on the
Penn Treebank for syntactic annotation and on the
Penn PropBank for predicate argument structures.
Semantic annotations include NEs, words senses
(linked to an ontology), and coreference informa-
tion. The OntoNotes corpus is distributed by the
Linguistic Data Consortium.2
German The T¨uBa-D/Z corpus (Hinrichs et al.,
2005) is a newspaper treebank based on data taken
from the daily issues of “die tageszeitung” (taz). It
currently comprises 794k words manually anno-
tated with semantic and coreference information.
Due to licensing restrictions of the original texts, a
taz-DVD must be purchased to obtain a license.2
Italian The LiveMemories corpus (Rodr´ıguez
et al., 2010) will include texts from the Italian
Wikipedia, blogs, news articles, and dialogues
</bodyText>
<footnote confidence="0.918665">
2Free user license agreements for the English and German
task datasets were issued to the task participants.
</footnote>
<bodyText confidence="0.9996581">
(MapTask). They are being annotated according
to the ARRAU annotation scheme with coref-
erence, agreement, and NE information on top
of automatically parsed data. The task dataset
included Wikipedia texts already annotated.
The datasets that were used in the task were ex-
tracted from the above-mentioned corpora. Ta-
ble 1 summarizes the number of documents
(docs), sentences (sents), and tokens in the train-
ing, development and test sets.3
</bodyText>
<subsectionHeader confidence="0.999872">
2.2 Preprocessing Systems
</subsectionHeader>
<bodyText confidence="0.998976333333333">
Catalan, Spanish, English Predicted lemmas
and PoS were generated using FreeLing4 for
Catalan/Spanish and SVMTagger5 for English.
Dependency information and predicate semantic
roles were generated with JointParser, a syntactic-
semantic parser.6
Dutch Lemmas, PoS and NEs were automat-
ically provided by the memory-based shallow
parser for Dutch (Daelemans et al., 1999), and de-
pendency information by the Alpino parser (van
Noord et al., 2006).
German Lemmas were predicted by TreeTagger
(Schmid, 1995), PoS and morphology by RFTag-
ger (Schmid and Laws, 2008), and dependency in-
formation by MaltParser (Hall and Nivre, 2008).
Italian Lemmas and PoS were provided by
TextPro,7 and dependency information by Malt-
Parser.8
</bodyText>
<footnote confidence="0.971445">
3The German and Dutch training datasets were not com-
pletely stable during the competition period due to a few er-
rors. Revised versions were released on March 2 and 20, re-
spectively. As to the test datasets, the Dutch and Italian doc-
uments with formatting errors were corrected after the eval-
uation period, with no variations in the ranking order of sys-
tems.
4http://www.lsi.upc.es/ nlp/freeling
5http://www.lsi.upc.edu/ nlp/SVMTool
6http://www.lsi.upc.edu// xlluis/?x=cat:5
7http://textpro.fbk.eu
8http://maltparser.org
</footnote>
<page confidence="0.997012">
2
</page>
<sectionHeader confidence="0.994097" genericHeader="method">
3 Task Description
</sectionHeader>
<bodyText confidence="0.999951">
Participants were asked to develop an automatic
system capable of assigning a discourse entity to
every mention,9 thus identifying all the NP men-
tions of every discourse entity. As there is no
standard annotation scheme for coreference and
the source corpora differed in certain aspects, the
coreference information of the task datasets was
produced according to three criteria:
</bodyText>
<listItem confidence="0.996592285714286">
• Only NP constituents and possessive deter-
miners can be mentions.
• Mentions must be referential expressions,
thus ruling out nominal predicates, appos-
itives, expletive NPs, attributive NPs, NPs
within idioms, etc.
• Singletons are also considered as entities
</listItem>
<bodyText confidence="0.949806125">
(i.e., entities with a single mention).
To help participants build their systems, the
task datasets also contained both gold-standard
and automatically predicted linguistic annotations
at the morphological, syntactic and semantic lev-
els. Considerable effort was devoted to provide
participants with a common and relatively simple
data representation for the six languages.
</bodyText>
<subsectionHeader confidence="0.998882">
3.1 Data Format
</subsectionHeader>
<bodyText confidence="0.999931722222222">
The task datasets as well as the participants’
answers were displayed in a uniform column-
based format, similar to the style used in previous
CoNLL shared tasks on syntactic and semantic de-
pendencies (2008/2009).10 Each dataset was pro-
vided as a single file per language. Since corefer-
ence is a linguistic relation at the discourse level,
documents constitute the basic unit, and are de-
limited by “#begin document ID” and “#end doc-
ument ID” comment lines. Within a document, the
information of each sentence is organized verti-
cally with one token per line, and a blank line after
the last token of each sentence. The information
associated with each token is described in several
columns (separated by “\t” characters) represent-
ing the following layers of linguistic annotation.
ID (column 1). Token identifiers in the sentence.
Token (column 2). Word forms.
</bodyText>
<footnote confidence="0.9865815">
9Following the terminology of the ACE program, a men-
tion is defined as an instance of reference to an object, and
an entity is the collection of mentions referring to the same
object in a document.
</footnote>
<page confidence="0.354679">
10http://www.cnts.ua.ac.be/conll2008
</page>
<figure confidence="0.755147928571429">
ID Token Intermediate columns Coref
1 Major ... (1
2 League ...
3 Baseball ... 1)
4 sent ...
5 its ...
6 head ...
7 of ...
8 security ... (3)12)
9 to ...
. . . . . . . . . . . .
27 The ... (1
28 league ... 1)
29 is ...
</figure>
<tableCaption confidence="0.618913">
Table 2: Format of the coreference annotations
(corresponding to example (1) in Section 1).
</tableCaption>
<bodyText confidence="0.995197942857143">
Lemma (column 3). Token lemmas.
PoS (column 5). Coarse PoS.
Feat (column 7). Morphological features (PoS
type, number, gender, case, tense, aspect,
etc.) separated by a pipe character.
Head (column 9). ID of the syntactic head (“0” if
the token is the tree root).
DepRel (column 11). Dependency relations cor-
responding to the dependencies described in
the Head column (“sentence” if the token is
the tree root).
NE (column 13). NE types in open-close notation.
Pred (column 15). Predicate semantic class.
APreds (column 17 and subsequent ones). For
each predicate in the Pred column, its seman-
tic roles/dependencies.
Coref (last column). Coreference relations in
open-close notation.
The above-mentioned columns are “gold-
standard columns,” whereas columns 4, 6, 8, 10,
12, 14, 16 and the penultimate contain the same
information as the respective previous column but
automatically predicted—using the preprocessing
systems listed in Section 2.2. Neither all layers
of linguistic annotation nor all gold-standard and
predicted columns were available for all six lan-
guages (underscore characters indicate missing in-
formation).
The coreference column follows an open-close
notation with an entity number in parentheses (see
Table 2). Every entity has an ID number, and ev-
ery mention is marked with the ID of the entity
it refers to: an opening parenthesis shows the be-
ginning of the mention (first token), while a clos-
ing parenthesis shows the end of the mention (last
</bodyText>
<equation confidence="0.600764">
(1)x(2
</equation>
<page confidence="0.96823">
3
</page>
<bodyText confidence="0.99973875">
token). For tokens belonging to more than one
mention, a pipe character is used to separate mul-
tiple entity IDs. The resulting annotation is a well-
formed nested structure (CF language).
</bodyText>
<subsectionHeader confidence="0.999096">
3.2 Evaluation Settings
</subsectionHeader>
<bodyText confidence="0.9999824">
In order to address our goal of studying the effect
of different levels of linguistic information (pre-
processing) on solving coreference relations, the
test was divided into four evaluation settings that
differed along two dimensions.
</bodyText>
<subsectionHeader confidence="0.378477">
Gold-standard versus Regular setting. Only
</subsectionHeader>
<bodyText confidence="0.999873304347826">
in the gold-standard setting were participants al-
lowed to use the gold-standard columns, includ-
ing the last one (of the test dataset) with true
mention boundaries. In the regular setting, they
were allowed to use only the automatically pre-
dicted columns. Obtaining better results in the
gold setting would provide evidence for the rel-
evance of using high-quality preprocessing infor-
mation. Since not all columns were available for
all six languages, the gold setting was only possi-
ble for Catalan, English, German, and Spanish.
Closed versus Open setting. In the closed set-
ting, systems had to be built strictly with the in-
formation provided in the task datasets. In con-
trast, there was no restriction on the resources that
participants could utilize in the open setting: sys-
tems could be developed using any external tools
and resources to predict the preprocessing infor-
mation, e.g., WordNet, Wikipedia, etc. The only
requirement was to use tools that had not been de-
veloped with the annotations of the test set. This
setting provided an open door into tools or re-
sources that improve performance.
</bodyText>
<subsectionHeader confidence="0.995407">
3.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999991411764706">
Since there is no agreement at present on a stan-
dard measure for coreference resolution evalua-
tion, one of our goals was to compare the rank-
ings produced by four different measures. The
task scorer provides results in the two mention-
based metrics B3 (Bagga and Baldwin, 1998) and
CEAF-φ3 (Luo, 2005), and the two link-based
metrics MUC (Vilain et al., 1995) and BLANC
(Recasens and Hovy, in prep). The first three mea-
sures have been widely used, while BLANC is a
proposal of a new measure interesting to test.
The mention detection subtask is measured with
recall, precision, and F1. Mentions are rewarded
with 1 point if their boundaries coincide with those
of the gold NP, with 0.5 points if their boundaries
are within the gold NP including its head, and
with 0 otherwise.
</bodyText>
<sectionHeader confidence="0.96275" genericHeader="method">
4 Participating Systems
</sectionHeader>
<bodyText confidence="0.9999805">
A total of twenty-two participants registered for
the task and downloaded the training materials.
From these, sixteen downloaded the test set but
only six (out of which two task organizers) sub-
mitted valid results (corresponding to nine system
runs or variants). These numbers show that the
task raised considerable interest but that the final
participation rate was comparatively low (slightly
below 30%).
The participating systems differed in terms of
architecture, machine learning method, etc. Ta-
ble 3 summarizes their main properties. Systems
like BART and Corry support several machine
learners, but Table 3 indicates the one used for the
SemEval run. The last column indicates the exter-
nal resources that were employed in the open set-
ting, thus it is empty for systems that participated
only in the closed setting. For more specific details
we address the reader to the system description pa-
pers in Erk and Strapparava (2010).
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="evaluation">
5 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999868739130435">
Table 4 shows the results obtained by two naive
baseline systems: (i) SINGLETONS considers each
mention as a separate entity, and (ii) ALL-IN-ONE
groups all the mentions in a document into a sin-
gle entity. These simple baselines reveal limita-
tions of the evaluation metrics, like the high scores
of CEAF and B3 for SINGLETONS. Interestingly
enough, the naive baseline scores turn out to be
hard to beat by the participating systems, as Ta-
ble 5 shows. Similarly, ALL-IN-ONE obtains high
scores in terms of MUC. Table 4 also reveals dif-
ferences between the distribution of entities in the
datasets. Dutch is clearly the most divergent cor-
pus mainly due to the fact that it only contains sin-
gletons for NEs.
Table 5 displays the results of all systems for all
languages and settings in the four evaluation met-
rics (the best scores in each setting are highlighted
in bold). Results are presented sequentially by lan-
guage and setting, and participating systems are
ordered alphabetically. The participation of sys-
tems across languages and settings is rather irreg-
ular,11 thus making it difficult to draw firm conclu-
</bodyText>
<footnote confidence="0.83288">
11Only 45 entries in Table 5 from 192 potential cases.
</footnote>
<page confidence="0.991826">
4
</page>
<table confidence="0.998795789473684">
System Architecture ML Methods External Resources
BART Closest-first with entity- MaxEnt (English, Ger- GermaNet &amp; gazetteers (Ger-
(Broscheit et al., 2010) mention model (English), man), Decision trees man), I-Cab gazetteers (Italian),
Closest-first model (German, (Italian) Berkeley parser, Stanford NER,
Italian) WordNet, Wikipedia name list,
U.S. census data (English)
Corry ILP, Pairwise model SVM Stanford parser &amp; NER, Word-
(Uryupina, 2010) Net, U.S. census data
RelaxCor Graph partitioning (solved by Decision trees, Rules WordNet
(Sapena et al., 2010) relaxation labeling)
SUCRE
(Kobdani and Sch¨utze, 2010) Best-first clustering, Rela- Decision trees, Naive —
tional database model, Regular Bayes, SVM, MaxEnt
feature definition language
TANL-1
(Attardi et al., 2010) Highest entity-mention simi- MaxEnt PoS tagger (Italian)
larity
UBIU
(Zhekova and K¨ubler, 2010) Pairwise model MBL —
</table>
<tableCaption confidence="0.999884">
Table 3: Main characteristics of the participating systems.
</tableCaption>
<bodyText confidence="0.999957981481482">
sions about the aims initially pursued by the task.
In the following, we summarize the most relevant
outcomes of the evaluation.
Regarding languages, English concentrates the
most participants (fifteen entries), followed by
German (eight), Catalan and Spanish (seven each),
Italian (five), and Dutch (three). The number of
languages addressed by each system ranges from
one (Corry) to six (UBIU and SUCRE); BART and
RelaxCor addressed three languages, and TANL-1
five. The best overall results are obtained for En-
glish followed by German, then Catalan, Spanish
and Italian, and finally Dutch. Apart from differ-
ences between corpora, there are other factors that
might explain this ranking: (i) the fact that most of
the systems were originally developed for English,
and (ii) differences in corpus size (German having
the largest corpus, and Dutch the smallest).
Regarding systems, there are no clear “win-
ners.” Note that no language-setting was ad-
dressed by all six systems. The BART system,
for instance, is either on its own or competing
against a single system. It emerges from par-
tial comparisons that SUCRE performs the best in
closedxregular for English, German, and Italian,
although it never outperforms the CEAF or B3 sin-
gleton baseline. While SUCRE always obtains the
best scores according to MUC and BLANC, Re-
laxCor and TANL-1 usually win based on CEAF
and B3. The Corry system presents three variants
optimized for CEAF (Corry-C), MUC (Corry-M),
and BLANC (Corry-B). Their results are consis-
tent with the bias introduced in the optimization
(see English:openxgold).
Depending on the evaluation metric then, the
rankings of systems vary with considerable score
differences. There is a significant positive corre-
lation between CEAF and B3 (Pearson’s r = 0.91,
p &lt; 0.01), and a significant lack of correlation be-
tween CEAF and MUC in terms of recall (Pear-
son’s r = 0.44, p &lt; 0.01). This fact stresses the
importance of defining appropriate metrics (or a
combination of them) for coreference evaluation.
Finally, regarding evaluation settings, the re-
sults in the gold setting are significantly better than
those in the regular. However, this might be a di-
rect effect of the mention recognition task. Men-
tion recognition in the regular setting falls more
than 20 F1 points with respect to the gold setting
(where correct mention boundaries were given).
As for the open versus closed setting, there is only
one system, RelaxCor for English, that addressed
the two. As expected, results show a slight im-
provement from closedxgold to openxgold.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.954401">
This paper has introduced the main features of
the SemEval-2010 task on coreference resolution.
</bodyText>
<page confidence="0.990641">
5
</page>
<table confidence="0.9988340625">
CEAF MUC B3 BLANC
R P F1 R P F1 R P F1 R P Blanc
SINGLETONS: Each mention forms a separate entity.
Catalan 61.2 61.2 61.2 0.0 0.0 0.0 61.2 100 75.9 50.0 48.7 49.3
Dutch 34.5 34.5 34.5 0.0 0.0 0.0 34.5 100 51.3 50.0 46.7 48.3
English 71.2 71.2 71.2 0.0 0.0 0.0 71.2 100 83.2 50.0 49.2 49.6
German 75.5 75.5 75.5 0.0 0.0 0.0 75.5 100 86.0 50.0 49.4 49.7
Italian 71.1 71.1 71.1 0.0 0.0 0.0 71.1 100 83.1 50.0 49.2 49.6
Spanish 62.2 62.2 62.2 0.0 0.0 0.0 62.2 100 76.7 50.0 48.8 49.4
ALL-IN-ONE: All mentions are grouped into a single entity.
Catalan 11.8 11.8 11.8 100 39.3 56.4 100 4.0 7.7 50.0 1.3 2.6
Dutch 19.7 19.7 19.7 100 66.3 79.8 100 8.0 14.9 50.0 3.2 6.2
English 10.5 10.5 10.5 100 29.2 45.2 100 3.5 6.7 50.0 0.8 1.6
German 8.2 8.2 8.2 100 24.8 39.7 100 2.4 4.7 50.0 0.6 1.1
Italian 11.4 11.4 11.4 100 29.0 45.0 100 2.1 4.1 50.0 0.8 1.5
Spanish 11.9 11.9 11.9 100 38.3 55.4 100 3.9 7.6 50.0 1.2 2.4
</table>
<tableCaption confidence="0.999797">
Table 4: Baseline scores.
</tableCaption>
<bodyText confidence="0.999973820512821">
The goal of the task was to evaluate and compare
automatic coreference resolution systems for six
different languages in four evaluation settings and
using four different metrics. This complex sce-
nario aimed at providing insight into several as-
pects of coreference resolution, including portabil-
ity across languages, relevance of linguistic infor-
mation at different levels, and behavior of alterna-
tive scoring metrics.
The task attracted considerable attention from a
number of researchers, but only six teams submit-
ted their final results. Participating systems did not
run their systems for all the languages and evalu-
ation settings, thus making direct comparisons be-
tween them very difficult. Nonetheless, we were
able to observe some interesting aspects from the
empirical evaluation.
An important conclusion was the confirmation
that different evaluation metrics provide different
system rankings and the scores are not commen-
surate. Attention thus needs to be paid to corefer-
ence evaluation. The behavior and applicability of
the scoring metrics requires further investigation
in order to guarantee a fair evaluation when com-
paring systems in the future. We hope to have the
opportunity to thoroughly discuss this and the rest
of interesting questions raised by the task during
the SemEval workshop at ACL 2010.
An additional valuable benefit is the set of re-
sources developed throughout the task. As task
organizers, we intend to facilitate the sharing of
datasets, scorers, and documentation by keeping
them available for future research use. We believe
that these resources will help to set future bench-
marks for the research community and will con-
tribute positively to the progress of the state of the
art in coreference resolution. We will maintain and
update the task website with post-SemEval contri-
butions.
</bodyText>
<sectionHeader confidence="0.985628" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999985038461539">
We would like to thank the following peo-
ple who contributed to the preparation of the
task datasets: Manuel Bertran (UB), Oriol
Borrega (UB), Orph´ee De Clercq (U. Ghent),
Francesca Delogu (U. Trento), Jes´us Gim´enez
(UPC), Eduard Hovy (ISI-USC), Richard Johans-
son (U. Trento), Xavier Llu´ıs (UPC), Montse
Nofre (UB), Llu´ıs Padr´o (UPC), Kepa Joseba
Rodr´ıguez (U. Trento), Mihai Surdeanu (Stan-
ford), Olga Uryupina (U. Trento), Lente Van Leu-
ven (UB), and Rita Zaragoza (UB). We would also
like to thank LDC and die tageszeitung for dis-
tributing freely the English and German datasets.
This work was funded in part by the Span-
ish Ministry of Science and Innovation through
the projects TEXT-MESS 2.0 (TIN2009-13391-
C04-04), OpenMT-2 (TIN2009-14675-C03), and
KNOW2 (TIN2009-14715-C04-04), and an FPU
doctoral scholarship (AP2006-00994) held by
M. Recasens. It also received financial sup-
port from the Seventh Framework Programme
of the EU (FP7/2007-2013) under GA 247762
(FAUST), from the STEVIN program of the Ned-
erlandse Taalunie through the COREA and SoNaR
projects, and from the Provincia Autonoma di
Trento through the LiveMemories project.
</bodyText>
<page confidence="0.999043">
6
</page>
<table confidence="0.99997012987013">
Mention detection CEAF MUC B3 BLANC
R P F1 R P F1 R P F1 R P F1 R P Blanc
Catalan
closed×gold
RelaxCor 100 100 100 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
SUCRE 100 100 100 68.7 68.7 68.7 54.1 58.4 56.2 76.6 77.4 77.0 72.4 60.2 63.6
TANL-1 100 96.8 98.4 66.0 63.9 64.9 17.2 57.7 26.5 64.4 93.3 76.2 52.8 79.8 54.4
UBIU 75.1 96.3 84.4 46.6 59.6 52.3 8.8 17.1 11.7 47.8 76.3 58.8 51.6 57.9 52.2
closed×regular
SUCRE 75.9 64.5 69.7 51.3 43.6 47.2 44.1 32.3 37.3 59.6 44.7 51.1 53.9 55.2 54.2
TANL-1 83.3 82.0 82.7 57.5 56.6 57.1 15.2 46.9 22.9 55.8 76.6 64.6 51.3 76.2 51.0
UBIU 51.4 70.9 59.6 33.2 45.7 38.4 6.5 12.6 8.6 32.4 55.7 40.9 50.2 53.7 47.8
open×gold
open×regular
Dutch
closed×gold
SUCRE 100 100 100 58.8 58.8 58.8 65.7 74.4 69.8 65.0 69.2 67.0 69.5 62.9 65.3
closed×regular
SUCRE 78.0 29.0 42.3 29.4 10.9 15.9 62.0 19.5 29.7 59.1 6.5 11.7 46.9 46.9 46.9
UBIU 41.5 29.9 34.7 20.5 14.6 17.0 6.7 11.0 8.3 13.3 23.4 17.0 50.0 52.4 32.3
open×gold
open×regular
English
closed×gold
RelaxCor 100 100 100 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
SUCRE 100 100 100 74.3 74.3 74.3 68.1 54.9 60.8 86.7 78.5 82.4 77.3 67.0 70.8
TANL-1 99.8 81.7 89.8 75.0 61.4 67.6 23.7 24.4 24.0 74.6 72.1 73.4 51.8 68.8 52.1
UBIU 92.5 99.5 95.9 63.4 68.2 65.7 17.2 25.5 20.5 67.8 83.5 74.8 52.6 60.8 54.0
closed×regular
SUCRE 78.4 83.0 80.7 61.0 64.5 62.7 57.7 48.1 52.5 68.3 65.9 67.1 58.9 65.7 61.2
TANL-1 79.6 68.9 73.9 61.7 53.4 57.3 23.8 25.5 24.6 62.1 60.5 61.3 50.9 68.0 49.3
UBIU 66.7 83.6 74.2 48.2 60.4 53.6 11.6 18.4 14.2 50.9 69.2 58.7 50.9 56.3 51.0
open×gold
Corry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8
Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5
Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7
RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
open×regular
BART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7
Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6
Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4
Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1
German
closed×gold
SUCRE 100 100 100 72.9 72.9 72.9 74.4 48.1 58.4 90.4 73.6 81.1 78.2 61.8 66.4
TANL-1 100 100 100 77.7 77.7 77.7 16.4 60.6 25.9 77.2 96.7 85.9 54.4 75.1 57.4
UBIU 92.6 95.5 94.0 67.4 68.9 68.2 22.1 21.7 21.9 73.7 77.9 75.7 60.0 77.2 64.5
closed×regular
SUCRE 79.3 77.5 78.4 60.6 59.2 59.9 49.3 35.0 40.9 69.1 60.1 64.3 52.7 59.3 53.6
TANL-1 60.9 57.7 59.2 50.9 48.2 49.5 10.2 31.5 15.4 47.2 54.9 50.7 50.2 63.0 44.7
UBIU 50.6 66.8 57.6 39.4 51.9 44.8 9.5 11.4 10.4 41.2 53.7 46.6 50.2 54.4 48.0
open×gold
BART 94.3 93.7 94.0 67.1 66.7 66.9 70.5 40.1 51.1 85.3 64.4 73.4 65.5 61.0 62.8
open×regular
BART 82.5 82.3 82.4 61.4 61.2 61.3 61.4 36.1 45.5 75.3 58.3 65.7 55.9 60.3 57.3
Italian
closed×gold
SUCRE 98.4 98.4 98.4 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9
closed×regular
SUCRE 84.6 98.1 90.8 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7
UBIU 46.8 35.9 40.6 37.9 29.0 32.9 2.9 4.6 3.6 38.4 31.9 34.8 50.0 46.6 37.2
open×gold
open×regular
BART 42.8 80.7 55.9 35.0 66.1 45.8 35.3 54.0 42.7 34.6 70.6 46.4 57.1 68.1 59.6
TANL-1 90.5 73.8 81.3 62.2 50.7 55.9 37.2 28.3 32.1 66.8 56.5 61.2 50.7 69.3 48.5
Spanish
closed×gold
RelaxCor 100 100 100 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
SUCRE 100 100 100 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5
TANL-1 100 96.8 98.4 66.9 64.7 65.8 16.6 56.5 25.7 65.2 93.4 76.8 52.5 79.0 54.1
UBIU 73.8 96.4 83.6 45.7 59.6 51.7 9.6 18.8 12.7 46.8 77.1 58.3 52.9 63.9 54.3
closed×regular
SUCRE 74.9 66.3 70.3 56.3 49.9 52.9 35.8 36.8 36.3 56.6 54.6 55.6 52.1 61.2 51.4
TANL-1 82.2 84.1 83.1 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4
UBIU 51.1 72.7 60.0 33.6 47.6 39.4 7.6 14.4 10.0 32.8 57.1 41.6 50.4 54.6 48.4
open×gold
open×regular
</table>
<tableCaption confidence="0.998291">
Table 5: Official results of the participating systems for all languages, settings, and metrics.
</tableCaption>
<page confidence="0.999509">
7
</page>
<sectionHeader confidence="0.990083" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999830485436893">
Giuseppe Attardi, Stefano Dei Rossi, and Maria Simi.
2010. TANL-1: coreference resolution by parse
analysis and similarity clustering. In Proceedings
of SemEval-2.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563–566.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Ro-
mano, Olga Uryupina, Yannick Versley, and Roberto
Zanoli. 2010. BART: A multilingual anaphora res-
olution system. In Proceedings of SemEval-2.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL 1999.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program – Tasks, data, and evaluation.
In Proceedings of LREC 2004, pages 837–840.
Katrin Erk and Carlo Strapparava, editors. 2010. Pro-
ceedings of SemEval-2.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the ACL
Workshop on Parsing German (PaGe 2008), pages
47–54.
Erhard W. Hinrichs, Sandra K¨ubler, and Karin Nau-
mann. 2005. A unified representation for morpho-
logical, syntactic, semantic, and referential annota-
tions. In Proceedings of the ACL Workshop on Fron-
tiers in Corpus Annotation II: Pie in the Sky, pages
13–20.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 Coreference Task Definition – Version 3.0.
In Proceedings of MUC-7.
V´eronique Hoste and Guy De Pauw. 2006. KNACK-
2002: A richly annotated corpus of Dutch written
text. In Proceedings of LREC 2006, pages 1432–
1437.
Hamidreza Kobdani and Hinrich Sch¨utze. 2010. SU-
CRE: A modular system for coreference resolution.
In Proceedings of SemEval-2.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25–32.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI 1995, pages 1050–1055.
Thomas S. Morton. 1999. Using coreference in ques-
tion answering. In Proceedings of TREC-8, pages
85–89.
Constantin Orasan, Dan Cristea, Ruslan Mitkov, and
Ant´onio Branco. 2008. Anaphora Resolution Exer-
cise: An overview. In Proceedings of LREC 2008.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
the International Conference on Semantic Comput-
ing (ICSC 2007), pages 517–526.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens and M. Ant`onia Marti. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI:10.1007/s10579-009-9108-x.
Kepa Joseba Rodriguez, Francesca Delogu, Yannick
Versley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of Wikipedia and blogs in
the Live Memories Corpus. In Proceedings of
LREC 2010, pages 157–163.
Emili Sapena, Lluis Padr´o, and Jordi Turmo. 2010.
RelaxCor: A global relaxation labeling approach to
coreference resolution for the SemEval-2 Corefer-
ence Task. In Proceedings of SemEval-2.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained POS tagging. In Pro-
ceedings of COLING 2008, pages 777–784.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT Workshop, pages
47–50.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jov, and Karel Jeek. 2007. Two uses of anaphora
resolution in summarization. Information Process-
ing and Management: an International Journal,
43(6):1663–1680.
Olga Uryupina. 2010. Corry: A system for corefer-
ence resolution. In Proceedings of SemEval-2.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of LREC 2006.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45–52.
Desislava Zhekova and Sandra K¨ubler. 2010. UBIU:
A language-independent system for coreference res-
olution. In Proceedings of SemEval-2.
</reference>
<page confidence="0.99849">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748247">
<title confidence="0.998204">SemEval-2010 Task 1: Coreference Resolution in Multiple Languages</title>
<author confidence="0.994684">Ant`onia</author>
<affiliation confidence="0.957853">CLiC, University of Barcelona, TALP, Technical University of Catalonia, University College Ghent, veronique.hoste@hogent.be University of Essex/University of Trento, poesio@essex.ac.uk University of T¨ubingen,</affiliation>
<abstract confidence="0.9965352">This paper presents the SemEval-2010 on Resolution in Multi- The goal was to evaluate and compare automatic coreference resolution systems for six different languages (Catalan, Dutch, English, German, Italian, and Spanish) in four evaluation settings and using four different metrics. Such a rich scenario had the potential to provide insight into key issues concerning coreference resolution: (i) the portability of systems across languages, (ii) the relevance of different levels of linguistic information, and (iii) the behavior of scoring metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Stefano Dei Rossi</author>
<author>Maria Simi</author>
</authors>
<title>TANL-1: coreference resolution by parse analysis and similarity clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2.</booktitle>
<contexts>
<context position="16962" citStr="Attardi et al., 2010" startWordPosition="2627" endWordPosition="2630">) mention model (English), man), Decision trees man), I-Cab gazetteers (Italian), Closest-first model (German, (Italian) Berkeley parser, Stanford NER, Italian) WordNet, Wikipedia name list, U.S. census data (English) Corry ILP, Pairwise model SVM Stanford parser &amp; NER, Word(Uryupina, 2010) Net, U.S. census data RelaxCor Graph partitioning (solved by Decision trees, Rules WordNet (Sapena et al., 2010) relaxation labeling) SUCRE (Kobdani and Sch¨utze, 2010) Best-first clustering, Rela- Decision trees, Naive — tional database model, Regular Bayes, SVM, MaxEnt feature definition language TANL-1 (Attardi et al., 2010) Highest entity-mention simi- MaxEnt PoS tagger (Italian) larity UBIU (Zhekova and K¨ubler, 2010) Pairwise model MBL — Table 3: Main characteristics of the participating systems. sions about the aims initially pursued by the task. In the following, we summarize the most relevant outcomes of the evaluation. Regarding languages, English concentrates the most participants (fifteen entries), followed by German (eight), Catalan and Spanish (seven each), Italian (five), and Dutch (three). The number of languages addressed by each system ranges from one (Corry) to six (UBIU and SUCRE); BART and Relax</context>
</contexts>
<marker>Attardi, Rossi, Simi, 2010</marker>
<rawString>Giuseppe Attardi, Stefano Dei Rossi, and Maria Simi. 2010. TANL-1: coreference resolution by parse analysis and similarity clustering. In Proceedings of SemEval-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC Workshop on Linguistic Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="13537" citStr="Bagga and Baldwin, 1998" startWordPosition="2081" endWordPosition="2084">stems could be developed using any external tools and resources to predict the preprocessing information, e.g., WordNet, Wikipedia, etc. The only requirement was to use tools that had not been developed with the annotations of the test set. This setting provided an open door into tools or resources that improve performance. 3.3 Evaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1. Mentions are rewarded with 1 point if their boundaries coincide with those of the gold NP, with 0.5 points if their boundaries are within the gold NP including its head, and with 0 otherwise. 4 Participating Systems A total of twenty-two participants registered for the task and downloaded t</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the LREC Workshop on Linguistic Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Broscheit</author>
<author>Massimo Poesio</author>
</authors>
<title>Simone Paolo Ponzetto, Kepa Joseba</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2.</booktitle>
<location>Rodriguez, Lorenza Romano, Olga Uryupina, Yannick</location>
<marker>Broscheit, Poesio, 2010</marker>
<rawString>Samuel Broscheit, Massimo Poesio, Simone Paolo Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano, Olga Uryupina, Yannick Versley, and Roberto Zanoli. 2010. BART: A multilingual anaphora resolution system. In Proceedings of SemEval-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="6853" citStr="Daelemans et al., 1999" startWordPosition="1013" endWordPosition="1016">ady annotated. The datasets that were used in the task were extracted from the above-mentioned corpora. Table 1 summarizes the number of documents (docs), sentences (sents), and tokens in the training, development and test sets.3 2.2 Preprocessing Systems Catalan, Spanish, English Predicted lemmas and PoS were generated using FreeLing4 for Catalan/Spanish and SVMTagger5 for English. Dependency information and predicate semantic roles were generated with JointParser, a syntacticsemantic parser.6 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3The German and Dutch training datasets were not completely stable during the competition period due to a few errors. Revised versions were released on March 2 and 20, respectively. As to the test datasets, the Dutch and Italian documents with formatt</context>
</contexts>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>Walter Daelemans, Sabine Buchholz, and Jorn Veenstra. 1999. Memory-based shallow parsing. In Proceedings of CoNLL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) program – Tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>837--840</pages>
<contexts>
<context position="1975" citStr="Doddington et al., 2004" startWordPosition="274" endWordPosition="277">sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: • To what extent is it possible to implement a general coreference resolution system portable to different languages? How much language-specific tuning is necessary? • How helpful are morphology, syntax and semantics for solving coreference relations? How much preprocessing is needed? Does its quality (perfect linguistic input versus noisy automatic input) really matter? • How (dis)similar are different coreference evaluation metrics—MUC, B-CUBED, CEAF and BLANC? Do they all provide the same ranking? Are they correlated? Our goal</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) program – Tasks, data, and evaluation. In Proceedings of LREC 2004, pages 837–840.</rawString>
</citation>
<citation valid="true">
<date>2010</date>
<booktitle>Proceedings of SemEval-2.</booktitle>
<editor>Katrin Erk and Carlo Strapparava, editors.</editor>
<contexts>
<context position="14994" citStr="(2010)" startWordPosition="2323" endWordPosition="2323">ut that the final participation rate was comparatively low (slightly below 30%). The participating systems differed in terms of architecture, machine learning method, etc. Table 3 summarizes their main properties. Systems like BART and Corry support several machine learners, but Table 3 indicates the one used for the SemEval run. The last column indicates the external resources that were employed in the open setting, thus it is empty for systems that participated only in the closed setting. For more specific details we address the reader to the system description papers in Erk and Strapparava (2010). 5 Results and Evaluation Table 4 shows the results obtained by two naive baseline systems: (i) SINGLETONS considers each mention as a separate entity, and (ii) ALL-IN-ONE groups all the mentions in a document into a single entity. These simple baselines reveal limitations of the evaluation metrics, like the high scores of CEAF and B3 for SINGLETONS. Interestingly enough, the naive baseline scores turn out to be hard to beat by the participating systems, as Table 5 shows. Similarly, ALL-IN-ONE obtains high scores in terms of MUC. Table 4 also reveals differences between the distribution of en</context>
</contexts>
<marker>2010</marker>
<rawString>Katrin Erk and Carlo Strapparava, editors. 2010. Proceedings of SemEval-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>A dependencydriven parser for German dependency and constituency representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL Workshop on Parsing German</booktitle>
<pages>47--54</pages>
<location>PaGe</location>
<contexts>
<context position="7107" citStr="Hall and Nivre, 2008" startWordPosition="1054" endWordPosition="1057">Catalan, Spanish, English Predicted lemmas and PoS were generated using FreeLing4 for Catalan/Spanish and SVMTagger5 for English. Dependency information and predicate semantic roles were generated with JointParser, a syntacticsemantic parser.6 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3The German and Dutch training datasets were not completely stable during the competition period due to a few errors. Revised versions were released on March 2 and 20, respectively. As to the test datasets, the Dutch and Italian documents with formatting errors were corrected after the evaluation period, with no variations in the ranking order of systems. 4http://www.lsi.upc.es/ nlp/freeling 5http://www.lsi.upc.edu/ nlp/SVMTool 6http://www.lsi.upc.edu// xlluis/?x=cat:5 7http://textpro.fbk.eu 8http://</context>
</contexts>
<marker>Hall, Nivre, 2008</marker>
<rawString>Johan Hall and Joakim Nivre. 2008. A dependencydriven parser for German dependency and constituency representations. In Proceedings of the ACL Workshop on Parsing German (PaGe 2008), pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard W Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Karin Naumann</author>
</authors>
<title>A unified representation for morphological, syntactic, semantic, and referential annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky,</booktitle>
<pages>13--20</pages>
<marker>Hinrichs, K¨ubler, Naumann, 2005</marker>
<rawString>Erhard W. Hinrichs, Sandra K¨ubler, and Karin Naumann. 2005. A unified representation for morphological, syntactic, semantic, and referential annotations. In Proceedings of the ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 13–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<date>1997</date>
<booktitle>MUC-7 Coreference Task Definition – Version 3.0. In Proceedings of MUC-7.</booktitle>
<contexts>
<context position="1944" citStr="Hirschman and Chinchor, 1997" startWordPosition="269" endWordPosition="272">ommunity. (1) Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: • To what extent is it possible to implement a general coreference resolution system portable to different languages? How much language-specific tuning is necessary? • How helpful are morphology, syntax and semantics for solving coreference relations? How much preprocessing is needed? Does its quality (perfect linguistic input versus noisy automatic input) really matter? • How (dis)similar are different coreference evaluation metrics—MUC, B-CUBED, CEAF and BLANC? Do they all provide the same ranking</context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor. 1997. MUC-7 Coreference Task Definition – Version 3.0. In Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´eronique Hoste</author>
<author>Guy De Pauw</author>
</authors>
<title>KNACK2002: A richly annotated corpus of Dutch written text.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>1432--1437</pages>
<marker>Hoste, De Pauw, 2006</marker>
<rawString>V´eronique Hoste and Guy De Pauw. 2006. KNACK2002: A richly annotated corpus of Dutch written text. In Proceedings of LREC 2006, pages 1432– 1437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamidreza Kobdani</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>SUCRE: A modular system for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2.</booktitle>
<marker>Kobdani, Sch¨utze, 2010</marker>
<rawString>Hamidreza Kobdani and Hinrich Sch¨utze. 2010. SUCRE: A modular system for coreference resolution. In Proceedings of SemEval-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP</booktitle>
<pages>25--32</pages>
<contexts>
<context position="13561" citStr="Luo, 2005" startWordPosition="2087" endWordPosition="2088">ernal tools and resources to predict the preprocessing information, e.g., WordNet, Wikipedia, etc. The only requirement was to use tools that had not been developed with the annotations of the test set. This setting provided an open door into tools or resources that improve performance. 3.3 Evaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1. Mentions are rewarded with 1 point if their boundaries coincide with those of the gold NP, with 0.5 points if their boundaries are within the gold NP including its head, and with 0 otherwise. 4 Participating Systems A total of twenty-two participants registered for the task and downloaded the training materials. F</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLTEMNLP 2005, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph F McCarthy</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1712" citStr="McCarthy and Lehnert, 1995" startWordPosition="237" endWordPosition="240">r of scoring metrics. 1 Introduction The task of coreference resolution, defined as the identification of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community. (1) Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: • To what extent is it possible to implement a general coreference resolution system portable to different languages? How much language-specific tuning is necessary? • How helpful are morphology, syntax and semantics for solving coreference relations? How much preprocessi</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of IJCAI 1995, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Morton</author>
</authors>
<title>Using coreference in question answering.</title>
<date>1999</date>
<booktitle>In Proceedings of TREC-8,</booktitle>
<pages>85--89</pages>
<contexts>
<context position="1794" citStr="Morton, 1999" startWordPosition="249" endWordPosition="250">cation of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community. (1) Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: • To what extent is it possible to implement a general coreference resolution system portable to different languages? How much language-specific tuning is necessary? • How helpful are morphology, syntax and semantics for solving coreference relations? How much preprocessing is needed? Does its quality (perfect linguistic input versus noisy automatic in</context>
</contexts>
<marker>Morton, 1999</marker>
<rawString>Thomas S. Morton. 1999. Using coreference in question answering. In Proceedings of TREC-8, pages 85–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin Orasan</author>
<author>Dan Cristea</author>
<author>Ruslan Mitkov</author>
<author>Ant´onio Branco</author>
</authors>
<title>Anaphora Resolution Exercise: An overview.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="2006" citStr="Orasan et al., 2008" startWordPosition="280" endWordPosition="283">o to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: • To what extent is it possible to implement a general coreference resolution system portable to different languages? How much language-specific tuning is necessary? • How helpful are morphology, syntax and semantics for solving coreference relations? How much preprocessing is needed? Does its quality (perfect linguistic input versus noisy automatic input) really matter? • How (dis)similar are different coreference evaluation metrics—MUC, B-CUBED, CEAF and BLANC? Do they all provide the same ranking? Are they correlated? Our goal was to address these questions</context>
</contexts>
<marker>Orasan, Cristea, Mitkov, Branco, 2008</marker>
<rawString>Constantin Orasan, Dan Cristea, Ruslan Mitkov, and Ant´onio Branco. 2008. Anaphora Resolution Exercise: An overview. In Proceedings of LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Eduard Hovy</author>
<author>Mitch Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: A unified relational semantic representation.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Semantic Computing (ICSC</booktitle>
<pages>517--526</pages>
<contexts>
<context position="4991" citStr="Pradhan et al., 2007" startWordPosition="735" endWordPosition="738">alan and a Spanish treebank of 500k words each, mainly from newspapers and news agencies (El Peri´odico, EFE, ACN). Manual annotation exists for arguments and thematic roles, predicate semantic classes, NEs, WordNet nominal senses, and coreference relations. AnCora are freely available for research purposes. Dutch The KNACK-2002 corpus (Hoste and De Pauw, 2006) contains 267 documents from the Flemish weekly magazine Knack. They were manually annotated with coreference information on top of semi-automatically annotated PoS tags, phrase chunks, and NEs. English The OntoNotes Release 2.0 corpus (Pradhan et al., 2007) covers newswire and broadcast news data: 300k words from The Wall Street Journal, and 200k words from the TDT-4 collection, respectively. OntoNotes builds on the Penn Treebank for syntactic annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently compr</context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2007. Ontonotes: A unified relational semantic representation. In Proceedings of the International Conference on Semantic Computing (ICSC 2007), pages 517–526.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marta</author>
</authors>
<title>Recasens and Eduard Hovy. in prep. BLANC: Implementing the Rand Index for Coreference Evaluation.</title>
<marker>Marta, </marker>
<rawString>Marta Recasens and Eduard Hovy. in prep. BLANC: Implementing the Rand Index for Coreference Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>M Ant`onia Marti</author>
</authors>
<title>AnCoraCO: Coreferentially annotated corpora for Spanish and Catalan. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>10--1007</pages>
<marker>Recasens, Marti, 2009</marker>
<rawString>Marta Recasens and M. Ant`onia Marti. 2009. AnCoraCO: Coreferentially annotated corpora for Spanish and Catalan. Language Resources and Evaluation, DOI:10.1007/s10579-009-9108-x.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kepa Joseba Rodriguez</author>
<author>Francesca Delogu</author>
<author>Yannick Versley</author>
<author>Egon Stemle</author>
<author>Massimo Poesio</author>
</authors>
<title>Anaphoric annotation of Wikipedia and blogs in the Live Memories Corpus.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>157--163</pages>
<marker>Rodriguez, Delogu, Versley, Stemle, Poesio, 2010</marker>
<rawString>Kepa Joseba Rodriguez, Francesca Delogu, Yannick Versley, Egon Stemle, and Massimo Poesio. 2010. Anaphoric annotation of Wikipedia and blogs in the Live Memories Corpus. In Proceedings of LREC 2010, pages 157–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emili Sapena</author>
<author>Lluis Padr´o</author>
<author>Jordi Turmo</author>
</authors>
<title>RelaxCor: A global relaxation labeling approach to coreference resolution for the SemEval-2 Coreference Task.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2.</booktitle>
<marker>Sapena, Padr´o, Turmo, 2010</marker>
<rawString>Emili Sapena, Lluis Padr´o, and Jordi Turmo. 2010. RelaxCor: A global relaxation labeling approach to coreference resolution for the SemEval-2 Coreference Task. In Proceedings of SemEval-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>777--784</pages>
<contexts>
<context position="7042" citStr="Schmid and Laws, 2008" startWordPosition="1044" endWordPosition="1047">e training, development and test sets.3 2.2 Preprocessing Systems Catalan, Spanish, English Predicted lemmas and PoS were generated using FreeLing4 for Catalan/Spanish and SVMTagger5 for English. Dependency information and predicate semantic roles were generated with JointParser, a syntacticsemantic parser.6 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3The German and Dutch training datasets were not completely stable during the competition period due to a few errors. Revised versions were released on March 2 and 20, respectively. As to the test datasets, the Dutch and Italian documents with formatting errors were corrected after the evaluation period, with no variations in the ranking order of systems. 4http://www.lsi.upc.es/ nlp/freeling 5http://www.lsi.upc.edu/ nlp/SVMTool 6http://</context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging. In Proceedings of COLING 2008, pages 777–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in part-ofspeech tagging with an application to German.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL SIGDAT Workshop,</booktitle>
<pages>47--50</pages>
<contexts>
<context position="6986" citStr="Schmid, 1995" startWordPosition="1036" endWordPosition="1037">nts (docs), sentences (sents), and tokens in the training, development and test sets.3 2.2 Preprocessing Systems Catalan, Spanish, English Predicted lemmas and PoS were generated using FreeLing4 for Catalan/Spanish and SVMTagger5 for English. Dependency information and predicate semantic roles were generated with JointParser, a syntacticsemantic parser.6 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3The German and Dutch training datasets were not completely stable during the competition period due to a few errors. Revised versions were released on March 2 and 20, respectively. As to the test datasets, the Dutch and Italian documents with formatting errors were corrected after the evaluation period, with no variations in the ranking order of systems. 4http://www.lsi.upc.es/ nl</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements in part-ofspeech tagging with an application to German. In Proceedings of the ACL SIGDAT Workshop, pages 47–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Massimo Poesio</author>
<author>Mijail A Kabadjov</author>
<author>Karel Jeek</author>
</authors>
<title>Two uses of anaphora resolution in summarization.</title>
<date>2007</date>
<booktitle>Information Processing and Management: an International Journal,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="1759" citStr="Steinberger et al., 2007" startWordPosition="243" endWordPosition="246">coreference resolution, defined as the identification of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community. (1) Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: • To what extent is it possible to implement a general coreference resolution system portable to different languages? How much language-specific tuning is necessary? • How helpful are morphology, syntax and semantics for solving coreference relations? How much preprocessing is needed? Does its quality (perfect linguis</context>
</contexts>
<marker>Steinberger, Poesio, Kabadjov, Jeek, 2007</marker>
<rawString>Josef Steinberger, Massimo Poesio, Mijail A. Kabadjov, and Karel Jeek. 2007. Two uses of anaphora resolution in summarization. Information Processing and Management: an International Journal, 43(6):1663–1680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Corry: A system for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2.</booktitle>
<contexts>
<context position="16632" citStr="Uryupina, 2010" startWordPosition="2582" endWordPosition="2584">ation of systems across languages and settings is rather irregular,11 thus making it difficult to draw firm conclu11Only 45 entries in Table 5 from 192 potential cases. 4 System Architecture ML Methods External Resources BART Closest-first with entity- MaxEnt (English, Ger- GermaNet &amp; gazetteers (Ger(Broscheit et al., 2010) mention model (English), man), Decision trees man), I-Cab gazetteers (Italian), Closest-first model (German, (Italian) Berkeley parser, Stanford NER, Italian) WordNet, Wikipedia name list, U.S. census data (English) Corry ILP, Pairwise model SVM Stanford parser &amp; NER, Word(Uryupina, 2010) Net, U.S. census data RelaxCor Graph partitioning (solved by Decision trees, Rules WordNet (Sapena et al., 2010) relaxation labeling) SUCRE (Kobdani and Sch¨utze, 2010) Best-first clustering, Rela- Decision trees, Naive — tional database model, Regular Bayes, SVM, MaxEnt feature definition language TANL-1 (Attardi et al., 2010) Highest entity-mention simi- MaxEnt PoS tagger (Italian) larity UBIU (Zhekova and K¨ubler, 2010) Pairwise model MBL — Table 3: Main characteristics of the participating systems. sions about the aims initially pursued by the task. In the following, we summarize the most</context>
</contexts>
<marker>Uryupina, 2010</marker>
<rawString>Olga Uryupina. 2010. Corry: A system for coreference resolution. In Proceedings of SemEval-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
<author>Ineke Schuurman</author>
<author>Vincent Vandeghinste</author>
</authors>
<title>Syntactic annotation of large corpora in STEVIN.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>van Noord, Schuurman, Vandeghinste, 2006</marker>
<rawString>Gertjan van Noord, Ineke Schuurman, and Vincent Vandeghinste. 2006. Syntactic annotation of large corpora in STEVIN. In Proceedings of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="13619" citStr="Vilain et al., 1995" startWordPosition="2095" endWordPosition="2098">essing information, e.g., WordNet, Wikipedia, etc. The only requirement was to use tools that had not been developed with the annotations of the test set. This setting provided an open door into tools or resources that improve performance. 3.3 Evaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1. Mentions are rewarded with 1 point if their boundaries coincide with those of the gold NP, with 0.5 points if their boundaries are within the gold NP including its head, and with 0 otherwise. 4 Participating Systems A total of twenty-two participants registered for the task and downloaded the training materials. From these, sixteen downloaded the test set but only six (o</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of MUC-6, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desislava Zhekova</author>
<author>Sandra K¨ubler</author>
</authors>
<title>UBIU: A language-independent system for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2.</booktitle>
<marker>Zhekova, K¨ubler, 2010</marker>
<rawString>Desislava Zhekova and Sandra K¨ubler. 2010. UBIU: A language-independent system for coreference resolution. In Proceedings of SemEval-2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>