<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.976482">
Parenthetical Constructions - an Argument against Modularity
</title>
<author confidence="0.980074">
Eva Banik
</author>
<affiliation confidence="0.955208">
The Open University
</affiliation>
<address confidence="0.674221">
Milton Keynes, UK
</address>
<email confidence="0.995959">
e.banik@open.ac.uk
</email>
<sectionHeader confidence="0.993827" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834857142857">
This paper presents an argument against
modularizing linguistic information in nat-
ural language generation systems. We ar-
gue that complex linguistic constructions
require grammatical information to be lo-
cated in the same module, in order to
avoid over-complicating the system archi-
tecture. We demonstrate this point by
showing how parenthetical constructions
— which have only been generated in pre-
vious systems using an aggregation or re-
vision module — can be generated by a
surface realizer when using an integrated
grammar.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993920754717">
The ultimate aim of research on natural language
generation is to develop large-scale, domain inde-
pendent NLG systems, which are able to generate
high quality, fluent and well-formatted texts. Ide-
ally the produced texts will be as long as needed
to convey the information given in the input and
should be presented in a style that is appropriate
for the purposes of the user. Current NLG systems
typically produce paragraph-length text tailored to
a specific domain and the grammars in these sys-
tems contain only a limited number of grammati-
cal constructions, typically collected during a cor-
pus study of example documents. Often the gram-
mar is implemented using schemas or “canned”
expressions, and individual grammatical levels are
distributed in independent modules.
Organizing the grammar this way severely lim-
its the flexibility of NLG systems. It has long been
recognized in the literature that text fluency can be
improved by modeling interactions between gram-
mar modules. The most commonly mentioned
interactions are those among discourse/rhetorical
relations and syntax (Scott and Souza, 1990;
Hovy, 1993; Callaway, 2003), rhetorical rela-
tions, syntax and referring expressions (Kibble
and Power, 2004); and layout and referring ex-
pressions (N. Bouayad-Agha, 2001). It is clear
that in order to generate high quality, coherent
discourse, a generator needs access to a gram-
mar which is able to model the interdependent,
context-sensitive behaviour of these separate lin-
guistic phenomena.
In this paper we draw a parallel between gram-
mar design and the design of natural language gen-
eration systems. We argue that in order to gener-
ate complex linguistic constructions, current NLG
systems tend to have overly complicated architec-
tures. To illustrate this point we show how a sur-
face realizer can take on tasks from other com-
ponents when linguistic information from differ-
ent grammar modules (and hence, system mod-
ules) is integrated. This simplifies system archi-
tecture by reducing the need for interaction be-
tween modules and enables the generator to pro-
duce more complex and coherent text. We illus-
trate this point by first showing constraints that
parenthetical constructions impose on pronomi-
nalization. Then we present a grammar which in-
tegrates a representation for referring expressions
into a syntax/discourse grammar. Finally we show
that using this grammar, we can generate complex,
coherent paragraphs which contain parenthetical
constructions using only a surface realizer.
</bodyText>
<sectionHeader confidence="0.866629" genericHeader="introduction">
2 The problem of generating
parenthetical constructions
</sectionHeader>
<bodyText confidence="0.999711375">
Parentheticals are constructions that provide less
important or background information in texts and
they are a prime example of interactions between
referring expressions, syntax, layout and discourse
structure. Parentheticals help readers distinguish
between more and less important propositions and
therefore significantly increase the fluency and
readability of the generated text. Despite this ma-
</bodyText>
<page confidence="0.994287">
46
</page>
<note confidence="0.998988">
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 46–53,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9975265">
jor effect on the quality of the generated text, cur-
rent natural language generation systems still do
not have a principled way of producing paren-
theticals. In this paper we focus on parenthetical
constructions which take the form of a subordi-
nate clause introduced by a discourse connective.
Some examples of this type of parentheticals in the
Wall Street Journal are illustrated (1):
</bodyText>
<listItem confidence="0.844046">
(1) a The irony is that the attack commercial,
</listItem>
<bodyText confidence="0.998878818181818">
after getting a boost in last year’s
presidential campaign, has come of age
in an off-off election year with only a
few contests scattered across the
country.
b the 1989 fall total of 80, while well
below 1988 activity, shows a steady
ratcheting up in citizen referenda and
initiatives
c pollination, while easy in corn because
the carrier is wind, is more complex
and involves insects as carriers in crops
such as cotton
The examples in (2) illustrate the difficulties in
generating parenthetical constructions by show-
ing some possible but incoherent realizations of
the same message. In particular, they illustrate
the importance of appropriate punctuation marks
(2a), syntactic requirements of discourse connec-
tives (2b), the limit on embedding (2c), and the
importance of ordering syntactic arguments (the-
matic structure/information structure) (2d).
</bodyText>
<listItem confidence="0.905486235294118">
(2) a # The FDA though it bans Elixir since it
contains Gestodene approves Elixir
Plus.
b # The FDA – but it bans Elixir since it
contains Gestodene – approves Elixir
Plus.
c # The FDA though since Elixir contains
Gestodene , it bans Elixir approves
Elixir Plus.
d # The FDA, since Gestodene is an
ingredient of Elixir, bans Elixir. But it
approves Elixir Plus.
Correct realizations of the same message would
include:
(3) a The FDA — though it bans Elixir since
it contains Gestodene — approves
Elixir Plus .
</listItem>
<bodyText confidence="0.989193156862745">
b The FDA bans Elixir because it
contains Gestodene. However, Elixir
Plus is approved by the FDA
c The FDA approves Elixir Plus although
Elixir — since it contains Gestodene —
is banned by the FDA.
Generation systems that produce output similar
to the examples in (3) have three kinds of strate-
gies: either a text planning module chooses a dis-
course connective and decides the position and
ordering of clauses (Hovy, 1993) or aggregation
is considered to be one of the tasks of the sen-
tence planning module (Shaw, 2002); or a revi-
sion module performs aggregation opportunisti-
cally (Robin, 1994; Callaway and Lester, 1997).
However, none of these systems handle paren-
thetical constructions in a principled way. Sys-
tems where aggregation is part of the text planning
module only produce complex sentences made
up of clauses joined by discourse connectives –
sentence-medial subordinate clauses are not gen-
erated at all. In revision-based systems, the output
often needs to be corrected after aggregation. For
example, Robin’s system includes various trans-
formations to correct redundancies, ambiguities or
invalid lexical collocations introduced by the revi-
sion module. In Shaw’s system, the referring ex-
pression generation module is run twice, once be-
fore and once after aggregation. In general, the
ordering of aggregation rules and the interactions
between them pose further problems where ag-
gregation is separated into an independent mod-
ule.We propose a different approach to modeling
interactions between linguistic information in sep-
arate grammar modules. We argue that constraints
that are at the interface of modules (syntactic con-
straints on referring expressions, discourse-level
constraints on syntax, constraints imposed by lay-
out on discourse, etc.) should be stored in an in-
tegrated grammar, and only straightforward deci-
sions — which do not require information from a
separate grammatical level — should be separated
out into individual modules.
As an example, we show a grammar which is
capable of generating parenthetical constructions
in a principled way. The grammar includes
- a representation for discourse connectives
and discourse-level constraints they impose
on syntax;
- referring expressions and syntactic con-
straints on them;
</bodyText>
<page confidence="0.998792">
47
</page>
<bodyText confidence="0.982842">
- elements of layout (punctuation marks for
main clauses and parentheticals).
We show that by incorporating the above kinds
of linguistic information into the grammar of a
surface realizer we can improve the flexibility of
the system (i.e., generate more paraphrases for the
same input) and improve the quality of the gener-
ated text without adding more modules to the sys-
tem.
</bodyText>
<subsectionHeader confidence="0.8931975">
2.1 Syntactic constraints on
pronominalization
</subsectionHeader>
<bodyText confidence="0.9997698">
To design a grammar for parenthetical construc-
tions, we have carried out a corpus study on em-
bedded rhetorical relations in the RST treebank
(Banik and Lee, 2008). The corpus study has
shown that the most numerous class of embed-
ded subordinate clauses that occur in sentence-
medial position contain a subject pronoun (as in
4a). This embedded subject pronoun in all cases
referred back to the subject of the matrix clause,
which always immediately preceded the subordi-
nate clause. The pronoun can be either explicit (as
in 4a) or implicit (as in the examples in 1). Of the
119 sentence-medial subordinate clauses that we
looked at in the study, 35 were of this type (what
we call pseudo-relatives).1 This suggests that in
sentence-medial subordinate clauses (or sentence-
final ones immediately following the main clause
object) the type of a referring expression is solely
determined by syntax, much like a WH-pronoun
in relative clauses.
</bodyText>
<listItem confidence="0.499236333333333">
(4) a Elixir, since it contains Gestodene, is
banned by the FDA.
b # Elixir, since Elixir contains
Gestodene, is banned by the FDA.
c # It, since Elixir contains Gestodene, is
banned by the FDA.
</listItem>
<bodyText confidence="0.994416851851852">
d # The FDA, since it contains
Gestodene, banned Elixir.
The constraints on the form of referring expres-
sions selected for the matrix clause and subordi-
nate clause subjects in these cases can be stated as
follows:
1Of the rest, 30 were ‘free’ subordinate clauses (subor-
dinate clauses that are equally felicitous in sentence-initial
or sentence final positions, typically they do not contain any
pronouns). The rest of the cases were either time adverbials
(20) or scopal elements (22).
- the subject of the subordinate clause has to be
realized as a pronoun. (c.f. 4b)
- the subject of the main clause cannot be a
pronoun (c.f. 4c)
- the subject pronoun in the subordinate clause
will be resolved as referring to an entity men-
tioned in the matrix clause; this entity has to
precede the subordinate clause (c.f. 4d )
In addition to modeling the above constraints, in
order to generate parentheticals a generation sys-
tem also has to
- insert the appropriate discourse connective
for the subordinate clause (c.f.2b) and
- insert appropriate punctuation marks on ei-
ther side of the subordinate clause to avoid
potential garden path effects.
</bodyText>
<sectionHeader confidence="0.98522" genericHeader="method">
3 An integrated discourse-syntax
grammar
</sectionHeader>
<bodyText confidence="0.9999878">
In order to generate coherent discourse, a gener-
ation system needs access to a grammar that is
capable of representing multisentential text. In
modular systems this is typically achieved by two
modules: a text planning module which constructs
a text plan and a surface realizer that converts
the text plan into sentences. However, text plan-
ning and linguistic realization are not two inde-
pendent processes and many linguistic decisions
are in fact made by the text planner. The inter-
actions between text planning and linguistic re-
alization in modular systems have been handled
in several ways, including backtracking (Appelt,
1985), interleaving the two components (McDon-
ald, 1983) and restrictive planning (Hovy, 1988).
These approaches however make the system in-
flexible because all possible interactions between
modules have to be anticipated by the system de-
signer.
Another, more recent approach to tackle this
problem is to use lexicalization not only for sen-
tences but also for texts. The theoretical back-
ground for lexicalization on the discourse level
has been laid down for Tree Adjoining Grammar
(Joshi and Schabes, 1997) by several researchers,
including Webber (2004), and Danlos (2000). In
particular, Danlos (2000) shows that extending
lexicalization to the discourse level makes it possi-
ble to completely integrate text planning and sur-
face realization.
</bodyText>
<page confidence="0.995458">
48
</page>
<bodyText confidence="0.999203411764706">
We have designed a Tree Adjoining Grammar
for parenthetical constructions following this lat-
ter approach. Elementary trees in the grammar
are associated with a flat semantic representation.
The trees integrate syntax and discourse represen-
tations in the sense that each sentence-level ele-
mentary tree includes one or more discourse-level
nodes. The elementary trees in Fig. 1 illustrate
what we mean by this: every lexical item that
would normally project a sentence in a syntactic
grammar (i.e., an S-rooted tree) here projects a dis-
course clause (i.e., a Dc rooted tree). Every pred-
icate that projects a discourse clause is assigned
two kinds of elementary trees: a discourse ini-
tial tree (e.g., Fig. 1a) and a discourse continu-
ing tree (e.g., Fig. 1b), which takes the preceding
discourse clause as an argument.
</bodyText>
<figureCaption confidence="0.999497">
Figure 1: Elementary syntax/discourse trees
</figureCaption>
<bodyText confidence="0.9997726">
The combination of these two trees corresponds
to the empty connective (⊕ in Danlos (2000)).
Other types of discourse connectives are imple-
mented in the grammar the usual way (see e.g.
Danlos (2000)).
</bodyText>
<sectionHeader confidence="0.97437" genericHeader="method">
4 Referring expressions
</sectionHeader>
<bodyText confidence="0.999972833333333">
One of the challenges of generating paraphrases
from a semantic representation is that in some ver-
sions there will be a mismatch between the num-
ber of noun phrases needed to make the output
syntactically well-formed and the number of se-
mantic arguments in the input which can poten-
tially become a noun phrase.
This happens whenever a discourse entity is the
argument of more than one semantic predicate.
For example, (5) shows possible realizations of the
following input where (5a) contains three syntac-
tic slots for “Elixir”, (5b,c) contain two slots, and
</bodyText>
<equation confidence="0.892155888888889">
(5d) only one:
h0:white-cream(e)
h1:contains(e,g)
h2:elixir(e)
h3:gestodene(g)
h4:ban(f,e)
h5:fda(f)
(5) a Elixir is a white cream. Elixir contains
gestodene. Elixir is banned by the FDA.
</equation>
<bodyText confidence="0.998758073170732">
b This white cream, Elixir, contains
gestodene. It is banned by the FDA.
c Elixir is a white cream, which contains
gestodene. It is banned by the FDA.
d Elixir, a white cream banned by the
FDA, contains gestodene.
The task of a generation system is to decide
what predicate-argument structure to choose and
to decide how the individual noun phrases should
be represented. In most systems creating the syn-
tactic “slots” is the task of a text planning or sen-
tence planning module, and filling them in with the
right noun phrases is the task of a referring expres-
sion generation module, i.e., the referring expres-
sion module decides whether an NP slot should be
realized as a name, a pronoun or a description.
This division of labour makes it difficult to rep-
resent syntactic constraints on pronominalization
exhibited by the examples in the previous section,
where pronouns are either prohibited or obligatory
in specific syntactic contexts.
To model these constraints we include a repre-
sentation for underspecified referring expressions
in the grammar by replacing NP substitution nodes
with a referring expression leaf node as illustrated
in Fig.2. This allows syntactic constraints to be
‘posted’ on referring expressions in the appropri-
ate contexts while completely specifying the form
of the underspecified slots still remains the task of
a referring expression module. In other words, we
factor out pronominalization decisions dictated by
syntax from pronominalization decisions dictated
by discourse level constraints.
Treating pronouns in subordinate clauses dif-
ferently from pronouns in main clauses has inde-
pendent justification from psycholinguistics and
theoretical linguistics. For example, Miltsakaki
(2003) has carried out psycholinguistic experi-
ments on complex sentences containing relative
clauses. The experiments show that pronouns in
embedded clauses tend to refer back to an entity
</bodyText>
<figure confidence="0.9887602">
h1:white-cream(e)
D,
��� � � �
S
/\
NPS.
[idx:e]
is cream
[idx:e]
VP
�� ��
V NP
Punct
.
(a) discourse initial
h2:contain(e,a)
D,
�� � �
D, S. D,
�� � �
S Punct
�� � �
NPS.
[idx:e] V
contains
(b) discourse continuing
VP .
�� � �
NPS.
[idx:a]
</figure>
<page confidence="0.882643">
49
</page>
<figureCaption confidence="0.999315">
Figure 2: Elementary trees with referring expressions
</figureCaption>
<bodyText confidence="0.999648058823529">
in the matrix clause, whereas referring expressions
in main clauses tend to find their antecedent in the
previous main clause. This suggests that pronom-
inalization should be treated differently in subor-
dinate clauses than in main clauses. Research in
theoretical linguistics underlines this claim, where
Kehler (2002) has shown that apparent discrepan-
cies between different accounts of pronominaliza-
tion can be reconciled if each method is applied in
a different discourse context.
To sum up, in this integrated approach part of
the job of the referring expression generation mod-
ule is taken over by the grammar, namely
- pronominalization of discourse entities in
subordinate clauses and
- decisions about when not to realize under-
specified referring expressions as pronouns.
</bodyText>
<sectionHeader confidence="0.9899835" genericHeader="method">
5 Representing parenthetical
constructions
</sectionHeader>
<bodyText confidence="0.999926333333333">
Integrating referring expressions into the grammar
this way makes it possible to state syntactic con-
straints on pronominalization.
</bodyText>
<subsectionHeader confidence="0.977903">
5.1 Pronoun prohibited
</subsectionHeader>
<bodyText confidence="0.963904235294118">
(6) a Elixir, an illegal drug, is banned by the
FDA.
b # It, an illegal drug, is banned by the
FDA.
The constraint that parenthetical constructions
such as appositives, relative clauses or parenthet-
ical subordinate clauses cannot follow a pronoun
is illustrated by the contrast in (6). Using the el-
ementary trees described in the previous section
this constraint can now be stated by adding a fea-
ture ([pron:no]) to the foot node of auxiliary
trees, as illustrated in Fig. 3. When the aux-
iliary tree is adjoined onto an NP, the feature is
percolated to the underspecified referring expres-
sion node, which will block the referring expres-
sion module from realizing this noun phrase as a
pronoun.
</bodyText>
<subsectionHeader confidence="0.942763">
5.2 Pronoun obligatory
</subsectionHeader>
<bodyText confidence="0.8873262">
(7) a Elixir, since it contains Gestodene, is
banned by the FDA.
b # Elixir, since Elixir contains
Gestodene, is banned by the FDA.
Another case where syntax imposes constraints
on pronominalization is contexts where pronouns
are not allowed, as illustrated by the example in
(7). The discourse connective ‘since‘ is assigned
an NP auxiliary tree in this context, which takes
the embedded clause as an argument. The features
on the auxiliary tree state that the subject of this
embedded clause should be expressed by a pro-
noun and that it should refer to the same discourse
entity as the head noun that the auxiliary tree ad-
joins to. When the discourse connective is com-
bined with the embedded clause, these features are
percolated to the referring expression in subject
position, requiring it to be realized by a pronoun.
Figure 4 illustrates the elementary trees and the
derived tree for the embedded clause in (7).
</bodyText>
<sectionHeader confidence="0.997831" genericHeader="method">
6 Comparison
</sectionHeader>
<bodyText confidence="0.999792166666667">
As an experiment, we have implemented a gram-
mar fragment in the GenI surface realizer (Kow,
2007) and regenerated an example from the
ICONOCLAST generator (Power et al., 2003).
The example we used is represented by the fol-
lowing input semantics:
</bodyText>
<footnote confidence="0.76744225">
h1: elixir(e)
h2: fda(f)
h3: elixir plus(p)
h4: gestodene(g)
h5: contain(e g)
h6: ban(f e)
h7: approve(f p)
h8: concession(h6 h7)
h9: cause(h5 h6)
h10: contain(p o)
h11: oestradiol(o)
h12: cause(h10 h7)
</footnote>
<figure confidence="0.993875071428571">
h2:contain(e,a)
Dc
I_✟ ❍❍
Dc 1 Dc
✟✟ ❍ ❍
S Punct
✟✟ ❍ ❍
RX
[idx:e] V
banned by
VP .
✟✟ ❍ ❍
RX
[idx:a]
</figure>
<page confidence="0.773914">
50
</page>
<figureCaption confidence="0.999993">
Figure 3: Pronouns not allowed before an appositive
Figure 4: Obligatory pronouns in parenthetical subordinate clauses
</figureCaption>
<figure confidence="0.9938129453125">
drug
(b) Derived tree for (6)
(a) Elementary trees for (6)
S
Punct
banned by
RX
[idx:f]
,
TextPhrase
// � �
,NP
// ��
DET N
h6:ban(f e)
D,e
/// � � �
Punct
.
NP∗
[pron:no]
,
NP
VP
/// � � �
V
h2:drug(e)
NP
/// � � �
TextPhrase
// � �
,NP
// ��
DET N
S
/// � � �
NP
RX
[idx:e]
VP
// �
�
V
RX
[idx:f]
banned by
NP
RX
~ ~
idx:e
pron:no
NP
//// �� � �
drug
D,e
/// � � �
�
////// �� � � �
.
(b) Derived tree for (7)
NP
V^P
S↓
Conn
V
NP
contain
RX
[idx:f]
h2:contain(e a)
S[s pron:?X]
�
///// �� � �
RX
� �
pron:?X
idx:e
h2:cause(h7 h0)
NP
//// � � � �
NP∗
(a) Elementary trees for (7)

 


pred:h7
pron:no
idx :e


TextClause
/// � � �
idx :h0
subj :e
s pron:yes




since
NP
///// � � � � �
NP
RX
� �
pron:yes
idx :e
VP
// ��
[idx:f]
NP
RX
V
contain
NP∗
TextClause
//// � � � �
Conn


since
pred:h7
pron:no
idx :e
S[s pron:yes]
�
//// � � �
</figure>
<bodyText confidence="0.993095076923077">
ICONOCLAST is a constraint-based system
which integrates text planning, document plan-
ning and pronominalization to generate all possi-
ble paraphrases for a given input. It uses a version
of Centering Theory (Grosz et al., 1995) adapted
to natural language generation to decide when to
pronominalize noun phrases in the generated text.
ICONOCLAST has an overgenerate and test ap-
proach, where all possible paraphrases are gener-
ated and the solutions are ranked according to a set
of soft constraints. The system generated 172 so-
lutions for the above input, of which (8) illustrates
the top three:
</bodyText>
<listItem confidence="0.581022">
(8) a Since Elixir contains gestodene it is
</listItem>
<bodyText confidence="0.970949730769231">
banned by the FDA. However, the FDA
approves Elixir Plus since Elixir Plus
contains oestradiol.
b Elixir contains gestodene so it is
banned by the FDA. However, the FDA
approves ElixirPlus since ElixirPlus
contains oestradiol.
c Elixir is banned by the FDA since it
contains gestodene. However,
ElixirPlus is approved by the FDA
since it contains oestradiol.
We have regenerated the same text, using only
a surface realizer and the grammar described in
the previous sections, without a referring expres-
sion generation module. A post-processing script
transforms RX nodes into a pronoun when they
have the relevant feature ([pron:yes]) and into
a name when the [pron] feature is missing or its
value is no. The surface realizer produced 208 so-
lutions for the same input, of which 96 contained
parentheticals. Some of the output is illustrated in
(9). Since sentence final parenthetical construc-
tions are impossible to distinguish from sentence-
final subordinate clauses in many cases, there is an
overlap between the solutions generated by ICON-
OCLAST and the 96 solutions generated by our
</bodyText>
<page confidence="0.992729">
51
</page>
<bodyText confidence="0.91887">
(9) a The FDA bans Elixir since Elixir contains gestodene. However, Elixir Plus (since it contains
oestradiol) is approved by the FDA.
b Since Elixir Plus contains oestradiol, although the FDA bans Elixir (since it contains
gestodene), Elixir Plus is approved by the FDA.
c Elixir contains gestodene. Consequently, Elixir is banned by the FDA. However, Elixir Plus
(since it contains oestradiol) is approved by the FDA.
d Elixir Plus contains oestradiol. Consequently, although the FDA bans Elixir (since it contains
gestodene), the FDA approves Elixir Plus.
e Elixir Plus (since it contains oestradiol) is approved by the FDA (although it bans Elixir since
it contains gestodene).
f The FDA bans Elixir (since it contains gestodene). However, Elixir Plus is approved by the
FDA since Elixir Plus contains oestradiol.
grammar which contain parentheticals. Also, de-
spite the fact that the two systems use the same
discourse connectives and a very similar grammar,
there are slight differences in the constructions
produced. For example, ICONOCLAST allows
subordinating conjunctions to “dominate” coordi-
nating conjunctions, producing solutions like the
one in (10), although these solutions are assigned
at least 4 defects in all cases. These constructions
are not allowed in our grammar.
(10) Although Elixir contains gestodene so it is
banned by the FDA ElixirPlus contains
oestradiol so it is approved by the FDA.
Though comparing the generated solutions is
not a straightforward task because of these subtle
differences and the sheer number of the solutions
produced, the two systems do generate a number
of very similar outputs, including the ones shown
in (8). However, a significant difference is that our
system generates coherent texts which include par-
enthetical constructions, and which are not gener-
ated by ICONOCLAST at all.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.99999">
Our grammar design was inspired by three
discourse-level extensions of Lexicalized Tree Ad-
joining Grammar. A common idea behind all these
approaches is to build an integrated text under-
standing or generation system in which the same
mechanisms are used for the sentence and dis-
course levels.
DLTAG (Webber, 2004) is an extension of
LTAG in which discourse syntax is projected by
different types of discourse connectives. In this
approach discourse-level syntax is considered to
be a separate layer on top of sentence-level syntax
and there are two kinds of discourse connectives:
anaphoric and structural (Webber et al., 2003).
This analysis is not suitable for natural language
generation systems which need to have an explicit
representation for the arguments of discourse con-
nectives.
G-TAG (Danlos, 2000) is another discourse-
level extension of TAG where underspecified ‘g-
derivation trees’ are created for a conceptual input
and grouped into lexical databases. A g-derivation
tree specifies a set of surface variants, one of
which is produced by linearization of the g-derived
tree. The other surface variants are created by a
post-processing module. While this methodology
efficiently reduces the search space of solutions by
grouping them together, it assumes that all variants
of the same sentence can be generated in the same
discourse context.
Most recently, Danlos (2008) introduces D-
STAG, a discourse level synchronous TAG cou-
pled with Segmented Discourse Representation
Theory (Asher, 1993). In this framework the sen-
tential grammar (S-TAG) and the discourse gram-
mar (D-STAG) are not integrated, therefore dis-
courses where arguments of discourse relations
come from discontinuous text spans (as in rela-
tive clauses or other types of parentheticals) are
not handled by the theory.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999820333333333">
We have presented an argument against modular-
izing linguistic information in natural language
generation systems. We have argued that com-
plex linguistic constructions which require inter-
actions between several system components are
best represented in natural language generation
</bodyText>
<page confidence="0.99349">
52
</page>
<bodyText confidence="0.999992117647059">
systems using an integrated grammar. As an ex-
ample, we have presented the problem of gener-
ating parenthetical constructions. Current natural
language generation systems either do not gener-
ate these constructions at all, or if they do, they
do not have a principled approach to the prob-
lem and generate parentheticals by adding more
modules to a pipeline. We have shown that par-
entheticals can be generated in a principled way
using a surface realizer, when it is equipped with
an integrated grammar which incorporates infor-
mation about syntax, discourse and referring ex-
pressions. The solutions produced by our surface
realizer demonstrate that this approach enhances
the fluency of the generated text and the flexibility
of generation systems, without adding extra com-
ponents or changing the system’s architecture.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916841463415">
D.E. Appelt. 1985. Planning English sentences. Cam-
bridge University Press, Cambridge.
N. Asher. 1993. Reference to Abstract Objects in En-
glish. Kluwer, Dordrecht.
E. Banik and A. Lee. 2008. A study of parenthet-
icals in discourse corpora – implications for NLG
systems. In Proceedings ofLREC 2008, Marrakesh.
C. B. Callaway and J. C. Lester. 1997. Dynami-
cally improving explanations: A revision-based ap-
proach to explanation generation. In Fifteenth Inter-
national Joint Conference on Artificial Intelligence,
pages 952–58, Nagoya, Japan.
C. B. Callaway. 2003. Integrating discourse markers
into a pipelined natural language generation archi-
tecture. In ACL ’03: Proceedings ofthe 41st Annual
Meeting on Association for Computational Linguis-
tics, pages 264–271.
L. Danlos. 2000. G-TAG: A lexicalized formalism for
text generation inspired by Tree Adjoining Gram-
mar. In A. Abeille and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, linguistic analy-
sis and processing, pages 343–370. CSLI, Stanford.
L. Danlos. 2008. D-STAG: Parsing discourse with
synchronous TAG and SDRT background. In Pro-
ceedings of the Third International Workshop on
Constraints in Discourse (CID’2008) Postdam.
B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Cen-
tering: a framework for modelling the local co-
herence of discourse. Computational Linguistics,
21(2):203–225.
E. H. Hovy. 1988. Two types of planning in language
generation. In Proceedings of the 26th annual meet-
ing on Association for Computational Linguistics,
pages 179–186, Morristown, NJ, USA. Association
for Computational Linguistics.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63(1-2):341–385.
A. K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. In Rosenberg and Salomaa, editors,
Handbook ofFormal Languages and Automata, vol-
ume 3, pages 69–124. Springer-Verlag, Heidelberg.
A. Kehler. 2002. Coherence, Reference and the Theory
of Grammar. CSLI.
R. Kibble and R. Power. 2004. Optimizing referential
coherence in text generation. Computational Lin-
guistics, 30(4):401–416.
E. Kow. 2007. Surface realisation: ambiguity and
determinism. Ph.D. thesis, Universite de Henri
Poincare.
D. D. McDonald. 1983. Natural language genera-
tion as a computational problem. In M. Brady and
Robert Berwick, editors, Computational Models of
Discourse, pages 209–265. MIT Press.
E. Miltsakaki. 2003. The Syntax-Discourse Interface:
Effects of the Main-Subordinate Distinction on At-
tention Structure. Ph.D. thesis, Department of Lin-
guistics, University of Pennsylvania.
R. Power N. Bouayad-Agha, D. Scott. 2001. The influ-
ence of layout on the interpretation of referring ex-
pressions. In L. Degand Y. Bestgen W. Spooren L.
van Waes, editor, Multidisciplinary Approaches to
Discourse, pages 133–141.
R. Power, D. Scott, and N. Bouayad-Agha. 2003.
Document structure. Computational Linguistics,
29(4):211–260.
J. Robin. 1994. Revision-based generation of Natu-
ral Language Summaries providing historical Back-
ground. Ph.D. thesis, Columbia University.
D. Scott and C. S. Souza. 1990. Getting the message
across in RST-based text generation. In C. Mellish
R. Dale M. Zock, editor, Current Research in Nat-
ural Language Generation, pages 31–56. Academic
Press.
J. Shaw. 2002. Clause Aggregation: An approach
to generating concise text. Ph.D. thesis, Columbia
University.
B. Webber, M. Stone, A. Joshi, and A. Knott. 2003.
Anaphora and discourse structure. Computational
Linguistics, 29(4):545–587.
B. Webber. 2004. D-LTAG: extending lexicalized TAG
to discourse. Cognitive Science, 28(5):751–779.
</reference>
<page confidence="0.999348">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.457067">
<title confidence="0.999408">Parenthetical Constructions an Argument against Modularity</title>
<author confidence="0.874495">Eva The Open Milton Keynes</author>
<email confidence="0.999326">e.banik@open.ac.uk</email>
<abstract confidence="0.9779116">This paper presents an argument against modularizing linguistic information in natural language generation systems. We argue that complex linguistic constructions require grammatical information to be located in the same module, in order to avoid over-complicating the system architecture. We demonstrate this point by showing how parenthetical constructions — which have only been generated in previous systems using an aggregation or revision module — can be generated by a surface realizer when using an integrated grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D E Appelt</author>
</authors>
<title>Planning English sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="11206" citStr="Appelt, 1985" startWordPosition="1768" endWordPosition="1769">te coherent discourse, a generation system needs access to a grammar that is capable of representing multisentential text. In modular systems this is typically achieved by two modules: a text planning module which constructs a text plan and a surface realizer that converts the text plan into sentences. However, text planning and linguistic realization are not two independent processes and many linguistic decisions are in fact made by the text planner. The interactions between text planning and linguistic realization in modular systems have been handled in several ways, including backtracking (Appelt, 1985), interleaving the two components (McDonald, 1983) and restrictive planning (Hovy, 1988). These approaches however make the system inflexible because all possible interactions between modules have to be anticipated by the system designer. Another, more recent approach to tackle this problem is to use lexicalization not only for sentences but also for texts. The theoretical background for lexicalization on the discourse level has been laid down for Tree Adjoining Grammar (Joshi and Schabes, 1997) by several researchers, including Webber (2004), and Danlos (2000). In particular, Danlos (2000) sh</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>D.E. Appelt. 1985. Planning English sentences. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Asher</author>
</authors>
<title>Reference to Abstract Objects in English.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="25208" citStr="Asher, 1993" startWordPosition="4078" endWordPosition="4079">es’ are created for a conceptual input and grouped into lexical databases. A g-derivation tree specifies a set of surface variants, one of which is produced by linearization of the g-derived tree. The other surface variants are created by a post-processing module. While this methodology efficiently reduces the search space of solutions by grouping them together, it assumes that all variants of the same sentence can be generated in the same discourse context. Most recently, Danlos (2008) introduces DSTAG, a discourse level synchronous TAG coupled with Segmented Discourse Representation Theory (Asher, 1993). In this framework the sentential grammar (S-TAG) and the discourse grammar (D-STAG) are not integrated, therefore discourses where arguments of discourse relations come from discontinuous text spans (as in relative clauses or other types of parentheticals) are not handled by the theory. 8 Conclusions We have presented an argument against modularizing linguistic information in natural language generation systems. We have argued that complex linguistic constructions which require interactions between several system components are best represented in natural language generation 52 systems using</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>N. Asher. 1993. Reference to Abstract Objects in English. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Banik</author>
<author>A Lee</author>
</authors>
<title>A study of parentheticals in discourse corpora – implications for NLG systems.</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC 2008, Marrakesh.</booktitle>
<contexts>
<context position="8427" citStr="Banik and Lee, 2008" startWordPosition="1308" endWordPosition="1311"> syntactic constraints on them; 47 - elements of layout (punctuation marks for main clauses and parentheticals). We show that by incorporating the above kinds of linguistic information into the grammar of a surface realizer we can improve the flexibility of the system (i.e., generate more paraphrases for the same input) and improve the quality of the generated text without adding more modules to the system. 2.1 Syntactic constraints on pronominalization To design a grammar for parenthetical constructions, we have carried out a corpus study on embedded rhetorical relations in the RST treebank (Banik and Lee, 2008). The corpus study has shown that the most numerous class of embedded subordinate clauses that occur in sentencemedial position contain a subject pronoun (as in 4a). This embedded subject pronoun in all cases referred back to the subject of the matrix clause, which always immediately preceded the subordinate clause. The pronoun can be either explicit (as in 4a) or implicit (as in the examples in 1). Of the 119 sentence-medial subordinate clauses that we looked at in the study, 35 were of this type (what we call pseudo-relatives).1 This suggests that in sentence-medial subordinate clauses (or s</context>
</contexts>
<marker>Banik, Lee, 2008</marker>
<rawString>E. Banik and A. Lee. 2008. A study of parentheticals in discourse corpora – implications for NLG systems. In Proceedings ofLREC 2008, Marrakesh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Callaway</author>
<author>J C Lester</author>
</authors>
<title>Dynamically improving explanations: A revision-based approach to explanation generation.</title>
<date>1997</date>
<booktitle>In Fifteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>952--58</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="6208" citStr="Callaway and Lester, 1997" startWordPosition="969" endWordPosition="972">us . b The FDA bans Elixir because it contains Gestodene. However, Elixir Plus is approved by the FDA c The FDA approves Elixir Plus although Elixir — since it contains Gestodene — is banned by the FDA. Generation systems that produce output similar to the examples in (3) have three kinds of strategies: either a text planning module chooses a discourse connective and decides the position and ordering of clauses (Hovy, 1993) or aggregation is considered to be one of the tasks of the sentence planning module (Shaw, 2002); or a revision module performs aggregation opportunistically (Robin, 1994; Callaway and Lester, 1997). However, none of these systems handle parenthetical constructions in a principled way. Systems where aggregation is part of the text planning module only produce complex sentences made up of clauses joined by discourse connectives – sentence-medial subordinate clauses are not generated at all. In revision-based systems, the output often needs to be corrected after aggregation. For example, Robin’s system includes various transformations to correct redundancies, ambiguities or invalid lexical collocations introduced by the revision module. In Shaw’s system, the referring expression generation</context>
</contexts>
<marker>Callaway, Lester, 1997</marker>
<rawString>C. B. Callaway and J. C. Lester. 1997. Dynamically improving explanations: A revision-based approach to explanation generation. In Fifteenth International Joint Conference on Artificial Intelligence, pages 952–58, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Callaway</author>
</authors>
<title>Integrating discourse markers into a pipelined natural language generation architecture. In</title>
<date>2003</date>
<booktitle>ACL ’03: Proceedings ofthe 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="1809" citStr="Callaway, 2003" startWordPosition="274" endWordPosition="275">limited number of grammatical constructions, typically collected during a corpus study of example documents. Often the grammar is implemented using schemas or “canned” expressions, and individual grammatical levels are distributed in independent modules. Organizing the grammar this way severely limits the flexibility of NLG systems. It has long been recognized in the literature that text fluency can be improved by modeling interactions between grammar modules. The most commonly mentioned interactions are those among discourse/rhetorical relations and syntax (Scott and Souza, 1990; Hovy, 1993; Callaway, 2003), rhetorical relations, syntax and referring expressions (Kibble and Power, 2004); and layout and referring expressions (N. Bouayad-Agha, 2001). It is clear that in order to generate high quality, coherent discourse, a generator needs access to a grammar which is able to model the interdependent, context-sensitive behaviour of these separate linguistic phenomena. In this paper we draw a parallel between grammar design and the design of natural language generation systems. We argue that in order to generate complex linguistic constructions, current NLG systems tend to have overly complicated ar</context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>C. B. Callaway. 2003. Integrating discourse markers into a pipelined natural language generation architecture. In ACL ’03: Proceedings ofthe 41st Annual Meeting on Association for Computational Linguistics, pages 264–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
</authors>
<title>G-TAG: A lexicalized formalism for text generation inspired by Tree Adjoining Grammar.</title>
<date>2000</date>
<pages>343--370</pages>
<editor>In A. Abeille and O. Rambow, editors,</editor>
<publisher>CSLI, Stanford.</publisher>
<contexts>
<context position="11773" citStr="Danlos (2000)" startWordPosition="1855" endWordPosition="1856">al ways, including backtracking (Appelt, 1985), interleaving the two components (McDonald, 1983) and restrictive planning (Hovy, 1988). These approaches however make the system inflexible because all possible interactions between modules have to be anticipated by the system designer. Another, more recent approach to tackle this problem is to use lexicalization not only for sentences but also for texts. The theoretical background for lexicalization on the discourse level has been laid down for Tree Adjoining Grammar (Joshi and Schabes, 1997) by several researchers, including Webber (2004), and Danlos (2000). In particular, Danlos (2000) shows that extending lexicalization to the discourse level makes it possible to completely integrate text planning and surface realization. 48 We have designed a Tree Adjoining Grammar for parenthetical constructions following this latter approach. Elementary trees in the grammar are associated with a flat semantic representation. The trees integrate syntax and discourse representations in the sense that each sentence-level elementary tree includes one or more discourse-level nodes. The elementary trees in Fig. 1 illustrate what we mean by this: every lexical ite</context>
<context position="13004" citStr="Danlos (2000)" startWordPosition="2050" endWordPosition="2051">lly project a sentence in a syntactic grammar (i.e., an S-rooted tree) here projects a discourse clause (i.e., a Dc rooted tree). Every predicate that projects a discourse clause is assigned two kinds of elementary trees: a discourse initial tree (e.g., Fig. 1a) and a discourse continuing tree (e.g., Fig. 1b), which takes the preceding discourse clause as an argument. Figure 1: Elementary syntax/discourse trees The combination of these two trees corresponds to the empty connective (⊕ in Danlos (2000)). Other types of discourse connectives are implemented in the grammar the usual way (see e.g. Danlos (2000)). 4 Referring expressions One of the challenges of generating paraphrases from a semantic representation is that in some versions there will be a mismatch between the number of noun phrases needed to make the output syntactically well-formed and the number of semantic arguments in the input which can potentially become a noun phrase. This happens whenever a discourse entity is the argument of more than one semantic predicate. For example, (5) shows possible realizations of the following input where (5a) contains three syntactic slots for “Elixir”, (5b,c) contain two slots, and (5d) only one: </context>
<context position="24515" citStr="Danlos, 2000" startWordPosition="3973" endWordPosition="3974">r generation system in which the same mechanisms are used for the sentence and discourse levels. DLTAG (Webber, 2004) is an extension of LTAG in which discourse syntax is projected by different types of discourse connectives. In this approach discourse-level syntax is considered to be a separate layer on top of sentence-level syntax and there are two kinds of discourse connectives: anaphoric and structural (Webber et al., 2003). This analysis is not suitable for natural language generation systems which need to have an explicit representation for the arguments of discourse connectives. G-TAG (Danlos, 2000) is another discourselevel extension of TAG where underspecified ‘gderivation trees’ are created for a conceptual input and grouped into lexical databases. A g-derivation tree specifies a set of surface variants, one of which is produced by linearization of the g-derived tree. The other surface variants are created by a post-processing module. While this methodology efficiently reduces the search space of solutions by grouping them together, it assumes that all variants of the same sentence can be generated in the same discourse context. Most recently, Danlos (2008) introduces DSTAG, a discour</context>
</contexts>
<marker>Danlos, 2000</marker>
<rawString>L. Danlos. 2000. G-TAG: A lexicalized formalism for text generation inspired by Tree Adjoining Grammar. In A. Abeille and O. Rambow, editors, Tree Adjoining Grammars: Formalisms, linguistic analysis and processing, pages 343–370. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
</authors>
<title>D-STAG: Parsing discourse with synchronous TAG and SDRT background.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Workshop on Constraints in Discourse (CID’2008) Postdam.</booktitle>
<contexts>
<context position="25087" citStr="Danlos (2008)" startWordPosition="4061" endWordPosition="4062">scourse connectives. G-TAG (Danlos, 2000) is another discourselevel extension of TAG where underspecified ‘gderivation trees’ are created for a conceptual input and grouped into lexical databases. A g-derivation tree specifies a set of surface variants, one of which is produced by linearization of the g-derived tree. The other surface variants are created by a post-processing module. While this methodology efficiently reduces the search space of solutions by grouping them together, it assumes that all variants of the same sentence can be generated in the same discourse context. Most recently, Danlos (2008) introduces DSTAG, a discourse level synchronous TAG coupled with Segmented Discourse Representation Theory (Asher, 1993). In this framework the sentential grammar (S-TAG) and the discourse grammar (D-STAG) are not integrated, therefore discourses where arguments of discourse relations come from discontinuous text spans (as in relative clauses or other types of parentheticals) are not handled by the theory. 8 Conclusions We have presented an argument against modularizing linguistic information in natural language generation systems. We have argued that complex linguistic constructions which re</context>
</contexts>
<marker>Danlos, 2008</marker>
<rawString>L. Danlos. 2008. D-STAG: Parsing discourse with synchronous TAG and SDRT background. In Proceedings of the Third International Workshop on Constraints in Discourse (CID’2008) Postdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: a framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="20354" citStr="Grosz et al., 1995" startWordPosition="3313" endWordPosition="3316"> a) S[s pron:?X] � ///// �� � � RX � � pron:?X idx:e h2:cause(h7 h0) NP //// � � � � NP∗ (a) Elementary trees for (7)      pred:h7 pron:no idx :e   TextClause /// � � � idx :h0 subj :e s pron:yes     since NP ///// � � � � � NP RX � � pron:yes idx :e VP // �� [idx:f] NP RX V contain NP∗ TextClause //// � � � � Conn   since pred:h7 pron:no idx :e S[s pron:yes] � //// � � � ICONOCLAST is a constraint-based system which integrates text planning, document planning and pronominalization to generate all possible paraphrases for a given input. It uses a version of Centering Theory (Grosz et al., 1995) adapted to natural language generation to decide when to pronominalize noun phrases in the generated text. ICONOCLAST has an overgenerate and test approach, where all possible paraphrases are generated and the solutions are ranked according to a set of soft constraints. The system generated 172 solutions for the above input, of which (8) illustrates the top three: (8) a Since Elixir contains gestodene it is banned by the FDA. However, the FDA approves Elixir Plus since Elixir Plus contains oestradiol. b Elixir contains gestodene so it is banned by the FDA. However, the FDA approves ElixirPlus</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Centering: a framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
</authors>
<title>Two types of planning in language generation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>179--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11294" citStr="Hovy, 1988" startWordPosition="1780" endWordPosition="1781">presenting multisentential text. In modular systems this is typically achieved by two modules: a text planning module which constructs a text plan and a surface realizer that converts the text plan into sentences. However, text planning and linguistic realization are not two independent processes and many linguistic decisions are in fact made by the text planner. The interactions between text planning and linguistic realization in modular systems have been handled in several ways, including backtracking (Appelt, 1985), interleaving the two components (McDonald, 1983) and restrictive planning (Hovy, 1988). These approaches however make the system inflexible because all possible interactions between modules have to be anticipated by the system designer. Another, more recent approach to tackle this problem is to use lexicalization not only for sentences but also for texts. The theoretical background for lexicalization on the discourse level has been laid down for Tree Adjoining Grammar (Joshi and Schabes, 1997) by several researchers, including Webber (2004), and Danlos (2000). In particular, Danlos (2000) shows that extending lexicalization to the discourse level makes it possible to completely</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>E. H. Hovy. 1988. Two types of planning in language generation. In Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 179–186, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="1792" citStr="Hovy, 1993" startWordPosition="272" endWordPosition="273">tain only a limited number of grammatical constructions, typically collected during a corpus study of example documents. Often the grammar is implemented using schemas or “canned” expressions, and individual grammatical levels are distributed in independent modules. Organizing the grammar this way severely limits the flexibility of NLG systems. It has long been recognized in the literature that text fluency can be improved by modeling interactions between grammar modules. The most commonly mentioned interactions are those among discourse/rhetorical relations and syntax (Scott and Souza, 1990; Hovy, 1993; Callaway, 2003), rhetorical relations, syntax and referring expressions (Kibble and Power, 2004); and layout and referring expressions (N. Bouayad-Agha, 2001). It is clear that in order to generate high quality, coherent discourse, a generator needs access to a grammar which is able to model the interdependent, context-sensitive behaviour of these separate linguistic phenomena. In this paper we draw a parallel between grammar design and the design of natural language generation systems. We argue that in order to generate complex linguistic constructions, current NLG systems tend to have over</context>
<context position="6009" citStr="Hovy, 1993" startWordPosition="938" endWordPosition="939">bans Elixir. But it approves Elixir Plus. Correct realizations of the same message would include: (3) a The FDA — though it bans Elixir since it contains Gestodene — approves Elixir Plus . b The FDA bans Elixir because it contains Gestodene. However, Elixir Plus is approved by the FDA c The FDA approves Elixir Plus although Elixir — since it contains Gestodene — is banned by the FDA. Generation systems that produce output similar to the examples in (3) have three kinds of strategies: either a text planning module chooses a discourse connective and decides the position and ordering of clauses (Hovy, 1993) or aggregation is considered to be one of the tasks of the sentence planning module (Shaw, 2002); or a revision module performs aggregation opportunistically (Robin, 1994; Callaway and Lester, 1997). However, none of these systems handle parenthetical constructions in a principled way. Systems where aggregation is part of the text planning module only produce complex sentences made up of clauses joined by discourse connectives – sentence-medial subordinate clauses are not generated at all. In revision-based systems, the output often needs to be corrected after aggregation. For example, Robin’</context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>Eduard H. Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelligence, 63(1-2):341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-Adjoining Grammars.</title>
<date>1997</date>
<booktitle>In Rosenberg and Salomaa, editors, Handbook ofFormal Languages and Automata,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<publisher>Springer-Verlag,</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="11706" citStr="Joshi and Schabes, 1997" startWordPosition="1844" endWordPosition="1847">nning and linguistic realization in modular systems have been handled in several ways, including backtracking (Appelt, 1985), interleaving the two components (McDonald, 1983) and restrictive planning (Hovy, 1988). These approaches however make the system inflexible because all possible interactions between modules have to be anticipated by the system designer. Another, more recent approach to tackle this problem is to use lexicalization not only for sentences but also for texts. The theoretical background for lexicalization on the discourse level has been laid down for Tree Adjoining Grammar (Joshi and Schabes, 1997) by several researchers, including Webber (2004), and Danlos (2000). In particular, Danlos (2000) shows that extending lexicalization to the discourse level makes it possible to completely integrate text planning and surface realization. 48 We have designed a Tree Adjoining Grammar for parenthetical constructions following this latter approach. Elementary trees in the grammar are associated with a flat semantic representation. The trees integrate syntax and discourse representations in the sense that each sentence-level elementary tree includes one or more discourse-level nodes. The elementary</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A. K. Joshi and Y. Schabes. 1997. Tree-Adjoining Grammars. In Rosenberg and Salomaa, editors, Handbook ofFormal Languages and Automata, volume 3, pages 69–124. Springer-Verlag, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>Coherence, Reference and the Theory of Grammar.</title>
<date>2002</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="16294" citStr="Kehler (2002)" startWordPosition="2572" endWordPosition="2573">:white-cream(e) D, ��� � � � S /\ NPS. [idx:e] is cream [idx:e] VP �� �� V NP Punct . (a) discourse initial h2:contain(e,a) D, �� � � D, S. D, �� � � S Punct �� � � NPS. [idx:e] V contains (b) discourse continuing VP . �� � � NPS. [idx:a] 49 Figure 2: Elementary trees with referring expressions in the matrix clause, whereas referring expressions in main clauses tend to find their antecedent in the previous main clause. This suggests that pronominalization should be treated differently in subordinate clauses than in main clauses. Research in theoretical linguistics underlines this claim, where Kehler (2002) has shown that apparent discrepancies between different accounts of pronominalization can be reconciled if each method is applied in a different discourse context. To sum up, in this integrated approach part of the job of the referring expression generation module is taken over by the grammar, namely - pronominalization of discourse entities in subordinate clauses and - decisions about when not to realize underspecified referring expressions as pronouns. 5 Representing parenthetical constructions Integrating referring expressions into the grammar this way makes it possible to state syntactic </context>
</contexts>
<marker>Kehler, 2002</marker>
<rawString>A. Kehler. 2002. Coherence, Reference and the Theory of Grammar. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kibble</author>
<author>R Power</author>
</authors>
<title>Optimizing referential coherence in text generation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1890" citStr="Kibble and Power, 2004" startWordPosition="283" endWordPosition="286"> corpus study of example documents. Often the grammar is implemented using schemas or “canned” expressions, and individual grammatical levels are distributed in independent modules. Organizing the grammar this way severely limits the flexibility of NLG systems. It has long been recognized in the literature that text fluency can be improved by modeling interactions between grammar modules. The most commonly mentioned interactions are those among discourse/rhetorical relations and syntax (Scott and Souza, 1990; Hovy, 1993; Callaway, 2003), rhetorical relations, syntax and referring expressions (Kibble and Power, 2004); and layout and referring expressions (N. Bouayad-Agha, 2001). It is clear that in order to generate high quality, coherent discourse, a generator needs access to a grammar which is able to model the interdependent, context-sensitive behaviour of these separate linguistic phenomena. In this paper we draw a parallel between grammar design and the design of natural language generation systems. We argue that in order to generate complex linguistic constructions, current NLG systems tend to have overly complicated architectures. To illustrate this point we show how a surface realizer can take on </context>
</contexts>
<marker>Kibble, Power, 2004</marker>
<rawString>R. Kibble and R. Power. 2004. Optimizing referential coherence in text generation. Computational Linguistics, 30(4):401–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kow</author>
</authors>
<title>Surface realisation: ambiguity and determinism.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Universite de Henri Poincare.</institution>
<contexts>
<context position="18696" citStr="Kow, 2007" startWordPosition="2965" endWordPosition="2966">atures on the auxiliary tree state that the subject of this embedded clause should be expressed by a pronoun and that it should refer to the same discourse entity as the head noun that the auxiliary tree adjoins to. When the discourse connective is combined with the embedded clause, these features are percolated to the referring expression in subject position, requiring it to be realized by a pronoun. Figure 4 illustrates the elementary trees and the derived tree for the embedded clause in (7). 6 Comparison As an experiment, we have implemented a grammar fragment in the GenI surface realizer (Kow, 2007) and regenerated an example from the ICONOCLAST generator (Power et al., 2003). The example we used is represented by the following input semantics: h1: elixir(e) h2: fda(f) h3: elixir plus(p) h4: gestodene(g) h5: contain(e g) h6: ban(f e) h7: approve(f p) h8: concession(h6 h7) h9: cause(h5 h6) h10: contain(p o) h11: oestradiol(o) h12: cause(h10 h7) h2:contain(e,a) Dc I_✟ ❍❍ Dc 1 Dc ✟✟ ❍ ❍ S Punct ✟✟ ❍ ❍ RX [idx:e] V banned by VP . ✟✟ ❍ ❍ RX [idx:a] 50 Figure 3: Pronouns not allowed before an appositive Figure 4: Obligatory pronouns in parenthetical subordinate clauses drug (b) Derived tree fo</context>
</contexts>
<marker>Kow, 2007</marker>
<rawString>E. Kow. 2007. Surface realisation: ambiguity and determinism. Ph.D. thesis, Universite de Henri Poincare.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D McDonald</author>
</authors>
<title>Natural language generation as a computational problem.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>209--265</pages>
<editor>In M. Brady and Robert Berwick, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11256" citStr="McDonald, 1983" startWordPosition="1774" endWordPosition="1776"> access to a grammar that is capable of representing multisentential text. In modular systems this is typically achieved by two modules: a text planning module which constructs a text plan and a surface realizer that converts the text plan into sentences. However, text planning and linguistic realization are not two independent processes and many linguistic decisions are in fact made by the text planner. The interactions between text planning and linguistic realization in modular systems have been handled in several ways, including backtracking (Appelt, 1985), interleaving the two components (McDonald, 1983) and restrictive planning (Hovy, 1988). These approaches however make the system inflexible because all possible interactions between modules have to be anticipated by the system designer. Another, more recent approach to tackle this problem is to use lexicalization not only for sentences but also for texts. The theoretical background for lexicalization on the discourse level has been laid down for Tree Adjoining Grammar (Joshi and Schabes, 1997) by several researchers, including Webber (2004), and Danlos (2000). In particular, Danlos (2000) shows that extending lexicalization to the discourse</context>
</contexts>
<marker>McDonald, 1983</marker>
<rawString>D. D. McDonald. 1983. Natural language generation as a computational problem. In M. Brady and Robert Berwick, editors, Computational Models of Discourse, pages 209–265. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
</authors>
<title>The Syntax-Discourse Interface: Effects of the Main-Subordinate Distinction on Attention Structure.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Linguistics, University of Pennsylvania.</institution>
<contexts>
<context position="15496" citStr="Miltsakaki (2003)" startWordPosition="2437" endWordPosition="2438"> expression leaf node as illustrated in Fig.2. This allows syntactic constraints to be ‘posted’ on referring expressions in the appropriate contexts while completely specifying the form of the underspecified slots still remains the task of a referring expression module. In other words, we factor out pronominalization decisions dictated by syntax from pronominalization decisions dictated by discourse level constraints. Treating pronouns in subordinate clauses differently from pronouns in main clauses has independent justification from psycholinguistics and theoretical linguistics. For example, Miltsakaki (2003) has carried out psycholinguistic experiments on complex sentences containing relative clauses. The experiments show that pronouns in embedded clauses tend to refer back to an entity h1:white-cream(e) D, ��� � � � S /\ NPS. [idx:e] is cream [idx:e] VP �� �� V NP Punct . (a) discourse initial h2:contain(e,a) D, �� � � D, S. D, �� � � S Punct �� � � NPS. [idx:e] V contains (b) discourse continuing VP . �� � � NPS. [idx:a] 49 Figure 2: Elementary trees with referring expressions in the matrix clause, whereas referring expressions in main clauses tend to find their antecedent in the previous main </context>
</contexts>
<marker>Miltsakaki, 2003</marker>
<rawString>E. Miltsakaki. 2003. The Syntax-Discourse Interface: Effects of the Main-Subordinate Distinction on Attention Structure. Ph.D. thesis, Department of Linguistics, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power N Bouayad-Agha</author>
<author>D Scott</author>
</authors>
<title>The influence of layout on the interpretation of referring expressions.</title>
<date>2001</date>
<booktitle>Multidisciplinary Approaches to Discourse,</booktitle>
<pages>133--141</pages>
<editor>In L. Degand Y. Bestgen W. Spooren L. van Waes, editor,</editor>
<marker>Bouayad-Agha, Scott, 2001</marker>
<rawString>R. Power N. Bouayad-Agha, D. Scott. 2001. The influence of layout on the interpretation of referring expressions. In L. Degand Y. Bestgen W. Spooren L. van Waes, editor, Multidisciplinary Approaches to Discourse, pages 133–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
<author>D Scott</author>
<author>N Bouayad-Agha</author>
</authors>
<date>2003</date>
<booktitle>Document structure. Computational Linguistics,</booktitle>
<pages>29--4</pages>
<contexts>
<context position="18774" citStr="Power et al., 2003" startWordPosition="2975" endWordPosition="2978">clause should be expressed by a pronoun and that it should refer to the same discourse entity as the head noun that the auxiliary tree adjoins to. When the discourse connective is combined with the embedded clause, these features are percolated to the referring expression in subject position, requiring it to be realized by a pronoun. Figure 4 illustrates the elementary trees and the derived tree for the embedded clause in (7). 6 Comparison As an experiment, we have implemented a grammar fragment in the GenI surface realizer (Kow, 2007) and regenerated an example from the ICONOCLAST generator (Power et al., 2003). The example we used is represented by the following input semantics: h1: elixir(e) h2: fda(f) h3: elixir plus(p) h4: gestodene(g) h5: contain(e g) h6: ban(f e) h7: approve(f p) h8: concession(h6 h7) h9: cause(h5 h6) h10: contain(p o) h11: oestradiol(o) h12: cause(h10 h7) h2:contain(e,a) Dc I_✟ ❍❍ Dc 1 Dc ✟✟ ❍ ❍ S Punct ✟✟ ❍ ❍ RX [idx:e] V banned by VP . ✟✟ ❍ ❍ RX [idx:a] 50 Figure 3: Pronouns not allowed before an appositive Figure 4: Obligatory pronouns in parenthetical subordinate clauses drug (b) Derived tree for (6) (a) Elementary trees for (6) S Punct banned by RX [idx:f] , TextPhrase /</context>
</contexts>
<marker>Power, Scott, Bouayad-Agha, 2003</marker>
<rawString>R. Power, D. Scott, and N. Bouayad-Agha. 2003. Document structure. Computational Linguistics, 29(4):211–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>Revision-based generation of Natural Language Summaries providing historical Background.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="6180" citStr="Robin, 1994" startWordPosition="967" endWordPosition="968">ves Elixir Plus . b The FDA bans Elixir because it contains Gestodene. However, Elixir Plus is approved by the FDA c The FDA approves Elixir Plus although Elixir — since it contains Gestodene — is banned by the FDA. Generation systems that produce output similar to the examples in (3) have three kinds of strategies: either a text planning module chooses a discourse connective and decides the position and ordering of clauses (Hovy, 1993) or aggregation is considered to be one of the tasks of the sentence planning module (Shaw, 2002); or a revision module performs aggregation opportunistically (Robin, 1994; Callaway and Lester, 1997). However, none of these systems handle parenthetical constructions in a principled way. Systems where aggregation is part of the text planning module only produce complex sentences made up of clauses joined by discourse connectives – sentence-medial subordinate clauses are not generated at all. In revision-based systems, the output often needs to be corrected after aggregation. For example, Robin’s system includes various transformations to correct redundancies, ambiguities or invalid lexical collocations introduced by the revision module. In Shaw’s system, the ref</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>J. Robin. 1994. Revision-based generation of Natural Language Summaries providing historical Background. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Scott</author>
<author>C S Souza</author>
</authors>
<title>Getting the message across in RST-based text generation.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>31--56</pages>
<editor>In C. Mellish R. Dale M. Zock, editor,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="1780" citStr="Scott and Souza, 1990" startWordPosition="268" endWordPosition="271">rs in these systems contain only a limited number of grammatical constructions, typically collected during a corpus study of example documents. Often the grammar is implemented using schemas or “canned” expressions, and individual grammatical levels are distributed in independent modules. Organizing the grammar this way severely limits the flexibility of NLG systems. It has long been recognized in the literature that text fluency can be improved by modeling interactions between grammar modules. The most commonly mentioned interactions are those among discourse/rhetorical relations and syntax (Scott and Souza, 1990; Hovy, 1993; Callaway, 2003), rhetorical relations, syntax and referring expressions (Kibble and Power, 2004); and layout and referring expressions (N. Bouayad-Agha, 2001). It is clear that in order to generate high quality, coherent discourse, a generator needs access to a grammar which is able to model the interdependent, context-sensitive behaviour of these separate linguistic phenomena. In this paper we draw a parallel between grammar design and the design of natural language generation systems. We argue that in order to generate complex linguistic constructions, current NLG systems tend </context>
</contexts>
<marker>Scott, Souza, 1990</marker>
<rawString>D. Scott and C. S. Souza. 1990. Getting the message across in RST-based text generation. In C. Mellish R. Dale M. Zock, editor, Current Research in Natural Language Generation, pages 31–56. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shaw</author>
</authors>
<title>Clause Aggregation: An approach to generating concise text.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="6106" citStr="Shaw, 2002" startWordPosition="956" endWordPosition="957"> (3) a The FDA — though it bans Elixir since it contains Gestodene — approves Elixir Plus . b The FDA bans Elixir because it contains Gestodene. However, Elixir Plus is approved by the FDA c The FDA approves Elixir Plus although Elixir — since it contains Gestodene — is banned by the FDA. Generation systems that produce output similar to the examples in (3) have three kinds of strategies: either a text planning module chooses a discourse connective and decides the position and ordering of clauses (Hovy, 1993) or aggregation is considered to be one of the tasks of the sentence planning module (Shaw, 2002); or a revision module performs aggregation opportunistically (Robin, 1994; Callaway and Lester, 1997). However, none of these systems handle parenthetical constructions in a principled way. Systems where aggregation is part of the text planning module only produce complex sentences made up of clauses joined by discourse connectives – sentence-medial subordinate clauses are not generated at all. In revision-based systems, the output often needs to be corrected after aggregation. For example, Robin’s system includes various transformations to correct redundancies, ambiguities or invalid lexical</context>
</contexts>
<marker>Shaw, 2002</marker>
<rawString>J. Shaw. 2002. Clause Aggregation: An approach to generating concise text. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
<author>M Stone</author>
<author>A Joshi</author>
<author>A Knott</author>
</authors>
<title>Anaphora and discourse structure.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="24333" citStr="Webber et al., 2003" startWordPosition="3944" endWordPosition="3947">ammar design was inspired by three discourse-level extensions of Lexicalized Tree Adjoining Grammar. A common idea behind all these approaches is to build an integrated text understanding or generation system in which the same mechanisms are used for the sentence and discourse levels. DLTAG (Webber, 2004) is an extension of LTAG in which discourse syntax is projected by different types of discourse connectives. In this approach discourse-level syntax is considered to be a separate layer on top of sentence-level syntax and there are two kinds of discourse connectives: anaphoric and structural (Webber et al., 2003). This analysis is not suitable for natural language generation systems which need to have an explicit representation for the arguments of discourse connectives. G-TAG (Danlos, 2000) is another discourselevel extension of TAG where underspecified ‘gderivation trees’ are created for a conceptual input and grouped into lexical databases. A g-derivation tree specifies a set of surface variants, one of which is produced by linearization of the g-derived tree. The other surface variants are created by a post-processing module. While this methodology efficiently reduces the search space of solutions</context>
</contexts>
<marker>Webber, Stone, Joshi, Knott, 2003</marker>
<rawString>B. Webber, M. Stone, A. Joshi, and A. Knott. 2003. Anaphora and discourse structure. Computational Linguistics, 29(4):545–587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
</authors>
<title>D-LTAG: extending lexicalized TAG to discourse.</title>
<date>2004</date>
<journal>Cognitive Science,</journal>
<volume>28</volume>
<issue>5</issue>
<contexts>
<context position="11754" citStr="Webber (2004)" startWordPosition="1852" endWordPosition="1853">en handled in several ways, including backtracking (Appelt, 1985), interleaving the two components (McDonald, 1983) and restrictive planning (Hovy, 1988). These approaches however make the system inflexible because all possible interactions between modules have to be anticipated by the system designer. Another, more recent approach to tackle this problem is to use lexicalization not only for sentences but also for texts. The theoretical background for lexicalization on the discourse level has been laid down for Tree Adjoining Grammar (Joshi and Schabes, 1997) by several researchers, including Webber (2004), and Danlos (2000). In particular, Danlos (2000) shows that extending lexicalization to the discourse level makes it possible to completely integrate text planning and surface realization. 48 We have designed a Tree Adjoining Grammar for parenthetical constructions following this latter approach. Elementary trees in the grammar are associated with a flat semantic representation. The trees integrate syntax and discourse representations in the sense that each sentence-level elementary tree includes one or more discourse-level nodes. The elementary trees in Fig. 1 illustrate what we mean by this</context>
<context position="24019" citStr="Webber, 2004" startWordPosition="3897" endWordPosition="3898">solutions produced, the two systems do generate a number of very similar outputs, including the ones shown in (8). However, a significant difference is that our system generates coherent texts which include parenthetical constructions, and which are not generated by ICONOCLAST at all. 7 Related work Our grammar design was inspired by three discourse-level extensions of Lexicalized Tree Adjoining Grammar. A common idea behind all these approaches is to build an integrated text understanding or generation system in which the same mechanisms are used for the sentence and discourse levels. DLTAG (Webber, 2004) is an extension of LTAG in which discourse syntax is projected by different types of discourse connectives. In this approach discourse-level syntax is considered to be a separate layer on top of sentence-level syntax and there are two kinds of discourse connectives: anaphoric and structural (Webber et al., 2003). This analysis is not suitable for natural language generation systems which need to have an explicit representation for the arguments of discourse connectives. G-TAG (Danlos, 2000) is another discourselevel extension of TAG where underspecified ‘gderivation trees’ are created for a c</context>
</contexts>
<marker>Webber, 2004</marker>
<rawString>B. Webber. 2004. D-LTAG: extending lexicalized TAG to discourse. Cognitive Science, 28(5):751–779.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>