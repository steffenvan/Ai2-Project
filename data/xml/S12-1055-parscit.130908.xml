<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000908">
<title confidence="0.971202">
UTD: Determining Relational Similarity Using Lexical Patterns
</title>
<author confidence="0.996339">
Bryan Rink and Sanda Harabagiu
</author>
<affiliation confidence="0.997164">
University of Texas at Dallas
</affiliation>
<address confidence="0.9621575">
P.O. Box 830688; MS EC31
Richardson, TX, 75083-0688, USA
</address>
<email confidence="0.999624">
{bryan,sanda}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.995867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995003470588235">
In this paper we present our approach for
assigning degrees of relational similarity to
pairs of words in the SemEval-2012 Task 2.
To measure relational similarity we employed
lexical patterns that can match against word
pairs within a large corpus of 12 million docu-
ments. Patterns are weighted by obtaining sta-
tistically estimated lower bounds on their pre-
cision for extracting word pairs from a given
relation. Finally, word pairs are ranked based
on a model predicting the probability that they
belong to the relation of interest. This ap-
proach achieved the best results on the Se-
mEval 2012 Task 2, obtaining a Spearman cor-
relation of 0.229 and an accuracy on reproduc-
ing human answers to MaxDiff questions of
39.4%.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.874337722222222">
Considerable prior research has examined and elab-
orated upon a wide variety of semantic relations
between concepts along with techniques for auto-
matically discovering pairs of concepts for which
a relation holds (Bejar et al., 1991; Stephens and
Chen, 1996; Rosario and Hearst, 2004; Khoo and
Na, 2006; Girju et al., 2009). However, most pre-
vious work has considered membership assignment
for a semantic relation as a binary property. In this
paper we discuss an approach which assigns a de-
gree of membership to a pair of concepts for a given
relation. For example, for the semantic relation
CLASS-INCLUSION (Taxonomic), the concept pairs
weapon:spear and bird:robin are stronger members
Consider the following word pairs: millionaire:money,
author:copyright, robin:nest. These X:Y pairs share a
relation “X R Y”. Now consider the following word
pairs:
</bodyText>
<figure confidence="0.9925344">
(1) teacher:students
(2) farmer:crops
(3) homeowner:door
(4) shrubs:roots
Which of the numbered word pairs is the MOST illus-
</figure>
<figureCaption confidence="0.948803666666667">
trative example of the same relation “X R Y”?
Which of the above numbered word pairs is the
LEAST illustrative example of the same relation “X
R Y”?
Figure 1: Example Phase 2 MaxDiff question for the re-
lation 2h PART-WHOLE: Creature:Possession.
</figureCaption>
<bodyText confidence="0.999743388888889">
of the relationship than hair:brown, because brown
may describe many things other than hair, and brown
is also used much less frequently as a noun than the
words in the first two word pairs. Task 2 of Se-
mEval 2012 (Jurgens et al., 2012) was designed to
evaluate the effectiveness of automatic approaches
for determining the similarity of a pair of concepts
to a specific semantic relation. The task focused on
79 semantic relations from Bejar et al. (1991) which
broadly fall into the ten categories enumerated in Ta-
ble 1.
The data for the task was collected in two phases
using Amazon Mechanical Turk 1. During Phase
1, Turkers were asked to provide pairs of words
which fit a relation template, such as “X pos-
sesses/owns/has Y”. Turkers provided word pairs
such as expert:experience, mall:shops, letters:words,
and doctor:degree. A total of 3,218 word pairs
</bodyText>
<footnote confidence="0.993167">
1http://www.mturk.com/mturk/
</footnote>
<page confidence="0.918226">
413
</page>
<note confidence="0.7183625">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 413–418,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<table confidence="0.938965727272727">
Category Example word pairs Relations
CLASS-INCLUSION flower:tulip, weapon:knife, clothing:shirt, queen:Elizabeth 5
PART-WHOLE car:engine, fleet:ship, mile:yard, kickoff:football 10
SIMILAR car:auto, stream:river, eating:gluttony, colt:horse 8
CONTRAST alive:dead, old:young, east:west, happy:morbid 8
ATTRIBUTE beggar:poor, malleable:molded, soldier:fight, exercise:vigorous 8
NON-ATTRIBUTE sound:inaudible, exemplary:criticized, war:tranquility, dull:cunning 8
CASE RELATIONS tailor:suit, farmer:tractor, teach:student, king:crown 8
CAUSE-PURPOSE joke:laughter, fatigue:sleep, gasoline:car, assassin:death 8
SPACE-TIME bookshelf:books, coast:ocean, infancy:cradle, rivet:girder 9
REFERENCE smile:friendliness, person:portrait, recipe:cake, astronomy:stars 6
</table>
<tableCaption confidence="0.993516">
Table 1: The ten categories of semantic relations used in SemEval 2012 Task 2. Each word pair has been taken from a
different subcategory of each major category.
</tableCaption>
<bodyText confidence="0.998234636363637">
across 79 relations were provided by Turkers in
Phase 1. Some of these word pairs are naturally
more representative of the relationship than others.
Therefore, in the second phase, each word pair was
presented to a different set of Turkers for ranking
in the form of MaxDiff (Louviere and Woodworth,
1991) questions. Figure 1 shows an example MaxD-
iff question for the relation 2h PART-WHOLE: Crea-
ture:Possession (“X possesses/owns/has Y”). In each
MaxDiff question, Turkers were simply asked to se-
lect the word pair which was the most illustrative
of the relation and the word pair which was the
least illustrative of the relation. For the example in
Figure 1, most Turkers chose either shrubs:roots or
farmer:crops as the most illustrative of the Crea-
ture:Possession relation, and homeowner:door as
the least illustrative. When Turkers select a pair of
words they are performing a semantic inference that
we wanted to also perform in a computational man-
ner. In this paper we present a method for automat-
ically ranking word pairs according to their related-
ness to a given semantic relation.
</bodyText>
<sectionHeader confidence="0.932965" genericHeader="method">
2 Approach for Determining Relational
Similarity
</sectionHeader>
<bodyText confidence="0.968956733333334">
In the vein of previous methods for determining re-
lational similarity (Turney, 2011; Turney, 2008a;
Turney, 2008b; Turney, 2005), we propose two ap-
proaches using patterns generated from the contexts
in which the word pairs occur. Our corpus consists
of 8.4 million documents from Gigaword (Parker
and Consortium, 2009) and over 4 million articles
from Wikipedia. For each word pair, &lt;W1&gt;, &lt;W2&gt;
provided by Turkers in Phase 1, as well as the three
relation examples, we collected all contexts which
matched the schema:
“ [0 or more non-content words] &lt;W1&gt; [0 to 7
words] &lt;W2&gt; [0 or more non-content words]”
We also include those contexts where W1 and W2
are swapped. The window size of seven words was
determined based on experiments on the training set
of ten relations provided by the task organizers. For
the non-content words, we considered closed class
words such as determiners (the, who, every), prepo-
sitions (in, on, instead of), and conjunctions (and,
but). Members of these classes were collected from
their corresponding Wikipedia pages. Below we
provide a sample of the 7,022 contexts found for the
word pair love:hate:
“they &lt;W1&gt; to &lt;W2&gt; it”
“&lt;W1&gt; and &lt;W2&gt; the most. by”
“between &lt;W1&gt; &amp; &lt;W2&gt;”
“&lt;W1&gt; you then i &lt;W2&gt; you and”
We restrict the context before and after the word pair
to non-content words in order to match longer con-
texts without introducing exponential growth in the
number of patterns and the consequential sparsity
problems. These contexts are directly used as pat-
terns. To generate additional patterns we have one
method for shortening contexts and two methods for
generating patterns from contexts.
Any contexts which contain words before &lt;W1&gt;
or after &lt;W1&gt; are used to create additional shorter
contexts by successively removing leading and trail-
ing words. For example, the context “as much
&lt;W1&gt; in the &lt;W2&gt; as his” for the word pair
money:bank would generate the following shortened
contexts:
“much &lt;W1&gt; in the &lt;W2&gt; as his”
“&lt;W1&gt; in the &lt;W2&gt; as his”
</bodyText>
<page confidence="0.994653">
414
</page>
<bodyText confidence="0.985627370370371">
“as much &lt;W1&gt; in the &lt;W2&gt;” as
“as much &lt;W1&gt; in the &lt;W2&gt;”
“much &lt;W1&gt; in the &lt;W2&gt; as”
“&lt;W1&gt; in the &lt;W2&gt; as”
“&lt;W1&gt; in the &lt;W2&gt;”
These shortened contexts are used, along with the
original context, to generate patterns.
The first pattern generation method replaces each
word between &lt;W1&gt; and &lt;W2&gt; with a wildcard ([&amp;quot;
]+ means one or more non-space characters). For ex-
ample:
“as much &lt;W1&gt; [&amp;quot; ]+ the &lt;W2&gt; as”
“as much &lt;W1&gt; in [&amp;quot; ]+ &lt;W2&gt; as”
The second pattern generation technique allows for
a single word to be matched in the context between
the arguments &lt;W1&gt; and &lt;W2&gt;, along with arbi-
trary matching of other tokens in the context. For
example, the context for red:stop “the &lt;W1&gt; flag is
flagged to indicate a &lt;W2&gt;” will generate new pat-
terns such as:
“the &lt;W1&gt;.* flag .*&lt;W2&gt;”
“the &lt;W1&gt;.* is .*&lt;W2&gt;”
“the &lt;W1&gt;.* flagged .*&lt;W2&gt;”
“the &lt;W1&gt;.* indicate .*&lt;W2&gt;”
After all patterns have been generated, they are used
by our two approaches to assign relational similarity
scores to word pairs.
</bodyText>
<subsectionHeader confidence="0.934167">
2.1 UTD-NB Approach
</subsectionHeader>
<bodyText confidence="0.999949">
The first of our two approaches, UTD-NB, assigns
weights to patterns which are then used to assign
similarity scores to word pairs. The approach begins
by obtaining all word pairs associated with a rela-
tion. Each relation is associated with a target set (T)
of word pairs from two sources: (i) the three or four
example word pairs provided for each relation, and
(ii) the word pairs provided by Turkers in Phase 1.
We collect all of the contexts for those word pairs to
generate patterns. The UTD-NB approach assumes
that the word pairs provided by Turkers, while noisy,
can be used to characterize the relation. As an exam-
ple, consider these word pairs provided by Turkers
for the relation 8a (Cause:Effect) illness:discomfort,
fire:burns, accident:damage. A pattern which ex-
tracts these word pairs is: “&lt;W1&gt; that caused [&amp;quot; ]+
&lt;W2&gt;”. This pattern is unlikely to match the con-
texts of word pairs from other relations. Therefore,
we use the statistics about how many target word
</bodyText>
<figureCaption confidence="0.6020275">
Figure 2: Probabilistic model for the word pairs extracted
by patterns, for a single relation.
</figureCaption>
<bodyText confidence="0.993948447368421">
pairs a pattern extracts versus how many non-target
pairs a pattern extracts to assign a weight to the pat-
tern. A pattern which matches many of the word
pairs from the target relation and few (or none) of the
word pairs from other relations is likely to be a good
indicator of that relation. For example, the pattern
P1 for the relation 8a (Cause:Effect): “the &lt;W1&gt;.*
caused .*&lt;W2&gt; to his” matches only three word
pairs: explosion:damage, accident:damage, and in-
jury:pain, all of them belonging to the target rela-
tion. Conversely, the pattern P2: “&lt;W1&gt;.* caus-
ing .*&lt;W2&gt; but” matches five words pairs. How-
ever, only three of them belong to the target relation:
hit:injury, explosion:damage, germs:sickness. The
remaining two: city:people, action:alarm belong to
other relations: .
We use the number of target word pairs extracted,
x, and the total number of word pairs extracted, n,
to calculate T: the probability that a word pair ex-
tracted by the pattern will belong to the target re-
lation. The maximum likelihood estimate for T is
�, however for small values of x this estimate has
�
a high variance and can significantly overestimate
the true value. Therefore, we used the Wilson in-
terval score for determining a lower bound on T at
a 99.9% confidence level. This gives the pattern P1
above with x = 3 and n = 3 a lower bound on T
of 21.7% and P2 with x = 3 and n = 5 a lower
bound on T of 16.6%. We use this lower bound as
the pattern’s weight. These pattern weights are then
combined to score each word pair for the target rela-
tion.
We model the word pairs extracted by the patterns
as a generative process shown in Figure 2. Each pat-
tern, p, is associated with with a precision, T, which
is the probability that a word pair extracted by that
pattern is a member of the target relation. The ob-
</bodyText>
<page confidence="0.996355">
415
</page>
<bodyText confidence="0.999767214285715">
served word pairs extracted by a pattern are denoted
by w. Our model assumes that a word pair extracted
by a pattern may be drawn from one of two distinct
distributions over word pairs: a distribution for the
target relation t, and a background distribution over
word pairs b. The generation of a word pair be-
gins with a binary variable x drawn from a Bernoulli
distribution parametrized by T (the pattern’s preci-
sion), which represents whether a word pair is gen-
erated according to a relation specific distribution, or
a background distribution. More explicitly, if x = 1,
then a word pair w is generated by the target relation
distribution t, and if x = 0, a word pair is generated
by the background distribution b.
We may not yet perform any meaningful infer-
ence because no evidence has been observed to cor-
rectly infer whether the target distribution or the
background distribution generated w. Therefore we
use the pattern weights derived above (based on the
lower bounds on the pattern precisions) as that pat-
tern’s value of T. For estimating the distributions
t� and b, we assume that x is 1 (w is generated by
t) if and only if T ≥ 0.1 and the word pair w be-
longs to the target set of word pairs T. This thresh-
old on T has a filtering effect on the patterns, and
those patterns below the threshold are treated as non-
indicative of the relation. These assumptions allow
us to estimate the parameters for t� and b:
</bodyText>
<equation confidence="0.977048">
� #(w,h) if w E T
P(wl�t) = #(h) (1)
0 if w ∈6 T
b) = #(w, ¬h) + #(w, h)1w0T (2)
Eu #(u, ¬h) + #(u, h)1uoT
</equation>
<bodyText confidence="0.999976636363636">
where #(w, h) is the number of times w was ex-
tracted by a high precision pattern (,r ≥ 10%), and
#(h) is the number of word pairs extracted by a high
precision pattern.
The only remaining hidden variable in the model
is x which we can now estimate using the inferred
distributions for the other variables. We chose to use
the probability of x for a word pair w as the score
by which we rank the word pairs. Furthermore, we
use only the probability of x for the highest ranking
pattern p which extracted w:
</bodyText>
<equation confidence="0.994742333333333">
P(x = 1, w|p)
P(x = 1|p, w) = (3)
P(w|p)
</equation>
<bodyText confidence="0.985788678571429">
where P(x = 1, w|p) = Tp × t(w) and P(w|p) =
P(x = 1, w|p) + P(x = 0, w|p)
This method of scoring word pairs accounts for
how common a word pair is overall. For example
for the relation 4c (CONTRAST: Reverse), the word
pair white:black occurs very commonly in both high
precision patterns and low precision patterns (those
more likely associated with other relations). There-
fore even though the word pair shares its highest
ranking pattern with the pair eat:fast, white:black re-
ceives a score of 0.019 while eat:fast receives a score
of 0.216 because t(white : black) = 0.006 and
b(white : black) = 0.104, while teat : fast) =
0.0016 and b(eat : fast) = 0.0018. However,
if a pattern with 100% precision were to extract
white:black, the pair would appropriately receive a
score of 1.0 despite being much more common in the
background distribution. This is motivated by our
assumption that such a pattern can only extract word
pairs which truly belong to the relation. Another
motivation for scoring word pairs by their highest
ranking pattern is that it does not depend on any
assumption of independence between the patterns
which extract the pairs. For example, the pattern
“&lt;W1&gt; , not &lt;W2&gt; . ” extracts largely the same
word pairs as “&lt;W1&gt; [ˆ ]+ not &lt;W2&gt; .” and thus its
matches should not be taken as additional evidence
about the word pairs.
</bodyText>
<subsectionHeader confidence="0.980364">
2.2 UTD-SVM Approach
</subsectionHeader>
<bodyText confidence="0.999988769230769">
Our second approach uses an SVM-rank (Joachims,
2006) model to rank the word pairs. Each word pair
from a target relation is represented as a binary fea-
ture vector indicating which patterns extracted the
word pair. We train the SVM-rank classifier by as-
signing all word pairs from the target relation rank 2,
and all word pairs from other relations with rank 1.
The SVM model is then trained and used to classify
the word pairs from the target relation. Even though
the model is used to classify the same word pairs it
was trained on, it still provides higher scores to word
pairs more likely to belong to the target relation. We
directly rank the word pairs using these scores.
</bodyText>
<sectionHeader confidence="0.99904" genericHeader="method">
3 Discussion
</sectionHeader>
<bodyText confidence="0.9938985">
The organizers of SemEval 2012 Task 2 viewed re-
lational similarity in two different ways. The first
</bodyText>
<equation confidence="0.559883">
P(w|
</equation>
<page confidence="0.952755">
416
</page>
<table confidence="0.5408005">
Word pair % Most illustrative -
% Least illustrative
</table>
<equation confidence="0.821747230769231">
“freezing:warm” 56.0
“earsplitting:quiet” 36.0
“evil:angelic” 18.0
“ancient:modern” 12.0
“disastrous:peaceful” 6.0
“ecstatic:disgruntled” 2.0
“disgusting:tasty” 0.0
“beautiful:plain” -2.0
“dirty:sterile” -4.0
“wrinkled:smooth” -6.0
“sweet:sour” -20.0
“disgruntled:ecstatic” -32.0
“white:gray” -54.0
</equation>
<tableCaption confidence="0.863079833333333">
Table 2: A sample of the 41 word pairs provided by
Amazon Mechanical Turk participants for the relation 4f
(CONTRAST: Asymmetric Contrary - X and Y are at op-
posite ends of the same scale). The word pairs are ranked
by how illustrative of the relation participants found each
pair to be.
</tableCaption>
<bodyText confidence="0.99963425">
view was that of solving a MaxDiff problem, ques-
tion in which participants are shown a list of four
word pairs and asked to select the most and least
illustrative pairs. The second view of relation simi-
larity considers the task of assigning scores to a ac-
cording to their similarity to the relation of interest.
The first column of Table 2 provides an example of
word pairs that Amazon Turkers said belonged to the
4f: CONTRAST: Asymmetric Contrary relation in
Phase 1, ranked according to how well other Turk-
ers felt they represented the relation. The score in
the second column is calculated as the percentage of
how often Turkers rated a word pair as the most il-
lustrative and how often Turkers rated the word pair
as the least illustrative.
Both of our approaches for determining relation
similarity assign scores directly to the word pairs
collected in Phase 1, with the goal of ranking the
words in the same order that was induced from the
responses by Amazon Mechanical Turkers.
</bodyText>
<subsectionHeader confidence="0.99852">
3.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9998625">
SemEval-2012 Task 2 had two official evaluation
metrics. The first directly measured the accuracy
of automatically choosing the most and least illus-
trative word pairs among a set of four word pairs
taken from responses during Phase 1. The accuracy
of choosing the most illustrative word pair and the
</bodyText>
<table confidence="0.99927675">
Team-Algorithm Spearman MaxDiff
UTD-NB 0.229 39.4
UTD-SVM 0.116 34.7
Duluth-V0 0.050 32.4
Duluth-V1 0.039 31.5
Duluth-V2 0.038 31.1
BUAP 0.014 31.7
Random 0.018 31.2
</table>
<tableCaption confidence="0.984861333333333">
Table 3: Results for all systems participating in SemEval
2012 Task 2 on relational similarity, including a random
baseline.
</tableCaption>
<bodyText confidence="0.999891625">
accuracy of choosing the least illustrative word pair
were calculated separately and averaged to produce
the MaxDiff accuracy.
The second evaluation metric measured the corre-
lation between an automatic ranking of word pairs
for a relation and a ranking induced by the Turkers’
responses to the MaxDiff questions. The word pairs
were given scores equal to the percentage of times
they were chosen by Turkers as the most illustra-
tive example for a relation minus the percentage of
times they were chosen as the least illustrative. Sys-
tems were then evaluated according to their Spear-
man rank correlation with the ranking of word pairs
induced by that score. Spearman correlations range
from -1 for a negative correlation to 1.0 for a perfect
correlation.
</bodyText>
<subsectionHeader confidence="0.752418">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.9999035">
Table 3 shows the results for the six systems which
participated in SemEval-2012 Task 2, along with the
results for a baseline which ranks each word pair
randomly. Our two approaches achieved the best re-
sults on both evaluation metrics. Our UTD-NB ap-
proach achieves much better performance than our
UTD-SVM approach, likely due to the unconven-
tional use of the SVM to classify its own training
data. That said, the results are still significantly
higher than those of other participants. This may
be attributed to our incorporation of better patterns
or our use of a large corpus. It might also be a con-
sequence of our approaches considering all of the
testing word pairs simultaneously.
Table 4 shows the results for each of the ten cat-
egories of relations. The best results are achieved
on SPACE-TIME relations, while the lowest perfor-
mance is on the NON-ATTRIBUTE relations. NON-
</bodyText>
<page confidence="0.99373">
417
</page>
<table confidence="0.999796916666666">
Category Rndm BUAP UTD UMD
NB V0
1 CLASS-INCLUSION 0.057 0.064 0.233 0.045
2 PART-WHOLE 0.012 0.066 0.252 -0.061
3 SIMILAR 0.026 -0.036 0.214 0.183
4 CONTRAST -0.049 0.000 0.206 0.142
5 ATTRIBUTE 0.037 -0.095 0.158 0.044
6 NON-ATTRIBUTE -0.070 0.009 0.098 0.079
7 CASE RELATIONS 0.090 -0.037 0.241 -0.011
8 CAUSE-PURPOSE -0.011 0.114 0.183 0.021
9 SPACE-TIME 0.013 0.035 0.375 0.055
10 REFERENCE 0.142 -0.001 0.346 0.028
</table>
<tableCaption confidence="0.9918935">
Table 4: Spearman correlation results for the best system
from each team, across all ten categories of relations.
</tableCaption>
<bodyText confidence="0.999912041666667">
ATTRIBUTE relations associate objects and actions
with an atypical attribute (harmony:discordant, im-
mortal:death, recluse:socialize). Because the pairs
of words associated with these relation are not typ-
ically associated together, our approach likely per-
forms poorly on these relations because our ap-
proach is based on finding the pairs of words to-
gether in a large corpus.
An interesting consequence of the 10% precision
threshold used in the UTD-NB approach is that 24
relations had no patterns exceeding the threshold
and therefore produced zeroes as scores for all word
pairs. However, word pairs which never occurred
within seven tokens of each other in our corpus re-
ceived a negative score and were ranked lower. Such
rankings tend to produce Spearman scores around
0.0. Our lowest Spearman score was -0.068, while
other teams had low scores of -0.344 and -0.266,
both occurring on relations for which UTD-NB pro-
duced no positive word pair scores. There are two
lessons to be learned from this result: (i) the UTD-
NB approach does a good job of recognizing when
it cannot rank word pairs, and (ii) such relations are
likely difficult and worth further investigation.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999988214285714">
We described the UTD approaches to determining
relation similarity using lexical patterns from a large
corpus. Combined with a probabilistic model for
word pair extraction by those patterns, we were able
to achieve the highest performance at the SemEval
2012 Task 2. Our results showed the approach
significantly outperformed a model which used an
SVM-rank model used to classify its own training
set. The approach also performed well across a wide
range of relation types and argument classes which
included nouns, adjectives, verbs, and adverbs. This
implies that the approaches presented in this pa-
per could be successfully applied to other domains
which involve semantic relations.
</bodyText>
<sectionHeader confidence="0.99639" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99972352173913">
Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson.
1991. Cognitive and psychometric analysis of analog-
ical problem solving. Recent research in psychology.
Springer-Verlag Publishing.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2009.
Classification of semantic relations between nominals.
Language Resources and Evaluation, 43(2):105–121.
Thorsten Joachims. 2006. Training linear SVMs in linear
time. In Proceedings of the 12th ACM SIGKDD inter-
national conference KDD ’06, page 217, New York,
New York, USA, August. ACM Press.
David A. Jurgens, Saif M. Mohammad, Peter D. Turney,
and Keith J. Holyoak. 2012. SemEval-2012 Task 2:
Measuring Degrees of Relational Similarity. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012).
Christopher S G Khoo and Jin-cheon Na. 2006. Seman-
tic relations in information science. Annual Review of
Information Science and Technology, 40(1):157–228.
Jordan J Louviere and G G Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Robert Parker and Linguistic Data Consortium. 2009.
English gigawordfourth edition. Linguistic Data Con-
sortium.
Barbara Rosario and Marti A. Hearst. 2004. Classifying
semantic relations in bioscience texts. In Proceedings
of the AACL ’04, pages 430–es, July.
Larry M. Stephens and Yufeng F. Chen. 1996. Principles
for organizing semantic relations in large knowledge
bases. IEEE Transactions on Knowledge and Data
Engineering, 8(3):492–496, June.
Peter D. Turney. 2005. Measuring Semantic Similarity
by Latent Relational Analysis. In International Joint
Conference On Artificial Intelligence, volume 19.
Peter D. Turney. 2008a. A Uniform Approach to Analo-
gies, Synonyms, Antonyms, and Associations. In Pro-
ceedings of COLING ’08, August.
Peter D. Turney. 2008b. The Latent Relation Mapping
Engine: Algorithm and Experiments. Journal of Arti-
ficial Intelligence Research, 33:615–655.
Peter D. Turney. 2011. Analogy perception applied
to seven tests of word comprehension. Journal of
Experimental &amp; Theoretical Artificial Intelligence,
23(3):343–362, July.
</reference>
<page confidence="0.992909">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870476">
<title confidence="0.999861">UTD: Determining Relational Similarity Using Lexical Patterns</title>
<author confidence="0.999364">Bryan Rink</author>
<author confidence="0.999364">Sanda Harabagiu</author>
<affiliation confidence="0.999607">University of Texas at Dallas</affiliation>
<address confidence="0.9906675">P.O. Box 830688; MS EC31 Richardson, TX, 75083-0688, USA</address>
<abstract confidence="0.993366666666667">In this paper we present our approach for assigning degrees of relational similarity to pairs of words in the SemEval-2012 Task 2. To measure relational similarity we employed lexical patterns that can match against word pairs within a large corpus of 12 million documents. Patterns are weighted by obtaining statistically estimated lower bounds on their precision for extracting word pairs from a given relation. Finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest. This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Isaac I Bejar</author>
<author>Roger Chaffin</author>
<author>Susan E Embretson</author>
</authors>
<title>Cognitive and psychometric analysis of analogical problem solving. Recent research in psychology.</title>
<date>1991</date>
<publisher>Springer-Verlag Publishing.</publisher>
<contexts>
<context position="1187" citStr="Bejar et al., 1991" startWordPosition="183" endWordPosition="186">their precision for extracting word pairs from a given relation. Finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest. This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%. 1 Introduction Considerable prior research has examined and elaborated upon a wide variety of semantic relations between concepts along with techniques for automatically discovering pairs of concepts for which a relation holds (Bejar et al., 1991; Stephens and Chen, 1996; Rosario and Hearst, 2004; Khoo and Na, 2006; Girju et al., 2009). However, most previous work has considered membership assignment for a semantic relation as a binary property. In this paper we discuss an approach which assigns a degree of membership to a pair of concepts for a given relation. For example, for the semantic relation CLASS-INCLUSION (Taxonomic), the concept pairs weapon:spear and bird:robin are stronger members Consider the following word pairs: millionaire:money, author:copyright, robin:nest. These X:Y pairs share a relation “X R Y”. Now consider the </context>
<context position="2634" citStr="Bejar et al. (1991)" startWordPosition="418" endWordPosition="421">rs is the LEAST illustrative example of the same relation “X R Y”? Figure 1: Example Phase 2 MaxDiff question for the relation 2h PART-WHOLE: Creature:Possession. of the relationship than hair:brown, because brown may describe many things other than hair, and brown is also used much less frequently as a noun than the words in the first two word pairs. Task 2 of SemEval 2012 (Jurgens et al., 2012) was designed to evaluate the effectiveness of automatic approaches for determining the similarity of a pair of concepts to a specific semantic relation. The task focused on 79 semantic relations from Bejar et al. (1991) which broadly fall into the ten categories enumerated in Table 1. The data for the task was collected in two phases using Amazon Mechanical Turk 1. During Phase 1, Turkers were asked to provide pairs of words which fit a relation template, such as “X possesses/owns/has Y”. Turkers provided word pairs such as expert:experience, mall:shops, letters:words, and doctor:degree. A total of 3,218 word pairs 1http://www.mturk.com/mturk/ 413 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 413–418, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Lingui</context>
</contexts>
<marker>Bejar, Chaffin, Embretson, 1991</marker>
<rawString>Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson. 1991. Cognitive and psychometric analysis of analogical problem solving. Recent research in psychology. Springer-Verlag Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Classification of semantic relations between nominals.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="1278" citStr="Girju et al., 2009" startWordPosition="199" endWordPosition="202">anked based on a model predicting the probability that they belong to the relation of interest. This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%. 1 Introduction Considerable prior research has examined and elaborated upon a wide variety of semantic relations between concepts along with techniques for automatically discovering pairs of concepts for which a relation holds (Bejar et al., 1991; Stephens and Chen, 1996; Rosario and Hearst, 2004; Khoo and Na, 2006; Girju et al., 2009). However, most previous work has considered membership assignment for a semantic relation as a binary property. In this paper we discuss an approach which assigns a degree of membership to a pair of concepts for a given relation. For example, for the semantic relation CLASS-INCLUSION (Taxonomic), the concept pairs weapon:spear and bird:robin are stronger members Consider the following word pairs: millionaire:money, author:copyright, robin:nest. These X:Y pairs share a relation “X R Y”. Now consider the following word pairs: (1) teacher:students (2) farmer:crops (3) homeowner:door (4) shrubs:r</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2009</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2009. Classification of semantic relations between nominals. Language Resources and Evaluation, 43(2):105–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference KDD ’06,</booktitle>
<pages>217</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA,</location>
<contexts>
<context position="14658" citStr="Joachims, 2006" startWordPosition="2447" endWordPosition="2448">ore common in the background distribution. This is motivated by our assumption that such a pattern can only extract word pairs which truly belong to the relation. Another motivation for scoring word pairs by their highest ranking pattern is that it does not depend on any assumption of independence between the patterns which extract the pairs. For example, the pattern “&lt;W1&gt; , not &lt;W2&gt; . ” extracts largely the same word pairs as “&lt;W1&gt; [ˆ ]+ not &lt;W2&gt; .” and thus its matches should not be taken as additional evidence about the word pairs. 2.2 UTD-SVM Approach Our second approach uses an SVM-rank (Joachims, 2006) model to rank the word pairs. Each word pair from a target relation is represented as a binary feature vector indicating which patterns extracted the word pair. We train the SVM-rank classifier by assigning all word pairs from the target relation rank 2, and all word pairs from other relations with rank 1. The SVM model is then trained and used to classify the word pairs from the target relation. Even though the model is used to classify the same word pairs it was trained on, it still provides higher scores to word pairs more likely to belong to the target relation. We directly rank the word </context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD international conference KDD ’06, page 217, New York, New York, USA, August. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Jurgens</author>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
<author>Keith J Holyoak</author>
</authors>
<title>SemEval-2012 Task 2: Measuring Degrees of Relational Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="2414" citStr="Jurgens et al., 2012" startWordPosition="383" endWordPosition="386">ing word pairs: (1) teacher:students (2) farmer:crops (3) homeowner:door (4) shrubs:roots Which of the numbered word pairs is the MOST illustrative example of the same relation “X R Y”? Which of the above numbered word pairs is the LEAST illustrative example of the same relation “X R Y”? Figure 1: Example Phase 2 MaxDiff question for the relation 2h PART-WHOLE: Creature:Possession. of the relationship than hair:brown, because brown may describe many things other than hair, and brown is also used much less frequently as a noun than the words in the first two word pairs. Task 2 of SemEval 2012 (Jurgens et al., 2012) was designed to evaluate the effectiveness of automatic approaches for determining the similarity of a pair of concepts to a specific semantic relation. The task focused on 79 semantic relations from Bejar et al. (1991) which broadly fall into the ten categories enumerated in Table 1. The data for the task was collected in two phases using Amazon Mechanical Turk 1. During Phase 1, Turkers were asked to provide pairs of words which fit a relation template, such as “X possesses/owns/has Y”. Turkers provided word pairs such as expert:experience, mall:shops, letters:words, and doctor:degree. A to</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>David A. Jurgens, Saif M. Mohammad, Peter D. Turney, and Keith J. Holyoak. 2012. SemEval-2012 Task 2: Measuring Degrees of Relational Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher S G Khoo</author>
<author>Jin-cheon Na</author>
</authors>
<date>2006</date>
<booktitle>Semantic relations in information science. Annual Review of Information Science and Technology,</booktitle>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="1257" citStr="Khoo and Na, 2006" startWordPosition="195" endWordPosition="198">y, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest. This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%. 1 Introduction Considerable prior research has examined and elaborated upon a wide variety of semantic relations between concepts along with techniques for automatically discovering pairs of concepts for which a relation holds (Bejar et al., 1991; Stephens and Chen, 1996; Rosario and Hearst, 2004; Khoo and Na, 2006; Girju et al., 2009). However, most previous work has considered membership assignment for a semantic relation as a binary property. In this paper we discuss an approach which assigns a degree of membership to a pair of concepts for a given relation. For example, for the semantic relation CLASS-INCLUSION (Taxonomic), the concept pairs weapon:spear and bird:robin are stronger members Consider the following word pairs: millionaire:money, author:copyright, robin:nest. These X:Y pairs share a relation “X R Y”. Now consider the following word pairs: (1) teacher:students (2) farmer:crops (3) homeow</context>
</contexts>
<marker>Khoo, Na, 2006</marker>
<rawString>Christopher S G Khoo and Jin-cheon Na. 2006. Semantic relations in information science. Annual Review of Information Science and Technology, 40(1):157–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan J Louviere</author>
<author>G G Woodworth</author>
</authors>
<title>Bestworst scaling: A model for the largest difference judgments.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>University of Alberta.</institution>
<contexts>
<context position="4467" citStr="Louviere and Woodworth, 1991" startWordPosition="652" endWordPosition="655">sin:death 8 SPACE-TIME bookshelf:books, coast:ocean, infancy:cradle, rivet:girder 9 REFERENCE smile:friendliness, person:portrait, recipe:cake, astronomy:stars 6 Table 1: The ten categories of semantic relations used in SemEval 2012 Task 2. Each word pair has been taken from a different subcategory of each major category. across 79 relations were provided by Turkers in Phase 1. Some of these word pairs are naturally more representative of the relationship than others. Therefore, in the second phase, each word pair was presented to a different set of Turkers for ranking in the form of MaxDiff (Louviere and Woodworth, 1991) questions. Figure 1 shows an example MaxDiff question for the relation 2h PART-WHOLE: Creature:Possession (“X possesses/owns/has Y”). In each MaxDiff question, Turkers were simply asked to select the word pair which was the most illustrative of the relation and the word pair which was the least illustrative of the relation. For the example in Figure 1, most Turkers chose either shrubs:roots or farmer:crops as the most illustrative of the Creature:Possession relation, and homeowner:door as the least illustrative. When Turkers select a pair of words they are performing a semantic inference that</context>
</contexts>
<marker>Louviere, Woodworth, 1991</marker>
<rawString>Jordan J Louviere and G G Woodworth. 1991. Bestworst scaling: A model for the largest difference judgments. Technical report, University of Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>Linguistic Data Consortium</author>
</authors>
<title>English gigawordfourth edition. Linguistic Data Consortium.</title>
<date>2009</date>
<contexts>
<context position="5620" citStr="Parker and Consortium, 2009" startWordPosition="835" endWordPosition="838">urkers select a pair of words they are performing a semantic inference that we wanted to also perform in a computational manner. In this paper we present a method for automatically ranking word pairs according to their relatedness to a given semantic relation. 2 Approach for Determining Relational Similarity In the vein of previous methods for determining relational similarity (Turney, 2011; Turney, 2008a; Turney, 2008b; Turney, 2005), we propose two approaches using patterns generated from the contexts in which the word pairs occur. Our corpus consists of 8.4 million documents from Gigaword (Parker and Consortium, 2009) and over 4 million articles from Wikipedia. For each word pair, &lt;W1&gt;, &lt;W2&gt; provided by Turkers in Phase 1, as well as the three relation examples, we collected all contexts which matched the schema: “ [0 or more non-content words] &lt;W1&gt; [0 to 7 words] &lt;W2&gt; [0 or more non-content words]” We also include those contexts where W1 and W2 are swapped. The window size of seven words was determined based on experiments on the training set of ten relations provided by the task organizers. For the non-content words, we considered closed class words such as determiners (the, who, every), prepositions (in</context>
</contexts>
<marker>Parker, Consortium, 2009</marker>
<rawString>Robert Parker and Linguistic Data Consortium. 2009. English gigawordfourth edition. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti A Hearst</author>
</authors>
<title>Classifying semantic relations in bioscience texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the AACL ’04,</booktitle>
<pages>430</pages>
<contexts>
<context position="1238" citStr="Rosario and Hearst, 2004" startWordPosition="191" endWordPosition="194">m a given relation. Finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest. This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%. 1 Introduction Considerable prior research has examined and elaborated upon a wide variety of semantic relations between concepts along with techniques for automatically discovering pairs of concepts for which a relation holds (Bejar et al., 1991; Stephens and Chen, 1996; Rosario and Hearst, 2004; Khoo and Na, 2006; Girju et al., 2009). However, most previous work has considered membership assignment for a semantic relation as a binary property. In this paper we discuss an approach which assigns a degree of membership to a pair of concepts for a given relation. For example, for the semantic relation CLASS-INCLUSION (Taxonomic), the concept pairs weapon:spear and bird:robin are stronger members Consider the following word pairs: millionaire:money, author:copyright, robin:nest. These X:Y pairs share a relation “X R Y”. Now consider the following word pairs: (1) teacher:students (2) farm</context>
</contexts>
<marker>Rosario, Hearst, 2004</marker>
<rawString>Barbara Rosario and Marti A. Hearst. 2004. Classifying semantic relations in bioscience texts. In Proceedings of the AACL ’04, pages 430–es, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry M Stephens</author>
<author>Yufeng F Chen</author>
</authors>
<title>Principles for organizing semantic relations in large knowledge bases.</title>
<date>1996</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="1212" citStr="Stephens and Chen, 1996" startWordPosition="187" endWordPosition="190">extracting word pairs from a given relation. Finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest. This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%. 1 Introduction Considerable prior research has examined and elaborated upon a wide variety of semantic relations between concepts along with techniques for automatically discovering pairs of concepts for which a relation holds (Bejar et al., 1991; Stephens and Chen, 1996; Rosario and Hearst, 2004; Khoo and Na, 2006; Girju et al., 2009). However, most previous work has considered membership assignment for a semantic relation as a binary property. In this paper we discuss an approach which assigns a degree of membership to a pair of concepts for a given relation. For example, for the semantic relation CLASS-INCLUSION (Taxonomic), the concept pairs weapon:spear and bird:robin are stronger members Consider the following word pairs: millionaire:money, author:copyright, robin:nest. These X:Y pairs share a relation “X R Y”. Now consider the following word pairs: (1)</context>
</contexts>
<marker>Stephens, Chen, 1996</marker>
<rawString>Larry M. Stephens and Yufeng F. Chen. 1996. Principles for organizing semantic relations in large knowledge bases. IEEE Transactions on Knowledge and Data Engineering, 8(3):492–496, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Measuring Semantic Similarity by Latent Relational Analysis.</title>
<date>2005</date>
<booktitle>In International Joint Conference On Artificial Intelligence,</booktitle>
<volume>19</volume>
<contexts>
<context position="5430" citStr="Turney, 2005" startWordPosition="807" endWordPosition="808">re 1, most Turkers chose either shrubs:roots or farmer:crops as the most illustrative of the Creature:Possession relation, and homeowner:door as the least illustrative. When Turkers select a pair of words they are performing a semantic inference that we wanted to also perform in a computational manner. In this paper we present a method for automatically ranking word pairs according to their relatedness to a given semantic relation. 2 Approach for Determining Relational Similarity In the vein of previous methods for determining relational similarity (Turney, 2011; Turney, 2008a; Turney, 2008b; Turney, 2005), we propose two approaches using patterns generated from the contexts in which the word pairs occur. Our corpus consists of 8.4 million documents from Gigaword (Parker and Consortium, 2009) and over 4 million articles from Wikipedia. For each word pair, &lt;W1&gt;, &lt;W2&gt; provided by Turkers in Phase 1, as well as the three relation examples, we collected all contexts which matched the schema: “ [0 or more non-content words] &lt;W1&gt; [0 to 7 words] &lt;W2&gt; [0 or more non-content words]” We also include those contexts where W1 and W2 are swapped. The window size of seven words was determined based on experim</context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Peter D. Turney. 2005. Measuring Semantic Similarity by Latent Relational Analysis. In International Joint Conference On Artificial Intelligence, volume 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING ’08,</booktitle>
<contexts>
<context position="5399" citStr="Turney, 2008" startWordPosition="803" endWordPosition="804">ation. For the example in Figure 1, most Turkers chose either shrubs:roots or farmer:crops as the most illustrative of the Creature:Possession relation, and homeowner:door as the least illustrative. When Turkers select a pair of words they are performing a semantic inference that we wanted to also perform in a computational manner. In this paper we present a method for automatically ranking word pairs according to their relatedness to a given semantic relation. 2 Approach for Determining Relational Similarity In the vein of previous methods for determining relational similarity (Turney, 2011; Turney, 2008a; Turney, 2008b; Turney, 2005), we propose two approaches using patterns generated from the contexts in which the word pairs occur. Our corpus consists of 8.4 million documents from Gigaword (Parker and Consortium, 2009) and over 4 million articles from Wikipedia. For each word pair, &lt;W1&gt;, &lt;W2&gt; provided by Turkers in Phase 1, as well as the three relation examples, we collected all contexts which matched the schema: “ [0 or more non-content words] &lt;W1&gt; [0 to 7 words] &lt;W2&gt; [0 or more non-content words]” We also include those contexts where W1 and W2 are swapped. The window size of seven words </context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008a. A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations. In Proceedings of COLING ’08, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>The Latent Relation Mapping Engine: Algorithm and Experiments.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>33--615</pages>
<contexts>
<context position="5399" citStr="Turney, 2008" startWordPosition="803" endWordPosition="804">ation. For the example in Figure 1, most Turkers chose either shrubs:roots or farmer:crops as the most illustrative of the Creature:Possession relation, and homeowner:door as the least illustrative. When Turkers select a pair of words they are performing a semantic inference that we wanted to also perform in a computational manner. In this paper we present a method for automatically ranking word pairs according to their relatedness to a given semantic relation. 2 Approach for Determining Relational Similarity In the vein of previous methods for determining relational similarity (Turney, 2011; Turney, 2008a; Turney, 2008b; Turney, 2005), we propose two approaches using patterns generated from the contexts in which the word pairs occur. Our corpus consists of 8.4 million documents from Gigaword (Parker and Consortium, 2009) and over 4 million articles from Wikipedia. For each word pair, &lt;W1&gt;, &lt;W2&gt; provided by Turkers in Phase 1, as well as the three relation examples, we collected all contexts which matched the schema: “ [0 or more non-content words] &lt;W1&gt; [0 to 7 words] &lt;W2&gt; [0 or more non-content words]” We also include those contexts where W1 and W2 are swapped. The window size of seven words </context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008b. The Latent Relation Mapping Engine: Algorithm and Experiments. Journal of Artificial Intelligence Research, 33:615–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Analogy perception applied to seven tests of word comprehension.</title>
<date>2011</date>
<journal>Journal of Experimental &amp; Theoretical Artificial Intelligence,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="5385" citStr="Turney, 2011" startWordPosition="801" endWordPosition="802">ive of the relation. For the example in Figure 1, most Turkers chose either shrubs:roots or farmer:crops as the most illustrative of the Creature:Possession relation, and homeowner:door as the least illustrative. When Turkers select a pair of words they are performing a semantic inference that we wanted to also perform in a computational manner. In this paper we present a method for automatically ranking word pairs according to their relatedness to a given semantic relation. 2 Approach for Determining Relational Similarity In the vein of previous methods for determining relational similarity (Turney, 2011; Turney, 2008a; Turney, 2008b; Turney, 2005), we propose two approaches using patterns generated from the contexts in which the word pairs occur. Our corpus consists of 8.4 million documents from Gigaword (Parker and Consortium, 2009) and over 4 million articles from Wikipedia. For each word pair, &lt;W1&gt;, &lt;W2&gt; provided by Turkers in Phase 1, as well as the three relation examples, we collected all contexts which matched the schema: “ [0 or more non-content words] &lt;W1&gt; [0 to 7 words] &lt;W2&gt; [0 or more non-content words]” We also include those contexts where W1 and W2 are swapped. The window size o</context>
</contexts>
<marker>Turney, 2011</marker>
<rawString>Peter D. Turney. 2011. Analogy perception applied to seven tests of word comprehension. Journal of Experimental &amp; Theoretical Artificial Intelligence, 23(3):343–362, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>