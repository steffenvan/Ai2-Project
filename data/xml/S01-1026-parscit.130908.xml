<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.062414">
<title confidence="0.942262166666667">
Use of Machine Readable Dictionaries for Word-Sense
Disambiguation in SENSEVAL-2
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
</title>
<email confidence="0.962211">
ken@clres.corn
</email>
<sectionHeader confidence="0.950676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998350423076923">
CL Research&apos;s word-sense disambiguation
(WSD) system is part of the DIMAP dictionary
software, designed to use any full dictionary as
the basis for unsupervised disambiguation.
Official SENSEVAL-2 results were generated
using WordNet, and separately using the New
Oxford Dictionary of English (NODE). The
disambiguation functionality exploits whatever
information is made available by the lexical
database. Special routines examined multiword
units and contextual clues (both collocations,
definition and example content words, and
subject matter analyses); syntactic constraints
have not yet been employed. The official coarse-
grained precision was 0.367 for the lexical
sample task and 0.460 for the all-words task
(these are actually recall, with actual precision of
0.390 and 0.506 for the two tasks). NODE
definitions were automatically mapped into
WordNet, with precision of 0.405 and 0.418 on
75% and 70% mapping for the lexical sample
and all-words tasks, respectively, comparable to
WordNet. Bug fixes and implementation of
incomplete routines have increased the precision
for the lexical sample to 0.429 (with many
improvements still likely).
</bodyText>
<sectionHeader confidence="0.797078" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.99876353125">
CL Research&apos;s participation in SENSEVAL-2 was
designed to (1) extend WSD techniques from
SENSEVAL-1 (Litkowski, 2000), (2) generalize
WSD mechanisms to rely on a full dictionary rather
than a small set of entries where individual crafting
might intrude, and (3) investigate WSD using one
dictionary mapped into another (WordNet). Results
indicate positive achievements for each of these
goals. Time constraints precluded a complete
assessment of the upper limits that can be achieved.
In particular, although the general architecture from
SENSEVAL-1 was retained, several specific WSD
routines were not reimplemented. Incomplete testing,
debugging, and implementation of new routines
significantly affected the official results. Several of
these problems are investigated more fully below.
CL Research&apos;s WSD functionality is implemented in
DIMAP&apos;, designed primarily for creation and
maintenance of lexicons for natural language
processing. In particular, DIMAP is designed to
make machine-readable dictionaries (MRDs)
tractable and to create semantic networks (similar to
WordNet (F ellb aum, 1998) and MindNet
(Richardson, 1997)) automatically by analyzing and
parsing definitions. Section 1 describes the
dictionary preparation techniques for WordNet and
NODE (The New Oxford Dictionary of English,
1998), as well as the mapping from NODE to
WordNet. Section 2 describes the WSD techniques
used in SENSEVAL-2. Section 3 describes the
SENSEVAL-2 results and section 4 discusses these
results..
</bodyText>
<sectionHeader confidence="0.839917" genericHeader="method">
1 Dictionary Preparation
</sectionHeader>
<bodyText confidence="0.997539363636364">
DIMAP can disambiguate any text against WordNet
or any other dictionary converted to DIMAP, with a
special emphasis on corpus instances for specific
lemmas. The dictionaries used for disambiguation
operate in the background (as distinguished from the
foreground development and maintenance of a
dictionary), with rapid btree lookup to access and
examine the characteristics of multiple senses of a
word after a sentence has been parsed. DIMAP
allows multiple senses for each entry, with fields for
the definitions, usage notes, hypemyms, hyponyms,
</bodyText>
<footnote confidence="0.9896655">
&apos;Dictionary MAintenance Programs, available from CL
Research at http://wvvw.clres.com.
</footnote>
<page confidence="0.998587">
107
</page>
<bodyText confidence="0.999895781609196">
arbitrary other semantic relations, and feature
structures containing arbitrary information.
WordNet is already integrated in DIMAP in several
ways, but for SENSEVAL-2, WordNet was entirely
converted to alphabetic format for use as the
disambiguation dictionary. In this conversion, all
WordNet information (e.g., verb frames and glosses)
and relations are retained. Glosses are analyzed into
definition, examples, usage or subject labels, and
usage notes (e.g., &amp;quot;used with &apos;of&amp;quot;). Verb frames are
used to build collocation patterns, typical subjects
and objects, and grammatical characterizations (e.g.,
transitivity). WordNet file and sense numbers are
converted into a unique identifier for each sense.
A separate &amp;quot;phrase&amp;quot; dictionary was constructed from
all noun and verb multiword units (IvIWUs), using
WordNet&apos;s sense index file. For nouns, an entry was
created for the last word (i.e., the head), with the first
word(s) acting as a &amp;quot;hynonymic&amp;quot; indicator; an entry
was also created for the first word, with the
following word(s) acting as a collocation pattern
(e.g., &amp;quot;work of art&amp;quot; is a hyponym of art and a
collocation pattern under work, written &amp;quot;— of art&amp;quot;).
For verbs, an entry was created for the first word,
with a collocation pattern (e.g., &amp;quot;keep an eye on&amp;quot; is
entered as a collocation pattern &amp;quot;— an eye on&amp;quot; under
keep). In disambiguation, this dictionary was
examined first for a match, with the full phrase then
used to identify the sense inventory rather than a
single word.
NODE was prepared in a similar manner, with
several additions. A conversion program transformed
the MRD files into various fields in DIMAP, the
notable difference being the much richer and more
formal structure (e.g., lexical preferences, grammar
fields, and subsensing). Conversion also
considerably expanded the number of entries by
making headwords of all variant forms (fully
duplicating the other lexical information of the root
form) and phrases run on to single lemma entries.
E.g., &amp;quot;(as) happy as a sandboy (or Larry or a
clam&amp;quot; under happy was converted into six
headwords (based on the alternatives indicated by the
parentheses), as well as a collocation pattern for a
sense under happy, written &amp;quot;(as1?) — as (a sandboy
Larry I a clam)&amp;quot;, with the tilde marking the target
word.
NODE was then subjected to definition processing
and parsing. Definition processing consists of further
expansion of the print dictionary: (1) grabbing the
definitions of cross-references and (2) assigning
parts of speech to phrases based on analysis of their
definitions. Definition parsing puts the definition
into a sentence frame appropriate to the part of
speech, making use of typical subjects, objects, and
modificands. The sentence parse tree was then
analyzed to extract various semantic relations,
including the superordinate or hypernym, holonyms,
meronyms, satellites, telic roles, and frame elements.
After parsing was completed, a phrase dictionary
was also created for NODE.2
The SENSEVAL tasks were run separately against
the WordNet and NODE sense inventories, with the
WordNet results submitted. To investigate the
viability of mapping for WSD, subdictionaries were
created for each of the lexical sample words and for
each of the all-words texts. For the lexical sample
words, the subdictionaries consisted of the main
word and all entries identifiable from the phrase
dictionary for that word. (For bar, in NODE, there
were 13 entries where bar was the first word in an
MWU and 50 entries where it was the head noun; for
begin, there was only one entry.) For the all-words
texts, a list was made of all the task words to be
disambiguated (including some phrases) and a
subdictionary constructed from this list. For both
tasks, the creation of these subdictionaries was fully
automatic; no hand manipulation was involved.
The NODE dictionaries were then mapped into the
WordNet dictionaries (see Litkowski, 1999), using
overlap among words and semantic relations. The 73
dictionaries for the lexical sample words gave rise to
1372 WordNet entries and 1722 NODE entries.&apos;
Only 491 entries were common (i.e., no mappings
were available for the remaining 1231 NODE
entries); 881 entries in WordNet were therefore
inaccessible through NODE. For the entries in
</bodyText>
<footnote confidence="0.675673857142857">
2WordNet definitions were not parsed. In an experiment,
the semantic relations identifiable through parsing were
frequently inconsistent with those already given in
WordNet, so it was decided not to confound the
disambiguation.
3Entries included all parts of speech; disambiguation was
required to identify the part of speech as well.
</footnote>
<page confidence="0.998233">
108
</page>
<bodyText confidence="0.9999415">
common, there was an average of 5.6 senses, of
which only 64% were mappable into WordNet. The
a priori probability of successful mapping into the
appropriate WordNet sense is 0.064, the baseline for
assessing WSD via another dictionary mapped into
the WordNet sense-tagged keys.&apos;
</bodyText>
<sectionHeader confidence="0.829402" genericHeader="method">
2 Disambiguation Techniques
</sectionHeader>
<bodyText confidence="0.999621811320755">
The lexical sample and all-words texts were
modified slightly. Satellite tags were removed and
entity references were converted to an ASCII
character. In the all-words texts, contraction and
quotation mark discontinuities were undone. These
changes made the texts more like normal text
processing conditions.
The texts were next reduced to sentences. For the
lexical sample, a sentence was assumed to consist of
a single line. For the all-words texts, a sentence
splitter identified the sentences, which were next
submitted to the parser. The DIMAP parser produced
a parse tree for each sentence, with constituent
phrases when the sentence was not parsable with the
granunar, allowing the WSD phase to continue.
The first step in the WSD used the part of speech of
the tagged word to select the appropriate sense
inventory. Nouns, verbs, and adjectives were looked
up in the phrase dictionary; if the tagged word was
part of an MWU, the word was changed to the
MWU and the MWIJ&apos;s sense inventory was used
instead.
The dictionary entry for the word was then accessed.
Before evaluating the senses, the topic area of the
context provided by the sentence was &amp;quot;established&amp;quot;
(only for NODE). Subject labels for all senses of all
content words in the context were tallied.
Each sense of the target was then evaluated. Senses
in a different part of speech were dropped from
consideration. The different pieces of information in
the sense were assessed: collocation patterns,
contextual clue words, contextual overlap with
definitions and examples, and topical area matches.
Points were given to each sense and the sense with
the highest score was selected; in case of a tie, the
Note that a mapping from WordNet to NODE is likely
to generate similar mismatch statistics.
first sense in the dictionary was selected.&apos;
Collocation pattern testing (requiring an exact match
with surrounding text) was given the largest number
of points (10), sufficient in general to dominate
sense selection. Contextual clue words (a particle or
preposition) was given a small score (2 points). Each
content word of the context added two points if
present in the sense&apos;s definition or examples, so that
considerable overlap could become quite significant.
For topic testing, a sense having a subject label
matching one of the context topic areas was awarded
one point for each word in the context that had a
similar subject label (e.g., if four words in the
context had a medical subject label, four points
would be awarded if the instant sense also had a
medical label).
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999148266666667">
As shown in Table 1, using WordNet as the
disambiguation dictionary resulted in an overall
precision (and recall) of 0.293 at the fine-grained
level and 0.367 at the coarse-gained level. Since CL
Research did not use the training data in any way,
running the training data also provided another test
of the system. The results are remarkably consistent,
both overall and for each part of speech. Using
NODE as the disambiguation dictionary and
mapping its senses into WordNet senses achieved
comparable levels of precision, although recall was
somewhat lower, as indicated by the difference in the
number of items on which the precision was
calculated. Overall, about 75% of the senses were
mapped into WordNet.
</bodyText>
<table confidence="0.994837">
, Table 2. MU-Words
_
.Run :Items . fine .Coarse
_
WordNet 2473 0.451 0.460
NODE 1727 0.416 0.418
</table>
<bodyText confidence="0.679966666666667">
For the all-words task, the disambiguation results
Several other functions were implemented only in stub
form at the time of the test runs, to evaluate: type
restrictions (e.g., transitivity), presence of accompanying
grammatical constituents (e.g., infinitive phrase or
complements), form restrictions (such as number and
participial), grammatical role (e.g., as a modifier), and
selectional restrictions (such as subject, object,
modificand, and internal arguments).
</bodyText>
<page confidence="0.991426">
109
</page>
<tableCaption confidence="0.806048">
Table 1. Lexical &apos;atuidePrecision
</tableCaption>
<table confidence="0.990026714285714">
Adjectives . &apos;Nouns &apos; Verbs Total
Run Items . Fine- Coarse Items Fine Coarse !terns • :. fine -Coarse &apos; I Fine Coarse
ordNet Test 768 - - 1726 0.33§ - 1834 0.225 0.305 4328 0.293 0.367
0.354 0.354 0.434
ODE Test 420 0.288 0.288 1403 0.402 0.539 1394 0.219 0.305 3217 0.308 0,405
ordNet Training 1533 0.365 0.365 3455 0.334 0.444 3623 0.219 0.299 8611 0.291 0.369
ODE Training 864 0.116 0.116 2848 0.366- 0.483 2567 0.227 0.315 6249 0.276 0.365
</table>
<bodyText confidence="0.999242666666667">
were significantly higher than for the lexical sample,
with a precision (and recall) of 0.460 for the
WordNet coarse-grained level. For NODE, about
70% were mapped into WordNet (indicated by the
reduced number of items), with precision on the
mapped items only slightly less.&apos;
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.997885263157895">
Because of the usual bugs and incomplete
implementation, the official results do not adequately
indicate the potential of our approach. The official
results are actually recall rather than precision, since
an answer was submitted when it shouldn&apos;t have
been, as distinguished from cases where the parser
picked the wrong part of speech or was unable to
select a sense. The actual precision for the lexical
sample task is 0.311 for the fine gain and 0.390 for
the coarse grain, and for the all-words task, 0.496
and 0.506 for fine and coarse grains, respectively.
Minimal debugging and inability to implement
several routines significantly affected the scores.
Examining the reasons for failures in the test runs
and making program fixes has thus far resulted in
increasing precision (and recall) to 0.340 and 0.429
for the lexical sample. Further improvements are
likely, although it is not clear whether the
SENSEVAL-1 precision of 0.67 is achievable using
only the information available in WordNet.
It is more likely that using NODE will achieve better
results. Improvements in automatic mapping have
now reached 90% mapping; it is also relatively easy
to make manual adjustments to the maps to achieve
even higher performance from the lexicographically-
based lexical resource. Since the automatic mapping
is inaccurate to an unknown degree (perhaps 25-
30%), improving the maps will achieve better results
oFor both tasks, NODE senses were identified for all
words, but could be mapped only for the percentages
given.
using NODE via WordNet, rather than WordNet
alone. Using NODE also provides a much richer set
of data upon which to make improvements in WSD.
Finally, since NODE is lexicographically-based and
with an arguably better sense inventory, we are
confident that our WSD would have scored much
higher if the taggers had used this inventory.
</bodyText>
<sectionHeader confidence="0.847291" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999957333333333">
Given the very preliminary implementation of the
disambiguation routines and lack of adequate
debugging, the results indicate that using MRDs (and
even mapping from one into another) shows
considerable potential for unsupervised and general
word-sense disambiguation.
</bodyText>
<sectionHeader confidence="0.983383" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99810975">
I wish to thank Oxford University Press for making
NODE available (Patrick Hanks and Rob Scriven)
and for many useful discussions (Glynnis Chantrell
and Judy Pearsall).
</bodyText>
<sectionHeader confidence="0.995155" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662214285714">
Fellbaum, C. (1998). WordNet: An electronic lexical
database. Cambridge, Massachusetts: MIT Press.
Litkowski, K. C. (1999, 21-22 June). Towards a Meaning-
Full Comparison of Lexical Resources. Association for
Computational Linguistics Special Interest Group on the
Lexicon Workshop. College Park, MD.
Litkowski, K. C. (2000). SENSEVAL: The CL Research
Experience. Computers and the Humanities, 34(1-2),
153-158.
The New Oxford Dictionary of English (J. Pearsall, Ed.).
(1998). Oxford: Clarendon Press.
Richardson, S. D. (1997). Determining similarity and
inferring relations in a lexical knowledge base [Diss],
New York, NY: The City University of New York.
</reference>
<page confidence="0.998394">
110
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.687663">
<title confidence="0.9903775">Machine Readable Dictionaries for Disambiguation in SENSEVAL-2</title>
<author confidence="0.992613">C Kenneth</author>
<affiliation confidence="0.823867">CL</affiliation>
<address confidence="0.967483">9208 Gue Damascus, MD</address>
<email confidence="0.925866">ken@clres.corn</email>
<abstract confidence="0.998002703703703">CL Research&apos;s word-sense disambiguation (WSD) system is part of the DIMAP dictionary software, designed to use any full dictionary as the basis for unsupervised disambiguation. Official SENSEVAL-2 results were generated using WordNet, and separately using the New Oxford Dictionary of English (NODE). The disambiguation functionality exploits whatever information is made available by the lexical database. Special routines examined multiword units and contextual clues (both collocations, definition and example content words, and subject matter analyses); syntactic constraints have not yet been employed. The official coarsegrained precision was 0.367 for the lexical sample task and 0.460 for the all-words task (these are actually recall, with actual precision of 0.390 and 0.506 for the two tasks). NODE definitions were automatically mapped into WordNet, with precision of 0.405 and 0.418 on 75% and 70% mapping for the lexical sample and all-words tasks, respectively, comparable to WordNet. Bug fixes and implementation of incomplete routines have increased the precision for the lexical sample to 0.429 (with many improvements still likely).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (1998). WordNet: An electronic lexical database. Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Litkowski</author>
</authors>
<title>Towards a MeaningFull Comparison of Lexical Resources. Association for Computational Linguistics Special Interest Group on the Lexicon Workshop. College Park,</title>
<date>1999</date>
<pages>21--22</pages>
<location>MD.</location>
<contexts>
<context position="7403" citStr="Litkowski, 1999" startWordPosition="1107" endWordPosition="1108"> consisted of the main word and all entries identifiable from the phrase dictionary for that word. (For bar, in NODE, there were 13 entries where bar was the first word in an MWU and 50 entries where it was the head noun; for begin, there was only one entry.) For the all-words texts, a list was made of all the task words to be disambiguated (including some phrases) and a subdictionary constructed from this list. For both tasks, the creation of these subdictionaries was fully automatic; no hand manipulation was involved. The NODE dictionaries were then mapped into the WordNet dictionaries (see Litkowski, 1999), using overlap among words and semantic relations. The 73 dictionaries for the lexical sample words gave rise to 1372 WordNet entries and 1722 NODE entries.&apos; Only 491 entries were common (i.e., no mappings were available for the remaining 1231 NODE entries); 881 entries in WordNet were therefore inaccessible through NODE. For the entries in 2WordNet definitions were not parsed. In an experiment, the semantic relations identifiable through parsing were frequently inconsistent with those already given in WordNet, so it was decided not to confound the disambiguation. 3Entries included all parts </context>
</contexts>
<marker>Litkowski, 1999</marker>
<rawString>Litkowski, K. C. (1999, 21-22 June). Towards a MeaningFull Comparison of Lexical Resources. Association for Computational Linguistics Special Interest Group on the Lexicon Workshop. College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Litkowski</author>
</authors>
<date>2000</date>
<booktitle>SENSEVAL: The CL Research Experience. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1453" citStr="Litkowski, 2000" startWordPosition="203" endWordPosition="204">ical sample task and 0.460 for the all-words task (these are actually recall, with actual precision of 0.390 and 0.506 for the two tasks). NODE definitions were automatically mapped into WordNet, with precision of 0.405 and 0.418 on 75% and 70% mapping for the lexical sample and all-words tasks, respectively, comparable to WordNet. Bug fixes and implementation of incomplete routines have increased the precision for the lexical sample to 0.429 (with many improvements still likely). Introduction CL Research&apos;s participation in SENSEVAL-2 was designed to (1) extend WSD techniques from SENSEVAL-1 (Litkowski, 2000), (2) generalize WSD mechanisms to rely on a full dictionary rather than a small set of entries where individual crafting might intrude, and (3) investigate WSD using one dictionary mapped into another (WordNet). Results indicate positive achievements for each of these goals. Time constraints precluded a complete assessment of the upper limits that can be achieved. In particular, although the general architecture from SENSEVAL-1 was retained, several specific WSD routines were not reimplemented. Incomplete testing, debugging, and implementation of new routines significantly affected the offici</context>
</contexts>
<marker>Litkowski, 2000</marker>
<rawString>Litkowski, K. C. (2000). SENSEVAL: The CL Research Experience. Computers and the Humanities, 34(1-2), 153-158.</rawString>
</citation>
<citation valid="true">
<title>The New Oxford Dictionary of English</title>
<date>1998</date>
<publisher>Clarendon Press.</publisher>
<location>Oxford:</location>
<marker>1998</marker>
<rawString>The New Oxford Dictionary of English (J. Pearsall, Ed.). (1998). Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Richardson</author>
</authors>
<title>Determining similarity and inferring relations in a lexical knowledge base</title>
<date>1997</date>
<publisher>The City University of</publisher>
<location>[Diss], New York, NY:</location>
<contexts>
<context position="2466" citStr="Richardson, 1997" startWordPosition="345" endWordPosition="346">general architecture from SENSEVAL-1 was retained, several specific WSD routines were not reimplemented. Incomplete testing, debugging, and implementation of new routines significantly affected the official results. Several of these problems are investigated more fully below. CL Research&apos;s WSD functionality is implemented in DIMAP&apos;, designed primarily for creation and maintenance of lexicons for natural language processing. In particular, DIMAP is designed to make machine-readable dictionaries (MRDs) tractable and to create semantic networks (similar to WordNet (F ellb aum, 1998) and MindNet (Richardson, 1997)) automatically by analyzing and parsing definitions. Section 1 describes the dictionary preparation techniques for WordNet and NODE (The New Oxford Dictionary of English, 1998), as well as the mapping from NODE to WordNet. Section 2 describes the WSD techniques used in SENSEVAL-2. Section 3 describes the SENSEVAL-2 results and section 4 discusses these results.. 1 Dictionary Preparation DIMAP can disambiguate any text against WordNet or any other dictionary converted to DIMAP, with a special emphasis on corpus instances for specific lemmas. The dictionaries used for disambiguation operate in </context>
</contexts>
<marker>Richardson, 1997</marker>
<rawString>Richardson, S. D. (1997). Determining similarity and inferring relations in a lexical knowledge base [Diss], New York, NY: The City University of New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>