<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<note confidence="0.444070666666667">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 25-32.
Association for Computational Linguistics.
</note>
<title confidence="0.997577">
Modeling Consensus: Classifier Combination
for Word Sense Disambiguation
</title>
<author confidence="0.988378">
Radu Florian and David Yarowsky
</author>
<affiliation confidence="0.928173">
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.730195">
Baltimore, MD 21218, USA
</address>
<email confidence="0.998894">
{rflorian,yarowsky}@cs.jhu.edu
</email>
<sectionHeader confidence="0.996663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960133333333">
This paper demonstrates the substantial empirical
success of classifier combination for the word sense
disambiguation task. It investigates more than 10
classifier combination methods, including second
order classifier stacking, over 6 major structurally
different base classifiers (enhanced Naïve Bayes,
cosine, Bayes Ratio, decision lists, transformation-
based learning and maximum variance boosted mix-
ture models). The paper also includes in-depth per-
formance analysis sensitive to properties of the fea-
ture space and component classifiers. When eval-
uated on the standard SENSEVAL1 and 2 data sets
on 4 languages (English, Spanish, Basque, and
Swedish), classifier combination performance ex-
ceeds the best published results on these data sets.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889534883721">
Classifier combination has been extensively stud-
ied in the last decade, and has been shown to be
successful in improving the performance of diverse
NLP applications, including POS tagging (Brill and
Wu, 1998; van Halteren et al., 2001), base noun
phrase chunking (Sang et al., 2000), parsing (Hen-
derson and Brill, 1999) and word sense disambigua-
tion (Kilgarriff and Rosenzweig, 2000; Stevenson
and Wilks, 2001). There are several reasons why
classifier combination is useful. First, by consulting
the output of multiple classifiers, the system will im-
prove its robustness. Second, it is possible that the
problem can be decomposed into orthogonal feature
spaces (e.g. linguistic constraints and word occur-
rence statistics) and it is often better to train dif-
ferent classifiers in each of the feature spaces and
then combine their output, instead of designing a
complex system that handles the multimodal infor-
mation. Third, it has been shown by Perrone and
Cooper (1993) that it is possible to reduce the clas-
sification error by a factor of 1 (N is the number of
classifiers) by combination, if the classifiers’ errors
are uncorrelated and unbiased.
The target task studied here is word sense disam-
biguation in the SENSEVAL evaluation framework
(Kilgarriff and Palmer, 2000; Edmonds and Cotton,
2001) with comparative tests in English, Spanish,
Swedish and Basque lexical-sample sense tagging
over a combined sample of 37730 instances of 234
polysemous words.
This paper offers a detailed comparative evalu-
ation and description of the problem of classifier
combination over a structurally and procedurally
diverse set of six both well established and orig-
inal classifiers: extended Naïve Bayes, BayesRa-
tio, Cosine, non-hierarchical Decision Lists, Trans-
formation Based Learning (TBL), and the MMVC
classifiers, briefly described in Section 4. These
systems have different space-searching strategies,
ranging from discriminant functions (BayesRatio)
to data likelihood (Bayes, Cosine) to decision rules
(TBL, Decision Lists), and therefore are amenable
to combination.
</bodyText>
<sectionHeader confidence="0.99626" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999874647058824">
Related work in classifier combination is discussed
throughout this article. For the specific task of
word sense disambiguation, the first empirical study
was presented in Kilgarriff and Rosenzweig (2000),
where the authors combined the output of the par-
ticipating SENSEVAL1 systems via simple (non-
weighted) voting, using either Absolute Majority,
Relative Majority, or Unanimous voting. Steven-
son and Wilks (2001) presented a classifier com-
bination framework where 3 disambiguation meth-
ods (simulated annealing, subject codes and selec-
tional restrictions) were combined using the TiMBL
memory-based approach (Daelemans et al., 1999).
Pedersen (2000) presents experiments with an en-
semble of Naïve Bayes classifiers, which outper-
form all previous published results on two ambigu-
ous words (line and interest).
</bodyText>
<sectionHeader confidence="0.993557" genericHeader="method">
3 The WSD Feature Space
</sectionHeader>
<bodyText confidence="0.99989475">
The feature space is a critical factor in classifier de-
sign, given the need to fuel the diverse strengths of
the component classifiers. Thus its quality is of-
ten highly correlated with performance. For this
</bodyText>
<table confidence="0.969782277777778">
An ancient stone church stands amid the fields,
the sound of bells ...
Feat. Type Word POS Lemma
Context ancient JJ ancient/J
Context stone NN stone/N
Context church NNP church/N
Context stands VBZ stand/V
Context amid IN amid/I
Context fields NN field/N
Context ... ... ...
Syntactic (predicate-argument) features
SubjectTo stands_Sbj VBZ stand_Sbj/V
Modifier stone_mod JJ ancient_mod/J
Ngram collocational features
-1 bigram stone_L JJ ancient_L/J
+1 bigram stands_R VBZ stand_R/V
11 trigram stone • stands JJ•VBZ stone/J•stands/V
... ... ... ...
</table>
<figureCaption confidence="0.9976415">
Figure 1: Example sentence and extracted features from
the SENSEVAL2 word church
</figureCaption>
<bodyText confidence="0.999992230769231">
reason, we used a rich feature space based on raw
words, lemmas and part-of-speech (POS) tags in a
variety of positional and syntactical relationships to
the target word. These positions include traditional
unordered bag-of-word context, local bigram and
trigram collocations and several syntactic relation-
ships based on predicate-argument structure. Their
use is illustrated on a sample English sentence for
the target word church in Figure 1. While an exten-
sive evaluation of feature type to WSD performance
is beyond the scope of this paper, Section 6 sketches
an analysis of the individual feature contribution to
each of the classifier types.
</bodyText>
<subsectionHeader confidence="0.9865785">
3.1 Part-of-Speech Tagging and
Lemmatization
</subsectionHeader>
<bodyText confidence="0.97076">
Part-of-speech tagger availability varied across the
languages that are studied here. An electronically
available transformation-based POS tagger (Ngai
and Florian, 2001) was trained on standard labeled
data for English (Penn Treebank), Swedish (SUC-
1 corpus), and Basque. For Spanish, an minimally
supervised tagger (Cucerzan and Yarowsky, 2000)
was used. Lemmatization was performed using an
existing trie-based supervised models for English,
and a combination of supervised and unsupervised
methods (Yarowsky and Wicentowski, 2000) for all
the other languages.
</bodyText>
<subsectionHeader confidence="0.998897">
3.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9969255">
The syntactic features extracted for a target word
depend on the word’s part of speech:
</bodyText>
<listItem confidence="0.997154857142857">
• verbs: the head noun of the verb’s object, par-
ticle/preposition and prepositional object;
• nouns: the headword of any verb-object,
subject-verb or noun-noun relationships iden-
tified for the target word;
• adjectives: the head noun modified by the ad-
jective.
</listItem>
<bodyText confidence="0.742238333333333">
The extraction process was performed using heuris-
tic patterns and regular expressions over the parts-
of-speech surrounding the target word1.
</bodyText>
<sectionHeader confidence="0.991989" genericHeader="method">
4 Classifier Models for Word Sense
Disambiguation
</sectionHeader>
<bodyText confidence="0.998439833333333">
This section briefly introduces the 6 classifier mod-
els used in this study. Among these models, the
Naïve Bayes variants (NB henceforth) (Pedersen,
1998; Manning and Schütze, 1999) and Cosine dif-
fer slightly from off-the-shelf versions, and only the
differences will be described.
</bodyText>
<subsectionHeader confidence="0.941421">
4.1 Vector-based Models: Enhanced Naïve
Bayes and Cosine Models
</subsectionHeader>
<bodyText confidence="0.986722666666667">
Many of the systems used in this research share
a common vector representation, which captures
traditional bag-of-words, extended ngram and
predicate-argument features in a single data struc-
ture. In these models, a vector is created for each
document in the collection: d = (d~)1~1
</bodyText>
<equation confidence="0.842298">
��1 , dj =
</equation>
<bodyText confidence="0.997950352941177">
Cj � W�, where cj is the number of times the feature
f3 appears in document d, N is the number of words
in d and Wj is a weight associated with the feature
fj2. Confusion between the same word participat-
ing in multiple feature roles is avoided by append-
ing the feature values with their positional type (e.g.
stands_Sbj, ancient_L are distinct from stands and
ancient in unmarked bag-of-words context).
The notable difference between the extended
models and others described in the literature, aside
from the use of more sophisticated features than
the traditional bag-of-words, is the variable weight-
ing of feature types noted above. These differences
yield a boost in the NB performance (relative to ba-
sic Naïve Bayes) of between 3.5% (Basque) and
10% (Spanish), with an average improvement of
7.25% over the four languages.
</bodyText>
<subsectionHeader confidence="0.981177">
4.2 The BayesRatio Model
</subsectionHeader>
<bodyText confidence="0.999116666666667">
The BayesRatio model (BR henceforth) is a vector-
based model using the likelihood ratio framework
described in Gale et al. (1992):
</bodyText>
<footnote confidence="0.987977">
1The feature extraction on the in English data was per-
formed by first identifying text chunks, and then using heuris-
tics on the chunks to extract the syntactic information.
2The weight Wj depends on the type of the feature fj: for
the bag-of-word features, this weight is inversely proportional
to the distance between the target word and the feature, while
for predicate-argument and extended ngram features it is a em-
pirically estimated weight (on a per language basis).
</footnote>
<equation confidence="0.993327923076923">
DecisionLists
TBL
P (—sld) —arg
ax
P (S) TT
m
s P (—s) fEd
11
P (s1d)
s = arg max
S
P (f s)
P JI—)
</equation>
<bodyText confidence="0.9996834">
where s� is the selected sense, d denotes documents
and f denotes features. By utilizing the binary ra-
tio for k-way modeling of feature probabilities, this
approach performs well on tasks where the data is
sparse.
</bodyText>
<subsectionHeader confidence="0.991938">
4.3 The MMVC Model
</subsectionHeader>
<bodyText confidence="0.9982325">
The Mixture Maximum Variance Correction classi-
fier (MMVC henceforth) (Cucerzan and Yarowsky,
2002) is a two step classifier. First, the sense proba-
bility is computed as a linear mixture
</bodyText>
<equation confidence="0.9417095">
P(sld) =� P(slf, d)P(fld) �=� P(slf)P(fld)
fEd fEd
</equation>
<bodyText confidence="0.99996">
where the probability P (sIw) is estimated from
data and P (wId) is computed as a weighted normal-
ized similarity between the word w and the target
word x (also taking into account the distance in the
document between w and x). In a second pass, the
sense whose variance exceeds a theoretically moti-
vated threshold is selected as the final sense label
(for details, see Cucerzan and Yarowsky (2002)).
</bodyText>
<subsectionHeader confidence="0.997402">
4.4 The Discriminative Models
</subsectionHeader>
<bodyText confidence="0.9999867">
Two discriminative models are used in the exper-
iments presented in Section 5 - a transformation-
based learning system (TBL henceforth) (Brill,
1995; Ngai and Florian, 2001) and a non-
hierarchical decision lists system (DL henceforth)
(Yarowsky, 1996). For prediction, these systems
utilize local n-grams around the target word (up to
3 words/lemma/POS to the left/right), bag-of-words
and lemma/collocation (±20 words around the tar-
get word, grouped by different window sizes) and
the syntactic features listed in Section 3.2.
The TBL system was modified to include redun-
dant rules that do not improve absolute accuracy on
training data in the traditional greedy training al-
gorithm, but are nonetheless positively correlated
with a particular sense. The benefit of this approach
is that predictive but redundant features in training
context may appear by themselves in new test con-
texts, improving coverage and increasing TBL base
model performance by 1-2%.
</bodyText>
<sectionHeader confidence="0.995399" genericHeader="method">
5 Models for Classifier Combination
</sectionHeader>
<bodyText confidence="0.952363">
One necessary property for success in combining
classifiers is that the errors produced by the com-
ponent classifiers should not be positively corre-
lated. On one extreme, if the classifier outputs are
</bodyText>
<figure confidence="0.967055666666667">
BayesRatio
MMVC
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<figureCaption confidence="0.999231">
Figure 2: Empirically-derived classifier similarity
</figureCaption>
<bodyText confidence="0.984303142857143">
strongly correlated, they will have a very high inter-
agreement rate and there is little to be gained from
the joint output. On the other extreme, Perrone and
Cooper (1993) show that, if the errors made by the
classifiers are uncorrelated and unbiased, then by
considering a classifier that selects the class that
maximizes the posterior class probability average
</bodyText>
<equation confidence="0.9996586">
c� = arg max
C P (c) = arg max
C N
1 N Pk (c) (1)
k=1
</equation>
<bodyText confidence="0.999695809523809">
the error is reduced by a factor of 1. This case
is mostly of theoretical interest, since in practice
all the classifiers will tend to make errors on the
“harder” samples.
Figure 3(a) shows the classifier inter-agreement
among the six classifiers presented in Section 4, on
the English data. Only two of them, BayesRatio and
cosine, have an agreement rate of over 80%3, while
the agreement rate can be as low as 63% (BayesRa-
tio and TBL). The average agreement is 71.7%. The
fact that the classifiers’ output are not strongly cor-
related suggests that the differences in performance
among them can be systematically exploited to im-
prove the overall classification. All individual clas-
sifiers have high stand-alone performance; each is
individually competitive with the best single SEN-
SEVAL2 systems and are fortuitously diverse in rel-
ative performance, as shown in Table 3(b). A den-
dogram of the similarity between the classifiers is
shown in Figure 2, derived using maximum linkage
hierarchical agglomerative clustering.
</bodyText>
<subsectionHeader confidence="0.996344">
5.1 Major Types of Classifier Combination
</subsectionHeader>
<bodyText confidence="0.999992083333333">
There are three major types of classifier combina-
tion (Xu et al., 1992). The most general type is the
case where the classifiers output a posterior class
probability distribution for each sample (which can
be interpolated). In the second case, systems only
output a set of labels, together with a ordering of
preference (likelihood). In the third and most re-
strictive case, the classifications consist of just a sin-
gle label, without rank or probability. Combining
classifiers in each one of these cases has different
properties; the remainder of this section examines
models appropriate to each situation.
</bodyText>
<footnote confidence="0.9578635">
3The performance is measured using 5-fold cross validation
on training data.
</footnote>
<figure confidence="0.94975675">
Bayes
Cosine
m Bayes
m Cosine
</figure>
<equation confidence="0.950282115384615">
� � � � � � � � � BayesRatio
� � � � � � � �
� � � � � � DL
� � � � � �
� � � � � � TBL
� � � � � �
� �
�
�
� � � � � � � � � � � � MMVC � � �
� � � � � � � � � � � � �
� � � � � � � � � � � � � � � �
� � � � � � � � � � � � � � �
� � � � � � � � � � � � � � �
� � � � � � � � � � �
� � � � � �
� � � � � � � � � � � � � � � � � � �
� � � � � � � � �
� � � � � � � � � �
� � � � � � � � � �
� �
� �
� � � � � � � � � � � �1�I61�IYI1 � � � � � � � � � � �� ��
� � � � � �
��������������������
��������������������
</equation>
<table confidence="0.926423727272727">
Bayes
� � Cosine BayesRatio DL TBL MMVC
System SENSEVAL1 SENSEVAL2
EN EN ES EU SV
Baseline 63.2 48.3 45.9 62.7 46.2
NB 80.4 65.7 67.9 71.2 66.7
BR 79.8 65.3 69.0 69.6 68.0
Cosine 74.0 62.2 65.9 66.0 66.4
DL 79.9 63.2 65.1 70.7 61.5
TBL 80.7 64.4 64.7 69.4 62.7
MMVC 81.1 66.7 66.7 69.7 61.9
</table>
<figure confidence="0.990277636363636">
0.85
0.8
0.75
0.7
0.65
0.6
Classifier Aggreement (% of data)
0.55
0.5
(a) Classifier inter-agreement on SENSEVAL2 (b) Individual classifier performance; best performers are
English data shown in bold
</figure>
<figureCaption confidence="0.999494">
Figure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data)
</figureCaption>
<subsectionHeader confidence="0.966502">
5.2 Combining the Posterior Sense Probability
Distributions
</subsectionHeader>
<bodyText confidence="0.9881332">
One of the simplest ways to combine the poste-
rior probability distributions is via direct averaging
(Equation (1)). Surprisingly, this method obtains
reasonably good results, despite its simplicity and
the fact that is not theoretically motivated under a
Bayes framework. Its success is highly dependent
on the condition that the classifiers’ errors are un-
correlated (Tumer and Gosh, 1995).
The averaging method is a particular case of
weighted mixture:4 N
</bodyText>
<equation confidence="0.9995502">
P (sjx, d) = P (kjx, d) - Pk (sjx, d) =
k=1
N
Ak (x, d) - Pk (sIx, d) (2)
k=1
</equation>
<bodyText confidence="0.996383454545455">
where Ak (d, d) is the weight assigned to the clas-
sifier Iv in the mixture and Pk (sIx, d) is the poste-
rior probability distribution output by classifier Iv;
for Ak (x, d) = N we obtain Equation (1).
The mixture interpolation coefficients can be
computed at different levels of granularity. For
instance, one can make the assumption that
P (IvIx, d) = P (IvIx) and then the coefficients will
be computed at word level; if P (IvIx, d) = P (Iv)
then the coefficients will be estimated on the entire
data.
One way to estimate these parameters is by linear
regression (Fuhr, 1989): estimate the coefficients
that minimize the mean square error (MSE)
min � ����� C (x, d) - N Ak (x, d) • p (.Ix, d) ����� 2
x d II k=1 (3)
where C (x, d) is the target vector of the cor-
rect classification of word x in document d:
4Note that we are computing a probability conditioned both
on the target word x and the document d, because the docu-
ments are associated with a particular target word x; this for-
malization works mainly for the lexical choice task.
</bodyText>
<equation confidence="0.99778025">
C (x, d) (s) = 6 (s, sx,d) , sx,d being the goldstan-
dard sense of x in d and 6 the Kronecker function:
0 if x 7�y
6 (x, y) = 1 if x = y
</equation>
<bodyText confidence="0.9997639">
As shown in Fuhr (1989), Perrone and Cooper
(1993), the solution to the optimization problem (3)
can be obtained by solving a linear set of equations.
The resulting classifier will have a lower square er-
ror than the average classifier (since the average
classifier is a particular case of weighted mixture).
Another common method to compute the A pa-
rameters is by using the Expectation-Maximization
(EM) algorithm (Dempster et al., 1977). One
can estimate the coefficients such as to max-
imize the log-likelihood of the data, L =
Y�x Ed.x log P (sx,djx, d). In this particular opti-
mization problem, the search space is convex, and
therefore a solution exists and is unique, and it can
be obtained by the usual EM algorithm (see Berger
(1996) for a detailed description).
An alternative method for estimating the parame-
ters Ak is to approximate them with the performance
of the Ivth classifier (aperformance-based combiner)
(van Halteren et al., 1998; Sang et al., 2000)
</bodyText>
<equation confidence="0.988552">
Ak (x, d) = P (Ck_is_correctlx, d) (4)
</equation>
<bodyText confidence="0.9999148">
therefore giving more weight to classifiers that have
a smaller classification error (the method will be re-
ferred to as PB). The probabilities in Equation (4)
are estimated directly from data, using the maxi-
mum likelihood principle.
</bodyText>
<subsectionHeader confidence="0.996474">
5.3 Combination based on Order Statistics
</subsectionHeader>
<bodyText confidence="0.99984775">
In cases where there are reasons to believe that the
posterior probability distribution output by a clas-
sifier is poorly estimated5, but that the relative or-
dering of senses matches the truth, a combination
</bodyText>
<footnote confidence="0.989068333333333">
5For instance, in sparse classification spaces, the Naïve
Bayes classifier will assign a probability very close to 1 to the
most likely sense, and close to 0 for the other ones.
</footnote>
<bodyText confidence="0.995052">
strategy based on the relative ranking of sense pos-
terior probabilities is more appropriate. The sense
posterior probability can be computed as
</bodyText>
<equation confidence="0.96591">
Ale (x, k) rankle (sIx, d)
P (slx, d) =
</equation>
<bodyText confidence="0.999849666666667">
where the rank of a sense s is inversely proportional
to the number of senses that are (strictly) more prob-
able than sense s:
</bodyText>
<equation confidence="0.63631">
rankk (slx, d) _ (I{s IPk (s lx, d) &gt; Pk (slx, d)} +1) 1
</equation>
<bodyText confidence="0.999947666666667">
This method will tend to prefer senses that appear
closer to the top of the likelihood list for most of the
classifiers, therefore being more robust both in cases
where one classifier makes a large error and in cases
where some classifiers consistently overestimate the
posterior sense probability of the most likely sense.
</bodyText>
<subsectionHeader confidence="0.978654">
5.4 The Classifier Republic: Voting
</subsectionHeader>
<bodyText confidence="0.967792533333333">
Some classification methods frequently used in
NLP directly minimize the classification error and
do not usually provide a probability distribution
over classes/senses (e.g. TBL and decision lists).
There are also situations where the user does not
have access to the probability distribution, such as
when the available classifier is a black-box that only
outputs the best classification. A very common
technique for combination in such a case is by vot-
ing (Brill and Wu, 1998; van Halteren et al., 1998;
Sang et al., 2000). In the simplest model, each clas-
sifier votes for its classification and the sense that
receives the most number of votes wins. The behav-
ior is identical to selecting the sense with the highest
posterior probability, computed as
</bodyText>
<equation confidence="0.9924595">
E
le Ale (x, d) • b (s, sle (x, d))
EEAle (x, d) � b (t, �sle (x, d)) (6)
� le
</equation>
<bodyText confidence="0.999684357142857">
where b is the Kronecker function and sle (x, d) is
the classification of the kth classifier. The Ale co-
efficients can be either equal (in a perfect classifier
democracy), or they can be estimated with any of
the techniques presented in Section 5.2. Section
6 presents an empirical evaluation of these tech-
niques.
Van Halteren et al. (1998) introduce a modified
version of voting called TagPair. Under this model,
the conditional probability that the word sense is s
given that classifier i outputs st and classifier j out-
puts s2, P (sIAi (x, d) = sl, Sj (x, d) = s2), is com-
puted on development data, and the posterior prob-
ability is estimated as
</bodyText>
<equation confidence="0.982197333333333">
N
P (slx, d) a 6 (s, sk (x, d)) + 6 (s, sm,j (x, d))
k=1 j&lt;m (7)
</equation>
<bodyText confidence="0.9995002">
where s2j(x, d) = arg maxi P (tIs2 (x, d) , s; (x, d)).
Each classifier votes for its classification and every
pair of classifiers votes for the sense that is most
likely given the joint classification. In the experi-
ments presented in van Halteren et al. (1998), this
method was the best performer among the presented
methods. Van Halteren et al. (2001) extend this
method to arbitrarily long conditioning sequences,
obtaining the best published POS tagging results on
four corpora.
</bodyText>
<sectionHeader confidence="0.995083" genericHeader="method">
6 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999985970588235">
To empirically test the combination methods pre-
sented in the previous section, we ran experiments
on the SENSEVAL1 English data and data from four
SENSEVAL2 lexical sample tasks: English(EN),
Spanish(ES), Basque(EU) and Swedish(SV). Un-
less explicitly stated otherwise, all the results in the
following section were obtained by performing 5-
fold cross-validation6. To avoid the potential for
over-optimization, a single final evaluation system
was run once on the otherwise untouched test data,
as presented in Section 6.3.
The data consists of contexts associated with a
specific word to be sense tagged (target word); the
context size varies from 1 sentence (Spanish) to
5 sentences (English, Swedish). Table 1 presents
some statistics collected on the training data for the
five data sets. Some of the tasks are quite challeng-
ing (e.g. SENSEVAL2 English task) – as illustrated
by the mean participating systems’ accuracies in Ta-
ble 5.
Outlining the claim that feature selection is im-
portant for WSD, Table 2 presents the marginal loss
in performance of either only using one of the po-
sitional feature classes or excluding one of the po-
sitional feature classes relative to the algorithm’s
full performance using all available feature classes.
It is interesting to note that the feature-attractive
methods (NB,BR,Cosine) depend heavily on the
BagOfWords features, while discriminative methods
are most dependent on LocalContext features. For
an extensive evaluation of factors influencing the
WSD performance (including representational fea-
tures), we refer the readers to Yarowsky and Florian
(2002).
</bodyText>
<subsectionHeader confidence="0.997906">
6.1 Combination Performance
</subsectionHeader>
<bodyText confidence="0.988815">
Table 3 shows the fine-grained sense accuracy (per-
cent of exact correct senses) results of running the
</bodyText>
<footnote confidence="0.8989166">
6When parameters needed to be estimated, a 3-1-1 split was
used: the systems were trained on three parts, parameters esti-
mated on the fourth (in a round-robin fashion) and performance
tested on the fifth; special care was taken such that no “test”
data was used in training classifiers or parameter estimation.
</footnote>
<table confidence="0.870108636363636">
E
le
E EAle (x, k) rankle (s�lx, d) (5)
�� le
P (slx, d) =
SE1 SENSEVAL2
EN EN ES EU SV
#words 42 73 39 40 40
#samples 12479 8611 4480 3444 8716
avg #senses/word 11.3 10.7 4.9 4.8 11.1
avg #samples/sense 26.21 9.96 23.4 17.9 19.5
</table>
<tableCaption confidence="0.99766">
Table 1: Training set characteristics
</tableCaption>
<table confidence="0.999805625">
Performance drop relative to full system (%)
NB Cosine BR TBL DL
BoW Ftrs Only -6.4 -4.8 -4.8 -6.0 -3.2
Local Ftrs Only -18.4 -11.5 -6.1 -1.5 -3.3
Syntactic Ftrs Only -28.1 -14.9 -5.4 -5.4 -4.8
No BoW Ftrs -14.7 -8.1 -5.3 -0.5* -2.0
No Local Ftrs -3.5 -0.8* -2.2 -2.9 -4.5
No Syntactic Ftrs -1.1 -0.8* -1.3 -1.0 -2.3
</table>
<tableCaption confidence="0.8183425">
Table 2: Individual feature type contribution to perfor-
mance. Fields marked with * indicate that the difference
in performance was not statistically significant at a 0.01
level (paired McNemar test).
</tableCaption>
<bodyText confidence="0.999770411764706">
classifier combination methods for 5 classifiers, NB
(Naïve Bayes), BR (BayesRatio), TBL, DL and
MMVC, including the average classifier accuracy
and the best classification accuracy. Before examin-
ing the results, it is worth mentioning that the meth-
ods which estimate parameters are doing so on a
smaller training size (3/5, to be precise), and this
can have an effect on how well the parameters are
estimated. After the parameters are estimated, how-
ever, the interpolation is done between probability
distributions that are computed on 4/5 of the train-
ing data, similarly to the methods that do not esti-
mate any parameters.
The unweighted averaging model of probability
interpolation (Equation (1)) performs well, obtain-
ing over 1% mean absolute performance over the
best classifier7, the difference in performance is
statistically significant in all cases except Swedish
and Spanish. Of the classifier combination tech-
niques, rank-based combination and performance-
based voting perform best. Their mean 2% absolute
improvement over the single best classifier is signif-
icant in all languages. Also, their accuracy improve-
ment relative to uniform-weight probability interpo-
lation is statistically significant in aggregate and for
all languages except Basque (where there is gener-
ally a small difference among all classifiers).
To ensure that we benefit from the performance
improvement of each of the stronger combination
methods and also to increase robustness, a final av-
eraging method is applied to the output of the best
performing combiners (creating a stacked classi-
fier). The last line in Table 3 shows the results ob-
tained by averaging the rank-based, EM-vote and
</bodyText>
<footnote confidence="0.879064">
7The best individual classifier differs with language, as
shown in Figure 3(b).
</footnote>
<table confidence="0.978549789473684">
Method SE1 SENSEVAL2
EN EN ES EU SV
Individual Classifiers
Mean Acc 79.5 65.0 66.6 70.4 65.9
Best Acc 81.1 66.7 68.8 71.2 68.0
Probability Interpolation
Averaging 82.7 68.0 69.3 72.2 68.16
MSE 82.8 68.1 69.7 71.0 69.2
EM 82.7 68.4 69.6 72.1 69.1
PB 82.8 68.0 69.4 72.2 68.7
Rank-based Combination
rank 83.1 68.6 71.0 72.1 70.3
Count-based Combination (Voting)
Simple Vote 82.8 68.1 70.9 72.1 70.0
TagPair 82.9 68.3 70.9 72.1 70.0
EM 83.0 68.4 70.5 71.7 70.0
PB 83.1 68.5 70.8 72.0 70.3
Stacking (Meta-Combination)
Prob. Interp. 83.2 68.6 71.0 72.3 70.4
</table>
<tableCaption confidence="0.999447666666667">
Table 3: Classifier combination accuracy over 5 base
classifiers: NB, BR, TBL, DL, MMVC. Best perform-
ing methods are shown in bold.
</tableCaption>
<table confidence="0.999823">
Estimation Level word POS ALL Interp
Accuracy 68.1 68.2 68.0 68.4
CrossEntropy 1.623 1.635 1.646 1.632
</table>
<tableCaption confidence="0.9948705">
Table 4: Accuracy for different EM-weighted probability
interpolation models for SENSEVAL2
</tableCaption>
<bodyText confidence="0.999359925925926">
PB-vote methods’ output. The difference in perfor-
mance between the stacked classifier and the best
classifier is statistically significant for all data sets
at a significance level of at least 10-5, as measured
by a paired McNemar test.
One interesting observation is that for all meth-
ods of A-parameter estimation (EM, PB and uniform
weighting) the count-based and rank-based strate-
gies that ignore relative probability magnitudes out-
perform their equivalent combination models using
probability interpolation. This is especially the case
when the base classifier scores have substantially
different ranges or variances; using relative ranks
effectively normalizes for such differences in model
behavior.
For the three methods that estimate the interpo-
lation weights – MSE, EM and PB – three vari-
ants were investigated. These were distinguished by
the granularity at which the weights are estimated:
at word level (Ak (x, d) = Ak (x)), at POS level
(Ak (x, d) = Ak (pos (x))) and over the entire train-
ing set (Ak (x, d) = AJ Table 4 displays the results
obtained by estimating the parameters using EM at
different sample granularities for the SENSEVAL2
English data. The number in the last column is ob-
tained by interpolating the first three systems. Also
displayed is cross-entropy, a measure of how well
</bodyText>
<figure confidence="0.994263323144104">
Difference in Accuracy vs
6−way Combination
1:1 Bayes BayesRatio &amp;quot;.Cosine - DL TBL -. MMVC
Difference in classification accuracy (%)
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
−1.2
−1
−0.8
−0.6
−0.4
−0 2
0
0.2
0.4
0.6 ���
���
English Spanish Swedish Basque
(a) Performance drop when eliminating one classifier
(marginal performance contribution)
Percent of available training data
(b) Performance drop when eliminating one classifer,
versus training data size
10 20 40 50 80
Bayes BayesRatio
MMVC
DL
TBL
Cosine
���
���
���
���
���
���
� � ���
��� ��
��
����
���� ��
�� ��
�� ��
��
����
���� ��
��
����
����
��
�� ��
�� ��
��
����
���� ��
��
����
���� ��
�� ��
�� ��
�� ��
�� ��
��
����
���� ��
��
����
���� ��
��
��
�� ��
��
�� ��
��
��
�� ��
�� ����
���� ��
��
�� ��
��
����
���� ��
��
�� ��
��
�� ��
�� ��
��
��
�� � � ��
��
����
���� ��
��
�� �� ��
�� ��
��
����
���� ��
��
�� ��
��
�� ��
�� � � �� ��
�� ��
��
��
�� ��
�� ��
��
����
���� ��
��
�� ����
���� ��
��
�� ��
�� ��
���� ��
�� ����
����
��
�� ��
�� � � ��
��
���� �� ��
��
��
�� ��
����
���� �� ��
��
��
�� ����
���� ��
��
�� ��
�� ��
����
�� ��
�� ��
��
�� � � ��
�� ��
���� ��
�� ��
��
�� �� ����
���� ��
�� �� ��
��
����
����
��
�� ����
����
��
�� ��
�� �� ��
��
����
����
��
�� ��
�� ��
�� ��
��
���� � � ��
�� ��
���� ��
���� ��
��
����
��
�� ��
�� ��
�� ����
����
� � ��
�� ��
�� ��
��
��
�� ��
�� ���� ��
�� ��
�� ���� ��
�� ��
��
��
�� ��
��
����
���� ���� ��
�� ����
����
��
�� ����
��
�� ��
�� ��
��
��
�� ��
�� � �
��
��
��
��
��
��
��
��
��
��
��
��
��
��
Senseval2 dataset
11
</figure>
<figureCaption confidence="0.999863">
Figure 4: Individual basic classifiers’ contributi on to the final classifier combination performance.
</figureCaption>
<bodyText confidence="0.9831665">
the combination classifier estimates the sense prob-
abilities, CE = — Ex d P (sx,d) log P (sIx, d).
</bodyText>
<subsectionHeader confidence="0.565686">
6.2 Individual Systems Contribution to
Combination
</subsectionHeader>
<bodyText confidence="0.99778668">
An interesting issue pertaining to classifier combi-
nation is what is the marginal contribution to final
combined performance of the individual classifier.
A suitable measure of this contribution is the dif-
ference in performance between a combination sys-
tem’s behavior with and without the particular clas-
sifier. The more negative the accuracy difference on
omission, the more valuable the classifier is to the
ensemble system.
Figure 4(a) displays the drop in performance ob-
tained by eliminating in turn each classifier from the
6-way combination, across four languages, while
Figure 4(b) shows the contribution of each classifier
on the SENSEVAL2 English data for different train-
ing sizes (10%-80%)8. Note that the classifiers with
the greatest marginal contribution to the combined
system performance are not always the best single
performing classifiers (Table 3(b)), but those with
the most effective original exploitation of the com-
mon feature space. On average, the classifier that
contributes the most to the combined system’s per-
formance is the TBL classifier, with an average im-
provement of 0.66% across the 4 languages. Also,
note that TBL and DL offer the greatest marginal
contribution on smaller training sizes (Figure 4(b)).
</bodyText>
<subsectionHeader confidence="0.995557">
6.3 Performance on Test Data
</subsectionHeader>
<bodyText confidence="0.972835">
At all points in this article, experiments have been
based strictly on the original SENSEVAL1 and SEN-
SEVAL2 training sets via cross-validation. The of-
ficial SENSEVAL1 and SENSEVAL2 test sets were
</bodyText>
<footnote confidence="0.885815666666667">
8The latter graph is obtained by sampling repeatedly a
prespecified ratio of training samples from 3 of the 5 cross-
validation splits, and testing on the other 2.
</footnote>
<bodyText confidence="0.9997128">
unused and unexamined during experimentation to
avoid any possibility of indirect optimization on this
data. But to provide results more readily compara-
ble to the official benchmarks, a single consensus
system was created for each language using linear
average stacking on the top three classifier combi-
nation methods in Table 3 for conservative robust-
ness. The final frozen consensus system for each
language was applied once to the SENSEVAL test
sets. The fine-grained results are shown in Table
5. For each language, the single new stacked com-
bination system outperforms the best previously re-
ported SENSEVAL results on the identical test data9.
As far as we know, they represent the best published
results for any of these five SENSEVAL tasks.
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99892064">
In conclusion, we have presented a comparative
evaluation study of combining six structurally and
procedurally different classifiers utilizing a rich
common feature space. Various classifier combi-
nation methods, including count-based, rank-based
and probability-based combinations are described
and evaluated. The experiments encompass super-
vised lexical sample tasks in four diverse languages:
English, Spanish, Swedish, and Basque.
9To evaluate systems on the full disambiguation task, it is
appropriate to compare them on their accuracy at 100% test-
data coverage, which is equivalent to system recall in the offi-
cial SENSEVAL scores. However, it can also be useful to con-
sider performance on only the subset of data for which a sys-
tem is confident enough to answer, measured by the secondary
measure precision. One useful byproduct of the CBV method
is the confidence it assigns to each sample, which we measured
by the number of classifiers that voted for the sample. If one
restricts system output to only those test instances where all
participating classifiers agree, consensus system performance
is 83.4% precision at a recall of 43%, for an F-measure of 56.7
on the SENSEVAL2 English lexical sample task. This outper-
forms the two supervised SENSEVAL2 systems that only had
partial coverage, which exhibited 82.9% precision at a recall of
28% (F=41.9) and 66.5% precision at 34.4% recall (F=47.9).
</bodyText>
<table confidence="0.998485857142857">
SENSEVAL1 SENSEVAL2 Sense Classification Accuracy
English
English Spanish Swedish Basque
Mean Official SENSEVAL Systems Accuracy 73.1±2.9 55.7±5.3 59.6±5.0 58.4±6.6 74.4±1.8
Best Previously Published SENSEVAL Accuracy 77.1% 64.2% 71.2% 70.1% 75.7%
Best Individual Classifier Accuracy 77.1% 62.5% 69.6% 68.6% 75.6%
New (Stacking) Accuracy 79.7% 66.5% 72.4% 71.9% 76.7%
</table>
<tableCaption confidence="0.999554">
Table 5: Final Performance (Frozen Systems) on SENSEVAL Lexical Sample WSD Test Data
</tableCaption>
<bodyText confidence="0.999978142857143">
The experiments show substantial variation in
single classifier performance across different lan-
guages and data sizes. They also show that this
variation can be successfully exploited by 10 differ-
ent classifier combination methods (and their meta-
voting consensus), each of which outperforms both
the single best classifier system and standard classi-
fier combination models on each of the 4 focus lan-
guages. Furthermore, when the stacking consensus
systems were frozen and applied once to the other-
wise untouched test sets, they substantially outper-
formed all previously known SENSEVAL1 and SEN-
SEVAL2 results on 4 languages, obtaining the best
published results on these data sets.
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999298">
The authors would like to thank Noah Smith for his
comments on an earlier version of this paper, and
the anonymous reviewers for their useful comments.
This work was supported by NSF grant IIS-9985033
and ONR/MURI contract N00014-01-1-0685.
</bodyText>
<sectionHeader confidence="0.998832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997540625">
A. Berger. 1996. Convexity, maximum likelihood
and all that. http://www.cs.cmu.edu/afs/cs/user/aberger/
www/ps/convex.ps.
E. Brill and J. Wu. 1998. Classifier combination for improved
lexical disambiguation. In Proceedings of COLING-ACL’98,
pages 191–195.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543–565.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings ofACL-2000, pages 270–277.
S. Cucerzan and D. Yarowsky. 2002. Augmented mixture models
for lexical disambiguation. In Proceedings of EMNLP-2002.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Timbl:
Tilburg memory based learner - version 1.0. Technical Report
ilk9803, Tilburg University, The Netherlands.
A.P. Dempster, N.M. Laird, , and D.B. Rubin. 1977. Maximum
likelihood from incomplete data via the EM algorithm. Jour-
nal of the Royal statistical Society, 39(1):1–38.
P. Edmonds and S. Cotton. 2001. SENSEVAL-2: Overview. In
Proceedings of SENSEVAL-2, pages 1–6.
N. Fuhr. 1989. Optimum polynomial retrieval funcions based
on the probability ranking principle. ACM Transactions on
Information Systems, 7(3):183–204.
W. Gale, K. Church, and D. Yarowsky. 1992. A method for
disambiguating word senses in a large corpus. Computers and
the Humanities, 26:415–439.
J. Henderson and E. Brill. 1999. Exploiting diversity in natural
language processing: Combining parsers. In Proceedings on
EMNLP99, pages 187–194.
A. Kilgarriff and M. Palmer. 2000. Introduction to the special
issue on senseval. Computer and the Humanities, 34(1):1-13.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and re-
sults for English Senseval. Computers and the Humanities,
34(1):15-48.
C.D. Manning and H. Schütze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
G. Ngai and R. Florian. 2001. Transformation-based learning in
the fast lane. In Proceedings ofNAACL’01, pages 40–47.
T. Pedersen. 1998. Naïve Bayes as a satisficing model. In Work-
ing Notes of the AAAI Symposium on Satisficing Models.
T. Pedersen. 2000. A simple approach to building ensembles of
naive bayesian classifiers for word sense disambiguation. In
Proceedings ofNAACL’00, pages 63–69.
M. P. Perrone and L. N. Cooper. 1993. When networks disagree:
Ensemble methods for hybrid neural networks. In R. J. Mam-
mone, editor, Neural Networksfor Speech and Image Process-
ing, pages 126–142. Chapman-Hall.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling,
Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Apply-
ing system combination to base noun phrase identification. In
Proceedings of COLING 2000, pages 857–863.
M. Stevenson and Y. Wilks. 2001. The interaction of knowl-
edge sources in word sense disambiguation. Computational
Linguistics, 27(3):321–349.
K. Tumer and J. Gosh. 1995. Theoretical foundations of linear
and order statistics combiners for neural pattern classifiers.
Technical Report TR-95-02-98, University of Texas, Austin.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improv-
ing data driven wordclass tagging by system combination. In
Proceedings of COLING-ACL’98, pages 491–497.
H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Im-
proving accuracy in word class tagging through the combina-
tion fo machine learning systems. Computational Linguistics,
27(2):199–230.
L. Xu, A. Krzyzak, and C. Suen. 1992. Methods of com-
bining multiple classifires and their applications to handwrit-
ing recognition. IEEE Trans. on Systems, Man. Cybernet,
22(3):418–435.
D. Yarowsky and R. Florian. 2002. Evaluating sense disambigua-
tion performance across diverse parameter spaces. To appear
in Journal ofNatural Language Engineering.
D. Yarowsky and R. Wicentowski. 2000. Minimally supervised
morphological analysis by multimodal alignment. In Pro-
ceedings ofACL-2000, pages 207–216.
D. Yarowsky. 1996. Homograph disambiguation in speech
synthesis. In J. Olive J. van Santen, R. Sproat and
J. Hirschberg, editors, Progress in Speech Synthesis, pages
159–175. Springer-Verlag.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560177">
<note confidence="0.968533">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 25-32. Association for Computational Linguistics.</note>
<title confidence="0.999505">Modeling Consensus: Classifier Combination for Word Sense Disambiguation</title>
<author confidence="0.9985">Florian</author>
<affiliation confidence="0.87656">Department of Computer Science Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.992625">Baltimore, MD 21218,</address>
<email confidence="0.999818">rflorian@cs.jhu.edu</email>
<email confidence="0.999818">yarowsky@cs.jhu.edu</email>
<abstract confidence="0.9990745625">This paper demonstrates the substantial empirical success of classifier combination for the word sense disambiguation task. It investigates more than 10 classifier combination methods, including second order classifier stacking, over 6 major structurally different base classifiers (enhanced Naïve Bayes, cosine, Bayes Ratio, decision lists, transformationbased learning and maximum variance boosted mixture models). The paper also includes in-depth performance analysis sensitive to properties of the feature space and component classifiers. When evalon the standard and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
</authors>
<title>Convexity, maximum likelihood and all that. http://www.cs.cmu.edu/afs/cs/user/aberger/ www/ps/convex.ps.</title>
<date>1996</date>
<contexts>
<context position="16946" citStr="Berger (1996)" startWordPosition="2852" endWordPosition="2853">inear set of equations. The resulting classifier will have a lower square error than the average classifier (since the average classifier is a particular case of weighted mixture). Another common method to compute the A parameters is by using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). One can estimate the coefficients such as to maximize the log-likelihood of the data, L = Y�x Ed.x log P (sx,djx, d). In this particular optimization problem, the search space is convex, and therefore a solution exists and is unique, and it can be obtained by the usual EM algorithm (see Berger (1996) for a detailed description). An alternative method for estimating the parameters Ak is to approximate them with the performance of the Ivth classifier (aperformance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak (x, d) = P (Ck_is_correctlx, d) (4) therefore giving more weight to classifiers that have a smaller classification error (the method will be referred to as PB). The probabilities in Equation (4) are estimated directly from data, using the maximum likelihood principle. 5.3 Combination based on Order Statistics In cases where there are reasons to believe that the post</context>
</contexts>
<marker>Berger, 1996</marker>
<rawString>A. Berger. 1996. Convexity, maximum likelihood and all that. http://www.cs.cmu.edu/afs/cs/user/aberger/ www/ps/convex.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>191--195</pages>
<contexts>
<context position="1410" citStr="Brill and Wu, 1998" startWordPosition="190" endWordPosition="193">mationbased learning and maximum variance boosted mixture models). The paper also includes in-depth performance analysis sensitive to properties of the feature space and component classifiers. When evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets. 1 Introduction Classifier combination has been extensively studied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging (Brill and Wu, 1998; van Halteren et al., 2001), base noun phrase chunking (Sang et al., 2000), parsing (Henderson and Brill, 1999) and word sense disambiguation (Kilgarriff and Rosenzweig, 2000; Stevenson and Wilks, 2001). There are several reasons why classifier combination is useful. First, by consulting the output of multiple classifiers, the system will improve its robustness. Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces </context>
<context position="19078" citStr="Brill and Wu, 1998" startWordPosition="3205" endWordPosition="3208">ere some classifiers consistently overestimate the posterior sense probability of the most likely sense. 5.4 The Classifier Republic: Voting Some classification methods frequently used in NLP directly minimize the classification error and do not usually provide a probability distribution over classes/senses (e.g. TBL and decision lists). There are also situations where the user does not have access to the probability distribution, such as when the available classifier is a black-box that only outputs the best classification. A very common technique for combination in such a case is by voting (Brill and Wu, 1998; van Halteren et al., 1998; Sang et al., 2000). In the simplest model, each classifier votes for its classification and the sense that receives the most number of votes wins. The behavior is identical to selecting the sense with the highest posterior probability, computed as E le Ale (x, d) • b (s, sle (x, d)) EEAle (x, d) � b (t, �sle (x, d)) (6) � le where b is the Kronecker function and sle (x, d) is the classification of the kth classifier. The Ale coefficients can be either equal (in a perfect classifier democracy), or they can be estimated with any of the techniques presented in Section</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of COLING-ACL’98, pages 191–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="10013" citStr="Brill, 1995" startWordPosition="1536" endWordPosition="1537"> d)P(fld) �=� P(slf)P(fld) fEd fEd where the probability P (sIw) is estimated from data and P (wId) is computed as a weighted normalized similarity between the word w and the target word x (also taking into account the distance in the document between w and x). In a second pass, the sense whose variance exceeds a theoretically motivated threshold is selected as the final sense label (for details, see Cucerzan and Yarowsky (2002)). 4.4 The Discriminative Models Two discriminative models are used in the experiments presented in Section 5 - a transformationbased learning system (TBL henceforth) (Brill, 1995; Ngai and Florian, 2001) and a nonhierarchical decision lists system (DL henceforth) (Yarowsky, 1996). For prediction, these systems utilize local n-grams around the target word (up to 3 words/lemma/POS to the left/right), bag-of-words and lemma/collocation (±20 words around the target word, grouped by different window sizes) and the syntactic features listed in Section 3.2. The TBL system was modified to include redundant rules that do not improve absolute accuracy on training data in the traditional greedy training algorithm, but are nonetheless positively correlated with a particular sense</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent minimally supervised induction of lexical probabilities.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL-2000,</booktitle>
<pages>270--277</pages>
<contexts>
<context position="5995" citStr="Cucerzan and Yarowsky, 2000" startWordPosition="876" endWordPosition="879">he target word church in Figure 1. While an extensive evaluation of feature type to WSD performance is beyond the scope of this paper, Section 6 sketches an analysis of the individual feature contribution to each of the classifier types. 3.1 Part-of-Speech Tagging and Lemmatization Part-of-speech tagger availability varied across the languages that are studied here. An electronically available transformation-based POS tagger (Ngai and Florian, 2001) was trained on standard labeled data for English (Penn Treebank), Swedish (SUC1 corpus), and Basque. For Spanish, an minimally supervised tagger (Cucerzan and Yarowsky, 2000) was used. Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000) for all the other languages. 3.2 Syntactic Features The syntactic features extracted for a target word depend on the word’s part of speech: • verbs: the head noun of the verb’s object, particle/preposition and prepositional object; • nouns: the headword of any verb-object, subject-verb or noun-noun relationships identified for the target word; • adjectives: the head noun modified by the adjective. The extrac</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2000</marker>
<rawString>S. Cucerzan and D. Yarowsky. 2000. Language independent minimally supervised induction of lexical probabilities. In Proceedings ofACL-2000, pages 270–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Augmented mixture models for lexical disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-2002.</booktitle>
<contexts>
<context position="9298" citStr="Cucerzan and Yarowsky, 2002" startWordPosition="1413" endWordPosition="1416">ersely proportional to the distance between the target word and the feature, while for predicate-argument and extended ngram features it is a empirically estimated weight (on a per language basis). DecisionLists TBL P (—sld) —arg ax P (S) TT m s P (—s) fEd 11 P (s1d) s = arg max S P (f s) P JI—) where s� is the selected sense, d denotes documents and f denotes features. By utilizing the binary ratio for k-way modeling of feature probabilities, this approach performs well on tasks where the data is sparse. 4.3 The MMVC Model The Mixture Maximum Variance Correction classifier (MMVC henceforth) (Cucerzan and Yarowsky, 2002) is a two step classifier. First, the sense probability is computed as a linear mixture P(sld) =� P(slf, d)P(fld) �=� P(slf)P(fld) fEd fEd where the probability P (sIw) is estimated from data and P (wId) is computed as a weighted normalized similarity between the word w and the target word x (also taking into account the distance in the document between w and x). In a second pass, the sense whose variance exceeds a theoretically motivated threshold is selected as the final sense label (for details, see Cucerzan and Yarowsky (2002)). 4.4 The Discriminative Models Two discriminative models are u</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>S. Cucerzan and D. Yarowsky. 2002. Augmented mixture models for lexical disambiguation. In Proceedings of EMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Timbl: Tilburg memory based learner - version 1.0.</title>
<date>1999</date>
<tech>Technical Report ilk9803,</tech>
<institution>Tilburg University, The Netherlands.</institution>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Timbl: Tilburg memory based learner - version 1.0. Technical Report ilk9803, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Dempster, Laird, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, , and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal statistical Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Edmonds</author>
<author>S Cotton</author>
</authors>
<title>SENSEVAL-2: Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL-2,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="2503" citStr="Edmonds and Cotton, 2001" startWordPosition="365" endWordPosition="368">tic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal information. Third, it has been shown by Perrone and Cooper (1993) that it is possible to reduce the classification error by a factor of 1 (N is the number of classifiers) by combination, if the classifiers’ errors are uncorrelated and unbiased. The target task studied here is word sense disambiguation in the SENSEVAL evaluation framework (Kilgarriff and Palmer, 2000; Edmonds and Cotton, 2001) with comparative tests in English, Spanish, Swedish and Basque lexical-sample sense tagging over a combined sample of 37730 instances of 234 polysemous words. This paper offers a detailed comparative evaluation and description of the problem of classifier combination over a structurally and procedurally diverse set of six both well established and original classifiers: extended Naïve Bayes, BayesRatio, Cosine, non-hierarchical Decision Lists, Transformation Based Learning (TBL), and the MMVC classifiers, briefly described in Section 4. These systems have different space-searching strategies, </context>
</contexts>
<marker>Edmonds, Cotton, 2001</marker>
<rawString>P. Edmonds and S. Cotton. 2001. SENSEVAL-2: Overview. In Proceedings of SENSEVAL-2, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
</authors>
<title>Optimum polynomial retrieval funcions based on the probability ranking principle.</title>
<date>1989</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="15607" citStr="Fuhr, 1989" startWordPosition="2598" endWordPosition="2599"> N Ak (x, d) - Pk (sIx, d) (2) k=1 where Ak (d, d) is the weight assigned to the classifier Iv in the mixture and Pk (sIx, d) is the posterior probability distribution output by classifier Iv; for Ak (x, d) = N we obtain Equation (1). The mixture interpolation coefficients can be computed at different levels of granularity. For instance, one can make the assumption that P (IvIx, d) = P (IvIx) and then the coefficients will be computed at word level; if P (IvIx, d) = P (Iv) then the coefficients will be estimated on the entire data. One way to estimate these parameters is by linear regression (Fuhr, 1989): estimate the coefficients that minimize the mean square error (MSE) min � ����� C (x, d) - N Ak (x, d) • p (.Ix, d) ����� 2 x d II k=1 (3) where C (x, d) is the target vector of the correct classification of word x in document d: 4Note that we are computing a probability conditioned both on the target word x and the document d, because the documents are associated with a particular target word x; this formalization works mainly for the lexical choice task. C (x, d) (s) = 6 (s, sx,d) , sx,d being the goldstandard sense of x in d and 6 the Kronecker function: 0 if x 7�y 6 (x, y) = 1 if x = y A</context>
</contexts>
<marker>Fuhr, 1989</marker>
<rawString>N. Fuhr. 1989. Optimum polynomial retrieval funcions based on the probability ranking principle. ACM Transactions on Information Systems, 7(3):183–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<contexts>
<context position="8393" citStr="Gale et al. (1992)" startWordPosition="1255" endWordPosition="1258">cient in unmarked bag-of-words context). The notable difference between the extended models and others described in the literature, aside from the use of more sophisticated features than the traditional bag-of-words, is the variable weighting of feature types noted above. These differences yield a boost in the NB performance (relative to basic Naïve Bayes) of between 3.5% (Basque) and 10% (Spanish), with an average improvement of 7.25% over the four languages. 4.2 The BayesRatio Model The BayesRatio model (BR henceforth) is a vectorbased model using the likelihood ratio framework described in Gale et al. (1992): 1The feature extraction on the in English data was performed by first identifying text chunks, and then using heuristics on the chunks to extract the syntactic information. 2The weight Wj depends on the type of the feature fj: for the bag-of-word features, this weight is inversely proportional to the distance between the target word and the feature, while for predicate-argument and extended ngram features it is a empirically estimated weight (on a per language basis). DecisionLists TBL P (—sld) —arg ax P (S) TT m s P (—s) fEd 11 P (s1d) s = arg max S P (f s) P JI—) where s� is the selected s</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>E Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings on EMNLP99,</booktitle>
<pages>187--194</pages>
<contexts>
<context position="1522" citStr="Henderson and Brill, 1999" startWordPosition="208" endWordPosition="212">ormance analysis sensitive to properties of the feature space and component classifiers. When evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets. 1 Introduction Classifier combination has been extensively studied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging (Brill and Wu, 1998; van Halteren et al., 2001), base noun phrase chunking (Sang et al., 2000), parsing (Henderson and Brill, 1999) and word sense disambiguation (Kilgarriff and Rosenzweig, 2000; Stevenson and Wilks, 2001). There are several reasons why classifier combination is useful. First, by consulting the output of multiple classifiers, the system will improve its robustness. Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal information. Th</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>J. Henderson and E. Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings on EMNLP99, pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
</authors>
<title>Introduction to the special issue on senseval.</title>
<date>2000</date>
<booktitle>Computer and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="2476" citStr="Kilgarriff and Palmer, 2000" startWordPosition="361" endWordPosition="364"> feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal information. Third, it has been shown by Perrone and Cooper (1993) that it is possible to reduce the classification error by a factor of 1 (N is the number of classifiers) by combination, if the classifiers’ errors are uncorrelated and unbiased. The target task studied here is word sense disambiguation in the SENSEVAL evaluation framework (Kilgarriff and Palmer, 2000; Edmonds and Cotton, 2001) with comparative tests in English, Spanish, Swedish and Basque lexical-sample sense tagging over a combined sample of 37730 instances of 234 polysemous words. This paper offers a detailed comparative evaluation and description of the problem of classifier combination over a structurally and procedurally diverse set of six both well established and original classifiers: extended Naïve Bayes, BayesRatio, Cosine, non-hierarchical Decision Lists, Transformation Based Learning (TBL), and the MMVC classifiers, briefly described in Section 4. These systems have different s</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>A. Kilgarriff and M. Palmer. 2000. Introduction to the special issue on senseval. Computer and the Humanities, 34(1):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<title>Framework and results for English Senseval. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="1585" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="218" endWordPosition="221">ace and component classifiers. When evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets. 1 Introduction Classifier combination has been extensively studied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging (Brill and Wu, 1998; van Halteren et al., 2001), base noun phrase chunking (Sang et al., 2000), parsing (Henderson and Brill, 1999) and word sense disambiguation (Kilgarriff and Rosenzweig, 2000; Stevenson and Wilks, 2001). There are several reasons why classifier combination is useful. First, by consulting the output of multiple classifiers, the system will improve its robustness. Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal information. Third, it has been shown by Perrone and Cooper (1993) that it is </context>
<context position="3491" citStr="Kilgarriff and Rosenzweig (2000)" startWordPosition="503" endWordPosition="506">nal classifiers: extended Naïve Bayes, BayesRatio, Cosine, non-hierarchical Decision Lists, Transformation Based Learning (TBL), and the MMVC classifiers, briefly described in Section 4. These systems have different space-searching strategies, ranging from discriminant functions (BayesRatio) to data likelihood (Bayes, Cosine) to decision rules (TBL, Decision Lists), and therefore are amenable to combination. 2 Previous Work Related work in classifier combination is discussed throughout this article. For the specific task of word sense disambiguation, the first empirical study was presented in Kilgarriff and Rosenzweig (2000), where the authors combined the output of the participating SENSEVAL1 systems via simple (nonweighted) voting, using either Absolute Majority, Relative Majority, or Unanimous voting. Stevenson and Wilks (2001) presented a classifier combination framework where 3 disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al., 1999). Pedersen (2000) presents experiments with an ensemble of Naïve Bayes classifiers, which outperform all previous published results on two ambiguous words (line and intere</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>A. Kilgarriff and J. Rosenzweig. 2000. Framework and results for English Senseval. Computers and the Humanities, 34(1):15-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6955" citStr="Manning and Schütze, 1999" startWordPosition="1020" endWordPosition="1023">bs: the head noun of the verb’s object, particle/preposition and prepositional object; • nouns: the headword of any verb-object, subject-verb or noun-noun relationships identified for the target word; • adjectives: the head noun modified by the adjective. The extraction process was performed using heuristic patterns and regular expressions over the partsof-speech surrounding the target word1. 4 Classifier Models for Word Sense Disambiguation This section briefly introduces the 6 classifier models used in this study. Among these models, the Naïve Bayes variants (NB henceforth) (Pedersen, 1998; Manning and Schütze, 1999) and Cosine differ slightly from off-the-shelf versions, and only the differences will be described. 4.1 Vector-based Models: Enhanced Naïve Bayes and Cosine Models Many of the systems used in this research share a common vector representation, which captures traditional bag-of-words, extended ngram and predicate-argument features in a single data structure. In these models, a vector is created for each document in the collection: d = (d~)1~1 ��1 , dj = Cj � W�, where cj is the number of times the feature f3 appears in document d, N is the number of words in d and Wj is a weight associated wit</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>C.D. Manning and H. Schütze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>R Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL’01,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="5820" citStr="Ngai and Florian, 2001" startWordPosition="850" endWordPosition="853">cal bigram and trigram collocations and several syntactic relationships based on predicate-argument structure. Their use is illustrated on a sample English sentence for the target word church in Figure 1. While an extensive evaluation of feature type to WSD performance is beyond the scope of this paper, Section 6 sketches an analysis of the individual feature contribution to each of the classifier types. 3.1 Part-of-Speech Tagging and Lemmatization Part-of-speech tagger availability varied across the languages that are studied here. An electronically available transformation-based POS tagger (Ngai and Florian, 2001) was trained on standard labeled data for English (Penn Treebank), Swedish (SUC1 corpus), and Basque. For Spanish, an minimally supervised tagger (Cucerzan and Yarowsky, 2000) was used. Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000) for all the other languages. 3.2 Syntactic Features The syntactic features extracted for a target word depend on the word’s part of speech: • verbs: the head noun of the verb’s object, particle/preposition and prepositional object; • no</context>
<context position="10038" citStr="Ngai and Florian, 2001" startWordPosition="1538" endWordPosition="1541"> P(slf)P(fld) fEd fEd where the probability P (sIw) is estimated from data and P (wId) is computed as a weighted normalized similarity between the word w and the target word x (also taking into account the distance in the document between w and x). In a second pass, the sense whose variance exceeds a theoretically motivated threshold is selected as the final sense label (for details, see Cucerzan and Yarowsky (2002)). 4.4 The Discriminative Models Two discriminative models are used in the experiments presented in Section 5 - a transformationbased learning system (TBL henceforth) (Brill, 1995; Ngai and Florian, 2001) and a nonhierarchical decision lists system (DL henceforth) (Yarowsky, 1996). For prediction, these systems utilize local n-grams around the target word (up to 3 words/lemma/POS to the left/right), bag-of-words and lemma/collocation (±20 words around the target word, grouped by different window sizes) and the syntactic features listed in Section 3.2. The TBL system was modified to include redundant rules that do not improve absolute accuracy on training data in the traditional greedy training algorithm, but are nonetheless positively correlated with a particular sense. The benefit of this app</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>G. Ngai and R. Florian. 2001. Transformation-based learning in the fast lane. In Proceedings ofNAACL’01, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Naïve Bayes as a satisficing model.</title>
<date>1998</date>
<booktitle>In Working Notes of the AAAI Symposium on Satisficing Models.</booktitle>
<contexts>
<context position="6927" citStr="Pedersen, 1998" startWordPosition="1018" endWordPosition="1019">of speech: • verbs: the head noun of the verb’s object, particle/preposition and prepositional object; • nouns: the headword of any verb-object, subject-verb or noun-noun relationships identified for the target word; • adjectives: the head noun modified by the adjective. The extraction process was performed using heuristic patterns and regular expressions over the partsof-speech surrounding the target word1. 4 Classifier Models for Word Sense Disambiguation This section briefly introduces the 6 classifier models used in this study. Among these models, the Naïve Bayes variants (NB henceforth) (Pedersen, 1998; Manning and Schütze, 1999) and Cosine differ slightly from off-the-shelf versions, and only the differences will be described. 4.1 Vector-based Models: Enhanced Naïve Bayes and Cosine Models Many of the systems used in this research share a common vector representation, which captures traditional bag-of-words, extended ngram and predicate-argument features in a single data structure. In these models, a vector is created for each document in the collection: d = (d~)1~1 ��1 , dj = Cj � W�, where cj is the number of times the feature f3 appears in document d, N is the number of words in d and W</context>
</contexts>
<marker>Pedersen, 1998</marker>
<rawString>T. Pedersen. 1998. Naïve Bayes as a satisficing model. In Working Notes of the AAAI Symposium on Satisficing Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL’00,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="3937" citStr="Pedersen (2000)" startWordPosition="568" endWordPosition="569">ion is discussed throughout this article. For the specific task of word sense disambiguation, the first empirical study was presented in Kilgarriff and Rosenzweig (2000), where the authors combined the output of the participating SENSEVAL1 systems via simple (nonweighted) voting, using either Absolute Majority, Relative Majority, or Unanimous voting. Stevenson and Wilks (2001) presented a classifier combination framework where 3 disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al., 1999). Pedersen (2000) presents experiments with an ensemble of Naïve Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). 3 The WSD Feature Space The feature space is a critical factor in classifier design, given the need to fuel the diverse strengths of the component classifiers. Thus its quality is often highly correlated with performance. For this An ancient stone church stands amid the fields, the sound of bells ... Feat. Type Word POS Lemma Context ancient JJ ancient/J Context stone NN stone/N Context church NNP church/N Context stands VBZ stand/V Cont</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>T. Pedersen. 2000. A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation. In Proceedings ofNAACL’00, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Perrone</author>
<author>L N Cooper</author>
</authors>
<title>When networks disagree: Ensemble methods for hybrid neural networks.</title>
<date>1993</date>
<booktitle>Neural Networksfor Speech and Image Processing,</booktitle>
<pages>126--142</pages>
<editor>In R. J. Mammone, editor,</editor>
<publisher>Chapman-Hall.</publisher>
<contexts>
<context position="2173" citStr="Perrone and Cooper (1993)" startWordPosition="311" endWordPosition="314">tion (Kilgarriff and Rosenzweig, 2000; Stevenson and Wilks, 2001). There are several reasons why classifier combination is useful. First, by consulting the output of multiple classifiers, the system will improve its robustness. Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal information. Third, it has been shown by Perrone and Cooper (1993) that it is possible to reduce the classification error by a factor of 1 (N is the number of classifiers) by combination, if the classifiers’ errors are uncorrelated and unbiased. The target task studied here is word sense disambiguation in the SENSEVAL evaluation framework (Kilgarriff and Palmer, 2000; Edmonds and Cotton, 2001) with comparative tests in English, Spanish, Swedish and Basque lexical-sample sense tagging over a combined sample of 37730 instances of 234 polysemous words. This paper offers a detailed comparative evaluation and description of the problem of classifier combination o</context>
<context position="11322" citStr="Perrone and Cooper (1993)" startWordPosition="1738" endWordPosition="1741"> context may appear by themselves in new test contexts, improving coverage and increasing TBL base model performance by 1-2%. 5 Models for Classifier Combination One necessary property for success in combining classifiers is that the errors produced by the component classifiers should not be positively correlated. On one extreme, if the classifier outputs are BayesRatio MMVC 0.0 0.2 0.4 0.6 0.8 1.0 Figure 2: Empirically-derived classifier similarity strongly correlated, they will have a very high interagreement rate and there is little to be gained from the joint output. On the other extreme, Perrone and Cooper (1993) show that, if the errors made by the classifiers are uncorrelated and unbiased, then by considering a classifier that selects the class that maximizes the posterior class probability average c� = arg max C P (c) = arg max C N 1 N Pk (c) (1) k=1 the error is reduced by a factor of 1. This case is mostly of theoretical interest, since in practice all the classifiers will tend to make errors on the “harder” samples. Figure 3(a) shows the classifier inter-agreement among the six classifiers presented in Section 4, on the English data. Only two of them, BayesRatio and cosine, have an agreement rat</context>
<context position="16256" citStr="Perrone and Cooper (1993)" startWordPosition="2733" endWordPosition="2736">nts that minimize the mean square error (MSE) min � ����� C (x, d) - N Ak (x, d) • p (.Ix, d) ����� 2 x d II k=1 (3) where C (x, d) is the target vector of the correct classification of word x in document d: 4Note that we are computing a probability conditioned both on the target word x and the document d, because the documents are associated with a particular target word x; this formalization works mainly for the lexical choice task. C (x, d) (s) = 6 (s, sx,d) , sx,d being the goldstandard sense of x in d and 6 the Kronecker function: 0 if x 7�y 6 (x, y) = 1 if x = y As shown in Fuhr (1989), Perrone and Cooper (1993), the solution to the optimization problem (3) can be obtained by solving a linear set of equations. The resulting classifier will have a lower square error than the average classifier (since the average classifier is a particular case of weighted mixture). Another common method to compute the A parameters is by using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). One can estimate the coefficients such as to maximize the log-likelihood of the data, L = Y�x Ed.x log P (sx,djx, d). In this particular optimization problem, the search space is convex, and therefore a solution</context>
</contexts>
<marker>Perrone, Cooper, 1993</marker>
<rawString>M. P. Perrone and L. N. Cooper. 1993. When networks disagree: Ensemble methods for hybrid neural networks. In R. J. Mammone, editor, Neural Networksfor Speech and Image Processing, pages 126–142. Chapman-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>W Daelemans</author>
<author>H Dejean</author>
<author>R Koeling</author>
<author>Y Krymolowsky</author>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>Applying system combination to base noun phrase identification.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>857--863</pages>
<contexts>
<context position="1485" citStr="Sang et al., 2000" startWordPosition="203" endWordPosition="206">r also includes in-depth performance analysis sensitive to properties of the feature space and component classifiers. When evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets. 1 Introduction Classifier combination has been extensively studied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging (Brill and Wu, 1998; van Halteren et al., 2001), base noun phrase chunking (Sang et al., 2000), parsing (Henderson and Brill, 1999) and word sense disambiguation (Kilgarriff and Rosenzweig, 2000; Stevenson and Wilks, 2001). There are several reasons why classifier combination is useful. First, by consulting the output of multiple classifiers, the system will improve its robustness. Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that h</context>
<context position="17174" citStr="Sang et al., 2000" startWordPosition="2885" endWordPosition="2888">ameters is by using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). One can estimate the coefficients such as to maximize the log-likelihood of the data, L = Y�x Ed.x log P (sx,djx, d). In this particular optimization problem, the search space is convex, and therefore a solution exists and is unique, and it can be obtained by the usual EM algorithm (see Berger (1996) for a detailed description). An alternative method for estimating the parameters Ak is to approximate them with the performance of the Ivth classifier (aperformance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak (x, d) = P (Ck_is_correctlx, d) (4) therefore giving more weight to classifiers that have a smaller classification error (the method will be referred to as PB). The probabilities in Equation (4) are estimated directly from data, using the maximum likelihood principle. 5.3 Combination based on Order Statistics In cases where there are reasons to believe that the posterior probability distribution output by a classifier is poorly estimated5, but that the relative ordering of senses matches the truth, a combination 5For instance, in sparse classification spaces, the Naïve Bayes classifier wil</context>
<context position="19125" citStr="Sang et al., 2000" startWordPosition="3214" endWordPosition="3217">the posterior sense probability of the most likely sense. 5.4 The Classifier Republic: Voting Some classification methods frequently used in NLP directly minimize the classification error and do not usually provide a probability distribution over classes/senses (e.g. TBL and decision lists). There are also situations where the user does not have access to the probability distribution, such as when the available classifier is a black-box that only outputs the best classification. A very common technique for combination in such a case is by voting (Brill and Wu, 1998; van Halteren et al., 1998; Sang et al., 2000). In the simplest model, each classifier votes for its classification and the sense that receives the most number of votes wins. The behavior is identical to selecting the sense with the highest posterior probability, computed as E le Ale (x, d) • b (s, sle (x, d)) EEAle (x, d) � b (t, �sle (x, d)) (6) � le where b is the Kronecker function and sle (x, d) is the classification of the kth classifier. The Ale coefficients can be either equal (in a perfect classifier democracy), or they can be estimated with any of the techniques presented in Section 5.2. Section 6 presents an empirical evaluatio</context>
</contexts>
<marker>Sang, Daelemans, Dejean, Koeling, Krymolowsky, Punyakanok, Roth, 2000</marker>
<rawString>E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling, Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Applying system combination to base noun phrase identification. In Proceedings of COLING 2000, pages 857–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>Y Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="1613" citStr="Stevenson and Wilks, 2001" startWordPosition="222" endWordPosition="225">en evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets. 1 Introduction Classifier combination has been extensively studied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging (Brill and Wu, 1998; van Halteren et al., 2001), base noun phrase chunking (Sang et al., 2000), parsing (Henderson and Brill, 1999) and word sense disambiguation (Kilgarriff and Rosenzweig, 2000; Stevenson and Wilks, 2001). There are several reasons why classifier combination is useful. First, by consulting the output of multiple classifiers, the system will improve its robustness. Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occurrence statistics) and it is often better to train different classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal information. Third, it has been shown by Perrone and Cooper (1993) that it is possible to reduce the class</context>
<context position="3701" citStr="Stevenson and Wilks (2001)" startWordPosition="533" endWordPosition="537">space-searching strategies, ranging from discriminant functions (BayesRatio) to data likelihood (Bayes, Cosine) to decision rules (TBL, Decision Lists), and therefore are amenable to combination. 2 Previous Work Related work in classifier combination is discussed throughout this article. For the specific task of word sense disambiguation, the first empirical study was presented in Kilgarriff and Rosenzweig (2000), where the authors combined the output of the participating SENSEVAL1 systems via simple (nonweighted) voting, using either Absolute Majority, Relative Majority, or Unanimous voting. Stevenson and Wilks (2001) presented a classifier combination framework where 3 disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al., 1999). Pedersen (2000) presents experiments with an ensemble of Naïve Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). 3 The WSD Feature Space The feature space is a critical factor in classifier design, given the need to fuel the diverse strengths of the component classifiers. Thus its quality is often highly correlated w</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>M. Stevenson and Y. Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27(3):321–349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tumer</author>
<author>J Gosh</author>
</authors>
<title>Theoretical foundations of linear and order statistics combiners for neural pattern classifiers.</title>
<date>1995</date>
<tech>Technical Report TR-95-02-98,</tech>
<institution>University of Texas,</institution>
<location>Austin.</location>
<contexts>
<context position="14885" citStr="Tumer and Gosh, 1995" startWordPosition="2457" endWordPosition="2460">) Individual classifier performance; best performers are English data shown in bold Figure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data) 5.2 Combining the Posterior Sense Probability Distributions One of the simplest ways to combine the posterior probability distributions is via direct averaging (Equation (1)). Surprisingly, this method obtains reasonably good results, despite its simplicity and the fact that is not theoretically motivated under a Bayes framework. Its success is highly dependent on the condition that the classifiers’ errors are uncorrelated (Tumer and Gosh, 1995). The averaging method is a particular case of weighted mixture:4 N P (sjx, d) = P (kjx, d) - Pk (sjx, d) = k=1 N Ak (x, d) - Pk (sIx, d) (2) k=1 where Ak (d, d) is the weight assigned to the classifier Iv in the mixture and Pk (sIx, d) is the posterior probability distribution output by classifier Iv; for Ak (x, d) = N we obtain Equation (1). The mixture interpolation coefficients can be computed at different levels of granularity. For instance, one can make the assumption that P (IvIx, d) = P (IvIx) and then the coefficients will be computed at word level; if P (IvIx, d) = P (Iv) then the co</context>
</contexts>
<marker>Tumer, Gosh, 1995</marker>
<rawString>K. Tumer and J. Gosh. 1995. Theoretical foundations of linear and order statistics combiners for neural pattern classifiers. Technical Report TR-95-02-98, University of Texas, Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>491--497</pages>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLING-ACL’98, pages 491–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving accuracy in word class tagging through the combination fo machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Improving accuracy in word class tagging through the combination fo machine learning systems. Computational Linguistics, 27(2):199–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xu</author>
<author>A Krzyzak</author>
<author>C Suen</author>
</authors>
<title>Methods of combining multiple classifires and their applications to handwriting recognition.</title>
<date>1992</date>
<journal>IEEE Trans. on Systems, Man. Cybernet,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="12701" citStr="Xu et al., 1992" startWordPosition="1968" endWordPosition="1971">trongly correlated suggests that the differences in performance among them can be systematically exploited to improve the overall classification. All individual classifiers have high stand-alone performance; each is individually competitive with the best single SENSEVAL2 systems and are fortuitously diverse in relative performance, as shown in Table 3(b). A dendogram of the similarity between the classifiers is shown in Figure 2, derived using maximum linkage hierarchical agglomerative clustering. 5.1 Major Types of Classifier Combination There are three major types of classifier combination (Xu et al., 1992). The most general type is the case where the classifiers output a posterior class probability distribution for each sample (which can be interpolated). In the second case, systems only output a set of labels, together with a ordering of preference (likelihood). In the third and most restrictive case, the classifications consist of just a single label, without rank or probability. Combining classifiers in each one of these cases has different properties; the remainder of this section examines models appropriate to each situation. 3The performance is measured using 5-fold cross validation on tr</context>
</contexts>
<marker>Xu, Krzyzak, Suen, 1992</marker>
<rawString>L. Xu, A. Krzyzak, and C. Suen. 1992. Methods of combining multiple classifires and their applications to handwriting recognition. IEEE Trans. on Systems, Man. Cybernet, 22(3):418–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Florian</author>
</authors>
<title>Evaluating sense disambiguation performance across diverse parameter spaces.</title>
<date>2002</date>
<journal>Journal ofNatural Language Engineering.</journal>
<note>To appear in</note>
<contexts>
<context position="22253" citStr="Yarowsky and Florian (2002)" startWordPosition="3738" endWordPosition="3741">lection is important for WSD, Table 2 presents the marginal loss in performance of either only using one of the positional feature classes or excluding one of the positional feature classes relative to the algorithm’s full performance using all available feature classes. It is interesting to note that the feature-attractive methods (NB,BR,Cosine) depend heavily on the BagOfWords features, while discriminative methods are most dependent on LocalContext features. For an extensive evaluation of factors influencing the WSD performance (including representational features), we refer the readers to Yarowsky and Florian (2002). 6.1 Combination Performance Table 3 shows the fine-grained sense accuracy (percent of exact correct senses) results of running the 6When parameters needed to be estimated, a 3-1-1 split was used: the systems were trained on three parts, parameters estimated on the fourth (in a round-robin fashion) and performance tested on the fifth; special care was taken such that no “test” data was used in training classifiers or parameter estimation. E le E EAle (x, k) rankle (s�lx, d) (5) �� le P (slx, d) = SE1 SENSEVAL2 EN EN ES EU SV #words 42 73 39 40 40 #samples 12479 8611 4480 3444 8716 avg #senses</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>D. Yarowsky and R. Florian. 2002. Evaluating sense disambiguation performance across diverse parameter spaces. To appear in Journal ofNatural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL-2000,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="6183" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="901" endWordPosition="904">eature contribution to each of the classifier types. 3.1 Part-of-Speech Tagging and Lemmatization Part-of-speech tagger availability varied across the languages that are studied here. An electronically available transformation-based POS tagger (Ngai and Florian, 2001) was trained on standard labeled data for English (Penn Treebank), Swedish (SUC1 corpus), and Basque. For Spanish, an minimally supervised tagger (Cucerzan and Yarowsky, 2000) was used. Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000) for all the other languages. 3.2 Syntactic Features The syntactic features extracted for a target word depend on the word’s part of speech: • verbs: the head noun of the verb’s object, particle/preposition and prepositional object; • nouns: the headword of any verb-object, subject-verb or noun-noun relationships identified for the target word; • adjectives: the head noun modified by the adjective. The extraction process was performed using heuristic patterns and regular expressions over the partsof-speech surrounding the target word1. 4 Classifier Models for Word Sense Disambiguation This sec</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>D. Yarowsky and R. Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings ofACL-2000, pages 207–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Homograph disambiguation in speech synthesis.</title>
<date>1996</date>
<booktitle>Progress in Speech Synthesis,</booktitle>
<pages>159--175</pages>
<editor>In J. Olive J. van Santen, R. Sproat and J. Hirschberg, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="10115" citStr="Yarowsky, 1996" startWordPosition="1551" endWordPosition="1552">) is computed as a weighted normalized similarity between the word w and the target word x (also taking into account the distance in the document between w and x). In a second pass, the sense whose variance exceeds a theoretically motivated threshold is selected as the final sense label (for details, see Cucerzan and Yarowsky (2002)). 4.4 The Discriminative Models Two discriminative models are used in the experiments presented in Section 5 - a transformationbased learning system (TBL henceforth) (Brill, 1995; Ngai and Florian, 2001) and a nonhierarchical decision lists system (DL henceforth) (Yarowsky, 1996). For prediction, these systems utilize local n-grams around the target word (up to 3 words/lemma/POS to the left/right), bag-of-words and lemma/collocation (±20 words around the target word, grouped by different window sizes) and the syntactic features listed in Section 3.2. The TBL system was modified to include redundant rules that do not improve absolute accuracy on training data in the traditional greedy training algorithm, but are nonetheless positively correlated with a particular sense. The benefit of this approach is that predictive but redundant features in training context may appea</context>
</contexts>
<marker>Yarowsky, 1996</marker>
<rawString>D. Yarowsky. 1996. Homograph disambiguation in speech synthesis. In J. Olive J. van Santen, R. Sproat and J. Hirschberg, editors, Progress in Speech Synthesis, pages 159–175. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>