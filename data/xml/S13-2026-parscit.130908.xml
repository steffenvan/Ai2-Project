<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.228448">
<title confidence="0.996504">
MELODI: A Supervised Distributional Approach
for Free Paraphrasing of Noun Compounds
</title>
<author confidence="0.963115">
Tim Van de Cruys Stergos Afantenos Philippe Muller
</author>
<affiliation confidence="0.881309">
IRIT, CNRS IRIT, Toulouse University IRIT, Toulouse University
</affiliation>
<email confidence="0.977734">
tim.vandecruys@irit.fr stergos.afantenos@irit.fr philippe.muller@irit.fr
</email>
<sectionHeader confidence="0.995158" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999264363636364">
This paper describes the system submitted
by the MELODI team for the SemEval-2013
Task 4: Free Paraphrases of Noun Compounds
(Hendrickx et al., 2013). Our approach com-
bines the strength of an unsupervised distri-
butional word space model with a supervised
maximum-entropy classification model; the
distributional model yields a feature represen-
tation for a particular compound noun, which
is subsequently used by the classifier to induce
a number of appropriate paraphrases.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982828571429">
Interpretation of noun compounds is making explicit
the relation between the component nouns, for in-
stance that running shoes are shoes used in running
activities, while leather shoes are made from leather.
The relations can have very different meanings, and
existing work either postulates a fixed set of rela-
tions (Tratz and Hovy, 2010) or relies on appropri-
ate descriptions of the relations, through constrained
verbal paraphrases (Butnariu et al., 2010) or uncon-
strained paraphrases as in the present campaign. The
latter is much simpler for annotation purposes, but
raises difficult challenges involving not only com-
pound interpretation but also paraphrase evaluation
and ranking.
In terms of constrained verbal paraphrases
Wubben (2010), for example, uses a supervised
memory-based ranker using features from the
Google n-gram corpus as well as WordNet. Nulty
and Costello (2010) rank paraphrases of compounds
according to the number of times they co-occurred
with other paraphrases for other compounds. They
use these co-occurrences to compute conditional
probabilities estimating is-a relations between para-
phrases. Li et al. (2010) provide a hybrid sys-
tem which combines a Bayesian algorithm exploit-
ing Google n-grams, a score which captures human
preferences at the tail distribution of the training
data, as well as a metric that captures pairwise para-
phrase preferences.
Our methodology consists of two steps. First,
an unsupervised distributional word space model is
constructed, which yields a feature representation
for a particular compound. The feature representa-
tion is then used by a maximum entropy classifier to
induce a number of appropriate paraphrases.
</bodyText>
<sectionHeader confidence="0.998831" genericHeader="method">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.995046">
2.1 Distributional word space model
</subsectionHeader>
<bodyText confidence="0.999963583333333">
In order to induce appropriate feature representa-
tions for the various noun compounds, we start by
constructing a standard distributional word space
model for nouns. We construct a co-occurrence
matrix of the 5K most frequent nouns1 by the 2K
most frequent context words2, which occur in a win-
dow of 5 words to the left and right of the target
word. The bare frequencies of the word-context ma-
trix are weighted using pointwise mutual informa-
tion (Church and Hanks, 1990).
Next, we compute a joint, compositional repre-
sentation of the noun compound, combining the se-
</bodyText>
<footnote confidence="0.999682666666667">
1making sure all nouns that appear in the training and test
set are included
2excluding the 50 most frequent context words as stop words
</footnote>
<page confidence="0.977707">
144
</page>
<bodyText confidence="0.9900825">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 144–147, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
mantics of the head noun with the modifier noun. To
do so, we make use of a simple vector-based multi-
plicative model of compositionality, as proposed by
Mitchell and Lapata (2008). In order to compute the
compositional representation of a compound noun,
this model takes the elementwise multiplication of
the vectors for the head noun and the modifier noun,
i.e.
</bodyText>
<equation confidence="0.703766">
pi = uivi
</equation>
<bodyText confidence="0.996525">
for each feature i. The resulting features are used as
input to our next classification step.
We compare the performance of the abovemen-
tioned compositional model with a simpler model
that only takes into account the semantics of the
head noun. This model only uses the context fea-
tures for the head noun as input to our second clas-
sification step. This means that the model only takes
into account the semantics of the head noun, and ig-
nores the semantics of the modifier noun.
</bodyText>
<subsectionHeader confidence="0.999423">
2.2 Maximum entropy classification
</subsectionHeader>
<bodyText confidence="0.999996470588235">
The second step of our paraphrasing system consists
of a supervised maximum entropy classification ap-
proach. Training vectors for each noun compound
from the training set are constructed according to
the approach described in the previous section. The
(non-zero) context features yielded by the first step
are used as input for the maximum entropy classi-
fier, together with the appropriate paraphrase labels
and the label counts (used to weight the instances),
which are extracted from the training set.
We then deploy the model in order to induce a
probability distribution over the various paraphrase
labels. Every paraphrase label above a threshold 0 is
considered an appropriate paraphrase. Using a por-
tion of held-out training data (20%), we set 0 = 0.01
for our official submission. In this paper, we show a
number of results using different thresholds.
</bodyText>
<subsectionHeader confidence="0.999837">
2.3 Set of paraphrases labels
</subsectionHeader>
<bodyText confidence="0.999876681818182">
For our classification approach to work, we need to
extract an appropriate set of paraphrase labels from
the training data. In order to create this set, we
substitute the nouns that appear in the training set’s
paraphrases by dummy variables. Table 1 gives an
example of three different paraphrases and the re-
sulting paraphrase labels after substitution. Note
that we did not apply any NLP techniques to prop-
erly deal with inflected words.
We apply a frequency threshold of 2 (counted over
all the instances), so we discard paraphrase labels
that appear only once in the training set. This gives
us a total of 285 possible paraphrase labels.
One possible disadvantage of this supervised ap-
proach is a loss of recall on unseen paraphrases. A
rough estimation shows that our set of training labels
accounts for only 25% of the similarly constructed
labels extracted from the test set. However, the most
frequently used paraphrase labels are present in both
training and test set, so this does not prevent our
system to come up with a number of suitable para-
phrases for the test set.
</bodyText>
<subsectionHeader confidence="0.97157">
2.4 Implementational details
</subsectionHeader>
<bodyText confidence="0.9998932">
All frequency co-occurrence information has been
extracted from the ukWaC corpus (Baroni et al.,
2009). The corpus has been part of speech tagged
and lemmatized with Stanford Part-Of-Speech Tag-
ger (Toutanova and Manning, 2000; Toutanova et
al., 2003). Distributional word space algorithms
have been implemented in Python. The maximum
entropy classifier was implemented using the Maxi-
mum Entropy Modeling Toolkit for Python and C++
(Le, 2004).
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999968111111111">
Table 2 shows the results of the different systems in
terms of the isomorphic and non-isomorphic evalu-
ation measures defined by the task organizers (Hen-
drickx et al., 2013). For comparison, we include a
number of baselines. The first baseline assigns the
two most frequent paraphrase labels (Y of X, Y for
X) to each test instance; the second baseline assigns
the four most frequent paraphrase labels (Y of X, Y
for X, Y on X, Y in X); and the third baseline assigns
all of the possible 285 paraphrase labels as correct
answer for each test instance.
For both our primary system (the multiplicative
model) and our contrastive system (the head noun
model), we vary the threshold used to select the final
set of paraphrases. A threshold 0 = 0.01 results in
a smaller set of paraphrases, whereas a threshold of
0 = 0.001 results in a broad set of paraphrases. Our
official submission uses the former threshold.
</bodyText>
<page confidence="0.998663">
145
</page>
<tableCaption confidence="0.7308384">
compound paraphrase paraphrase label
textile company company that makes textiles Y that makes Xs
textile company company that produces textiles Y that produces Xs
textile company company in textile industry Y in X industry
Table 1: Example of induced paraphrase labels
</tableCaption>
<table confidence="0.999712375">
model 0 isomorphic non-isomorphic
baseline (2) – .058 .808
baseline (4) – .090 .633
baseline (all) – .332 .200
multiplicative .01 .130 .548
.001 .270 .259
head noun .01 .136 .536
.001 .277 .302
</table>
<tableCaption confidence="0.999417">
Table 2: Results
</tableCaption>
<bodyText confidence="0.99991865625">
First of all, we note that the different baseline
models are able to obtain substantial scores for the
different evaluation measures. The first two base-
lines, which use a limited number of paraphrase
labels, perform very well in terms of the non-
isomorphic evaluation measure. The third baseline,
which uses a very large number of candidate para-
phrase labels, gets more balanced results in terms of
both the isomorphic and non-isomorphic measure.
Considering our different thresholds, the results
of our models are in line with the baseline re-
sults. A larger threshold, which results in a smaller
number of paraphrase labels, reaches a higher non-
isomorphic score. A smaller threshold, which re-
sults in a larger number of paraphrase labels, gives
more balanced results for the isomorphic and non-
isomorphic measure.
There does not seem to be a significant difference
between our primary system (multiplicative) and our
contrastive system (head noun). For 0 = 0.01, the
results of both models are very similar; for 0 =
0.001, the head noun model reaches slightly better
results, in particular for the non-isomorphic score.
Finally, we note that our models do not seem to
improve significantly on the baseline scores. For
0 = 0.001, the results of our models seem somewhat
more balanced compared to the all baseline, but the
differences are not very large. In general, our sys-
tems (in line with the other systems participating in
the task) seem to have a hard time beating a num-
ber of simple baselines, in terms of the evaluation
measures defined by the task.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999964375">
We have presented a system for producing free para-
phrases of noun compounds. Our methodology con-
sists of two steps. First, an unsupervised distribu-
tional word space model is constructed, which is
used to compute a feature representation for a par-
ticular compound. The feature representation is then
used by a maximum entropy classifier to induce a
number of appropriate paraphrases.
Although our models do seem to yield slightly
more balanced scores than the baseline models, the
differences are not very large. Moreover, there is
no substantial difference between our primary mul-
tiplicative model, which takes into account the se-
mantics of both head and modifier noun, and our
contrastive model, which only uses the semantics of
the head noun.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997707266666667">
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2 task 9: The interpretation of noun
compounds using paraphrasing verbs and prepositions.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 39–44, Uppsala, Sweden,
July. Association for Computational Linguistics.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information &amp; lexicography.
Computational Linguistics, 16(1):22–29.
</reference>
<page confidence="0.994606">
146
</page>
<reference confidence="0.999449976744186">
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diar-
muid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale.
2013. SemEval-2013 task 4: Free paraphrases of noun
compounds. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ’13, June.
Zhang Le. 2004. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.ac.
uk/lzhang10/maxent_toolkit.html.
Guofu Li, Alejandra Lopez-Fernandez, and Tony Veale.
2010. Ucd-goggle: A hybrid system for noun com-
pound paraphrasing. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
230–233, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings ofACL-
08: HLT, pages 236–244.
Paul Nulty and Fintan Costello. 2010. Ucd-pn: Select-
ing general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 234–237, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63–70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252–259.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 678–687, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Sander Wubben. 2010. Uvt: Memory-based pairwise
ranking of paraphrasing verbs. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 260–263, Uppsala, Sweden, July. Association
for Computational Linguistics.
</reference>
<page confidence="0.998095">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619743">
<title confidence="0.9988675">A Supervised Distributional for Free Paraphrasing of Noun Compounds</title>
<author confidence="0.999495">Tim Van_de_Cruys Stergos Afantenos Philippe Muller</author>
<affiliation confidence="0.714211">IRIT, CNRS IRIT, Toulouse University IRIT, Toulouse</affiliation>
<email confidence="0.934449">tim.vandecruys@irit.frstergos.afantenos@irit.frphilippe.muller@irit.fr</email>
<abstract confidence="0.9872665">This paper describes the system submitted by the MELODI team for the SemEval-2013 Task 4: Free Paraphrases of Noun Compounds (Hendrickx et al., 2013). Our approach combines the strength of an unsupervised distributional word space model with a supervised maximum-entropy classification model; the distributional model yields a feature representation for a particular compound noun, which is subsequently used by the classifier to induce a number of appropriate paraphrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="6443" citStr="Baroni et al., 2009" startWordPosition="1006" endWordPosition="1009">ves us a total of 285 possible paraphrase labels. One possible disadvantage of this supervised approach is a loss of recall on unseen paraphrases. A rough estimation shows that our set of training labels accounts for only 25% of the similarly constructed labels extracted from the test set. However, the most frequently used paraphrase labels are present in both training and test set, so this does not prevent our system to come up with a number of suitable paraphrases for the test set. 2.4 Implementational details All frequency co-occurrence information has been extracted from the ukWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). Distributional word space algorithms have been implemented in Python. The maximum entropy classifier was implemented using the Maximum Entropy Modeling Toolkit for Python and C++ (Le, 2004). 3 Results Table 2 shows the results of the different systems in terms of the isomorphic and non-isomorphic evaluation measures defined by the task organizers (Hendrickx et al., 2013). For comparison, we include a number of baselines. The first baseline assign</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Su Nam Kim</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Stan Szpakowicz</author>
<author>Tony Veale</author>
</authors>
<title>Semeval-2 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>39--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Butnariu, Kim, Nakov, S´eaghdha, Szpakowicz, Veale, 2010</marker>
<rawString>Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diarmuid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale. 2010. Semeval-2 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 39–44, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information &amp; lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2967" citStr="Church and Hanks, 1990" startWordPosition="439" endWordPosition="442">esentation is then used by a maximum entropy classifier to induce a number of appropriate paraphrases. 2 Methodology 2.1 Distributional word space model In order to induce appropriate feature representations for the various noun compounds, we start by constructing a standard distributional word space model for nouns. We construct a co-occurrence matrix of the 5K most frequent nouns1 by the 2K most frequent context words2, which occur in a window of 5 words to the left and right of the target word. The bare frequencies of the word-context matrix are weighted using pointwise mutual information (Church and Hanks, 1990). Next, we compute a joint, compositional representation of the noun compound, combining the se1making sure all nouns that appear in the training and test set are included 2excluding the 50 most frequent context words as stop words 144 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 144–147, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics mantics of the head noun with the modifier noun. To do so, we make use of a simple vector-based multiplicative mo</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information &amp; lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Stan Szpakowicz</author>
<author>Tony Veale</author>
</authors>
<title>SemEval-2013 task 4: Free paraphrases of noun compounds.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<marker>Hendrickx, Kozareva, Nakov, S´eaghdha, Szpakowicz, Veale, 2013</marker>
<rawString>Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale. 2013. SemEval-2013 task 4: Free paraphrases of noun compounds. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Le</author>
</authors>
<title>Maximum entropy modeling toolkit for python and c++.</title>
<date>2004</date>
<note>http://homepages.inf.ed.ac. uk/lzhang10/maxent_toolkit.html.</note>
<contexts>
<context position="6782" citStr="Le, 2004" startWordPosition="1059" endWordPosition="1060">sent in both training and test set, so this does not prevent our system to come up with a number of suitable paraphrases for the test set. 2.4 Implementational details All frequency co-occurrence information has been extracted from the ukWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). Distributional word space algorithms have been implemented in Python. The maximum entropy classifier was implemented using the Maximum Entropy Modeling Toolkit for Python and C++ (Le, 2004). 3 Results Table 2 shows the results of the different systems in terms of the isomorphic and non-isomorphic evaluation measures defined by the task organizers (Hendrickx et al., 2013). For comparison, we include a number of baselines. The first baseline assigns the two most frequent paraphrase labels (Y of X, Y for X) to each test instance; the second baseline assigns the four most frequent paraphrase labels (Y of X, Y for X, Y on X, Y in X); and the third baseline assigns all of the possible 285 paraphrase labels as correct answer for each test instance. For both our primary system (the mult</context>
</contexts>
<marker>Le, 2004</marker>
<rawString>Zhang Le. 2004. Maximum entropy modeling toolkit for python and c++. http://homepages.inf.ed.ac. uk/lzhang10/maxent_toolkit.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guofu Li</author>
<author>Alejandra Lopez-Fernandez</author>
<author>Tony Veale</author>
</authors>
<title>Ucd-goggle: A hybrid system for noun compound paraphrasing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>230--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1910" citStr="Li et al. (2010)" startWordPosition="270" endWordPosition="273">r is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the number of times they co-occurred with other paraphrases for other compounds. They use these co-occurrences to compute conditional probabilities estimating is-a relations between paraphrases. Li et al. (2010) provide a hybrid system which combines a Bayesian algorithm exploiting Google n-grams, a score which captures human preferences at the tail distribution of the training data, as well as a metric that captures pairwise paraphrase preferences. Our methodology consists of two steps. First, an unsupervised distributional word space model is constructed, which yields a feature representation for a particular compound. The feature representation is then used by a maximum entropy classifier to induce a number of appropriate paraphrases. 2 Methodology 2.1 Distributional word space model In order to i</context>
</contexts>
<marker>Li, Lopez-Fernandez, Veale, 2010</marker>
<rawString>Guofu Li, Alejandra Lopez-Fernandez, and Tony Veale. 2010. Ucd-goggle: A hybrid system for noun compound paraphrasing. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 230–233, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition. proceedings ofACL08: HLT,</title>
<date>2008</date>
<pages>236--244</pages>
<contexts>
<context position="3633" citStr="Mitchell and Lapata (2008)" startWordPosition="541" endWordPosition="544">l representation of the noun compound, combining the se1making sure all nouns that appear in the training and test set are included 2excluding the 50 most frequent context words as stop words 144 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 144–147, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics mantics of the head noun with the modifier noun. To do so, we make use of a simple vector-based multiplicative model of compositionality, as proposed by Mitchell and Lapata (2008). In order to compute the compositional representation of a compound noun, this model takes the elementwise multiplication of the vectors for the head noun and the modifier noun, i.e. pi = uivi for each feature i. The resulting features are used as input to our next classification step. We compare the performance of the abovementioned compositional model with a simpler model that only takes into account the semantics of the head noun. This model only uses the context features for the head noun as input to our second classification step. This means that the model only takes into account the sem</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. proceedings ofACL08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Nulty</author>
<author>Fintan Costello</author>
</authors>
<title>Ucd-pn: Selecting general paraphrases using conditional probability.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>234--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1655" citStr="Nulty and Costello (2010)" startWordPosition="234" endWordPosition="237">xisting work either postulates a fixed set of relations (Tratz and Hovy, 2010) or relies on appropriate descriptions of the relations, through constrained verbal paraphrases (Butnariu et al., 2010) or unconstrained paraphrases as in the present campaign. The latter is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the number of times they co-occurred with other paraphrases for other compounds. They use these co-occurrences to compute conditional probabilities estimating is-a relations between paraphrases. Li et al. (2010) provide a hybrid system which combines a Bayesian algorithm exploiting Google n-grams, a score which captures human preferences at the tail distribution of the training data, as well as a metric that captures pairwise paraphrase preferences. Our methodology consists of two steps. First, an unsupervised distributional word space model is const</context>
</contexts>
<marker>Nulty, Costello, 2010</marker>
<rawString>Paul Nulty and Fintan Costello. 2010. Ucd-pn: Selecting general paraphrases using conditional probability. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 234–237, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000),</booktitle>
<pages>63--70</pages>
<contexts>
<context position="6566" citStr="Toutanova and Manning, 2000" startWordPosition="1025" endWordPosition="1028"> recall on unseen paraphrases. A rough estimation shows that our set of training labels accounts for only 25% of the similarly constructed labels extracted from the test set. However, the most frequently used paraphrase labels are present in both training and test set, so this does not prevent our system to come up with a number of suitable paraphrases for the test set. 2.4 Implementational details All frequency co-occurrence information has been extracted from the ukWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). Distributional word space algorithms have been implemented in Python. The maximum entropy classifier was implemented using the Maximum Entropy Modeling Toolkit for Python and C++ (Le, 2004). 3 Results Table 2 shows the results of the different systems in terms of the isomorphic and non-isomorphic evaluation measures defined by the task organizers (Hendrickx et al., 2013). For comparison, we include a number of baselines. The first baseline assigns the two most frequent paraphrase labels (Y of X, Y for X) to each test instance; the second baseline assigns the four mos</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000), pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="6591" citStr="Toutanova et al., 2003" startWordPosition="1029" endWordPosition="1032">. A rough estimation shows that our set of training labels accounts for only 25% of the similarly constructed labels extracted from the test set. However, the most frequently used paraphrase labels are present in both training and test set, so this does not prevent our system to come up with a number of suitable paraphrases for the test set. 2.4 Implementational details All frequency co-occurrence information has been extracted from the ukWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). Distributional word space algorithms have been implemented in Python. The maximum entropy classifier was implemented using the Maximum Entropy Modeling Toolkit for Python and C++ (Le, 2004). 3 Results Table 2 shows the results of the different systems in terms of the isomorphic and non-isomorphic evaluation measures defined by the task organizers (Hendrickx et al., 2013). For comparison, we include a number of baselines. The first baseline assigns the two most frequent paraphrase labels (Y of X, Y for X) to each test instance; the second baseline assigns the four most frequent paraphrase lab</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A taxonomy, dataset, and classifier for automatic noun compound interpretation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>678--687</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1108" citStr="Tratz and Hovy, 2010" startWordPosition="154" endWordPosition="157">istributional word space model with a supervised maximum-entropy classification model; the distributional model yields a feature representation for a particular compound noun, which is subsequently used by the classifier to induce a number of appropriate paraphrases. 1 Introduction Interpretation of noun compounds is making explicit the relation between the component nouns, for instance that running shoes are shoes used in running activities, while leather shoes are made from leather. The relations can have very different meanings, and existing work either postulates a fixed set of relations (Tratz and Hovy, 2010) or relies on appropriate descriptions of the relations, through constrained verbal paraphrases (Butnariu et al., 2010) or unconstrained paraphrases as in the present campaign. The latter is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the numbe</context>
</contexts>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2010. A taxonomy, dataset, and classifier for automatic noun compound interpretation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678–687, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
</authors>
<title>Uvt: Memory-based pairwise ranking of paraphrasing verbs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>260--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1512" citStr="Wubben (2010)" startWordPosition="214" endWordPosition="215"> shoes used in running activities, while leather shoes are made from leather. The relations can have very different meanings, and existing work either postulates a fixed set of relations (Tratz and Hovy, 2010) or relies on appropriate descriptions of the relations, through constrained verbal paraphrases (Butnariu et al., 2010) or unconstrained paraphrases as in the present campaign. The latter is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the number of times they co-occurred with other paraphrases for other compounds. They use these co-occurrences to compute conditional probabilities estimating is-a relations between paraphrases. Li et al. (2010) provide a hybrid system which combines a Bayesian algorithm exploiting Google n-grams, a score which captures human preferences at the tail distribution of the training data, as well as a metric that c</context>
</contexts>
<marker>Wubben, 2010</marker>
<rawString>Sander Wubben. 2010. Uvt: Memory-based pairwise ranking of paraphrasing verbs. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 260–263, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>