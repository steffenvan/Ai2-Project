<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008784">
<title confidence="0.999731">
Exploiting Context for Biomedical Entity Recognition:
From Syntax to the Web
</title>
<author confidence="0.968888">
Jenny Finkel,* Shipra Dingare,† Huy Nguyen,*
Malvina Nissim,† Christopher Manning,* and Gail Sinclair†
</author>
<affiliation confidence="0.985105">
*Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.827363">
Stanford, CA 93405-9040
United States
</address>
<email confidence="0.7867105">
{jrfinkel|htnguyen|manning}
@cs.stanford.edu
</email>
<sectionHeader confidence="0.993812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982875">
We describe a machine learning system for the
recognition of names in biomedical texts. The sys-
tem makes extensive use of local and syntactic fea-
tures within the text, as well as external resources
including the web and gazetteers. It achieves an F-
score of 70% on the Coling 2004 NLPBA/BioNLP
shared task of identifying five biomedical named en-
tities in the GENIA corpus.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993172">
The explosion of information in the fields of molec-
ular biology and genetics has provided a unique
opportunity for natural language processing tech-
niques to aid researchers and curators of databases
in the biomedical field by providing text mining
services. Yet typical natural language processing
tasks such as named entity recognition, informa-
tion extraction, and word sense disambiguation are
particularly challenging in the biomedical domain
with its highly complex and idiosyncratic language.
With the increasing use of shared tasks and shared
evaluation procedures (e.g., the recent BioCreative,
TREC, and KDD Cup), it is rapidly becoming clear
that performance in this domain is markedly lower
than the field has come to expect from the standard
domain of newswire. The Coling 2004 shared task
focuses on the problem of Named Entity Recogni-
tion, requiring participating systems to identify the
five named entities of protein, RNA, DNA, cell line,
and cell type in the GENIA corpus of MEDLINE
abstracts (Ohta et al., 2002). In this paper we de-
scribe a machine learning system incorporating a di-
verse set of features and various external resources
to accomplish this task. We describe our system in
detail and also discuss some sources of error.
</bodyText>
<sectionHeader confidence="0.981931" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.777112045454545">
Our system is a Maximum Entropy Markov Model,
which further develops a system earlier used for the
†Institute for Communicating and
Collaborative Systems
University of Edinburgh
Edinburgh EH8 9LW
United Kingdom
{sdingar1|mnissim|csincla1}
@inf.ed.ac.uk
CoNLL 2003 shared task (Klein et al., 2003) and the
2004 BioCreative critical assessment of information
extraction systems, a task that involved identifying
gene and protein name mentions but not distinguish-
ing between them (Dingare et al., 2004). Unlike
the above two tasks, many of the entities in the cur-
rent task do not have good internal cues for distin-
guishing the class of entity: various systematic pol-
ysemies and the widespread use of acronyms mean
that internal cues are lacking. The challenge was
thus to make better use of contextual features, in-
cluding local and syntactic features, and external re-
sources in order to succeed at this task.
</bodyText>
<subsectionHeader confidence="0.992554">
2.1 Local Features
</subsectionHeader>
<bodyText confidence="0.9999545">
We used a variety of features describing the imme-
diate content and context of each word, including
the word itself, the previous and next words, word
prefixes and suffix of up to a length of 6 characters,
word shapes, and features describing the named en-
tity tags assigned to the previous words. Word
shapes refer to a mapping of each word onto equiva-
lence classes that encodes attributes such as length,
capitalization, numerals, greek letters, and so on.
For instance, “Varicella-zoster” would become Xx-
xxx, “mRNA” would become xXXX, and “CPA1”
would become XXXd. We also incorporated part-of-
speech tagging, using the TnT tagger(Brants, 2000)
retrained on the GENIA corpus gold standard part-
of-speech tagging. We also used various interaction
terms (conjunctions) of these base-level features in
various ways. The full set of local features is out-
lined in Table 1.
</bodyText>
<subsectionHeader confidence="0.995066">
2.2 External Resources
</subsectionHeader>
<bodyText confidence="0.92059575">
We made use of a number of external resources, in-
cluding gazetteers, web-querying, use of the sur-
rounding abstract, and frequency counts from the
British National Corpus.
</bodyText>
<page confidence="0.999121">
88
</page>
<table confidence="0.999587178571429">
Word Features wi, wi−1, wi+1
Disjunction of 5 prev words
Disjunction of 5 next words
TnT POS POSi, POSi−1, POSi+1
Prefix/suffix Up to a length of 6
Abbreviations abbri
abbri−1 + abbri
abbri + abbri+1
abbri−1 + abbri + abbri+1
Word Shape shapei, shapei−1, shapei+1
shapei−1 + shapei
shapei + shapei+1
shapei−1 + shapei + shapei+1
Prev NE NEi−1, NEi−2 + NEi−1
NEi−3 + NEi−2 + NEi−1
Prev NE + Word NEi−1 + wi
Prev NE + POS NEi−1 + POSi−1 + POSi
NEi−2 + NEi−1 + POSi−2 +
POSi−1 + POSi
Prev NE + Shape NEi−1 + shapei
NEi−1 + shapei+1
NEi−1 + shapei−1 + shapei
NEi−2 + NEi−1 + shapei−2 +
shapei−1 + shapei
Paren-Matching Signals when one parenthesis
in a pair has been assigned a
different tag than the other in a
window of 4 words
</table>
<tableCaption confidence="0.998929">
Table 1: Local Features (+ indicates conjunction)
</tableCaption>
<subsectionHeader confidence="0.46972">
2.2.1 Frequency
</subsectionHeader>
<bodyText confidence="0.999925411764706">
Many entries in gazetteers are ambiguous words,
occasionally used in the sense that the gazetteer
seeks to represent, but at least as frequently not.
So while the information that a token was seen in
a gazetteer is an unreliable indicator of whether it
is an entity, less frequent words are less likely to be
ambiguous than more frequent ones. Additionally,
more frequent words are likely to have been seen
often in the training data and the system should be
better at classifying them, while less frequent words
are a common source of error and their classifica-
tion is more likely to benefit from the use of external
resources. We assigned each word in the training
and testing data a frequency category correspond-
ing to its frequency in the British National Corpus,
a 100 million word balanced corpus, and used con-
junctions of this category and certain other features.
</bodyText>
<subsectionHeader confidence="0.643582">
2.2.2 Gazetteers
</subsectionHeader>
<bodyText confidence="0.999941090909091">
Our gazetteer contained only gene names and was
compiled from lists from biomedical websites (such
as LocusLink) as well as from the Gene Ontol-
ogy and the data provided for the BioCreative 2004
tasks. The final gazetteer contained 1,731,496 en-
tries. Because it contained only gene names, and for
the reasons discussed earlier, we suspect that it was
not terribly useful for identifying the presences of
entities, but rather that it mainly helped to establish
the exact beginning and ending point of multi-word
entities recognized mainly through other features.
</bodyText>
<subsectionHeader confidence="0.737725">
2.2.3 Web
</subsectionHeader>
<bodyText confidence="0.999986571428572">
For each of the named entity classes, we built in-
dicative contexts, such as “X mRNA” for RNA, or
“X ligation” for protein. For each entity X which
had a frequency lower than 10 in the British Na-
tional Corpus, we submitted instantiations of each
pattern to the web, using the Google API, and ob-
tained the number of hits. The pattern that returned
the highest number of hits determined the feature
value (e.g., “web-protein”, or “web-RNA”). If no
hits were returned by any pattern, a value “O-web”
was assigned. This value was also assigned to all
words whose frequency was higher than 10 (using
yet another value for words with higher frequency
did not improve the tagger’s performance).
</bodyText>
<sectionHeader confidence="0.473465" genericHeader="method">
2.2.4 Abstracts
</sectionHeader>
<bodyText confidence="0.999907416666667">
A number of NER systems have made effective use
of how the same token was tagged in different parts
of the same document (see (Curran and Clark, 2003)
and (Mikheev et al., 1999)). A token which appears
in an unindicative context in one sentence may ap-
pear in a very obvious context in another sentence
in the same abstract. To leverage this we tagged
each abstract twice, providing for each token a fea-
ture indicating whether it was tagged as an entity
elsewhere in the abstract. This information was
only useful when combined with information on fre-
quency.
</bodyText>
<subsectionHeader confidence="0.992155">
2.3 Deeper Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999571315789474">
While the local features discussed earlier are all
fairly surface level, our system also makes use of
deeper syntactic features. We fully parsed the train-
ing and testing data using the Stanford Parser of
(Klein and Manning, 2003) operating on the TnT
part-of-speech tagging – we believe that the un-
lexicalized nature of this parser makes it a partic-
ularly suitable statistical parser to use when there
is a large domain mismatch between the training
material (Wall Street Journal text) and the target
domain, but have not yet carefully evaluated this.
Then, for each word in the sentence which is in-
side a noun phrase, the head and governor of the
noun phrase are extracted. These features are not
very useful when identifying only two classes (such
as GENE and OTHER in the BioCreative task), but
they were quite useful for this task because of the
large number of classes which the system needed to
distinguish between. Because the classifier is now
</bodyText>
<page confidence="0.998998">
89
</page>
<bodyText confidence="0.999960473684211">
choosing between classes where members can look
very similar, longer distance information can pro-
vide a better representation of the context in which
the word appears. For instance, the word phospho-
rylation occurs in the training corpus 492 times, 482
of which it is was classified as other. However,
it is the governor of 738 words, of which 443 are
protein, 292 are other and only 3 are cell
line.
We also made use of abbreviation matching to
help ensure consistency of labels. Abbreviations
and long forms were extracted from the data using
the method of (Schwartz and Hearst, 2003). This
data was combined with a list of other abbreviations
and long forms extracted from the BioCreative 2004
task. Then all occurrences of either the long or short
forms in the data was labeled. These labels were in-
cluded in the system as features and helped to im-
prove boundary detection.
</bodyText>
<subsectionHeader confidence="0.995301">
2.4 Adjacent Entities
</subsectionHeader>
<bodyText confidence="0.999995882352941">
When training our classifier, we merged the B- and
I- labels for each class, so it did not learn how to
differentiate between the first word of a class and
internal word. There were several motivations for
doing this. Foremost was memory concerns; our fi-
nal system trained on just the six classes had 1.5
million features – we just did not have the resources
to train it over more classes without giving up many
of our features. Our second motivation was that by
merging the beginning and internal labels for a par-
ticular class, the classifier would see more examples
of that class and learn better how to identify it. The
drawback of this move is that when two entities be-
longing to the same class are adjacent, our classifier
will automatically merge them into one entity. We
did attempt to split them back up using NP chunks,
but this severely reduced performance.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999980666666667">
Our results on the evaluation data and a confusion
matrix are shown in Tables 2 and 4. Table 4 sug-
gests areas for further work. Collapsing the B- and
I- tags does cost us quite abit. Otherwise confusions
between some named entity and being nothing are
most of the errors, although protein/DNA and cell-
line/cell-type confusions are also noticeable.
Analysis of performance in biomedical Named
Entity Recognition tends to be dominated by the
perceived poorness of the results, stemming from
the twin beliefs that performance of roughly ninety
percent is the state-of-the-art and that performance
of 100% (or close to that) is possible and the goal
to be aimed for. Both of these beliefs are ques-
tionable, as the top MUC 7 performance of 93.39%
</bodyText>
<table confidence="0.999915909090909">
Entity Precision Recall F-Score
Fully Correct
protein 77.40% 68.48% 72.67%
DNA 66.19% 69.62% 67.86%
RNA 72.03% 65.89% 68.83%
cell line 59.00% 47.12% 52.40%
cell type 62.62% 76.97% 69.06%
Overall 71.62% 68.56% 70.06%
Left Boundary Correct
protein 82.89% 73.34% 77.82%
DNA 68.47% 72.01% 70.19%
RNA 75.42% 68.99% 72.06%
cell line 63.80% 50.96% 56.66%
cell type 63.93% 78.57% 70.49%
Overall 75.72% 72.48% 74.07%
Right Boundary Correct
protein 84.70% 74.96% 79.53%
DNA 74.43% 78.29% 76.31%
RNA 78.81% 72.09% 75.30%
cell line 70.2% 56.07% 62.34%
cell type 71.68% 88.10% 79.05%
Overall 79.65% 76.24% 77.91%
</table>
<tableCaption confidence="0.99977">
Table 2: Results on the evaluation data
</tableCaption>
<bodyText confidence="0.9998248">
(Mikheev et al., 1998) in the domain of newswire
text used an easier performance metric where incor-
rect boundaries were given partial credit, while both
the biomedical NER shared tasks to date have used
an exact match criterion where one is doubly penal-
ized (both as a FP and as a FN) for incorrect bound-
aries. However, the difference in metric clearly can-
not account entirely for the performance discrep-
ancy between newswire NER and biomedical NER.
Biomedical NER appears to be a harder task due
to the widespread ambiguity of terms out of con-
text, the complexity of medical language, and the
apparent need for expert domain knowledge. These
are problems that more sophisticated machine learn-
ing systems using resources such as ontologies and
deep processing might be able to overcome. How-
ever, one should also consider the inherent “fuzzi-
ness” of the classification task. The few existing
studies of inter-annotator agreement for biomedi-
cal named entities have measured agreement be-
tween 87%(Hirschman, 2003) and 89%(Demetrious
and Gaizauskas, 2003). As far as we know there
are no inter-annotator agreement results for the GE-
NIA corpus, and it is necessary to have such results
before properly evaluating the performance of sys-
tems. In particular, the fact that BioNLP sought to
distinguish between gene and protein names, when
these are known to be systematically ambiguous,
and when in fact in the GENIA corpus many enti-
ties were doubly classified as “protein molecule or
</bodyText>
<page confidence="0.989038">
90
</page>
<table confidence="0.999114923076923">
gold\ans B- DNA B- RNA cell line cell type protein I- O
I- I- B- I- B- I- B-
B-DNA 723 39 0 0 1 0 0 0 154 1 138
I-DNA 52 1390 0 0 0 0 0 0 19 71 257
B-RNA 1 0 89 3 0 0 0 0 14 0 11
I-RNA 0 1 5 164 0 0 0 0 2 0 15
B-cell line 3 0 0 0 319 41 37 5 12 1 82
I-cell line 0 6 0 0 24 713 5 104 0 14 123
B-cell type 1 0 0 0 164 22 1228 90 31 5 380
I-cell type 0 0 0 0 13 383 88 2101 8 27 371
B-protein 48 5 10 3 20 1 19 3 4200 192 566
I-protein 6 66 0 11 0 10 2 25 245 3630 779
O 170 240 25 26 85 142 184 132 1042 656 78945
</table>
<tableCaption confidence="0.98952">
Table 3: Our confusion matrix over the evaluation data
</tableCaption>
<table confidence="0.99081675">
human B-cell type
monocytes I-cell type
human O
monocytes B-cell type
macrophages B-cell type
primary B-cell type JJ
T I-cell type NN
lymphocytes I-cell type NNS
primary O JJ
peripheral B-cell type JJ
blood I-cell type NN
lymphocytes I-cell type NNS
</table>
<tableCaption confidence="0.99964">
Table 4: Examples of annotation inconsistencies
</tableCaption>
<bodyText confidence="0.999876470588235">
region” and “DNA molecule or region”, suggests
that inter-annotator agreement could be low, and
that many entities in fact have more than one classi-
fication.
One area where GENIA appears inconsistent is
in the labeling of preceding adjectives. The data
was selected by querying for the term human, yet
the term is labeled inconsistently, as is shown in Ta-
ble 4. Of the 1790 times the term human occurred
before or at the beginning of an entity in the train-
ing data, it was not classified as part of the entity
110 times. In the test data, there is only on instance
(out of 130) where the term is excluded. Adjectives
are excluded approximately 25% of the time in both
the training and evaluation data. There are also in-
consistencies when two entities are separated by the
word and.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="conclusions">
4 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9290692">
This paper is based on work supported in part by a
Scottish Enterprise Edinburgh-Stanford Link Grant
(R36759), as part of the SEER project, and in part
the National Science Foundation under the Knowl-
edge Discovery and Dissemination program.
</bodyText>
<sectionHeader confidence="0.998942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99994030952381">
Thorsten Brants. 2000. TnT – a statistical part-of-speech
tagger. In ANLP 6, pages 224–231.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the Seventh Conference on Natural
Language Learning (CoNLL-03), pages 164–167.
George Demetrious and Rob Gaizauskas. 2003. Corpus
resources for development and evaluation of a biolog-
ical text mining system. In Proceedings of the Third
Meeting of the Special Interest Group on Text Mining,
Brisbane, Australia, July.
Shipra Dingare, Jenny Rose Finkel, Christopher Man-
ning, Malvina Nissim, and Beatrice Alex. 2004. Ex-
ploring the boundaries: Gene and protein identifica-
tion in biomedical text. In Proceedings of the BioCre-
ative Workshop.
Lynette Hirschman. 2003. Using biological resources to
bootstrap text mining.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 41, pages 423–430.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In CoNLL 7, pages 180–
183.
Andrei Mikheev, Claire Grover, and Mark Moens. 1998.
Description of the LTG system used for MUC-7. In
Proceedings ofMUC-7.
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers. In Pro-
ceedings of the ninth conference on European chap-
ter of the Association for Computational Linguistics,
pages 1–8. Association for Computational Linguis-
tics.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun’ichi
Tsujii. 2002. GENIA corpus: an annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of he Human Language Technology Confer-
ence, pages 73–77.
Ariel Schwartz and Marti Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocomput-
ing, Kauai, Jan.
</reference>
<page confidence="0.999173">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.110388">
<title confidence="0.8304445">Exploiting Context for Biomedical Entity From Syntax to the Web</title>
<author confidence="0.8911645">Shipra Manning Finkel</author>
<author confidence="0.8911645">Gail</author>
<affiliation confidence="0.7290615">Department of Computer Stanford</affiliation>
<address confidence="0.73977">Stanford, CA</address>
<note confidence="0.734828">United</note>
<email confidence="0.959411">@cs.stanford.edu</email>
<abstract confidence="0.979793444444444">We describe a machine learning system for the recognition of names in biomedical texts. The system makes extensive use of local and syntactic features within the text, as well as external resources including the web and gazetteers. It achieves an Fscore of 70% on the Coling 2004 NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In ANLP 6,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="3552" citStr="Brants, 2000" startWordPosition="553" endWordPosition="554">escribing the immediate content and context of each word, including the word itself, the previous and next words, word prefixes and suffix of up to a length of 6 characters, word shapes, and features describing the named entity tags assigned to the previous words. Word shapes refer to a mapping of each word onto equivalence classes that encodes attributes such as length, capitalization, numerals, greek letters, and so on. For instance, “Varicella-zoster” would become Xxxxx, “mRNA” would become xXXX, and “CPA1” would become XXXd. We also incorporated part-ofspeech tagging, using the TnT tagger(Brants, 2000) retrained on the GENIA corpus gold standard partof-speech tagging. We also used various interaction terms (conjunctions) of these base-level features in various ways. The full set of local features is outlined in Table 1. 2.2 External Resources We made use of a number of external resources, including gazetteers, web-querying, use of the surrounding abstract, and frequency counts from the British National Corpus. 88 Word Features wi, wi−1, wi+1 Disjunction of 5 prev words Disjunction of 5 next words TnT POS POSi, POSi−1, POSi+1 Prefix/suffix Up to a length of 6 Abbreviations abbri abbri−1 + ab</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-of-speech tagger. In ANLP 6, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent NER using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03),</booktitle>
<pages>164--167</pages>
<contexts>
<context position="7076" citStr="Curran and Clark, 2003" startWordPosition="1156" endWordPosition="1159">ations of each pattern to the web, using the Google API, and obtained the number of hits. The pattern that returned the highest number of hits determined the feature value (e.g., “web-protein”, or “web-RNA”). If no hits were returned by any pattern, a value “O-web” was assigned. This value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the tagger’s performance). 2.2.4 Abstracts A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al., 1999)). A token which appears in an unindicative context in one sentence may appear in a very obvious context in another sentence in the same abstract. To leverage this we tagged each abstract twice, providing for each token a feature indicating whether it was tagged as an entity elsewhere in the abstract. This information was only useful when combined with information on frequency. 2.3 Deeper Syntactic Features While the local features discussed earlier are all fairly surface level, our system also makes use of deeper syntactic features. We fully parsed the training and </context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Language independent NER using a maximum entropy tagger. In Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03), pages 164–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Demetrious</author>
<author>Rob Gaizauskas</author>
</authors>
<title>Corpus resources for development and evaluation of a biological text mining system.</title>
<date>2003</date>
<booktitle>In Proceedings of the Third Meeting of the Special Interest Group on Text Mining,</booktitle>
<location>Brisbane, Australia,</location>
<contexts>
<context position="12694" citStr="Demetrious and Gaizauskas, 2003" startWordPosition="2097" endWordPosition="2100">wswire NER and biomedical NER. Biomedical NER appears to be a harder task due to the widespread ambiguity of terms out of context, the complexity of medical language, and the apparent need for expert domain knowledge. These are problems that more sophisticated machine learning systems using resources such as ontologies and deep processing might be able to overcome. However, one should also consider the inherent “fuzziness” of the classification task. The few existing studies of inter-annotator agreement for biomedical named entities have measured agreement between 87%(Hirschman, 2003) and 89%(Demetrious and Gaizauskas, 2003). As far as we know there are no inter-annotator agreement results for the GENIA corpus, and it is necessary to have such results before properly evaluating the performance of systems. In particular, the fact that BioNLP sought to distinguish between gene and protein names, when these are known to be systematically ambiguous, and when in fact in the GENIA corpus many entities were doubly classified as “protein molecule or 90 gold\ans B- DNA B- RNA cell line cell type protein I- O I- I- B- I- B- I- BB-DNA 723 39 0 0 1 0 0 0 154 1 138 I-DNA 52 1390 0 0 0 0 0 0 19 71 257 B-RNA 1 0 89 3 0 0 0 0 14</context>
</contexts>
<marker>Demetrious, Gaizauskas, 2003</marker>
<rawString>George Demetrious and Rob Gaizauskas. 2003. Corpus resources for development and evaluation of a biological text mining system. In Proceedings of the Third Meeting of the Special Interest Group on Text Mining, Brisbane, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shipra Dingare</author>
<author>Jenny Rose Finkel</author>
<author>Christopher Manning</author>
<author>Malvina Nissim</author>
<author>Beatrice Alex</author>
</authors>
<title>Exploring the boundaries: Gene and protein identification in biomedical text.</title>
<date>2004</date>
<booktitle>In Proceedings of the BioCreative Workshop.</booktitle>
<contexts>
<context position="2482" citStr="Dingare et al., 2004" startWordPosition="372" endWordPosition="375">es to accomplish this task. We describe our system in detail and also discuss some sources of error. 2 System Description Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the †Institute for Communicating and Collaborative Systems University of Edinburgh Edinburgh EH8 9LW United Kingdom {sdingar1|mnissim|csincla1} @inf.ed.ac.uk CoNLL 2003 shared task (Klein et al., 2003) and the 2004 BioCreative critical assessment of information extraction systems, a task that involved identifying gene and protein name mentions but not distinguishing between them (Dingare et al., 2004). Unlike the above two tasks, many of the entities in the current task do not have good internal cues for distinguishing the class of entity: various systematic polysemies and the widespread use of acronyms mean that internal cues are lacking. The challenge was thus to make better use of contextual features, including local and syntactic features, and external resources in order to succeed at this task. 2.1 Local Features We used a variety of features describing the immediate content and context of each word, including the word itself, the previous and next words, word prefixes and suffix of u</context>
</contexts>
<marker>Dingare, Finkel, Manning, Nissim, Alex, 2004</marker>
<rawString>Shipra Dingare, Jenny Rose Finkel, Christopher Manning, Malvina Nissim, and Beatrice Alex. 2004. Exploring the boundaries: Gene and protein identification in biomedical text. In Proceedings of the BioCreative Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>Using biological resources to bootstrap text mining.</title>
<date>2003</date>
<contexts>
<context position="12653" citStr="Hirschman, 2003" startWordPosition="2094" endWordPosition="2095">ce discrepancy between newswire NER and biomedical NER. Biomedical NER appears to be a harder task due to the widespread ambiguity of terms out of context, the complexity of medical language, and the apparent need for expert domain knowledge. These are problems that more sophisticated machine learning systems using resources such as ontologies and deep processing might be able to overcome. However, one should also consider the inherent “fuzziness” of the classification task. The few existing studies of inter-annotator agreement for biomedical named entities have measured agreement between 87%(Hirschman, 2003) and 89%(Demetrious and Gaizauskas, 2003). As far as we know there are no inter-annotator agreement results for the GENIA corpus, and it is necessary to have such results before properly evaluating the performance of systems. In particular, the fact that BioNLP sought to distinguish between gene and protein names, when these are known to be systematically ambiguous, and when in fact in the GENIA corpus many entities were doubly classified as “protein molecule or 90 gold\ans B- DNA B- RNA cell line cell type protein I- O I- I- B- I- B- I- BB-DNA 723 39 0 0 1 0 0 0 154 1 138 I-DNA 52 1390 0 0 0 </context>
</contexts>
<marker>Hirschman, 2003</marker>
<rawString>Lynette Hirschman. 2003. Using biological resources to bootstrap text mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL 41,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="7743" citStr="Klein and Manning, 2003" startWordPosition="1268" endWordPosition="1271"> appears in an unindicative context in one sentence may appear in a very obvious context in another sentence in the same abstract. To leverage this we tagged each abstract twice, providing for each token a feature indicating whether it was tagged as an entity elsewhere in the abstract. This information was only useful when combined with information on frequency. 2.3 Deeper Syntactic Features While the local features discussed earlier are all fairly surface level, our system also makes use of deeper syntactic features. We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging – we believe that the unlexicalized nature of this parser makes it a particularly suitable statistical parser to use when there is a large domain mismatch between the training material (Wall Street Journal text) and the target domain, but have not yet carefully evaluated this. Then, for each word in the sentence which is inside a noun phrase, the head and governor of the noun phrase are extracted. These features are not very useful when identifying only two classes (such as GENE and OTHER in the BioCreative task), but they were quite useful for this</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In ACL 41, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Joseph Smarr</author>
<author>Huy Nguyen</author>
<author>Christopher D Manning</author>
</authors>
<title>Named entity recognition with character-level models.</title>
<date>2003</date>
<journal>In CoNLL</journal>
<volume>7</volume>
<pages>180--183</pages>
<contexts>
<context position="2279" citStr="Klein et al., 2003" startWordPosition="342" endWordPosition="345">ine, and cell type in the GENIA corpus of MEDLINE abstracts (Ohta et al., 2002). In this paper we describe a machine learning system incorporating a diverse set of features and various external resources to accomplish this task. We describe our system in detail and also discuss some sources of error. 2 System Description Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the †Institute for Communicating and Collaborative Systems University of Edinburgh Edinburgh EH8 9LW United Kingdom {sdingar1|mnissim|csincla1} @inf.ed.ac.uk CoNLL 2003 shared task (Klein et al., 2003) and the 2004 BioCreative critical assessment of information extraction systems, a task that involved identifying gene and protein name mentions but not distinguishing between them (Dingare et al., 2004). Unlike the above two tasks, many of the entities in the current task do not have good internal cues for distinguishing the class of entity: various systematic polysemies and the widespread use of acronyms mean that internal cues are lacking. The challenge was thus to make better use of contextual features, including local and syntactic features, and external resources in order to succeed at t</context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D. Manning. 2003. Named entity recognition with character-level models. In CoNLL 7, pages 180– 183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
<author>Claire Grover</author>
<author>Mark Moens</author>
</authors>
<title>Description of the LTG system used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings ofMUC-7.</booktitle>
<contexts>
<context position="11665" citStr="Mikheev et al., 1998" startWordPosition="1931" endWordPosition="1934">rect protein 77.40% 68.48% 72.67% DNA 66.19% 69.62% 67.86% RNA 72.03% 65.89% 68.83% cell line 59.00% 47.12% 52.40% cell type 62.62% 76.97% 69.06% Overall 71.62% 68.56% 70.06% Left Boundary Correct protein 82.89% 73.34% 77.82% DNA 68.47% 72.01% 70.19% RNA 75.42% 68.99% 72.06% cell line 63.80% 50.96% 56.66% cell type 63.93% 78.57% 70.49% Overall 75.72% 72.48% 74.07% Right Boundary Correct protein 84.70% 74.96% 79.53% DNA 74.43% 78.29% 76.31% RNA 78.81% 72.09% 75.30% cell line 70.2% 56.07% 62.34% cell type 71.68% 88.10% 79.05% Overall 79.65% 76.24% 77.91% Table 2: Results on the evaluation data (Mikheev et al., 1998) in the domain of newswire text used an easier performance metric where incorrect boundaries were given partial credit, while both the biomedical NER shared tasks to date have used an exact match criterion where one is doubly penalized (both as a FP and as a FN) for incorrect boundaries. However, the difference in metric clearly cannot account entirely for the performance discrepancy between newswire NER and biomedical NER. Biomedical NER appears to be a harder task due to the widespread ambiguity of terms out of context, the complexity of medical language, and the apparent need for expert dom</context>
</contexts>
<marker>Mikheev, Grover, Moens, 1998</marker>
<rawString>Andrei Mikheev, Claire Grover, and Mark Moens. 1998. Description of the LTG system used for MUC-7. In Proceedings ofMUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
<author>Claire Grover</author>
</authors>
<title>Named entity recognition without gazetteers.</title>
<date>1999</date>
<booktitle>In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7103" citStr="Mikheev et al., 1999" startWordPosition="1161" endWordPosition="1164"> web, using the Google API, and obtained the number of hits. The pattern that returned the highest number of hits determined the feature value (e.g., “web-protein”, or “web-RNA”). If no hits were returned by any pattern, a value “O-web” was assigned. This value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the tagger’s performance). 2.2.4 Abstracts A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al., 1999)). A token which appears in an unindicative context in one sentence may appear in a very obvious context in another sentence in the same abstract. To leverage this we tagged each abstract twice, providing for each token a feature indicating whether it was tagged as an entity elsewhere in the abstract. This information was only useful when combined with information on frequency. 2.3 Deeper Syntactic Features While the local features discussed earlier are all fairly surface level, our system also makes use of deeper syntactic features. We fully parsed the training and testing data using the Stan</context>
</contexts>
<marker>Mikheev, Moens, Grover, 1999</marker>
<rawString>Andrei Mikheev, Marc Moens, and Claire Grover. 1999. Named entity recognition without gazetteers. In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Hideki Mima</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus: an annotated research abstract corpus in molecular biology domain.</title>
<date>2002</date>
<booktitle>In Proceedings of he Human Language Technology Conference,</booktitle>
<pages>73--77</pages>
<contexts>
<context position="1739" citStr="Ohta et al., 2002" startWordPosition="260" endWordPosition="263">hallenging in the biomedical domain with its highly complex and idiosyncratic language. With the increasing use of shared tasks and shared evaluation procedures (e.g., the recent BioCreative, TREC, and KDD Cup), it is rapidly becoming clear that performance in this domain is markedly lower than the field has come to expect from the standard domain of newswire. The Coling 2004 shared task focuses on the problem of Named Entity Recognition, requiring participating systems to identify the five named entities of protein, RNA, DNA, cell line, and cell type in the GENIA corpus of MEDLINE abstracts (Ohta et al., 2002). In this paper we describe a machine learning system incorporating a diverse set of features and various external resources to accomplish this task. We describe our system in detail and also discuss some sources of error. 2 System Description Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the †Institute for Communicating and Collaborative Systems University of Edinburgh Edinburgh EH8 9LW United Kingdom {sdingar1|mnissim|csincla1} @inf.ed.ac.uk CoNLL 2003 shared task (Klein et al., 2003) and the 2004 BioCreative critical assessment of information</context>
</contexts>
<marker>Ohta, Tateisi, Mima, Tsujii, 2002</marker>
<rawString>Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun’ichi Tsujii. 2002. GENIA corpus: an annotated research abstract corpus in molecular biology domain. In Proceedings of he Human Language Technology Conference, pages 73–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Schwartz</author>
<author>Marti Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In Pacific Symposium on Biocomputing, Kauai,</booktitle>
<contexts>
<context position="9054" citStr="Schwartz and Hearst, 2003" startWordPosition="1493" endWordPosition="1496">etween. Because the classifier is now 89 choosing between classes where members can look very similar, longer distance information can provide a better representation of the context in which the word appears. For instance, the word phosphorylation occurs in the training corpus 492 times, 482 of which it is was classified as other. However, it is the governor of 738 words, of which 443 are protein, 292 are other and only 3 are cell line. We also made use of abbreviation matching to help ensure consistency of labels. Abbreviations and long forms were extracted from the data using the method of (Schwartz and Hearst, 2003). This data was combined with a list of other abbreviations and long forms extracted from the BioCreative 2004 task. Then all occurrences of either the long or short forms in the data was labeled. These labels were included in the system as features and helped to improve boundary detection. 2.4 Adjacent Entities When training our classifier, we merged the B- and I- labels for each class, so it did not learn how to differentiate between the first word of a class and internal word. There were several motivations for doing this. Foremost was memory concerns; our final system trained on just the s</context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel Schwartz and Marti Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. In Pacific Symposium on Biocomputing, Kauai, Jan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>