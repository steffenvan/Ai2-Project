<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000086">
<title confidence="0.958668">
Extracting Sense-Disambiguated Example Sentences
From Parallel Corpora
</title>
<author confidence="0.646836">
Gerard de Melo
</author>
<affiliation confidence="0.5288475">
Max Planck Institute for Informatics
Saarbrucken, Germany
</affiliation>
<email confidence="0.524803">
demelo@mpi-inf.mpg.de
</email>
<sectionHeader confidence="0.968848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999212045454545">
Example sentences provide an intuitive means of
grasping the meaning of a word, and are fre-
quently used to complement conventional word
definitions. When a word has multiple meanings,
it is useful to have example sentences for spe-
cific senses (and hence definitions) of that word
rather than indiscriminately lumping all of them
together. In this paper, we investigate to what
extent such sense-specific example sentences can
be extracted from parallel corpora using lexical
knowledge bases for multiple languages as a sense
index. We use word sense disambiguation heuris-
tics and a cross-lingual measure of semantic sim-
ilarity to link example sentences to specific word
senses. From the sentences found for a given
sense, an algorithm then selects a smaller sub-
set that can be presented to end users, taking
into account both representativeness and diver-
sity. Preliminary results show that a precision
of around 80% can be obtained for a reasonable
number of word senses, and that the subset se-
lection yields convincing results.
</bodyText>
<sectionHeader confidence="0.981457" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.9104345">
Example Sentence Extraction, Parallel Corpora, Disambiguar
tion, Lexical Databases
</keyword>
<sectionHeader confidence="0.99851" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.975739842857143">
Many dictionaries provide not only definitions but also
short sentences that demonstrate how a given word is
used in context. Linguists and average dictionary users
alike appreciate genuine examples of a word being em-
ployed in a sentence.
Goal An example sentence for a word sense is any
genuine sentence that contains that word being used
in the respective sense. A set of example sentences
may (1) allow the user to grasp a word’s meaning, and
(2) see in what circumstances a word would typically
be used in practice.
The first aspect is relevant because traditional in-
tensional word definitions may be too abstract or even
confusing to users of a dictionary. Often, the meaning
of a word can be determined from its context. In con-
junction with conventional definitions, example sen-
tences may allow users to verify whether they have
correctly interpreted a definition.
Gerhard Weikum
Max Planck Institute for Informatics
Saarbrucken, Germany
weikum@mpi-inf.mpg.de
The second aspect is relevant since example sentences
may reveal possible contexts a word can be used in.
For instance, synonymous words such as ‘child’ and
‘youngster’ can have the same meaning, yet be used in
somewhat different contexts. Examples provide evi-
dence of typical collocations and expressions, e.g. the
word ‘birth’ often occurs as in ‘to give birth’ or ‘birth rate’
(but not *‘to give nascence’ or *‘nascence rate’).
For this reason, dictionaries typically include not
only conventional definitions, but also example sen-
tences that convey additional information about the
meaning of a word. These are often short, limited in
number, and in some dictionaries elicited rather than
genuine. Hence, retrieving further example sentences
can be helpful for lexicographical purposes, or to make
the meanings and use more clear to language learners
and other laypeople. In modern digital dictionaries,
the tight space constraints of print media no longer
apply, and thus a larger number of example sentences
can be presented to the user on demand.
Our aim is to automatically obtain a set of sense-
disambiguated example sentences that are known to
mention a specific sense of a word. For instance, for
a polysemous word such as ‘bat’, we would like to ob-
tain a set of example sentences that refer to the animal
sense (e.g. ‘There were many bats flying out of the cave.’),
and, separately, a list of example sentences that men-
tion the word in its sports sense (e.g. ‘In professional
baseball, only wooden bats are permitted.’).
When a user browses a digital dictionary or lex-
ical database, the example sentences could then be
provided together with the relevant definitions of the
word. Even in digital media, however, more exam-
ples may be available than can initially be displayed.
For this reason, a means of choosing a restricted set
of particularly representative example sentences is an
additional requirement.
Contribution Our approach consists of two major
building blocks that address the two issues just de-
scribed. The first step (Section 2) involves extract-
ing the sense-disambiguated example sentences from
a parallel corpus by harnessing cross-lingual informa-
tion to aid in assigning sentences to word senses. The
second step (Section 3) selects a limited set of partic-
ularly representative example sentences for each word
sense, using an algorithm that assesses the contribu-
tions made by individual sentences. We provide pre-
liminary experimental results in Section 5.
</bodyText>
<page confidence="0.984813">
40
</page>
<subsubsectionHeader confidence="0.307759">
Workshop On Definition Extraction 2009 - Borovets, Bulgaria, pages 40–46,
</subsubsectionHeader>
<sectionHeader confidence="0.805001" genericHeader="method">
2 Example Extraction
</sectionHeader>
<bodyText confidence="0.999062051282051">
In the example extraction step, we connect sentences
from a corpus to word senses in a given sense inven-
tory whenever we are sufficiently confident that the
sentence is an example of the corresponding word be-
ing used in the respective sense.
Conventional word sense disambiguation heuristics
could be used to determine word senses for a mono-
lingual text, and then the sentences in that text could
be linked to the respective senses. Unfortunately, even
the most sophisticated all-words disambiguation tech-
niques are currently not reliable enough when a fine-
grained sense inventory is used [14].
The intuition behind our method is that, given a
parallel text that has been word aligned, we can jointly
look at both versions of the text and determine the
most likely senses of certain words with significantly
greater accuracy than for any single version of the text.
After word alignment, we independently apply word
sense disambiguation heuristics for each of the lan-
guages to obtain ranked lists of senses for each word.
One then analyses to what degree the ranked lists for
aligned words overlap. In many cases, this makes it
possible to infer the sense of a word much more reli-
ably than with conventional disambiguation heuristics.
In such a case, we can use the respective sentence in
which it occurs as an example sentence for that sense.
Lexical Alignment In the past, parallel corpora
had been rather difficult to obtain. This has changed
with the increasing multilinguality of the Web as well
as the greater demand for such resources resulting
from the rise of statistical machine translation. Resnik
and Smith [15] showed that the Web can be mined
to obtain parallel corpora, while Tiedemann [21] built
such corpora from sources such as movie subtitles and
manuals of open source software.
To compare the senses of words in both versions of
a text, such parallel corpora first need to be word-
aligned. This means that occurrences of terms (in-
dividual words or possibly lexicalized multi-word ex-
pressions) in one language need to be connected to the
corresponding occurrences of semantically equivalent
terms in the document for the other language.
This is usually accomplished by first aligning sen-
tences, and then using global cooccurrence-based
statistics to connect words of two corresponding sen-
tences. Superficial similarities between words and
part-of-speech information provide additional clues.
We rely on pre-existing tools to perform this align-
ment, as will be explained in Section 5.
Disambiguation An important prerequisite for our
approach is the existence of a word sense database.
This resource must provide a fairly complete listing of
word senses for a given word in any of the languages
involved. We use the WordNet lexical database for
the English language and the Spanish WordNet for
the Spanish language (see Section 5).
Our system iterates over the sentences in the par-
allel corpus, simultaneously looking at two different
languages a, b. Whenever an occurrence of a word ta
in a is aligned with a word tb in b, and ta is believed
to be linked to a word sense sa with a sufficiently high
confidence score, we make the sentence where ta was
found an example sentence of sa.
The confidence score is assigned as follows:
The auxiliary function σ(t) yields the set of all
senses associated with t in the sense inventory,
and σ(t, s) is the corresponding indicator function
(σ(t, s) = 1 if s E σ(t) and 0 otherwise).
In practice, looking up the possible senses of a
word requires a morphological analysis to obtain lem-
matized forms of words and determine their part-of-
speech. We also rely on a look-ahead window to detect
multi-word expressions occurring in the text that have
their own sense identifier in the sense knowledge base.
The function csim(tb, sa) measures the cross-lingual
similarity between the likely senses of a term tb in
language b and a specific sense sa for the word from
language a:
</bodyText>
<equation confidence="0.9850085">
�csim(tb, sa) = sim(sa, sb) wsd(tb, sb) (1)
sbEv(tb)
</equation>
<bodyText confidence="0.983341388888889">
These functions build on a monolingual word sense dis-
ambiguation function wsd(t, s) and a sense similarity
measure sim(s1, s2).
Monolingual Word Sense Disambiguation The
wsd(t, s) function provides an initial monolingual dis-
ambiguation by measuring the similarity between the
context of t in the corpus and a similar contextual
string created for the sense s. For the former we use
the current sentence being disambiguated (which con-
tains t). The latter is created by concatenating glosses
and terms associated with the sense s itself or with
senses s&apos; directly related via hyponymy, holonymy,
derivation, or instance relations, or via up to 2 levels
of hypernymy. These context strings are stemmed us-
ing the Porter algorithm [13], and feature vectors v(t),
v(s) with term frequency values are created based on
the bag-of-words vector space model. The result is
then computed as
</bodyText>
<equation confidence="0.997811666666667">
wsd(t, s) = σ(t, s) �
α + v(s)Tv(t) (2)
||v(s)  |v(t)||
</equation>
<bodyText confidence="0.999741083333333">
Unlike standard word sense disambiguation setups, we
prefer obtaining a weighted set of multiple possibly rel-
evant senses rather than just the sense with the highest
confidence score. We use α as a smoothing parame-
ter: For higher values of α, the function tends towards
a uniform distribution of scores among the relevant
senses, i.e. among those with σ(t, s) = 1.
Semantic Similarity For the semantic similarity
measure, we do not rely on generic measures of se-
mantic relatedness often described in the literature [1].
The purpose of this measure here is to identify only
word senses that are identical or nearly identical (e.g.
</bodyText>
<equation confidence="0.99475875">
σ(ta, sa)csim(tb, sa)
score(ta, sa) = wsd(ta, sa)
E σ(ta, s&apos;)csim(tb, s&apos;)
s&apos;Ev(te.)
</equation>
<page confidence="0.987733">
41
</page>
<bodyText confidence="0.999945333333333">
the senses for ‘house’ and ‘home’) rather than arbitrary
forms of association (e.g. between ‘house’ and ‘door’).
We use the following relatedness measure:
</bodyText>
<equation confidence="0.994814">
1 s1 = s2
1 s1, s2 in near-synonymy relationship
1 s1, s2 in hypernymy relationship
1 s1, s2 in hyponymy relationship
0 otherwise
</equation>
<bodyText confidence="0.9978675">
The relational information between senses used here is
provided by WordNet.
</bodyText>
<sectionHeader confidence="0.985609" genericHeader="method">
3 Example Selection
</sectionHeader>
<bodyText confidence="0.999819043478261">
For computational applications, obtaining a repository
of perhaps several hundred or even thousand examples
for a single word sense can be useful. When displaying
examples to human users, it is often better to provide
a limited selection at first. The challenge then is de-
ciding which sentences to choose.
We assume there is a space constraint in form of a
limit k on the number of sentences that shall be pre-
sented to the user. Given a possibly large number of
example sentences for a specific word sense, we must
choose up to k example sentences that showcase typi-
cal contextual collocations and thereby aid the user in
discerning the meaning and use of a term.
Assets Each example sentence can be thought of as
having certain assets in this respect. For example,
for the financial sense of the word ‘account’, the fact
that an example sentence contains the bigram ‘bank ac-
count’ could be considered an asset. Another sentence
may contain the commonly used expression ‘open an
account’.
Our approach looks at 7 different sets of assets (in
our case, neighbourhood n-grams) for each example
sentence x associated with a word sense.
</bodyText>
<listItem confidence="0.997483352941176">
• A1m(x): the original unigram word occurrences
for which the example is provided, e.g. ‘account’
or ‘accounts’ (note that there might be different
word forms, and additionally, in WordNet, mul-
tiple synonymous words can in fact be associated
with a single word sense identifier)
• A3 m(x): word 3-grams incorporating a preceding
and a following word, e.g. ‘bank account number’
• A2p(x): word 2-grams incorporating previous
words, e.g. ‘bank account’
• A3p(x): word 3-grams incorporating previous
words, e.g. ‘open an account’
• A2f(x): word 2-grams incorporating following
words, e.g. ‘account manager’
• A3 f(x): word 3-grams incorporating following
words, e.g. ‘account number is’
• A*m(x): the entire sentence
</listItem>
<bodyText confidence="0.999941464285714">
For each of these n-gram sets A1m, A3m, A2p, etc., we
also consider the corresponding counter function a1m,
a3m, a2p, etc., that counts how often the n-gram oc-
curs in the example sentence in the respective relative
position. Usually, this will either be 0 or 1, though
an example sentence may also contain multiple occur-
rences of the word being described, so higher values
do occur. Note that in the above use of the words uni-
gram and n-gram, if the original word being described
is a multi-word-expression, it is only counted as one
word, e.g. when considering examples for the multi-
word expression ‘bank account’ instead of just ‘account’,
the sequence ‘opening a bank account’ would be consid-
ered a 3-gram.
Our aim will be to choose example sentences that
provide representative examples of each of these n-
gram sets, so each asset will be given a weight. A*m(x),
which contains the entire sentence, is a special case
where we define w(a) for a E A*m(x) to be the cosine
similarity with the gloss context string, as for the word
sense disambiguation in Section 2. These weights bias
our selection towards example sentences that more
clearly reflect the meaning of the word. Apart from
this, each n-gram is given a weight based on its relative
frequency within the set. For instance, with respect to
A3p, a frequent expressions like ‘open an account’ should
receive a much higher weight than ‘Peter’s chequing ac-
count’. For an n-gram a in the set A1m(a), we assign
</bodyText>
<equation confidence="0.86674775">
a weight w(a) =
a—1
(x,a) and equivalently for the
��i=1 a(xi,a)
</equation>
<bodyText confidence="0.981578625">
other n-gram asset sets A3m(x), A2p(x), etc.
Objective Of course, at this point one could sim-
ply select the top k sentences with respect to the total
weight of the n-grams they have as assets. Such an
approach however is likely to lead to a very homoge-
nous result set: n-grams with a high weight occur in
many sentences, and hence could easily dominate the
ranking.
Instead, we define the goal as follows: Given a set
of assets A (in our case, n-grams), a set of items
X = {x1, ... , xn} (in our case, example sentences),
each associated with specific assets A(xi) C A (in our
case, the union of n-grams returned by A1m, A3m, A2 p,
etc.), and a limit k, the goal is to choose a set C of
items with cardinality |C |&lt; k such that the total
weight of the assets
</bodyText>
<equation confidence="0.998282666666667">
E w(a) (3)
aE U A(x)
xEC
</equation>
<bodyText confidence="0.962314272727273">
is maximized.
While this formalization aims at ensuring that items
with highly weighted assets occur in the example set,
e.g. a sentence containing ‘open an account’, it also en-
forces a certain level of diversity. The latter is achieved
by counting the weight of each asset only once, thus
if one sentence includes ‘open an account’, then there is
no direct benefit for including a second sentence with
that same n-gram.
The goal can equivalently be expressed in an integer
linear program formalization as follows. Define
</bodyText>
<equation confidence="0.961382461538462">
1 a E A(xi)
a�(xi, a) =0 otherwise.
�
���� �
�����
sim(s1, s2) =
42
Our objective is then:
maximize caw(a)
a
s.t. ca &lt; cx1a&apos;(x1, a) + ··· + cx„a&apos;(x,, a)
cxi + ··· + cx„ &lt; k
ca,cxi E {0, 1}
</equation>
<bodyText confidence="0.999710142857143">
This means that we wish to maximize the weight of
the assets (n-grams) with ca = 1, where ca can only
be 1 if an appropriate cxi = 1, i.e. an appropriate item
(example sentence) xi is chosen for the result set.
We use a greedy heuristic to find solutions, since the
problem is NP-hard.
Proof. We prove the NP-hardness by reducing the NP-
hard vertex cover problem to our setting. Given a
graph G = (V, E) and a positive integer k, the vertex
cover problem consists in determining whether a set
of vertices C of size at most k exists, such that each
e E E is incident to at least one v E C. Now set n =
|V  |and define the items x0, ... , x,,, to be the vertices
v E V . Further, define A = E as the set of assets and
A(xi) as the set of edges incident to xi. Give these
edges uniform weights w(e) = 1. Having determined k
items that maximize Equation 3, we can then simply
test whether the score is equal to |E|. If it is, then
obviously there exists a set of at most k vertices such
that every edge e E E is covered. If not, then no
vertex cover with at most k vertices can exist, because
otherwise we could choose that vertex cover as the
set of items and obtain a higher objective score (since
more edges would be covered). Hence, any vertex cover
problem could be answered using an exact algorithm
for our problem setting.
Approach The algorithm we use (Algorithm 3.1) re-
lies on a simple greedy heuristic. It repeatedly chooses
the highest-ranked sentence x E X given the current
asset weights w, then resets the weights w(a) of all as-
sets a E A(x) to zero to ensure that they are no longer
considered when choosing further sentences. Ties can
be broken arbitrarily (in practice, we first compare the
disambiguation scores from Section 2 and choose the
highest one).
</bodyText>
<listItem confidence="0.863293">
Algorithm 3.1 Sentence Selection algorithm
1: procedure SELECT(X, k, w)
2: C &lt;-- 0
3: while |C |&lt; k n |X |&gt; 0 do
4: x &lt;-- argmax E w(a)
</listItem>
<equation confidence="0.6977076">
xEX\C aEA(x)
5: C &lt;-- C U {x}
s: for all a E A(x) do
7: w(a) &lt;-- 0
8: return C
</equation>
<bodyText confidence="0.999337875">
Prior to running the algorithm, an additional filtering
may be used. For instance, one may filter out examples
that are too long or too short (e.g. incomplete phrases
or headlines and titles). One could also allow hiding
sentences with possibly offensive or vulgar language.
If the number of example sentences is too large to
do a linear scan of all sentences (e.g. in the case of
highly frequent words such as conjunctions), we may
also choose to let the algorithm run on a smaller ran-
dom sample X&apos; C X of sentences as input.
A useful feature of this greedy algorithm is that it
allows emitting a ranked list of entities. Having run
the algorithm for a large k, perhaps even k = oo, we
can easily obtain the respective output for any k&apos; &lt; k
simply by pruning the ranked list generated for k. This
can be very useful for interactive user interfaces.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999761037735849">
Several means of generating example sentences for
word senses have been proposed. Shinnou et al. [19]
extract example sentences for a word from a corpus
and attempt to distinguish senses by passing human-
labelled sentences as input to a clustering algorithm.
This method requires significant human involvement
and unlike our approach does not disambiguate senses
with respect to a specific sense inventory.
Chklovski and Mihalcea [2] presented a Web inter-
face that asks Web users to tag sentences with the cor-
rect word sense and relies on active learning methods
to select sentences that are hard to tag automatically.
A different approach suggested by Mihalcea [10]
finds example sentences by using a set of seed expres-
sions to create appropriate queries to Web search en-
gines. For example, for the fibre optic channel sense
of word ‘channel’, appropriate queries would be ‘optical
fiber channel’, ‘channel telephone’, ‘transmission channel’.
This method works well when such multi-word con-
structions can be constructed and could be used to
complement our approach.
Another more recent approach [11] clusters words
based on a dependency parse of a monolingual corpus.
This means that for each word a set of similar words is
available. One then tries to match example sentences
from the corpus with example sentences already given
in WordNet, taking into account the word similarities.
Our approach uses a different strategy by relying
on parallel corpora. The intuition that lexical ambi-
guities in parallel corpora can be resolved more easily
has been used by a number of works on word sense
disambiguation. Dagan et al. [3] provided an initial
linguistic analysis of this hypothesis. Several studies
[9, 5, etc.] then implemented this idea in word sense
disambiguation algorithms. These approaches are sim-
ilar to our work. They use simple heuristics on par-
allel corpora to arrive at sense-labelled data that can
then be used for word sense disambiguation, while our
approach relies on a word sense heuristic to create ex-
ample sentences from a parallel corpus.
With regards to the challenge of selecting the most
valuable examples, Fujii et al. [8] proposed a method
for choosing example sentences for word sense disam-
biguation systems. Unlike our approach, which aims at
representative examples for end users, their approach
aims at examples likely to be useful for training a dis-
ambiguation system. Their proposal selects example
sentences that are hard to classify automatically due to
the associated uncertainty, so particularly clear exam-
ples of a word’s use are in fact less likely to get elected.
Rychly et al. [17] presented a semi-supervised selec-
tion system that learns scores based on combinations
of weak classifiers. These classifiers rely on features
</bodyText>
<page confidence="0.999134">
43
</page>
<table confidence="0.998429">
Corpus Covered Example Accuracy
Senses Sentences (Wilson interval)
OpenSubtitles English-Spanish 13,559 117,078 0.815 f 0.081
OpenSubtitles Spanish-English 8,833 113,018 0.798 f 0.090
OpenOffice.org English-Spanish 1,341 13,295 0.803 f 0.081
OpenOffice.org Spanish-English 932 11,181 0.793 f 0.087
</table>
<tableCaption confidence="0.999633">
Table 1: Number and Accuracy of sense-disambiguated example sentences
</tableCaption>
<bodyText confidence="0.999907">
such as word lists, sentence/word length, keyword po-
sition, etc. Since the system does not take into ac-
count diversity when generating a selection, it would
be interesting to combine our algorithm with the scores
from their classifiers as additional assets.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999035">
We conducted preliminary experiments on multiple
corpora to evaluate the usefulness of our approach.
</bodyText>
<subsectionHeader confidence="0.897675">
5.1 Resources
</subsectionHeader>
<bodyText confidence="0.9997981">
In terms of parallel corpora, we relied on parts of the
OPUS collection [21], in particular the OpenSubtitles
[22] and the OpenOffice.org corpora. We made use
of GIZA++ [12] and Uplug [20] to produce the word
alignments for these corpora. Additionally, we evalu-
ated example sentence selection for undisambiguated
sentences using a subset of the Reuters RCV1 corpus
[16], consisting of 39,351 documents.
The following lexical knowledge bases were used to
build up the sense inventory:
</bodyText>
<listItem confidence="0.91193125">
• The original Princeton WordNet 3.0 [7] for the
English language.
• The Spanish WordNet jointly developed by three
research groups in Spain [6]. Since it was created
</listItem>
<bodyText confidence="0.8999558">
in alignment with WordNet 1.6, we applied sense
mappings [4] to obtain sense identifiers aligned
with the version 3.0 of WordNet.
When linking words in the corpus to this inventory, the
TreeTagger [18] was used for morphological analysis.
</bodyText>
<subsectionHeader confidence="0.984038">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999976785714286">
We generated sense-disambiguated example sentences
for several setups, and evaluated random samples by
assessing whether or not the word was indeed used
in the sense determined by our method. The results
were generalized using Wilson score intervals, and are
presented in Table 1. The smoothing parameter α
from Section 2 was set to 0.3. In Table 2, we provide
a few anecdotic examples of the output.
In general, this approach yields high-quality exam-
ple sentences compared to current systems for mono-
lingual text [14]. Automatic word alignment is known
to be error-prone, and many heuristics have been pro-
posed to mitigate the effects of this, e.g. aligning in
both directions and then intersecting the alignment.
In our setting, incorrect alignments are unlikely to
lead to incorrect example sentences. This is because
two erroneously aligned words in most cases have very
different meanings and hence are unlikely to share a
semantically similar word sense.
The main cause of the inaccuracies we encountered
instead turned out to be the sense inventory’s incom-
pleteness. For instance, when an English word has
multiple senses shared by the aligned Spanish word,
but the sense inventory only lists one of those senses
for the Spanish word, our method would lead us to
believe that that sense is the right one with high cer-
tainty. On a few occasions, incorrect output by the
morphological analyser induced errors. For example,
when the word ‘shed’ was labelled a verb although it
was used as a noun, the wrong sense was selected.
A drawback of our approach is that the number of
word senses covered is limited. To some degree, this
can be addressed by using larger corpora and more
language combinations. A reasonably full level of cov-
erage of the senses listed in WordNet would however
likely also require relaxing the scoring functions to take
into account also less obvious (and hence less reliable)
input sentences.
We also applied the sentence selection approach de-
scribed in Section 3. Table 3 provides ranked lists
of example sentences created using Algorithm 3.1. It
is clear that frequent collocations such as ‘right side’,
‘electrical current’, and ‘when nightfall comes’ are given a
high weight. We also see at least one example sentence
wrongly associated with a sense (‘convey’). Since the
algorithm does not depend on sense-disambiguated ex-
ample sentences, we additionally show sentences from
the monolingual RCV1 corpus in Table 4. A larger
number of example sentences is typically available
here, so the algorithm suceeds even better at choos-
ing sentences that highlight typical collocations, e.g.
‘long term’, ‘a long time’ for the word ‘long’, or ‘colonial
rule’ and ‘colonial power’ for ‘colonial’. The RCV1 cor-
pus is strongly biased towards the financial domain,
which is reflected in the example sentences chosen by
the algorithm.
</bodyText>
<sectionHeader confidence="0.996957" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99989725">
We have presented a framework for extracting sense-
disambiguated example sentences from parallel cor-
pora and selecting limited numbers of sentences given
space constraints.
In the future, we plan on exploiting alignments with
additional languages by using additional versions of
WordNet. This would be particularly useful for pairs
of languages that are phylogenetically unrelated, as
these are more likely to have different patterns of
homonymy, and hence a word in one language is less
likely to share more than one meaning with a word in
the other language.
</bodyText>
<page confidence="0.993606">
44
</page>
<table confidence="0.894296272727273">
line (something, as a cord or rope, that is long I got some fishing line if you want me to
and thin and flexible) stitch that.
Von Sefelt, get the stern line.
line (the descendants of one individual) What line of kings do you descend from?
My line has ended.
catch (catch up with and possibly overtake) He’s got 100 laps to catch Beau Brandenburg
if he wants to become world champion.
They won’t catch up.
catch (grasp with the mind or develop I didn’t catch your name.
an understanding of) Sorry, I didn’t catch it.
talk (exchange thoughts, talk with) Why don’t we have a seat and talk it over.
Okay I’ll talk to you but one condition...
talk (use language) But we’ll be listening from the kitchen so talk
loud.
You spit when you talk.
opening (a ceremony accompanying the start We don’t have much time until the opening
of some enterprise) day of Exhibition.
What a disaster tomorrow is the opening
ceremony!
opening (the first performance, as of a It will be rehearsed in the morning ready for
theatrical production) the opening tomorrow night.
You ready for our big opening night?
</table>
<tableCaption confidence="0.863199">
Table 2: Samples of Sense-Disambiguated Example Sentences from the OpenSubtitles Corpus (in some cases
with multiple words for a single sense identifier)
</tableCaption>
<bodyText confidence="0.999870625">
The approach could also be extended to simultane-
ously consider aligned sentences from more than two
languages to harness example sentences when individ-
ual alignments of two languages do not provide enough
information for a reliable disambiguation.
For sentence selection, one could consider investi-
gating additional input information for the algorithm,
e.g. sentence lengths.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941175675676">
[1] A. Budanitsky and G. Hirst. Evaluating wordnet-based mea-
sures of lexical semantic relatedness. Computational Linguis-
tics, 32(1):13–47, 2006.
[2] T. Chklovski and R. Mihalcea. Building a sense tagged corpus
with open mind word expert. In Proc. ACL 2002 Workshop
on Word Sense Disambiguation, pages 116–122, Morristown,
NJ, USA, 2002. Association for Computational Linguistics.
[3] I. Dagan and A. Itai. Two languages are more informative than
one. In Proc. ACL 1991, pages 130–137, 1991.
[4] J. Daude, L. Padro, and G. Rigau. Making wordnet mappings
robust. In Proc. 19th Congreso de la Sociedad Espafiola para
el Procesamiento del Lenguage Natural (SEPLN), Universi-
dad de Alcala de Henares. Madrid, Spain, 2003.
[5] M. Diab. An unsupervised method for multilingual word sense
tagging using parallel corpora: a preliminary investigation.
In Proc. ACL 2000 Workshop on Word Senses and Multi-
linguality, pages 1–9, Morristown, NJ, USA, 2000. Association
for Computational Linguistics.
[6] J. Farreres, G. Rigau, and H. Rodriguez. Using wordnet for
building wordnets. In Proc. COLING-ACL Workshop on Us-
age of WordNet in Natural Language Processing Systems,
Montral, Canada, 1998.
[7] C. Fellbaum, editor. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication). The MIT
Press, 1998.
[8] A. Fujii, T. Tokunaga, K. Inui, and H. Tanaka. Selective sam-
pling for example-based word sense disambiguation. Compu-
tational Linguistics, 24(4):573–597, 1998.
[9] W. A. Gale, K. W. Church, and D. Yarowsky. Using bilin-
gual materials to develop word sense disambiguation meth-
ods. In Proc. 4th International Conference on Theoretical
and Methodological Issues in Machine Translation: Empiri-
cist vs. Rationalist Methods in MT, pages 101–112, Montreal,
Canada, 1992.
[10] R. Mihalcea. Bootstrapping large sense tagged corpora. In
Proc. 3rd International Conference on Language Resources
and Evaluation (LREC), Las Palmas, 2002.
[11] B. Z. Nik Adilah Hanin and F. Fukumoto. Example-assignment
to wordnet thesaurus based on clustering of similar words.
IPSJ SIG Notes, 2008(46):59–64.
[12] F. J. Och and H. Ney. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19–51, 2003.
[13] M. F. Porter. An algorithm for suffix stripping. Program,
14(3):130–137, 1980.
[14] S. Pradhan, E. Loper, D. Dligach, and M. Palmer. Semeval-
2007 task-17: English lexical sample, srl and all words. In
Proc. 4th International Workshop on Semantic Evaluations
(SemEval-2007), pages 87–92, Prague, Czech Republic, June
2007. Association for Computational Linguistics.
[15] P. Resnik and N. A. Smith. The web as a parallel corpus.
Comput. Linguist., 29(3):349–380, 2003.
[16] Reuters. Reuters Corpus, vol. 1: English Language, 1996-08-20
to 1997-08-19, 2000.
[17] P. Rychly, M. Husak, A. Kilgarriff, M. Rundell, and
K. McAdam. GDEX: automatically finding good dictionary
examples in a corpus. In Proc. XIII EURALEX International
Congress, Barcelona, Spain, 2008.
[18] H. Schmid. Probabilistic part-of-speech tagging using decision
trees. In Intl. Conference on New Methods in Language Pro-
cessing, Manchester, UK, 1994.
[19] H. Shinnou and M. Sasaki. Division of example sentences based
on the meaning of a target word using semi-supervised cluster-
ing. In Proc. 6th International Conference on Language Re-
sources and Evaluation (LREC 2008), Marrakech, Morocco,
2008.
[20] J. Tiedemann. Combining clues for word alignment. In Proc.
EACL 2003, pages 339–346, Morristown, NJ, USA, 2003. As-
sociation for Computational Linguistics.
[21] J. Tiedemann. The OPUS corpus - parallel &amp; free. In Proc. 4th
International Conference on Language Resources and Eval-
uation (LREC 2004), 2004.
[22] J. Tiedemann. Improved sentence alignment for movie subti-
tles. In Proc. RANLP 2007, 2007.
</reference>
<page confidence="0.998249">
45
</page>
<bodyText confidence="0.992366">
being or located on or In America we drive on the right side of the road.
directed toward the side of I’ll tie down your right arm so you can learn to throw a
the body to the east when left.
facing north If we wait from the right side, we have an advantage there.
put up with something or You can’t stand it can you?
somebody unpleasant You really think I can tolerate such an act?
No one can stand that harmonica all day long.
using or providing or Not the electric chair.
producing or transmitting or Some electrical current circulating through my body.
operated by electricity Near as I can tell it’s an electrical impulse.
take something or somebody And they were kind enough to take me in here.
with oneself somewhere It conveys such a great feeling.
We interrupt this program to bring you a special news
bulletin.
the time of day immediately When nightfall comes go get dressed for the show.
following sunset You have until dusk to give yourselves up.
At dusk they return loaded with fish.
</bodyText>
<tableCaption confidence="0.976002">
Table 3: Example Sentence Rankings (OpenSubtitles Corpus)
</tableCaption>
<bodyText confidence="0.999781151515152">
long In the long term interest rate market, the yield of the key 182nd 10 year
Japanese government bond (JGB) fell to 2.060 percent early on Tuesday, a
record low for any benchmark 10-year JGB.
“The government and opposition have gambled away the last chance for a long
time to prove they recognise the country’s problems, and that they put the
national good above their own power interests”, news weekly Der Spiegel said.
As long as the index keeps hovering between 957 and 995, we will maintain
our short term neutral recommendation.
colonial Hong Kong came to the end of 156 years of British colonial rule on June 30
and is now an autonomous capitalist region of China, running all its own
affairs except defence and diplomacy.
The letter was sent in error to the embassy of Portugal – the former colonial
power in East Timor – and was neither returned nor forwarded to the
Indonesian embassy.
Sino-British relations hit a snag when former Governor Chris Patten launched
electoral reforms in the twilight years of colonial rule despite fierce opposition
by Beijing.
purchase Romania’s State Ownership Fund (FPS), the country’s main privatisation
body, said on Wednesday it had accepted five bids for the purchase of a 50.98
percent stake in the largest local cement maker Romcim.
Grand Hotel Group said on Wednesday it has agreed to procure an option to
purchase the remaining 50 percent of the Grand Hyatt complex in Melbourne
from hotel developer and investor Lustig &amp; Moar.
The purchase price for the business, which had 1996 calendar year sales of
about $25 million, was not disclosed.
gold Coach Ian Stacker said his team had hoped to meet the US in the gold medal
play offs, but because of an early loss to Turkey the team did not get the draw
they had counted on.
He said India’s exports of gold and silver jewellery were worth $600 million
annually against world trade of about $20 billion.
In the bullion market spot gold was quoted at $323.80/30 early compared to
the London morning fix of $324.05 and the New York close Friday of
$324.40/90.
</bodyText>
<tableCaption confidence="0.97948">
Table 4: Example Sentence Rankings (RCV1 Corpus)
</tableCaption>
<page confidence="0.999355">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.217017">
<title confidence="0.9826495">Extracting Sense-Disambiguated Example From Parallel Corpora</title>
<author confidence="0.968121">Gerard de_Max Planck Institute for</author>
<email confidence="0.391928">Saarbrucken,</email>
<abstract confidence="0.999932826086957">Example sentences provide an intuitive means of grasping the meaning of a word, and are frequently used to complement conventional word definitions. When a word has multiple meanings, it is useful to have example sentences for specific senses (and hence definitions) of that word rather than indiscriminately lumping all of them together. In this paper, we investigate to what extent such sense-specific example sentences can be extracted from parallel corpora using lexical knowledge bases for multiple languages as a sense index. We use word sense disambiguation heuristics and a cross-lingual measure of semantic similarity to link example sentences to specific word senses. From the sentences found for a given sense, an algorithm then selects a smaller subset that can be presented to end users, taking into account both representativeness and diversity. Preliminary results show that a precision of around 80% can be obtained for a reasonable number of word senses, and that the subset selection yields convincing results.</abstract>
<keyword confidence="0.9861945">Keywords Example Sentence Extraction, Parallel Corpora, Disambiguar</keyword>
<intro confidence="0.631788">tion, Lexical Databases</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="10294" citStr="[1]" startWordPosition="1665" endWordPosition="1665">he result is then computed as wsd(t, s) = σ(t, s) � α + v(s)Tv(t) (2) ||v(s) |v(t)|| Unlike standard word sense disambiguation setups, we prefer obtaining a weighted set of multiple possibly relevant senses rather than just the sense with the highest confidence score. We use α as a smoothing parameter: For higher values of α, the function tends towards a uniform distribution of scores among the relevant senses, i.e. among those with σ(t, s) = 1. Semantic Similarity For the semantic similarity measure, we do not rely on generic measures of semantic relatedness often described in the literature [1]. The purpose of this measure here is to identify only word senses that are identical or nearly identical (e.g. σ(ta, sa)csim(tb, sa) score(ta, sa) = wsd(ta, sa) E σ(ta, s&apos;)csim(tb, s&apos;) s&apos;Ev(te.) 41 the senses for ‘house’ and ‘home’) rather than arbitrary forms of association (e.g. between ‘house’ and ‘door’). We use the following relatedness measure: 1 s1 = s2 1 s1, s2 in near-synonymy relationship 1 s1, s2 in hypernymy relationship 1 s1, s2 in hyponymy relationship 0 otherwise The relational information between senses used here is provided by WordNet. 3 Example Selection For computational ap</context>
</contexts>
<marker>[1]</marker>
<rawString>A. Budanitsky and G. Hirst. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>R Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with open mind word expert.</title>
<date>2002</date>
<booktitle>In Proc. ACL 2002 Workshop on Word Sense Disambiguation,</booktitle>
<pages>116--122</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="19027" citStr="[2]" startWordPosition="3224" endWordPosition="3224">ily obtain the respective output for any k&apos; &lt; k simply by pruning the ranked list generated for k. This can be very useful for interactive user interfaces. 4 Related Work Several means of generating example sentences for word senses have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a set of seed expressions to create appropriate queries to Web search engines. For example, for the fibre optic channel sense of word ‘channel’, appropriate queries would be ‘optical fiber channel’, ‘channel telephone’, ‘transmission channel’. This method works well when such multi-word constructions can be constructed and could be</context>
</contexts>
<marker>[2]</marker>
<rawString>T. Chklovski and R. Mihalcea. Building a sense tagged corpus with open mind word expert. In Proc. ACL 2002 Workshop on Word Sense Disambiguation, pages 116–122, Morristown, NJ, USA, 2002. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proc. ACL</booktitle>
<pages>130--137</pages>
<contexts>
<context position="20219" citStr="[3]" startWordPosition="3419" endWordPosition="3419">nd could be used to complement our approach. Another more recent approach [11] clusters words based on a dependency parse of a monolingual corpus. This means that for each word a set of similar words is available. One then tries to match example sentences from the corpus with example sentences already given in WordNet, taking into account the word similarities. Our approach uses a different strategy by relying on parallel corpora. The intuition that lexical ambiguities in parallel corpora can be resolved more easily has been used by a number of works on word sense disambiguation. Dagan et al. [3] provided an initial linguistic analysis of this hypothesis. Several studies [9, 5, etc.] then implemented this idea in word sense disambiguation algorithms. These approaches are similar to our work. They use simple heuristics on parallel corpora to arrive at sense-labelled data that can then be used for word sense disambiguation, while our approach relies on a word sense heuristic to create example sentences from a parallel corpus. With regards to the challenge of selecting the most valuable examples, Fujii et al. [8] proposed a method for choosing example sentences for word sense disambiguat</context>
</contexts>
<marker>[3]</marker>
<rawString>I. Dagan and A. Itai. Two languages are more informative than one. In Proc. ACL 1991, pages 130–137, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daude</author>
<author>L Padro</author>
<author>G Rigau</author>
</authors>
<title>Making wordnet mappings robust.</title>
<date>2003</date>
<booktitle>In Proc. 19th Congreso de la Sociedad Espafiola para el Procesamiento del Lenguage Natural (SEPLN), Universidad de Alcala de Henares.</booktitle>
<location>Madrid,</location>
<contexts>
<context position="22825" citStr="[4]" startWordPosition="3819" endWordPosition="3819">s [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When linking words in the corpus to this inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method. The results were generalized using Wilson score intervals, and are presented in Table 1. The smoothing parameter α from Section 2 was set to 0.3. In Table 2, we provide a few anecdotic examples of the output. In g</context>
</contexts>
<marker>[4]</marker>
<rawString>J. Daude, L. Padro, and G. Rigau. Making wordnet mappings robust. In Proc. 19th Congreso de la Sociedad Espafiola para el Procesamiento del Lenguage Natural (SEPLN), Universidad de Alcala de Henares. Madrid, Spain, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
</authors>
<title>An unsupervised method for multilingual word sense tagging using parallel corpora: a preliminary investigation.</title>
<date>2000</date>
<booktitle>In Proc. ACL 2000 Workshop on Word Senses and Multilinguality,</booktitle>
<pages>1--9</pages>
<location>Morristown, NJ, USA,</location>
<marker>[5]</marker>
<rawString>M. Diab. An unsupervised method for multilingual word sense tagging using parallel corpora: a preliminary investigation. In Proc. ACL 2000 Workshop on Word Senses and Multilinguality, pages 1–9, Morristown, NJ, USA, 2000. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Farreres</author>
<author>G Rigau</author>
<author>H Rodriguez</author>
</authors>
<title>Using wordnet for building wordnets.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<location>Montral, Canada,</location>
<contexts>
<context position="22742" citStr="[6]" startWordPosition="3805" endWordPosition="3805">ora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When linking words in the corpus to this inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method. The results were generalized using Wilson score intervals, and are presented in Table 1. The smoothing parameter α from Section 2 </context>
</contexts>
<marker>[6]</marker>
<rawString>J. Farreres, G. Rigau, and H. Rodriguez. Using wordnet for building wordnets. In Proc. COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems, Montral, Canada, 1998.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The</booktitle>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="22638" citStr="[7]" startWordPosition="3788" endWordPosition="3788"> on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When linking words in the corpus to this inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method. The results were generaliz</context>
</contexts>
<marker>[7]</marker>
<rawString>C. Fellbaum, editor. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fujii</author>
<author>T Tokunaga</author>
<author>K Inui</author>
<author>H Tanaka</author>
</authors>
<title>Selective sampling for example-based word sense disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="20743" citStr="[8]" startWordPosition="3504" endWordPosition="3504">s been used by a number of works on word sense disambiguation. Dagan et al. [3] provided an initial linguistic analysis of this hypothesis. Several studies [9, 5, etc.] then implemented this idea in word sense disambiguation algorithms. These approaches are similar to our work. They use simple heuristics on parallel corpora to arrive at sense-labelled data that can then be used for word sense disambiguation, while our approach relies on a word sense heuristic to create example sentences from a parallel corpus. With regards to the challenge of selecting the most valuable examples, Fujii et al. [8] proposed a method for choosing example sentences for word sense disambiguation systems. Unlike our approach, which aims at representative examples for end users, their approach aims at examples likely to be useful for training a disambiguation system. Their proposal selects example sentences that are hard to classify automatically due to the associated uncertainty, so particularly clear examples of a word’s use are in fact less likely to get elected. Rychly et al. [17] presented a semi-supervised selection system that learns scores based on combinations of weak classifiers. These classifiers </context>
</contexts>
<marker>[8]</marker>
<rawString>A. Fujii, T. Tokunaga, K. Inui, and H. Tanaka. Selective sampling for example-based word sense disambiguation. Computational Linguistics, 24(4):573–597, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proc. 4th International Conference on Theoretical and Methodological Issues in Machine Translation: Empiricist vs. Rationalist Methods in MT,</booktitle>
<pages>101--112</pages>
<location>Montreal, Canada,</location>
<marker>[9]</marker>
<rawString>W. A. Gale, K. W. Church, and D. Yarowsky. Using bilingual materials to develop word sense disambiguation methods. In Proc. 4th International Conference on Theoretical and Methodological Issues in Machine Translation: Empiricist vs. Rationalist Methods in MT, pages 101–112, Montreal, Canada, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Bootstrapping large sense tagged corpora.</title>
<date>2002</date>
<booktitle>In Proc. 3rd International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="19260" citStr="[10]" startWordPosition="3264" endWordPosition="3264"> have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a set of seed expressions to create appropriate queries to Web search engines. For example, for the fibre optic channel sense of word ‘channel’, appropriate queries would be ‘optical fiber channel’, ‘channel telephone’, ‘transmission channel’. This method works well when such multi-word constructions can be constructed and could be used to complement our approach. Another more recent approach [11] clusters words based on a dependency parse of a monolingual corpus. This means that for each word a set of similar words is available. One then tries to match exampl</context>
</contexts>
<marker>[10]</marker>
<rawString>R. Mihalcea. Bootstrapping large sense tagged corpora. In Proc. 3rd International Conference on Language Resources and Evaluation (LREC), Las Palmas, 2002.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Z Nik Adilah Hanin</author>
<author>F Fukumoto</author>
</authors>
<title>Example-assignment to wordnet thesaurus based on clustering of similar words.</title>
<journal>IPSJ SIG Notes,</journal>
<volume>2008</volume>
<issue>46</issue>
<contexts>
<context position="19694" citStr="[11]" startWordPosition="3331" endWordPosition="3331"> with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a set of seed expressions to create appropriate queries to Web search engines. For example, for the fibre optic channel sense of word ‘channel’, appropriate queries would be ‘optical fiber channel’, ‘channel telephone’, ‘transmission channel’. This method works well when such multi-word constructions can be constructed and could be used to complement our approach. Another more recent approach [11] clusters words based on a dependency parse of a monolingual corpus. This means that for each word a set of similar words is available. One then tries to match example sentences from the corpus with example sentences already given in WordNet, taking into account the word similarities. Our approach uses a different strategy by relying on parallel corpora. The intuition that lexical ambiguities in parallel corpora can be resolved more easily has been used by a number of works on word sense disambiguation. Dagan et al. [3] provided an initial linguistic analysis of this hypothesis. Several studie</context>
</contexts>
<marker>[11]</marker>
<rawString>B. Z. Nik Adilah Hanin and F. Fukumoto. Example-assignment to wordnet thesaurus based on clustering of similar words. IPSJ SIG Notes, 2008(46):59–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="22287" citStr="[12]" startWordPosition="3734" endWordPosition="3734">curacy of sense-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of W</context>
</contexts>
<marker>[12]</marker>
<rawString>F. J. Och and H. Ney. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="9572" citStr="[13]" startWordPosition="1543" endWordPosition="1543">. Monolingual Word Sense Disambiguation The wsd(t, s) function provides an initial monolingual disambiguation by measuring the similarity between the context of t in the corpus and a similar contextual string created for the sense s. For the former we use the current sentence being disambiguated (which contains t). The latter is created by concatenating glosses and terms associated with the sense s itself or with senses s&apos; directly related via hyponymy, holonymy, derivation, or instance relations, or via up to 2 levels of hypernymy. These context strings are stemmed using the Porter algorithm [13], and feature vectors v(t), v(s) with term frequency values are created based on the bag-of-words vector space model. The result is then computed as wsd(t, s) = σ(t, s) � α + v(s)Tv(t) (2) ||v(s) |v(t)|| Unlike standard word sense disambiguation setups, we prefer obtaining a weighted set of multiple possibly relevant senses rather than just the sense with the highest confidence score. We use α as a smoothing parameter: For higher values of α, the function tends towards a uniform distribution of scores among the relevant senses, i.e. among those with σ(t, s) = 1. Semantic Similarity For the sem</context>
</contexts>
<marker>[13]</marker>
<rawString>M. F. Porter. An algorithm for suffix stripping. Program, 14(3):130–137, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>E Loper</author>
<author>D Dligach</author>
<author>M Palmer</author>
</authors>
<title>Semeval2007 task-17: English lexical sample, srl and all words.</title>
<date>2007</date>
<booktitle>In Proc. 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5453" citStr="[14]" startWordPosition="858" endWordPosition="858">Extraction In the example extraction step, we connect sentences from a corpus to word senses in a given sense inventory whenever we are sufficiently confident that the sentence is an example of the corresponding word being used in the respective sense. Conventional word sense disambiguation heuristics could be used to determine word senses for a monolingual text, and then the sentences in that text could be linked to the respective senses. Unfortunately, even the most sophisticated all-words disambiguation techniques are currently not reliable enough when a finegrained sense inventory is used [14]. The intuition behind our method is that, given a parallel text that has been word aligned, we can jointly look at both versions of the text and determine the most likely senses of certain words with significantly greater accuracy than for any single version of the text. After word alignment, we independently apply word sense disambiguation heuristics for each of the languages to obtain ranked lists of senses for each word. One then analyses to what degree the ranked lists for aligned words overlap. In many cases, this makes it possible to infer the sense of a word much more reliably than wit</context>
<context position="23538" citStr="[14]" startWordPosition="3933" endWordPosition="3933">s inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method. The results were generalized using Wilson score intervals, and are presented in Table 1. The smoothing parameter α from Section 2 was set to 0.3. In Table 2, we provide a few anecdotic examples of the output. In general, this approach yields high-quality example sentences compared to current systems for monolingual text [14]. Automatic word alignment is known to be error-prone, and many heuristics have been proposed to mitigate the effects of this, e.g. aligning in both directions and then intersecting the alignment. In our setting, incorrect alignments are unlikely to lead to incorrect example sentences. This is because two erroneously aligned words in most cases have very different meanings and hence are unlikely to share a semantically similar word sense. The main cause of the inaccuracies we encountered instead turned out to be the sense inventory’s incompleteness. For instance, when an English word has multi</context>
</contexts>
<marker>[14]</marker>
<rawString>S. Pradhan, E. Loper, D. Dligach, and M. Palmer. Semeval2007 task-17: English lexical sample, srl and all words. In Proc. 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague, Czech Republic, June 2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>N A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="6483" citStr="[15]" startWordPosition="1031" endWordPosition="1031">d. One then analyses to what degree the ranked lists for aligned words overlap. In many cases, this makes it possible to infer the sense of a word much more reliably than with conventional disambiguation heuristics. In such a case, we can use the respective sentence in which it occurs as an example sentence for that sense. Lexical Alignment In the past, parallel corpora had been rather difficult to obtain. This has changed with the increasing multilinguality of the Web as well as the greater demand for such resources resulting from the rise of statistical machine translation. Resnik and Smith [15] showed that the Web can be mined to obtain parallel corpora, while Tiedemann [21] built such corpora from sources such as movie subtitles and manuals of open source software. To compare the senses of words in both versions of a text, such parallel corpora first need to be wordaligned. This means that occurrences of terms (individual words or possibly lexicalized multi-word expressions) in one language need to be connected to the corresponding occurrences of semantically equivalent terms in the document for the other language. This is usually accomplished by first aligning sentences, and then </context>
</contexts>
<marker>[15]</marker>
<rawString>P. Resnik and N. A. Smith. The web as a parallel corpus. Comput. Linguist., 29(3):349–380, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reuters</author>
</authors>
<title>Reuters Corpus,</title>
<date>1996</date>
<volume>1</volume>
<institution>English Language,</institution>
<contexts>
<context position="22483" citStr="[16]" startWordPosition="3764" endWordPosition="3764">it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When linking words in the corpus to this inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several </context>
</contexts>
<marker>[16]</marker>
<rawString>Reuters. Reuters Corpus, vol. 1: English Language, 1996-08-20 to 1997-08-19, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rychly</author>
<author>M Husak</author>
<author>A Kilgarriff</author>
<author>M Rundell</author>
<author>K McAdam</author>
</authors>
<title>GDEX: automatically finding good dictionary examples in a corpus.</title>
<date>2008</date>
<booktitle>In Proc. XIII EURALEX International Congress,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="21217" citStr="[17]" startWordPosition="3580" endWordPosition="3580"> example sentences from a parallel corpus. With regards to the challenge of selecting the most valuable examples, Fujii et al. [8] proposed a method for choosing example sentences for word sense disambiguation systems. Unlike our approach, which aims at representative examples for end users, their approach aims at examples likely to be useful for training a disambiguation system. Their proposal selects example sentences that are hard to classify automatically due to the associated uncertainty, so particularly clear examples of a word’s use are in fact less likely to get elected. Rychly et al. [17] presented a semi-supervised selection system that learns scores based on combinations of weak classifiers. These classifiers rely on features 43 Corpus Covered Example Accuracy Senses Sentences (Wilson interval) OpenSubtitles English-Spanish 13,559 117,078 0.815 f 0.081 OpenSubtitles Spanish-English 8,833 113,018 0.798 f 0.090 OpenOffice.org English-Spanish 1,341 13,295 0.803 f 0.081 OpenOffice.org Spanish-English 932 11,181 0.793 f 0.087 Table 1: Number and Accuracy of sense-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does</context>
</contexts>
<marker>[17]</marker>
<rawString>P. Rychly, M. Husak, A. Kilgarriff, M. Rundell, and K. McAdam. GDEX: automatically finding good dictionary examples in a corpus. In Proc. XIII EURALEX International Congress, Barcelona, Spain, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Intl. Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK,</location>
<contexts>
<context position="22966" citStr="[18]" startWordPosition="3842" endWordPosition="3842">lly, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When linking words in the corpus to this inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method. The results were generalized using Wilson score intervals, and are presented in Table 1. The smoothing parameter α from Section 2 was set to 0.3. In Table 2, we provide a few anecdotic examples of the output. In general, this approach yields high-quality example sentences compared to current systems for monolingual text [14]. Automatic word alignment i</context>
</contexts>
<marker>[18]</marker>
<rawString>H. Schmid. Probabilistic part-of-speech tagging using decision trees. In Intl. Conference on New Methods in Language Processing, Manchester, UK, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shinnou</author>
<author>M Sasaki</author>
</authors>
<title>Division of example sentences based on the meaning of a target word using semi-supervised clustering.</title>
<date>2008</date>
<booktitle>In Proc. 6th International Conference on Language Resources and Evaluation (LREC 2008),</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="18696" citStr="[19]" startWordPosition="3174" endWordPosition="3174">.g. in the case of highly frequent words such as conjunctions), we may also choose to let the algorithm run on a smaller random sample X&apos; C X of sentences as input. A useful feature of this greedy algorithm is that it allows emitting a ranked list of entities. Having run the algorithm for a large k, perhaps even k = oo, we can easily obtain the respective output for any k&apos; &lt; k simply by pruning the ranked list generated for k. This can be very useful for interactive user interfaces. 4 Related Work Several means of generating example sentences for word senses have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a </context>
</contexts>
<marker>[19]</marker>
<rawString>H. Shinnou and M. Sasaki. Division of example sentences based on the meaning of a target word using semi-supervised clustering. In Proc. 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>Combining clues for word alignment.</title>
<date>2003</date>
<booktitle>In Proc. EACL 2003,</booktitle>
<pages>339--346</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="22302" citStr="[20]" startWordPosition="3737" endWordPosition="3737">-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When li</context>
</contexts>
<marker>[20]</marker>
<rawString>J. Tiedemann. Combining clues for word alignment. In Proc. EACL 2003, pages 339–346, Morristown, NJ, USA, 2003. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>The OPUS corpus - parallel &amp; free.</title>
<date>2004</date>
<booktitle>In Proc. 4th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="6565" citStr="[21]" startWordPosition="1045" endWordPosition="1045"> many cases, this makes it possible to infer the sense of a word much more reliably than with conventional disambiguation heuristics. In such a case, we can use the respective sentence in which it occurs as an example sentence for that sense. Lexical Alignment In the past, parallel corpora had been rather difficult to obtain. This has changed with the increasing multilinguality of the Web as well as the greater demand for such resources resulting from the rise of statistical machine translation. Resnik and Smith [15] showed that the Web can be mined to obtain parallel corpora, while Tiedemann [21] built such corpora from sources such as movie subtitles and manuals of open source software. To compare the senses of words in both versions of a text, such parallel corpora first need to be wordaligned. This means that occurrences of terms (individual words or possibly lexicalized multi-word expressions) in one language need to be connected to the corresponding occurrences of semantically equivalent terms in the document for the other language. This is usually accomplished by first aligning sentences, and then using global cooccurrence-based statistics to connect words of two corresponding s</context>
<context position="22190" citStr="[21]" startWordPosition="3719" endWordPosition="3719">,295 0.803 f 0.081 OpenOffice.org Spanish-English 932 11,181 0.793 f 0.087 Table 1: Number and Accuracy of sense-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet</context>
</contexts>
<marker>[21]</marker>
<rawString>J. Tiedemann. The OPUS corpus - parallel &amp; free. In Proc. 4th International Conference on Language Resources and Evaluation (LREC 2004), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>Improved sentence alignment for movie subtitles.</title>
<date>2007</date>
<booktitle>In Proc. RANLP</booktitle>
<contexts>
<context position="22228" citStr="[22]" startWordPosition="3724" endWordPosition="3724">ish-English 932 11,181 0.793 f 0.087 Table 1: Number and Accuracy of sense-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to</context>
</contexts>
<marker>[22]</marker>
<rawString>J. Tiedemann. Improved sentence alignment for movie subtitles. In Proc. RANLP 2007, 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>