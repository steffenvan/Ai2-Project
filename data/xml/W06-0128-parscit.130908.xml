<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012416">
<title confidence="0.995179">
Chinese Word Segmentation using Various Dictionaries
</title>
<author confidence="0.994919">
Guo-Wei Bian
</author>
<affiliation confidence="0.9486795">
Department of Information Management
Huafan University, Taiwan, R.O.C.
</affiliation>
<email confidence="0.989665">
gwbian@cc.hfu.edu.tw
</email>
<sectionHeader confidence="0.993642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993009375">
Most of the Chinese word segmentation
systems utilizes monolingual dictionary
and are used for monolingual processing.
For the tasks of machine translation (MT)
and cross-language information retrieval
(CLIR), another translation dictionary
may be used to transfer the words of
documents from the source languages to
target languages. The inconsistencies re-
sulting from the two types of dictionaries
(segmentation dictionary and transfer
dictionary) may produce some problems
for MT and CLIR. This paper shows the
effectiveness of the external resources
(bilingual dictionary and word list) for
Chinese word segmentations.
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974125">
Most of the Chinese word segmentations are
used for monolingual processing. In general, the
word segmentation program utilizes the word
entries, part-of-speech (POS) information (Chen
and Liu, 1992) in a monolingual dictionary,
segmentation rules (Palmer, 1997), and some
statistical information (Sproat, et al., 1994). For
the tasks of machine translation (MT) (Bian and
Chen, 1998) and cross-language information re-
trieval (CLIR) (Bian and Chen, 2000), another
translation dictionary may be used to transfer the
words of documents from the source languages
to target languages. Because of the inconsisten-
cies resulting from the two types of dictionaries
(segmentation dictionary and transfer dictionary),
this approach has the problems that some seg-
mented words cannot be found in the transfer
dictionary.
In this paper, we focus on the effectiveness of
the Chinese word segmentation using different
dictionaries. Four different dictionaries (or word
lists) and two different testing collections (testing
data) are used to evaluate the results of the Chi-
nese word segmentation.
</bodyText>
<sectionHeader confidence="0.932889" genericHeader="method">
2 Chinese Word Segmentation System
</sectionHeader>
<bodyText confidence="0.995597190476191">
The segmentation system used only the vari-
ous dictionaries in this design. In this paper, the
other possible resources (POS, segmentation
rules, word segmentation guide, and statistical
information) are ignored to test the average per-
formance between different testing collections
specially followed the different segmented
guidelines.
The longest-matching method is adopted in
this Chinese segmentation system. The segmen-
tation processing searches for a dictionary entry
corresponding to the longest sequence of Chinese
characters from left to right. The system pro-
vided the approximate matching to search a sub-
string of the input with the entry in the dictionary
if no total matching is found. For example, the
system will segment the input “看著隨時可能
結束生命的妹妹” as “看著 隨時 可能 結束
命 的 妹妹” which matched the term with the
entry “看著辦” in dictionary if no entry “看著 ”
found.
</bodyText>
<subsectionHeader confidence="0.988957">
2.1 Various Dictionaries
</subsectionHeader>
<bodyText confidence="0.999924277777778">
The word segmentation are evaluated using
different dictionaries (or word lists) and different
testing collections (testing data). There are four
dictionaries are used: the first one is converted
from an English-Chinese bilingual dictionary,
and the other three are extracted from the train-
ing corpora.
The original English-Chinese dictionary (Bian
and Chen, 1998), which containing about 67,000
English word entries, is converted to a new Chi-
nese-English dictionary (called CEDIC later).
There are 125,719 Chinese word entries in this
CEDIC.
The terms in the various training corpora (the
Sinica Corpus and the City University Corpus)
are extracted to build the different word lists as
the segmentation dictionaries (called CKIP and
CityU later). The tokens starting with the special
</bodyText>
<page confidence="0.946363">
166
</page>
<bodyText confidence="0.971228941176471">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 166–168,
Sydney, July 2006. c�2006 Association for Computational Linguistics
characters or punctuation marks are ignored.
The following shows some examples:
, , , , , , , , ─,
─ , , , ○○○, ..., ,
, , , , , .com,
Table 1 lists the number of tokens (#tokens),
the number of ignored tokens (#ignored), the
number of words (#words), and the unique words
(#unique) for each dictionaries. There are
140,971 unique words are extracted from the
training collection of Sinica Corpus, and 75,433
respected to the training set of the City
University Corpus. These two dictionaries are
combined to another dictionary which containing
174,398 unique words.
</bodyText>
<table confidence="0.9976212">
#Tokens #Ignored #Words #Unique
CKIP (CK) 5,468,793 894,686 4,574,107 140,971
CityU (CT) 1,643,421 257,032 1,386,389 75,433
CKIP+CityU 7,112,214 1,151,718 5,960,496 174,398
(CK + CT)
</table>
<tableCaption confidence="0.9414115">
Table 1. Statistical Information of the Extracted
Dictionaries
</tableCaption>
<sectionHeader confidence="0.996851" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.997123842105263">
To evaluate the results of Chinese word seg-
mentations, we implement 8 experiments (runs)
using the 4 different dictionaries (CEDIC, CK,
CT, and CK+CT) mentioned in previous section.
Two test collections (the Sinica Corpus and the
City University Corpus) are used to measure the
precision, recall, and an evenly-weighted F-
measure for the Chinese words segmentations.
Table 2 shows the F-measure of the
experimental results, and the Figure 1 illustrates
the comparisons of the segmentation perform-
ances. The symbol (*) indicates that the run is a
closed test, which only uses the training material
from the training data for the particular corpus.
We can find that the larger dictionary (CK+CT)
produces better segmentation results even the
word lists are combined from the different re-
sources (corpora) and followed the different
guidelines of word segmentations.
</bodyText>
<table confidence="0.994719333333333">
CEDIC CK + CT CK CT
CKIP 0.710 0.695 0.692* 0.611
CityU 0.481 0.589 0.547 0.513*
</table>
<tableCaption confidence="0.956893">
Table 2. The F-measure results of segmentation per-
formances using various dictionaries (*: closed test)
</tableCaption>
<figureCaption confidence="0.978374">
Figure 1. The comparison of segmentation perform-
ances using various dictionaries (*: close test)
</figureCaption>
<subsectionHeader confidence="0.7305815">
3.1 Error Analysis
3.1.1 Format Error of Result File
</subsectionHeader>
<bodyText confidence="0.999364529411765">
The results file for word segmentation is re-
quired to appear with one line for each sen-
tence/line in the test file with words and punctua-
tion separated by whitespace. Our system makes
some mistakes to produce no whitespace before
English terms and Arabic numbers, and produce
no whitespace after Chinese punctuation marks.
This formatting problem has made many adja-
cent segmented words to be evaluated as errors.
A sentence with such errors is listed below
The standard answer of the testing collection
(CityU) of the City University Corpus has 7,512
sentences and 220,147 words. The total number
of English terms, Arabic numbers, and Chinese
punctuation marks is 37,644. Such formatting
problem makes the error rate of about 30% for
the City University Corpus.
</bodyText>
<subsectionHeader confidence="0.785125">
3.1.2 Different Viewpoints of Segmentations
</subsectionHeader>
<bodyText confidence="0.999887666666667">
In our experiments, there are different word
lists extracted from the different training corpora.
Some errors are produced because of the differ-
</bodyText>
<figure confidence="0.9774955">
CEDIC CK+CT CK CT
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
CKIP
CityU
</figure>
<equation confidence="0.981931592592593">
特別 行政區
(簡 稱
“9+2 &amp;quot; ) 。
福建 、江西 、湖南 、廣東 、廣西 、海南 、四
川 、貴 州 、雲南 9 個 省區 ,以及 香港 、澳門
(Our Answer)
與 大珠三角 相鄰 、相互
間 經貿 關係 密切 的
(Standard)
與 大珠三角 相鄰 、
相互 間 經貿 關係 密切 的
福建 、
澳門 特別 行政區 (
江西 、
湖南 、
南 、
港 、
“ 9+2
簡稱
,
&amp;quot;
廣東 、 廣西 、 海
四川 、
貴州 、
雲南 9 個 省區
以及 香
) 。
</equation>
<page confidence="0.991764">
167
</page>
<bodyText confidence="0.999658857142857">
ent results of word segmentations in the training
corpora according to the different guidelines.
Table 3 shows some different results. The first
column (CKIP) is the standard answer of the test-
ing collection of Sinica Corpus, and the second
column (HFUIM) is our answer. The third and
fourth columns are the words with their frequen-
cies appeared in the training collections of Sinica
Corpus and City University Corpus. For exam-
ple, our system produces the word “心中”, but
the standard answer of Sinica Corpus is “心” and
“中”. However, the word “心中” appear 61
times in the training collection of City University
Corpus.
</bodyText>
<sectionHeader confidence="0.588498" genericHeader="method">
CKIP HFUIM CKIP-Training CityU-Training
</sectionHeader>
<equation confidence="0.977875230769231">
**婦 ** 婦 **婦 (0)
整 夜 整夜 整 (1839) 整夜 (2)
夜 (366)
看 著 看著 看著辦 (4)
眼看著 (20)
心 中 心中 心 (2551) 心中 (61)
中 (16694)
這 個 這個 這 (32409) 這個 (714)
個 (39558)
死 後 死後 死 (984) 死後 (18)
後 (7967)
所 需 所需 所 (9012) 所需 (35)
需 (963)
</equation>
<tableCaption confidence="0.993304">
Table 3. The Different Segmentation Results
</tableCaption>
<subsectionHeader confidence="0.691529">
3.1.3 Inconsistency of Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999839857142857">
Some errors of word segmentations are re-
ported because of the inconsistency of word
segmentations. The following shows such a
problem. For example, the word “還有“ appears
317 times in the training data, but it has been
treated as two terms (“還” and “有”) 19 times in
the golden standard of the testing data.
</bodyText>
<sectionHeader confidence="0.998664" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999982166666667">
In this paper, we discuss the effectiveness of
the Chinese word segmentation using various
dictionaries. In the experimental results, we can
find that the larger dictionary will produce better
segmentation results even the word lists are
combined from the different resources (corpora)
and followed the different guidelines of word
segmentations. Some results show that the ex-
ternal resource (e.g., the bilingual dictionary) can
perform the task of Chinese word segmentation
better than the monolingual dictionary which
extracted from the training corpus.
</bodyText>
<sectionHeader confidence="0.992974" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.988004083333333">
Bian, G.W. and Chen, H.H. (2000). &amp;quot;Cross Language
Information Access to Multilingual Collections on
the Internet.&amp;quot; Journal of American Society for In-
formation Science &amp; Technology (JASIST), Special
Issue on Digital Libraries, 51(3), 2000, 281-296.
Bian, G.W. and Chen, H.H. (1998). &amp;quot;Integrating
Query Translation and Document Translation in a
Cross-Language Information Retrieval System.&amp;quot;
Machine Translation and the Information Soap
(AMTA ‘98), D. Farwell, L Gerber, and E. Hovy
(Eds.), Lecture Notes in Computer Science, Vol.
1529, Springer-Verlag, pp. 250-265, 1998
Chen, K.J and Liu, S.H (1992), “word identification
for Mandarin Chinese sentences” Proceedings of
the 14th conference on Computational linguistics,
pp. 101-107, France, 1992
Palmer, D. (1997), “A trainable rule-based algorithm
for word segmentation”, Proceeding of ACL&apos;97,
321-328, 1997.
Sproat, R., et al. (1994) “A Stochastic Finite-State
Word-Segmentation Algorithm for Chinese”, Pro-
ceeding of 32nd Annual Meeting of ACL, New Mex-
ico, pp. 66-73.
(Training data)
</reference>
<listItem confidence="0.909262">
• 歐盟 委員會 設置 等 問題 上 還有 一些 T
同 聲音
(Golden Standard)
• 目前 * 地 機場 還有 一些 商業 問題
談
• 鍾 透� 身上 還 有 一 個 紋身 圖案
• 還 有 其他 歐國盃 的 有趣 專題 , 萬勿錯
過 已 出版 的 《 明報 歐洲 國家盃
刊 》 。
</listItem>
<page confidence="0.995505">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840323">
<title confidence="0.999891">Chinese Word Segmentation using Various Dictionaries</title>
<author confidence="0.999934">Guo-Wei Bian</author>
<affiliation confidence="0.9608305">Department of Information Huafan University, Taiwan, R.O.C.</affiliation>
<email confidence="0.961445">gwbian@cc.hfu.edu.tw</email>
<abstract confidence="0.996517764705882">Most of the Chinese word segmentation systems utilizes monolingual dictionary and are used for monolingual processing. For the tasks of machine translation (MT) and cross-language information retrieval (CLIR), another translation dictionary may be used to transfer the words of documents from the source languages to target languages. The inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary) may produce some problems for MT and CLIR. This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G W Bian</author>
<author>H H Chen</author>
</authors>
<title>Cross Language Information Access to Multilingual Collections on the Internet.&amp;quot;</title>
<date>2000</date>
<journal>Journal of American Society for Information Science &amp; Technology (JASIST), Special Issue on Digital Libraries,</journal>
<volume>51</volume>
<issue>3</issue>
<pages>281--296</pages>
<contexts>
<context position="1258" citStr="Bian and Chen, 2000" startWordPosition="169" endWordPosition="172">lems for MT and CLIR. This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations. 1 Introduction Most of the Chinese word segmentations are used for monolingual processing. In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al., 1994). For the tasks of machine translation (MT) (Bian and Chen, 1998) and cross-language information retrieval (CLIR) (Bian and Chen, 2000), another translation dictionary may be used to transfer the words of documents from the source languages to target languages. Because of the inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary), this approach has the problems that some segmented words cannot be found in the transfer dictionary. In this paper, we focus on the effectiveness of the Chinese word segmentation using different dictionaries. Four different dictionaries (or word lists) and two different testing collections (testing data) are used to evaluate the results of the </context>
</contexts>
<marker>Bian, Chen, 2000</marker>
<rawString>Bian, G.W. and Chen, H.H. (2000). &amp;quot;Cross Language Information Access to Multilingual Collections on the Internet.&amp;quot; Journal of American Society for Information Science &amp; Technology (JASIST), Special Issue on Digital Libraries, 51(3), 2000, 281-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Bian</author>
<author>H H Chen</author>
</authors>
<title>Integrating Query Translation and Document Translation in a Cross-Language Information Retrieval System.&amp;quot;</title>
<date>1998</date>
<booktitle>Machine Translation and the Information Soap (AMTA ‘98),</booktitle>
<volume>1529</volume>
<pages>250--265</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="1188" citStr="Bian and Chen, 1998" startWordPosition="159" endWordPosition="162">segmentation dictionary and transfer dictionary) may produce some problems for MT and CLIR. This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations. 1 Introduction Most of the Chinese word segmentations are used for monolingual processing. In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al., 1994). For the tasks of machine translation (MT) (Bian and Chen, 1998) and cross-language information retrieval (CLIR) (Bian and Chen, 2000), another translation dictionary may be used to transfer the words of documents from the source languages to target languages. Because of the inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary), this approach has the problems that some segmented words cannot be found in the transfer dictionary. In this paper, we focus on the effectiveness of the Chinese word segmentation using different dictionaries. Four different dictionaries (or word lists) and two different testi</context>
<context position="3176" citStr="Bian and Chen, 1998" startWordPosition="461" endWordPosition="464"> the entry in the dictionary if no total matching is found. For example, the system will segment the input “看著隨時可能 結束生命的妹妹” as “看著 隨時 可能 結束 命 的 妹妹” which matched the term with the entry “看著辦” in dictionary if no entry “看著 ” found. 2.1 Various Dictionaries The word segmentation are evaluated using different dictionaries (or word lists) and different testing collections (testing data). There are four dictionaries are used: the first one is converted from an English-Chinese bilingual dictionary, and the other three are extracted from the training corpora. The original English-Chinese dictionary (Bian and Chen, 1998), which containing about 67,000 English word entries, is converted to a new Chinese-English dictionary (called CEDIC later). There are 125,719 Chinese word entries in this CEDIC. The terms in the various training corpora (the Sinica Corpus and the City University Corpus) are extracted to build the different word lists as the segmentation dictionaries (called CKIP and CityU later). The tokens starting with the special 166 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 166–168, Sydney, July 2006. c�2006 Association for Computational Linguistics characters or punct</context>
</contexts>
<marker>Bian, Chen, 1998</marker>
<rawString>Bian, G.W. and Chen, H.H. (1998). &amp;quot;Integrating Query Translation and Document Translation in a Cross-Language Information Retrieval System.&amp;quot; Machine Translation and the Information Soap (AMTA ‘98), D. Farwell, L Gerber, and E. Hovy (Eds.), Lecture Notes in Computer Science, Vol. 1529, Springer-Verlag, pp. 250-265, 1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Chen</author>
<author>S H Liu</author>
</authors>
<title>word identification for Mandarin Chinese sentences”</title>
<date>1992</date>
<booktitle>Proceedings of the 14th conference on Computational linguistics,</booktitle>
<pages>101--107</pages>
<contexts>
<context position="1003" citStr="Chen and Liu, 1992" startWordPosition="132" endWordPosition="135"> translation dictionary may be used to transfer the words of documents from the source languages to target languages. The inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary) may produce some problems for MT and CLIR. This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations. 1 Introduction Most of the Chinese word segmentations are used for monolingual processing. In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al., 1994). For the tasks of machine translation (MT) (Bian and Chen, 1998) and cross-language information retrieval (CLIR) (Bian and Chen, 2000), another translation dictionary may be used to transfer the words of documents from the source languages to target languages. Because of the inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary), this approach has the problems that some segmented words cannot be found in the transfer di</context>
</contexts>
<marker>Chen, Liu, 1992</marker>
<rawString>Chen, K.J and Liu, S.H (1992), “word identification for Mandarin Chinese sentences” Proceedings of the 14th conference on Computational linguistics, pp. 101-107, France, 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation”, Proceeding of ACL&apos;97,</title>
<date>1997</date>
<pages>321--328</pages>
<contexts>
<context position="1066" citStr="Palmer, 1997" startWordPosition="142" endWordPosition="143">s from the source languages to target languages. The inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary) may produce some problems for MT and CLIR. This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations. 1 Introduction Most of the Chinese word segmentations are used for monolingual processing. In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al., 1994). For the tasks of machine translation (MT) (Bian and Chen, 1998) and cross-language information retrieval (CLIR) (Bian and Chen, 2000), another translation dictionary may be used to transfer the words of documents from the source languages to target languages. Because of the inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary), this approach has the problems that some segmented words cannot be found in the transfer dictionary. In this paper, we focus on the effectiveness of the C</context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>Palmer, D. (1997), “A trainable rule-based algorithm for word segmentation”, Proceeding of ACL&apos;97, 321-328, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
</authors>
<title>A Stochastic Finite-State Word-Segmentation Algorithm for Chinese”,</title>
<date>1994</date>
<booktitle>Proceeding of 32nd Annual Meeting of ACL,</booktitle>
<pages>66--73</pages>
<location>New</location>
<marker>Sproat, 1994</marker>
<rawString>Sproat, R., et al. (1994) “A Stochastic Finite-State Word-Segmentation Algorithm for Chinese”, Proceeding of 32nd Annual Meeting of ACL, New Mexico, pp. 66-73.</rawString>
</citation>
<citation valid="false">
<note>(Training data)</note>
<marker></marker>
<rawString>(Training data)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>