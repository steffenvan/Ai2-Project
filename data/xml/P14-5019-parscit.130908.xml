<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.086879">
<title confidence="0.99808">
lex4all: A language-independent tool for building and evaluating
pronunciation lexicons for small-vocabulary speech recognition
</title>
<author confidence="0.999632">
Anjana Vakil, Max Paulus, Alexis Palmer, and Michaela Regneri
</author>
<affiliation confidence="0.999053">
Department of Computational Linguistics, Saarland University
</affiliation>
<email confidence="0.991496">
{anjanav,mpaulus,apalmer,regneri}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.997287" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896695652174">
This paper describes lex4all, an open-
source PC application for the generation
and evaluation of pronunciation lexicons
in any language. With just a few minutes
of recorded audio and no expert knowl-
edge of linguistics or speech technology,
individuals or organizations seeking to
create speech-driven applications in low-
resource languages can build lexicons en-
abling the recognition of small vocabular-
ies (up to 100 terms, roughly) in the target
language using an existing recognition en-
gine designed for a high-resource source
language (e.g. English). To build such lex-
icons, we employ an existing method for
cross-language phoneme-mapping. The
application also offers a built-in audio
recorder that facilitates data collection, a
significantly faster implementation of the
phoneme-mapping technique, and an eval-
uation module that expedites research on
small-vocabulary speech recognition for
low-resource languages.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943161290322">
In recent years it has been demonstrated that
speech recognition interfaces can be extremely
beneficial for applications in the developing world
(Sherwani and Rosenfeld, 2008; Sherwani, 2009;
Bali et al., 2013). Typically, such applications
target low-resource languages (LRLs) for which
large collections of speech data are unavailable,
preventing the training or adaptation of recogni-
tion engines for these languages. However, an ex-
isting recognizer for a completely unrelated high-
resource language (HRL), such as English, can
be used to perform small-vocabulary recognition
tasks in the LRL, given a pronunciation lexicon
mapping each term in the target vocabulary to a
sequence of phonemes in the HRL, i.e. phonemes
which the recognizer can model.
This is the motivation behind lex4all,1 an open-
source application that allows users to automati-
cally create a mapped pronunciation lexicon for
terms in any language, using a small number of
speech recordings and an out-of-the-box recog-
nition engine for a HRL. The resulting lexicon
can then be used with the HRL recognizer to add
small-vocabulary speech recognition functionality
to applications in the LRL, without the need for
the large amounts of data and expertise in speech
technologies required to train a new recognizer.
This paper describes the lex4all application and
its utility for the rapid creation and evaluation of
pronunciation lexicons enabling small-vocabulary
speech recognition in any language.
</bodyText>
<sectionHeader confidence="0.868881" genericHeader="introduction">
2 Background and related work
</sectionHeader>
<bodyText confidence="0.999789578947368">
Several commercial speech recognition systems
offer high-level Application Programming Inter-
faces (APIs) that make it extremely simple to add
voice interfaces to an application, requiring very
little general technical expertise and virtually no
knowledge of the inner workings of the recogni-
tion engine. If the target language is supported by
the system – the Microsoft Speech Platform,2 for
example, supports over 20 languages – this makes
it very easy to create speech-driven applications.
If, however, the target language is one of the
many thousands of LRLs for which high-quality
recognition engines have not yet been devel-
oped, alternative strategies for developing speech-
recognition interfaces must be employed. Though
tools for quickly training recognizers for new lan-
guages exist (e.g. CMUSphinx3), they typically
require many hours of training audio to produce
effective models, data which is by definition not
</bodyText>
<footnote confidence="0.999945666666667">
1http://lex4all.github.io/lex4all/
2http://msdn.microsoft.com/en-us/library/hh361572
3http://www.cmusphinx.org
</footnote>
<page confidence="0.988307">
109
</page>
<bodyText confidence="0.990296692307693">
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 109–114,
Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics
available for LRLs. In efforts to overcome this
data scarcity problem, recent years have seen
the development of techniques for rapidly adapt-
ing multilingual or language-independent acoustic
and language models to new languages from rela-
tively small amounts of data (Schultz and Waibel,
2001; Kim and Khudanpur, 2003), methods for
building resources such as pronunciation dictio-
naries from web-crawled data (Schlippe et al.,
2014), and even a web-based interface, the Rapid
Language Adaptation Toolkit4 (RLAT), which al-
lows non-expert users to exploit these techniques
to create speech recognition and synthesis tools
for new languages (Vu et al., 2010). While they
greatly reduce the amount of data needed to build
new recognizers, these approaches still require
non-trivial amounts of speech and text in the target
language, which may be an obstacle for very low-
or zero-resource languages. Furthermore, even
high-level tools such as RLAT still demand some
understanding of linguistics/language technology,
and thus may not be accessible to all users.
However, many useful applications (e.g. for ac-
cessing information or conducting basic transac-
tions by telephone) only require small-vocabulary
recognition, i.e. discrimination between a few
dozen terms (words or short phrases). For ex-
ample, VideoKheti (Bali et al., 2013), a text-
free smartphone application that delivers agricul-
tural information to low-literate farmers in In-
dia, recognizes 79 Hindi terms. For such small-
vocabulary applications, an engine designed to
recognize speech in a HRL can be used as-is to
perform recognition of the LRL terms, given a
grammar describing the allowable combinations
and sequences of terms to be recognized, and a
pronunciation lexicon mapping each target term to
at least one pronunciation (sequence of phonemes)
in the HRL (see Fig. 1 for an example).
This is the thinking behind Speech-based Auto-
mated Learning of Accent and Articulation Map-
ping, or “Salaam” (Sherwani, 2009; Qiao et al.,
2010; Chan and Rosenfeld, 2012), a method of
cross-language phoneme-mapping that discovers
accurate source-language pronunciations for terms
in the target language. The basic idea is to discover
the best pronunciation (phoneme sequence) for a
target term by using the source-language recog-
nition engine to perform phone decoding on one
or more utterances of the term. As commercial
</bodyText>
<footnote confidence="0.674252">
4http://i19pc5.ira.uka.de/rlat-dev
</footnote>
<figure confidence="0.9308387">
&lt;lexicon version=&amp;quot;1.0&amp;quot; xmlns=&amp;quot;http://www
.w3.org/2005/01/pronunciation-
lexicon&amp;quot; xml:lang=&amp;quot;en-US&amp;quot; alphabet
=&amp;quot;x-microsoft-ups&amp;quot;&gt;
&lt;lexeme&gt;
&lt;grapheme&gt;beeni&lt;/grapheme&gt;
&lt;phoneme&gt;B E NG I&lt;/phoneme&gt;
&lt;phoneme&gt;B EI N I I&lt;/phoneme&gt;
&lt;/lexeme&gt;
&lt;/lexicon&gt;
</figure>
<figureCaption confidence="0.949339">
Figure 1: Sample XML lexicon mapping the
Yoruba word beeni (“yes”) to two possible se-
quences of American English phonemes.
</figureCaption>
<bodyText confidence="0.999861135135135">
recognizers such as Microsoft’s are designed for
word-decoding, and their APIs do not usually al-
low users access to the phone-decoding mode, the
Salaam approach uses a specially designed “super-
wildcard” recognition grammar to mimic phone
decoding and guide pronunciation discovery (Qiao
et al., 2010; Chan and Rosenfeld, 2012). This al-
lows the recognizer to identify the phoneme se-
quence best matching a given term, without any
prior indication of how many phonemes that se-
quence should contain.
Given this grammar and one or more audio
recordings of the term, Qiao et al. (2010) use an it-
erative training algorithm to discover the best pro-
nunciation(s) for that term, one phoneme at a time.
Compared to pronunciations hand-written by a lin-
guist, pronunciations generated automatically by
this algorithm yield substantially higher recog-
nition accuracy: Qiao et al. (2010) report word
recognition accuracy rates in the range of 75-95%
for vocabularies of 50 terms. Chan and Rosen-
feld (2012) improve accuracy on larger vocabu-
laries (up to approximately 88% for 100 terms)
by applying an iterative discriminative training al-
gorithm, identifying and removing pronunciations
that cause confusion between word types.
The Salaam method is fully automatic, demand-
ing expertise neither in speech technology nor
in linguistics, and requires only a few recorded
utterances of each word. At least two projects
have successfully used the Salaam method to add
voice interfaces to real applications: an Urdu
telephone-based health information system (Sher-
wani, 2009), and the VideoKheti application men-
tioned above (Bali et al., 2013). What has not ex-
isted before now is an interface that makes this ap-
proach accessible to any user.
</bodyText>
<page confidence="0.986636">
110
</page>
<bodyText confidence="0.999978428571428">
Given the established success of the Salaam
method, our contribution is to create a more time-
efficient implementation of the pronunciation-
discovery algorithm and integrate it into an easy-
to-use graphical application. In the following sec-
tions, we describe this application and our slightly
modified implementation of the Salaam method.
</bodyText>
<sectionHeader confidence="0.949521" genericHeader="method">
3 System overview
</sectionHeader>
<bodyText confidence="0.99995625">
We have developed lex4all as a desktop applica-
tion for Microsoft Windows,5 since it relies on the
Microsoft Speech Platform (MSP) as explained in
Section 4.1. The application and its source code
are freely available via GitHub.6
The application’s core feature is its lexicon-
building tool, the architecture of which is illus-
trated in Figure 2. A simple graphical user in-
terface (GUI) allows users to type in the written
form of each term in the target vocabulary, and
select one or more audio recordings (.wav files)
of that term. Given this input, the program uses
the Salaam method to find the best pronuncia-
tion(s) for each term. This requires a pre-trained
recognition engine for a HRL as well as a series
of dynamically-created recognition grammars; the
engine and grammars are constructed and man-
aged using the MSP. We note here that our imple-
mentation of Salaam deviates slightly from that of
Qiao et al. (2010), improving the time-efficiency
and thus usability of the system (see Sec. 4).
Once pronunciations for all terms in the vocab-
ulary have been generated, the application outputs
a pronunciation lexicon for the given terms as an
XML file conforming to the Pronunciation Lexi-
con Specification.7 This lexicon can then be di-
rectly included in a speech recognition application
built using the MSP API or a similar toolkit.
</bodyText>
<sectionHeader confidence="0.980366" genericHeader="method">
4 Pronunciation mapping
</sectionHeader>
<subsectionHeader confidence="0.996891">
4.1 Recognition engine
</subsectionHeader>
<bodyText confidence="0.99996175">
For the HRL recognizer we use the US English
recognition engine of the MSP. The engine is used
as-is, with no modifications to its underlying mod-
els. We choose the MSP for its robustness and
ease of use, as well as to maintain comparability
with the work of Qiao et al. (2010) and Chan and
Rosenfeld (2012). Following these authors, we
use an engine designed for server-side recognition
</bodyText>
<footnote confidence="0.999488333333333">
5Windows 7 or 8 (64-bit).
6http://github.com/lex4all/lex4all
7http://www.w3.org/TR/pronunciation-lexicon/
</footnote>
<figureCaption confidence="0.9004015">
Figure 2: Overview of the core components of the
lex4all lexicon-building application.
</figureCaption>
<bodyText confidence="0.999397833333333">
of low-quality audio, since we aim to enable the
creation of useful applications for LRLs, includ-
ing those spoken in developing-world communi-
ties, and such applications should be able to cope
with telephone-quality audio or similar (Sherwani
and Rosenfeld, 2008).
</bodyText>
<subsectionHeader confidence="0.997332">
4.2 Implementation of the Salaam method
</subsectionHeader>
<bodyText confidence="0.999565066666667">
Pronunciations (sequences of source-language
phonemes) for each term in the target vocabu-
lary are generated from the audio sample(s) of
that term using the iterative Salaam algorithm
(Sec. 2), which employs the source-language rec-
ognizer and a special recognition grammar. In
the first pass, the algorithm finds the best candi-
date(s) for the first phoneme of the sample(s), then
the first two phonemes in the second pass, and so
on until a stopping criterion is met. In our im-
plementation, we stop iterations if the top-scoring
sequence for a term has not changed for three con-
secutive iterations (Chan and Rosenfeld, 2012), or
if the best sequence from a given pass has a lower
confidence score than the best sequence from the
</bodyText>
<page confidence="0.994867">
111
</page>
<bodyText confidence="0.999596125">
previous pass (Qiao et al., 2010). In both cases, at
least three passes are required.
After the iterative training has completed, the n-
best pronunciation sequences (with n specified by
users – see Sec. 5.2) for each term are written to
the lexicon, each in a &lt;phoneme&gt; element corre-
sponding to the &lt;grapheme&gt; element containing
the term’s orthographic form (see Fig. 1).
</bodyText>
<subsectionHeader confidence="0.999917">
4.3 Running time
</subsectionHeader>
<bodyText confidence="0.999962975609756">
A major challenge we faced in engineering a user-
friendly application based on the Salaam algo-
rithm was its long running time. The algorithm
depends on a “super-wildcard” grammar that al-
lows the recognizer to match each sample of a
given term to a “phrase” of 0-10 “words”, each
word comprising any possible sequence of 1, 2, or
3 source-language phonemes (Qiao et al., 2010).
Given the 40 phonemes of US English, this gives
over 65,000 possibilities for each word, resulting
in a huge training grammar and thus a long pro-
cessing time. For a 25-term vocabulary with 5
training samples per term, the process takes ap-
proximately 1-2 hours on a standard modern lap-
top. For development and research, this long train-
ing time is a serious disadvantage.
To speed up training, we limit the length of each
“word” in the grammar to only one phoneme, in-
stead of up to 3, giving e.g. 40 possibilities in-
stead of tens of thousands. The algorithm can still
discover pronunciation sequences of an arbitrary
length, since, in each iteration, the phonemes dis-
covered so far are prepended to the super-wildcard
grammar, such that the phoneme sequence of the
first “word” in the phrase grows longer with each
pass (Qiao et al., 2010). However, the new imple-
mentation is an order of magnitude faster: con-
structing the same 25-term lexicon on the same
hardware takes approximately 2-5 minutes, i.e.
less than 10% of the previous training time.
To ensure that the new implementation’s vastly
improved running time does not come at the cost
of reduced recognition accuracy, we evaluate and
compare word recognition accuracy rates using
lexicons built with the old and new implementa-
tions. The data we use for this evaluation is a
subset of the Yoruba data collected by Qiao et al.
(2010), comprising 25 Yoruba terms (words) ut-
tered by 2 speakers (1 male, 1 female), with 5
samples of each term per speaker. To determine
same-speaker accuracy for each of the two speak-
</bodyText>
<table confidence="0.997793857142857">
Old New p
Female average 72.8 73.6 0.75
Male average 90.4 90.4 1.00
Overall average 81.6 82 0.81
Trained on male 70.4 66.4 –
Trained on female 76.8 77.6 –
Average 73.6 72 0.63
</table>
<tableCaption confidence="0.78126925">
Table 1: Word recognition accuracy for Yoruba us-
ing old (slower) and new (faster) implementations,
with p-values from t-tests for significance of dif-
ference in means. Bold indicates highest accuracy.
</tableCaption>
<bodyText confidence="0.999944722222222">
ers, we perform a leave-one-out evaluation on the
five samples recorded per term per speaker. Cross-
speaker accuracy is evaluated by training the sys-
tem on all five samples of each term recorded by
one speaker, and testing on all five samples from
the other speaker. We perform paired two-tailed t-
tests on the results to assess the significance of the
differences in mean accuracy.
The results of our evaluation, given in Table 1,
indicate no statistically significant difference in
accuracy between the two implementations (all p-
values are above 0.5 and thus clearly insignifi-
cant). As our new, modified implementation of the
Salaam algorithm is much faster than the original,
yet equally accurate, lex4all uses the new imple-
mentation by default, although for research pur-
poses we leave users the option of using the origi-
nal (slower) implementation (see Section 5.2).
</bodyText>
<subsectionHeader confidence="0.999064">
4.4 Discriminative training
</subsectionHeader>
<bodyText confidence="0.999899">
Chan and Rosenfeld (2012) achieve increased ac-
curacy (gains of up to 5 percentage points) by
applying an iterative discriminative training al-
gorithm. This algorithm takes as input the set
of mapped pronunciations generated using the
Salaam algorithm; in each iteration, it simulates
recognition of the training audio samples using
these pronunciations, and outputs a ranked list of
the pronunciations in the lexicon that best match
each sample. Pronunciations that cause “confu-
sion” between words in the vocabulary, i.e. pro-
nunciations that the recognizer matches to sam-
ples of the wrong word type, are thus identified
and removed from the lexicon, and the process is
repeated in the next iteration.
We implement this accuracy-boosting algorithm
in lex4all, and apply it by default. To enable fine-
</bodyText>
<figure confidence="0.86168225">
Cross-
speaker
Same-
speaker
</figure>
<page confidence="0.995035">
112
</page>
<bodyText confidence="0.999294">
tuning and experimentation, we leave users the
option to change the number of passes (4 by de-
fault) or to disable discriminative training entirely,
as mentioned in Section 5.2.
</bodyText>
<sectionHeader confidence="0.985651" genericHeader="method">
5 User interface
</sectionHeader>
<bodyText confidence="0.999990766666667">
As mentioned above, we aim to make the creation
and evaluation of lexicons simple, fast, and above
all accessible to users with no expertise in speech
or language technologies. Therefore, the applica-
tion makes use of a simple GUI that allows users
to quickly and easily specify input and output file
paths, and to control the parameters of the lexicon-
building algorithms.
Figure 3 shows the main interface of the lex4all
lexicon builder. This window displays the terms
that have been specified and the number of audio
samples that have been selected for each word.
Another form, accessed via the “Add word” or
“Edit” buttons, allows users to add to or edit the
vocabulary by simply typing in the desired ortho-
graphic form of the word and selecting the audio
sample(s) to be used for pronunciation discovery
(see Sec. 5.1 for more details on audio input).
Once the target vocabulary and training audio
have been specified, and the additional options
have been set if desired, users click the “Build
Lexicon” button and specify the desired name and
target directory of the lexicon file to be saved, and
pronunciation discovery begins. When all pronun-
ciations have been generated, a success message
displaying the elapsed training time is displayed,
and users may either proceed to the evaluation
module to assess the newly created lexicon (see
Sec. 6), or return to the main interface to build an-
other lexicon.
</bodyText>
<subsectionHeader confidence="0.999392">
5.1 Audio input and recording
</subsectionHeader>
<bodyText confidence="0.999738916666667">
The GUI allows users to easily browse their file
system for pre-recorded audio samples (.wav
files) to be used for lexicon training. To simplify
data collection and enable the development of lexi-
cons even for zero-resource languages, lex4all also
offers a simple built-in audio recorder to record
new speech samples.
The recorder, built using the open-source library
NAudio,8 takes the default audio input device as
its source and records one channel with a sampling
rate of 8 kHz, as the recognition engine we employ
is designed for low-quality audio (see Section 4.1).
</bodyText>
<footnote confidence="0.946003">
8http://naudio.codeplex.com/
</footnote>
<figureCaption confidence="0.999453">
Figure 3: Screenshot of the lexicon builder.
</figureCaption>
<subsectionHeader confidence="0.996711">
5.2 Additional options
</subsectionHeader>
<bodyText confidence="0.999990428571429">
As seen in Figure 3, lex4all includes optional con-
trols for quick and easy fine-tuning of the lexicon-
building process (the default settings are pictured).
First of all, users can specify the maxi-
mum number of pronunciations (&lt;phoneme&gt; el-
ements) per word that the lexicon may contain;
allowing more pronunciations per word may im-
prove recognition accuracy (Qiao et al., 2010;
Chan and Rosenfeld, 2012). Secondly, users may
train the lexicon using our modified, faster imple-
mentation of the Salaam algorithm or the origi-
nal implementation. Finally, users may choose
whether or not discriminative training is applied,
and if so, how many passes are run (see Sec. 4.4).
</bodyText>
<sectionHeader confidence="0.994951" genericHeader="evaluation">
6 Evaluation module for research
</sectionHeader>
<bodyText confidence="0.999849428571428">
In addition to its primary utility as a lexicon-
building tool, lex4all is also a valuable research
aide thanks to an evaluation module that allows
users to quickly and easily evaluate the lexicons
they have created. The evaluation tool allows users
to browse their file system for an XML lexicon file
that they wish to evaluate; this may be a lexicon
created using lex4all, or any other lexicon in the
same format. As in the main interface, users then
select one or more audio samples (.wav files)
for each term they wish to evaluate. The system
then attempts to recognize each sample using the
given lexicon, and reports the counts and percent-
ages of correct, incorrect, and failed recognitions.
</bodyText>
<page confidence="0.99753">
113
</page>
<bodyText confidence="0.988366875">
Users may optionally save this report, along with
a confusion matrix of word types, as a comma-
separated values (.csv) file.
The evaluation module thus allows users to
quickly and easily assess different configurations
of the lexicon-building tool, by simply changing
the settings using the GUI and evaluating the re-
sulting lexicons. Furthermore, as the applica-
tion’s source code is freely available and modifi-
able, researchers may even replace entire modules
of the system (e.g. use a different pronunciation-
discovery algorithm), and use the evaluation mod-
ule to quickly assess the results. Therefore, lex4all
facilitates not only application development but
also further research into small-vocabulary speech
recognition using mapped pronunciation lexicons.
</bodyText>
<sectionHeader confidence="0.991417" genericHeader="conclusions">
7 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999998513513513">
We have presented lex4all, an open-source appli-
cation that enables the rapid automatic creation
of pronunciation lexicons in any (low-resource)
language, using an out-of-the-box commercial
recognizer for a high-resource language and the
Salaam method for cross-language pronunciation
mapping (Qiao et al., 2010; Chan and Rosen-
feld, 2012). The application thus makes small-
vocabulary speech recognition interfaces feasible
in any language, since only minutes of training au-
dio are required; given the built-in audio recorder,
lexicons can be constructed even for zero-resource
languages. Furthermore, lex4all’s flexible and
open design and easy-to-use evaluation module
make it a valuable tool for research in language-
independent small-vocabulary recognition.
In future work, we plan to expand the selection
of source-language recognizers; at the moment,
lex4all only uses US English as the source lan-
guage, but any of the 20+ other HRLs supported
by the MSP could be added. This would enable in-
vestigation of the target-language recognition ac-
curacy obtained using different source languages,
though our initial exploration of this issue sug-
gests that phonetic similarity between the source
and target languages might not significantly affect
accuracy (Vakil and Palmer, 2014). Another future
goal is to improve and extend the functionality of
the audio-recording tool to make it more flexible
and user-friendly. Finally, as a complement to the
application, it would be beneficial to create a cen-
tral online data repository where users can upload
the lexicons they have built and the speech sam-
ples they have recorded. Over time, this could be-
come a valuable collection of LRL data, enabling
developers and researchers to share and re-use data
among languages or language families.
</bodyText>
<sectionHeader confidence="0.997281" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997254142857143">
The first author was partially supported by a
Deutschlandstipendium scholarship sponsored by
IMC AG. We thank Roni Rosenfeld, Hao Yee
Chan, and Mark Qiao for generously sharing their
speech data and valuable advice, and Dietrich
Klakow, Florian Metze, and the three anonymous
reviewers for their feedback.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999667388888889">
Kalika Bali, Sunayana Sitaram, Sebastien Cuendet,
and Indrani Medhi. 2013. A Hindi speech recog-
nizer for an agricultural video search application. In
ACM DEV ’13.
Hao Yee Chan and Roni Rosenfeld. 2012. Discrimi-
native pronunciation learning for speech recognition
for resource scarce languages. In ACM DEV ’12.
Woosung Kim and Sanjeev Khudanpur. 2003. Lan-
guage model adaptation using cross-lingual infor-
mation. In Eurospeech.
Fang Qiao, Jahanzeb Sherwani, and Roni Rosenfeld.
2010. Small-vocabulary speech recognition for
resource-scarce languages. In ACM DEV ’10.
Tim Schlippe, Sebastian Ochs, and Tanja Schultz.
2014. Web-based tools and methods for rapid pro-
nunciation dictionary creation. Speech Communica-
tion, 56:101–118.
Tanja Schultz and Alex Waibel. 2001. Language-
independent and language-adaptive acoustic model-
ing for speech recognition. Speech Communication,
35(1-2):31 – 51.
Jahanzeb Sherwani and Roni Rosenfeld. 2008. The
case for speech technology for developing regions.
In HCI for Community and International Develop-
ment Workshop, Florence, Italy.
Jahanzeb Sherwani. 2009. Speech interfaces for in-
formation access by low literate users. Ph.D. thesis,
Carnegie Mellon University.
Anjana Vakil and Alexis Palmer. 2014. Cross-
language mapping for small-vocabulary ASR in
under-resourced languages: Investigating the impact
of source language choice. In SLTU ’14.
Ngoc Thang Vu, Tim Schlippe, Franziska Kraus, and
Tanja Schultz. 2010. Rapid bootstrapping of five
Eastern European languages using the Rapid Lan-
guage Adaptation Toolkit. In Interspeech.
</reference>
<page confidence="0.998656">
114
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925579">
<title confidence="0.9951095">A language-independent tool for building and pronunciation lexicons for small-vocabulary speech recognition</title>
<author confidence="0.99182">Anjana Vakil</author>
<author confidence="0.99182">Max Paulus</author>
<author confidence="0.99182">Alexis Palmer</author>
<author confidence="0.99182">Michaela</author>
<affiliation confidence="0.943922">Department of Computational Linguistics, Saarland</affiliation>
<abstract confidence="0.999540041666667">paper describes an opensource PC application for the generation and evaluation of pronunciation lexicons in any language. With just a few minutes of recorded audio and no expert knowledge of linguistics or speech technology, individuals or organizations seeking to create speech-driven applications in lowresource languages can build lexicons enabling the recognition of small vocabularies (up to 100 terms, roughly) in the target language using an existing recognition engine designed for a high-resource source language (e.g. English). To build such lexicons, we employ an existing method for cross-language phoneme-mapping. The application also offers a built-in audio recorder that facilitates data collection, a significantly faster implementation of the phoneme-mapping technique, and an evaluation module that expedites research on small-vocabulary speech recognition for low-resource languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kalika Bali</author>
<author>Sunayana Sitaram</author>
<author>Sebastien Cuendet</author>
<author>Indrani Medhi</author>
</authors>
<title>A Hindi speech recognizer for an agricultural video search application.</title>
<date>2013</date>
<booktitle>In ACM DEV ’13.</booktitle>
<contexts>
<context position="1457" citStr="Bali et al., 2013" startWordPosition="192" endWordPosition="195">ce language (e.g. English). To build such lexicons, we employ an existing method for cross-language phoneme-mapping. The application also offers a built-in audio recorder that facilitates data collection, a significantly faster implementation of the phoneme-mapping technique, and an evaluation module that expedites research on small-vocabulary speech recognition for low-resource languages. 1 Introduction In recent years it has been demonstrated that speech recognition interfaces can be extremely beneficial for applications in the developing world (Sherwani and Rosenfeld, 2008; Sherwani, 2009; Bali et al., 2013). Typically, such applications target low-resource languages (LRLs) for which large collections of speech data are unavailable, preventing the training or adaptation of recognition engines for these languages. However, an existing recognizer for a completely unrelated highresource language (HRL), such as English, can be used to perform small-vocabulary recognition tasks in the LRL, given a pronunciation lexicon mapping each term in the target vocabulary to a sequence of phonemes in the HRL, i.e. phonemes which the recognizer can model. This is the motivation behind lex4all,1 an opensource appl</context>
<context position="5319" citStr="Bali et al., 2013" startWordPosition="753" endWordPosition="756"> to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very lowor zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users. However, many useful applications (e.g. for accessing information or conducting basic transactions by telephone) only require small-vocabulary recognition, i.e. discrimination between a few dozen terms (words or short phrases). For example, VideoKheti (Bali et al., 2013), a textfree smartphone application that delivers agricultural information to low-literate farmers in India, recognizes 79 Hindi terms. For such smallvocabulary applications, an engine designed to recognize speech in a HRL can be used as-is to perform recognition of the LRL terms, given a grammar describing the allowable combinations and sequences of terms to be recognized, and a pronunciation lexicon mapping each target term to at least one pronunciation (sequence of phonemes) in the HRL (see Fig. 1 for an example). This is the thinking behind Speech-based Automated Learning of Accent and Art</context>
<context position="8379" citStr="Bali et al., 2013" startWordPosition="1210" endWordPosition="1213">uracy on larger vocabularies (up to approximately 88% for 100 terms) by applying an iterative discriminative training algorithm, identifying and removing pronunciations that cause confusion between word types. The Salaam method is fully automatic, demanding expertise neither in speech technology nor in linguistics, and requires only a few recorded utterances of each word. At least two projects have successfully used the Salaam method to add voice interfaces to real applications: an Urdu telephone-based health information system (Sherwani, 2009), and the VideoKheti application mentioned above (Bali et al., 2013). What has not existed before now is an interface that makes this approach accessible to any user. 110 Given the established success of the Salaam method, our contribution is to create a more timeefficient implementation of the pronunciationdiscovery algorithm and integrate it into an easyto-use graphical application. In the following sections, we describe this application and our slightly modified implementation of the Salaam method. 3 System overview We have developed lex4all as a desktop application for Microsoft Windows,5 since it relies on the Microsoft Speech Platform (MSP) as explained </context>
</contexts>
<marker>Bali, Sitaram, Cuendet, Medhi, 2013</marker>
<rawString>Kalika Bali, Sunayana Sitaram, Sebastien Cuendet, and Indrani Medhi. 2013. A Hindi speech recognizer for an agricultural video search application. In ACM DEV ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Yee Chan</author>
<author>Roni Rosenfeld</author>
</authors>
<title>Discriminative pronunciation learning for speech recognition for resource scarce languages. In</title>
<date>2012</date>
<journal>ACM DEV</journal>
<volume>12</volume>
<contexts>
<context position="6011" citStr="Chan and Rosenfeld, 2012" startWordPosition="864" endWordPosition="867">rmation to low-literate farmers in India, recognizes 79 Hindi terms. For such smallvocabulary applications, an engine designed to recognize speech in a HRL can be used as-is to perform recognition of the LRL terms, given a grammar describing the allowable combinations and sequences of terms to be recognized, and a pronunciation lexicon mapping each target term to at least one pronunciation (sequence of phonemes) in the HRL (see Fig. 1 for an example). This is the thinking behind Speech-based Automated Learning of Accent and Articulation Mapping, or “Salaam” (Sherwani, 2009; Qiao et al., 2010; Chan and Rosenfeld, 2012), a method of cross-language phoneme-mapping that discovers accurate source-language pronunciations for terms in the target language. The basic idea is to discover the best pronunciation (phoneme sequence) for a target term by using the source-language recognition engine to perform phone decoding on one or more utterances of the term. As commercial 4http://i19pc5.ira.uka.de/rlat-dev &lt;lexicon version=&amp;quot;1.0&amp;quot; xmlns=&amp;quot;http://www .w3.org/2005/01/pronunciationlexicon&amp;quot; xml:lang=&amp;quot;en-US&amp;quot; alphabet =&amp;quot;x-microsoft-ups&amp;quot;&gt; &lt;lexeme&gt; &lt;grapheme&gt;beeni&lt;/grapheme&gt; &lt;phoneme&gt;B E NG I&lt;/phoneme&gt; &lt;phoneme&gt;B EI N I I&lt;/phon</context>
<context position="7749" citStr="Chan and Rosenfeld (2012)" startWordPosition="1115" endWordPosition="1119">fy the phoneme sequence best matching a given term, without any prior indication of how many phonemes that sequence should contain. Given this grammar and one or more audio recordings of the term, Qiao et al. (2010) use an iterative training algorithm to discover the best pronunciation(s) for that term, one phoneme at a time. Compared to pronunciations hand-written by a linguist, pronunciations generated automatically by this algorithm yield substantially higher recognition accuracy: Qiao et al. (2010) report word recognition accuracy rates in the range of 75-95% for vocabularies of 50 terms. Chan and Rosenfeld (2012) improve accuracy on larger vocabularies (up to approximately 88% for 100 terms) by applying an iterative discriminative training algorithm, identifying and removing pronunciations that cause confusion between word types. The Salaam method is fully automatic, demanding expertise neither in speech technology nor in linguistics, and requires only a few recorded utterances of each word. At least two projects have successfully used the Salaam method to add voice interfaces to real applications: an Urdu telephone-based health information system (Sherwani, 2009), and the VideoKheti application menti</context>
<context position="10522" citStr="Chan and Rosenfeld (2012)" startWordPosition="1569" endWordPosition="1572">erated, the application outputs a pronunciation lexicon for the given terms as an XML file conforming to the Pronunciation Lexicon Specification.7 This lexicon can then be directly included in a speech recognition application built using the MSP API or a similar toolkit. 4 Pronunciation mapping 4.1 Recognition engine For the HRL recognizer we use the US English recognition engine of the MSP. The engine is used as-is, with no modifications to its underlying models. We choose the MSP for its robustness and ease of use, as well as to maintain comparability with the work of Qiao et al. (2010) and Chan and Rosenfeld (2012). Following these authors, we use an engine designed for server-side recognition 5Windows 7 or 8 (64-bit). 6http://github.com/lex4all/lex4all 7http://www.w3.org/TR/pronunciation-lexicon/ Figure 2: Overview of the core components of the lex4all lexicon-building application. of low-quality audio, since we aim to enable the creation of useful applications for LRLs, including those spoken in developing-world communities, and such applications should be able to cope with telephone-quality audio or similar (Sherwani and Rosenfeld, 2008). 4.2 Implementation of the Salaam method Pronunciations (sequen</context>
<context position="15461" citStr="Chan and Rosenfeld (2012)" startWordPosition="2381" endWordPosition="2384">ests on the results to assess the significance of the differences in mean accuracy. The results of our evaluation, given in Table 1, indicate no statistically significant difference in accuracy between the two implementations (all pvalues are above 0.5 and thus clearly insignificant). As our new, modified implementation of the Salaam algorithm is much faster than the original, yet equally accurate, lex4all uses the new implementation by default, although for research purposes we leave users the option of using the original (slower) implementation (see Section 5.2). 4.4 Discriminative training Chan and Rosenfeld (2012) achieve increased accuracy (gains of up to 5 percentage points) by applying an iterative discriminative training algorithm. This algorithm takes as input the set of mapped pronunciations generated using the Salaam algorithm; in each iteration, it simulates recognition of the training audio samples using these pronunciations, and outputs a ranked list of the pronunciations in the lexicon that best match each sample. Pronunciations that cause “confusion” between words in the vocabulary, i.e. pronunciations that the recognizer matches to samples of the wrong word type, are thus identified and re</context>
<context position="18963" citStr="Chan and Rosenfeld, 2012" startWordPosition="2948" endWordPosition="2951">channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1). 8http://naudio.codeplex.com/ Figure 3: Screenshot of the lexicon builder. 5.2 Additional options As seen in Figure 3, lex4all includes optional controls for quick and easy fine-tuning of the lexiconbuilding process (the default settings are pictured). First of all, users can specify the maximum number of pronunciations (&lt;phoneme&gt; elements) per word that the lexicon may contain; allowing more pronunciations per word may improve recognition accuracy (Qiao et al., 2010; Chan and Rosenfeld, 2012). Secondly, users may train the lexicon using our modified, faster implementation of the Salaam algorithm or the original implementation. Finally, users may choose whether or not discriminative training is applied, and if so, how many passes are run (see Sec. 4.4). 6 Evaluation module for research In addition to its primary utility as a lexiconbuilding tool, lex4all is also a valuable research aide thanks to an evaluation module that allows users to quickly and easily evaluate the lexicons they have created. The evaluation tool allows users to browse their file system for an XML lexicon file t</context>
<context position="21087" citStr="Chan and Rosenfeld, 2012" startWordPosition="3278" endWordPosition="3282">nciationdiscovery algorithm), and use the evaluation module to quickly assess the results. Therefore, lex4all facilitates not only application development but also further research into small-vocabulary speech recognition using mapped pronunciation lexicons. 7 Conclusion and future work We have presented lex4all, an open-source application that enables the rapid automatic creation of pronunciation lexicons in any (low-resource) language, using an out-of-the-box commercial recognizer for a high-resource language and the Salaam method for cross-language pronunciation mapping (Qiao et al., 2010; Chan and Rosenfeld, 2012). The application thus makes smallvocabulary speech recognition interfaces feasible in any language, since only minutes of training audio are required; given the built-in audio recorder, lexicons can be constructed even for zero-resource languages. Furthermore, lex4all’s flexible and open design and easy-to-use evaluation module make it a valuable tool for research in languageindependent small-vocabulary recognition. In future work, we plan to expand the selection of source-language recognizers; at the moment, lex4all only uses US English as the source language, but any of the 20+ other HRLs s</context>
</contexts>
<marker>Chan, Rosenfeld, 2012</marker>
<rawString>Hao Yee Chan and Roni Rosenfeld. 2012. Discriminative pronunciation learning for speech recognition for resource scarce languages. In ACM DEV ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Woosung Kim</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Language model adaptation using cross-lingual information.</title>
<date>2003</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="4312" citStr="Kim and Khudanpur, 2003" startWordPosition="603" endWordPosition="606">io/lex4all/ 2http://msdn.microsoft.com/en-us/library/hh361572 3http://www.cmusphinx.org 109 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 109–114, Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapting multilingual or language-independent acoustic and language models to new languages from relatively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictionaries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit4 (RLAT), which allows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very lowor zero-resource languages. Furthermore, even high-le</context>
</contexts>
<marker>Kim, Khudanpur, 2003</marker>
<rawString>Woosung Kim and Sanjeev Khudanpur. 2003. Language model adaptation using cross-lingual information. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Qiao</author>
<author>Jahanzeb Sherwani</author>
<author>Roni Rosenfeld</author>
</authors>
<title>Small-vocabulary speech recognition for resource-scarce languages. In</title>
<date>2010</date>
<journal>ACM DEV</journal>
<volume>10</volume>
<contexts>
<context position="5984" citStr="Qiao et al., 2010" startWordPosition="860" endWordPosition="863">s agricultural information to low-literate farmers in India, recognizes 79 Hindi terms. For such smallvocabulary applications, an engine designed to recognize speech in a HRL can be used as-is to perform recognition of the LRL terms, given a grammar describing the allowable combinations and sequences of terms to be recognized, and a pronunciation lexicon mapping each target term to at least one pronunciation (sequence of phonemes) in the HRL (see Fig. 1 for an example). This is the thinking behind Speech-based Automated Learning of Accent and Articulation Mapping, or “Salaam” (Sherwani, 2009; Qiao et al., 2010; Chan and Rosenfeld, 2012), a method of cross-language phoneme-mapping that discovers accurate source-language pronunciations for terms in the target language. The basic idea is to discover the best pronunciation (phoneme sequence) for a target term by using the source-language recognition engine to perform phone decoding on one or more utterances of the term. As commercial 4http://i19pc5.ira.uka.de/rlat-dev &lt;lexicon version=&amp;quot;1.0&amp;quot; xmlns=&amp;quot;http://www .w3.org/2005/01/pronunciationlexicon&amp;quot; xml:lang=&amp;quot;en-US&amp;quot; alphabet =&amp;quot;x-microsoft-ups&amp;quot;&gt; &lt;lexeme&gt; &lt;grapheme&gt;beeni&lt;/grapheme&gt; &lt;phoneme&gt;B E NG I&lt;/phoneme</context>
<context position="7339" citStr="Qiao et al. (2010)" startWordPosition="1052" endWordPosition="1055">equences of American English phonemes. recognizers such as Microsoft’s are designed for word-decoding, and their APIs do not usually allow users access to the phone-decoding mode, the Salaam approach uses a specially designed “superwildcard” recognition grammar to mimic phone decoding and guide pronunciation discovery (Qiao et al., 2010; Chan and Rosenfeld, 2012). This allows the recognizer to identify the phoneme sequence best matching a given term, without any prior indication of how many phonemes that sequence should contain. Given this grammar and one or more audio recordings of the term, Qiao et al. (2010) use an iterative training algorithm to discover the best pronunciation(s) for that term, one phoneme at a time. Compared to pronunciations hand-written by a linguist, pronunciations generated automatically by this algorithm yield substantially higher recognition accuracy: Qiao et al. (2010) report word recognition accuracy rates in the range of 75-95% for vocabularies of 50 terms. Chan and Rosenfeld (2012) improve accuracy on larger vocabularies (up to approximately 88% for 100 terms) by applying an iterative discriminative training algorithm, identifying and removing pronunciations that caus</context>
<context position="9753" citStr="Qiao et al. (2010)" startWordPosition="1439" endWordPosition="1442">ecture of which is illustrated in Figure 2. A simple graphical user interface (GUI) allows users to type in the written form of each term in the target vocabulary, and select one or more audio recordings (.wav files) of that term. Given this input, the program uses the Salaam method to find the best pronunciation(s) for each term. This requires a pre-trained recognition engine for a HRL as well as a series of dynamically-created recognition grammars; the engine and grammars are constructed and managed using the MSP. We note here that our implementation of Salaam deviates slightly from that of Qiao et al. (2010), improving the time-efficiency and thus usability of the system (see Sec. 4). Once pronunciations for all terms in the vocabulary have been generated, the application outputs a pronunciation lexicon for the given terms as an XML file conforming to the Pronunciation Lexicon Specification.7 This lexicon can then be directly included in a speech recognition application built using the MSP API or a similar toolkit. 4 Pronunciation mapping 4.1 Recognition engine For the HRL recognizer we use the US English recognition engine of the MSP. The engine is used as-is, with no modifications to its underl</context>
<context position="11865" citStr="Qiao et al., 2010" startWordPosition="1774" endWordPosition="1777">sing the iterative Salaam algorithm (Sec. 2), which employs the source-language recognizer and a special recognition grammar. In the first pass, the algorithm finds the best candidate(s) for the first phoneme of the sample(s), then the first two phonemes in the second pass, and so on until a stopping criterion is met. In our implementation, we stop iterations if the top-scoring sequence for a term has not changed for three consecutive iterations (Chan and Rosenfeld, 2012), or if the best sequence from a given pass has a lower confidence score than the best sequence from the 111 previous pass (Qiao et al., 2010). In both cases, at least three passes are required. After the iterative training has completed, the nbest pronunciation sequences (with n specified by users – see Sec. 5.2) for each term are written to the lexicon, each in a &lt;phoneme&gt; element corresponding to the &lt;grapheme&gt; element containing the term’s orthographic form (see Fig. 1). 4.3 Running time A major challenge we faced in engineering a userfriendly application based on the Salaam algorithm was its long running time. The algorithm depends on a “super-wildcard” grammar that allows the recognizer to match each sample of a given term to </context>
<context position="13433" citStr="Qiao et al., 2010" startWordPosition="2044" endWordPosition="2047">he process takes approximately 1-2 hours on a standard modern laptop. For development and research, this long training time is a serious disadvantage. To speed up training, we limit the length of each “word” in the grammar to only one phoneme, instead of up to 3, giving e.g. 40 possibilities instead of tens of thousands. The algorithm can still discover pronunciation sequences of an arbitrary length, since, in each iteration, the phonemes discovered so far are prepended to the super-wildcard grammar, such that the phoneme sequence of the first “word” in the phrase grows longer with each pass (Qiao et al., 2010). However, the new implementation is an order of magnitude faster: constructing the same 25-term lexicon on the same hardware takes approximately 2-5 minutes, i.e. less than 10% of the previous training time. To ensure that the new implementation’s vastly improved running time does not come at the cost of reduced recognition accuracy, we evaluate and compare word recognition accuracy rates using lexicons built with the old and new implementations. The data we use for this evaluation is a subset of the Yoruba data collected by Qiao et al. (2010), comprising 25 Yoruba terms (words) uttered by 2 </context>
<context position="18936" citStr="Qiao et al., 2010" startWordPosition="2944" endWordPosition="2947">ce and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1). 8http://naudio.codeplex.com/ Figure 3: Screenshot of the lexicon builder. 5.2 Additional options As seen in Figure 3, lex4all includes optional controls for quick and easy fine-tuning of the lexiconbuilding process (the default settings are pictured). First of all, users can specify the maximum number of pronunciations (&lt;phoneme&gt; elements) per word that the lexicon may contain; allowing more pronunciations per word may improve recognition accuracy (Qiao et al., 2010; Chan and Rosenfeld, 2012). Secondly, users may train the lexicon using our modified, faster implementation of the Salaam algorithm or the original implementation. Finally, users may choose whether or not discriminative training is applied, and if so, how many passes are run (see Sec. 4.4). 6 Evaluation module for research In addition to its primary utility as a lexiconbuilding tool, lex4all is also a valuable research aide thanks to an evaluation module that allows users to quickly and easily evaluate the lexicons they have created. The evaluation tool allows users to browse their file syste</context>
<context position="21060" citStr="Qiao et al., 2010" startWordPosition="3274" endWordPosition="3277">e a different pronunciationdiscovery algorithm), and use the evaluation module to quickly assess the results. Therefore, lex4all facilitates not only application development but also further research into small-vocabulary speech recognition using mapped pronunciation lexicons. 7 Conclusion and future work We have presented lex4all, an open-source application that enables the rapid automatic creation of pronunciation lexicons in any (low-resource) language, using an out-of-the-box commercial recognizer for a high-resource language and the Salaam method for cross-language pronunciation mapping (Qiao et al., 2010; Chan and Rosenfeld, 2012). The application thus makes smallvocabulary speech recognition interfaces feasible in any language, since only minutes of training audio are required; given the built-in audio recorder, lexicons can be constructed even for zero-resource languages. Furthermore, lex4all’s flexible and open design and easy-to-use evaluation module make it a valuable tool for research in languageindependent small-vocabulary recognition. In future work, we plan to expand the selection of source-language recognizers; at the moment, lex4all only uses US English as the source language, but </context>
</contexts>
<marker>Qiao, Sherwani, Rosenfeld, 2010</marker>
<rawString>Fang Qiao, Jahanzeb Sherwani, and Roni Rosenfeld. 2010. Small-vocabulary speech recognition for resource-scarce languages. In ACM DEV ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Schlippe</author>
<author>Sebastian Ochs</author>
<author>Tanja Schultz</author>
</authors>
<title>Web-based tools and methods for rapid pronunciation dictionary creation.</title>
<date>2014</date>
<journal>Speech Communication,</journal>
<pages>56--101</pages>
<contexts>
<context position="4425" citStr="Schlippe et al., 2014" startWordPosition="619" endWordPosition="622">ual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 109–114, Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapting multilingual or language-independent acoustic and language models to new languages from relatively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictionaries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit4 (RLAT), which allows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very lowor zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be ac</context>
</contexts>
<marker>Schlippe, Ochs, Schultz, 2014</marker>
<rawString>Tim Schlippe, Sebastian Ochs, and Tanja Schultz. 2014. Web-based tools and methods for rapid pronunciation dictionary creation. Speech Communication, 56:101–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanja Schultz</author>
<author>Alex Waibel</author>
</authors>
<title>Languageindependent and language-adaptive acoustic modeling for speech recognition.</title>
<date>2001</date>
<journal>Speech Communication, 35(1-2):31 –</journal>
<volume>51</volume>
<contexts>
<context position="4286" citStr="Schultz and Waibel, 2001" startWordPosition="599" endWordPosition="602">ot 1http://lex4all.github.io/lex4all/ 2http://msdn.microsoft.com/en-us/library/hh361572 3http://www.cmusphinx.org 109 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 109–114, Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapting multilingual or language-independent acoustic and language models to new languages from relatively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictionaries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit4 (RLAT), which allows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very lowor zero-resource languages.</context>
</contexts>
<marker>Schultz, Waibel, 2001</marker>
<rawString>Tanja Schultz and Alex Waibel. 2001. Languageindependent and language-adaptive acoustic modeling for speech recognition. Speech Communication, 35(1-2):31 – 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahanzeb Sherwani</author>
<author>Roni Rosenfeld</author>
</authors>
<title>The case for speech technology for developing regions.</title>
<date>2008</date>
<booktitle>In HCI for Community and International Development Workshop,</booktitle>
<location>Florence, Italy.</location>
<contexts>
<context position="1421" citStr="Sherwani and Rosenfeld, 2008" startWordPosition="186" endWordPosition="189">ition engine designed for a high-resource source language (e.g. English). To build such lexicons, we employ an existing method for cross-language phoneme-mapping. The application also offers a built-in audio recorder that facilitates data collection, a significantly faster implementation of the phoneme-mapping technique, and an evaluation module that expedites research on small-vocabulary speech recognition for low-resource languages. 1 Introduction In recent years it has been demonstrated that speech recognition interfaces can be extremely beneficial for applications in the developing world (Sherwani and Rosenfeld, 2008; Sherwani, 2009; Bali et al., 2013). Typically, such applications target low-resource languages (LRLs) for which large collections of speech data are unavailable, preventing the training or adaptation of recognition engines for these languages. However, an existing recognizer for a completely unrelated highresource language (HRL), such as English, can be used to perform small-vocabulary recognition tasks in the LRL, given a pronunciation lexicon mapping each term in the target vocabulary to a sequence of phonemes in the HRL, i.e. phonemes which the recognizer can model. This is the motivation</context>
<context position="11058" citStr="Sherwani and Rosenfeld, 2008" startWordPosition="1639" endWordPosition="1642">s to maintain comparability with the work of Qiao et al. (2010) and Chan and Rosenfeld (2012). Following these authors, we use an engine designed for server-side recognition 5Windows 7 or 8 (64-bit). 6http://github.com/lex4all/lex4all 7http://www.w3.org/TR/pronunciation-lexicon/ Figure 2: Overview of the core components of the lex4all lexicon-building application. of low-quality audio, since we aim to enable the creation of useful applications for LRLs, including those spoken in developing-world communities, and such applications should be able to cope with telephone-quality audio or similar (Sherwani and Rosenfeld, 2008). 4.2 Implementation of the Salaam method Pronunciations (sequences of source-language phonemes) for each term in the target vocabulary are generated from the audio sample(s) of that term using the iterative Salaam algorithm (Sec. 2), which employs the source-language recognizer and a special recognition grammar. In the first pass, the algorithm finds the best candidate(s) for the first phoneme of the sample(s), then the first two phonemes in the second pass, and so on until a stopping criterion is met. In our implementation, we stop iterations if the top-scoring sequence for a term has not ch</context>
</contexts>
<marker>Sherwani, Rosenfeld, 2008</marker>
<rawString>Jahanzeb Sherwani and Roni Rosenfeld. 2008. The case for speech technology for developing regions. In HCI for Community and International Development Workshop, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahanzeb Sherwani</author>
</authors>
<title>Speech interfaces for information access by low literate users.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="1437" citStr="Sherwani, 2009" startWordPosition="190" endWordPosition="191">gh-resource source language (e.g. English). To build such lexicons, we employ an existing method for cross-language phoneme-mapping. The application also offers a built-in audio recorder that facilitates data collection, a significantly faster implementation of the phoneme-mapping technique, and an evaluation module that expedites research on small-vocabulary speech recognition for low-resource languages. 1 Introduction In recent years it has been demonstrated that speech recognition interfaces can be extremely beneficial for applications in the developing world (Sherwani and Rosenfeld, 2008; Sherwani, 2009; Bali et al., 2013). Typically, such applications target low-resource languages (LRLs) for which large collections of speech data are unavailable, preventing the training or adaptation of recognition engines for these languages. However, an existing recognizer for a completely unrelated highresource language (HRL), such as English, can be used to perform small-vocabulary recognition tasks in the LRL, given a pronunciation lexicon mapping each term in the target vocabulary to a sequence of phonemes in the HRL, i.e. phonemes which the recognizer can model. This is the motivation behind lex4all,</context>
<context position="5965" citStr="Sherwani, 2009" startWordPosition="858" endWordPosition="859">ion that delivers agricultural information to low-literate farmers in India, recognizes 79 Hindi terms. For such smallvocabulary applications, an engine designed to recognize speech in a HRL can be used as-is to perform recognition of the LRL terms, given a grammar describing the allowable combinations and sequences of terms to be recognized, and a pronunciation lexicon mapping each target term to at least one pronunciation (sequence of phonemes) in the HRL (see Fig. 1 for an example). This is the thinking behind Speech-based Automated Learning of Accent and Articulation Mapping, or “Salaam” (Sherwani, 2009; Qiao et al., 2010; Chan and Rosenfeld, 2012), a method of cross-language phoneme-mapping that discovers accurate source-language pronunciations for terms in the target language. The basic idea is to discover the best pronunciation (phoneme sequence) for a target term by using the source-language recognition engine to perform phone decoding on one or more utterances of the term. As commercial 4http://i19pc5.ira.uka.de/rlat-dev &lt;lexicon version=&amp;quot;1.0&amp;quot; xmlns=&amp;quot;http://www .w3.org/2005/01/pronunciationlexicon&amp;quot; xml:lang=&amp;quot;en-US&amp;quot; alphabet =&amp;quot;x-microsoft-ups&amp;quot;&gt; &lt;lexeme&gt; &lt;grapheme&gt;beeni&lt;/grapheme&gt; &lt;phonem</context>
<context position="8311" citStr="Sherwani, 2009" startWordPosition="1200" endWordPosition="1202">r vocabularies of 50 terms. Chan and Rosenfeld (2012) improve accuracy on larger vocabularies (up to approximately 88% for 100 terms) by applying an iterative discriminative training algorithm, identifying and removing pronunciations that cause confusion between word types. The Salaam method is fully automatic, demanding expertise neither in speech technology nor in linguistics, and requires only a few recorded utterances of each word. At least two projects have successfully used the Salaam method to add voice interfaces to real applications: an Urdu telephone-based health information system (Sherwani, 2009), and the VideoKheti application mentioned above (Bali et al., 2013). What has not existed before now is an interface that makes this approach accessible to any user. 110 Given the established success of the Salaam method, our contribution is to create a more timeefficient implementation of the pronunciationdiscovery algorithm and integrate it into an easyto-use graphical application. In the following sections, we describe this application and our slightly modified implementation of the Salaam method. 3 System overview We have developed lex4all as a desktop application for Microsoft Windows,5 </context>
</contexts>
<marker>Sherwani, 2009</marker>
<rawString>Jahanzeb Sherwani. 2009. Speech interfaces for information access by low literate users. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anjana Vakil</author>
<author>Alexis Palmer</author>
</authors>
<title>Crosslanguage mapping for small-vocabulary ASR in under-resourced languages: Investigating the impact of source language choice.</title>
<date>2014</date>
<booktitle>In SLTU ’14.</booktitle>
<contexts>
<context position="22025" citStr="Vakil and Palmer, 2014" startWordPosition="3418" endWordPosition="3421">aluation module make it a valuable tool for research in languageindependent small-vocabulary recognition. In future work, we plan to expand the selection of source-language recognizers; at the moment, lex4all only uses US English as the source language, but any of the 20+ other HRLs supported by the MSP could be added. This would enable investigation of the target-language recognition accuracy obtained using different source languages, though our initial exploration of this issue suggests that phonetic similarity between the source and target languages might not significantly affect accuracy (Vakil and Palmer, 2014). Another future goal is to improve and extend the functionality of the audio-recording tool to make it more flexible and user-friendly. Finally, as a complement to the application, it would be beneficial to create a central online data repository where users can upload the lexicons they have built and the speech samples they have recorded. Over time, this could become a valuable collection of LRL data, enabling developers and researchers to share and re-use data among languages or language families. Acknowledgements The first author was partially supported by a Deutschlandstipendium scholarsh</context>
</contexts>
<marker>Vakil, Palmer, 2014</marker>
<rawString>Anjana Vakil and Alexis Palmer. 2014. Crosslanguage mapping for small-vocabulary ASR in under-resourced languages: Investigating the impact of source language choice. In SLTU ’14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc Thang Vu</author>
<author>Tim Schlippe</author>
<author>Franziska Kraus</author>
<author>Tanja Schultz</author>
</authors>
<title>Rapid bootstrapping of five Eastern European languages using the Rapid Language Adaptation Toolkit. In Interspeech.</title>
<date>2010</date>
<contexts>
<context position="4648" citStr="Vu et al., 2010" startWordPosition="653" endWordPosition="656">rcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapting multilingual or language-independent acoustic and language models to new languages from relatively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictionaries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit4 (RLAT), which allows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very lowor zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users. However, many useful applications (e.g. for accessing information or conducting basic transactions by telephone) only require small-vocabulary recognition, i.e. discrimination between a few dozen term</context>
</contexts>
<marker>Vu, Schlippe, Kraus, Schultz, 2010</marker>
<rawString>Ngoc Thang Vu, Tim Schlippe, Franziska Kraus, and Tanja Schultz. 2010. Rapid bootstrapping of five Eastern European languages using the Rapid Language Adaptation Toolkit. In Interspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>