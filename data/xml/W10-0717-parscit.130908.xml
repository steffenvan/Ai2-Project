<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.256467">
<title confidence="0.998147">
Using Mechanical Turk to Annotate Lexicons for
Less Commonly Used Languages
</title>
<author confidence="0.980509">
Ann Irvine and Alexandre Klementiev
</author>
<affiliation confidence="0.920543">
Computer Science Department
Johns Hopkins University
</affiliation>
<address confidence="0.959347">
Baltimore, MD 21218
</address>
<email confidence="0.9998">
{anni,aklement}@jhu.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808772727273">
In this work we present results from using
Amazon’s Mechanical Turk (MTurk) to an-
notate translation lexicons between English
and a large set of less commonly used lan-
guages. We generate candidate translations for
100 English words in each of 42 foreign lan-
guages using Wikipedia and a lexicon induc-
tion framework. We evaluate the MTurk an-
notations by using positive and negative con-
trol candidate translations. Additionally, we
evaluate the annotations by adding pairs to our
seed dictionaries, providing a feedback loop
into the induction system. MTurk workers are
more successful in annotating some languages
than others and are not evenly distributed
around the world or among the world’s lan-
guages. However, in general, we find that
MTurk is a valuable resource for gathering
cheap and simple annotations for most of the
languages that we explored, and these anno-
tations provide useful feedback in building a
larger, more accurate lexicon.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999874">
In this work, we make use of several free and cheap
resources to create high quality lexicons for less
commonly used languages. First, we take advan-
tage of small existing dictionaries and freely avail-
able Wikipedia monolingual data to induce addi-
tional lexical translation pairs. Then, we pay Me-
chanical Turk workers a small amount to check and
correct our system output. We can then use the up-
dated lexicons to inform another iteration of lexicon
induction, gather a second set of MTurk annotations,
and so on.
Here, we provide results of one iteration of MTurk
annotation. We discuss the feasibility of using
MTurk for annotating translation lexicons between
English and 42 less commonly used languages. Our
primary goal is to enlarge and enrich the small,
noisy bilingual dictionaries that we have for each
language. Our secondary goal is to study the quality
of annotations that we can expect to obtain for our
set of low resource languages. We evaluate the anno-
tations both alone and as feedback into our lexicon
induction system.
</bodyText>
<sectionHeader confidence="0.990229" genericHeader="method">
2 Inducing Translation Candidates
</sectionHeader>
<bodyText confidence="0.999775">
Various linguistic and corpus cues are helpful for re-
lating word translations across a pair of languages.
A plethora of prior work has exploited orthographic,
topic, and contextual similarity, to name a few
(Rapp, 1999; Fung and Yee, 1998; Koehn and
Knight, 2000; Mimno et al., 2009; Schafer and
Yarowsky, 2002; Haghighi et al., 2008; Garera et
al., 2008). In this work, our aim is to induce trans-
lation candidates for further MTurk annotation for a
large number of language pairs with varying degrees
of relatedness and resource availability. Therefore,
we opt for a simple and language agnostic approach
of using contextual information to score translations
and discover a set of candidates for further anno-
tation. Table 1 shows our 42 languages of interest
and the number of Wikipedia articles with interlin-
gual links to their English counterparts. The idea
is that tokens which tend to appear in the context
of a given type in one language should be similar
to contextual tokens of its translation in the other
language. Each word can thus be represented as a
</bodyText>
<page confidence="0.974162">
108
</page>
<note confidence="0.7171545">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 108–113,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.999867095238095">
Tigrinya 36 Punjabi 401
Kyrgyz 492 Somali 585
Nepali 1293 Tibetan 1358
Uighur 1814 Maltese 1896
Turkmen 3137 Kazakh 3470
Mongolian 4009 Tatar 4180
Kurdish 5059 Uzbek 5875
Kapampangan 6827 Urdu 7674
Irish 9859 Azeri 12568
Tamil 13470 Albanian 13714
Afrikaans 14315 Hindi 14824
Bangla 16026 Tagalog 17757
Latvian 22737 Bosnian 23144
Welsh 25292 Latin 31195
Basque 38594 Thai 40182
Farsi 58651 Bulgarian 68446
Serbian 71018 Indonesian 73962
Slovak 76421 Korean 84385
Turkish 86277 Ukrainan 91022
Romanian 97351 Russian 295944
Spanish 371130 Polish 438053
</table>
<tableCaption confidence="0.980944">
Table 1: Our 42 languages of interest and the number of
Wikipedia pages for each that have interlanguage links
with English.
</tableCaption>
<bodyText confidence="0.999947230769231">
vector of contextual word indices. Following Rapp
(1999), we use a small seed dictionary to project1
the contextual vector of a source word into the tar-
get language, and score its overlap with contextual
vectors of candidate translations, see Figure 1. Top
scoring target language words obtained in this man-
ner are used as candidate translations for MTurk an-
notation. While longer lists will increase the chance
of including correct translations and their morpho-
logical variants, they require more effort on the part
of annotators. To strike a reasonable balance, we ex-
tracted relatively short candidate lists, but allowed
MTurk users to type their own translations as well.
</bodyText>
<sectionHeader confidence="0.994777" genericHeader="method">
3 Mechanical Turk Task
</sectionHeader>
<bodyText confidence="0.9997344">
Following previous work on posting NLP tasks on
MTurk (Snow et al., 2008; Callison-Burch, 2009),
we use the service to gather annotations for proposed
bilingual lexicon entries. For 32 of our 42 languages
of interest, we were able to induce lexical translation
</bodyText>
<footnote confidence="0.993821">
1A simple string match is used for projection. While we
expect that more sophisticated approaches (e.g. exploiting mor-
phological analyses) are likely to help, we cannot assume that
such linguistic resources are available for our languages.
</footnote>
<figureCaption confidence="0.9950895">
Figure 1: Lexicon induction using contextual informa-
tion. First, contextual vectors are projected using small
dictionaries and then they are compared with the target
language candidates.
</figureCaption>
<bodyText confidence="0.999866866666667">
candidates and post them on MTurk for annotation.
We do not have dictionaries for the remaining ten,
so, for those languages, we simply posted a set of
100 English words and asked workers for manual
translations. We had three distinct workers translate
each word.
For the 32 languages for which we proposed
translation candidates, we divided our set of 100
English words into sets of ten English words to be
completed within a single HIT. MTurk defines HIT
(Human Intelligence Task) as a self-contained unit
of work that requesters can post and pay workers a
small fee for completing. We requested that three
MTurk workers complete each of the ten HITs for
each language. For each English word within a HIT,
we posted ten candidate translations in the foreign
language and asked users to check the boxes beside
any and all of the words that were translations of the
English word. We paid workers $0.10 for complet-
ing each HIT. If our seed dictionary included an en-
try for a given English word, we included that in the
candidate list as a positive control. Additionally, we
included a random word in the foreign language as
a negative control. The remaining eight or nine can-
didate translations were proposed by our induction
system. We randomized the order in which the can-
didates appeared to workers and presented the words
as images rather than text to discourage copying and
pasting into online translation systems.
In addition to gathering annotations on candidate
</bodyText>
<figure confidence="0.999359428571428">
e(i) f (1) f (2) f (N)
J
J
J
J
e(K-1)
e(K)
J
J
J
J
J
J
J
J
J
project
dict.
f (N-1)
f (N)
J
</figure>
<page confidence="0.78054">
109
</page>
<figureCaption confidence="0.996">
Figure 2: Distribution of MTurk workers around the
world
</figureCaption>
<bodyText confidence="0.9972085">
translations, we gathered the following information
in each HIT:
</bodyText>
<listItem confidence="0.990598666666667">
• Manual translations of each English word, es-
pecially for the cases where none of our pro-
posed candidate translations were accurate
• Geographical locations via IP addresses
• How the HIT was completed: knowledge of the
languages, paper dictionary, online dictionary
• Whether the workers were native speakers of
each language (English and foreign), and for
how many years they have spoken each
</listItem>
<sectionHeader confidence="0.99973" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9998845">
Figure 2 shows the percent of HITs that were com-
pleted in different countries. More than 60% of HITs
were completed by workers in India, more than half
of which were completed in the single city of Chen-
nai. Another 18% were completed in the United
States, and roughly 2% were completed in Romania,
Pakistan, Macedonia, Latvia, Bangladesh, and the
Philippines. Of all annotations, 54% reported that
the worker used knowledge of the two languages,
while 28% and 18% reported using paper and online
dictionaries, respectively, to complete the HITs.
Ninety-three MTurk workers completed at least
one of our HITs, and 53 completed at least two.
The average number of HITs completed per worker
was 12. One worker completed HITs for 17 differ-
ent languages, and nine workers completed HITs in
more than three languages. Of the ten prolific work-
ers, one was located in the United States, one in the
United Kingdom, and eight in India. Because we
posted each HIT three times, the minimum number
of workers per language was three. Exactly three
workers completed all ten HITs posted in the fol-
lowing languages: Kurdish, Maltese, Tatar, Kapam-
pangan, Uzbek, and Latvian. We found that the av-
erage number of workers per language was 5.2. Ten
distinct workers (identified with MTurk worker IDs)
completed Tamil HITs, and nine worked on the Farsi
HITs.
</bodyText>
<subsectionHeader confidence="0.998971">
4.1 Completion Time
</subsectionHeader>
<bodyText confidence="0.9999485">
Figure 3 shows the time that it took for our HITs
for 37 languages to be completed on MTurk. The
HITs for the following languages were posted for a
week and were never completed: Tigrinya, Uighur,
Tibetan, Kyrgyz, and Kazakh. All five of the un-
completed HIT sets required typing annotations, a
more time consuming task than checking transla-
tion candidates. Not surprisingly, languages with
many speakers (Hindi, Spanish, and Russian) and
languages spoken in and near India (Hindi, Tamil,
Urdu) were completed very quickly. The languages
for which we posted a manual translation only HIT
are marked with a * in Figure 3. The HIT type does
not seem to have affected the completion time.
</bodyText>
<subsectionHeader confidence="0.998451">
4.2 Annotation Quality
</subsectionHeader>
<bodyText confidence="0.99989845">
Lexicon Check Agreement. Figure 4 shows the
percent of positive control candidate translations
that were checked by the majority of workers (at
least two of three). The highest amounts of agree-
ment with the controls were for Spanish and Polish,
which indicates that those workers completed the
HITs more accurately than the workers who com-
pleted, for example, the Tatar and Thai HITs. How-
ever, as already mentioned, the seed dictionaries are
very noisy, so this finding may be confounded by
discrepancies in the quality of our dictionaries. The
noisy dictionaries also explain why agreement with
the positive controls is, in general, relatively low.
We also looked at the degree to which workers
agreed upon negative controls. The average per-
cent agreement between the (majority of) workers
and the negative controls over all 32 languages is
only 0.21%. The highest amount of agreement with
negative controls is for Kapampangan and Turkmen
(1.28% and 1.26%, respectively). These are two of
</bodyText>
<figure confidence="0.988169881355932">
Philippines
Bangladesh
Latvia
Macedonia None
Pakistan
Romania
Other
United States
India
110
Hours
160
140
120
100
40
80
60
20
0
Time to complete HITs
Time to first hit
Hindi*
Spanish
Russian
Tamil
Urdu*
Turkish
Romanian
Ukrainan*
Latvian
Polish
Albanian
Afrikaans
Bulgarian
Farsi
Indonesian
Welsh
Slovak
Tagalog
Maltese
Serbian
Uzbek
Thai
Bosnian
Korean
Nepali*
Irish
Mongolian*
Azeri
Punjabi
Bangla
Tatar
Kapampangan
Kurdish
Latin
Turkmen
Somali
Basque
</figure>
<figureCaption confidence="0.984025333333333">
Figure 3: Number of hours HITs posted on MTurk before completion; division of the time between posting and the
completion of one HIT and the time between the completion of the first and last HIT shown. HITs that required lexical
translation only (not checking candidate translations) are marked with an *.
</figureCaption>
<bodyText confidence="0.999956617647059">
the languages for which there was little agreement
with the positive controls, substantiating our claim
that those HITs were completed less accurately than
for other languages.
Manual Translation Agreement. For each En-
glish word, we encouraged workers to manually pro-
vide one or more translations into the foreign lan-
guage. Figure 5 shows the percent of English words
for which the MTurk workers provided and agreed
upon at least one manual translation. We defined
agreement as exact string match between at least
two of three workers, which is a conservative mea-
sure, especially for morphologically rich languages.
As shown, there was a large amount of agreement
among the manual translations for Ukrainian, Farsi,
Thai, and Korean. The MTurk workers did not pro-
vide any manual translations at all for the following
languages: Somali, Kurdish, Turkmen, Uzbek, Ka-
pampangan, and Tatar.
It’s easy to speculate that, despite discouraging
the use of online dictionaries and translation systems
by presenting text as images, users reached this high
level of agreement for manual translations by using
the same online translation systems. However, we
searched for 20 of the 57 English words for which
the workers agreed upon a manually entered Russian
translation in Google translate, and we found that the
Russian translation was the top Google translation
for only 11 of the 20 English words. Six of the Rus-
sian words did not appear at all in the list of trans-
lations for the given English word. Thus, we con-
clude that, at least for some of our languages of in-
terest, MTurk workers did provide accurate, human-
generated lexical translations.
</bodyText>
<subsectionHeader confidence="0.999064">
4.3 Using MTurk Annotations in Induction
</subsectionHeader>
<bodyText confidence="0.999904555555556">
To further test the usefulness of MTurk generated
bilingual lexicons, we supplemented our dictionar-
ies for each of the 37 languages for which we gath-
ered MTurk annotations with translation pairs that
workers agreed were good (both chosen from the
candidate set and manually translated). We com-
pared seed dictionaries of size 200 with those sup-
plemented with, on average, 69 translation pairs. We
found an average relative increase in accuracy of
our output candidate set (evaluated against complete
available dictionaries) of 53%. This improvement is
further evidence that we are able to gather high qual-
ity translations from MTurk, which can assist the
lexicon induction process. Additionally, this shows
that we could iteratively produce lexical translation
candidates and have MTurk workers annotate them,
supplementing the induction dictionaries over many
iterations. This framework would allow us to gener-
</bodyText>
<page confidence="0.988659">
111
</page>
<figure confidence="0.999681425">
Percent agreement on
positive controls
25
20
15
10
5
0
Spanish
Polish
Bosnian
Romanian
Irish
Bulgarian
Tagalog
Russian
Afrikaans
Welsh
Tamil
Maltese
Turkish
Latvian
Punjabi
Indonesian
Azeri
Slovak
Latin
Albanian
Uzbek
Basque
Serbian
Turkmen
Korean
Bangla
Somali
Kurdish
Farsi
Kapampangan
Tatar
Thai
</figure>
<figureCaption confidence="0.99882">
Figure 4: Percent of positive control candidate translations for which two or three workers checked as accurate.
</figureCaption>
<figure confidence="0.9998655">
Percent of English words with 100
agreed upon manual translations 90
80
70
60
50
40
30
20
10
0
Ukrainan*
Farsi
Thai
Korean
Irish
Maltese
Albanian
Slovak
Latvian
Turkish
Serbian
Welsh
Afrikaans
Indonesian
Hindi*
Polish
Russian
Urdu*
Tagalog
Romanian
Mongolian*
Spanish
Bulgarian
Bosnian
Azeri
Nepali*
Latin
Tamil
Punjabi
Bangla
Basque
Somali
Kurdish
Turkmen
Uzbek
Kapampangan
Tatar
</figure>
<figureCaption confidence="0.998299333333333">
Figure 5: Percent of 100 English words for which at least two of three MTurk workers provided at least one matching
manual translation; HITs that required lexical translation only (not checking candidate translations) are marked with
an *.
</figureCaption>
<bodyText confidence="0.984198">
ate very large and high quality dictionaries starting
with a very small set of seed translation pairs.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999352533333333">
The goal of this work was to use Amazon’s Mechan-
ical Turk to collect and evaluate the quality of trans-
lation lexicons for a large set of low resource lan-
guages. In order to make the annotation task easier
and maximize the amount of annotation given our
budget and time constraints, we used contextual sim-
ilarity along with small bilingual dictionaries to ex-
tract a set of translation candidates for MTurk anno-
tation. For ten of our languages without dictionaries,
we asked workers to type translations directly. We
were able to get complete annotations of both types
quickly for 37 of our languages. The other five lan-
guages required annotations of the latter type, which
may explain why they remained unfinished.
We used annotator agreement with positive and
negative controls to assess the quality of generated
lexicons and provide an indication of the relative
difficulty of obtaining high quality annotations for
each language. Not surprisingly, annotation agree-
ment tends to be low for those languages which are
especially low resource, as measured by the num-
ber of Wikipedia pages. Because there are relatively
few native speakers of these languages in the on-
line community, those HITs were likely completed
by non-native speakers. Finally, we demonstrated
that augmenting small seed dictionaries with the ob-
tained lexicons substantially impacts contextual lex-
icon induction with an average relative gain of 53%
in accuracy across languages.
In sum, we found that the iterative approach of au-
</bodyText>
<page confidence="0.993927">
112
</page>
<bodyText confidence="0.9996984">
tomatically generating noisy annotation and asking
MTurk users to correct it to be an effective means of
obtaining supervision. Our manual annotation tasks
are simple and annotation can be obtained quickly
for a large number of low resource languages.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866272727273">
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazons mechan-
ical turk. In Proceedings of EMNLP.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of ACL, pages 414–420.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2008. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In Proceedings
of CoNLL.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771–779.
Philipp Koehn and Kevin Knight. 2000. Estimating word
translation probabilities from unrelated monolingual
corpora using the EM algorithm. In Proceedings of
AAAI.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of EMNLP.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519–526.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity mea-
sures and bridge languages. In Proceedings of CoNLL,
pages 146–152.
Rion Snow, Brendan OConnor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP.
</reference>
<page confidence="0.99932">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.624427">
<title confidence="0.99873">Using Mechanical Turk to Annotate Lexicons Less Commonly Used Languages</title>
<author confidence="0.984832">Ann Irvine</author>
<author confidence="0.984832">Alexandre</author>
<affiliation confidence="0.8263905">Computer Science Johns Hopkins</affiliation>
<address confidence="0.971849">Baltimore, MD</address>
<abstract confidence="0.999681608695652">In this work we present results from using Amazon’s Mechanical Turk (MTurk) to annotate translation lexicons between English and a large set of less commonly used languages. We generate candidate translations for 100 English words in each of 42 foreign languages using Wikipedia and a lexicon induction framework. We evaluate the MTurk annotations by using positive and negative control candidate translations. Additionally, we evaluate the annotations by adding pairs to our seed dictionaries, providing a feedback loop into the induction system. MTurk workers are more successful in annotating some languages than others and are not evenly distributed around the world or among the world’s languages. However, in general, we find that MTurk is a valuable resource for gathering cheap and simple annotations for most of the languages that we explored, and these annotations provide useful feedback in building a larger, more accurate lexicon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using amazons mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4996" citStr="Callison-Burch, 2009" startWordPosition="795" endWordPosition="796">ore its overlap with contextual vectors of candidate translations, see Figure 1. Top scoring target language words obtained in this manner are used as candidate translations for MTurk annotation. While longer lists will increase the chance of including correct translations and their morphological variants, they require more effort on the part of annotators. To strike a reasonable balance, we extracted relatively short candidate lists, but allowed MTurk users to type their own translations as well. 3 Mechanical Turk Task Following previous work on posting NLP tasks on MTurk (Snow et al., 2008; Callison-Burch, 2009), we use the service to gather annotations for proposed bilingual lexicon entries. For 32 of our 42 languages of interest, we were able to induce lexical translation 1A simple string match is used for projection. While we expect that more sophisticated approaches (e.g. exploiting morphological analyses) are likely to help, we cannot assume that such linguistic resources are available for our languages. Figure 1: Lexicon induction using contextual information. First, contextual vectors are projected using small dictionaries and then they are compared with the target language candidates. candida</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using amazons mechanical turk. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>414--420</pages>
<contexts>
<context position="2485" citStr="Fung and Yee, 1998" startWordPosition="392" endWordPosition="395">used languages. Our primary goal is to enlarge and enrich the small, noisy bilingual dictionaries that we have for each language. Our secondary goal is to study the quality of annotations that we can expect to obtain for our set of low resource languages. We evaluate the annotations both alone and as feedback into our lexicon induction system. 2 Inducing Translation Candidates Various linguistic and corpus cues are helpful for relating word translations across a pair of languages. A plethora of prior work has exploited orthographic, topic, and contextual similarity, to name a few (Rapp, 1999; Fung and Yee, 1998; Koehn and Knight, 2000; Mimno et al., 2009; Schafer and Yarowsky, 2002; Haghighi et al., 2008; Garera et al., 2008). In this work, our aim is to induce translation candidates for further MTurk annotation for a large number of language pairs with varying degrees of relatedness and resource availability. Therefore, we opt for a simple and language agnostic approach of using contextual information to score translations and discover a set of candidates for further annotation. Table 1 shows our 42 languages of interest and the number of Wikipedia articles with interlingual links to their English </context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of ACL, pages 414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>Chris Callison-Burch</author>
<author>David Yarowsky</author>
</authors>
<title>Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2602" citStr="Garera et al., 2008" startWordPosition="412" endWordPosition="415">each language. Our secondary goal is to study the quality of annotations that we can expect to obtain for our set of low resource languages. We evaluate the annotations both alone and as feedback into our lexicon induction system. 2 Inducing Translation Candidates Various linguistic and corpus cues are helpful for relating word translations across a pair of languages. A plethora of prior work has exploited orthographic, topic, and contextual similarity, to name a few (Rapp, 1999; Fung and Yee, 1998; Koehn and Knight, 2000; Mimno et al., 2009; Schafer and Yarowsky, 2002; Haghighi et al., 2008; Garera et al., 2008). In this work, our aim is to induce translation candidates for further MTurk annotation for a large number of language pairs with varying degrees of relatedness and resource availability. Therefore, we opt for a simple and language agnostic approach of using contextual information to score translations and discover a set of candidates for further annotation. Table 1 shows our 42 languages of interest and the number of Wikipedia articles with interlingual links to their English counterparts. The idea is that tokens which tend to appear in the context of a given type in one language should be s</context>
</contexts>
<marker>Garera, Callison-Burch, Yarowsky, 2008</marker>
<rawString>Nikesh Garera, Chris Callison-Burch, and David Yarowsky. 2008. Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="2580" citStr="Haghighi et al., 2008" startWordPosition="408" endWordPosition="411">aries that we have for each language. Our secondary goal is to study the quality of annotations that we can expect to obtain for our set of low resource languages. We evaluate the annotations both alone and as feedback into our lexicon induction system. 2 Inducing Translation Candidates Various linguistic and corpus cues are helpful for relating word translations across a pair of languages. A plethora of prior work has exploited orthographic, topic, and contextual similarity, to name a few (Rapp, 1999; Fung and Yee, 1998; Koehn and Knight, 2000; Mimno et al., 2009; Schafer and Yarowsky, 2002; Haghighi et al., 2008; Garera et al., 2008). In this work, our aim is to induce translation candidates for further MTurk annotation for a large number of language pairs with varying degrees of relatedness and resource availability. Therefore, we opt for a simple and language agnostic approach of using contextual information to score translations and discover a set of candidates for further annotation. Table 1 shows our 42 languages of interest and the number of Wikipedia articles with interlingual links to their English counterparts. The idea is that tokens which tend to appear in the context of a given type in on</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Estimating word translation probabilities from unrelated monolingual corpora using the EM algorithm.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="2509" citStr="Koehn and Knight, 2000" startWordPosition="396" endWordPosition="399">primary goal is to enlarge and enrich the small, noisy bilingual dictionaries that we have for each language. Our secondary goal is to study the quality of annotations that we can expect to obtain for our set of low resource languages. We evaluate the annotations both alone and as feedback into our lexicon induction system. 2 Inducing Translation Candidates Various linguistic and corpus cues are helpful for relating word translations across a pair of languages. A plethora of prior work has exploited orthographic, topic, and contextual similarity, to name a few (Rapp, 1999; Fung and Yee, 1998; Koehn and Knight, 2000; Mimno et al., 2009; Schafer and Yarowsky, 2002; Haghighi et al., 2008; Garera et al., 2008). In this work, our aim is to induce translation candidates for further MTurk annotation for a large number of language pairs with varying degrees of relatedness and resource availability. Therefore, we opt for a simple and language agnostic approach of using contextual information to score translations and discover a set of candidates for further annotation. Table 1 shows our 42 languages of interest and the number of Wikipedia articles with interlingual links to their English counterparts. The idea i</context>
</contexts>
<marker>Koehn, Knight, 2000</marker>
<rawString>Philipp Koehn and Kevin Knight. 2000. Estimating word translation probabilities from unrelated monolingual corpora using the EM algorithm. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Jason Naradowsky</author>
<author>David Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2529" citStr="Mimno et al., 2009" startWordPosition="400" endWordPosition="403">ge and enrich the small, noisy bilingual dictionaries that we have for each language. Our secondary goal is to study the quality of annotations that we can expect to obtain for our set of low resource languages. We evaluate the annotations both alone and as feedback into our lexicon induction system. 2 Inducing Translation Candidates Various linguistic and corpus cues are helpful for relating word translations across a pair of languages. A plethora of prior work has exploited orthographic, topic, and contextual similarity, to name a few (Rapp, 1999; Fung and Yee, 1998; Koehn and Knight, 2000; Mimno et al., 2009; Schafer and Yarowsky, 2002; Haghighi et al., 2008; Garera et al., 2008). In this work, our aim is to induce translation candidates for further MTurk annotation for a large number of language pairs with varying degrees of relatedness and resource availability. Therefore, we opt for a simple and language agnostic approach of using contextual information to score translations and discover a set of candidates for further annotation. Table 1 shows our 42 languages of interest and the number of Wikipedia articles with interlingual links to their English counterparts. The idea is that tokens which </context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna Wallach, Jason Naradowsky, David Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>519--526</pages>
<contexts>
<context position="2465" citStr="Rapp, 1999" startWordPosition="390" endWordPosition="391">ss commonly used languages. Our primary goal is to enlarge and enrich the small, noisy bilingual dictionaries that we have for each language. Our secondary goal is to study the quality of annotations that we can expect to obtain for our set of low resource languages. We evaluate the annotations both alone and as feedback into our lexicon induction system. 2 Inducing Translation Candidates Various linguistic and corpus cues are helpful for relating word translations across a pair of languages. A plethora of prior work has exploited orthographic, topic, and contextual similarity, to name a few (Rapp, 1999; Fung and Yee, 1998; Koehn and Knight, 2000; Mimno et al., 2009; Schafer and Yarowsky, 2002; Haghighi et al., 2008; Garera et al., 2008). In this work, our aim is to induce translation candidates for further MTurk annotation for a large number of language pairs with varying degrees of relatedness and resource availability. Therefore, we opt for a simple and language agnostic approach of using contextual information to score translations and discover a set of candidates for further annotation. Table 1 shows our 42 languages of interest and the number of Wikipedia articles with interlingual lin</context>
<context position="4259" citStr="Rapp (1999)" startWordPosition="676" endWordPosition="677">ongolian 4009 Tatar 4180 Kurdish 5059 Uzbek 5875 Kapampangan 6827 Urdu 7674 Irish 9859 Azeri 12568 Tamil 13470 Albanian 13714 Afrikaans 14315 Hindi 14824 Bangla 16026 Tagalog 17757 Latvian 22737 Bosnian 23144 Welsh 25292 Latin 31195 Basque 38594 Thai 40182 Farsi 58651 Bulgarian 68446 Serbian 71018 Indonesian 73962 Slovak 76421 Korean 84385 Turkish 86277 Ukrainan 91022 Romanian 97351 Russian 295944 Spanish 371130 Polish 438053 Table 1: Our 42 languages of interest and the number of Wikipedia pages for each that have interlanguage links with English. vector of contextual word indices. Following Rapp (1999), we use a small seed dictionary to project1 the contextual vector of a source word into the target language, and score its overlap with contextual vectors of candidate translations, see Figure 1. Top scoring target language words obtained in this manner are used as candidate translations for MTurk annotation. While longer lists will increase the chance of including correct translations and their morphological variants, they require more effort on the part of annotators. To strike a reasonable balance, we extracted relatively short candidate lists, but allowed MTurk users to type their own tra</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of ACL, pages 519–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>146--152</pages>
<contexts>
<context position="2557" citStr="Schafer and Yarowsky, 2002" startWordPosition="404" endWordPosition="407">all, noisy bilingual dictionaries that we have for each language. Our secondary goal is to study the quality of annotations that we can expect to obtain for our set of low resource languages. We evaluate the annotations both alone and as feedback into our lexicon induction system. 2 Inducing Translation Candidates Various linguistic and corpus cues are helpful for relating word translations across a pair of languages. A plethora of prior work has exploited orthographic, topic, and contextual similarity, to name a few (Rapp, 1999; Fung and Yee, 1998; Koehn and Knight, 2000; Mimno et al., 2009; Schafer and Yarowsky, 2002; Haghighi et al., 2008; Garera et al., 2008). In this work, our aim is to induce translation candidates for further MTurk annotation for a large number of language pairs with varying degrees of relatedness and resource availability. Therefore, we opt for a simple and language agnostic approach of using contextual information to score translations and discover a set of candidates for further annotation. Table 1 shows our 42 languages of interest and the number of Wikipedia articles with interlingual links to their English counterparts. The idea is that tokens which tend to appear in the contex</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Proceedings of CoNLL, pages 146–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan OConnor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4973" citStr="Snow et al., 2008" startWordPosition="791" endWordPosition="794">et language, and score its overlap with contextual vectors of candidate translations, see Figure 1. Top scoring target language words obtained in this manner are used as candidate translations for MTurk annotation. While longer lists will increase the chance of including correct translations and their morphological variants, they require more effort on the part of annotators. To strike a reasonable balance, we extracted relatively short candidate lists, but allowed MTurk users to type their own translations as well. 3 Mechanical Turk Task Following previous work on posting NLP tasks on MTurk (Snow et al., 2008; Callison-Burch, 2009), we use the service to gather annotations for proposed bilingual lexicon entries. For 32 of our 42 languages of interest, we were able to induce lexical translation 1A simple string match is used for projection. While we expect that more sophisticated approaches (e.g. exploiting morphological analyses) are likely to help, we cannot assume that such linguistic resources are available for our languages. Figure 1: Lexicon induction using contextual information. First, contextual vectors are projected using small dictionaries and then they are compared with the target langu</context>
</contexts>
<marker>Snow, OConnor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan OConnor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>