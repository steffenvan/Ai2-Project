<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000847">
<title confidence="0.905491">
ezDI: A Supervised NLP System for Clinical Narrative Analysis
</title>
<author confidence="0.802029">
Parth Pathak, Pinal Patel, Vishal Panchal, Sagar Soni,
Kinjal Dani, Narayan Choudhary, Amrish Patel
</author>
<affiliation confidence="0.602065">
ezDI, LLC.
</affiliation>
<address confidence="0.454651">
{parth.p, pinal.p, vishal.p, sagar.s,
</address>
<email confidence="0.89518">
kinjal.d, narayan.c, amrish.p} @ezdi.us
</email>
<sectionHeader confidence="0.994395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896826086957">
This paper describes the approach used by
ezDI at the SemEval 2015 Task-14: ”Anal-
ysis of Clinical Text”. The task was di-
vided into two embedded tasks. Task-1 re-
quired determining disorder boundaries (in-
cluding the discontiguous ones) from a given
set of clinical notes and normalizing the dis-
orders by assigning a unique CUI from the
UMLS/SNOMEDCT1. Task-2 was about find-
ing different type of modifiers for given disor-
der mention. Task-2 was divided further into
two subtasks. In subtask-2a, gold set of disor-
der was already provided and system needed
to just fill modifier types into the pre-specified
slots. Subtask 2b did not provide any gold set
of disorders and both the disorders and its re-
lated modifiers are to be identified by the sys-
tem itself. In Task-1 our system was ranked
first with F-score of 0.757 for strict evalua-
tion and 0.788 for relaxed evaluation. In both
Task-2a and 2b our system was placed second
with weighted F-score of 0.88 and 0.795 re-
spectively.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999854">
Extracting medical information from clinical natural
text has gained a lot of attraction over the past few
years. Approximately 80% of patient related infor-
mation resides under unstructured transcribed text.
Amount of this unstructured text is increasing con-
stantly and automated methods of extracting crucial
information is of paramount interest to health care
informatics industry. Task-14 of SemEval 2015 on
</bodyText>
<footnote confidence="0.867689">
1http://www.nlm.nih.gov/research/umls/
</footnote>
<bodyText confidence="0.999558352941177">
”analysis of clinical text” addresses the same con-
cern.
Task-14 of SemEval-2015 was in continuation of
the 2013 ShaRe/CLEF Task-1 (Suominen, H. et al.,
2013) and task-7 of SemEval 2014. The task was
divided into two parts. In continuation of last year,
task-1 was about finding disorder mentions from the
clinical text and associating them with their related
CUIs (concept unique identifiers) as given in the
UMLS (Unified Medical Language System). This
year one additional task (Task-2) of disorder modi-
fier slot filing was added. Task-2 was further sub-
divided into two parts. In subtask-2a, a gold set of
disorder mentions was provided and the participants
had to only fill the pre-specified slots with the nor-
malized modifiers. In task 2b, no gold set of disorder
mentions was provided. Figure1 provides detailed
overview about task 1 and 2.
Clinical NLP has evolved a lot in the tasks re-
lated to medical entity detection. NLP systems
like cTAKES (Savova, Guergana K., et al., 2010),
MetaMap (A. Aronson, 2001) and MedLEE (C.
Friedman et al., 1994) have focused on rule based
and dictionary look-up approaches for thid task. Re-
cently a few attempts have been made to use su-
pervised and semi-supervised learning models. In
2009, Yefang Wang (Wang et al., 2009) used cas-
cading classifiers on manually annotated data and
achieved around 83.2% accuracy. In 2010, i2b2
shared task challenge focused on finding test, treat-
ment and problem mentions from clinical document.
From 2013 on-words, entity detection task is regu-
larly featuring in Share/CLEF and SemEval tasks.
Tasks related to modifier slot filling are relatively
</bodyText>
<page confidence="0.973813">
412
</page>
<bodyText confidence="0.946142863636364">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 412–416,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
new and no extensive research has been done yet.
However for negation modifier, negEx (Chapman
et al., 2011) or various other variants of negEx have
been used in the last 10 years. These are keyword
based dictionary look-up algorithms, but still gives
around 92% of accuracy. However, these algorithms
are not scalable because there is no proper mecha-
nism defined to detect boundary for given negated
keyword. In 2010 i2b2 challenge, there was a sepa-
rate task for detecting 5 categories of negation. Sys-
tems used in this task showcase various statistical
approaches and the accuracy numbers were in the
range of 90 to 93%.
In this paper we have proposed a hybrid super-
vised learning approach based on CRF and SVM
to find out disorder mentions from clinical doc-
uments, a dictionary look-up approach on a cus-
tomized UMLS meta-thesaurus to find correspond-
ing CUIs and a SVM based generic approach to find
out all different disorder modifiers.
</bodyText>
<figureCaption confidence="0.998216">
Figure 1: Task-2 with Examples.
</figureCaption>
<sectionHeader confidence="0.900869" genericHeader="method">
2 Data Set
</sectionHeader>
<bodyText confidence="0.999856125">
The SemEval-2015 corpus comprises of de-
identified plain text from MIMIC2 version 2.5
database. A disorder mention was defined as any
span of text which can be mapped to a concept in
UMLS and which belongs to the disorder semantic
group. Some other disorders which were not present
in the UMLS were marked as CUI-less. The train-
ing and development data sets of the previous year’s
</bodyText>
<footnote confidence="0.539967">
2http://mimic.physionet.org/database/
releases/70-version-25.html
</footnote>
<bodyText confidence="0.999905166666667">
task were combined to be used as training set (298
documents) while the test data set of the previous
year was used as development set. There were 100
documents used as test data set. Same set of 4 hun-
dred thousand unlabelled documents were added to
encourage use of unsupervised learning methods.
</bodyText>
<sectionHeader confidence="0.938184" genericHeader="method">
3 Disorder Detection and Normalization
</sectionHeader>
<bodyText confidence="0.999736545454545">
For Task-1 our system was very similar to the sys-
tem we developed last year (Pathak, et al, 2014).
Entity detection task was converted into sequence
labelling task using BIO format. A Conditional Ran-
dom Fields (CRF) was used to detect continuous en-
tity using CRF++3 toolkit. To detect discontiguous
entities, a binary SVM classifier was used to detect
whether relationship existed between two disorder
mentions or not. For contiguous entity detection
task, our feature set was very similar to the one we
used last year:
</bodyText>
<listItem confidence="0.9835769375">
• Standard features like bag of words (for win-
dow +2 to -2), word stemmer (snowball stem-
mer) 4, prefix and suffix of length 1 to 5.
• Orthographic features like word contains digit,
contains slash, contains special character and
word shape (ezDI becomes aA).
• Grammatical features like parts of speech
(PoS) tags for which we used an internally de-
veloped PoS tagger (Choudhary et al. , 2014),
chunk (using Charniak’s parser (Charniak and
Johnson , 2005)) and head of noun and verb
phrases.
• Dictionary look-up matches for window +2 to
-2, stop words
• Section header and document type information
and sentence cluster id
</listItem>
<bodyText confidence="0.996500833333334">
Support Vector Machine (LibSVM5) was used to
identify disjoint entities. For all the possible combi-
nation of entities within a sentence, we ran a binary
SVM classifier to find whether a relationship existed
between those two entities or not. Feature set con-
sisted of following features:
</bodyText>
<footnote confidence="0.9972645">
3http://crfpp.googlecode.com/
4http://snowball.tartarus.org/
5http://www.csie.ntu.edu.tw/\˜cjlin/
libsvm/
</footnote>
<page confidence="0.991314">
413
</page>
<listItem confidence="0.997356461538462">
• Bag of words, PoS tags and chunk labels for all
the tokens appearing in between two entities.
• Few simple rules were implemented on Char-
niak parse output to find relationship between
two entities. A binary feature was used stating
whether relationship was found using rules or
not.
• Position of preposition, conjunction, main verb
and special characters like colon (:), hyphen (-
) and semi colon (;) in the context of the first
entity.
• Binary feature stating whether any of the de-
tected entity contained head of a noun phrase.
</listItem>
<bodyText confidence="0.993862">
This hybrid approach was very helpful in detect-
ing disjoint entities. We got around 70% accuracy in
detecting disjoint entities using this approach.
</bodyText>
<subsectionHeader confidence="0.994233">
3.1 CUI Detection
</subsectionHeader>
<bodyText confidence="0.9664934">
CUI detection task was divided into three separate
steps:
1) Direct dictionary search: In the first step, for
each word found in an entity we found all of its lexi-
cal variants using LVG 6. After that, for all the possi-
ble permutations we tried searching the string in the
UMLS. If the string matched any UMLS entry, we
associated the corresponding CUI with that entity.
2) Dictionary search on modified entities: For a
better mapping of the entities detected by NLP in-
side the given input text, we found it to be a bet-
ter approach to divide the UMLS entities into vari-
ous phrases. This was done semi-automatically by
splitting the strings based on function words such
as prepositions, particles and non-nominal word
classes such as verbs, adjectives and adverbs. While
most of the disorder entities in UMLS can be con-
tained into a single noun phrase (NP) there are also
quite a few that contain multiple NPs related with
prepositional phrases (PPs), verb phrases (VPs) and
adjectival phrases (ADJPs).
This exercise gave us a modified version of the
UMLS disorder entities along with their CUIs. Table
4 gives a snapshot of what this customized UMLS
dictionary looked like.
</bodyText>
<footnote confidence="0.940521">
6http://lexsrv2.nlm.nih.gov/
</footnote>
<table confidence="0.9996734">
CUI Text P1 P2 P3
C001 Dribbling Dribbling from mouth
3132 from
mouth
C001 Bleeding Bleeding from nose
4591 from nose
C002 Hemorr- Hemo- from mouth
9163 hage from rrhage
mouth
C039 Chest pain Chest pain at rest
2685 at rest
9678 C026 Fatigue during pregn
Fatigue ancy
during
pregnancy
</table>
<tableCaption confidence="0.9893935">
Table 1: An example of the modified UMLS disorder en-
tities split as per their linguistic phrase types.
</tableCaption>
<listItem confidence="0.689674">
3) String similarity algorithm: If an entity was not
found even after the first two steps, then we gener-
ated a list of possible text span from UMLS which
can possibly match with the given entity. After
that, Levenshtein edit distance algorithm was used
to find best string match. If the best string match
was greater than a certain threshold value, the corre-
sponding CUI was associated with the entity other-
wise the entity was marked as ”CUI-less”.
</listItem>
<sectionHeader confidence="0.977473" genericHeader="method">
4 Modifier Detection:
</sectionHeader>
<bodyText confidence="0.963015117647059">
For this task we tried to develop a generic approach
so that it can be applied to any type of modifier. We
divided the task of modifier detection into two parts:
1) Modifier keywords detection 2) Relating detected
keywords with entity.
1) Modifier keywords detection: For each modi-
fier type, an extensive dictionary was prepared hav-
ing different possible keywords with its normalized
values. A simple dictionary look-up algorithm was
used to calculate a baseline accuracy. On train-
ing data set, accuracy ranged from 60% to 85% for
different modifier types. This baseline algorithm
achieved great recall but much less precision. To
counter this, we used CRF algorithm with common
features like bag of words, stem value and other or-
thographic features. CRF helped significantly in im-
proving precision for modifier keyword detection.
</bodyText>
<listItem confidence="0.636026">
2) Relating detected modifier with entities: We
</listItem>
<page confidence="0.998028">
414
</page>
<bodyText confidence="0.999955272727273">
treated this task similar to the task of finding re-
lationship between two entities. So a binary clas-
sifier was used to check if relationship existed be-
tween a modifier keyword and an entity or not. Fea-
ture set consisted of: Bag of Words between entity
and modifier keyword, PoS tags, a binary flag stat-
ing whether the modifier keyword and the entity ap-
peared in the same chunk or not, relative position
of entity and modifier, special characters appearing
in the sentence, section header (for subject modifier
type).
</bodyText>
<sectionHeader confidence="0.926585" genericHeader="method">
5 System Accuracy
</sectionHeader>
<bodyText confidence="0.9993815">
For Task-1, the accuracy was defined as the number
of pre-annotated spans with correctly generated
code divided by the total number of pre-annotated
spans.
</bodyText>
<figure confidence="0.522709">
Strict Accuracy — #of CUIs with Exact span match
— Total annotation in gold standard
#of CUIswithpartialspanmatch
Relaxed Accuracy —
</figure>
<figureCaption confidence="0.240074">
— Total annotation in gold standard
</figureCaption>
<bodyText confidence="0.98753675">
Both training and development data sets were
used for training purpose. We used only 1 run with
above mentioned system set up. We were ranked
first for this task with results shown in Table 3.
</bodyText>
<table confidence="0.999111333333333">
Precision Recall Accuracy
Strict 0.783 0.732 0.757
Relaxed 0.815 0.761 0.787
</table>
<tableCaption confidence="0.999061">
Table 2: Task-1 Results.
</tableCaption>
<bodyText confidence="0.999949">
For Task 2, weighted and unweighted accura-
cies were calculated. The unweighted accuracy is
the average of the per-disorder unweighted accuracy
over all the disorders in the test set. Each gold-
standard slot value is pre-assigned a weight based
on its prevalence in the training set. The weighted
accuracy is the average of the per-disorder weighted
accuracy over all the disorders in the test set.
Ranks for task-2 were given based on weighted
accuracy. ezDI was ranked second in both Task-2a
and Task-2b. The results were as given below:
</bodyText>
<sectionHeader confidence="0.999762" genericHeader="method">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.9816955">
Abbreviations and disjoint entities still cause a lot
of error in CUI normalization task. Dictionary re-
</bodyText>
<table confidence="0.993825666666667">
F A F*A WA F*WA
Task-2A 1 0.934 0.934 0.880 0.880
Task-2B 0.915 0.935 0.856 0.868 0.795
</table>
<tableCaption confidence="0.999714">
Table 3: Task-2 Results.
</tableCaption>
<bodyText confidence="0.999516375">
lated features are still not very helpful. Accuracy de-
creases significantly if medical domain is changed.
Probably more sophisticated approach will be re-
quired to fully utilize UMLS dictionary. There is
still a lot to explore in modifier detection. Statisti-
cal approaches are still not out-performing baseline
dictionary based approaches. Modifier boundary de-
tection is still a bigger challenge to be solved.
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999431090909091">
In this paper we have proposed a CRF and SVM
based hybrid approach to find disorder mentions
from a given clinical text, a novel dictionary look-up
approach for discovering CUIs from UMLS meta-
thesaurus and a generic statistical approach for mod-
ifier slot filling. Our system did produce competitive
results and was best among all the participants for
task 1. In future, we would like to explore semi-
supervised learning approaches to take advantage of
large amount of available un-annotated free clinical
text.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993827210526316">
Aronson, Alan R. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program.
Chapman W, Bridewell W, Hanbury P, Cooper G,
Buchanan B. 2001. A simple algorithm for identifying
negated findings and diseases in discharge summaries.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-best Parsing and MaxEnt Discriminative
Reranking.
Choudhary, Narayan, Parth Pathak, Pinal Patel, Vishal
Panchal. 2014. Annotating a Large Representative
Corpus of Clinical Notes for Parts of Speech.
Friedman C, Alderson PO, Austin JH, Cimino JJ, John-
son SB. 1994. A general natural-language text pro-
cessor for clinical radiology.
Pathak, Parth, Pinal Patel, Vishal Panchal, Narayan
Choudhary, Amrish Patel, Gautam Joshi. 2014. ezDI:
A Hybrid CRF and SVM based Model for Detecting
and Encoding Disorder Mentions in Clinical Notes.
</reference>
<page confidence="0.98743">
415
</page>
<reference confidence="0.99603">
Savova, Guergana K., James J. Masanz, Philip V. Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-
Schuler, and Christopher G. Chute. 2010. Mayo clin-
ical Text Analysis and Knowledge Extraction System
(cTAKES): architecture, component evaluation and
applications.
Suominen, Hanna, Sanna Salanter, Sumithra Velupillai,
Wendy W. Chapman, Guergana Savova, Noemie El-
hadad, Sameer Pradhan. 2013. Overview of the
ShARe/CLEF eHealth evaluation lab 2013.
Wang, Yefeng and Jon Patrick. 2009. Cascading classi-
fiers for named entity recognition in clinical notes.
</reference>
<page confidence="0.998569">
416
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265913">
<title confidence="0.999509">ezDI: A Supervised NLP System for Clinical Narrative Analysis</title>
<author confidence="0.737226">Parth Pathak</author>
<author confidence="0.737226">Pinal Patel</author>
<author confidence="0.737226">Vishal Panchal</author>
<author confidence="0.737226">Sagar Kinjal Dani</author>
<author confidence="0.737226">Narayan Choudhary</author>
<author confidence="0.737226">Amrish</author>
<email confidence="0.769134333333333">ezDI,pinal.p,vishal.p,narayan.c,</email>
<abstract confidence="0.993831">This paper describes the approach used by ezDI at the SemEval 2015 Task-14: ”Analysis of Clinical Text”. The task was divided into two embedded tasks. Task-1 required determining disorder boundaries (including the discontiguous ones) from a given set of clinical notes and normalizing the disorders by assigning a unique CUI from the Task-2 was about finding different type of modifiers for given disorder mention. Task-2 was divided further into two subtasks. In subtask-2a, gold set of disorder was already provided and system needed to just fill modifier types into the pre-specified slots. Subtask 2b did not provide any gold set of disorders and both the disorders and its related modifiers are to be identified by the system itself. In Task-1 our system was ranked first with F-score of 0.757 for strict evaluation and 0.788 for relaxed evaluation. In both Task-2a and 2b our system was placed second with weighted F-score of 0.88 and 0.795 respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program.</title>
<date>2001</date>
<contexts>
<context position="2715" citStr="Aronson, 2001" startWordPosition="430" endWordPosition="431">en in the UMLS (Unified Medical Language System). This year one additional task (Task-2) of disorder modifier slot filing was added. Task-2 was further subdivided into two parts. In subtask-2a, a gold set of disorder mentions was provided and the participants had to only fill the pre-specified slots with the normalized modifiers. In task 2b, no gold set of disorder mentions was provided. Figure1 provides detailed overview about task 1 and 2. Clinical NLP has evolved a lot in the tasks related to medical entity detection. NLP systems like cTAKES (Savova, Guergana K., et al., 2010), MetaMap (A. Aronson, 2001) and MedLEE (C. Friedman et al., 1994) have focused on rule based and dictionary look-up approaches for thid task. Recently a few attempts have been made to use supervised and semi-supervised learning models. In 2009, Yefang Wang (Wang et al., 2009) used cascading classifiers on manually annotated data and achieved around 83.2% accuracy. In 2010, i2b2 shared task challenge focused on finding test, treatment and problem mentions from clinical document. From 2013 on-words, entity detection task is regularly featuring in Share/CLEF and SemEval tasks. Tasks related to modifier slot filling are rel</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>Aronson, Alan R. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chapman</author>
<author>W Bridewell</author>
<author>P Hanbury</author>
<author>G Cooper</author>
<author>B Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Chapman W, Bridewell W, Hanbury P, Cooper G, Buchanan B. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine n-best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, Eugene and Mark Johnson. 2005. Coarseto-Fine n-best Parsing and MaxEnt Discriminative Reranking.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narayan Choudhary</author>
<author>Parth Pathak</author>
<author>Pinal Patel</author>
<author>Vishal Panchal</author>
</authors>
<title>Annotating a Large Representative Corpus of Clinical Notes for Parts of Speech.</title>
<date>2014</date>
<marker>Choudhary, Pathak, Patel, Panchal, 2014</marker>
<rawString>Choudhary, Narayan, Parth Pathak, Pinal Patel, Vishal Panchal. 2014. Annotating a Large Representative Corpus of Clinical Notes for Parts of Speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
<author>Alderson PO</author>
<author>Austin JH</author>
<author>Cimino JJ</author>
<author>Johnson SB</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<contexts>
<context position="2753" citStr="Friedman et al., 1994" startWordPosition="435" endWordPosition="438">l Language System). This year one additional task (Task-2) of disorder modifier slot filing was added. Task-2 was further subdivided into two parts. In subtask-2a, a gold set of disorder mentions was provided and the participants had to only fill the pre-specified slots with the normalized modifiers. In task 2b, no gold set of disorder mentions was provided. Figure1 provides detailed overview about task 1 and 2. Clinical NLP has evolved a lot in the tasks related to medical entity detection. NLP systems like cTAKES (Savova, Guergana K., et al., 2010), MetaMap (A. Aronson, 2001) and MedLEE (C. Friedman et al., 1994) have focused on rule based and dictionary look-up approaches for thid task. Recently a few attempts have been made to use supervised and semi-supervised learning models. In 2009, Yefang Wang (Wang et al., 2009) used cascading classifiers on manually annotated data and achieved around 83.2% accuracy. In 2010, i2b2 shared task challenge focused on finding test, treatment and problem mentions from clinical document. From 2013 on-words, entity detection task is regularly featuring in Share/CLEF and SemEval tasks. Tasks related to modifier slot filling are relatively 412 Proceedings of the 9th Int</context>
</contexts>
<marker>Friedman, PO, JH, JJ, SB, 1994</marker>
<rawString>Friedman C, Alderson PO, Austin JH, Cimino JJ, Johnson SB. 1994. A general natural-language text processor for clinical radiology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parth Pathak</author>
<author>Pinal Patel</author>
<author>Vishal Panchal</author>
<author>Narayan Choudhary</author>
<author>Amrish Patel</author>
<author>Gautam Joshi</author>
</authors>
<title>ezDI: A Hybrid CRF and SVM based Model for Detecting and Encoding Disorder Mentions in Clinical Notes.</title>
<date>2014</date>
<contexts>
<context position="5373" citStr="Pathak, et al, 2014" startWordPosition="860" endWordPosition="863">ich were not present in the UMLS were marked as CUI-less. The training and development data sets of the previous year’s 2http://mimic.physionet.org/database/ releases/70-version-25.html task were combined to be used as training set (298 documents) while the test data set of the previous year was used as development set. There were 100 documents used as test data set. Same set of 4 hundred thousand unlabelled documents were added to encourage use of unsupervised learning methods. 3 Disorder Detection and Normalization For Task-1 our system was very similar to the system we developed last year (Pathak, et al, 2014). Entity detection task was converted into sequence labelling task using BIO format. A Conditional Random Fields (CRF) was used to detect continuous entity using CRF++3 toolkit. To detect discontiguous entities, a binary SVM classifier was used to detect whether relationship existed between two disorder mentions or not. For contiguous entity detection task, our feature set was very similar to the one we used last year: • Standard features like bag of words (for window +2 to -2), word stemmer (snowball stemmer) 4, prefix and suffix of length 1 to 5. • Orthographic features like word contains di</context>
</contexts>
<marker>Pathak, Patel, Panchal, Choudhary, Patel, Joshi, 2014</marker>
<rawString>Pathak, Parth, Pinal Patel, Vishal Panchal, Narayan Choudhary, Amrish Patel, Gautam Joshi. 2014. ezDI: A Hybrid CRF and SVM based Model for Detecting and Encoding Disorder Mentions in Clinical Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guergana K Savova</author>
<author>James J Masanz</author>
<author>Philip V Ogren</author>
<author>Jiaping Zheng</author>
<author>Sunghwan Sohn</author>
<author>Karin C KipperSchuler</author>
<author>Christopher G Chute</author>
</authors>
<title>Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications.</title>
<date>2010</date>
<marker>Savova, Masanz, Ogren, Zheng, Sohn, KipperSchuler, Chute, 2010</marker>
<rawString>Savova, Guergana K., James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. KipperSchuler, and Christopher G. Chute. 2010. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Suominen</author>
<author>Sanna Salanter</author>
<author>Sumithra Velupillai</author>
<author>Wendy W Chapman</author>
</authors>
<title>Guergana Savova, Noemie Elhadad, Sameer Pradhan.</title>
<date>2013</date>
<marker>Suominen, Salanter, Velupillai, Chapman, 2013</marker>
<rawString>Suominen, Hanna, Sanna Salanter, Sumithra Velupillai, Wendy W. Chapman, Guergana Savova, Noemie Elhadad, Sameer Pradhan. 2013. Overview of the ShARe/CLEF eHealth evaluation lab 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yefeng Wang</author>
<author>Jon Patrick</author>
</authors>
<title>Cascading classifiers for named entity recognition in clinical notes.</title>
<date>2009</date>
<marker>Wang, Patrick, 2009</marker>
<rawString>Wang, Yefeng and Jon Patrick. 2009. Cascading classifiers for named entity recognition in clinical notes.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>