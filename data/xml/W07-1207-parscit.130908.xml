<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.214901">
<bodyText confidence="0.999714441176471">
in Figure 1. Our generic senses have been inspired
by FrameNet (Baker et al., 1998).
In addition, types are augmented with semantic
features derived from EuroWordNet (Vossen et al.,
1997) and extended. These are used to provide se-
lectional restrictions, similar to VerbNet (Kipper et
al., 2000). The constraints are intentionally weak,
excluding utterances unsuitable in most contexts
(the idea slept) but not attempting to eliminate
borderline combinations.
The generic selectional restrictions are effective
in improving overall parsing accuracy, while re-
maining valid across multiple domains. An
evaluation with an earlier version of the grammar
showed that if generic selectional restrictions were
removed, full sentence semantic accuracy de-
creased from 77.8% to 62.6% in an emergency
rescue domain, and from 67.9 to 52.5% in a medi-
cal domain (using the same versions of grammar
and lexicon) (Dzikovska, 2004).
The current version of our generic lexicon con-
tains approximately 6400 entries (excluding mor-
phological variants), and the current language on-
tology has 950 concepts. The lexicon can be sup-
plemented by searching large-scale lexical re-
sources such as WordNet (Fellbaum, 1998) and
Comlex (Grisham et al., 1994). If an unknown
word is encountered, an underspecified entry is
generated on the fly. The entry incorporates as
much information from the resource as possible,
such as part of speech and syntactic frame. It is
assigned an underspecified semantic classification
based on correspondences between our language
ontology and WordNet synsets.
</bodyText>
<subsectionHeader confidence="0.9611">
2.2 Grammar
</subsectionHeader>
<bodyText confidence="0.999729772727273">
The grammar is context-free, augmented with fea-
ture structures and feature unification, motivated
from X-bar theory, drawing on principles from
GPSG (e.g., head and foot features) and HPSG. A
detailed description of an early non-lexicalized
version of the formalism is in (Allen, 1995). Like
HPSG, our grammar is strongly lexicalized, with
the lexical features defining arguments and com-
plement structures for head words. Unlike HPSG,
however, the features are not typed and rather than
multiple inheritance, the parser supports a set of
orthogonal single inheritance hierarchies to capture
different syntactic and semantic properties. Struc-
tural variants such as passives, dative shifts, ger-
unds, and so on are captured in the context-free
rule base. The grammar has broad coverage of
spoken English, supporting a wide range of con-
versational constructs. It also directly encodes
conventional conversational acts, including stan-
dard surface speech acts such as inform, request
and question, as well as acknowledgments, accep-
tances, rejections, apologies, greetings, corrections,
and other speech acts common in conversation.
To support having both a broad domain-general
grammar and the ability to produce deep domain-
specific semantic representations, the semantic
knowledge is captured in three distinct layers (Fig-
ure 2), which are compiled together before parsing
to create efficient domain-specific interpretation.
The first level is primarily encoded in the gram-
mar, and defines an interpretation of the utterance
in terms of generic grammatical relations. The sec-
ond is encoded in the lexicon and defines an inter-
pretation in terms of a generic language-based on-
tology and generic roles. The third is encoded by a
set of ontology-mapping rules that are defined for
each domain, and defines an interpretation in terms
of the target application ontology. While these lev-
els are defined separately, the parser can produce
all three levels simultaneously, and exploit do-
main-specific semantic restrictions to simultane-
ously improve semantic accuracy and parsing effi-
ciency. In this paper we focus on the middle level,
the generic LF.
</bodyText>
<figure confidence="0.997954076923077">
CONSUME
MOVE
ACQUIRE
SELECT
COMPATIBLE
WITH
TAKE-TIME
Take an aspirin
Take it to the store
Take a picture
I’ll take that one
The projector takes 100 volts
It took three hours
</figure>
<figureCaption confidence="0.998948">
Figure 1: Some generic senses of take in lexicon
</figureCaption>
<page confidence="0.981589">
50
</page>
<figureCaption confidence="0.999727">
Figure 2: The Levels of Representation computed by the Parser
</figureCaption>
<bodyText confidence="0.999802151515151">
The rules in the grammar are weighted, and
weights are combined, similar to how probabilities
are computed in a PCFG. The weights, however,
are not strictly probabilities (e.g., it is possible to
have weights greater than 1); rather, they encode
structural preferences. The parser operates in a
best-first manner and as long as weights never ex-
ceed 1.0, is guaranteed to find the highest weighted
parse first. If weights are allowed to exceed 1.0,
then the parser becomes more “depth-first” and it
is possible to “garden-path” and find globally sub-
optimal solutions first, although eventually all in-
terpretations can still be found.
The grammar used in all our applications uses
these hand-tuned rule weights, which have proven
to work relatively well across domains. We do not
use a statistical parser based on a trained corpus
because in most dialogue-system projects, suffi-
cient amounts of training data are not available and
would be too time consuming to collect. In the one
domain in which we have a reasonable amount of
training data (about 9300 utterances), we experi-
mented with a PCFG using trained probabilities
with the Collins algorithm, but were not able to
improve on the hand-tuned preferences in overall
performance (Elsner et al., 2005).
Figure 3 summarizes some of the most impor-
tant preferences encoded in our rule weights. Be-
cause we are dealing with speech, which is often
ungrammatical and fragmented, the grammar in-
cludes “robust” rules (e.g., allowing dropped de-
terminers) that would not be found in a grammar of
written English.
</bodyText>
<sectionHeader confidence="0.945556" genericHeader="method">
3 The Logical Form Language
</sectionHeader>
<bodyText confidence="0.999466774193549">
The logical form language captures a domain-
independent semantic representation of the utter-
ance. As shown later in this paper, it can be seen as
a variant of MRS (Copestake et al., 2006) but is
expressed in a frame-like notation rather than
predicate calculus. In addition, it has a relatively
simple method of computing possible quantifier
scoping, drawing from the approaches by (Hobbs
&amp; Shieber, 1987) and (Alshawi, 1990).
A logical form is set of terms that can be viewed
as a rooted graph with each term being a node
identified by a unique ID (the variable). There are
three types of terms. The first corresponds to gen-
eralized quantifiers, and is on the form (&lt;quant&gt;
&lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example,
the NP Every dog would be captured by the term
(Every d1 DOG). The second type of term is the
propositional term, which is represented in a neo-
Davidsonian representation (e.g., Parsons, 1990)
using reified events and properties. It has the form
(F &lt;id&gt; &lt;type&gt; &lt;arguments&gt;*). The propositional
terms produced from Every dog hates a cat would
be (F h1 HATE :Experiencer d1 :Theme c1). The
third type of term is the speech act, which has the
same form as propositional terms except for the
initial indicator SA identifying it as a performed
speech act. The speech act for Every dog hates a
cat would be (SA sa1 INFORM :content h1). Put-
ting this all together, we get the following (con-
densed) LF representation from the parser for
Every large dog hates a cat (shown in graphical
</bodyText>
<subsubsectionHeader confidence="0.595513">
Prefer
</subsubsectionHeader>
<listItem confidence="0.9997805">
• Interpretations without gaps to those with gaps
• Subcategorized interpretations over adjuncts
• Right attachment of PPs and adverbials
• Fully specified constituents over those with
dropped or “implicit” arguments
• Adjectival modification over noun-noun modifi-
cation
• Standard rules over “robust” rules
</listItem>
<figureCaption confidence="0.963825">
Figure 3: Some Key Preferences used in Parsing
</figureCaption>
<page confidence="0.986689">
51
</page>
<bodyText confidence="0.95241">
form in Figure 4).
</bodyText>
<equation confidence="0.993451">
(SA x1 TELL :content x2)
(F x2 HATE :experience x3 :theme x5)
(Every x3 DOG :mods (x4))
(F x4 LARGE :of x3)
(A x5 CAT)
</equation>
<sectionHeader confidence="0.726958" genericHeader="method">
4 Comparison of LF and MRS
</sectionHeader>
<bodyText confidence="0.9966914">
Minimal Recursion Semantics (MRS) (Copestake
et al. 2006) is a semantic formalism which has
been widely adopted in the last several years. This
has motivated some research on how this formal-
ism compares to some traditional semantic for-
malisms. For example, Fuchss et al. (2004) for-
mally show that the translation from MRS to
Dominance Constraints is feasible. We have also
found that MRS is very similar to LF in its de-
scriptive power. In fact, we can convert every LF
to an equivalent MRS structure with a simple algo-
rithm.
First, consider the sentence Every dog hates a
cat. Figure 5 shows the LF and MRS representa-
tions for this sentence.
</bodyText>
<figureCaption confidence="0.9811025">
Figure 5: The LF (left) and MRS (right) representations
for the sentence “Every dog hates a cat.”
</figureCaption>
<bodyText confidence="0.997883469387755">
The first step toward converting LF to MRS is to
express LF terms as n-ary relationships. For exam-
ple we express the LF term (F v1 Hate
:Experiencer x :Theme y) as Hate(x, y). For quanti-
fier terms, we break the LF term into two relations:
one for the quantifier itself and one for the restric-
tion. For example (Every x Dog) is converted to
Every(x) and Dog(x).
There is a small change in the conversion proce-
dure when the sentence contains some modifiers.
Consider the modifier large in the sentence Every
large dog hates a cat. In the LF, we bring the
modifier in the term which defines the semantic
head, using a :MODS slot. In the MRS, however,
modifiers are separate EPs labeled with same han-
dle as the head’s. To cover this, for each LF term T
which has a (:MODS vk) slot, and the LF term T1
which defines the variable vk, we assign the same
handle to both T and T1. For example for the terms
(F x Dog :MODS v2) and (F v2 Large :OF x), we
assign the same handle to both Dog(x) and
Large(x). Similar approach applies when the modi-
fier itself is a scopal term, such as in the sentence
Every cat in a room sleeps. Figure 7 shows LF and
MRS representations for this sentence. Figure 8,
summarizes all these steps as an algorithm which
takes a LF representation as the input and gener-
ates its equivalent MRS.
There is a small change in the conversion proce-
dure when the sentence contains some modifiers.
Consider the modifier large in the sentence Every
large dog hates a cat. In the LF, we bring the
modifier in the term which defines the semantic
head, using a :MODS slot. In the MRS, however,
modifiers are separate EPs labeled with same han-
dle as the head’s. To cover this, for each LF term T
which has a (:MODS vk) slot, and the LF term T1
which defines the variable vk, we assign the same
handle to both T and T1. For example for the terms
(F x Dog :MODS v2) and (F v2 Large :OF x), we
assign the same handle to both Dog(x) and
Large(x). Similar approach applies when the modi-
fier itself is a scopal term, such as in the sentence
Every cat in a room sleeps. Figure 7 shows LF and
MRS representations for this sentence. Figure 8,
summarizes all these steps as an algorithm which
takes a LF representation as the input and gener-
ates its equivalent MRS.
The next step is to bring handles into the repre-
</bodyText>
<figureCaption confidence="0.991920666666667">
Figure 6: The steps of converting the LF for
“Every cat hates a cat” to its MRS representation
Figure 4: The LF in graphical form
</figureCaption>
<page confidence="0.986041">
52
</page>
<bodyText confidence="0.999924388888889">
sentation. First, we assign a different handle to
each term. Then, for each quantifier term such as
Every(x), we add two handles as the arguments of
the relation: one for the restriction and one for the
body as in h2: Every(x, h6, h7). Finally, we add the
handle constraints to the MRS. We have two types
of handle constraint. The first type comes from the
restriction of each quantifier. We add a qeq rela-
tionship between the restriction handle argument of
the quantifier term and the handle of the actual re-
striction term. The second type of constraint is the
qeq relationship which defines the top handle of
the MRS. The speech act term in every LF refers to
a formula term as content (:content slot), which is
actually the heart of the LF. We build a qeq rela-
tionship between h0 (the top handle) and the han-
dle of this formula term. Figure 6 shows the effect
of applying these steps to the above example.
</bodyText>
<figureCaption confidence="0.998238">
Figure 7: The LF and MRS representations for the sen-
tence “Every cat in a room sleeps.”
</figureCaption>
<bodyText confidence="0.993140684210526">
Another interesting issue about these two formal-
isms is that the effect of applying the simple scop-
ing algorithms referred in section 3 to generate all
possible interpretations of a LF is the same as ap-
plying MRS axioms and handle constraints to gen-
erate all scope-resolved MRSs. For instance, the
example in (Copestake et al. 2006), Every nephew
of some famous politician saw a pony has the same
5 interpretations using either approach.
As the last point here, we need to mention that
the algorithm in Figure 8 does not consider fixed-
scopal terms such as scopal adverbials or negation.
However, we believe that the framework itself is
able to support these types of scopal term and with
a small modification, the scoping algorithm will
work well in assigning different possible interpre-
tations. We leave the full discussion about these
details as well as the detailed proof of the other
claims we made here to another paper.
</bodyText>
<sectionHeader confidence="0.997138" genericHeader="method">
5 Generic Discourse Interpretation
</sectionHeader>
<bodyText confidence="0.999961133333333">
With a generic semantic representation, we can
then define generic discourse processing capabili-
ties that can be used in any application. All of
these methods have a corresponding capability at
the domain-specific level for an application, but we
will not discuss this further here. We also do not
discuss the support for language generation which
uses the same discourse context.
There are three core discourse interpretation ca-
pabilities that the system provides: reference reso-
lution, ellipsis processing, and speech act interpre-
tation. All our different dialog systems use the
same discourse processing, whether the task in-
volves collaborative problem solving, learning
from instruction or automated tutoring.
</bodyText>
<subsectionHeader confidence="0.975136">
5.1 Reference Resolution
</subsectionHeader>
<bodyText confidence="0.999868">
Our domain-independent representation supports
reference resolution in two ways. First, the quanti-
fiers and dependency structure extracted from the
sentence allow for implementing reference resolu-
tion algorithms based on extracted syntactic fea-
tures. The system uses different strategies for re-
</bodyText>
<figureCaption confidence="0.998702">
Figure 8: The LF-MRS conversion algorithm
</figureCaption>
<page confidence="0.993557">
53
</page>
<bodyText confidence="0.999701533333333">
solving each type of referring expression along the
lines described in (Byron, 2002).
Second, domain-independent semantic informa-
tion helps greatly in resolving pronouns and defi-
nite descriptions. The general capability provided
for resolving referring expressions is to search
through the discourse history for the most recent
entity that matches the semantic requirements,
where recency within an utterance may be reor-
dered to reflect focusing heuristics (Tetreault,
2001). For definite descriptions, the semantic in-
formation required is explicit in the lexicon. For
pronouns, the parser can often compute semantic
features from verb argument restrictions. For in-
stance, the pronoun it carries little semantic infor-
mation by itself, but in the utterance Eat it we
know we are looking for an edible object. This
simple technique performs well in practice.
Because of the knowledge in the lexicon for role
nouns such as author, we can also handle simple
bridging reference. Consider the discourse frag-
ment That book came from the library. The author
É. The semantic representation of the author in-
cludes its implicit argument, e.g., (The x1
AUTHOR :of b1). Furthermore, the term b1 has
the semantic feature INFO-CONTENT, which in-
cludes objects that “contain” information such as
books, articles, songs, etc.., which allows the pro-
noun to correctly resolve via bridging to the book
in the previous utterance.
</bodyText>
<subsectionHeader confidence="0.987893">
5.2 Ellipsis
</subsectionHeader>
<bodyText confidence="0.999984333333333">
The parser produces a representation of fragmen-
tary utterances similar to (Schlangen and Las-
carides, 2003). The main difference is that instead
of using a single underspecified unknown_rel
predicate to resolve in discourse context, we use a
speech act term as the underspecified relation, dif-
ferentiating between a number of common rela-
tions such as acknowledgments, politeness expres-
sions, noun phrases and underspecified predicates
(PP, ADJP and VP fragments). The representations
of the underspecified predicates also include an
IMPRO in place of the unspecified argument.
We currently handle only a few key cases of el-
lipsis. The first is question/answer pairs. By re-
taining the logical form of the question in the dis-
course history, it is relatively easy to reconstruct
the full content of short answers (e.g., in Who ate
the pizza? John? the answer maps to the represen-
tation that John ate the pizza). In addition, we
handle common follow-up questions (e.g., Did
John buy a book? How about a magazine?) by per-
forming a semantic closeness matching of the
fragment into the previous utterance and substitut-
ing the most similar terms. The resulting term can
then be used to update the context. This process is
similar to the resolution process in (Schlangen and
Lascarides, 2003), though the syntactic parallelism
constraint is not checked. It could also be easily
extended to cover other fragment types, as the
grammar provides all the necessary information.
</bodyText>
<subsectionHeader confidence="0.99852">
5.3 Speech Act Interpretation
</subsectionHeader>
<bodyText confidence="0.999962078947369">
The presence of domain-independent semantic
classes allows us to encode a large set of these
common conversational pattern independently of
the application task and domain. These include
rules to handle short answers to questions, ac-
knowledgements and common politeness expres-
sions, as well as common inferences such as inter-
preting I need to do X as please do X.
Given our focus on problem solving domains,
we are generally interested in identifying more
than just the illocutionary force of an utterance.
For instance, in a domain for planning how to
evacuate people off an island, the utterance Can
we remove the people by helicopter? is not only
ambiguous between being a true Y-N question or a
suggestion of a course of action, but at the problem
solving level it might intended to (1) introduce a
new goal, (2) elaborate or extend the solution to
the current problem, or (3) suggest a modification
to an existing solution (e.g., moving them by
truck). One can only choose between these read-
ings using domain specific reasoning about the
current task. The point here is that the interpreta-
tion rules are still generic across all domains and
expressed using the generic LF, yet the interpreta-
tions produced are evaluated using domain-specific
reasoning. This interleaving of generic interpreta-
tion and domain-specific reasoning is enabled by
our ontology mappings.
Similarly, in tutoring domains students often
phrase their answers as check questions. In an an-
swer to the question Which components are in a
closed path, the student may say Is the bulb in 3 in
a closed path? The domain-independent represen-
tation is used to identify the surface form of this
utterance as a yes-no question. The dialogue man-
ager then formulates two hypotheses: that this is a
hedged answer, or a real question. If a domain-
</bodyText>
<page confidence="0.995972">
54
</page>
<bodyText confidence="0.999923285714286">
specific tutoring component confirms the former
hypothesis, the dialogue manager will proceed
with verifying answer correctness and carrying on
remediation as necessary. Otherwise (such as for Is
the bulb in 5 connected to a battery in the same
context), the utterance is a question that can be
answered by querying the domain reasoner.
</bodyText>
<subsectionHeader confidence="0.996621">
5.4 A Note on Generic Capabilities
</subsectionHeader>
<bodyText confidence="0.999986888888889">
A key point is that these generic discourse inter-
pretation capabilities are enabled because of the
detailed generic semantic interpretation produced
by the parser. If the parser produced a more shal-
low representation, then the discourse interpreta-
tion techniques would be significantly degraded.
On the other hand, if we developed a new repre-
sentation for each domain, then we would have to
rebuild all the discourse processing for the domain.
</bodyText>
<sectionHeader confidence="0.998121" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999809493150685">
Our evaluation is aimed at assessing two main
features of the grammar and lexicon: portability
and accuracy. We use two main evaluation criteria:
full sentence accuracy, that takes into account both
syntactic and semantic accuracy of the system, and
sense tagging accuracy, to demonstrate that the
word senses included in the system can be distin-
guished with a combination of syntactic and do-
main-independent semantic information.
As a measure of the breadth of grammatical
coverage of our system, we have evaluated our
coverage on the CSLI LKB (Linguistic Knowledge
Building) test suite (Copestake, 1999). The test
suite contains approximately 1350 sentences, of
which about 400 are ungrammatical. We use a full-
sentence accuracy measure to evaluate our cover-
age, since this is the most meaningful measure in
terms of what we require as parser output in our
applications. For a sentence representation to be
counted as correct by this measure, both the syn-
tactic structure and the semantic representation
must be correct, which includes the correct as-
signment of word senses, dependency relations
among terms, and speech act type. Our current
coverage for the diverse grammatical phenomena
in the corpus is 64% full-sentence accuracy.
We also report the number of spanning parses
found, because in our system there are cases in
which the syntactic parse is correct, but an incor-
rect word sense may have been assigned, since we
disambiguate senses using not only syntactic
structure but also semantic features as selectional
restrictions on arguments. For example, in The
manager interviewed Browne after working, the
parser assigns working the sense LF::FUNCTION,
used with non-agentive subjects, instead of the cor-
rect sense for agentive subjects, LF::WORKING.
For the grammatical utterances in the test suite, our
parser found spanning parses for 80%.
While the ungrammatical sentences in the set are
an important tool for constraining grammar output,
our grammar is designed to find a reasonable inter-
pretation for natural speech, which often is less
than perfect. For example, we have low preference
grammar rules that allow dropped subjects, miss-
ing determiners, and wrong subject verb agree-
ment. In addition, utterances are often fragmentary,
so even those without spanning parses may be con-
sidered correct. Our grammar allows all major con-
stituents (NP, VP, ADJP, ADVP) as valid utter-
ances. As a result, our system produces spanning
parses for 46% of the “ungrammatical” utterances.
We have not yet done a detailed error analysis.
As a measure of system portability to new do-
mains, we have evaluated our system coverage on
the ATIS (Airline Travel Information System)
speech corpus, which we have never used before.
For this evaluation, the proper names (cities, air-
ports, airline companies) in the ATIS corpus were
added to our lexicon, but no other development
work was performed. We parsed 116 randomly
selected test sentences and hand-checked the re-
sults using our full-sentence accuracy measure.
Our baseline coverage of these utterances is 53%
full-sentence semantic accuracy. Of the 55 utter-
ances that were not completely correct, we found
spanning parses for 36% (20). Reasons that span-
ning parses were marked as wrong include incor-
rect word senses (e.g., for stop in I would like it to
have a stop in Phoenix) or PP-attachment. Reasons
that no spanning parse was found include missing
senses for existing words (e.g., serve as in Does
that flight serve dinner).
</bodyText>
<sectionHeader confidence="0.999663" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999975714285714">
We presented a deep parser and semantic inter-
preter for use in dialogue systems. An important
question to ask is how it compares to other existing
formalisms. At present there is no easy way to
make such comparison. One possible criterion is
grammatical coverage. Looking at the grammar
coverage/accuracy on the TSNLP suite that was
</bodyText>
<page confidence="0.995093">
55
</page>
<bodyText confidence="0.999986459459459">
used to evaluate the LINGO ERG grammar, our
grammar demonstrates 80% coverage (number of
spanning parses). The reported figure for LINGO
ERG coverage of CSLI is 77% (Oepen, 1999), but
this number has undoubtedly improved in the 9-
year development period. For example, the current
reported coverage figures on spoken dialogue cor-
pora are close to 90% (Oepen et al., 2002).
However, the grammar coverage alone is not a
satisfactory measure for a deep NLP system for use
in practical applications, because the logical forms
and therefore the capabilities of deep NLP systems
differ significantly. A major distinguishing feature
of our system is that the logical form it outputs
uses semantically motivated word senses. LINGO
ERG, in contrast, contains only syntactically moti-
vated word senses. For example, the words end and
finish are not related in any obvious way. This re-
flects a difference in underlying philosophy.
LINGO ERG aims for linguistic precision, and as
can be seen from our experiments, requiring the
parser to select correct domain-independent word
senses lowers accuracy.
Our system, however, is built with the goal of
easy portability within the context of dialogue
systems. The availability of word senses simplifies
the design of domain-independent interpretation
components, such as reference resolution and
speech act interpretation components that use do-
main-independent syntactic and semantic informa-
tion to encode conventional interpretation rules.
If the LINGO ERG grammar were to be put in a
dialogue system that requires domain interpretation
and reasoning, an additional lexical interpretation
module would have to be developed to perform
word sense disambiguation as well as interpreta-
tion, something that has not yet been done.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996525">
We thank 3 reviewers for helpful comments. This
work was supported by NSF IIS-0328811, DARPA
NBCHD30010 via subcontract to SRI #03-000223
and ONR N00014051004-3 and –8.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855232142857">
H. Alshawi. 1990. Resolving Quasi Logical Forms.
Computational Linguistics 16(3):133-144.
W. Baker, C. Fillmore and J. B. Lowe. 1998. The Ber-
keley FrameNet Project. COLING-ACL&apos;98, Montréal.
D. Byron. 2002. Resolving Pronominal Reference to
Abstract Entities. ACL-02, Philadelphia.
A. Copestake. 1999. The (New) LKB System. CSLI.
A. Copestake, D. Flickinger, C. Pollard and I. Sag.
2006. Minimal Recursion Semantics: An Introduc-
tion. Research on Language and Computation,
3(4):281-332.
M. Dzikovska. 2004. A Practical Semantic Representa-
tion for Natural Language Parsing. Ph.D. Thesis,
University of Rochester.
M. Dzikovska, J. Allen and M. Swift. Forthcoming.
Linking Semantic and Knowledge Representations in
a Multi-domain Dialogue System. Journal of Logic
and Computation.
M. Dzikovska, J. Allen and M. Swift. 2003. Integrating
Linguistic and Domain Knowledge for Spoken Dia-
logue Systems in Multiple Domains. Workshop on
Knowledge and Reasoning in Practical Dialogue
Systems, IJCAI-2003, Acapulco.
M. Elsner, M. Swift, J. Allen and D. Gildea. 2005. On-
line Statistics for a Unification-based Dialogue
Parser. IWPT05, Vancouver.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
R. Fuchss, A. Koller, J. Niehren, S. Thater. 2004.
Minimal Recursion Semantics as Dominance Con-
straints. ACL-04, Barcelona.
R. Grisham, C. Macleod and A. Meyers. 1994. Comlex
Syntax: Building a Computational Lexicon. COLING
94, Kyoto.
J. Hobbs and S. Shieber. 1987. An Algorithm for Gen-
erating Quantifier Scopings. Computational Linguis-
tics 13(1-2):47-63.
K. Kipper, H. T. Dang and M. Palmer. 2000. Class-
based Construction of a Verb Lexicon. AAAI-2000.
S. Oepen, D. Flickinger, K. Toutanova and C. Manning.
2002. Lingo Redwoods: A Rich and Dynamic Tree-
bank for HPSG. First Workshop on Treebanks and
Linguistic Theories (TLT2002).
S. Oepen (1999). [incr tsdb()] User Manual.
www.delph-in.net/itsdb/publications/manual.ps.gz.
T. Parsons. 1990. Events in the Semantics of English. A
Study in Subatomic Semantics. MIT Press.
D. Schlangen and A. Lascarides 2003. The Interpreta-
tion of Non-Sentential Utterances in Dialogue. SIG-
DIAL-03, Sapporo.
J. Tetreault. 2001. A Corpus-Based Evaluation of Cen-
tering and Pronoun Resolution. Computational Lin-
guistics. 27(4):507-520.
Vossen, P. (1997) EuroWordNet: A Multilingual Data-
base for Information Retrieval. In Proc. of the Delos
workshop on Cross-language Information Retrieval.
</reference>
<page confidence="0.998402">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.698291">in Figure 1. Our generic senses have been inspired</title>
<abstract confidence="0.988218198830409">by FrameNet (Baker et al., 1998). In addition, types are augmented with semantic features derived from EuroWordNet (Vossen et al., 1997) and extended. These are used to provide selectional restrictions, similar to VerbNet (Kipper et al., 2000). The constraints are intentionally weak, excluding utterances unsuitable in most contexts idea but not attempting to eliminate borderline combinations. The generic selectional restrictions are effective in improving overall parsing accuracy, while remaining valid across multiple domains. An evaluation with an earlier version of the grammar showed that if generic selectional restrictions were removed, full sentence semantic accuracy decreased from 77.8% to 62.6% in an emergency rescue domain, and from 67.9 to 52.5% in a medical domain (using the same versions of grammar and lexicon) (Dzikovska, 2004). The current version of our generic lexicon contains approximately 6400 entries (excluding morphological variants), and the current language ontology has 950 concepts. The lexicon can be supplemented by searching large-scale lexical resources such as WordNet (Fellbaum, 1998) and Comlex (Grisham et al., 1994). If an unknown word is encountered, an underspecified entry is generated on the fly. The entry incorporates as much information from the resource as possible, such as part of speech and syntactic frame. It is assigned an underspecified semantic classification based on correspondences between our language ontology and WordNet synsets. 2.2 Grammar The grammar is context-free, augmented with feature structures and feature unification, motivated from X-bar theory, drawing on principles from GPSG (e.g., head and foot features) and HPSG. A detailed description of an early non-lexicalized version of the formalism is in (Allen, 1995). Like HPSG, our grammar is strongly lexicalized, with the lexical features defining arguments and complement structures for head words. Unlike HPSG, however, the features are not typed and rather than multiple inheritance, the parser supports a set of orthogonal single inheritance hierarchies to capture different syntactic and semantic properties. Structural variants such as passives, dative shifts, gerunds, and so on are captured in the context-free rule base. The grammar has broad coverage of spoken English, supporting a wide range of conversational constructs. It also directly encodes conventional conversational acts, including standard surface speech acts such as inform, request and question, as well as acknowledgments, acceptances, rejections, apologies, greetings, corrections, and other speech acts common in conversation. To support having both a broad domain-general grammar and the ability to produce deep domainspecific semantic representations, the semantic knowledge is captured in three distinct layers (Figure 2), which are compiled together before parsing to create efficient domain-specific interpretation. The first level is primarily encoded in the grammar, and defines an interpretation of the utterance in terms of generic grammatical relations. The second is encoded in the lexicon and defines an interpretation in terms of a generic language-based ontology and generic roles. The third is encoded by a set of ontology-mapping rules that are defined for each domain, and defines an interpretation in terms of the target application ontology. While these levels are defined separately, the parser can produce all three levels simultaneously, and exploit domain-specific semantic restrictions to simultaneously improve semantic accuracy and parsing efficiency. In this paper we focus on the middle level, the generic LF. CONSUME MOVE ACQUIRE SELECT COMPATIBLE WITH TAKE-TIME Take an aspirin Take it to the store Take a picture I’ll take that one The projector takes 100 volts It took three hours Figure 1: Some generic senses of take in lexicon 50 Figure 2: The Levels of Representation computed by the Parser The rules in the grammar are weighted, and weights are combined, similar to how probabilities are computed in a PCFG. The weights, however, are not strictly probabilities (e.g., it is possible to have weights greater than 1); rather, they encode structural preferences. The parser operates in a best-first manner and as long as weights never exceed 1.0, is guaranteed to find the highest weighted parse first. If weights are allowed to exceed 1.0, then the parser becomes more “depth-first” and it is possible to “garden-path” and find globally suboptimal solutions first, although eventually all interpretations can still be found. The grammar used in all our applications uses these hand-tuned rule weights, which have proven to work relatively well across domains. We do not use a statistical parser based on a trained corpus because in most dialogue-system projects, sufficient amounts of training data are not available and would be too time consuming to collect. In the one domain in which we have a reasonable amount of training data (about 9300 utterances), we experimented with a PCFG using trained probabilities with the Collins algorithm, but were not able to improve on the hand-tuned preferences in overall performance (Elsner et al., 2005). Figure 3 summarizes some of the most important preferences encoded in our rule weights. Because we are dealing with speech, which is often ungrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In addition, it has a relatively simple method of computing possible quantifier scoping, drawing from the approaches by (Hobbs &amp; Shieber, 1987) and (Alshawi, 1990). A logical form is set of terms that can be viewed as a rooted graph with each term being a node identified by a unique ID (the variable). There are three types of terms. The first corresponds to generalized quantifiers, and is on the form (&lt;quant&gt; &lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example, NP dog be captured by the term (Every d1 DOG). The second type of term is the propositional term, which is represented in a neo- Davidsonian representation (e.g., Parsons, 1990) using reified events and properties. It has the form (F &lt;id&gt; &lt;type&gt; &lt;arguments&gt;*). The propositional produced from dog hates a cat be (F h1 HATE :Experiencer d1 :Theme c1). The third type of term is the speech act, which has the same form as propositional terms except for the initial indicator SA identifying it as a performed speech act. The speech act for Every dog hates a cat would be (SA sa1 INFORM :content h1). Putting this all together, we get the following (condensed) LF representation from the parser for large dog hates a cat in graphical Prefer • Interpretations without gaps to those with gaps • Subcategorized interpretations over adjuncts • Right attachment of PPs and adverbials • Fully specified constituents over those with dropped or “implicit” arguments • Adjectival modification over noun-noun modification • Standard rules over “robust” rules Figure 3: Some Key Preferences used in Parsing 51 form in Figure 4).</abstract>
<note confidence="0.9845644">(SA x1 TELL :content x2) (F x2 HATE :experience x3 :theme x5) (Every x3 DOG :mods (x4)) (F x4 LARGE :of x3) (A x5 CAT)</note>
<title confidence="0.426884">4 Comparison of LF and MRS</title>
<author confidence="0.463016">Minimal Recursion Semantics</author>
<abstract confidence="0.999585336927224">et al. 2006) is a semantic formalism which has been widely adopted in the last several years. This has motivated some research on how this formalism compares to some traditional semantic formalisms. For example, Fuchss et al. (2004) formally show that the translation from MRS to Dominance Constraints is feasible. We have also found that MRS is very similar to LF in its descriptive power. In fact, we can convert every LF to an equivalent MRS structure with a simple algorithm. consider the sentence dog hates a Figure 5 shows the LF and MRS representations for this sentence. 5: LF (left) and MRS (right) representations for the sentence “Every dog hates a cat.” The first step toward converting LF to MRS is to express LF terms as n-ary relationships. For examwe express the LF term v1 Hate x :Theme y) For quantifier terms, we break the LF term into two relations: one for the quantifier itself and one for the restric- For example x Dog) converted to and There is a small change in the conversion procedure when the sentence contains some modifiers. the modifier the sentence dog hates a cat. the LF, we bring the modifier in the term which defines the semantic using a In the MRS, however, modifiers are separate EPs labeled with same handle as the head’s. To cover this, for each LF term T has a and the LF term T1 defines the variable assign the same handle to both T and T1. For example for the terms x Dog :MODS v2) v2 Large :OF we the same handle to both Similar approach applies when the modifier itself is a scopal term, such as in the sentence cat in a room Figure 7 shows LF and MRS representations for this sentence. Figure 8, summarizes all these steps as an algorithm which takes a LF representation as the input and generates its equivalent MRS. There is a small change in the conversion procedure when the sentence contains some modifiers. the modifier the sentence dog hates a cat. the LF, we bring the modifier in the term which defines the semantic using a In the MRS, however, modifiers are separate EPs labeled with same handle as the head’s. To cover this, for each LF term T has a and the LF term T1 defines the variable assign the same handle to both T and T1. For example for the terms x Dog :MODS v2) v2 Large :OF we the same handle to both Similar approach applies when the modifier itself is a scopal term, such as in the sentence cat in a room Figure 7 shows LF and MRS representations for this sentence. Figure 8, summarizes all these steps as an algorithm which takes a LF representation as the input and generates its equivalent MRS. next step is to bring handles into the repre- Figure 6: The steps of converting the LF for “Every cat hates a cat” to its MRS representation Figure 4: The LF in graphical form 52 sentation. First, we assign a different handle to each term. Then, for each quantifier term such as we add two handles as the arguments of the relation: one for the restriction and one for the as in Every(x, h6, Finally, we add the handle constraints to the MRS. We have two types of handle constraint. The first type comes from the restriction of each quantifier. We add a qeq relationship between the restriction handle argument of the quantifier term and the handle of the actual restriction term. The second type of constraint is the qeq relationship which defines the top handle of the MRS. The speech act term in every LF refers to formula term as content which is actually the heart of the LF. We build a qeq relationship between h0 (the top handle) and the handle of this formula term. Figure 6 shows the effect of applying these steps to the above example. Figure 7: The LF and MRS representations for the sentence “Every cat in a room sleeps.” Another interesting issue about these two formalisms is that the effect of applying the simple scoping algorithms referred in section 3 to generate all possible interpretations of a LF is the same as applying MRS axioms and handle constraints to generate all scope-resolved MRSs. For instance, the in (Copestake et al. 2006), nephew some famous politician saw a pony the same 5 interpretations using either approach. As the last point here, we need to mention that the algorithm in Figure 8 does not consider fixedscopal terms such as scopal adverbials or negation. However, we believe that the framework itself is able to support these types of scopal term and with a small modification, the scoping algorithm will work well in assigning different possible interpretations. We leave the full discussion about these details as well as the detailed proof of the other claims we made here to another paper. 5 Generic Discourse Interpretation With a generic semantic representation, we can then define generic discourse processing capabilities that can be used in any application. All of these methods have a corresponding capability at the domain-specific level for an application, but we will not discuss this further here. We also do not discuss the support for language generation which uses the same discourse context. There are three core discourse interpretation capabilities that the system provides: reference resolution, ellipsis processing, and speech act interpretation. All our different dialog systems use the same discourse processing, whether the task involves collaborative problem solving, learning from instruction or automated tutoring. 5.1 Reference Resolution Our domain-independent representation supports reference resolution in two ways. First, the quantifiers and dependency structure extracted from the sentence allow for implementing reference resolution algorithms based on extracted syntactic fea- The system uses different strategies for re- Figure 8: The LF-MRS conversion algorithm 53 solving each type of referring expression along the lines described in (Byron, 2002). Second, domain-independent semantic information helps greatly in resolving pronouns and definite descriptions. The general capability provided for resolving referring expressions is to search through the discourse history for the most recent entity that matches the semantic requirements, where recency within an utterance may be reordered to reflect focusing heuristics (Tetreault, 2001). For definite descriptions, the semantic information required is explicit in the lexicon. For pronouns, the parser can often compute semantic features from verb argument restrictions. For inthe pronoun little semantic inforby itself, but in the utterance it know we are looking for an edible object. This simple technique performs well in practice. Because of the knowledge in the lexicon for role such as we can also handle simple bridging reference. Consider the discourse fragbook came from the library. The author semantic representation of author includes its implicit argument, e.g., (The x1 AUTHOR :of b1). Furthermore, the term b1 has the semantic feature INFO-CONTENT, which includes objects that “contain” information such as books, articles, songs, etc.., which allows the pronoun to correctly resolve via bridging to the book in the previous utterance. 5.2 Ellipsis The parser produces a representation of fragmentary utterances similar to (Schlangen and Lascarides, 2003). The main difference is that instead using a single underspecified predicate to resolve in discourse context, we use a speech act term as the underspecified relation, differentiating between a number of common relations such as acknowledgments, politeness expressions, noun phrases and underspecified predicates (PP, ADJP and VP fragments). The representations of the underspecified predicates also include an IMPRO in place of the unspecified argument. We currently handle only a few key cases of ellipsis. The first is question/answer pairs. By retaining the logical form of the question in the discourse history, it is relatively easy to reconstruct full content of short answers (e.g., in ate pizza? John? answer maps to the representation that John ate the pizza). In addition, we common follow-up questions (e.g., buy a book? How about a magazine?) performing a semantic closeness matching of the fragment into the previous utterance and substituting the most similar terms. The resulting term can then be used to update the context. This process is similar to the resolution process in (Schlangen and Lascarides, 2003), though the syntactic parallelism constraint is not checked. It could also be easily extended to cover other fragment types, as the grammar provides all the necessary information. 5.3 Speech Act Interpretation The presence of domain-independent semantic classes allows us to encode a large set of these common conversational pattern independently of the application task and domain. These include rules to handle short answers to questions, acknowledgements and common politeness expressions, as well as common inferences such as interneed to do X do Given our focus on problem solving domains, we are generally interested in identifying more than just the illocutionary force of an utterance. For instance, in a domain for planning how to people off an island, the utterance remove the people by helicopter? not only ambiguous between being a true Y-N question or a suggestion of a course of action, but at the problem solving level it might intended to (1) introduce a new goal, (2) elaborate or extend the solution to the current problem, or (3) suggest a modification to an existing solution (e.g., moving them by truck). One can only choose between these readings using domain specific reasoning about the current task. The point here is that the interpretation rules are still generic across all domains and expressed using the generic LF, yet the interpretations produced are evaluated using domain-specific reasoning. This interleaving of generic interpretation and domain-specific reasoning is enabled by our ontology mappings. Similarly, in tutoring domains students often phrase their answers as check questions. In an anto the question components are in a the student may say the bulb in 3 in closed path? domain-independent representation is used to identify the surface form of this utterance as a yes-no question. The dialogue manager then formulates two hypotheses: that this is a answer, or a real question. If a domain- 54 specific tutoring component confirms the former hypothesis, the dialogue manager will proceed with verifying answer correctness and carrying on as necessary. Otherwise (such as for bulb in 5 connected to a battery the same context), the utterance is a question that can be answered by querying the domain reasoner. 5.4 A Note on Generic Capabilities A key point is that these generic discourse interpretation capabilities are enabled because of the detailed generic semantic interpretation produced by the parser. If the parser produced a more shallow representation, then the discourse interpretation techniques would be significantly degraded. On the other hand, if we developed a new representation for each domain, then we would have to rebuild all the discourse processing for the domain. 6 Evaluation Our evaluation is aimed at assessing two main features of the grammar and lexicon: portability and accuracy. We use two main evaluation criteria: full sentence accuracy, that takes into account both syntactic and semantic accuracy of the system, and sense tagging accuracy, to demonstrate that the word senses included in the system can be distinguished with a combination of syntactic and domain-independent semantic information. As a measure of the breadth of grammatical coverage of our system, we have evaluated our coverage on the CSLI LKB (Linguistic Knowledge Building) test suite (Copestake, 1999). The test suite contains approximately 1350 sentences, of which about 400 are ungrammatical. We use a fullsentence accuracy measure to evaluate our coverage, since this is the most meaningful measure in terms of what we require as parser output in our applications. For a sentence representation to be counted as correct by this measure, both the syntactic structure and the semantic representation must be correct, which includes the correct assignment of word senses, dependency relations among terms, and speech act type. Our current coverage for the diverse grammatical phenomena in the corpus is 64% full-sentence accuracy. We also report the number of spanning parses found, because in our system there are cases in which the syntactic parse is correct, but an incorrect word sense may have been assigned, since we disambiguate senses using not only syntactic structure but also semantic features as selectional on arguments. For example, in interviewed Browne after the assigns sense LF::FUNCTION, used with non-agentive subjects, instead of the correct sense for agentive subjects, LF::WORKING. For the grammatical utterances in the test suite, our parser found spanning parses for 80%. While the ungrammatical sentences in the set are an important tool for constraining grammar output, our grammar is designed to find a reasonable interpretation for natural speech, which often is less than perfect. For example, we have low preference grammar rules that allow dropped subjects, missing determiners, and wrong subject verb agreement. In addition, utterances are often fragmentary, so even those without spanning parses may be considered correct. Our grammar allows all major constituents (NP, VP, ADJP, ADVP) as valid utterances. As a result, our system produces spanning parses for 46% of the “ungrammatical” utterances. We have not yet done a detailed error analysis. As a measure of system portability to new domains, we have evaluated our system coverage on the ATIS (Airline Travel Information System) speech corpus, which we have never used before. For this evaluation, the proper names (cities, airports, airline companies) in the ATIS corpus were added to our lexicon, but no other development work was performed. We parsed 116 randomly selected test sentences and hand-checked the results using our full-sentence accuracy measure. Our baseline coverage of these utterances is 53% full-sentence semantic accuracy. Of the 55 utterances that were not completely correct, we found spanning parses for 36% (20). Reasons that spanning parses were marked as wrong include incorword senses (e.g., for would like it to a stop in or PP-attachment. Reasons that no spanning parse was found include missing for existing words (e.g., in flight serve 7 Discussion We presented a deep parser and semantic interpreter for use in dialogue systems. An important question to ask is how it compares to other existing formalisms. At present there is no easy way to make such comparison. One possible criterion is grammatical coverage. Looking at the grammar coverage/accuracy on the TSNLP suite that was 55 used to evaluate the LINGO ERG grammar, our grammar demonstrates 80% coverage (number of spanning parses). The reported figure for LINGO ERG coverage of CSLI is 77% (Oepen, 1999), but this number has undoubtedly improved in the 9year development period. For example, the current reported coverage figures on spoken dialogue corpora are close to 90% (Oepen et al., 2002). However, the grammar coverage alone is not a satisfactory measure for a deep NLP system for use in practical applications, because the logical forms and therefore the capabilities of deep NLP systems differ significantly. A major distinguishing feature of our system is that the logical form it outputs uses semantically motivated word senses. LINGO ERG, in contrast, contains only syntactically motiword senses. For example, the words not related in any obvious way. This reflects a difference in underlying philosophy. LINGO ERG aims for linguistic precision, and as can be seen from our experiments, requiring the parser to select correct domain-independent word senses lowers accuracy. Our system, however, is built with the goal of easy portability within the context of dialogue systems. The availability of word senses simplifies the design of domain-independent interpretation components, such as reference resolution and speech act interpretation components that use domain-independent syntactic and semantic information to encode conventional interpretation rules. If the LINGO ERG grammar were to be put in a dialogue system that requires domain interpretation and reasoning, an additional lexical interpretation module would have to be developed to perform word sense disambiguation as well as interpretation, something that has not yet been done.</abstract>
<note confidence="0.830845714285714">Acknowledgments We thank 3 reviewers for helpful comments. This work was supported by NSF IIS-0328811, DARPA NBCHD30010 via subcontract to SRI #03-000223 and ONR N00014051004-3 and –8. References H. Alshawi. 1990. Resolving Quasi Logical Forms. Linguistics W. Baker, C. Fillmore and J. B. Lowe. 1998. The Ber- FrameNet Project. Montréal. D. Byron. 2002. Resolving Pronominal Reference to Entities. Philadelphia. Copestake. 1999. (New) LKB CSLI. A. Copestake, D. Flickinger, C. Pollard and I. Sag. 2006. Minimal Recursion Semantics: An Introducon Language and 3(4):281-332. Dzikovska. 2004. Practical Semantic Representafor Natural Language Ph.D. Thesis, University of Rochester. M. Dzikovska, J. Allen and M. Swift. Forthcoming.</note>
<title confidence="0.90258375">Linking Semantic and Knowledge Representations in Multi-domain Dialogue System. of Logic M. Dzikovska, J. Allen and M. Swift. 2003. Integrating Linguistic and Domain Knowledge for Spoken Dia-</title>
<author confidence="0.563065">on</author>
<affiliation confidence="0.512692">Knowledge and Reasoning in Practical Dialogue</affiliation>
<address confidence="0.262499">IJCAI-2003,</address>
<author confidence="0.956048">On-</author>
<affiliation confidence="0.688581">line Statistics for a Unification-based Dialogue Vancouver.</affiliation>
<address confidence="0.728287">Fellbaum. 1998. An Electronic Lexical</address>
<affiliation confidence="0.499681">MIT Press.</affiliation>
<note confidence="0.933327833333333">R. Fuchss, A. Koller, J. Niehren, S. Thater. 2004. Minimal Recursion Semantics as Dominance Con- Barcelona. R. Grisham, C. Macleod and A. Meyers. 1994. Comlex Building a Computational Lexicon. Kyoto. J. Hobbs and S. Shieber. 1987. An Algorithm for Gen- Quantifier Scopings. Linguis- K. Kipper, H. T. Dang and M. Palmer. 2000. Class- Construction of a Verb Lexicon. S. Oepen, D. Flickinger, K. Toutanova and C. Manning. 2002. Lingo Redwoods: A Rich and Dynamic Tree-</note>
<title confidence="0.885687">for HPSG. Workshop on Treebanks and Theories</title>
<author confidence="0.892011">S Oepen</author>
<email confidence="0.801682">www.delph-in.net/itsdb/publications/manual.ps.gz.</email>
<note confidence="0.851131555555556">Parsons. 1990. in the Semantics of English. A in Subatomic MIT Press. D. Schlangen and A. Lascarides 2003. The Interpretaof Non-Sentential Utterances in Dialogue. SIG- Sapporo. J. Tetreault. 2001. A Corpus-Based Evaluation of Cenand Pronoun Resolution. Lin- 27(4):507-520. Vossen, P. (1997) EuroWordNet: A Multilingual Data-</note>
<title confidence="0.799461">for Information Retrieval. In of the Delos on Cross-language Information</title>
<intro confidence="0.51239">56</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Resolving Quasi Logical Forms.</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<pages>16--3</pages>
<contexts>
<context position="6004" citStr="Alshawi, 1990" startWordPosition="938" endWordPosition="939">n ungrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In addition, it has a relatively simple method of computing possible quantifier scoping, drawing from the approaches by (Hobbs &amp; Shieber, 1987) and (Alshawi, 1990). A logical form is set of terms that can be viewed as a rooted graph with each term being a node identified by a unique ID (the variable). There are three types of terms. The first corresponds to generalized quantifiers, and is on the form (&lt;quant&gt; &lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example, the NP Every dog would be captured by the term (Every d1 DOG). The second type of term is the propositional term, which is represented in a neoDavidsonian representation (e.g., Parsons, 1990) using reified events and properties. It has the form (F &lt;id&gt; &lt;type&gt; &lt;arguments&gt;*). The propositional terms pro</context>
</contexts>
<marker>Alshawi, 1990</marker>
<rawString>H. Alshawi. 1990. Resolving Quasi Logical Forms. Computational Linguistics 16(3):133-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Baker</author>
<author>C Fillmore</author>
<author>J B Lowe</author>
</authors>
<date>1998</date>
<booktitle>The Berkeley FrameNet Project. COLING-ACL&apos;98,</booktitle>
<location>Montréal.</location>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>W. Baker, C. Fillmore and J. B. Lowe. 1998. The Berkeley FrameNet Project. COLING-ACL&apos;98, Montréal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Byron</author>
</authors>
<title>Resolving Pronominal Reference to Abstract Entities.</title>
<date>2002</date>
<journal>ACL-02, Philadelphia. A. Copestake.</journal>
<publisher>CSLI.</publisher>
<contexts>
<context position="13905" citStr="Byron, 2002" startWordPosition="2325" endWordPosition="2326">fferent dialog systems use the same discourse processing, whether the task involves collaborative problem solving, learning from instruction or automated tutoring. 5.1 Reference Resolution Our domain-independent representation supports reference resolution in two ways. First, the quantifiers and dependency structure extracted from the sentence allow for implementing reference resolution algorithms based on extracted syntactic features. The system uses different strategies for reFigure 8: The LF-MRS conversion algorithm 53 solving each type of referring expression along the lines described in (Byron, 2002). Second, domain-independent semantic information helps greatly in resolving pronouns and definite descriptions. The general capability provided for resolving referring expressions is to search through the discourse history for the most recent entity that matches the semantic requirements, where recency within an utterance may be reordered to reflect focusing heuristics (Tetreault, 2001). For definite descriptions, the semantic information required is explicit in the lexicon. For pronouns, the parser can often compute semantic features from verb argument restrictions. For instance, the pronoun</context>
</contexts>
<marker>Byron, 2002</marker>
<rawString>D. Byron. 2002. Resolving Pronominal Reference to Abstract Entities. ACL-02, Philadelphia. A. Copestake. 1999. The (New) LKB System. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Minimal Recursion Semantics: An Introduction.</title>
<date>2006</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>3--4</pages>
<contexts>
<context position="5766" citStr="Copestake et al., 2006" startWordPosition="900" endWordPosition="903">thm, but were not able to improve on the hand-tuned preferences in overall performance (Elsner et al., 2005). Figure 3 summarizes some of the most important preferences encoded in our rule weights. Because we are dealing with speech, which is often ungrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In addition, it has a relatively simple method of computing possible quantifier scoping, drawing from the approaches by (Hobbs &amp; Shieber, 1987) and (Alshawi, 1990). A logical form is set of terms that can be viewed as a rooted graph with each term being a node identified by a unique ID (the variable). There are three types of terms. The first corresponds to generalized quantifiers, and is on the form (&lt;quant&gt; &lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example, the NP Every dog would be captured by the term (Every d1 DOG). T</context>
<context position="7664" citStr="Copestake et al. 2006" startWordPosition="1224" endWordPosition="1227">og hates a cat (shown in graphical Prefer • Interpretations without gaps to those with gaps • Subcategorized interpretations over adjuncts • Right attachment of PPs and adverbials • Fully specified constituents over those with dropped or “implicit” arguments • Adjectival modification over noun-noun modification • Standard rules over “robust” rules Figure 3: Some Key Preferences used in Parsing 51 form in Figure 4). (SA x1 TELL :content x2) (F x2 HATE :experience x3 :theme x5) (Every x3 DOG :mods (x4)) (F x4 LARGE :of x3) (A x5 CAT) 4 Comparison of LF and MRS Minimal Recursion Semantics (MRS) (Copestake et al. 2006) is a semantic formalism which has been widely adopted in the last several years. This has motivated some research on how this formalism compares to some traditional semantic formalisms. For example, Fuchss et al. (2004) formally show that the translation from MRS to Dominance Constraints is feasible. We have also found that MRS is very similar to LF in its descriptive power. In fact, we can convert every LF to an equivalent MRS structure with a simple algorithm. First, consider the sentence Every dog hates a cat. Figure 5 shows the LF and MRS representations for this sentence. Figure 5: The L</context>
<context position="12114" citStr="Copestake et al. 2006" startWordPosition="2049" endWordPosition="2052">ich is actually the heart of the LF. We build a qeq relationship between h0 (the top handle) and the handle of this formula term. Figure 6 shows the effect of applying these steps to the above example. Figure 7: The LF and MRS representations for the sentence “Every cat in a room sleeps.” Another interesting issue about these two formalisms is that the effect of applying the simple scoping algorithms referred in section 3 to generate all possible interpretations of a LF is the same as applying MRS axioms and handle constraints to generate all scope-resolved MRSs. For instance, the example in (Copestake et al. 2006), Every nephew of some famous politician saw a pony has the same 5 interpretations using either approach. As the last point here, we need to mention that the algorithm in Figure 8 does not consider fixedscopal terms such as scopal adverbials or negation. However, we believe that the framework itself is able to support these types of scopal term and with a small modification, the scoping algorithm will work well in assigning different possible interpretations. We leave the full discussion about these details as well as the detailed proof of the other claims we made here to another paper. 5 Gene</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2006</marker>
<rawString>A. Copestake, D. Flickinger, C. Pollard and I. Sag. 2006. Minimal Recursion Semantics: An Introduction. Research on Language and Computation, 3(4):281-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dzikovska</author>
</authors>
<title>A Practical Semantic Representation for Natural Language Parsing.</title>
<date>2004</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="913" citStr="Dzikovska, 2004" startWordPosition="136" endWordPosition="137">he constraints are intentionally weak, excluding utterances unsuitable in most contexts (the idea slept) but not attempting to eliminate borderline combinations. The generic selectional restrictions are effective in improving overall parsing accuracy, while remaining valid across multiple domains. An evaluation with an earlier version of the grammar showed that if generic selectional restrictions were removed, full sentence semantic accuracy decreased from 77.8% to 62.6% in an emergency rescue domain, and from 67.9 to 52.5% in a medical domain (using the same versions of grammar and lexicon) (Dzikovska, 2004). The current version of our generic lexicon contains approximately 6400 entries (excluding morphological variants), and the current language ontology has 950 concepts. The lexicon can be supplemented by searching large-scale lexical resources such as WordNet (Fellbaum, 1998) and Comlex (Grisham et al., 1994). If an unknown word is encountered, an underspecified entry is generated on the fly. The entry incorporates as much information from the resource as possible, such as part of speech and syntactic frame. It is assigned an underspecified semantic classification based on correspondences betw</context>
</contexts>
<marker>Dzikovska, 2004</marker>
<rawString>M. Dzikovska. 2004. A Practical Semantic Representation for Natural Language Parsing. Ph.D. Thesis, University of Rochester.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Forthcoming</author>
</authors>
<title>Linking Semantic and Knowledge Representations in a Multi-domain Dialogue System.</title>
<journal>Journal of Logic and Computation.</journal>
<marker>Forthcoming, </marker>
<rawString>M. Dzikovska, J. Allen and M. Swift. Forthcoming. Linking Semantic and Knowledge Representations in a Multi-domain Dialogue System. Journal of Logic and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dzikovska</author>
<author>J Allen</author>
<author>M Swift</author>
</authors>
<date>2003</date>
<booktitle>Integrating Linguistic and Domain Knowledge for Spoken Dialogue Systems in Multiple Domains. Workshop on Knowledge and Reasoning in Practical Dialogue Systems, IJCAI-2003,</booktitle>
<location>Acapulco.</location>
<marker>Dzikovska, Allen, Swift, 2003</marker>
<rawString>M. Dzikovska, J. Allen and M. Swift. 2003. Integrating Linguistic and Domain Knowledge for Spoken Dialogue Systems in Multiple Domains. Workshop on Knowledge and Reasoning in Practical Dialogue Systems, IJCAI-2003, Acapulco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>M Swift</author>
<author>J Allen</author>
<author>D Gildea</author>
</authors>
<title>Online Statistics for a Unification-based Dialogue Parser.</title>
<date>2005</date>
<location>IWPT05, Vancouver.</location>
<contexts>
<context position="5251" citStr="Elsner et al., 2005" startWordPosition="812" endWordPosition="815">The grammar used in all our applications uses these hand-tuned rule weights, which have proven to work relatively well across domains. We do not use a statistical parser based on a trained corpus because in most dialogue-system projects, sufficient amounts of training data are not available and would be too time consuming to collect. In the one domain in which we have a reasonable amount of training data (about 9300 utterances), we experimented with a PCFG using trained probabilities with the Collins algorithm, but were not able to improve on the hand-tuned preferences in overall performance (Elsner et al., 2005). Figure 3 summarizes some of the most important preferences encoded in our rule weights. Because we are dealing with speech, which is often ungrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In additio</context>
</contexts>
<marker>Elsner, Swift, Allen, Gildea, 2005</marker>
<rawString>M. Elsner, M. Swift, J. Allen and D. Gildea. 2005. Online Statistics for a Unification-based Dialogue Parser. IWPT05, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1189" citStr="Fellbaum, 1998" startWordPosition="178" endWordPosition="179">oss multiple domains. An evaluation with an earlier version of the grammar showed that if generic selectional restrictions were removed, full sentence semantic accuracy decreased from 77.8% to 62.6% in an emergency rescue domain, and from 67.9 to 52.5% in a medical domain (using the same versions of grammar and lexicon) (Dzikovska, 2004). The current version of our generic lexicon contains approximately 6400 entries (excluding morphological variants), and the current language ontology has 950 concepts. The lexicon can be supplemented by searching large-scale lexical resources such as WordNet (Fellbaum, 1998) and Comlex (Grisham et al., 1994). If an unknown word is encountered, an underspecified entry is generated on the fly. The entry incorporates as much information from the resource as possible, such as part of speech and syntactic frame. It is assigned an underspecified semantic classification based on correspondences between our language ontology and WordNet synsets. 2.2 Grammar The grammar is context-free, augmented with feature structures and feature unification, motivated from X-bar theory, drawing on principles from GPSG (e.g., head and foot features) and HPSG. A detailed description of a</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fuchss</author>
<author>A Koller</author>
<author>J Niehren</author>
<author>S Thater</author>
</authors>
<date>2004</date>
<booktitle>Minimal Recursion Semantics as Dominance Constraints. ACL-04,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="7884" citStr="Fuchss et al. (2004)" startWordPosition="1261" endWordPosition="1264">with dropped or “implicit” arguments • Adjectival modification over noun-noun modification • Standard rules over “robust” rules Figure 3: Some Key Preferences used in Parsing 51 form in Figure 4). (SA x1 TELL :content x2) (F x2 HATE :experience x3 :theme x5) (Every x3 DOG :mods (x4)) (F x4 LARGE :of x3) (A x5 CAT) 4 Comparison of LF and MRS Minimal Recursion Semantics (MRS) (Copestake et al. 2006) is a semantic formalism which has been widely adopted in the last several years. This has motivated some research on how this formalism compares to some traditional semantic formalisms. For example, Fuchss et al. (2004) formally show that the translation from MRS to Dominance Constraints is feasible. We have also found that MRS is very similar to LF in its descriptive power. In fact, we can convert every LF to an equivalent MRS structure with a simple algorithm. First, consider the sentence Every dog hates a cat. Figure 5 shows the LF and MRS representations for this sentence. Figure 5: The LF (left) and MRS (right) representations for the sentence “Every dog hates a cat.” The first step toward converting LF to MRS is to express LF terms as n-ary relationships. For example we express the LF term (F v1 Hate :</context>
</contexts>
<marker>Fuchss, Koller, Niehren, Thater, 2004</marker>
<rawString>R. Fuchss, A. Koller, J. Niehren, S. Thater. 2004. Minimal Recursion Semantics as Dominance Constraints. ACL-04, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grisham</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<date>1994</date>
<booktitle>Comlex Syntax: Building a Computational Lexicon. COLING 94,</booktitle>
<location>Kyoto.</location>
<contexts>
<context position="1223" citStr="Grisham et al., 1994" startWordPosition="182" endWordPosition="185">luation with an earlier version of the grammar showed that if generic selectional restrictions were removed, full sentence semantic accuracy decreased from 77.8% to 62.6% in an emergency rescue domain, and from 67.9 to 52.5% in a medical domain (using the same versions of grammar and lexicon) (Dzikovska, 2004). The current version of our generic lexicon contains approximately 6400 entries (excluding morphological variants), and the current language ontology has 950 concepts. The lexicon can be supplemented by searching large-scale lexical resources such as WordNet (Fellbaum, 1998) and Comlex (Grisham et al., 1994). If an unknown word is encountered, an underspecified entry is generated on the fly. The entry incorporates as much information from the resource as possible, such as part of speech and syntactic frame. It is assigned an underspecified semantic classification based on correspondences between our language ontology and WordNet synsets. 2.2 Grammar The grammar is context-free, augmented with feature structures and feature unification, motivated from X-bar theory, drawing on principles from GPSG (e.g., head and foot features) and HPSG. A detailed description of an early non-lexicalized version of</context>
</contexts>
<marker>Grisham, Macleod, Meyers, 1994</marker>
<rawString>R. Grisham, C. Macleod and A. Meyers. 1994. Comlex Syntax: Building a Computational Lexicon. COLING 94, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>S Shieber</author>
</authors>
<title>An Algorithm for Generating Quantifier Scopings.</title>
<date>1987</date>
<journal>Computational Linguistics</journal>
<pages>13--1</pages>
<contexts>
<context position="5984" citStr="Hobbs &amp; Shieber, 1987" startWordPosition="933" endWordPosition="936">g with speech, which is often ungrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In addition, it has a relatively simple method of computing possible quantifier scoping, drawing from the approaches by (Hobbs &amp; Shieber, 1987) and (Alshawi, 1990). A logical form is set of terms that can be viewed as a rooted graph with each term being a node identified by a unique ID (the variable). There are three types of terms. The first corresponds to generalized quantifiers, and is on the form (&lt;quant&gt; &lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example, the NP Every dog would be captured by the term (Every d1 DOG). The second type of term is the propositional term, which is represented in a neoDavidsonian representation (e.g., Parsons, 1990) using reified events and properties. It has the form (F &lt;id&gt; &lt;type&gt; &lt;arguments&gt;*). The pro</context>
</contexts>
<marker>Hobbs, Shieber, 1987</marker>
<rawString>J. Hobbs and S. Shieber. 1987. An Algorithm for Generating Quantifier Scopings. Computational Linguistics 13(1-2):47-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>Classbased Construction of a Verb Lexicon.</title>
<date>2000</date>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, H. T. Dang and M. Palmer. 2000. Classbased Construction of a Verb Lexicon. AAAI-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>D Flickinger</author>
<author>K Toutanova</author>
<author>C Manning</author>
</authors>
<title>Lingo Redwoods: A Rich and Dynamic Treebank for HPSG.</title>
<date>2002</date>
<booktitle>First Workshop on Treebanks and Linguistic Theories (TLT2002).</booktitle>
<contexts>
<context position="23573" citStr="Oepen et al., 2002" startWordPosition="3876" endWordPosition="3879">rtant question to ask is how it compares to other existing formalisms. At present there is no easy way to make such comparison. One possible criterion is grammatical coverage. Looking at the grammar coverage/accuracy on the TSNLP suite that was 55 used to evaluate the LINGO ERG grammar, our grammar demonstrates 80% coverage (number of spanning parses). The reported figure for LINGO ERG coverage of CSLI is 77% (Oepen, 1999), but this number has undoubtedly improved in the 9- year development period. For example, the current reported coverage figures on spoken dialogue corpora are close to 90% (Oepen et al., 2002). However, the grammar coverage alone is not a satisfactory measure for a deep NLP system for use in practical applications, because the logical forms and therefore the capabilities of deep NLP systems differ significantly. A major distinguishing feature of our system is that the logical form it outputs uses semantically motivated word senses. LINGO ERG, in contrast, contains only syntactically motivated word senses. For example, the words end and finish are not related in any obvious way. This reflects a difference in underlying philosophy. LINGO ERG aims for linguistic precision, and as can </context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2002</marker>
<rawString>S. Oepen, D. Flickinger, K. Toutanova and C. Manning. 2002. Lingo Redwoods: A Rich and Dynamic Treebank for HPSG. First Workshop on Treebanks and Linguistic Theories (TLT2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
</authors>
<date>1999</date>
<note>incr tsdb()] User Manual. www.delph-in.net/itsdb/publications/manual.ps.gz.</note>
<contexts>
<context position="23380" citStr="Oepen, 1999" startWordPosition="3846" endWordPosition="3847">missing senses for existing words (e.g., serve as in Does that flight serve dinner). 7 Discussion We presented a deep parser and semantic interpreter for use in dialogue systems. An important question to ask is how it compares to other existing formalisms. At present there is no easy way to make such comparison. One possible criterion is grammatical coverage. Looking at the grammar coverage/accuracy on the TSNLP suite that was 55 used to evaluate the LINGO ERG grammar, our grammar demonstrates 80% coverage (number of spanning parses). The reported figure for LINGO ERG coverage of CSLI is 77% (Oepen, 1999), but this number has undoubtedly improved in the 9- year development period. For example, the current reported coverage figures on spoken dialogue corpora are close to 90% (Oepen et al., 2002). However, the grammar coverage alone is not a satisfactory measure for a deep NLP system for use in practical applications, because the logical forms and therefore the capabilities of deep NLP systems differ significantly. A major distinguishing feature of our system is that the logical form it outputs uses semantically motivated word senses. LINGO ERG, in contrast, contains only syntactically motivated</context>
</contexts>
<marker>Oepen, 1999</marker>
<rawString>S. Oepen (1999). [incr tsdb()] User Manual. www.delph-in.net/itsdb/publications/manual.ps.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Parsons</author>
</authors>
<title>Events in the Semantics of English. A Study in Subatomic Semantics.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6493" citStr="Parsons, 1990" startWordPosition="1025" endWordPosition="1026">e method of computing possible quantifier scoping, drawing from the approaches by (Hobbs &amp; Shieber, 1987) and (Alshawi, 1990). A logical form is set of terms that can be viewed as a rooted graph with each term being a node identified by a unique ID (the variable). There are three types of terms. The first corresponds to generalized quantifiers, and is on the form (&lt;quant&gt; &lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example, the NP Every dog would be captured by the term (Every d1 DOG). The second type of term is the propositional term, which is represented in a neoDavidsonian representation (e.g., Parsons, 1990) using reified events and properties. It has the form (F &lt;id&gt; &lt;type&gt; &lt;arguments&gt;*). The propositional terms produced from Every dog hates a cat would be (F h1 HATE :Experiencer d1 :Theme c1). The third type of term is the speech act, which has the same form as propositional terms except for the initial indicator SA identifying it as a performed speech act. The speech act for Every dog hates a cat would be (SA sa1 INFORM :content h1). Putting this all together, we get the following (condensed) LF representation from the parser for Every large dog hates a cat (shown in graphical Prefer • Interpr</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>T. Parsons. 1990. Events in the Semantics of English. A Study in Subatomic Semantics. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schlangen</author>
<author>A Lascarides</author>
</authors>
<date>2003</date>
<booktitle>The Interpretation of Non-Sentential Utterances in Dialogue. SIGDIAL-03,</booktitle>
<location>Sapporo.</location>
<contexts>
<context position="15348" citStr="Schlangen and Lascarides, 2003" startWordPosition="2545" endWordPosition="2549"> for role nouns such as author, we can also handle simple bridging reference. Consider the discourse fragment That book came from the library. The author É. The semantic representation of the author includes its implicit argument, e.g., (The x1 AUTHOR :of b1). Furthermore, the term b1 has the semantic feature INFO-CONTENT, which includes objects that “contain” information such as books, articles, songs, etc.., which allows the pronoun to correctly resolve via bridging to the book in the previous utterance. 5.2 Ellipsis The parser produces a representation of fragmentary utterances similar to (Schlangen and Lascarides, 2003). The main difference is that instead of using a single underspecified unknown_rel predicate to resolve in discourse context, we use a speech act term as the underspecified relation, differentiating between a number of common relations such as acknowledgments, politeness expressions, noun phrases and underspecified predicates (PP, ADJP and VP fragments). The representations of the underspecified predicates also include an IMPRO in place of the unspecified argument. We currently handle only a few key cases of ellipsis. The first is question/answer pairs. By retaining the logical form of the que</context>
</contexts>
<marker>Schlangen, Lascarides, 2003</marker>
<rawString>D. Schlangen and A. Lascarides 2003. The Interpretation of Non-Sentential Utterances in Dialogue. SIGDIAL-03, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
</authors>
<title>A Corpus-Based Evaluation of Centering and Pronoun Resolution. Computational Linguistics.</title>
<date>2001</date>
<pages>27--4</pages>
<contexts>
<context position="14295" citStr="Tetreault, 2001" startWordPosition="2379" endWordPosition="2380">ion algorithms based on extracted syntactic features. The system uses different strategies for reFigure 8: The LF-MRS conversion algorithm 53 solving each type of referring expression along the lines described in (Byron, 2002). Second, domain-independent semantic information helps greatly in resolving pronouns and definite descriptions. The general capability provided for resolving referring expressions is to search through the discourse history for the most recent entity that matches the semantic requirements, where recency within an utterance may be reordered to reflect focusing heuristics (Tetreault, 2001). For definite descriptions, the semantic information required is explicit in the lexicon. For pronouns, the parser can often compute semantic features from verb argument restrictions. For instance, the pronoun it carries little semantic information by itself, but in the utterance Eat it we know we are looking for an edible object. This simple technique performs well in practice. Because of the knowledge in the lexicon for role nouns such as author, we can also handle simple bridging reference. Consider the discourse fragment That book came from the library. The author É. The semantic represen</context>
</contexts>
<marker>Tetreault, 2001</marker>
<rawString>J. Tetreault. 2001. A Corpus-Based Evaluation of Centering and Pronoun Resolution. Computational Linguistics. 27(4):507-520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
</authors>
<title>EuroWordNet: A Multilingual Database for Information Retrieval.</title>
<date>1997</date>
<booktitle>In Proc. of the Delos workshop on Cross-language Information Retrieval.</booktitle>
<marker>Vossen, 1997</marker>
<rawString>Vossen, P. (1997) EuroWordNet: A Multilingual Database for Information Retrieval. In Proc. of the Delos workshop on Cross-language Information Retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>