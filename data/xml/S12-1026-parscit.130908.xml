<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000666">
<title confidence="0.999041">
Unsupervised Induction of a Syntax-Semantics Lexicon
Using Iterative Refinement
</title>
<author confidence="0.938521">
Hagen F¨urstenau
</author>
<affiliation confidence="0.8736025">
CCLS
Columbia University
</affiliation>
<address confidence="0.903768">
New York, NY, USA
</address>
<email confidence="0.999152">
hagen@ccls.columbia.edu
</email>
<author confidence="0.864194">
Owen Rambow
</author>
<affiliation confidence="0.8333115">
CCLS
Columbia University
</affiliation>
<address confidence="0.903873">
New York, NY, USA
</address>
<email confidence="0.999545">
rambow@ccls.columbia.edu
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99937775">
We present a method for learning syntax-
semantics mappings for verbs from unanno-
tated corpora. We learn linkings, i.e., map-
pings from the syntactic arguments and ad-
juncts of a verb to its semantic roles. By learn-
ing such linkings, we do not need to model in-
dividual semantic roles independently of one
another, and we can exploit the relation be-
tween different mappings for the same verb,
or between mappings for different verbs. We
present an evaluation on a standard test set for
semantic role labeling.
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982698">
A verb can have several ways of mapping its seman-
tic arguments to syntax (“diathesis alternations”):
</bodyText>
<listItem confidence="0.9536755">
(1) a. We increased the response rate with SHK.
b. SHK increased the response rate.
</listItem>
<bodyText confidence="0.991232490566038">
c. The response rate increased.
The subject of increase can be the agent (1a), the in-
strument (1b), or the theme (what is being increased)
(1c). Other verbs that show this pattern include
break or melt.
Much theoretical and lexicographic (descriptive)
work has been devoted to determining how verbs
map their lexical predicate-argument structure to
syntactic arguments (Burzio, 1986; Levin, 1993).
The last decades have seen a surge in activity on
the computational front, spurred in part by efforts to
annotate large corpora for lexical semantics (Baker
et al., 1998; Palmer et al., 2005). Initially, we have
seen computational efforts devoted to finding classes
of verbs that share similar syntax-semantics map-
pings from annotated and unannotated corpora (La-
pata and Brew, 1999; Merlo and Stevenson, 2001).
More recently, there has been an explosion of inter-
est in semantic role labeling (with too many recent
publications to cite).
In this paper, we explore learning syntax-
semantics mappings for verbs from unannotated cor-
pora. We are specifically interested in learning link-
ings. A linking is a mapping for one verb from its
syntactic arguments and adjuncts to all of its se-
mantic roles, so that individual semantic roles are
not modeled independently of one another and so
that we can exploit the relation between different
mappings for the same verb (as in (1) above), or
between mappings for different verbs. We there-
fore follow Grenager and Manning (2006) in treat-
ing linkings as first-class objects; however, we dif-
fer from their work in two important respects. First,
we use semantic clustering of head words of argu-
ments in an approach that resembles topic modeling,
rather than directly modeling the subcategorization
of verbs with a distribution over words. Second and
most importantly, we do not make any assumptions
about the linkings, as do Grenager and Manning
(2006). They list a small set of rules from which
they derive all linkings possible in their model; in
contrast, we are able to learn any linking observed
in the data. Therefore, our approach is language-
independent. Grenager and Manning (2006) claim
that their rules represent “a weak form of Universal
Grammar”, but their rules lack such common linking
operations as the addition of an accusative reflex-
ive for the unaccusative (Romance) or case mark-
ing (many languages), and they include a specific
(English) preposition. We have no objection to us-
ing linguistic knowledge, but we do not feel that we
have the empirical basis as of now to provide a set
of Universal Grammar rules relevant for our task.
</bodyText>
<page confidence="0.967237">
180
</page>
<note confidence="0.972703">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 180–188,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999846211538462">
A complete syntax-semantics lexicon describes
how lexemes syntactically realize their semantic ar-
guments, and provides selectional preferences on
these dependents. Though rich lexical resources ex-
ist (such as the PropBank rolesets, the FrameNet lex-
icon, or VerbNet, which relates and extends these
sources), none of them is complete, not even for En-
glish, on which most of the efforts have focused.
However, if a complete syntax-semantics lexicon
did exist, it would be an extremely useful resource:
the task of shallow semantic parsing (semantic ar-
gument detection and semantic role labeling) could
be reduced to determining the best analysis accord-
ing to this lexicon. In fact, the learning model we
present in this paper is itself a semantic role labeling
model, since we can simply apply it to the data we
want to label semantically.
This paper is a step towards the unsupervised in-
duction of a complete syntax-semantics lexicon. We
present a unified procedure for associating verbs
with linkings and for associating the discovered se-
mantic roles with selectional preferences. As input,
we assume a syntactic representation scheme and a
parser which can produce syntactic representations
of unseen sentences in the chosen scheme reason-
ably well, as well as unlabeled text. We do not as-
sume a specific theory of lexical semantics, nor a
specific set of semantic roles. We induce a set of
linkings, which are mappings from semantic role
symbols to syntactic functions. We also induce a
lexicon, which associates a verb lemma with a dis-
tribution over the linkings, and which associates the
sematic role symbols with verb-specific selectional
preferences (which are distributions over distribu-
tions of words). We evaluate on the task of semantic
role labeling using PropBank (Palmer et al., 2005)
as a gold standard.
We focus on semantic arguments, as they are de-
fined specifically for each verb and thus have verb-
specific mappings to syntactic arguments, which
may further be subject to diathesis alternations. In
contrast, semantic adjuncts (modifiers) apply (in
principle) to all verbs, and do not participate in
diathesis alternations. For this reason, the Prop-
Bank lexicon includes arguments but not adjuncts
in its framesets. The method we present in this pa-
per is designed to find verb-specific arguments, and
we therefore take the results on semantic arguments
(Argn) as our primary result. On these, we achieve a
20% F-measure error reduction over a high syntac-
tic baseline (which maps each syntactic relation to a
single semantic argument).
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99996165">
As mentioned above, our approach is most similar
to that of Grenager and Manning (2006). However,
since their model uses hand-crafted rules, they are
able to predict and evaluate against actual PropBank
role labels, whereas our approach has to be evaluated
in terms of clustering quality.
The problem of unsupervised semantic role la-
beling has recently attracted some attention (Lang
and Lapata, 2011a; Lang and Lapata, 2011b; Titov
and Klementiev, 2012). While the present paper
shares the general aim of inducing semantic role
clusters in an unsupervised way, it differs in treat-
ing syntax-semantics linkings explicitly and model-
ing predicate-specific distributions over them.
Abend et al. (2009) address the problem of un-
supervised argument recognition, which we do not
address in the present paper. For the purpose of
building a complete unsupervised semantic parser,
a method such as theirs would be complementary to
our work.
</bodyText>
<sectionHeader confidence="0.994967" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999923315789474">
In this section, we decribe a model that generates
arguments for a given predicate instance. Specifi-
cally, this generative model describes the probability
of a given set of argument head words and associated
syntactic functions in terms of underlying semantic
roles, which are modelled as latent variables. The
semantic role labeling task is therefore framed as the
induction of these latent variables from the observed
data, which we assume to be preprocessed by a syn-
tactic parser.
The basic idea of our approach is to explicitly
model linkings between the syntactic realizations
and the underlying semantic roles of the arguments
in a predicate-argument structure. Since our model
of argument classification is completely unsuper-
vised, we cannot assign familiar semantic role labels
like Agent or Instrument, but rather aim at inducing
role clusters, i.e., clusters of argument instances that
share a semantic role. For example, each of the three
</bodyText>
<page confidence="0.997543">
181
</page>
<bodyText confidence="0.999953228571428">
instances of response rate in (1) should be assigned
to the same cluster. We assume a fixed maximum
number R of semantic roles per predicate and for-
mulate argument classification as the task of assign-
ing each argument in a predicate-argument struc-
ture to one of the numbered roles 1, ... , R. Such
an assignment can therefore be represented by an
R-tuple, where each role position is either filled
by one of the arguments or empty (denoted as E).
We represent each argument by its head word and
its syntactic function, i.e., the path of syntactic de-
pendency relations leading to it from the predicate.
In our example (1a), a possible assignment of ar-
guments to semantic roles could therefore be rep-
resented by a head word tuple (we, rate, E, SHK)
and a corresponding tuple of syntactic functions
(nsubj, dobj, E, prep with), where for the sake of the
example we have chosen R = 4 and the third se-
mantic role slot is empty. Note that this ordered
R-tuple thus represents a semantic labeling of the
unordered set of arguments, which our model takes
as input. While in the case of a single predicate-
argument structure the assignment of arguments to
arbitrary semantic role numbers does not provide
additional information, its value lies in the con-
sistent assignment of arguments to specific roles
across instances of the same predicate. For exam-
ple, to be consistent with the assignment above, (1b)
would have to be represented by (E, rate, E, SHK)
and (E, dobj, E, nsubj).
To formulate a generative model of argument tu-
ples, we separately consider the tuple of argument
head words and the tuple of syntactic functions. The
following two subsections will address each of these
in turn.
</bodyText>
<subsectionHeader confidence="0.999592">
3.1 Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.999868263157895">
The probability of an argument in a certain semantic
role depends strongly on the selectional preferences
of the predicate with respect to this role. In the con-
text of our model, we therefore need to describe the
probability P(wr|p, r) of an argument head word wr
depending on the predicate p and the role r. Instead
of directly modeling predicate- and role-specific dis-
tributions over head words, however, we model se-
lectional preferences as distributions χp,r(c) over se-
mantic word classes c = 1, ... , C (with C being a
fixed model parameter), each of which is in turn as-
sociated with a distribution ψc(wr) over the vocab-
ulary. They are thus similar to topics in semantic
topic models. An advantage of this approach is that
semantic word classes can be shared among different
predicates, which facilitates their inference. Techni-
cally, the introduction of semantic word classes can
be seen as a factorization of the probability of the
argument head P(wr|p, r) = Ep1 χp,r(c)ψc(wr).
</bodyText>
<subsectionHeader confidence="0.999459">
3.2 Linkings
</subsectionHeader>
<bodyText confidence="0.995230884615385">
Another important factor for the assignment of ar-
guments to semantic roles are their syntactic func-
tions. While in the preceding subsection we consid-
ered selectional preferences for each semantic role
separately (assuming their independence), the inter-
dependence between syntactic functions is crucial
and cannot be ignored: The assignment of an ar-
gument does not depend solely on its own syntactic
function, but on the whole subcategorization frame
of the predicate-argument structure. We therefore
have to model the probability of the whole tuple
y = (y1, ... , yR) of syntactic functions.
We assume that for each predicate there is a rela-
tively small number of ways in which it realizes its
arguments syntactically, i.e., in which semantic roles
are linked to syntactic functions. These may corre-
spond to alternations like those shown in (1). Instead
of directly modeling the predicate-specific probabil-
ity P(ylp), we consider predicate-specific distribu-
tions φp(l) over linkings l = (x1,... , xR). Such a
linking then gives rise to the tuple y = (y1, ... , yR)
by way of probability distributions P(yr|xr) =
ηx,.(yr). This allows us to keep the number of possi-
ble linkings l per predicate relatively small (by set-
ting φp(l) = 0 for most l), and generate a wide vari-
ety of syntactic function tuples y from them.
</bodyText>
<subsectionHeader confidence="0.999947">
3.3 Structure of the Model
</subsectionHeader>
<bodyText confidence="0.9989739">
Figure 1 presents our linking model. For each
predicate-argument structure in the corpus, it con-
tains observable variables for the predicate p and the
unordered set s of arguments, and further shows la-
tent variables for the linking l and (for each role r)
the semantic word class c, the head word w, and the
syntactic function y.
The distributions χp,r(c) and ψc(w) are drawn
from Dirichlet priors with symmetric parameters α
and β, respectively. In the case of the linking dis-
</bodyText>
<page confidence="0.995068">
182
</page>
<figureCaption confidence="0.9361832">
Figure 1: Representation of our linking model as a
Bayesian network. The nodes p and s are observed for
each of the N predicate-argument structures in the cor-
pus. The latent variables c, w, l, and y are inferred from
the data along with their distributions χ, ψ, φ, and η.
</figureCaption>
<bodyText confidence="0.999961954545455">
tribution φp(l), we are faced with an exponentially
large space of possible linkings (considering a set
G of syntactic functions, there are (|G |+ 1)R pos-
sible linkings). This is both computationally prob-
lematic and counter-intuitive. We therefore maintain
a global list L of permissible linkings and enforce
φp(l) = 0 for all l ∈/ L. On the set L we then draw
φp(l) from a Dirichlet prior with symmetric param-
eter γ. In Section 3.5, we will describe how the link-
ing list L is iteratively induced from the data.
We introduced the distribution ηx to allow for in-
cidental changes when generating the tuple of syn-
tactic functions out of the linking. If this pro-
cess were allowed to arbitrarily change any syntactic
function in the linking, the linkings would be too un-
constrained and not reflect the syntactic functions in
the corpus. We therefore parameterize ηx in such
a way that the only allowed modifications are the
addition or removal of syntactic functions from the
linking, but no change from one syntactic function
to another. We attain this by parameterizing ηx as
follows:
</bodyText>
<equation confidence="0.981095333333333">
{ ηE ifx=y=E
�G� if x = e and y ∈ G
1�η�
1 − ηx if x ∈ G and y=e
ηx if x = y ∈ G
0 else
</equation>
<bodyText confidence="0.999833454545454">
Here, G again denotes the set of all syntactic func-
tions. The parameter ηE is drawn from a uniform
prior on the interval [0.0, 1.0] and the |G |parame-
ters ηx for x ∈ G have uniform priors on [0.5, 1.0].
This has the effect that no syntactic function can
change into another, that a syntactic function is
never more probable to disappear than to stay, and
that all syntactic functions are added with the same
probability. This last property will be important for
the iterative refinement process described in Sec-
tion 3.5.
</bodyText>
<subsectionHeader confidence="0.971071">
3.4 Training
</subsectionHeader>
<bodyText confidence="0.999991411764706">
In this subsection, we describe how we train the
model described so far, assuming that we are given
a fixed linking list L. The following subsection will
address the problem of infering this list. In Sec-
tion 3.6, we will then describe how we apply the
trained model to infer semantic role assignments for
given predicate-argument structures.
To train the linking model, we apply a Gibbs sam-
pling procedure to the latent variables shown in Fig-
ure 1. In each sampling iteration, we first sample
the values of the latent variables of each predicate-
argument structure based on the current distribu-
tions, and then the latent distributions based on
counts obtained over the corpus. For each predicate-
argument structure, we begin with a blocked sam-
pling step, simultaneously drawing values for w and
y, while summing out c. This gives us
</bodyText>
<equation confidence="0.901942">
P(w, y|p,l, s) ∝
</equation>
<bodyText confidence="0.998420363636364">
where we have omitted the factor P(s|w, y), which
is uniform as long as we assume that w and y in-
deed represent permutations of the argument set s.
To sample efficiently from this distribution, we pre-
compute the inner sum (as a tensor contraction or,
equivalently, R matrix multiplications). We then
enumerate all permutations of the argument set and
compute their probabilities, defaulting to an approx-
imative beam search procedure in cases where the
space of permutations is too large.
Next, the linking l is sampled according to
</bodyText>
<equation confidence="0.994171">
P(l|p,y) ∝ P(l|p)P(y|l) = φp(l)
</equation>
<bodyText confidence="0.9954445">
Since the space L of possible linkings is small, com-
pletely enumerating the values of this distribution is
</bodyText>
<figure confidence="0.919576611111111">
P
φ Tl
l
χ w
C
w
Y
R
S
N
ηx(y) =
R ηxr(yr) �C χp,r(c)ψc(wr)
H c=1
r=1
R
H
r=1
ηxr(yr)
</figure>
<page confidence="0.986879">
183
</page>
<bodyText confidence="0.9672546">
not a problem.
After sampling the latent variables w, y, and l for
each corpus instance, we go on to apply Gibbs sam-
pling to the latent distributions. For example, for φp
we obtain
</bodyText>
<equation confidence="0.993802">
N
P(φpjp1, l1, ... , pN,lN) a P(φp) P(lijpi)
i=1
a Dir(γ)(φp) - � [φp(l)]n1(l) = Dir(~np + γ)(φp)
lEL
</equation>
<bodyText confidence="0.999646913043478">
Here np(l) is the number of corpus instances with
predicate p and latent linking l, and ~np is the vector
of these counts for a fixed p, indexed by l. Hence,
φp is drawn from the Dirichlet distribution parame-
terized by this vector, smoothed in each component
by γ.
In the same way, the sampling distributions for
χp,r and ψc are determined as Dir(~np,r + α) and
Dir(~nc + β), where each ~np,r is a vector of counts1
indexed by word classes c and each ~nc is a vector
of counts indexed by head words wr. Similarly,
we draw the parameter ηE in the parameterization
of ηx from Beta (n(c, c) + 1, ExEG n(c, x) + 1)
and approximate ηx by drawing ηx from
Beta (n(x, x) + 1, n(x, c) + 1) and redrawing
it uniformly from [0.5, 1.0], if it is smaller than 0.5.
In this context, n(x, y) refers to the number of times
the syntactic relation x is turned into y, counted
over all corpus instances and semantic roles.
To test for convergence of the sampling process,
we monitor the log-likelihood of the data. For each
predicate-argument structure with predicate pi and
argument set si, we have
</bodyText>
<equation confidence="0.96763375">
�P(pi, si) a P(ljpi)P(sijl) pz� P(sijli)
l
�= �P(w, y, sijli) = P(w, yjli) =: Li
w,y w,y==&gt;sz
</equation>
<bodyText confidence="0.998691166666667">
The approximation is rather crude (replacing an ex-
pected value by a single sample from P(ljpi)), but
we expect the errors to mostly cancel out over the
instances of the corpus. The last sum ranges over all
pairs (w, y) that represent permutations of the argu-
ment set s, and this can be computed as a by-product
</bodyText>
<footnote confidence="0.844915">
1Since we do not sample c, we use pseudo-counts based on
P(crgyp, r, wr) for each instance.
</footnote>
<bodyText confidence="0.99869875">
of the sampling process of w and y. We then com-
pute L := log HN i=1 Li = EN i=1 log Li, and termi-
nate the sampling process if L does not increase by
more than 0.1% over 5 iterations.
</bodyText>
<subsectionHeader confidence="0.852878">
3.5 Iterative Refinement of Possible Linkings
</subsectionHeader>
<bodyText confidence="0.999994947368421">
In Section 3.3, we have addressed the problem of
the exponentially large space of possible linkings by
introducing a subset L C GR from which linkings
may be drawn. We now need to clarify how this sub-
set is determined. In contrast to Grenager and Man-
ning (2006), we do not want to use any linguistic
intuitions or manual rules to specify this subset, but
rather automatically infer it from the data, so that the
model stays agnostic to the language and paradigm
of semantic roles. We therefore adopt a strategy of
iterative refinement.
We start with a very small set that only contains
the trivial linking (E, ... , c) and one linking for each
of the R most frequent syntactic functions, placing
the most frequent one in the first slot, the second one
in the second slot etc. We then run Gibbs sampling.
When it has converged in terms of log-likelihood,
we add some new linkings to L. These new link-
ings are inferred by inspecting the action of the step
from l to y in the generative model. Here, a syntac-
tic function may be added to or deleted from a link-
ing. If a particular syntactic function is frequently
added to some linking, then a corresponding linking,
i.e., one featuring this syntactic function and thus not
requiring such a modification, seems to be missing
from the set L. We therefore count for each link-
ing l how often it is either reduced by the deletion of
any syntactic function or expanded by the addition
of a syntactic function. We then rank these modifi-
cations in descending order and for each of them de-
termine the semantic role slot in which the modifica-
tion (deletion or addition) occured most frequently.
By applying the modification to this slot, each of the
linkings gives rise to a new one. We add the first a of
those, skipping new linkings if they are duplicates of
those we already have in the linking set. We iterate
this procedure, alternating between Gibbs sampling
to convergence and the addition of a new linkings.
</bodyText>
<subsectionHeader confidence="0.523818">
3.6 Inference
</subsectionHeader>
<bodyText confidence="0.998751">
To predict semantic roles for a given predicate and
argument set, we maximize P(l, w, yjp, s). If the
</bodyText>
<page confidence="0.997598">
184
</page>
<bodyText confidence="0.999860615384615">
space of permutations is too large for exhaustive
enumeration, we apply a similar beam search pro-
cedure as the one employed in training to approxi-
mately maximize P (w, y|p, s, l) for each value of l.
For efficiency, we do not marginalize over l. This
has the potential of reducing prediction quality, as
we do not predict the most likely role assignment,
but rather the most likely combination of role assign-
ment and latent linking.
In all experiments we averaged over 10 consec-
utive samples of the latent distributions, at the end
of the sampling process (i.e., when convergence has
been reached).
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999896888888889">
We train and evaluate our linking model on the data
set produced for the CoNLL-08 Shared Task on
Joint Parsing of Syntactic and Semantic Dependen-
cies (Surdeanu et al., 2008), which is based on the
PropBank corpus (Palmer et al., 2005). This data
set includes part-of-speech tags, lemmatized tokens,
and syntactic dependencies, which have been con-
verted from the manual syntactic annotation of the
underlying Penn Treebank (Marcus et al., 1993).
</bodyText>
<subsectionHeader confidence="0.998208">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999906368421053">
As input to our model, we decided not to use the syn-
tactic representation in the CoNLL-08 data set, but
instead to rely on Stanford Dependencies (de Marn-
effe et al., 2006), which seem to facilitate seman-
tic analysis. We thus used the Stanford Parser2 to
convert the underlying phrase structure trees of the
Penn Tree Bank into Stanford Dependencies. In the
resulting dependency analyses, the syntactic head
word of a semantic role may differ from the syntactic
head according to the provided syntax. We therefore
mapped the semantic role annotation onto the Stan-
ford Dependency trees by identifying the tree node
that covers the same set of tokens as the one marked
in the CoNLL-08 data set.
The focus of the present work is on the linking
behavior and classification of semantic arguments
and not their identification. The latter is a substan-
tially different task, and likely to be best addressed
by other approaches, such as that of (Abend et al.,
</bodyText>
<footnote confidence="0.9378625">
2version 1.6.8, available at http://nlp.stanford.
edu/software/lex-parser.shtml
</footnote>
<bodyText confidence="0.999237214285714">
2009). We therefore use gold standard information
of the CoNLL-08 data set for identifying argument
sets as input to our model. The task of our model is
then to classify these arguments into semantic roles.
We train our model on a corpus consisting of the
training and the test part of the CoNLL-08 data set,
which is permissible since as a unsupervised system
our model does not make any use of the annotated
argument labels for training. We test the model per-
formance against the gold argument classification on
the test part. For development purposes (both de-
signing the model and tuning the parameters as de-
scribed in Section 4.4), we train on the training and
development part and test on the development part.
</bodyText>
<subsectionHeader confidence="0.990371">
4.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9999332">
As explained above, our model does not predict spe-
cific role labels, such as those annotated in Prop-
Bank, but rather aims at clustering like argument
instances together. Since the (numbered) labels of
these clusters are arbitrary, we cannot evaluate the
predictions of our model against the PropBank gold
annotation directly. We follow Lang and Lapata
(2011b) in measuring the quality of our clustering
in terms of cluster purity and collocation instead.
Cluster purity is a measure of the degree to which
the predicted clusters meet the goal of containing
only instances with the same gold standard class la-
bel. Given predicted clusters C1, ... , CnG and gold
clusters G1, ... , GnG over a set of n argument in-
stances, it is defined as
</bodyText>
<equation confidence="0.578555">
max |Ci ∩ Gj|
j=1,...,nG
</equation>
<bodyText confidence="0.99892075">
Similarly, cluster collocation measures how well the
clustering meets the goal of clustering all gold in-
stances with the same label into a single predicted
cluster, formally:
</bodyText>
<equation confidence="0.6115855">
max |Ci ∩ Gj|
i=1,...,nG
</equation>
<bodyText confidence="0.9999085">
We determine purity and collocation separately for
each predicate type and then compute their micro-
average, i.e., weighting each score by the number of
argument instances of this precidate. Just as preci-
sion and recall, purity and collocation stand in trade-
off. In the next section, we therefore report their
</bodyText>
<equation confidence="0.904945714285714">
F1 score, i.e., their harmonic mean 2-Pu-Co
Pu+Co .
Pu = 1 nG
n i=1
1 nG
Co = n
j=1
</equation>
<page confidence="0.99542">
185
</page>
<subsectionHeader confidence="0.998758">
4.3 Syntactic Baseline
</subsectionHeader>
<bodyText confidence="0.999986777777778">
We compare the performance of our model with a
simple syntactic baseline that assumes that semantic
roles are identical with syntactic functions. We fol-
low Lang and Lapata (2011b) in clustering argument
instances of each predicate by their syntactic func-
tions. We do not restrict the number of clusters per
predicate. In contrast, Lang and Lapata (2011b) re-
strict the number of clusters to 21, which is the num-
ber of clusters their system generates. We found that
this reduces the baseline by 0.1% F1-score (Argn on
the development set, c.f. Table 1). If we reduce the
number of clusters in the baseline to the number of
clusters in our system (7), the baseline is reduced by
another 0.8% F1-score. These lower baselines are
due to lower purity values. In general, we find that a
smaller number of clusters results in lower F1 mea-
sure for the baseline; the reported baseline therefore
is the strictest possible.
</bodyText>
<subsectionHeader confidence="0.998534">
4.4 Parameters and Tuning
</subsectionHeader>
<bodyText confidence="0.99999575">
For all experiments, we fixed the number of seman-
tic roles at R = 7. This is the maximum size of the
argument set over all instances of the data set and
thus the lower limit for R. If R was set to a higher
value, the model would be able to account for the
possibility of a larger number of roles, out of which
never more than 7 are expressed simultaneously. We
leave such investigation to future work. We set the
symmetric parameters for the Dirichlet distributions
to α = 1.0, Q = 0.1, and -y = 1.0. This corresponds
to uninformative uniform priors for xp,r and Op, and
a prior encouraging a sparse lexical distribution 0,
similar as in topic models such as LDA (Blei et al.,
2003).
The number C of word classes, the number a of
additional linkings in each refinement of the linking
set L, and the number k of refinement steps were
tuned on the development set. We first fixed a = 10
and trained models for C = 10, 20,. . . ,100, per-
forming 50 refinement steps. The best F1 score was
obtained with C = 10 after k = 20 refinements (i.e.,
with 200 linkings). Next, we fixed these two param-
eters and trained models for a = 5,10,15, 20, 25.
Here, we confirmed an optimal value of a = 10.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999754">
In this section, we give quantitative results, compar-
ing our system to the syntactic baseline in terms of
cluster purity and collocation, and a qualitative dis-
cussion of some phenomena observed in the perfor-
mance of the model.
</bodyText>
<subsectionHeader confidence="0.991533">
5.1 Quantitative Results
</subsectionHeader>
<bodyText confidence="0.999989607142857">
Table 1 shows the results of applying our models to
the CoNLL-08 test with the parameter values tuned
in Section 4.4. For comparison, we also show re-
sults on the development set. The table is divided
into three parts, one only considering semantic ar-
guments (Argn), one considering adjuncts (ArgM),
and one aggregating results over both kinds of Prop-
Bank roles (Arg*). It can be seen that our model
consistently outperforms the syntactic baseline in
terms of collocation (by 10% on Argn, 3% on ArgM,
and 8.2% overall). In terms of purity, however, it
falls short of the baseline. As mentioned above,
there is a trade-off between purity and collocation.
Compared to our model, which we run with a total
of 7 semantic role slots, the baseline predicts a large
number of small argument clusters for each predi-
cate, whereas our model tends to group arguments
together based on selectional preferences.
In terms of F1 score, our model outperforms the
baseline by 3.6% on Argn, which translates into a
relative error reduction by 20%. On adjuncts, on
the other hand, our model falls short of the base-
line by almost 10% F1 score. This indicates that
our approach based on explicit representations of
linkings is most suited to the classification of argu-
ments rather than adjuncts, which do not participate
in diathesis alternations and do therefore not profit
as much from our explicit induction of linkings.
</bodyText>
<subsectionHeader confidence="0.995186">
5.2 Qualitative Observations
</subsectionHeader>
<bodyText confidence="0.92111775">
Among the verbs with at least 10 test instances, in-
clude shows the largest gain in F1 score over the
baseline. In the test corpus, we find an interesting
pair of sentences for this predicate:
(2) a. Mr. Herscu proceeded to launch an ambi-
tious, but ill-fated, $1 billion acquisition
binge that included Bonwit Teller and B.
Altman &amp; Co. [...]
</bodyText>
<page confidence="0.993689">
186
</page>
<table confidence="0.999642428571429">
Argn ArgM Arg*
Test Set Pu Co F1 Pu Co F1 Pu Co F1
Syntactic Baseline 90.6 75.4 82.3 87.0 73.3 79.6 88.0 74.9 80.9
Linking Model 86.4 85.4 85.9 64.4 76.3 69.8 74.5 83.1 78.6
Development Set Pu Co F1 Pu Co F1 Pu Co F1
Syntactic Baseline 91.5 73.9 81.8 88.7 78.6 83.3 89.2 75.1 81.5
Linking Model 85.6 84.4 85.0 67.7 79.9 73.3 75.2 83.2 79.0
</table>
<tableCaption confidence="0.84810375">
Table 1: Purity (Pu), collocation (Co), and Fl scores of our model and the syntactic baseline in percent. Performance
on arguments (Argn), adjuncts (ArgM), and overall results (Arg*) are shown separately.
b. Not included in the bid are Bonwit Teller or
B. Altman &amp; Co. [...]
</tableCaption>
<bodyText confidence="0.984605810810811">
The first of these two sentences is generated from the
linking (nsubj, dobj, c, c, c, c, -rcmod), which does
not need to be modified in any way to account for the
subject that (coreferent with the head of the pred-
icate in the modifying relative clause, binge) and
the direct object Teller (head of the phrase Bonwit
Teller and B. Altman &amp; Co.). These are assigned
to the first and second role slots, respectively. The
second sentence, on the other hand, is generated out
of the linking (prep in, nsubjpass, c, c, c, c, c). Here,
the passive subject Teller is assigned to the second
role slot (which we may interpret as the Includee),
while the first semantic role (the Includer) is labeled
on bid, which is realized in a prepositional phrase
headed by the preposition in. Note that this alter-
nation is not the general passive alternation though,
which would have led to Teller is not included by the
bid. Instead, the model learned a specific alternation
pattern for the predicate include.
But even where a specific linking has not been
learned, the model can often still infer a correct la-
beling by virtue of its selectional preference com-
ponent. In our corpus, the predicate give occurs
mostly with a direct and an indirect object as in
CNN recently gave most employees raises of as
much as 15%. The model therefore learns a link-
ing (nsubj, dobj, c, c, c, c, iobj), but fails to learn that
the Beneficient role can also be expressed with the
preposition to as in
(3) [...] only 25% give $2,500 or more to charity
each year.
However, when applying our model to this sentence,
it nonetheless assigns charity to the last role slot (the
same one previously occupied by the indirect ob-
ject). This is due to the fact that charity is a good
fit for the selectional preference of this role slot of
the predicate give.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.981134275862069">
We have presented a novel generative model of
predicate-argument structures that incorporates se-
lectional preferences of argument heads and explic-
itly describes linkings between semantic roles and
syntactic functions. The model iteratively induces
a lexicon of possible linkings from unlabeled data.
The trained model can be used to cluster given ar-
gument instances according to their semantic roles,
outperforming a competitive syntactic baseline.
The approach is independent of any particular lan-
guage or paradigm of semantic roles. However, in
its present form the model assumes that each predi-
cate has its own set of semantic roles. In formalisms
such as Frame Semantics (Baker et al., 1998), se-
mantic roles generalize across semantically similar
predicates belonging to the same frame. A natural
extension of our approach would therefore consist in
modeling predicate groups that share semantic roles
and selectional preferences.
Acknowledgments. This work was supported by the In-
telligence Advanced Research Projects Activity (IARPA) via
Department of Interior National Business Center (DoI/NBC)
contract number D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily rep-
resenting the official policies or endorsements, either expressed
or implied, of IARPA, DoI/NBC, or the U.S. Government.
</bodyText>
<page confidence="0.997296">
187
</page>
<sectionHeader confidence="0.998329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999900641791045">
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 28–36, Singapore.
Collin F. Baker, J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In 36th Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics (COLING-ACL’98), pages 86–90, Montr´eal.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Luigi Burzio. 1986. Italian Syntax: A Government-
Binding Approach. Reidel, Dordrecht.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC 2006.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 1–8,
Sydney, Australia.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1117–1126, Portland, Ore-
gon, USA.
Joel Lang and Mirella Lapata. 2011b. Unsupervised se-
mantic role induction with graph partitioning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1320–
1331, Edinburgh, Scotland, UK.
Maria Lapata and Chris Brew. 1999. Using subcatego-
rization to resolve verb class ambiguity. In In Proceed-
ings of Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 266—-274, College Park, MD.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Mitchell M. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19.2:313–330, June.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions of
argument structure. Computational Linguistics, 27(3).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159–177, Manchester,
England.
Ivan Titov and Alexandre Klementiev. 2012. A bayesian
approach to unsupervised semantic role induction. In
Proceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics,
Avignon, France, April.
</reference>
<page confidence="0.997527">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.097869">
<title confidence="0.978991">Unsupervised Induction of a Syntax-Semantics Using Iterative Refinement</title>
<author confidence="0.715327">Hagen</author>
<affiliation confidence="0.3385">Columbia</affiliation>
<address confidence="0.520978">New York, NY,</address>
<email confidence="0.999664">hagen@ccls.columbia.edu</email>
<author confidence="0.838728">Owen</author>
<affiliation confidence="0.689388">Columbia</affiliation>
<address confidence="0.622178">New York, NY,</address>
<email confidence="0.99979">rambow@ccls.columbia.edu</email>
<abstract confidence="0.999288846153846">We present a method for learning syntaxsemantics mappings for verbs from unannocorpora. We learn i.e., mappings from the syntactic arguments and adjuncts of a verb to its semantic roles. By learning such linkings, we do not need to model individual semantic roles independently of one another, and we can exploit the relation between different mappings for the same verb, or between mappings for different verbs. We present an evaluation on a standard test set for semantic role labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised argument identification for semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="6978" citStr="Abend et al. (2009)" startWordPosition="1108" endWordPosition="1111">g (2006). However, since their model uses hand-crafted rules, they are able to predict and evaluate against actual PropBank role labels, whereas our approach has to be evaluated in terms of clustering quality. The problem of unsupervised semantic role labeling has recently attracted some attention (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). While the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them. Abend et al. (2009) address the problem of unsupervised argument recognition, which we do not address in the present paper. For the purpose of building a complete unsupervised semantic parser, a method such as theirs would be complementary to our work. 3 Model In this section, we decribe a model that generates arguments for a given predicate instance. Specifically, this generative model describes the probability of a given set of argument head words and associated syntactic functions in terms of underlying semantic roles, which are modelled as latent variables. The semantic role labeling task is therefore framed</context>
</contexts>
<marker>Abend, Reichart, Rappoport, 2009</marker>
<rawString>Omri Abend, Roi Reichart, and Ari Rappoport. 2009. Unsupervised argument identification for semantic role labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 28–36, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98),</booktitle>
<pages>86--90</pages>
<location>Montr´eal.</location>
<contexts>
<context position="1525" citStr="Baker et al., 1998" startWordPosition="236" endWordPosition="239">rate with SHK. b. SHK increased the response rate. c. The response rate increased. The subject of increase can be the agent (1a), the instrument (1b), or the theme (what is being increased) (1c). Other verbs that show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to al</context>
<context position="31982" citStr="Baker et al., 1998" startWordPosition="5475" endWordPosition="5478">that incorporates selectional preferences of argument heads and explicitly describes linkings between semantic roles and syntactic functions. The model iteratively induces a lexicon of possible linkings from unlabeled data. The trained model can be used to cluster given argument instances according to their semantic roles, outperforming a competitive syntactic baseline. The approach is independent of any particular language or paradigm of semantic roles. However, in its present form the model assumes that each predicate has its own set of semantic roles. In formalisms such as Frame Semantics (Baker et al., 1998), semantic roles generalize across semantically similar predicates belonging to the same frame. A natural extension of our approach would therefore consist in modeling predicate groups that share semantic roles and selectional preferences. Acknowledgments. This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D11PC20153. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The vi</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98), pages 86–90, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="26304" citStr="Blei et al., 2003" startWordPosition="4483" endWordPosition="4486">= 7. This is the maximum size of the argument set over all instances of the data set and thus the lower limit for R. If R was set to a higher value, the model would be able to account for the possibility of a larger number of roles, out of which never more than 7 are expressed simultaneously. We leave such investigation to future work. We set the symmetric parameters for the Dirichlet distributions to α = 1.0, Q = 0.1, and -y = 1.0. This corresponds to uninformative uniform priors for xp,r and Op, and a prior encouraging a sparse lexical distribution 0, similar as in topic models such as LDA (Blei et al., 2003). The number C of word classes, the number a of additional linkings in each refinement of the linking set L, and the number k of refinement steps were tuned on the development set. We first fixed a = 10 and trained models for C = 10, 20,. . . ,100, performing 50 refinement steps. The best F1 score was obtained with C = 10 after k = 20 refinements (i.e., with 200 linkings). Next, we fixed these two parameters and trained models for a = 5,10,15, 20, 25. Here, we confirmed an optimal value of a = 10. 5 Results In this section, we give quantitative results, comparing our system to the syntactic ba</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Burzio</author>
</authors>
<title>Italian Syntax: A GovernmentBinding Approach.</title>
<date>1986</date>
<location>Reidel, Dordrecht.</location>
<contexts>
<context position="1340" citStr="Burzio, 1986" startWordPosition="207" endWordPosition="208">t for semantic role labeling. 1 Introduction A verb can have several ways of mapping its semantic arguments to syntax (“diathesis alternations”): (1) a. We increased the response rate with SHK. b. SHK increased the response rate. c. The response rate increased. The subject of increase can be the agent (1a), the instrument (1b), or the theme (what is being increased) (1c). Other verbs that show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemant</context>
</contexts>
<marker>Burzio, 1986</marker>
<rawString>Luigi Burzio. 1986. Italian Syntax: A GovernmentBinding Approach. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2421" citStr="Grenager and Manning (2006)" startWordPosition="382" endWordPosition="385">en an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for different verbs. We therefore follow Grenager and Manning (2006) in treating linkings as first-class objects; however, we differ from their work in two important respects. First, we use semantic clustering of head words of arguments in an approach that resembles topic modeling, rather than directly modeling the subcategorization of verbs with a distribution over words. Second and most importantly, we do not make any assumptions about the linkings, as do Grenager and Manning (2006). They list a small set of rules from which they derive all linkings possible in their model; in contrast, we are able to learn any linking observed in the data. Therefore, our ap</context>
<context position="6367" citStr="Grenager and Manning (2006)" startWordPosition="1016" endWordPosition="1019">emantic adjuncts (modifiers) apply (in principle) to all verbs, and do not participate in diathesis alternations. For this reason, the PropBank lexicon includes arguments but not adjuncts in its framesets. The method we present in this paper is designed to find verb-specific arguments, and we therefore take the results on semantic arguments (Argn) as our primary result. On these, we achieve a 20% F-measure error reduction over a high syntactic baseline (which maps each syntactic relation to a single semantic argument). 2 Related Work As mentioned above, our approach is most similar to that of Grenager and Manning (2006). However, since their model uses hand-crafted rules, they are able to predict and evaluate against actual PropBank role labels, whereas our approach has to be evaluated in terms of clustering quality. The problem of unsupervised semantic role labeling has recently attracted some attention (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). While the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them. Abend et</context>
<context position="18642" citStr="Grenager and Manning (2006)" startWordPosition="3155" endWordPosition="3159"> s, and this can be computed as a by-product 1Since we do not sample c, we use pseudo-counts based on P(crgyp, r, wr) for each instance. of the sampling process of w and y. We then compute L := log HN i=1 Li = EN i=1 log Li, and terminate the sampling process if L does not increase by more than 0.1% over 5 iterations. 3.5 Iterative Refinement of Possible Linkings In Section 3.3, we have addressed the problem of the exponentially large space of possible linkings by introducing a subset L C GR from which linkings may be drawn. We now need to clarify how this subset is determined. In contrast to Grenager and Manning (2006), we do not want to use any linguistic intuitions or manual rules to specify this subset, but rather automatically infer it from the data, so that the model stays agnostic to the language and paradigm of semantic roles. We therefore adopt a strategy of iterative refinement. We start with a very small set that only contains the trivial linking (E, ... , c) and one linking for each of the R most frequent syntactic functions, placing the most frequent one in the first slot, the second one in the second slot etc. We then run Gibbs sampling. When it has converged in terms of log-likelihood, we add </context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>Trond Grenager and Christopher D. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 1–8, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction via split-merge clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1117--1126</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="6680" citStr="Lang and Lapata, 2011" startWordPosition="1064" endWordPosition="1067">s on semantic arguments (Argn) as our primary result. On these, we achieve a 20% F-measure error reduction over a high syntactic baseline (which maps each syntactic relation to a single semantic argument). 2 Related Work As mentioned above, our approach is most similar to that of Grenager and Manning (2006). However, since their model uses hand-crafted rules, they are able to predict and evaluate against actual PropBank role labels, whereas our approach has to be evaluated in terms of clustering quality. The problem of unsupervised semantic role labeling has recently attracted some attention (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). While the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them. Abend et al. (2009) address the problem of unsupervised argument recognition, which we do not address in the present paper. For the purpose of building a complete unsupervised semantic parser, a method such as theirs would be complementary to our work. 3 Model In this section, we decribe a model that generates arguments</context>
<context position="23659" citStr="Lang and Lapata (2011" startWordPosition="4012" endWordPosition="4015">t the gold argument classification on the test part. For development purposes (both designing the model and tuning the parameters as described in Section 4.4), we train on the training and development part and test on the development part. 4.2 Evaluation Measures As explained above, our model does not predict specific role labels, such as those annotated in PropBank, but rather aims at clustering like argument instances together. Since the (numbered) labels of these clusters are arbitrary, we cannot evaluate the predictions of our model against the PropBank gold annotation directly. We follow Lang and Lapata (2011b) in measuring the quality of our clustering in terms of cluster purity and collocation instead. Cluster purity is a measure of the degree to which the predicted clusters meet the goal of containing only instances with the same gold standard class label. Given predicted clusters C1, ... , CnG and gold clusters G1, ... , GnG over a set of n argument instances, it is defined as max |Ci ∩ Gj| j=1,...,nG Similarly, cluster collocation measures how well the clustering meets the goal of clustering all gold instances with the same label into a single predicted cluster, formally: max |Ci ∩ Gj| i=1,..</context>
<context position="25036" citStr="Lang and Lapata (2011" startWordPosition="4250" endWordPosition="4253"> argument instances of this precidate. Just as precision and recall, purity and collocation stand in tradeoff. In the next section, we therefore report their F1 score, i.e., their harmonic mean 2-Pu-Co Pu+Co . Pu = 1 nG n i=1 1 nG Co = n j=1 185 4.3 Syntactic Baseline We compare the performance of our model with a simple syntactic baseline that assumes that semantic roles are identical with syntactic functions. We follow Lang and Lapata (2011b) in clustering argument instances of each predicate by their syntactic functions. We do not restrict the number of clusters per predicate. In contrast, Lang and Lapata (2011b) restrict the number of clusters to 21, which is the number of clusters their system generates. We found that this reduces the baseline by 0.1% F1-score (Argn on the development set, c.f. Table 1). If we reduce the number of clusters in the baseline to the number of clusters in our system (7), the baseline is reduced by another 0.8% F1-score. These lower baselines are due to lower purity values. In general, we find that a smaller number of clusters results in lower F1 measure for the baseline; the reported baseline therefore is the strictest possible. 4.4 Parameters and Tuning For all experi</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011a. Unsupervised semantic role induction via split-merge clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1117–1126, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction with graph partitioning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1320--1331</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="6680" citStr="Lang and Lapata, 2011" startWordPosition="1064" endWordPosition="1067">s on semantic arguments (Argn) as our primary result. On these, we achieve a 20% F-measure error reduction over a high syntactic baseline (which maps each syntactic relation to a single semantic argument). 2 Related Work As mentioned above, our approach is most similar to that of Grenager and Manning (2006). However, since their model uses hand-crafted rules, they are able to predict and evaluate against actual PropBank role labels, whereas our approach has to be evaluated in terms of clustering quality. The problem of unsupervised semantic role labeling has recently attracted some attention (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). While the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them. Abend et al. (2009) address the problem of unsupervised argument recognition, which we do not address in the present paper. For the purpose of building a complete unsupervised semantic parser, a method such as theirs would be complementary to our work. 3 Model In this section, we decribe a model that generates arguments</context>
<context position="23659" citStr="Lang and Lapata (2011" startWordPosition="4012" endWordPosition="4015">t the gold argument classification on the test part. For development purposes (both designing the model and tuning the parameters as described in Section 4.4), we train on the training and development part and test on the development part. 4.2 Evaluation Measures As explained above, our model does not predict specific role labels, such as those annotated in PropBank, but rather aims at clustering like argument instances together. Since the (numbered) labels of these clusters are arbitrary, we cannot evaluate the predictions of our model against the PropBank gold annotation directly. We follow Lang and Lapata (2011b) in measuring the quality of our clustering in terms of cluster purity and collocation instead. Cluster purity is a measure of the degree to which the predicted clusters meet the goal of containing only instances with the same gold standard class label. Given predicted clusters C1, ... , CnG and gold clusters G1, ... , GnG over a set of n argument instances, it is defined as max |Ci ∩ Gj| j=1,...,nG Similarly, cluster collocation measures how well the clustering meets the goal of clustering all gold instances with the same label into a single predicted cluster, formally: max |Ci ∩ Gj| i=1,..</context>
<context position="25036" citStr="Lang and Lapata (2011" startWordPosition="4250" endWordPosition="4253"> argument instances of this precidate. Just as precision and recall, purity and collocation stand in tradeoff. In the next section, we therefore report their F1 score, i.e., their harmonic mean 2-Pu-Co Pu+Co . Pu = 1 nG n i=1 1 nG Co = n j=1 185 4.3 Syntactic Baseline We compare the performance of our model with a simple syntactic baseline that assumes that semantic roles are identical with syntactic functions. We follow Lang and Lapata (2011b) in clustering argument instances of each predicate by their syntactic functions. We do not restrict the number of clusters per predicate. In contrast, Lang and Lapata (2011b) restrict the number of clusters to 21, which is the number of clusters their system generates. We found that this reduces the baseline by 0.1% F1-score (Argn on the development set, c.f. Table 1). If we reduce the number of clusters in the baseline to the number of clusters in our system (7), the baseline is reduced by another 0.8% F1-score. These lower baselines are due to lower purity values. In general, we find that a smaller number of clusters results in lower F1 measure for the baseline; the reported baseline therefore is the strictest possible. 4.4 Parameters and Tuning For all experi</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011b. Unsupervised semantic role induction with graph partitioning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320– 1331, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Chris Brew</author>
</authors>
<title>Using subcategorization to resolve verb class ambiguity. In</title>
<date>1999</date>
<booktitle>In Proceedings of Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>266--274</pages>
<location>College Park, MD.</location>
<contexts>
<context position="1737" citStr="Lapata and Brew, 1999" startWordPosition="267" endWordPosition="271">hat show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or b</context>
</contexts>
<marker>Lapata, Brew, 1999</marker>
<rawString>Maria Lapata and Chris Brew. 1999. Using subcategorization to resolve verb class ambiguity. In In Proceedings of Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 266—-274, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1354" citStr="Levin, 1993" startWordPosition="209" endWordPosition="210"> role labeling. 1 Introduction A verb can have several ways of mapping its semantic arguments to syntax (“diathesis alternations”): (1) a. We increased the response rate with SHK. b. SHK increased the response rate. c. The response rate increased. The subject of increase can be the agent (1a), the instrument (1b), or the theme (what is being increased) (1c). Other verbs that show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings f</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell M Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="21517" citStr="Marcus et al., 1993" startWordPosition="3657" endWordPosition="3660">eriments we averaged over 10 consecutive samples of the latent distributions, at the end of the sampling process (i.e., when convergence has been reached). 4 Experimental Setup We train and evaluate our linking model on the data set produced for the CoNLL-08 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008), which is based on the PropBank corpus (Palmer et al., 2005). This data set includes part-of-speech tags, lemmatized tokens, and syntactic dependencies, which have been converted from the manual syntactic annotation of the underlying Penn Treebank (Marcus et al., 1993). 4.1 Data Set As input to our model, we decided not to use the syntactic representation in the CoNLL-08 data set, but instead to rely on Stanford Dependencies (de Marneffe et al., 2006), which seem to facilitate semantic analysis. We thus used the Stanford Parser2 to convert the underlying phrase structure trees of the Penn Tree Bank into Stanford Dependencies. In the resulting dependency analyses, the syntactic head word of a semantic role may differ from the syntactic head according to the provided syntax. We therefore mapped the semantic role annotation onto the Stanford Dependency trees b</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell M. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19.2:313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatic verb classification based on statistical distributions of argument structure.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="1765" citStr="Merlo and Stevenson, 2001" startWordPosition="272" endWordPosition="275">nclude break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for differen</context>
</contexts>
<marker>Merlo, Stevenson, 2001</marker>
<rawString>Paola Merlo and Suzanne Stevenson. 2001. Automatic verb classification based on statistical distributions of argument structure. Computational Linguistics, 27(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="1547" citStr="Palmer et al., 2005" startWordPosition="240" endWordPosition="243">K increased the response rate. c. The response rate increased. The subject of increase can be the agent (1a), the instrument (1b), or the theme (what is being increased) (1c). Other verbs that show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic role</context>
<context position="5510" citStr="Palmer et al., 2005" startWordPosition="877" endWordPosition="880">representations of unseen sentences in the chosen scheme reasonably well, as well as unlabeled text. We do not assume a specific theory of lexical semantics, nor a specific set of semantic roles. We induce a set of linkings, which are mappings from semantic role symbols to syntactic functions. We also induce a lexicon, which associates a verb lemma with a distribution over the linkings, and which associates the sematic role symbols with verb-specific selectional preferences (which are distributions over distributions of words). We evaluate on the task of semantic role labeling using PropBank (Palmer et al., 2005) as a gold standard. We focus on semantic arguments, as they are defined specifically for each verb and thus have verbspecific mappings to syntactic arguments, which may further be subject to diathesis alternations. In contrast, semantic adjuncts (modifiers) apply (in principle) to all verbs, and do not participate in diathesis alternations. For this reason, the PropBank lexicon includes arguments but not adjuncts in its framesets. The method we present in this paper is designed to find verb-specific arguments, and we therefore take the results on semantic arguments (Argn) as our primary resul</context>
<context position="21308" citStr="Palmer et al., 2005" startWordPosition="3627" endWordPosition="3630">ize over l. This has the potential of reducing prediction quality, as we do not predict the most likely role assignment, but rather the most likely combination of role assignment and latent linking. In all experiments we averaged over 10 consecutive samples of the latent distributions, at the end of the sampling process (i.e., when convergence has been reached). 4 Experimental Setup We train and evaluate our linking model on the data set produced for the CoNLL-08 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008), which is based on the PropBank corpus (Palmer et al., 2005). This data set includes part-of-speech tags, lemmatized tokens, and syntactic dependencies, which have been converted from the manual syntactic annotation of the underlying Penn Treebank (Marcus et al., 1993). 4.1 Data Set As input to our model, we decided not to use the syntactic representation in the CoNLL-08 data set, but instead to rely on Stanford Dependencies (de Marneffe et al., 2006), which seem to facilitate semantic analysis. We thus used the Stanford Parser2 to convert the underlying phrase structure trees of the Penn Tree Bank into Stanford Dependencies. In the resulting dependenc</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The conll 2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<location>Manchester, England.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll 2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Avignon, France,</location>
<contexts>
<context position="6734" citStr="Titov and Klementiev, 2012" startWordPosition="1072" endWordPosition="1075">sult. On these, we achieve a 20% F-measure error reduction over a high syntactic baseline (which maps each syntactic relation to a single semantic argument). 2 Related Work As mentioned above, our approach is most similar to that of Grenager and Manning (2006). However, since their model uses hand-crafted rules, they are able to predict and evaluate against actual PropBank role labels, whereas our approach has to be evaluated in terms of clustering quality. The problem of unsupervised semantic role labeling has recently attracted some attention (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). While the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them. Abend et al. (2009) address the problem of unsupervised argument recognition, which we do not address in the present paper. For the purpose of building a complete unsupervised semantic parser, a method such as theirs would be complementary to our work. 3 Model In this section, we decribe a model that generates arguments for a given predicate instance. Specifically, this ge</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A bayesian approach to unsupervised semantic role induction. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, Avignon, France, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>