<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017773">
<note confidence="0.809054">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 157-159, Lisbon, Portugal, 2000.
</note>
<title confidence="0.942042">
Single-Classifier Memory-Based Phrase Chunking
</title>
<author confidence="0.830847">
Jorn Veenstra and Antal van den Bosch
</author>
<note confidence="0.366874">
ILK / Computational Linguistics, Tilburg University
Iveenstra, antalblOkub.n1
</note>
<sectionHeader confidence="0.992059" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999682">
In the shared task for CoNLL-2000, words and
tags form the basic multi-valued features for
predicting a rich phrase segmentation code.
While the tag features, containing WSJ part-of-
speech tags (Marcus et al., 1993), have about
45 values, the word features have more than
10,000 values. In our study we have looked at
how memory-based learning, as implemented in
the TiMBL software system (Daelemans et al.,
2000), can handle such features. We have lim-
ited our search to single classifiers, thereby ex-
plicitly ignoring the possibility to build a meta-
learning classifier architecture that could be ex-
pected to improve accuracy. Given this restric-
tion we have explored the following:
</bodyText>
<listItem confidence="0.981140722222222">
1. The generalization accuracy of TiMBL
with default settings (multi-valued fea-
tures, overlap metric, feature weighting).
2. The usage of MVDM (Stanfill and Waltz,
1986; Cost and Salzberg, 1993) (Section 2),
which should work well on word value pairs
with a medium or high frequency, but may
work badly on word value pairs with low
frequency.
3. The straightforward unpacking of feature
values into binary features. On some tasks
we have found that splitting multi-valued
features into several binary features can en-
hance performance of the classifier.
4. A heuristic search for complex features on
the basis of all unpacked feature values, and
using these complex features for the classi-
fication task.
</listItem>
<sectionHeader confidence="0.672993" genericHeader="categories and subject descriptors">
2 Methods and Data
</sectionHeader>
<bodyText confidence="0.996012416666667">
The data used for this shared task is compa-
rable to the dataset used in (Buchholz et al.,
1999), who found an optimal windowing context
size of five words and POS tags to the left, the
word itself, and three words and POS tags to
the right. We also used this window size, and
have applied TiMBL to the shared task data
using default TiMBL settings. TiMBL and the
abovementioned feature metrics are introduced
in the following sections.
IB1-IG The default TiMBL setting, iBl-IG,
(Daelemans et al., 1997) is a memory-based
learning algorithm that builds a database of
instances (the instance base) during learning.
An instance consists of a fixed-length vector of
n feature-value pairs, and an information field
containing the classification of that particular
feature-value vector. After the instance base
is built, new (test) instances are classified by
matching them to all instances in the instance
base, and by calculating with each match the
distance between the new instance X and the
memory instance Y.
The most basic metric for patterns with sym-
bolic features is the Overlap metric given in
equation 1; where A(X, Y) is the distance be-
tween patterns X and Y, represented by n fea-
tures, wi is a weight for feature i, and 6 is the
distance per feature. The k-NN algorithm with
this metric, and equal weighting for all features
is called ml (Aha et al., 1991). Usually k is set
to 1.
A(X, Y) = E wi 6(xi,yi) (1)
where: 6(xi, yi) = 0 if xi = yi, else 1
This distance metric simply counts the num-
ber of (mis)matching feature values in both pat-
</bodyText>
<page confidence="0.994374">
157
</page>
<table confidence="0.999137466666667">
method k AdjP AdvP ConjP Intj NP PP PRT SBAR VP tot
IB1-IG 1 60.9 75.3 17.6 66.7 91.0 95.9 65.4 78.2 91.6 90.5
IB1-IG 3 64.3 76.8 38.5 66.7 91.5 95.8 61.4 79.6 91.7 91.0
IB1-IG 5 65.4 76.5 38.5 66.7 91.6 95.8 63.7 78.6 91.6 91.0
IB1-IG 7 66.1 76.5 41.7 66.7 91.2 95.4 63.7 76.8 91.4 90.7
MVDM-all 1 58.3 73.9 34.5 0 90.2 95.6 54.6 78.1 89.7 89.6
MVDM-all 3 60.0 77.0 30.8 0 91.0 96.2 60.6 80.0 91.6 90.9
MVDM-all 5 61.0 76.8 30.8 0 91.3 96.1 60.1 79.5 91.9 91.0
MVDM-all 7 62.4 76.5 40.0 0 91.4 96.0 59.8 78.8 91.9 91.1
MVDM-POS 1 59.3 76.6 12.5 50.0 89.6 96.0 69.5 78.3 91.1 89.8
MVDM-POS 3 65.9 77.4 26.7 66.7 91.8 96.6 74.1 81.6 92.3 91.5
MVDM-POS 5 63.7 76.6 37.0 66.7 92.1 96.4 71.4 79.8 92.1 91.5
MVDM-POS 7 65.2 77.2 41.7 66.7 92.0 96.3 70.7 79.6 92.0 91.5
Unpacked features 1 49.4 72.9 47.1 0 88.7 95.8 59.1 79.3 89.1 88.8
Complex features 1 58.8 75.8 0 66.7 91.0 94.7 74.6 87.8 94.3 91.3
</table>
<tableCaption confidence="0.999969">
Table 1: Results on the shared task dataset, in the top row the best performing metric is shown.
</tableCaption>
<bodyText confidence="0.999463071428572">
terns. In the absence of information about
feature relevance, this is a reasonable choice.
However, Information Theory gives us a useful
tool for measuring feature relevance (Quinlan,
1986; Quinlan, 1993). Information Gain (IG)
weighting looks at each feature in isolation, and
measures how much information it contributes
to our knowledge of the correct class label. The
Information Gain of feature f is measured by
computing the difference in uncertainty (i.e. en-
tropy) between the situations without and with
knowledge of the value of that feature. The re-
sulting IG values can then be used as weights in
equation 1.
</bodyText>
<subsectionHeader confidence="0.528215">
Modified Value Difference Metric The
</subsectionHeader>
<bodyText confidence="0.995335363636364">
Modified Value Difference Metric (mvpm) (Cost
and Salzberg, 1993) estimates the distance be-
tween two values of a feature by comparing the
class distribution of both features. Mvpm can
give good estimates if there are enough occur-
rences of the two values, but for low-frequent
values unreliable values of MVDM can occur. For
this data we can expect that this sparseness ef-
fect hinders the word features more than the
POS features.
Unpacking Features Unpacking features
implies that all feature values receive individ-
ual weights. (Van den Bosch and Zavrel, 2000)
warn that this operation forces feature weights
to be based on less observations, which could
make the weights unrealistic in view of test
data. Moreover, the k nearest neighbors can
be expected to contain less instances with a
fixed k when unpacking features; this is usu-
ally not beneficial for generalization accuracy
(Van den Bosch and Zavrel, 2000).
Complex Features In the previously dis-
cussed versions of memory-based learning, fea-
tures are treated as independent. However,
sometimes combinations of features or feature
values may be very good predictors. Since there
are many possible combinations, search strate-
gies are needed to select the best. Such strate-
gies have been developed for rule induction algo-
rithms (Clark and Niblett, 1989; Quinlan, 1993;
Cohen, 1995), and they can be used to find com-
plex features for memory-based learning as well.
We followed the following procedure:
</bodyText>
<listItem confidence="0.9945245">
1. apply Ripper (Cohen, 1995) to the training
set, and collect the set of induced rules;
2. recode the instances in the training and test
set, by setting binary features denoting the
rules that apply to them;
3. apply memory-based learning to the re-
coded training set, and classify the recoded
test set.
</listItem>
<sectionHeader confidence="0.996664" genericHeader="general terms">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.993877222222222">
In Table 2 we give an overview of the exper-
iments with different metrics and settings. In
the first block of rows we give the results of the
default setting with IBMG and with a varying k
parameter (number of nearest neighbours). We
can see that a larger k improves performance to
a certain extent.
In the second series of experiments we have
used the MVDM metric. Here, we also varied the
</bodyText>
<page confidence="0.99639">
158
</page>
<bodyText confidence="0.99984875">
value of k. We found that a larger k yielded bet-
ter results. In a variant on this series we applied
MVDM only to the POS features. As expected
this variant gave slightly better results.
In the third series we unpacked the features.
Compared to the previous experiment the re-
sults were worse. Apparently, sparseness results
in bad feature weights. This negative effect ap-
pears to have outweighted any positive effect of
informative individual features.
In the last experiments we used Ripper to
generate 390 complex features. The results are
comparable to the best TiMBL settings.
In Table 2 we give an overview of the preci-
sion, recall and Fo = 1 of one of the best scoring
setting: IB1-IG with k = 3
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="acknowledgments">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999988875">
We found in the experiments that minor im-
provements on the default settings of TiMBL
can be obtained by applying MVDM, particu-
larly to the POS tags. A larger k generally
improved accuracy to a certain extent. Un-
packing the features did not give the expected
improvement. Complex features, however, did,
and seem a promising alley to go.
</bodyText>
<sectionHeader confidence="0.998841" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99913332">
D. W. Aha, D. Kibler, and M. Albert. 1991.
Instance-based learning algorithms. Machine
Learning, 6:37-66.
S. Buchholz, W. Daelemans, and J. Veenstra. 1999.
Cascaded grammatical relation assignment. In
Proceedings of EMNLP/VLC-99, pages 239-246,
University of Maryland, USA.
P. Clark and T. Niblett. 1989. The CN2 rule induc-
tion algorithm. Machine Learning, 3:261-284.
W. W. Cohen. 1995. Fast effective rule induction.
In Proc. of the Twelfth International Conference
on Machine Learning, Lake Tahoe, California.
S. Cost and S. Salzberg. 1993. A weighted nearest
neighbour algorithm for learning with symbolic
features. Machine Learning, 10:57-78.
W. Daelemans, A. Van den Bosch, and A. Weijters.
1997. icTree: using trees for compression and
classification in lazy learning algorithms. Artifi-
cial Intelligence Review, 11:407-423.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2000. TiMBL: Tilburg Mem-
ory Based Learner, version 3.0, reference manual.
Tech. Rep. ILK-0001, ILK, Tilburg University.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
</reference>
<table confidence="0.9991015">
test data precision recall F0=1
ADJP 68.73% 63.24% 65.87
ADVP 79.85% 75.06% 77.38
CONJP 19.05% 44.44% 26.67
INTJ 100.00% 50.00% 66.67
LST 0.00% 0.00% 0.00
NP 90.69% 92.86% 91.76
PP 96.00% 97.19% 96.59
PRT 80.22% 68.87% 74.11
SBAR 85.77% 77.76% 81.57
VP 91.86% 92.74% 92.30
all 91.05% 92.03% 91.54
</table>
<tableCaption confidence="0.915263">
Table 2: Overview of the precision, recall and
Fo = 1 of IB1-IG with k = 3 and MVDM.
</tableCaption>
<reference confidence="0.995962076923077">
glish: The penn treebank. Computational Lin-
guistics, 19(2):313-330.
J.R. Quinlan. 1986. Induction of Decision Trees.
Machine Learning, 1:81-206.
J.R. Quinlan. 1993. c4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
C. Stanfill and D. Waltz. 1986. Toward memory-
based reasoning. Communications of the ACM,
29(12):1213-1228, December.
A. Van den Bosch and J Zavrel. 2000. Un-
packing multi-valued features and classes in
memory-based language learning. In Proceedings
of ICML2000, Stanford University, CA, USA.
</reference>
<page confidence="0.99884">
159
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.757372">
<note confidence="0.96195">of CoNLL-2000 and LLL-2000, 157-159, Lisbon, Portugal, 2000.</note>
<title confidence="0.997585">Single-Classifier Memory-Based Phrase Chunking</title>
<author confidence="0.998574">Veenstra van_den</author>
<affiliation confidence="0.792721">Linguistics, Tilburg</affiliation>
<address confidence="0.985193">Iveenstra, antalblOkub.n1</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="3027" citStr="Aha et al., 1991" startWordPosition="488" endWordPosition="491">t particular feature-value vector. After the instance base is built, new (test) instances are classified by matching them to all instances in the instance base, and by calculating with each match the distance between the new instance X and the memory instance Y. The most basic metric for patterns with symbolic features is the Overlap metric given in equation 1; where A(X, Y) is the distance between patterns X and Y, represented by n features, wi is a weight for feature i, and 6 is the distance per feature. The k-NN algorithm with this metric, and equal weighting for all features is called ml (Aha et al., 1991). Usually k is set to 1. A(X, Y) = E wi 6(xi,yi) (1) where: 6(xi, yi) = 0 if xi = yi, else 1 This distance metric simply counts the number of (mis)matching feature values in both pat157 method k AdjP AdvP ConjP Intj NP PP PRT SBAR VP tot IB1-IG 1 60.9 75.3 17.6 66.7 91.0 95.9 65.4 78.2 91.6 90.5 IB1-IG 3 64.3 76.8 38.5 66.7 91.5 95.8 61.4 79.6 91.7 91.0 IB1-IG 5 65.4 76.5 38.5 66.7 91.6 95.8 63.7 78.6 91.6 91.0 IB1-IG 7 66.1 76.5 41.7 66.7 91.2 95.4 63.7 76.8 91.4 90.7 MVDM-all 1 58.3 73.9 34.5 0 90.2 95.6 54.6 78.1 89.7 89.6 MVDM-all 3 60.0 77.0 30.8 0 91.0 96.2 60.6 80.0 91.6 90.9 MVDM-all 5</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-based learning algorithms. Machine Learning, 6:37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>W Daelemans</author>
<author>J Veenstra</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC-99,</booktitle>
<pages>239--246</pages>
<institution>University of Maryland, USA.</institution>
<contexts>
<context position="1759" citStr="Buchholz et al., 1999" startWordPosition="273" endWordPosition="276">(Section 2), which should work well on word value pairs with a medium or high frequency, but may work badly on word value pairs with low frequency. 3. The straightforward unpacking of feature values into binary features. On some tasks we have found that splitting multi-valued features into several binary features can enhance performance of the classifier. 4. A heuristic search for complex features on the basis of all unpacked feature values, and using these complex features for the classification task. 2 Methods and Data The data used for this shared task is comparable to the dataset used in (Buchholz et al., 1999), who found an optimal windowing context size of five words and POS tags to the left, the word itself, and three words and POS tags to the right. We also used this window size, and have applied TiMBL to the shared task data using default TiMBL settings. TiMBL and the abovementioned feature metrics are introduced in the following sections. IB1-IG The default TiMBL setting, iBl-IG, (Daelemans et al., 1997) is a memory-based learning algorithm that builds a database of instances (the instance base) during learning. An instance consists of a fixed-length vector of n feature-value pairs, and an inf</context>
</contexts>
<marker>Buchholz, Daelemans, Veenstra, 1999</marker>
<rawString>S. Buchholz, W. Daelemans, and J. Veenstra. 1999. Cascaded grammatical relation assignment. In Proceedings of EMNLP/VLC-99, pages 239-246, University of Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clark</author>
<author>T Niblett</author>
</authors>
<date>1989</date>
<booktitle>The CN2 rule induction algorithm. Machine Learning,</booktitle>
<pages>3--261</pages>
<contexts>
<context position="6169" citStr="Clark and Niblett, 1989" startWordPosition="1041" endWordPosition="1044"> in view of test data. Moreover, the k nearest neighbors can be expected to contain less instances with a fixed k when unpacking features; this is usually not beneficial for generalization accuracy (Van den Bosch and Zavrel, 2000). Complex Features In the previously discussed versions of memory-based learning, features are treated as independent. However, sometimes combinations of features or feature values may be very good predictors. Since there are many possible combinations, search strategies are needed to select the best. Such strategies have been developed for rule induction algorithms (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995), and they can be used to find complex features for memory-based learning as well. We followed the following procedure: 1. apply Ripper (Cohen, 1995) to the training set, and collect the set of induced rules; 2. recode the instances in the training and test set, by setting binary features denoting the rules that apply to them; 3. apply memory-based learning to the recoded training set, and classify the recoded test set. 3 Experiments and Results In Table 2 we give an overview of the experiments with different metrics and settings. In the first block of rows we give</context>
</contexts>
<marker>Clark, Niblett, 1989</marker>
<rawString>P. Clark and T. Niblett. 1989. The CN2 rule induction algorithm. Machine Learning, 3:261-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proc. of the Twelfth International Conference on Machine Learning,</booktitle>
<location>Lake Tahoe, California.</location>
<contexts>
<context position="6198" citStr="Cohen, 1995" startWordPosition="1047" endWordPosition="1048">earest neighbors can be expected to contain less instances with a fixed k when unpacking features; this is usually not beneficial for generalization accuracy (Van den Bosch and Zavrel, 2000). Complex Features In the previously discussed versions of memory-based learning, features are treated as independent. However, sometimes combinations of features or feature values may be very good predictors. Since there are many possible combinations, search strategies are needed to select the best. Such strategies have been developed for rule induction algorithms (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995), and they can be used to find complex features for memory-based learning as well. We followed the following procedure: 1. apply Ripper (Cohen, 1995) to the training set, and collect the set of induced rules; 2. recode the instances in the training and test set, by setting binary features denoting the rules that apply to them; 3. apply memory-based learning to the recoded training set, and classify the recoded test set. 3 Experiments and Results In Table 2 we give an overview of the experiments with different metrics and settings. In the first block of rows we give the results of the default s</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. W. Cohen. 1995. Fast effective rule induction. In Proc. of the Twelfth International Conference on Machine Learning, Lake Tahoe, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cost</author>
<author>S Salzberg</author>
</authors>
<title>A weighted nearest neighbour algorithm for learning with symbolic features.</title>
<date>1993</date>
<booktitle>Machine Learning,</booktitle>
<pages>10--57</pages>
<contexts>
<context position="1136" citStr="Cost and Salzberg, 1993" startWordPosition="167" endWordPosition="170">ures have more than 10,000 values. In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features. We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a metalearning classifier architecture that could be expected to improve accuracy. Given this restriction we have explored the following: 1. The generalization accuracy of TiMBL with default settings (multi-valued features, overlap metric, feature weighting). 2. The usage of MVDM (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) (Section 2), which should work well on word value pairs with a medium or high frequency, but may work badly on word value pairs with low frequency. 3. The straightforward unpacking of feature values into binary features. On some tasks we have found that splitting multi-valued features into several binary features can enhance performance of the classifier. 4. A heuristic search for complex features on the basis of all unpacked feature values, and using these complex features for the classification task. 2 Methods and Data The data used for this shared task is comparable to the dataset used in </context>
<context position="4928" citStr="Cost and Salzberg, 1993" startWordPosition="841" endWordPosition="844">r, Information Theory gives us a useful tool for measuring feature relevance (Quinlan, 1986; Quinlan, 1993). Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f is measured by computing the difference in uncertainty (i.e. entropy) between the situations without and with knowledge of the value of that feature. The resulting IG values can then be used as weights in equation 1. Modified Value Difference Metric The Modified Value Difference Metric (mvpm) (Cost and Salzberg, 1993) estimates the distance between two values of a feature by comparing the class distribution of both features. Mvpm can give good estimates if there are enough occurrences of the two values, but for low-frequent values unreliable values of MVDM can occur. For this data we can expect that this sparseness effect hinders the word features more than the POS features. Unpacking Features Unpacking features implies that all feature values receive individual weights. (Van den Bosch and Zavrel, 2000) warn that this operation forces feature weights to be based on less observations, which could make the w</context>
</contexts>
<marker>Cost, Salzberg, 1993</marker>
<rawString>S. Cost and S. Salzberg. 1993. A weighted nearest neighbour algorithm for learning with symbolic features. Machine Learning, 10:57-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>A Weijters</author>
</authors>
<title>icTree: using trees for compression and classification in lazy learning algorithms.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<pages>11--407</pages>
<marker>Daelemans, Van den Bosch, Weijters, 1997</marker>
<rawString>W. Daelemans, A. Van den Bosch, and A. Weijters. 1997. icTree: using trees for compression and classification in lazy learning algorithms. Artificial Intelligence Review, 11:407-423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual.</title>
<date>2000</date>
<tech>Tech. Rep. ILK-0001,</tech>
<institution>ILK, Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2000</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2000. TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual. Tech. Rep. ILK-0001, ILK, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of Decision Trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--81</pages>
<contexts>
<context position="4395" citStr="Quinlan, 1986" startWordPosition="758" endWordPosition="759">3 91.1 89.8 MVDM-POS 3 65.9 77.4 26.7 66.7 91.8 96.6 74.1 81.6 92.3 91.5 MVDM-POS 5 63.7 76.6 37.0 66.7 92.1 96.4 71.4 79.8 92.1 91.5 MVDM-POS 7 65.2 77.2 41.7 66.7 92.0 96.3 70.7 79.6 92.0 91.5 Unpacked features 1 49.4 72.9 47.1 0 88.7 95.8 59.1 79.3 89.1 88.8 Complex features 1 58.8 75.8 0 66.7 91.0 94.7 74.6 87.8 94.3 91.3 Table 1: Results on the shared task dataset, in the top row the best performing metric is shown. terns. In the absence of information about feature relevance, this is a reasonable choice. However, Information Theory gives us a useful tool for measuring feature relevance (Quinlan, 1986; Quinlan, 1993). Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f is measured by computing the difference in uncertainty (i.e. entropy) between the situations without and with knowledge of the value of that feature. The resulting IG values can then be used as weights in equation 1. Modified Value Difference Metric The Modified Value Difference Metric (mvpm) (Cost and Salzberg, 1993) estimates the distance between two values of a feature by comparin</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J.R. Quinlan. 1986. Induction of Decision Trees. Machine Learning, 1:81-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>c4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="4411" citStr="Quinlan, 1993" startWordPosition="760" endWordPosition="761">M-POS 3 65.9 77.4 26.7 66.7 91.8 96.6 74.1 81.6 92.3 91.5 MVDM-POS 5 63.7 76.6 37.0 66.7 92.1 96.4 71.4 79.8 92.1 91.5 MVDM-POS 7 65.2 77.2 41.7 66.7 92.0 96.3 70.7 79.6 92.0 91.5 Unpacked features 1 49.4 72.9 47.1 0 88.7 95.8 59.1 79.3 89.1 88.8 Complex features 1 58.8 75.8 0 66.7 91.0 94.7 74.6 87.8 94.3 91.3 Table 1: Results on the shared task dataset, in the top row the best performing metric is shown. terns. In the absence of information about feature relevance, this is a reasonable choice. However, Information Theory gives us a useful tool for measuring feature relevance (Quinlan, 1986; Quinlan, 1993). Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f is measured by computing the difference in uncertainty (i.e. entropy) between the situations without and with knowledge of the value of that feature. The resulting IG values can then be used as weights in equation 1. Modified Value Difference Metric The Modified Value Difference Metric (mvpm) (Cost and Salzberg, 1993) estimates the distance between two values of a feature by comparing the class dist</context>
<context position="6184" citStr="Quinlan, 1993" startWordPosition="1045" endWordPosition="1046">reover, the k nearest neighbors can be expected to contain less instances with a fixed k when unpacking features; this is usually not beneficial for generalization accuracy (Van den Bosch and Zavrel, 2000). Complex Features In the previously discussed versions of memory-based learning, features are treated as independent. However, sometimes combinations of features or feature values may be very good predictors. Since there are many possible combinations, search strategies are needed to select the best. Such strategies have been developed for rule induction algorithms (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995), and they can be used to find complex features for memory-based learning as well. We followed the following procedure: 1. apply Ripper (Cohen, 1995) to the training set, and collect the set of induced rules; 2. recode the instances in the training and test set, by setting binary features denoting the rules that apply to them; 3. apply memory-based learning to the recoded training set, and classify the recoded test set. 3 Experiments and Results In Table 2 we give an overview of the experiments with different metrics and settings. In the first block of rows we give the results of</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan. 1993. c4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stanfill</author>
<author>D Waltz</author>
</authors>
<title>Toward memorybased reasoning.</title>
<date>1986</date>
<journal>Communications of the ACM,</journal>
<pages>29--12</pages>
<contexts>
<context position="1110" citStr="Stanfill and Waltz, 1986" startWordPosition="163" endWordPosition="166">t 45 values, the word features have more than 10,000 values. In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features. We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a metalearning classifier architecture that could be expected to improve accuracy. Given this restriction we have explored the following: 1. The generalization accuracy of TiMBL with default settings (multi-valued features, overlap metric, feature weighting). 2. The usage of MVDM (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) (Section 2), which should work well on word value pairs with a medium or high frequency, but may work badly on word value pairs with low frequency. 3. The straightforward unpacking of feature values into binary features. On some tasks we have found that splitting multi-valued features into several binary features can enhance performance of the classifier. 4. A heuristic search for complex features on the basis of all unpacked feature values, and using these complex features for the classification task. 2 Methods and Data The data used for this shared task is comparab</context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>C. Stanfill and D. Waltz. 1986. Toward memorybased reasoning. Communications of the ACM, 29(12):1213-1228, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Unpacking multi-valued features and classes in memory-based language learning.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML2000,</booktitle>
<location>Stanford University, CA, USA.</location>
<marker>Van den Bosch, Zavrel, 2000</marker>
<rawString>A. Van den Bosch and J Zavrel. 2000. Unpacking multi-valued features and classes in memory-based language learning. In Proceedings of ICML2000, Stanford University, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>