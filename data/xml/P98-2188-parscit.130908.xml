<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017973">
<title confidence="0.993777">
Dialogue Act Tagging with Transformation-Based Learning
</title>
<author confidence="0.99851">
Ken Samuel and Sandra Carberry and K. Vijay-Shanker
</author>
<affiliation confidence="0.9968715">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.7521885">
Newark, Delaware 19716 USA
{ samuel,carberry,vij ay} ©cis. udel. edu
</address>
<email confidence="0.781712">
http: //www. eecis. udel. edur {samuel,carberry,vij ay} /
</email>
<sectionHeader confidence="0.941579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867090909091">
For the task of recognizing dialogue acts, we are
applying the Transformation-Based Learning
(TBL) machine learning algorithm. To circum-
vent a sparse data problem, we extract values
of well-motivated features of utterances, such
as speaker direction, punctuation marks, and a
new feature, called dialogue act cues, which we
find to be more effective than cue phrases and
word n-grams in practice. We present strate-
gies for constructing a set of dialogue act cues
automatically by minimizing the entropy of the
distribution of dialogue acts in a training cor-
pus, filtering out irrelevant dialogue act cues,
and clustering semantically-related words. In
addition, to address limitations of TBL, we in-
troduce a Monte Carlo strategy for training ef-
ficiently and a committee method for comput-
ing confidence measures. These ideas are com-
bined in our working implementation, which la-
bels held-out data as accurately as any other
reported system for the dialogue act tagging
task.
</bodyText>
<sectionHeader confidence="0.961225" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999518347826087">
Although machine learning approaches have
achieved success in many areas of Natural Lan-
guage Processing, researchers have only recently
begun to investigate applying machine learn-
ing methods to discourse-level problems (Re-
ithinger and Klesen, 1997; Di Eugenio et al.,
1997; Wiebe et al., 1997; Andernach, 1996; Lit-
man, 1994). An important task in discourse
understanding is to interpret an utterance&apos;s di-
alogue act, which is a concise abstraction of a
speaker&apos;s intention, such as SUGGEST and AC-
CEPT. Recognizing dialogue acts is critical for
discourse-level understanding and can also be
useful for other applications, such as resolving
ambiguity in speech recognition. However, com-
puting dialogue acts is a challenging task, be-
cause often a dialogue act cannot be directly
inferred from a literal interpretation of an ut-
terance.
We have investigated applying Transforma-
tion-Based Learning (TBL) to the task of com-
puting dialogue acts. This method, which has
not been used previously in discourse, has a
number of attractive characteristics for our task.
However, it also has some limitations, which we
address with a Monte Carlo strategy that sig-
nificantly improves the training time efficiency
without compromising accuracy and a commit-
tee method that enables TBL to compute con-
fidence measures for the dialogue acts assigned
to utterances.
Our machine learning algorithm makes use
of abstract features extracted from utterances.
In addition, we utilize an entropy-minimization
approach to automatically identify dialogue act
cues, which are words and short phrases that
serve as signals for dialogue acts. Our experi-
ments demonstrate that dialogue act cues tend
to be more effective than cue phrases and word
n-grams, and this strategy can be further im-
proved by adding a filtering mechanism and a
semantic-clustering method. Although we still
plan to implement more modifications, our sys-
tem has already achieved success rates compa-
rable to the best reported results for computing
dialogue acts.
</bodyText>
<subsectionHeader confidence="0.821881">
Transformation-Based Learning
</subsectionHeader>
<bodyText confidence="0.99996475">
To compute dialogue acts, we are using a mod-
ified version of Brill&apos;s (1995a) Transformation-
Based Learning method. Given a tagged train-
ing corpus, TBL develops a learned model that
consists of a sequence of rules. For example, in
one experiment, our system produced 213 rules;
the first five rules are presented in Figure 1. To
label a new corpus of dialogues with dialogue
</bodyText>
<page confidence="0.984597">
1150
</page>
<bodyText confidence="0.999746083333333">
acts, the rules are applied, in turn, to every ut-
terance in the corpus, and each utterance that
satisfies the conditions of a rule is relabeled with
that rule&apos;s new tag. For example, the first rule
in Figure 1 labels every utterance with the tag
SUGGEST. Then, after the second, third, and
fourth rules are applied, the fifth rule changes
an utterance&apos;s tag to REJECT if it includes the
word &amp;quot;no&amp;quot;, and the preceding utterance is cur-
rently tagged SUGGEST. Note that an utter-
ance&apos;s tag may change several times as the dif-
ferent rules in the sequence are applied.
</bodyText>
<table confidence="0.952745375">
# Condition(s) New Tag
1 none SUGGEST
2 Includes &amp;quot;see&amp;quot; &amp; &amp;quot;you&amp;quot; BYE
3 Includes &amp;quot;sounds&amp;quot; ACCEPT
4 Length &lt; 4 words GREET
Prec. tag is nonel
5 Includes &amp;quot;no&amp;quot; REJECT
Prec. tag is SUGGEST
</table>
<figureCaption confidence="0.997623">
Figure 1: Rules produced by the system
</figureCaption>
<bodyText confidence="0.999888444444444">
To develop a sequence of rules from a tagged
training corpus, TBL attempts to produce rules
that will correctly label many of the utterances
in the training data. The system first gener-
ates all of the potential rules that would make
at least one label in the training corpus correct.
For each potential rule, its improvement score is
defined to be the number of correct tags in the
training corpus after the rule is applied minus
the number of correct tags in the training cor-
pus before the rule is applied. The potential rule
with the highest improvement score is applied
to the entire training corpus and output as the
next rule in the learned model. This process re-
peats (using the new tags assigned to utterances
in the training corpus), producing one rule for
each pass through the training data, until no
rule can be found with an improvement score
that surpasses some predefined threshold, O.
Since there are potentially an infinite number
of rules that could produce the dialogue acts
in the training data, it is necessary to restrict
the range of patterns that the system can
consider by providing a set of rule templates.
The system replaces variables in the templates
with appropriate values to generate rules.
For example, the following template can be
</bodyText>
<footnote confidence="0.9869635">
1This condition is true only for the first utterance of
a dialogue.
</footnote>
<figureCaption confidence="0.901092">
instantiated with y= &amp;quot;no&amp;quot;, X=SUGGEST,
and Y=REJECT to produce the last rule in
Figure 1.
</figureCaption>
<bodyText confidence="0.988087909090909">
IF utterance u contains the word w
AND the tag on the utterance preceding u is X
THEN change u&apos;s tag to Y
We have observed that TBL has a number
of attractive characteristics for the task of com-
puting dialogue acts. TBL has been effective on
a similar2 task, Part-of-Speech Tagging (Brill,
1995a). Also, TBL&apos;s rules are relatively intu-
itive, so a human can analyze the rules to deter-
mine what the system has learned and perhaps
develop a theory. TBL is very good at discard-
ing irrelevant rules, because the effect of irrel-
evant rules on a training corpus is essentially
random, resulting in low improvement scores.
In addition, our implementation can accommo-
date a wide variety of different types of features,
including set-valued features, features that con-
sider the context of surrounding utterances, and
features that can take distant context into ac-
count. These and other attractive characteris-
tics of TBL are discussed further in Samuel et
al. (1998b).
</bodyText>
<subsectionHeader confidence="0.951602">
Dialogue Act Tagging
</subsectionHeader>
<bodyText confidence="0.8948442">
To address a significant concern in machine
learning, called the sparse data problem, we
must select an appropriate set of features. Re-
searchers in discourse, such as Grosz and Sidner
(1986), Lambert (1993), Hirschberg and Litman
(1993), Chen (1995), Andernach (1996), Samuel
(1996), and Chu-Carroll (1998) have suggested
several features that might be relevant for the
task of computing dialogue acts. Our system
can consider the following features of an ut-
terance: 1) the cue phrases3 in the utterance;
2) the word n-grams3 in the utterance; 3) the
dialogue act cues3 in the utterance; 4) the en-
tire utterance for one-, two-, or three-word ut-
terances; 5) speaker information4 for the utter-
2The part-of-speech tag of a word is dependent on the
word&apos;s internal features and on the surrounding words;
similarly, the dialogue act of an utterance is dependent
on the utterance&apos;s internal features and on the surround-
ing utterances.
</bodyText>
<footnote confidence="0.97560175">
3This feature is defined later in this section.
41n our system, we are handling speaker information
differently from the previous research. For example, Rei-
thinger and Klesen (1997) combine the speaker direction
</footnote>
<page confidence="0.992278">
1151
</page>
<bodyText confidence="0.997650935064935">
ance; 6) the punctuation marks found in the
utterance; 7) the number of words in the ut-
terance; 8) the dialogue acts on the preceding
utterances; and 9) the dialogue acts on the fol-
lowing5 utterances. Other features that we still
plan to implement include: 10) surface speech
acts, to represent the syntactic structure of the
utterance in an abstract format; 11) the focus-
ing information, specifying which preceding ut-
terance should be considered the most salient
when interpreting the current utterance; 12) the
type of the subject of the utterance; and 13) the
type of the main verb of the utterance.
Like other researchers, we recognize that
the specific word substrings (words and short
phrases) in an utterance can provide impor-
tant clues for discourse processing, so we should
utilize a feature that captures this informa-
tion. Hirschberg and Litman (1993) and Knott
(1996) have identified sets of cue phrases. Un-
fortunately, we have found that these manually-
selected sets of cue phrases are insufficient for
our task, as they were motivated by different
domains and tasks, and these sets may be in-
complete.
Reithinger and Klesen (1997) utilized word
n-grams, which are all of the word substrings
(with a reasonable bound on the length) in the
training corpus. However, although TBL is ca-
pable of discarding irrelevant rules, if it is bom-
barded by an overwhelming number of irrele-
vant rules, performance may begin to suffer.
This is because the improvement scores of ir-
relevant rules are random, so if the system gen-
erates too many of these rules, some of their
scores might, by chance, be high enough for se-
lection in the final model, where they can affect
performance on new data.
As a happy medium between the two ex-
with the dialogue act to make act-speaker pairs, such as
&lt;SUGGEST,A-4B&gt; and &lt;REJECT,B--*A&gt;. But we
believe it is more effective to use the change of speaker
feature, which is defined to be false if the speaker of the
current utterance is the same as the speaker of the im-
mediately preceding utterance, and true otherwise.
6If the system is participating in the dialogue, rather
than simply listening, the future context may not always
be available. But for an utterance that is in the middle
of a speaker&apos;s turn, it is reasonable to consider the subse-
quent utterances within that same turn. And also, when
utterances from the later turns do become available, it
may be important to use this information to re-evaluate
any dialogue acts that were computed and determine if
the system might have misunderstood.
tremes of using a small set of hand-picked cue
phrases and considering the complete set of
word n-grams, we are automating the analy-
sis of the training corpus to determine which
word substrings are relevant. We introduce a
new feature called dialogue act cues: word sub-
strings that appear frequently in dialogue and
provide useful clues to help determine the ap-
propriate dialogue acts. To collect dialogue act
cues automatically from a training corpus, our
strategy is to select word substrings of one, two,
or three words to minimize the entropy of the
distribution of dialogue acts given a substring.
A substring is selected if the dialogue acts co-
occurring with it have a sufficiently low entropy,
discarding sparse data. Specifically,
c def. {SES I H(DIS) &lt;01 A #(s)&gt;02}
where C is the set of dialogue act cues, S
is the set of word substrings, D is the set
of dialogue acts, 01 and 02 are predefined
thresholds, #(x) is the number of times an
event, x, occurs in the training corpus, and
entropy6 is defined in the standard way:7
</bodyText>
<equation confidence="0.940802">
def
H(DJs) = — EdED P(d1s) log2 P(dis).
</equation>
<bodyText confidence="0.999675444444445">
The desirable dialogue act cues produced by
our experiments can be organized into three cat-
egories. Traditional cues are those cue phrases
that have previously been reported in the lit-
erature, such as &amp;quot;but&amp;quot; and &amp;quot;so&amp;quot;; potential cues
consist of other useful word substrings that have
not been considered, such as &amp;quot;thanks&amp;quot; and &amp;quot;see
you&amp;quot;; and for dialogues from a particular do-
main, there may be domain cues— for example,
the appointment-scheduling corpora have dia-
logue act cues, such as &amp;quot;what time&amp;quot; and &amp;quot;busy&amp;quot;.
Dialogue act cues in the first two categories
can be utilized for learning general rules that
should apply across domains, while the third
category constitutes information that can fine-
tune a model for a particular domain.
But this method is not sufficiently restrictive;
it selects many word substrings that do not sig-
</bodyText>
<footnote confidence="0.981462777777778">
6The entropy is capturing the distribution of dialogue
acts for utterances with a given word substring. By min-
imizing entropy, we are selecting a word substring if it
produces a highly skewed distribution of the dialogue
acts, and thus, if this word substring is found in an ut-
terance, it is relatively easy to determine the proper di-
alogue act.
7In practice, we estimate the probabilities with:
P(clis)
</footnote>
<page confidence="0.865671">
1152
</page>
<table confidence="0.991307285714286">
Category # Examples
Traditional cues 56 &amp;quot;and&amp;quot;, &amp;quot;because&amp;quot;, &amp;quot;but&amp;quot;, &amp;quot;so&amp;quot;, &amp;quot;then&amp;quot;
Potential cues 71 &amp;quot;bye&amp;quot;, &amp;quot;how &apos;bout&amp;quot;, &amp;quot;see you&amp;quot;, &amp;quot;sounds&amp;quot;, &amp;quot;thanks&amp;quot;
Domain cues 42 &amp;quot;busy&amp;quot;, &amp;quot;meet&amp;quot;, &amp;quot;o&apos;clock&amp;quot;, &amp;quot;tomorrow&amp;quot;, &amp;quot;what time&amp;quot;
Superstring cues 690 &amp;quot;and then&amp;quot;, &amp;quot;but the&amp;quot;, &amp;quot;how &apos;bout the&amp;quot;, &amp;quot;okay I&amp;quot;, &amp;quot;so we&amp;quot;
...with filtering 472 &amp;quot;and then&amp;quot;, &amp;quot;but the&amp;quot;, &amp;quot;no I&amp;quot;, &amp;quot;okay with&amp;quot;, &amp;quot;so we&amp;quot;
Undesirable cues 170 &amp;quot;a&amp;quot;, &amp;quot;be&amp;quot;, &amp;quot;had&amp;quot;, &amp;quot;in the&amp;quot;, &amp;quot;to&amp;quot;
</table>
<figureCaption confidence="0.99362">
Figure 2: A set of dialogue act cues divided into five categories
</figureCaption>
<bodyText confidence="0.999967195652174">
nal dialogue acts. In many cases, an undesirable
dialogue act cue contains a useful dialogue act
cue as a substring, so it should be relatively easy
to eliminate. Examples of these superstring cues
include &amp;quot;but the&amp;quot; and &amp;quot;okay I&amp;quot;. We have im-
plemented a straightforward filtering function
to address this problem. If a dialogue act cue,
such as &amp;quot;how &apos;bout the&amp;quot; is subsumed by a more
general dialogue act cue with a better entropy
score, such as &amp;quot;how &apos;bout&amp;quot;, then the first di-
alogue act cue only offers redundant informa-
tion, and so it should be removed from the set
of dialogue act cues to minimize the number of
irrelevant rules that are generated. Our filter
deletes a dialogue act cue if one of its substrings
happens to be another dialogue act cue with a
better or equivalent entropy score.
Another effective heuristic is to cluster cer-
tain dialogue act cues into semantic classes,
which can collapse several potential rules into
a single rule with significantly more data sup-
porting it. For example, in the appointment-
scheduling corpora, there is a strong correla-
tion between weekdays and the SUGGEST di-
alogue act, but to express this fact, it is nec-
essary to generate five separate rules. How-
ever, if the five weekdays are combined un-
der one label, lweekdayr , then the same in-
formation can be captured by a single rule
that has five times as much data supporting
it: lweekday8&amp;quot; == SUGGEST. We have ex-
perimented with clusters, such as lweekdayr ,
Imonthr , Inumberr , lordinal-numberr ,
and &amp;quot;8proper-namer .
We collected a set of dialogue act cues,
clustering words in six semantic classes, with
01 = H(T) (the entropy of the dialogue acts)
and 02 = 6. As shown in Figure 2, these dia-
logue act cues were distributed among the four
categories described above, with an additional
category for the remaining undesirable cues.
Note that our simple filtering technique success-
fully eliminated 218 of the superstring cues. We
plan to investigate more sophisticated filtering
approaches to target the remaining 472 super-
string cues.
</bodyText>
<subsectionHeader confidence="0.91442">
Limitations of TBL
</subsectionHeader>
<bodyText confidence="0.999916">
Although we have argued for the use of
Transformation-Based Learning for dialogue act
tagging, we have discovered a significant limita-
tion of the algorithm: The rule templates used
by TBL must be developed by a human, in ad-
vance. Since the omission of any relevant tem-
plates would handicap the system, it is essential
that these choices be made carefully. But, in di-
alogue act tagging, nobody knows exactly which
features and feature interactions are relevant, so
we would prefer to err on the side of caution by
constructing an overly-general set of templates,
allowing the system to learn which templates
are effective. Unfortunately, in training, TBL
must generate all of the potential rules for each
utterance during each pass through the train-
ing data, and our experimental results indicate
that it is necessary to severely limit the number
of potential rules that may be generated, or the
memory and time costs are so exorbitant that
the method becomes intractable.
Our solution to this problem is to implement
a Monte Carlo version of TBL to relax the re-
striction that TBL must perform an exhaus-
tive search. In a given pass through the train-
ing data, for each utterance that is incorrectly
tagged, only R of the possible template instan-
tiations are randomly selected, where R is a pa-
rameter that is set in advance. As long as R
is large enough, there doesn&apos;t appear to be any
significant degradation in performance. We be-
lieve that this is because the best rules tend
to be effective for many different utterances, so
there are many opportunities to find these rules
during training; the better a rule is, the more
likely it is to be generated. So, although ran-
</bodyText>
<page confidence="0.975192">
1153
</page>
<bodyText confidence="0.94977687037037">
dom sampling will miss many rules, it is still
highly likely to find the best rules.
Experimental tests show that this extension
enables the system to efficiently and effectively
consider a large number of potential rules. This
increases the applicability of the TBL method
to tasks where the relevant features and feature
interactions are not known in advance as well
as tasks where there are many relevant features
and feature interactions. In addition, it is no
longer critical that the human developer iden-
tify a minimal set of templates, and so this im-
provement decreases the labor demands on the
human developer.
Unlike probabilistic machine learning ap-
proaches, TBL fails to offer any measure of con-
fidence in the tags that it produces. Confidence
measures are useful in a wide variety of ways;
for example, we foresee that our module for tag-
ging dialogue acts can potentially be integrated
into a larger system so that, when TBL cannot
produce a tag with high confidence, other mod-
ules may be invoked to provide more evidence.
Unfortunately, due to the nature of the TBL
method, straightforward approaches for track-
ing the confidence of a rule during training have
been unsuccessful. To address this problem,
we are using the Committee-Based Sampling
method (Dagan and Engelson, 1995) and the
Boosting method (Freund and Schapire, 1996)
in a novel way: The system is trained multi-
ple times, to produce a few different but rea-
sonable models for the training data.8 To con-
struct these models, we adopted the strategy
introduced in the Boosting method, by biasing
the later models to focus on those utterances (in
the training set) that the earlier models tagged
incorrectly. Then, given new data, each model
independently tags the input, and the responses
are compared. A given tag&apos;s confidence measure
is based on how well the different models agree
on that tag. Our preliminary results with five
models show that this strategy produces use-
ful confidence measures — for nearly half of the
utterances, all five models agreed on the tag,
and over 90% of those tags were correct. In
addition, the overall accuracy of our system in-
8With the efficiencies introduced by our use of fea-
tures, dialogue act cue selection, and the Monte Carlo
approach, we can implement modifications that require
multiple executions of the algorithm, which would be in-
feasible otherwise.
creased significantly. More details on this work
are presented in Samuel et al. (1998b).
</bodyText>
<subsectionHeader confidence="0.926892">
Experimental Results
</subsectionHeader>
<bodyText confidence="0.999880142857143">
A survey of the other research projects that
have applied machine learning methods to the
dialogue act tagging task is presented in Samuel
et al. (1998a). The highest success rate was re-
ported by Reithinger and Klesen (1997), whose
system could correctly label 74.7% of the utter-
ances in a test corpus. Their work utilized an
N-Grams approach, in which an utterance&apos;s di-
alogue act was based on substrings of words as
well as the dialogue acts and speaker informa-
tion from the preceding two utterances. Vari-
ous probabilities were estimated from a training
corpus by counting the frequencies of specific
events, such as the number of times that each
pair of consecutive words co-occurred with each
dialogue act.
As a direct comparison, we applied our sys-
tem to Reithinger and Klesen&apos;s training set (143
dialogues, 2701 utterances) and disjoint testing
set (20 dialogues, 328 utterances), which consist
of utterances labeled with 18 different dialogue
acts. Using semantic clustering, e = 1 (the im-
provement score threshold), R = 14 (the Monte
Carlo sample size), a set of dialogue act cues,
change of speaker, the dialogue act on the pre-
ceding utterance, and other features, our sys-
tem achieved an average accuracy score over
five9 runs of 75.12% (a-=1.34%), including a
high score of 77.44%. We have also run di-
rect comparisons between our system and Deci-
sion Trees, determining that our system&apos;s per-
formance is also comparable to this popular ma-
chine learning method (Samuel et al., 1998b).
Figure 3 presents a series of experiments
which vary the set of word substrings utilized
by the system.° Each experiment was run ten
times, and the results were compared using a
two-tailed t test to determine that all of the ac-
curacy differences were significant at the 0.05
level, except for the differences between rows 3
&amp; 4, rows 4 &amp; 5, rows 4 &amp; 6, rows 5 &amp; 6, rows
5 &amp; 7, and rows 6 &amp; 7.
</bodyText>
<footnote confidence="0.997748142857143">
9This is to factor out the random aspect of the Monte
Carlo method.
19Note that these results cannot be compared with the
results presented above, since several parameter values
differ between the two sets of experiments.
11There are only 478 different cue phrases in the set,
but for our system, it was necessary to manipulate the
</footnote>
<page confidence="0.960711">
1154
</page>
<table confidence="0.997675375">
Word Substrings Accuracy
None 41.16% (0-.0.00%)
Cue phrases (from previous literature)&amp;quot; 936 61.74% (a=0.69%)
Word n-grams 16271 69.21% (a=0.94%)
Entropy minimization 1053 69.54% (0-.1.97%)
Entropy minimization with clustering 1029 70.18% (a=0.75%)
Entropy minimization with filtering 826 70.70% (0-.1.31%)
Entropy minimization with filtering and clustering 811 71.22% (a=1.25%)
</table>
<figureCaption confidence="0.995492">
Figure 3: Tagging accuracy on held-out data, using different sets of word substrings in training
</figureCaption>
<bodyText confidence="0.99997">
As the figure shows, when the system was re-
stricted from using any word substrings, its ac-
curacy on unseen data was only 41.16%. When
given access to all of the cue phrases proposed
in previous work,12 the accuracy rises signifi-
cantly (p &lt; 0.001) to 61.74%. But this result is
significantly lower (p &lt;0.001) than the 69.21%
accuracy produced by using all substrings of
one, two, or three words (word n-grams) in the
training data, as Reithinger and Klesen (1997)
did. And the entropy-minimization approach
with the filtering and clustering techniques pro-
duce dialogue act cues that cause the accu-
racy to rise significantly further (p = 0.003) to
71.22%.
Our experimental results show that the cue
phrases identified in the literature do not cap-
ture all of the word substrings that signal di-
alogue acts. On the other hand, the complete
set of word n-grams causes the performance of
TBL to suffer. Our dialogue act cues generate
the highest accuracy scores, using significantly
fewer word substrings than the word n-grams
approach.
</bodyText>
<sectionHeader confidence="0.998186" genericHeader="discussions">
Discussion
</sectionHeader>
<bodyText confidence="0.999768089285714">
This paper has presented the first attempt
to apply Transformation-Based Learning to
discourse-level problems. We utilized various
features of utterances to learn effectively from a
relatively small amount of data, and we have de-
veloped an entropy-minimization approach with
filtering and clustering that automatically col-
lects useful dialogue act cues from tagged train-
ing data. In addition, we have devised a Monte
data in various ways, such as including a capitalized ver-
sion of each cue phrase and splitting up contractions.
12See Hirschberg and Litman (1993) and Knott (1996)
for these lists of cue phrases. We also included 45 cue
phrases that we pinpointed by manually analyzing a
completely different set of dialogues, two years before
we began working with the VERBMOBIL corpora.
Carlo strategy and a committee method to ad-
dress some limitations of TBL. Although we
have only begun implementing our ideas, our
system has already matched Reithinger and
Klesen&apos;s success rate in computing dialogue
acts.
In the future, we plan to implement more fea-
tures, improve our method for collecting dia-
logue act cues, and investigate how these mod-
ifications improve our system&apos;s performance.
Also, for the semantic-clustering technique, we
selected the clusters of words by hand, but it
would be interesting to see how a taxonomy,
such as WordNet could be used to automate this
process.
When there is not enough tagged train-
ing data available, we would like the system
to learn from untagged data. Dagan and
Engelson&apos;s (1995) Committee-Based Sampling
method constructed multiple learned models
from a small set of tagged data, and then, only
when the models disagreed on a tag, a hu-
man was consulted for the correct tag. Brill
(1995b) developed an unsupervised version of
TBL for Part-of-Speech Tagging, but this algo-
rithm must be initialized with words that can
be tagged unambiguously,13 and in discourse,
there are very few unambiguous examples. We
intend to investigate a weakly-supervised ap-
proach that utilizes the confidence measures de-
scribed above. First, the system will be trained
on a relatively small set of tagged data, pro-
ducing a few different models. Then, given un-
tagged data, it will use the models to derive
dialogue acts with confidence measures. Those
tags that receive high confidence can be used
as unambiguous examples to drive the unsuper-
vised version of TBL.
While we contend that machine learning can
be an effective tool for identifying dialogue acts,
</bodyText>
<footnote confidence="0.817639">
13For example, &amp;quot;the&amp;quot; is always a Determiner.
</footnote>
<page confidence="0.99021">
1155
</page>
<bodyText confidence="0.9997215">
we do realize that machine learning may not be
able to completely solve this problem, as it is
unable to capture some relevant factors, such
as common-sense world knowledge. We envision
that our system may potentially be integrated
into a larger system that uses confidence mea-
sures to determine when world knowledge infor-
mation is required.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99992175">
We wish to thank the members of the VERBMO-
BIL research group at DFKI in Germany, partic-
ularly Norbert Reithinger, Jan Alexandersson,
and Elisabeth Maier, for providing us with the
opportunity to work with them and generously
granting us access to the VERBMOBIL corpora.
This work was partially supported by the NSF
Grant #GER-9354869.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995277839506173">
Toine Andernach. 1996. A machine learning ap-
proach to the classification of dialogue utter-
ances. In Proceedings of NeMLaP-2.
Eric Brill. 1995a. Transformation-based error-
driven learning and natural language process-
ing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4):543-566.
Eric Brill. 1995b. Unsupervised learning of
disambiguation rules for part of speech tag-
ging. In Proceedings of the Very Large Cor-
pora Workshop.
Kuang-Hua Chen. 1995. Topic identification
in discourse. In Proceedings of the Sev-
enth Meeting of the European Association for
Computational Linguistics, pages 267-271.
Jennifer Chu-Carroll. 1998. A statistical model
for discourse act recognition in dialogue in-
teractions. In Applying Machine Learning to
Discourse Processing: Papers from the 1998
AAAI Spring Symposium, pages 12-17. Tech-
nical Report #SS-98-01.
Ido Dagan and Sean P. Engelson. 1995.
Committee-based sampling for training prob-
abilistic classifiers. In Proceedings of the 12th
International Conference on Machine Learn-
ing, pages 150-157.
Barbara Di Eugenio, Johanna D. Moore, and
Massimo Paolucci. 1997. Learning features
that predict cue usage. In Proceedings of the
35th Annual Meeting of the ACL, pages 80-
87.
Yoav Freund and Robert E. Schapire. 1996.
Experiments with a new boosting algorithm.
In Proceedings of the Thirteenth International
Conference on Machine Learning.
Barbara Grosz and Candace Sidner. 1986.
Attention, intentions, and the structure
of discourse. Computational Linguistics,
12(3):175-204.
Julia Hirschberg and Diane Litman. 1993.
Empirical studies on the disambiguation
of cue phrases. Computational Linguistics,
19(3):501-530.
Alistair Knott. 1996. A Data-Driven Methodol-
ogy for Motivating a Set of Coherence Rela-
tions. Ph.D. thesis, University of Edinburgh.
Lynn Lambert. 1993. Recognizing Complex
Discourse Acts: A Tripartite Plan-Based
Model of Dialogue. Ph.D. thesis, The Univer-
sity of Delaware. Technical Report #93-19.
Diane J. Litman. 1994. Classifying cue phrases
in text and speech using machine learning. In
Proceedings of the 12th National Conference
on Artificial Intelligence, pages 806-813.
Norbert Reithinger and Martin Klesen. 1997.
Dialogue act classification using language
models. In Proceedings of EuroSpeech-97,
pages 2235-2238.
Ken Samuel, Sandra Carberry, and K. Vijay-
Shanker. 1998a. Computing dialogue acts
from features with transformation-based
learning. In Applying Machine Learning to
Discourse Processing: Papers from the 1998
AAAI Spring Symposium, pages 90-97. Tech-
nical Report #SS-98-01.
Ken Samuel, Sandra Carberry, and K. Vijay-
Shanker. 1998b. An investigation of
transformation-based learning in discourse.
In Machine Learning: Proceedings of the Fif-
teenth International Conference.
Kenneth B. Samuel. 1996. Using statistical
learning algorithms to compute discourse in-
formation. Technical Report #97-11, The
University of Delaware. Dissertation pro-
posal.
Janyce Wiebe, Tom O&apos;Hara, Kenneth McKee-
ver, and Thorsten Oehrstroem-Sandgren.
1997. An empirical approach to temporal ref-
erence resolution. In Proceedings of the Sec-
ond Conference on Empirical Methods in Nat-
ural Language Processing, pages 174-186.
</reference>
<page confidence="0.99441">
1156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832027">
<title confidence="0.999735">Dialogue Act Tagging with Transformation-Based Learning</title>
<author confidence="0.999579">Samuel Carberry Vijay-Shanker</author>
<affiliation confidence="0.999901">Department of Computer and Information Sciences University of Delaware</affiliation>
<address confidence="0.997311">Newark, Delaware 19716 USA</address>
<email confidence="0.894772">samuel©cis.udel.edu</email>
<email confidence="0.894772">carberry©cis.udel.edu</email>
<email confidence="0.894772">vijay©cis.udel.edu</email>
<web confidence="0.961588">http: //www. eecis. udel. edur {samuel,carberry,vij ay} /</web>
<abstract confidence="0.997967434782609">the task of recognizing acts, are applying the Transformation-Based Learning (TBL) machine learning algorithm. To circumdata problem, we extract values well-motivated features such as speaker direction, punctuation marks, and a feature, called act cues, we find to be more effective than cue phrases and word n-grams in practice. We present strategies for constructing a set of dialogue act cues automatically by minimizing the entropy of the distribution of dialogue acts in a training corfiltering out irrelevant dialogue and clustering semantically-related words. In addition, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computconfidence measures. These ideas comin working implementation, which laheld-out data as accurately other reported system for the dialogue act tagging task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Toine Andernach</author>
</authors>
<title>A machine learning approach to the classification of dialogue utterances.</title>
<date>1996</date>
<booktitle>In Proceedings of NeMLaP-2.</booktitle>
<contexts>
<context position="1610" citStr="Andernach, 1996" startWordPosition="240" endWordPosition="241">ions of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task. Introduction Although machine learning approaches have achieved success in many areas of Natural Language Processing, researchers have only recently begun to investigate applying machine learning methods to discourse-level problems (Reithinger and Klesen, 1997; Di Eugenio et al., 1997; Wiebe et al., 1997; Andernach, 1996; Litman, 1994). An important task in discourse understanding is to interpret an utterance&apos;s dialogue act, which is a concise abstraction of a speaker&apos;s intention, such as SUGGEST and ACCEPT. Recognizing dialogue acts is critical for discourse-level understanding and can also be useful for other applications, such as resolving ambiguity in speech recognition. However, computing dialogue acts is a challenging task, because often a dialogue act cannot be directly inferred from a literal interpretation of an utterance. We have investigated applying Transformation-Based Learning (TBL) to the task </context>
<context position="7160" citStr="Andernach (1996)" startWordPosition="1162" endWordPosition="1163">r implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an utterance is dependent on the utt</context>
</contexts>
<marker>Andernach, 1996</marker>
<rawString>Toine Andernach. 1996. A machine learning approach to the classification of dialogue utterances. In Proceedings of NeMLaP-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based errordriven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="6208" citStr="Brill, 1995" startWordPosition="1010" endWordPosition="1011">g a set of rule templates. The system replaces variables in the templates with appropriate values to generate rules. For example, the following template can be 1This condition is true only for the first utterance of a dialogue. instantiated with y= &amp;quot;no&amp;quot;, X=SUGGEST, and Y=REJECT to produce the last rule in Figure 1. IF utterance u contains the word w AND the tag on the utterance preceding u is X THEN change u&apos;s tag to Y We have observed that TBL has a number of attractive characteristics for the task of computing dialogue acts. TBL has been effective on a similar2 task, Part-of-Speech Tagging (Brill, 1995a). Also, TBL&apos;s rules are relatively intuitive, so a human can analyze the rules to determine what the system has learned and perhaps develop a theory. TBL is very good at discarding irrelevant rules, because the effect of irrelevant rules on a training corpus is essentially random, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive charact</context>
<context position="25036" citStr="Brill (1995" startWordPosition="4153" endWordPosition="4154"> investigate how these modifications improve our system&apos;s performance. Also, for the semantic-clustering technique, we selected the clusters of words by hand, but it would be interesting to see how a taxonomy, such as WordNet could be used to automate this process. When there is not enough tagged training data available, we would like the system to learn from untagged data. Dagan and Engelson&apos;s (1995) Committee-Based Sampling method constructed multiple learned models from a small set of tagged data, and then, only when the models disagreed on a tag, a human was consulted for the correct tag. Brill (1995b) developed an unsupervised version of TBL for Part-of-Speech Tagging, but this algorithm must be initialized with words that can be tagged unambiguously,13 and in discourse, there are very few unambiguous examples. We intend to investigate a weakly-supervised approach that utilizes the confidence measures described above. First, the system will be trained on a relatively small set of tagged data, producing a few different models. Then, given untagged data, it will use the models to derive dialogue acts with confidence measures. Those tags that receive high confidence can be used as unambiguo</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995a. Transformation-based errordriven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543-566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Unsupervised learning of disambiguation rules for part of speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the Very Large Corpora Workshop.</booktitle>
<contexts>
<context position="6208" citStr="Brill, 1995" startWordPosition="1010" endWordPosition="1011">g a set of rule templates. The system replaces variables in the templates with appropriate values to generate rules. For example, the following template can be 1This condition is true only for the first utterance of a dialogue. instantiated with y= &amp;quot;no&amp;quot;, X=SUGGEST, and Y=REJECT to produce the last rule in Figure 1. IF utterance u contains the word w AND the tag on the utterance preceding u is X THEN change u&apos;s tag to Y We have observed that TBL has a number of attractive characteristics for the task of computing dialogue acts. TBL has been effective on a similar2 task, Part-of-Speech Tagging (Brill, 1995a). Also, TBL&apos;s rules are relatively intuitive, so a human can analyze the rules to determine what the system has learned and perhaps develop a theory. TBL is very good at discarding irrelevant rules, because the effect of irrelevant rules on a training corpus is essentially random, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive charact</context>
<context position="25036" citStr="Brill (1995" startWordPosition="4153" endWordPosition="4154"> investigate how these modifications improve our system&apos;s performance. Also, for the semantic-clustering technique, we selected the clusters of words by hand, but it would be interesting to see how a taxonomy, such as WordNet could be used to automate this process. When there is not enough tagged training data available, we would like the system to learn from untagged data. Dagan and Engelson&apos;s (1995) Committee-Based Sampling method constructed multiple learned models from a small set of tagged data, and then, only when the models disagreed on a tag, a human was consulted for the correct tag. Brill (1995b) developed an unsupervised version of TBL for Part-of-Speech Tagging, but this algorithm must be initialized with words that can be tagged unambiguously,13 and in discourse, there are very few unambiguous examples. We intend to investigate a weakly-supervised approach that utilizes the confidence measures described above. First, the system will be trained on a relatively small set of tagged data, producing a few different models. Then, given untagged data, it will use the models to derive dialogue acts with confidence measures. Those tags that receive high confidence can be used as unambiguo</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995b. Unsupervised learning of disambiguation rules for part of speech tagging. In Proceedings of the Very Large Corpora Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuang-Hua Chen</author>
</authors>
<title>Topic identification in discourse.</title>
<date>1995</date>
<booktitle>In Proceedings of the Seventh Meeting of the European Association for Computational Linguistics,</booktitle>
<pages>267--271</pages>
<contexts>
<context position="7142" citStr="Chen (1995)" startWordPosition="1160" endWordPosition="1161"> addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an utterance is de</context>
</contexts>
<marker>Chen, 1995</marker>
<rawString>Kuang-Hua Chen. 1995. Topic identification in discourse. In Proceedings of the Seventh Meeting of the European Association for Computational Linguistics, pages 267-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>A statistical model for discourse act recognition in dialogue interactions.</title>
<date>1998</date>
<booktitle>In Applying Machine Learning to Discourse Processing: Papers from the 1998 AAAI Spring Symposium,</booktitle>
<tech>Technical Report #SS-98-01.</tech>
<pages>12--17</pages>
<contexts>
<context position="7199" citStr="Chu-Carroll (1998)" startWordPosition="1167" endWordPosition="1168">de variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an utterance is dependent on the utterance&apos;s internal features and on the s</context>
</contexts>
<marker>Chu-Carroll, 1998</marker>
<rawString>Jennifer Chu-Carroll. 1998. A statistical model for discourse act recognition in dialogue interactions. In Applying Machine Learning to Discourse Processing: Papers from the 1998 AAAI Spring Symposium, pages 12-17. Technical Report #SS-98-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Sean P Engelson</author>
</authors>
<title>Committee-based sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the 12th International Conference on Machine Learning,</booktitle>
<pages>150--157</pages>
<contexts>
<context position="18383" citStr="Dagan and Engelson, 1995" startWordPosition="3055" endWordPosition="3058">, TBL fails to offer any measure of confidence in the tags that it produces. Confidence measures are useful in a wide variety of ways; for example, we foresee that our module for tagging dialogue acts can potentially be integrated into a larger system so that, when TBL cannot produce a tag with high confidence, other modules may be invoked to provide more evidence. Unfortunately, due to the nature of the TBL method, straightforward approaches for tracking the confidence of a rule during training have been unsuccessful. To address this problem, we are using the Committee-Based Sampling method (Dagan and Engelson, 1995) and the Boosting method (Freund and Schapire, 1996) in a novel way: The system is trained multiple times, to produce a few different but reasonable models for the training data.8 To construct these models, we adopted the strategy introduced in the Boosting method, by biasing the later models to focus on those utterances (in the training set) that the earlier models tagged incorrectly. Then, given new data, each model independently tags the input, and the responses are compared. A given tag&apos;s confidence measure is based on how well the different models agree on that tag. Our preliminary result</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>Ido Dagan and Sean P. Engelson. 1995. Committee-based sampling for training probabilistic classifiers. In Proceedings of the 12th International Conference on Machine Learning, pages 150-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Johanna D Moore</author>
<author>Massimo Paolucci</author>
</authors>
<title>Learning features that predict cue usage.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>80--87</pages>
<marker>Di Eugenio, Moore, Paolucci, 1997</marker>
<rawString>Barbara Di Eugenio, Johanna D. Moore, and Massimo Paolucci. 1997. Learning features that predict cue usage. In Proceedings of the 35th Annual Meeting of the ACL, pages 80-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="18435" citStr="Freund and Schapire, 1996" startWordPosition="3063" endWordPosition="3066">he tags that it produces. Confidence measures are useful in a wide variety of ways; for example, we foresee that our module for tagging dialogue acts can potentially be integrated into a larger system so that, when TBL cannot produce a tag with high confidence, other modules may be invoked to provide more evidence. Unfortunately, due to the nature of the TBL method, straightforward approaches for tracking the confidence of a rule during training have been unsuccessful. To address this problem, we are using the Committee-Based Sampling method (Dagan and Engelson, 1995) and the Boosting method (Freund and Schapire, 1996) in a novel way: The system is trained multiple times, to produce a few different but reasonable models for the training data.8 To construct these models, we adopted the strategy introduced in the Boosting method, by biasing the later models to focus on those utterances (in the training set) that the earlier models tagged incorrectly. Then, given new data, each model independently tags the input, and the responses are compared. A given tag&apos;s confidence measure is based on how well the different models agree on that tag. Our preliminary results with five models show that this strategy produces </context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="7083" citStr="Grosz and Sidner (1986)" startWordPosition="1150" endWordPosition="1153">g corpus is essentially random, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surroundi</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candace Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--3</pages>
<contexts>
<context position="7129" citStr="Hirschberg and Litman (1993)" startWordPosition="1156" endWordPosition="1159"> in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an ut</context>
<context position="8900" citStr="Hirschberg and Litman (1993)" startWordPosition="1444" endWordPosition="1447">n to implement include: 10) surface speech acts, to represent the syntactic structure of the utterance in an abstract format; 11) the focusing information, specifying which preceding utterance should be considered the most salient when interpreting the current utterance; 12) the type of the subject of the utterance; and 13) the type of the main verb of the utterance. Like other researchers, we recognize that the specific word substrings (words and short phrases) in an utterance can provide important clues for discourse processing, so we should utilize a feature that captures this information. Hirschberg and Litman (1993) and Knott (1996) have identified sets of cue phrases. Unfortunately, we have found that these manuallyselected sets of cue phrases are insufficient for our task, as they were motivated by different domains and tasks, and these sets may be incomplete. Reithinger and Klesen (1997) utilized word n-grams, which are all of the word substrings (with a reasonable bound on the length) in the training corpus. However, although TBL is capable of discarding irrelevant rules, if it is bombarded by an overwhelming number of irrelevant rules, performance may begin to suffer. This is because the improvement</context>
<context position="23870" citStr="Hirschberg and Litman (1993)" startWordPosition="3958" endWordPosition="3961">fewer word substrings than the word n-grams approach. Discussion This paper has presented the first attempt to apply Transformation-Based Learning to discourse-level problems. We utilized various features of utterances to learn effectively from a relatively small amount of data, and we have developed an entropy-minimization approach with filtering and clustering that automatically collects useful dialogue act cues from tagged training data. In addition, we have devised a Monte data in various ways, such as including a capitalized version of each cue phrase and splitting up contractions. 12See Hirschberg and Litman (1993) and Knott (1996) for these lists of cue phrases. We also included 45 cue phrases that we pinpointed by manually analyzing a completely different set of dialogues, two years before we began working with the VERBMOBIL corpora. Carlo strategy and a committee method to address some limitations of TBL. Although we have only begun implementing our ideas, our system has already matched Reithinger and Klesen&apos;s success rate in computing dialogue acts. In the future, we plan to implement more features, improve our method for collecting dialogue act cues, and investigate how these modifications improve </context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A Data-Driven Methodology for Motivating a Set of Coherence Relations.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="8917" citStr="Knott (1996)" startWordPosition="1449" endWordPosition="1450">ce speech acts, to represent the syntactic structure of the utterance in an abstract format; 11) the focusing information, specifying which preceding utterance should be considered the most salient when interpreting the current utterance; 12) the type of the subject of the utterance; and 13) the type of the main verb of the utterance. Like other researchers, we recognize that the specific word substrings (words and short phrases) in an utterance can provide important clues for discourse processing, so we should utilize a feature that captures this information. Hirschberg and Litman (1993) and Knott (1996) have identified sets of cue phrases. Unfortunately, we have found that these manuallyselected sets of cue phrases are insufficient for our task, as they were motivated by different domains and tasks, and these sets may be incomplete. Reithinger and Klesen (1997) utilized word n-grams, which are all of the word substrings (with a reasonable bound on the length) in the training corpus. However, although TBL is capable of discarding irrelevant rules, if it is bombarded by an overwhelming number of irrelevant rules, performance may begin to suffer. This is because the improvement scores of irrele</context>
<context position="23887" citStr="Knott (1996)" startWordPosition="3963" endWordPosition="3964">rd n-grams approach. Discussion This paper has presented the first attempt to apply Transformation-Based Learning to discourse-level problems. We utilized various features of utterances to learn effectively from a relatively small amount of data, and we have developed an entropy-minimization approach with filtering and clustering that automatically collects useful dialogue act cues from tagged training data. In addition, we have devised a Monte data in various ways, such as including a capitalized version of each cue phrase and splitting up contractions. 12See Hirschberg and Litman (1993) and Knott (1996) for these lists of cue phrases. We also included 45 cue phrases that we pinpointed by manually analyzing a completely different set of dialogues, two years before we began working with the VERBMOBIL corpora. Carlo strategy and a committee method to address some limitations of TBL. Although we have only begun implementing our ideas, our system has already matched Reithinger and Klesen&apos;s success rate in computing dialogue acts. In the future, we plan to implement more features, improve our method for collecting dialogue act cues, and investigate how these modifications improve our system&apos;s perf</context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Alistair Knott. 1996. A Data-Driven Methodology for Motivating a Set of Coherence Relations. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
</authors>
<title>Recognizing Complex Discourse Acts: A Tripartite Plan-Based Model of Dialogue.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Delaware.</institution>
<contexts>
<context position="7099" citStr="Lambert (1993)" startWordPosition="1154" endWordPosition="1155">andom, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; simila</context>
</contexts>
<marker>Lambert, 1993</marker>
<rawString>Lynn Lambert. 1993. Recognizing Complex Discourse Acts: A Tripartite Plan-Based Model of Dialogue. Ph.D. thesis, The University of Delaware. Technical Report #93-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
</authors>
<title>Classifying cue phrases in text and speech using machine learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the 12th National Conference on Artificial Intelligence,</booktitle>
<pages>806--813</pages>
<contexts>
<context position="1625" citStr="Litman, 1994" startWordPosition="242" endWordPosition="244">ntroduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task. Introduction Although machine learning approaches have achieved success in many areas of Natural Language Processing, researchers have only recently begun to investigate applying machine learning methods to discourse-level problems (Reithinger and Klesen, 1997; Di Eugenio et al., 1997; Wiebe et al., 1997; Andernach, 1996; Litman, 1994). An important task in discourse understanding is to interpret an utterance&apos;s dialogue act, which is a concise abstraction of a speaker&apos;s intention, such as SUGGEST and ACCEPT. Recognizing dialogue acts is critical for discourse-level understanding and can also be useful for other applications, such as resolving ambiguity in speech recognition. However, computing dialogue acts is a challenging task, because often a dialogue act cannot be directly inferred from a literal interpretation of an utterance. We have investigated applying Transformation-Based Learning (TBL) to the task of computing di</context>
</contexts>
<marker>Litman, 1994</marker>
<rawString>Diane J. Litman. 1994. Classifying cue phrases in text and speech using machine learning. In Proceedings of the 12th National Conference on Artificial Intelligence, pages 806-813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Martin Klesen</author>
</authors>
<title>Dialogue act classification using language models.</title>
<date>1997</date>
<booktitle>In Proceedings of EuroSpeech-97,</booktitle>
<pages>2235--2238</pages>
<contexts>
<context position="1548" citStr="Reithinger and Klesen, 1997" startWordPosition="226" endWordPosition="230">and clustering semantically-related words. In addition, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task. Introduction Although machine learning approaches have achieved success in many areas of Natural Language Processing, researchers have only recently begun to investigate applying machine learning methods to discourse-level problems (Reithinger and Klesen, 1997; Di Eugenio et al., 1997; Wiebe et al., 1997; Andernach, 1996; Litman, 1994). An important task in discourse understanding is to interpret an utterance&apos;s dialogue act, which is a concise abstraction of a speaker&apos;s intention, such as SUGGEST and ACCEPT. Recognizing dialogue acts is critical for discourse-level understanding and can also be useful for other applications, such as resolving ambiguity in speech recognition. However, computing dialogue acts is a challenging task, because often a dialogue act cannot be directly inferred from a literal interpretation of an utterance. We have investig</context>
<context position="8003" citStr="Reithinger and Klesen (1997)" startWordPosition="1296" endWordPosition="1300">ses3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an utterance is dependent on the utterance&apos;s internal features and on the surrounding utterances. 3This feature is defined later in this section. 41n our system, we are handling speaker information differently from the previous research. For example, Reithinger and Klesen (1997) combine the speaker direction 1151 ance; 6) the punctuation marks found in the utterance; 7) the number of words in the utterance; 8) the dialogue acts on the preceding utterances; and 9) the dialogue acts on the following5 utterances. Other features that we still plan to implement include: 10) surface speech acts, to represent the syntactic structure of the utterance in an abstract format; 11) the focusing information, specifying which preceding utterance should be considered the most salient when interpreting the current utterance; 12) the type of the subject of the utterance; and 13) the t</context>
<context position="19794" citStr="Reithinger and Klesen (1997)" startWordPosition="3289" endWordPosition="3292">correct. In addition, the overall accuracy of our system in8With the efficiencies introduced by our use of features, dialogue act cue selection, and the Monte Carlo approach, we can implement modifications that require multiple executions of the algorithm, which would be infeasible otherwise. creased significantly. More details on this work are presented in Samuel et al. (1998b). Experimental Results A survey of the other research projects that have applied machine learning methods to the dialogue act tagging task is presented in Samuel et al. (1998a). The highest success rate was reported by Reithinger and Klesen (1997), whose system could correctly label 74.7% of the utterances in a test corpus. Their work utilized an N-Grams approach, in which an utterance&apos;s dialogue act was based on substrings of words as well as the dialogue acts and speaker information from the preceding two utterances. Various probabilities were estimated from a training corpus by counting the frequencies of specific events, such as the number of times that each pair of consecutive words co-occurred with each dialogue act. As a direct comparison, we applied our system to Reithinger and Klesen&apos;s training set (143 dialogues, 2701 utteran</context>
<context position="22728" citStr="Reithinger and Klesen (1997)" startWordPosition="3779" endWordPosition="3782">ropy minimization with filtering and clustering 811 71.22% (a=1.25%) Figure 3: Tagging accuracy on held-out data, using different sets of word substrings in training As the figure shows, when the system was restricted from using any word substrings, its accuracy on unseen data was only 41.16%. When given access to all of the cue phrases proposed in previous work,12 the accuracy rises significantly (p &lt; 0.001) to 61.74%. But this result is significantly lower (p &lt;0.001) than the 69.21% accuracy produced by using all substrings of one, two, or three words (word n-grams) in the training data, as Reithinger and Klesen (1997) did. And the entropy-minimization approach with the filtering and clustering techniques produce dialogue act cues that cause the accuracy to rise significantly further (p = 0.003) to 71.22%. Our experimental results show that the cue phrases identified in the literature do not capture all of the word substrings that signal dialogue acts. On the other hand, the complete set of word n-grams causes the performance of TBL to suffer. Our dialogue act cues generate the highest accuracy scores, using significantly fewer word substrings than the word n-grams approach. Discussion This paper has presen</context>
</contexts>
<marker>Reithinger, Klesen, 1997</marker>
<rawString>Norbert Reithinger and Martin Klesen. 1997. Dialogue act classification using language models. In Proceedings of EuroSpeech-97, pages 2235-2238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
<author>Sandra Carberry</author>
<author>K VijayShanker</author>
</authors>
<title>Computing dialogue acts from features with transformation-based learning.</title>
<date>1998</date>
<booktitle>In Applying Machine Learning to Discourse Processing: Papers from the</booktitle>
<tech>Technical Report #SS-98-01.</tech>
<pages>90--97</pages>
<contexts>
<context position="6868" citStr="Samuel et al. (1998" startWordPosition="1116" endWordPosition="1119">itive, so a human can analyze the rules to determine what the system has learned and perhaps develop a theory. TBL is very good at discarding irrelevant rules, because the effect of irrelevant rules on a training corpus is essentially random, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in th</context>
<context position="19545" citStr="Samuel et al. (1998" startWordPosition="3249" endWordPosition="3252">ifferent models agree on that tag. Our preliminary results with five models show that this strategy produces useful confidence measures — for nearly half of the utterances, all five models agreed on the tag, and over 90% of those tags were correct. In addition, the overall accuracy of our system in8With the efficiencies introduced by our use of features, dialogue act cue selection, and the Monte Carlo approach, we can implement modifications that require multiple executions of the algorithm, which would be infeasible otherwise. creased significantly. More details on this work are presented in Samuel et al. (1998b). Experimental Results A survey of the other research projects that have applied machine learning methods to the dialogue act tagging task is presented in Samuel et al. (1998a). The highest success rate was reported by Reithinger and Klesen (1997), whose system could correctly label 74.7% of the utterances in a test corpus. Their work utilized an N-Grams approach, in which an utterance&apos;s dialogue act was based on substrings of words as well as the dialogue acts and speaker information from the preceding two utterances. Various probabilities were estimated from a training corpus by counting t</context>
<context position="21057" citStr="Samuel et al., 1998" startWordPosition="3500" endWordPosition="3503"> 328 utterances), which consist of utterances labeled with 18 different dialogue acts. Using semantic clustering, e = 1 (the improvement score threshold), R = 14 (the Monte Carlo sample size), a set of dialogue act cues, change of speaker, the dialogue act on the preceding utterance, and other features, our system achieved an average accuracy score over five9 runs of 75.12% (a-=1.34%), including a high score of 77.44%. We have also run direct comparisons between our system and Decision Trees, determining that our system&apos;s performance is also comparable to this popular machine learning method (Samuel et al., 1998b). Figure 3 presents a series of experiments which vary the set of word substrings utilized by the system.° Each experiment was run ten times, and the results were compared using a two-tailed t test to determine that all of the accuracy differences were significant at the 0.05 level, except for the differences between rows 3 &amp; 4, rows 4 &amp; 5, rows 4 &amp; 6, rows 5 &amp; 6, rows 5 &amp; 7, and rows 6 &amp; 7. 9This is to factor out the random aspect of the Monte Carlo method. 19Note that these results cannot be compared with the results presented above, since several parameter values differ between the two se</context>
</contexts>
<marker>Samuel, Carberry, VijayShanker, 1998</marker>
<rawString>Ken Samuel, Sandra Carberry, and K. VijayShanker. 1998a. Computing dialogue acts from features with transformation-based learning. In Applying Machine Learning to Discourse Processing: Papers from the 1998 AAAI Spring Symposium, pages 90-97. Technical Report #SS-98-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
<author>Sandra Carberry</author>
<author>K VijayShanker</author>
</authors>
<title>An investigation of transformation-based learning in discourse.</title>
<date>1998</date>
<booktitle>In Machine Learning: Proceedings of the Fifteenth International Conference.</booktitle>
<contexts>
<context position="6868" citStr="Samuel et al. (1998" startWordPosition="1116" endWordPosition="1119">itive, so a human can analyze the rules to determine what the system has learned and perhaps develop a theory. TBL is very good at discarding irrelevant rules, because the effect of irrelevant rules on a training corpus is essentially random, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in th</context>
<context position="19545" citStr="Samuel et al. (1998" startWordPosition="3249" endWordPosition="3252">ifferent models agree on that tag. Our preliminary results with five models show that this strategy produces useful confidence measures — for nearly half of the utterances, all five models agreed on the tag, and over 90% of those tags were correct. In addition, the overall accuracy of our system in8With the efficiencies introduced by our use of features, dialogue act cue selection, and the Monte Carlo approach, we can implement modifications that require multiple executions of the algorithm, which would be infeasible otherwise. creased significantly. More details on this work are presented in Samuel et al. (1998b). Experimental Results A survey of the other research projects that have applied machine learning methods to the dialogue act tagging task is presented in Samuel et al. (1998a). The highest success rate was reported by Reithinger and Klesen (1997), whose system could correctly label 74.7% of the utterances in a test corpus. Their work utilized an N-Grams approach, in which an utterance&apos;s dialogue act was based on substrings of words as well as the dialogue acts and speaker information from the preceding two utterances. Various probabilities were estimated from a training corpus by counting t</context>
<context position="21057" citStr="Samuel et al., 1998" startWordPosition="3500" endWordPosition="3503"> 328 utterances), which consist of utterances labeled with 18 different dialogue acts. Using semantic clustering, e = 1 (the improvement score threshold), R = 14 (the Monte Carlo sample size), a set of dialogue act cues, change of speaker, the dialogue act on the preceding utterance, and other features, our system achieved an average accuracy score over five9 runs of 75.12% (a-=1.34%), including a high score of 77.44%. We have also run direct comparisons between our system and Decision Trees, determining that our system&apos;s performance is also comparable to this popular machine learning method (Samuel et al., 1998b). Figure 3 presents a series of experiments which vary the set of word substrings utilized by the system.° Each experiment was run ten times, and the results were compared using a two-tailed t test to determine that all of the accuracy differences were significant at the 0.05 level, except for the differences between rows 3 &amp; 4, rows 4 &amp; 5, rows 4 &amp; 6, rows 5 &amp; 6, rows 5 &amp; 7, and rows 6 &amp; 7. 9This is to factor out the random aspect of the Monte Carlo method. 19Note that these results cannot be compared with the results presented above, since several parameter values differ between the two se</context>
</contexts>
<marker>Samuel, Carberry, VijayShanker, 1998</marker>
<rawString>Ken Samuel, Sandra Carberry, and K. VijayShanker. 1998b. An investigation of transformation-based learning in discourse. In Machine Learning: Proceedings of the Fifteenth International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth B Samuel</author>
</authors>
<title>Using statistical learning algorithms to compute discourse information.</title>
<date>1996</date>
<tech>Technical Report #97-11,</tech>
<institution>The University of Delaware.</institution>
<note>Dissertation proposal.</note>
<contexts>
<context position="7175" citStr="Samuel (1996)" startWordPosition="1164" endWordPosition="1165">an accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases3 in the utterance; 2) the word n-grams3 in the utterance; 3) the dialogue act cues3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an utterance is dependent on the utterance&apos;s intern</context>
</contexts>
<marker>Samuel, 1996</marker>
<rawString>Kenneth B. Samuel. 1996. Using statistical learning algorithms to compute discourse information. Technical Report #97-11, The University of Delaware. Dissertation proposal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Tom O&apos;Hara</author>
<author>Kenneth McKeever</author>
<author>Thorsten Oehrstroem-Sandgren</author>
</authors>
<title>An empirical approach to temporal reference resolution.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>174--186</pages>
<contexts>
<context position="1593" citStr="Wiebe et al., 1997" startWordPosition="236" endWordPosition="239">, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task. Introduction Although machine learning approaches have achieved success in many areas of Natural Language Processing, researchers have only recently begun to investigate applying machine learning methods to discourse-level problems (Reithinger and Klesen, 1997; Di Eugenio et al., 1997; Wiebe et al., 1997; Andernach, 1996; Litman, 1994). An important task in discourse understanding is to interpret an utterance&apos;s dialogue act, which is a concise abstraction of a speaker&apos;s intention, such as SUGGEST and ACCEPT. Recognizing dialogue acts is critical for discourse-level understanding and can also be useful for other applications, such as resolving ambiguity in speech recognition. However, computing dialogue acts is a challenging task, because often a dialogue act cannot be directly inferred from a literal interpretation of an utterance. We have investigated applying Transformation-Based Learning (</context>
</contexts>
<marker>Wiebe, O&apos;Hara, McKeever, Oehrstroem-Sandgren, 1997</marker>
<rawString>Janyce Wiebe, Tom O&apos;Hara, Kenneth McKeever, and Thorsten Oehrstroem-Sandgren. 1997. An empirical approach to temporal reference resolution. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 174-186.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>