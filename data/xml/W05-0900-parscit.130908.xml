<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.870278166666667">
Intrinsic and Extrinsic
Evaluation Measures for
Machine Translation and/or
Summarization
Proceedings of the ACL-05 Workshop
Organizing Committee
</title>
<author confidence="0.89181625">
Jade Goldstein, US Department of Defense
Alon Lavie, CMU Language Technologies Institute
Chin-Yew Lin, USC Information Sciences Institute
Clare Voss, US Army Research Laboratory
</author>
<affiliation confidence="0.845321333333333">
29 June 2005
University of Michigan
Ann Arbor, Michigan, USA
</affiliation>
<figure confidence="0.67393725">
Production and Manufacturing by
Omnipress Inc.
Post Office Box 7214
Madison, WI 53707-7214
</figure>
<affiliation confidence="0.552712666666667">
c�2005 The Association for Computational Linguistics
Order copies of this and other ACL proceedings from:
Association for Computational Linguistics (ACL)
</affiliation>
<address confidence="0.7440544">
75 Paterson Street, Suite 9
New Brunswick, NJ 08901
USA
Tel: +1-732-342-9100
Fax: +1-732-342-9339
</address>
<email confidence="0.809281">
acl@aclweb.org
</email>
<page confidence="0.508016">
ii
</page>
<subsectionHeader confidence="0.805398">
Preface
</subsectionHeader>
<bodyText confidence="0.99202485">
This workshop is the first meeting to focus on the challenges that the machine translation (MT) and
summarization communities face in developing valid and useful evaluation measures. Our aim is to
bring these two communities together to learn from each other’s approaches.
Prior ACL workshops on evaluation have had as their central focus a core computational task (e.g.,
word sense disambiguation, parsing), a genre (e.g., dialogue, multi-modal interfaces), a computational
technique (e.g., unsupervised learning, finite state models), a resource (e.g., parallel texts, WordNet),
or a process (e.g., reading comprehension, question-answering). This workshop, in clear contrast, has
as its central focus the examination of evaluation measures, or ”meta-evaluation” as Dan Melamed has
noted.
The initial impetus for this workshop came at the biennial meeting of the Association for Machine
Translation in the Americas (AMTA) held at Georgetown University in September 2004, when the
following question arose in a discussion session: ”Why isn’t recall a part of MT evaluation the way that
it is for summarization evaluation?” Several of us continued this discussion afterwards and proposed to
convene together again more formally to address this question and other evaluation challenges that both
the MT and summarization communities have been tackling.
We wish to thank Bonnie Dorr and Ed Hovy, in particular, for their encouragement and contributions
in shaping the initial workshop proposal and the subsequent call for papers. Boyan Onyshkevych,
Barb Wheatley, Donna Harmon, and Judith Klavans also provided insightful comments in the proposal
writing phase of the workshop that helped guide and focus the topics we chose to address.
We also would like also to thank several others. Charles Wayne, Joe Olive, Donna Harmon, Hoa
Dang, Lori Buckland, and Chris Cieri were critical in helping make the datasets available to workshop
participants. Jason Eisner and Philipp Koehn, ACL publications chairs, provided us invaluable
assistance in preparing the proceedings.
Many thanks also to the Program Committee and additional reviewers who graciously spent time with a
short schedule to review submitted papers and provide valuable feedback. We have an exciting program
for which we thank the many authors who submitted their research papers.
Jade Goldstein, Alon Lavie, Chin-Yew Lin, Clare Voss
June 2005
Excerpts from Call for Papers
This one-day workshop will focus on the challenges that the MT and summarization communities face
in developing valid and useful evaluation measures. Our aim is to bring these two communities together
to learn from each other’s approaches.
In the past few years, we have witnessed—in both MT and summarization evaluation—the innovation
of ngram-based intrinsic metrics that automatically score system-outputs against human-produced
reference documents (e.g., IBM’s BLEU and ISI/USC’s counterpart ROUGE). Similarly, there has
been renewed interest in user applications and task-based extrinsic measures in both communities (e.g.,
DUC’05 and TIDES’04). Most recently, evaluation efforts have tested for correlations to cross-validate
independently derived intrinsic and extrinsic assessments of system-outputs with each other and with
human judgments on output, such as accuracy and fluency.
The concrete questions that we hope to see addressed in this workshop include, but are not limited to:
</bodyText>
<listItem confidence="0.976211722222222">
• How adequately do intrinsic measures capture the variation between system-outputs and human-
generated reference documents (summaries or translations)? What methods exist for calibrating
and controlling the variation in linguistic complexity and content differences in input test-sets
and reference sets? How much variation exists within these constructed sets? How does that
variation affect different intrinsic measures? How many reference documents are needed for
effective scoring?
• How can intrinsic measures go beyond simple n-gram matching, to quantify the similarity
between system-output and human-references? What other features and weighting alternatives
lead to better metrics for both MT and summarization? How can intrinsic measures capture
fluency and adequacy? Which types of new intrinsic metrics are needed to adequately evaluate
non-extractive summaries and paraphrasing (e.g.,interlingual) translations?
• How effectively do extrinsic (or proxy extrinsic) measures capture the quality of system output,
as needed for downstream use in human tasks, such as triage (document relevance judgments),
extraction (factual question answering), and report writing; and in automated tasks, such as
filtering, information extraction, and question-answering? For example, when is an MT system
good enough that a summarization system benefits from the additional information available in
the MT output?
• How should metrics for MT and summarization be assessed and compared? What characteristics
</listItem>
<bodyText confidence="0.898508692307692">
should a good metric possess? When is one evaluation method better than another? What are
the most effective ways of assessing the correlation testing and statistical modeling that seek to
predict human task performance or human notions of output quality (e.g., fluency and adequacy)
from ”cheaper” automatic metrics? How reliable are human judgments?
Anyone with an interest in MT or summarization evaluation research or in issues pertaining to the
combination of MT and summarization is encouraged to participate in the workshop. We are looking
for research papers on the aforementioned topics, as well as position papers that identify limitations in
current approaches and describe promising future research directions.
To faciliate the comparison of different measures during the workshop, we will be making available data
sets in advance for workshop participants to test their approaches to evaluation. Although the shared
data sets are separated, we would encourage participants to apply their automatic metrics on both data
sets and report comparative results in the workshop.
iv
</bodyText>
<subsectionHeader confidence="0.833187">
Shared Data Sets
</subsectionHeader>
<bodyText confidence="0.920511666666667">
Shared Data Set for MT Evaluation:
The shared data set consists of the 2003 TIDES MT-Eval Test Data for both Chinese-to-English and
Arabic-to-English MT. For each of these two language-pair data sets, the following is provided:
</bodyText>
<listItem confidence="0.999032166666667">
• The set of test sentences in the original source language (Chinese or Arabic)
• MT system output for the set of sentences for 7 different MT systems
• A collection of 4 reference translations (human translated) into English
• Human judgments of MT quality (adequacy and fluency) for the various MT system translations
of every sentence. Each sentence was judged by two subjects, each of which assigned both an
adequacy score and a fluency score, in the integer range of [1-5].
</listItem>
<bodyText confidence="0.728234666666667">
Shared Data Set for Summarization Evaluation:
The summarization shared data set consists of four years’ worth of data from past Document
Understanding Conferences (DUC) including:
</bodyText>
<listItem confidence="0.896084142857143">
• Documents
• Summaries, results, etc.
– Manually created summaries
– Automatically created baseline summaries
– Submitted summaries created by the participating groups’ systems
– Tables with the evaluation results
– Additional supporting data and software
</listItem>
<figure confidence="0.853014842105263">
v
Organizers:
Jade Goldstein US Department of Defense, USA
Alon Lavie CMU Language Technologies Institute, USA
Chin-Yew Lin USC Information Sciences Institute, USA
Clare Voss Army Research Laboratory, USA
Program Committee:
Yasuhiro Akiba ATR, Japan
Leslie Barrett TransClick, USA
Bonnie Dorr University of Maryland, USA
Tony Hartley University of Leeds, UK
John Henderson MITRE, USA
Chiori Hori CMU Language Technologies Institue, USA
Eduard Hovy USC Information Sciences Insititute, USA
Doug Jones MIT Lincoln Laboratory, USA
Philipp Koehn University of Edinburgh, UK
Marie-Francine Moens Katholieke Universiteit, Leuven, Belgium
Hermann Ney RWTH Aachen, Germany
Franz Och Google, USA
</figure>
<affiliation confidence="0.946969">
Rebecca Passonneau Columbia University, USA
Andrei Popescu-Belis University of Geneva ISSCO/TIM/ETI, Switzerland
Dragomir Radev University Michigan, USA
Karen Sparck Jones University of Cambridge Computer Laboratory, UK
Simone Teufel University of Cambridge Computer Laboratory, UK
</affiliation>
<table confidence="0.835293888888889">
Nicola Ueffing RWTH Aachen, Germany
Hans van Halteren University of Nijmegen, The Netherlands
Michelle Vanni Army Research Laboratory, USA
Dekai Wu HKUST, Hong Kong
Additional Reviewers:
Chad Langley US Department of Defense, USA
Gregor Leusch RWTH Aachen, Germany
Klaus Macherey Google,USA
Wolfang Macherey Google,USA
</table>
<affiliation confidence="0.512653">
Christof Monz University of Maryland, USA
Judith Schlesinger IDA Center for Computing Sciences, USA
</affiliation>
<page confidence="0.641304">
vi
</page>
<tableCaption confidence="0.951597">
Table of Contents
</tableCaption>
<figure confidence="0.343927363636364">
A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?
Bonnie Dorr, Christof Monz, Stacy President, Richard Schwartz and David Zajic 1
On the Subjectivity of Human Authored Summaries
BalaKrishna Kolluru and Yoshihiko Gotoh 9
Preprocessing and Normalization for Automatic Evaluation of Machine Translation
Gregor Leusch, Nicola Ueffing, David Vilar and Hermann Ney 17
Syntactic Features for Evaluation of Machine Translation
Ding Liu and Daniel Gildea 25
Evaluating Automatic Summaries of Meeting Recordings
Gabriel Murray, Steve Renals, Jean Carletta and Johanna Moore 33
Evaluating Summaries and Answers: Two Sides of the Same Coin?
Jimmy Lin and Dina Demner-Fushman 41
Evaluating DUC 2004 Tasks with the QARLA Framework
Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas and Felisa Verdejo 49
On Some Pitfalls in Automatic Evaluation and Significance Testing for MT
Stefan Riezler and John T. Maxwell 57
METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments
Satanjeev Banerjee and Alon Lavie 65
vii
Workshop Program
Wednesday, June 29, 2005
8:45–8:50 Opening Remarks
</figure>
<figureCaption confidence="0.132796">
Session 1: Summarization Metrics I
</figureCaption>
<reference confidence="0.973127878787879">
8:50–9:15 A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Cor-
relate?
Bonnie Dorr, Christof Monz, Stacy President, Richard Schwartz and David Zajic
9:15–9:40 On the Subjectivity of Human Authored Summaries
BalaKrishna Kolluru and Yoshihiko Gotoh
Session 2: MT Metrics I
9:40–10:05 Preprocessing and Normalization for Automatic Evaluation of Machine Translation
Gregor Leusch, Nicola Ueffing, David Vilar and Hermann Ney
10:05–10:30 Syntactic Features for Evaluation of Machine Translation
Ding Liu and Daniel Gildea
10:30–11:00 Break
Session 3: Invited Talk
11:00–12:00 Results of the Multilingual Summarization Evaluation, Kathy McKeown
Session 4: Student Session - Work in Progress
12:00–12:15 Evaluation of Sentence Selection on Spoken Dialogue, Xiaodan Zhu
12:15–12:30 Toward a Predictive Statistical Model of Task-based Performance, Calandra R. Tate
12:30–2:15 Lunch
Session 5: Summarization Metrics II
2:15–2:40 Evaluating Automatic Summaries of Meeting Recordings
Gabriel Murray, Steve Renals, Jean Carletta and Johanna Moore
2:40–3:05 Evaluating Summaries and Answers: Two Sides of the Same Coin?
Jimmy Lin and Dina Demner-Fushman
3:05–3:30 Evaluating DUC 2004 Tasks with the QARLA Framework
Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas and Felisa Verdejo
Session 6: MT Metrics II
4:00–4:25 On Some Pitfalls in Automatic Evaluation and Significance Testing for MT
Stefan Riezler and John T. Maxwell
4:25–4:50 METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with
Human Judgments
Satanjeev Banerjee and Alon Lavie
Session 7: Panel Discussion and Open Forum on Future Plans
4:50–5:50 Panel Discussion
5:50–6:00 Future Plans
</reference>
<page confidence="0.802602">
ix
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.847331666666667">Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization Proceedings of the ACL-05 Workshop Organizing Committee</title>
<author confidence="0.515163">Jade Goldstein</author>
<author confidence="0.515163">US Department of Defense</author>
<affiliation confidence="0.900228">Alon Lavie, CMU Language Technologies Institute Chin-Yew Lin, USC Information Sciences Institute Clare Voss, US Army Research Laboratory</affiliation>
<address confidence="0.499951">29 June 2005</address>
<affiliation confidence="0.981224">University of</affiliation>
<address confidence="0.971255">Ann Arbor, Michigan, USA</address>
<author confidence="0.67151">Production</author>
<author confidence="0.67151">Manufacturing by</author>
<affiliation confidence="0.991886">Omnipress Inc.</affiliation>
<address confidence="0.997521">Post Office Box 7214 Madison, WI 53707-7214</address>
<title confidence="0.691025">The Association for Computational Linguistics</title>
<author confidence="0.551911">Order copies of this</author>
<author confidence="0.551911">other ACL proceedings from</author>
<affiliation confidence="0.854452">Association for Computational Linguistics (ACL)</affiliation>
<address confidence="0.99835">75 Paterson Street, Suite 9 New Brunswick, NJ 08901 USA</address>
<phone confidence="0.99915">Tel: +1-732-342-9100 Fax: +1-732-342-9339</phone>
<email confidence="0.663084">acl@aclweb.org</email>
<abstract confidence="0.987206285714286">ii Preface This workshop is the first meeting to focus on the challenges that the machine translation (MT) and summarization communities face in developing valid and useful evaluation measures. Our aim is to bring these two communities together to learn from each other’s approaches. Prior ACL workshops on evaluation have had as their central focus a core computational task (e.g., word sense disambiguation, parsing), a genre (e.g., dialogue, multi-modal interfaces), a computational technique (e.g., unsupervised learning, finite state models), a resource (e.g., parallel texts, WordNet), or a process (e.g., reading comprehension, question-answering). This workshop, in clear contrast, has as its central focus the examination of evaluation measures, or ”meta-evaluation” as Dan Melamed has noted. The initial impetus for this workshop came at the biennial meeting of the Association for Machine Translation in the Americas (AMTA) held at Georgetown University in September 2004, when the following question arose in a discussion session: ”Why isn’t recall a part of MT evaluation the way that it is for summarization evaluation?” Several of us continued this discussion afterwards and proposed to convene together again more formally to address this question and other evaluation challenges that both the MT and summarization communities have been tackling. We wish to thank Bonnie Dorr and Ed Hovy, in particular, for their encouragement and contributions in shaping the initial workshop proposal and the subsequent call for papers. Boyan Onyshkevych, Barb Wheatley, Donna Harmon, and Judith Klavans also provided insightful comments in the proposal writing phase of the workshop that helped guide and focus the topics we chose to address. We also would like also to thank several others. Charles Wayne, Joe Olive, Donna Harmon, Hoa Dang, Lori Buckland, and Chris Cieri were critical in helping make the datasets available to workshop participants. Jason Eisner and Philipp Koehn, ACL publications chairs, provided us invaluable assistance in preparing the proceedings. Many thanks also to the Program Committee and additional reviewers who graciously spent time with a short schedule to review submitted papers and provide valuable feedback. We have an exciting program for which we thank the many authors who submitted their research papers.</abstract>
<author confidence="0.564567">Jade Goldstein</author>
<author confidence="0.564567">Alon Lavie</author>
<author confidence="0.564567">Chin-Yew Lin</author>
<author confidence="0.564567">Clare Voss</author>
<date confidence="0.965096">June 2005</date>
<abstract confidence="0.991386523809524">Excerpts from Call for Papers This one-day workshop will focus on the challenges that the MT and summarization communities face in developing valid and useful evaluation measures. Our aim is to bring these two communities together to learn from each other’s approaches. In the past few years, we have witnessed—in both MT and summarization evaluation—the innovation of ngram-based intrinsic metrics that automatically score system-outputs against human-produced reference documents (e.g., IBM’s BLEU and ISI/USC’s counterpart ROUGE). Similarly, there has been renewed interest in user applications and task-based extrinsic measures in both communities (e.g., DUC’05 and TIDES’04). Most recently, evaluation efforts have tested for correlations to cross-validate independently derived intrinsic and extrinsic assessments of system-outputs with each other and with human judgments on output, such as accuracy and fluency. The concrete questions that we hope to see addressed in this workshop include, but are not limited to: • How adequately do intrinsic measures capture the variation between system-outputs and humangenerated reference documents (summaries or translations)? What methods exist for calibrating and controlling the variation in linguistic complexity and content differences in input test-sets and reference sets? How much variation exists within these constructed sets? How does that variation affect different intrinsic measures? How many reference documents are needed for effective scoring? • How can intrinsic measures go beyond simple n-gram matching, to quantify the similarity between system-output and human-references? What other features and weighting alternatives lead to better metrics for both MT and summarization? How can intrinsic measures capture fluency and adequacy? Which types of new intrinsic metrics are needed to adequately evaluate non-extractive summaries and paraphrasing (e.g.,interlingual) translations? • How effectively do extrinsic (or proxy extrinsic) measures capture the quality of system output, as needed for downstream use in human tasks, such as triage (document relevance judgments), extraction (factual question answering), and report writing; and in automated tasks, such as filtering, information extraction, and question-answering? For example, when is an MT system good enough that a summarization system benefits from the additional information available in the MT output? • How should metrics for MT and summarization be assessed and compared? What characteristics should a good metric possess? When is one evaluation method better than another? What are the most effective ways of assessing the correlation testing and statistical modeling that seek to predict human task performance or human notions of output quality (e.g., fluency and adequacy) from ”cheaper” automatic metrics? How reliable are human judgments? Anyone with an interest in MT or summarization evaluation research or in issues pertaining to the combination of MT and summarization is encouraged to participate in the workshop. We are looking for research papers on the aforementioned topics, as well as position papers that identify limitations in current approaches and describe promising future research directions. To faciliate the comparison of different measures during the workshop, we will be making available data sets in advance for workshop participants to test their approaches to evaluation. Although the shared data sets are separated, we would encourage participants to apply their automatic metrics on both data sets and report comparative results in the workshop.</abstract>
<email confidence="0.357755">iv</email>
<note confidence="0.668113">Shared Data Sets Shared Data Set for MT Evaluation: The shared data set consists of the 2003 TIDES MT-Eval Test Data for both Chinese-to-English and Arabic-to-English MT. For each of these two language-pair data sets, the following is provided: • The set of test sentences in the original source language (Chinese or Arabic) • MT system output for the set of sentences for 7 different MT systems</note>
<abstract confidence="0.92704325">A collection of 4 reference translations (human translated) into English • Human judgments of MT quality (adequacy and fluency) for the various MT system translations of every sentence. Each sentence was judged by two subjects, each of which assigned both an adequacy score and a fluency score, in the integer range of [1-5].</abstract>
<title confidence="0.694437">Shared Data Set for Summarization Evaluation: The summarization shared data set consists of four years’ worth of data from past Document Understanding Conferences (DUC) including: • Documents</title>
<abstract confidence="0.968461142857143">Summaries, results, etc. created summaries created baseline summaries summaries created by the participating groups’ systems with the evaluation results supporting data and software v</abstract>
<note confidence="0.7245372">Organizers: Jade Goldstein US Department of Defense, USA Alon Lavie CMU Language Technologies Institute, USA Chin-Yew Lin USC Information Sciences Institute, USA Clare Voss Army Research Laboratory, USA</note>
<title confidence="0.483495">Program Committee:</title>
<author confidence="0.54132525">Yasuhiro Akiba ATR</author>
<author confidence="0.54132525">Japan Leslie Barrett TransClick</author>
<author confidence="0.54132525">USA Bonnie Dorr University of Maryland</author>
<author confidence="0.54132525">USA Tony Hartley University of Leeds</author>
<author confidence="0.54132525">UK</author>
<address confidence="0.71469275">John Henderson MITRE, USA Chiori Hori CMU Language Technologies Institue, USA Eduard Hovy USC Information Sciences Insititute, USA Doug Jones MIT Lincoln Laboratory, USA</address>
<author confidence="0.478069666666667">Philipp Koehn University of Edinburgh</author>
<author confidence="0.478069666666667">UK Marie-Francine Moens Katholieke Universiteit</author>
<author confidence="0.478069666666667">Belgium Hermann Ney RWTH Aachen Leuven</author>
<author confidence="0.478069666666667">Germany</author>
<note confidence="0.57936475">Franz Och Google, USA Rebecca Passonneau Columbia University, USA Andrei Popescu-Belis University of Geneva ISSCO/TIM/ETI, Switzerland Dragomir Radev University Michigan, USA</note>
<author confidence="0.861571166666667">Karen Sparck Jones University of Cambridge Computer Laboratory</author>
<author confidence="0.861571166666667">UK Simone Teufel University of Cambridge Computer Laboratory</author>
<author confidence="0.861571166666667">UK Nicola Ueffing RWTH Aachen</author>
<author confidence="0.861571166666667">Germany Hans van_Halteren University of Nijmegen</author>
<author confidence="0.861571166666667">The Netherlands Michelle Vanni Army Research Laboratory</author>
<author confidence="0.861571166666667">USA Dekai Wu HKUST</author>
<author confidence="0.861571166666667">Hong Kong</author>
<affiliation confidence="0.709172">Additional Reviewers:</affiliation>
<address confidence="0.897371">Chad Langley US Department of Defense, USA</address>
<author confidence="0.938617">Gregor Leusch RWTH Aachen</author>
<author confidence="0.938617">Germany Klaus Macherey Google</author>
<author confidence="0.938617">USA</author>
<affiliation confidence="0.706164">Wolfang Macherey Google,USA</affiliation>
<address confidence="0.744938">Christof Monz University of Maryland, USA Judith Schlesinger IDA Center for Computing Sciences, USA</address>
<email confidence="0.802684">vi</email>
<title confidence="0.9250812">Table of Contents A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate? Dorr, Christof Monz, Stacy President, Richard Schwartz and David Zajic 1 On the Subjectivity of Human Authored Summaries Kolluru and Yoshihiko Gotoh Preprocessing and Normalization for Automatic Evaluation of Machine Translation Leusch, Nicola Ueffing, David Vilar and Hermann Ney17 Syntactic Features for Evaluation of Machine Translation Ding Liu and Daniel Gildea 25 Evaluating Automatic Summaries of Meeting Recordings</title>
<address confidence="0.348982">Murray, Steve Renals, Jean Carletta and Johanna Moore33</address>
<title confidence="0.721512428571429">Evaluating Summaries and Answers: Two Sides of the Same Coin? Lin and Dina Demner-Fushman41 Evaluating DUC 2004 Tasks with the QARLA Framework Amig´o, Julio Gonzalo, Anselmo and Felisa Verdejo49 On Some Pitfalls in Automatic Evaluation and Significance Testing for MT Riezler and John T. Maxwell57 METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
<author confidence="0.821821">Satanjeev Banerjee</author>
<author confidence="0.821821">Alon Lavie</author>
<email confidence="0.849157">vii</email>
<affiliation confidence="0.721621">Workshop Program</affiliation>
<address confidence="0.766331">Wednesday, June 29, 2005 8:45–8:50 Opening Remarks</address>
<note confidence="0.598890857142857">Session 1: Summarization Metrics I 8:50–9:15A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE relate? Bonnie Dorr, Christof Monz, Stacy President, Richard Schwartz and David Zajic 9:15–9:40On the Subjectivity of Human Authored BalaKrishna Kolluru and Yoshihiko Gotoh Session 2: MT Metrics I</note>
<title confidence="0.989651">9:40–10:05Preprocessing and Normalization for Automatic Evaluation of Machine Translation</title>
<author confidence="0.993205">Gregor Leusch</author>
<author confidence="0.993205">Nicola Ueffing</author>
<author confidence="0.993205">David Vilar</author>
<author confidence="0.993205">Hermann Ney</author>
<title confidence="0.489396">10:05–10:30Syntactic Features for Evaluation of Machine Translation</title>
<author confidence="0.909456">Ding Liu</author>
<author confidence="0.909456">Daniel Gildea</author>
<address confidence="0.459786">10:30–11:00 Break</address>
<note confidence="0.605900333333333">Session 3: Invited Talk of the Multilingual Summarization Kathy McKeown Session 4: Student Session - Work in Progress</note>
<author confidence="0.852325">of Sentence Selection on Spoken Xiaodan Zhu</author>
<affiliation confidence="0.622935">a Predictive Statistical Model of Task-based Calandra R. Tate</affiliation>
<address confidence="0.627781">12:30–2:15 Lunch Session 5: Summarization Metrics II</address>
<title confidence="0.878799">2:15–2:40Evaluating Automatic Summaries of Meeting</title>
<author confidence="0.793926666666667">Evaluating Summaries</author>
<author confidence="0.793926666666667">Answers Two Sides of the Same Jimmy Lin</author>
<author confidence="0.793926666666667">Dina Demner-Fushman</author>
<affiliation confidence="0.31288">3:05–3:30Evaluating DUC 2004 Tasks with the QARLA Framework</affiliation>
<address confidence="0.3490025">Amig´o, Julio Gonzalo, Anselmo and Felisa Verdejo Session 6: MT Metrics II</address>
<title confidence="0.863146">4:00–4:25On Some Pitfalls in Automatic Evaluation and Significance Testing for</title>
<author confidence="0.991522">Stefan Riezler</author>
<author confidence="0.991522">John T Maxwell</author>
<title confidence="0.8246825">4:25–4:50METEOR: An Automatic Metric for MT Evaluation with Improved Correlation Human Judgments</title>
<author confidence="0.926201">Satanjeev Banerjee</author>
<author confidence="0.926201">Alon Lavie</author>
<address confidence="0.493303">Session 7: Panel Discussion and Open Forum on Future Plans 4:50–5:50 Panel Discussion 5:50–6:00 Future Plans</address>
<intro confidence="0.668534">ix</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>8:50–9:15 A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?</title>
<marker></marker>
<rawString>8:50–9:15 A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bonnie Dorr</author>
<author>Christof Monz</author>
<author>Stacy President</author>
</authors>
<title>Richard Schwartz and David Zajic 9:15–9:40 On the Subjectivity of Human Authored Summaries BalaKrishna Kolluru and Yoshihiko Gotoh Session 2: MT Metrics I 9:40–10:05 Preprocessing and Normalization for Automatic Evaluation of Machine Translation Gregor Leusch, Nicola Ueffing, David Vilar and Hermann Ney 10:05–10:30 Syntactic Features for Evaluation of Machine Translation</title>
<marker>Dorr, Monz, President, </marker>
<rawString>Bonnie Dorr, Christof Monz, Stacy President, Richard Schwartz and David Zajic 9:15–9:40 On the Subjectivity of Human Authored Summaries BalaKrishna Kolluru and Yoshihiko Gotoh Session 2: MT Metrics I 9:40–10:05 Preprocessing and Normalization for Automatic Evaluation of Machine Translation Gregor Leusch, Nicola Ueffing, David Vilar and Hermann Ney 10:05–10:30 Syntactic Features for Evaluation of Machine Translation</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>10:30–11:00 Break Session 3: Invited Talk 11:00–12:00 Results of the Multilingual Summarization Evaluation, Kathy McKeown Session 4: Student Session -</title>
<booktitle>Work in Progress 12:00–12:15 Evaluation of Sentence Selection on Spoken Dialogue, Xiaodan Zhu 12:15–12:30 Toward</booktitle>
<marker>Liu, Gildea, </marker>
<rawString>Ding Liu and Daniel Gildea 10:30–11:00 Break Session 3: Invited Talk 11:00–12:00 Results of the Multilingual Summarization Evaluation, Kathy McKeown Session 4: Student Session - Work in Progress 12:00–12:15 Evaluation of Sentence Selection on Spoken Dialogue, Xiaodan Zhu 12:15–12:30 Toward a Predictive Statistical Model of Task-based Performance, Calandra R. Tate 12:30–2:15 Lunch</rawString>
</citation>
<citation valid="false">
<title>Session 5: Summarization Metrics II 2:15–2:40 Evaluating Automatic Summaries of Meeting Recordings Gabriel Murray, Steve Renals, Jean Carletta and Johanna Moore</title>
<marker></marker>
<rawString>Session 5: Summarization Metrics II 2:15–2:40 Evaluating Automatic Summaries of Meeting Recordings Gabriel Murray, Steve Renals, Jean Carletta and Johanna Moore</rawString>
</citation>
<citation valid="false">
<title>2:40–3:05 Evaluating Summaries and Answers: Two Sides of the Same Coin?</title>
<booktitle>Jimmy Lin and Dina Demner-Fushman 3:05–3:30 Evaluating DUC 2004 Tasks with the QARLA Framework</booktitle>
<marker></marker>
<rawString>2:40–3:05 Evaluating Summaries and Answers: Two Sides of the Same Coin? Jimmy Lin and Dina Demner-Fushman 3:05–3:30 Evaluating DUC 2004 Tasks with the QARLA Framework</rawString>
</citation>
<citation valid="false">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
</authors>
<title>Anselmo Pe˜nas and Felisa Verdejo Session 6: MT Metrics II</title>
<marker>Amig´o, Gonzalo, </marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas and Felisa Verdejo Session 6: MT Metrics II</rawString>
</citation>
<citation valid="false">
<authors>
<author>T John</author>
</authors>
<title>4:00–4:25 On Some Pitfalls in Automatic Evaluation and Significance Testing for MT Stefan Riezler and</title>
<publisher>Maxwell</publisher>
<marker>John, </marker>
<rawString>4:00–4:25 On Some Pitfalls in Automatic Evaluation and Significance Testing for MT Stefan Riezler and John T. Maxwell</rawString>
</citation>
<citation valid="false">
<title>An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
<marker></marker>
<rawString>4:25–4:50 METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</rawString>
</citation>
<citation valid="false">
<authors>
<author>Satanjeev Banerjee</author>
</authors>
<title>and Alon Lavie Session 7:</title>
<booktitle>Panel Discussion and Open Forum on Future Plans 4:50–5:50 Panel Discussion 5:50–6:00 Future Plans</booktitle>
<marker>Banerjee, </marker>
<rawString>Satanjeev Banerjee and Alon Lavie Session 7: Panel Discussion and Open Forum on Future Plans 4:50–5:50 Panel Discussion 5:50–6:00 Future Plans</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>