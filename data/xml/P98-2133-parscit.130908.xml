<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.906802">
LiLFeS — Towards a Practical HPSG Parser *
</title>
<author confidence="0.80676">
MAKIN° Takaki+, YOSHIDA Minoru*, TORISAWA Kentaro*, TSUJII Jun&apos;ichif:
</author>
<affiliation confidence="0.784614">
fTsujii Group, Department of Information Science, University of Tokyo
</affiliation>
<address confidence="0.379347">
Hongo 7-3-1, Bunkyo-Ku, Tokyo 113-0033, Japan
</address>
<keyword confidence="0.3573565">
mak,mino,torisawa,tsujiil is.s.u-tokyo.ac.jp
:CCL, UMIST, U.K.
</keyword>
<sectionHeader confidence="0.979577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871666666667">
This paper presents the LiLFeS system, an
efficient feature-structure description language
for HPSG. The core engine of LiLFeS is an
Abstract Machine for Attribute-Value Logics,
proposed by Carpenter and Qu. Basic design
policies, the current status, and performance
evaluation of the LiLFeS system are described.
The paper discusses two implementations of
the LiLFeS. The first one is based on an emu-
lator of the abstract machine, while the second
one uses a native-code compiler and therefore
is much more efficient than the first one.
</bodyText>
<sectionHeader confidence="0.98792" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999939153846154">
Inefficiency is the major reason why the HPSG
formalism (Pollard and Sag, 1993) has not been
used for practical applications. However, one
can claim that HPSG may not be so inefficient; it
is just that an efficient implementation of HPSG
has not been seriously pursued till now.
We set a goal for the performance of our HPSG
parser: 100 milliseconds of average parsing time
on a sentence in real-world corpora. If our HPSG
parser accomplished this goal, it would be capable
to parse about 1,000,000 sentences in a day, and
could be used for applications such as knowledge
acquisition from corpora.
</bodyText>
<subsectionHeader confidence="0.9995005">
1.1 Existing Systems for Typed Feature
Structures (TFSs)
</subsectionHeader>
<bodyText confidence="0.999780428571429">
Since Typed Feature Structures (TFSs) (Carpenter,
1992) are the basic data structures in HPSG, the
efficiency of handling TFSs has been considered
as the key to improve the efficiency of an HPSG
parser. There are two representative systems that
handle TFSs&apos;: ALE (Carpenter and Penn, 1994), a
TFS interpreter written in Prolog, and ProFIT
</bodyText>
<footnote confidence="0.895779727272727">
* This research is partially funded by the project of Japan
Society for the Promotion of Science (JSPS-RFTF96P00502).
I LIFE (AIt-lcaci et al., 1994) is also famous, but we do not
discuss it because it does not follow Carpenter&apos;s TFS defini-
tion. Moreover, our separate experiments show that LIFE is
more than 10 times slower than emulator-based LiLFeS. As
for AMALIA (Wintner, 1997), we cannot make experiments
since it is not freely distributed. His experiments in his
dissertation shows that AMALIA is 15 time faster than ALE
at maximum; it is close to emulator-based LiLFeS, and is
outperformed by native-code compiler of LiLFeS.
</footnote>
<bodyText confidence="0.986317083333334">
(Erbach, 1995), a TFS-to-Prolog-term compiler.
However, as the comparison of these systems
with our system (Section 3.2) shows, neither of
these two systems is able to achieve the efficiency
we established as our goal. Moreover, these two
systems have serious disadvantages as a frame-
work for practical applications. The ProFIT
approach, for example, tends to consume too much
memory for execution. It is also difficult, if not
impossible, to combine them with other techniques
like parallel parsing, etc., because these two sys-
tems have been embedded in Prolog.
</bodyText>
<subsectionHeader confidence="0.994175">
1.2 Our Approach
</subsectionHeader>
<bodyText confidence="0.879701">
One of the promising directions of improving the
efficiency of handling TFSs while retaining a ne-
cessary amount of flexibility is to take up the idea
of AMAVL proposed in (Carpenter and Qu, 1995)
to design a general programming system based on
TFS.
LiLFeS is a logic programming system thus
designed and developed by our group, based on
AMAVL implementation. LiLFeS can be char-
acterized as follows.
</bodyText>
<listItem confidence="0.97853375">
• Architecture based on an AMAVL implementa-
tion, which compiles a TFS into a sequence of
abstract machine instructions, and performs
unification of the TFS by emulating the execu-
tion of those instructions. Although the pro-
posal of such an AMAVL was already made in
1995, no serious implementation has been re-
ported. We believe that LiLFeS is the first se-
rious treatment of the proposal.
• Rich language specification: We have adopted a
language syntax similar to Prolog. LiLFeS as a
prograrruning language has almost the full capa-
bilities of ordinary Prolog systems. Furthermore,
we provide efficient built-in predicates that are
often required in NLP applications, such as TFS
copy, equivalence check, and associative arrays.
• Independent language system: In order to devel-
op an efficient and portable language system,
we chose not to develop the language depending
on an existing high-level language such as
Prolog. Instead, we programmed the LiLFeS
system from scratch. The independence also
allows us to provide various built-in predicates
in efficient ways.
</listItem>
<subsectionHeader confidence="0.998802">
1.3 Structure of This Paper
</subsectionHeader>
<bodyText confidence="0.989546">
Section 2 describes LiLFeS as a programming
</bodyText>
<page confidence="0.975881">
807
</page>
<figure confidence="0.572794333333333">
my list &lt;- [loot].
e list &lt;- [my list].
ne_list &lt;- [my list]
+ [FIRST\ bot, REST\ my list].
append(e_list, X, X).
append( (FIRST\ A &amp; REST\ X),
Y,
(FIRST\ A &amp; REST\ Z) ) :-
append( X, Y, Z ).
</figure>
<figureCaption confidence="0.980335">
Figure 2 Sample LiLFeS Program
</figureCaption>
<bodyText confidence="0.97877536">
language. Section 3 gives a brief description of
the AMAVL we implemented, the core inference
engine of the LiLFeS system. In Section 4, we
discuss the current status of the LiLFeS system
and the results of experiments on the system per-
formance. Section 5 describes a native-code
compiler we are currently developing on the LiL-
FeS system, and discusses its performance.
2 LiLFeS as a Programming Language
LiLFeS has basically the same syntax as Prolog,
except that it uses TFSs instead of terms. Types
and features must be defined before being used in
TFS terms.
Figure 2 show the definition of the predicate ap—
pend in LiLFeS. The first paragraph contains the
type definitions of my list, e_list, and
ne_list. The type nelist, for example, is a
subtype of the type my list, and has two appro-
priate features, FIRST and REST. The value of
the feature REST is restricted to the type my list
or one of its subtypes. The type bot is the uni-
versal type that subsumes all types.
The rest of the program is definite clauses. As
one can see, the predicate append is represented
by TFSs instead of Prolog first-order terms2.
</bodyText>
<sectionHeader confidence="0.938263" genericHeader="introduction">
3 Abstract Machine for Attribute-
</sectionHeader>
<subsectionHeader confidence="0.621204">
Value Logics
</subsectionHeader>
<bodyText confidence="0.9998788">
The Abstract Machine for Attribute-Value Logics
(AMAVL) is the unification engine of the LiLFeS
system. AMAVL provides (1) efficient represen-
tation of TFSs on the memory, and (2) compilation
of TFSs into abstract machine codes.
</bodyText>
<subsectionHeader confidence="0.999963">
3.1 Representation of a TFS on the Memory
</subsectionHeader>
<bodyText confidence="0.999850714285714">
AMAVL, as does LiLFeS, requires all TFSs to be
totally well-typed3. In other words, (1) the types
and features should be explicitly declared, (2) ap-
propriateness of specifications between types and
features should be properly declared, and (3) all
TFSs should follow these appropriateness specifi-
cations. Provided these requirements are satisfied,
</bodyText>
<footnote confidence="0.88017275">
2 Note that LiLFeS has built-in list types as Prolog does.
This program is just an example to illustrate how a TFS is
represented in LiLFeS
3 For a more formal definition, see (Carpenter, 1992).
</footnote>
<figureCaption confidence="0.996537">
Figure 1 Compiling TFS
</figureCaption>
<bodyText confidence="0.9995341875">
AMAVL efficiently represents a TFS in memory.
The representation of a TFS on memory resem-
bles the graph notation of a TFS; A node is repre-
sented by n+1 continuous data cells, where n is the
number of features outgoing from the node. The
first data cell contains the type of the node, and the
rest of the data cells contain pointers to the values
of the corresponding features.
The merit of this representation is that feature
names need not be represented in the TFS repre-
sentation on memory. The requirements on a
TFS guarantee that the kinds and number of fea-
tures are statically defined and constant for a given
type, therefore we can determine the offset of the
pointer to a given feature only by referring to the
type of the given node.
</bodyText>
<subsectionHeader confidence="0.999715">
3.2 TFS as an Instruction Sequence
</subsectionHeader>
<bodyText confidence="0.999122571428572">
Unification is an operation defined between two
TFSs. However, in most cases, one of the two
TFSs is known in advance at compile-time. We
can therefore compile the TFS into a sequence of
specialized codes for unification, as illustrated in
Figure 1, rather than using a general unification
routine. The compiled unification codes are
specialized for given specific TFSs and therefore
much more efficient than a general unifier. This
is because any general unifier has to traverse both
TFSs each time unification occurs at run-time.
Many studies have been reported for compiling
unification of Prolog terms (for example, WAM
(AIt-Kaci, 1991)). However, the TFS unification
is much more complex than Prolog-term unifica-
tion, because (1) unification between different
types may succeed due to the existence of a type
hierarchy, and (2) features must be merged in the
fixed-offset TFS representation on memory.
AMAVL compiles a type hierarchy and prepares
for the complex situations described above. A
TFS itself is compiled into a sequence of four
kinds of instructions: ADDNEW (the unification of
TFS types), UNIFYVAR (creation of structure-
sharing), PUSH (feature traversing) and POP (end of
PUSH block). These instructions refer to the
compiled type hierarchy, if necessary.
These operations are implemented following the
</bodyText>
<table confidence="0.966043">
tlo t2
Y
Specialized
Unifier U
result
ti U t2
808
(Parsing time per sentence, Unit: seconds)
Our Goal 0.100
CYK-style 1.050
naive parser _ 0.350
Parser based on
Torisawa&apos;s algorithm
Condit&apos;on: 600 sentences from EDR Japanese corpus (average
length 21 words), Average in the parsing of successful y parsed
539 sentences
Environment: DEC Alpha 500/400MHz with 256MB memory
Components Lines
WAM/AMAVL Emulator 5,434
LiLFeS-to-WAM/AMAVL Compiler 6,091
Built-in Functions 9,530
TFS Display Routine 2,320
Others (Class Library etc.) 2,374
Total 25,749
</table>
<tableCaption confidence="0.998816">
Table 1 Source Code Lines of the LiLFeS System
</tableCaption>
<bodyText confidence="0.9622444">
original proposal of AMAVL. We also added
several other instructions in our AMAVL imple-
mentation, such as initialization of instructions for
successive unifications and combined instructions
for reducing overhead.
</bodyText>
<sectionHeader confidence="0.99876" genericHeader="method">
4 LiLFeS System
</sectionHeader>
<bodyText confidence="0.999958888888889">
The LiLFeS system is designed based on two
abstract machines: AMAVL for TFS representa-
tion and unification procedures, and Warren&apos;s
Abstract Machine (WAM) (AR-Kaci, 1991) for
control of execution of definite clause programs.
In this section we describe the current status of the
LiLFeS system and applications running on it.
Thereafter, we discuss the performance of LiLFeS
in our experiments.
</bodyText>
<subsectionHeader confidence="0.998932">
4.1 Current Status of the LiLFeS System
</subsectionHeader>
<bodyText confidence="0.997763416666667">
The LiLFeS system is developed as a combination
of AMAVL/WAM emulator, TFS compiler, and
built-in support functions. They are all written in
C++ with the source code of more than 25,000
lines (See Table 1). The source code can be
compiled by GNU C++, and we have confirmed
operation on Sun Sun0S4/Solaris, DEC Digital
UNIX, and Microsoft Windows.
We have several practical applications on the
LiLFeS system. We currently have several dif-
ferent parsers for HPSG and HPSG grammars of
Japanese and English, as follows:
</bodyText>
<listItem confidence="0.9689975">
• A underspecified Japanese grammar developed
by our group (Mitsuisi, 1998). Lexicon con-
</listItem>
<bodyText confidence="0.8663185">
sists of TFSs each of which has more than 100
nodes. The grammar can produce parse trees
for 88% of the corpus of the real world texts
(EDR Japanese corpus), 60% of which are given
correct parse trees4. This grammar is used for
the experiments in the next section.
</bodyText>
<listItem confidence="0.8258515">
• XHPSG, An HPSG English grammar (Tateisi,
1997). The grammar is converted from the
XTAG grammar (XTAG group, 1995), which
has more than 300,000 lexical entries.
• A naïve parser using a CYK-like algorithm.
Although using a simple algorithm, the parser
utilizes the full capabilities provided by LiLFeS,
such as built-in predicates (TFS copy, array op-
</listItem>
<tableCaption confidence="0.533573">
The grammar does not contain semantic analysis such as
coreference resolution.
Table 2 Parsing Performance Evaluation
with a Practical Grammar
eration, etc.).
</tableCaption>
<listItem confidence="0.808580888888889">
• A parser based on the Torisawa&apos;s parsing algo-
rithm (Torisawa and Tsujii, 1996). This algo-
rithm compiles an HPSG grammar into 2 parts:
its CFG skeletons and a remaining part, and
parses a sentence in two phases. Although the
parser is not a complete implementation of the
algorithm, its efficiency benefits from its 2-
phase parsing, which reduces the amount of uni-
fication.
</listItem>
<bodyText confidence="0.9612155">
These parsers and grammars are used for the per-
formance evaluations in the next section.
</bodyText>
<subsectionHeader confidence="0.99194">
4.2 Performance Evaluation
</subsectionHeader>
<bodyText confidence="0.99808215625">
We evaluated the performance of the LiLFeS sys-
tem over three aspects: Parsing performance of
LiLFeS, comparison to other TFS systems, and
comparison to different Prolog systems.
Table 2 shows the performance of HPSG parsers
on a real-world corpus. However, even with the
sophisticated algorithm, the parsing speed is 3.5
times slower than intended. To achieve our goal,
we need a drastic improvement of a performance.
We therefore performed the following experiments
to find out the problem.
Table 3 shows the performance comparison to
other TFS systems, ALE and ProF1T. Two
grammars are used in the experiments: &amp;quot;Simple&amp;quot; is
a small HPSG-like grammar written by our group,
while &amp;quot;HPSG&amp;quot; is the small-lexicon HPSG gram-
mar distributed with the ALE package. In the
&amp;quot;Simple&amp;quot; experiments, the LiLFeS system is far
more efficient than ALE, but is outperformed by
ProFTT. However, in the &amp;quot;HPSG&amp;quot; experiment,
which has to handle much more complex TFSs
than &amp;quot;Simple&amp;quot; experiments, LiLFeS is clearly
better than ProFTT.
On the contrary, with simple data LiLFeS is re-
latively inefficient. Experiments in Table 4,
which show comparisons to Prolog systems, show
that the performance of LiLFeS is significantly
worse than that of those Prolog systems.
To summarize, the performance of LiLFeS is far
more impressive when it has to handle complex
TFSs. This fact indicates that the TFS engine in
LiLFeS is efficient but that the other parts, i.e. the
</bodyText>
<page confidence="0.997103">
809
</page>
<table confidence="0.939453142857143">
Grammar Simple Simple HPSG
S stem 1 answer 64 answers
LiLFeS system (emulator-based) 5.70 322.4 2.56*
ALE on SICStus WAM emulation 225.60 10560 37.71*
ALE on SICStus native-code 67.05 3046 26.69*
ProFIT on SICStus WAM emulation 2.94 127.51 8.08
ProFIT on SICStus native-code 1.48 64.08 9.78
(Unit: seconds)
Simple: a simple HPSG like grammar, parsed 1000 times by a bottom-up parser, 9-word sentence results in 1 parse-tree
HPSG: a toy HPSG grammar distributed with ALE, parsed by a parser distributed with ProFIT,
14-word sentence results in 134 parse-trees
*: ALE built-in parser is used instead of parser written in definite clauses
f: The parser program is translated to avoid the &amp;quot;call&amp;quot; built-in, which contains some problems in the LiLFeS implementation
(Environment: Sun UltraSparc 1/167MHz with 128MB memory)
</table>
<tableCaption confidence="0.996513">
Table 3 Performance Comparison to Other TFS Systems nit- seconds
</tableCaption>
<bodyText confidence="0.999203777777778">
parts concerning LiLFeS as a general logic pro-
gramming system, are not yet efficient enough.
This means that, in order to improve the LiLFeS
system as a whole, we have to include various
optimization techniques already encoded in recent
Prolog implementations.
Thus we decided to redesign and optimize the
whole system. The next section describes this
optimized LiLFeS.
</bodyText>
<sectionHeader confidence="0.984889" genericHeader="method">
5 LiLFeS Native-Code Compiler
</sectionHeader>
<bodyText confidence="0.99994">
We are currently developing a native-code com-
piler of LiLFeS in order to attain maximum per-
formance. This section at first describes the
design policies of the compiler, and then, describes
the current status of implementation. The results
of the performance evaluations on the native-code
compiler are also presented.
</bodyText>
<subsectionHeader confidence="0.9964945">
5.1 Design Policies of the LiLFeS Native-
Code Compiler
</subsectionHeader>
<bodyText confidence="0.991436">
The design policies for the LiLFeS native-code
compiler are:
</bodyText>
<listItem confidence="0.916332888888889">
• Native code output. We chose native-code
compiling for optimal efficiency. Although
this costs high for development, the resulting ef-
ficiency will compensate the cost.
• Execution model close to a real machine. We
designed the execution model by referring to the
implementation of Aquarius Prolog (Van Roy,
1990), an optimizing native-code compiler for
Prolog. Aquarius Prolog adopts an execution
</listItem>
<bodyText confidence="0.930891428571429">
model with an instruction set that is fine-grained
and close to an instruction set of a real machine.
As a result, the output code can be optimized up
to the real-machine instruction level. In parti-
cular, we fully redesigned the AMAVL instruc-
tions as fine-grained instructions, which allow
extensive optimizations on compiled TFS code.
</bodyText>
<listItem confidence="0.986436333333333">
• Static code analysis. The types of variables
can be determined by analyzing the flow of data
within a program. The result of this dataflow
</listItem>
<table confidence="0.999109888888889">
Application fib(30) rev(1000)
System &apos; 10 times
LiLFeS (emulator-based) 106.4 48.7
SICStus WAM emulation 5.21 4.84
SICStus native-code 1.12 2.02
Aquarius Prolog 1.27 0.953
fib(30): Naïve calculation of Fibonacc&amp;quot;(30) = 1346269
rev(1000): Naive reverse of 1000-element list
(Environment: Sun UltraSparc 1/167MHz with 128MB memory)
</table>
<tableCaption confidence="0.997416">
Table 4 Performance Comparison to Prolog Systems
</tableCaption>
<bodyText confidence="0.995749333333333">
analysis will help to further optimize in the
compilation process.
These techniques are the basis of latest Prolog
systems. It is therefore expected that LiLFeS
augmented with these techniques becomes as effi-
cient as commercially available Prolog systems.
</bodyText>
<subsectionHeader confidence="0.992001">
5.2 Current Status of the LiLFeS Native-
code Compiler
</subsectionHeader>
<bodyText confidence="0.999958666666667">
We are developing the LiLFeS native-code com-
piler in LiLFeS itself. This is because the best
language that manipulates TFSs is LiLFeS; low-
level languages, such as C, are not appropriate for
TFS manipulation.
Currently all of the basic components have been
implemented. We are now working on further
code optimizations and implementation of built-in
functions on the native-code compiler.
</bodyText>
<subsectionHeader confidence="0.990549">
5.3 Performance Evaluation of the LiL-
FeS Native-Code Compiler
</subsectionHeader>
<bodyText confidence="0.9999716">
We evaluated the performance of the LiLFeS na-
tive-code compiler with the same experiments as
used in Section 4.2. The results of the experi-
ments are shown in Table 5 and Table 6.
The results of the native-code compiler are sig-
nificantly better than those of the emulator-based
LiLFeS system. In particular, comparison to
Prolog (Table 6) shows that the LiLFeS native-
code compiler achieves a speedup of 20 to 30
times compared to emulator-based LiLFeS, and
</bodyText>
<page confidence="0.988396">
810
</page>
<table confidence="0.9983403">
Grammar Simple Simple HPSG
System 1 answer 64 answers
LiLFeS native-code compiler 1.46 77.51 0.92k
LiLFeS system (WAM based) 5.70 322.4 2.56k
ALE on SICStus WAM emulation 225.60 10560 37.71*
ALE on SICStus native-code 67.05 3046 26.69*
ProFIT on SICStus WAM emulation 2.94 127.51 8.08
ProFIT on SICStus native-code 1.48 64.08 9.78
(Unit: seconds)
(See notes in Table 3 for environment and other notes)
</table>
<tableCaption confidence="0.997049">
Table 5 Performance Comparison of LiLFeS Native-Code Compiler to Other TFS Systems
</tableCaption>
<subsectionHeader confidence="0.666944">
(Some of the data is overlapped to Table 3)
</subsectionHeader>
<bodyText confidence="0.999900433333333">
approaches to the native-code compiler versions of
commercial Prolog systems. We can say that the
bottleneck of the emulator-based LiLFeS system is
effectively eliminated.
The result of the comparison to other TFS sys-
tems (Table 5) shows a speedup of 3-5 times from
the emulator-based LiLFeS. It is still slower than
ProFIT + SICStus native-code compiler in some
experiments, though the difference is very small.
We think the reason is the different traversing
order between ProFIT + SICStus (breadth-first)
and LiLFeS native-code compiler (depth-first)5.
What is notable in those experiments is that the
LiLFeS native code compiler shows a far better
performance in the &amp;quot;HPSG&amp;quot; experiment than all
other systems. Since the &amp;quot;HPSG&amp;quot; experiment
focuses on the efficiency of TFS handling, this
means that the native code compiler improves the
TFS handling capability.
We cannot yet perform the experiments on real-
world text parsing, because the implementation of
the native-code compiler is not completed. How-
ever, we can estimate the result from the experi-
ment result on emulator-based LiLFeS (350 milli-
seconds with sophisticated algorithm) and speed
ratio between emulator-based LiLFeS and native-
code compiler (3 to 5 times speed-up). The esti-
mated parsing time is 120ms — 70ms per sentence;
so we can say that we will be able to achieve our
goal of 100ms in the near future.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999978857142857">
We developed LiLFeS, a logic programming lan-
guage for TFSs. Using AMAVL emulator as a
core of the inference engine, the LiLFeS system
achieves high efficiency on complex TFSs. We
are now developing a native-code compiler ver-
sion of LiLFeS; the prototype showed a significant
speedup from the emulator-based version.
</bodyText>
<footnote confidence="0.559693333333333">
5 We confirmed in the separate experiments that execution
time of the &amp;quot;Simple&amp;quot; test varies up to 15% by changing the
traversing order.
</footnote>
<table confidence="0.992570555555556">
Application fib(30) rev(1000)
lystem 10 times
LiLFeS native-code compiler 2.45 2.29
LiLFeS (emulator-based) 106.4 48.7
SICStus WAM emulation 5.21 4.84
SICStus native-code 119 2.02
Asuarius Prolog 1.27 0.953
(Unit: seconds)
(See notes in Table 4 for environment and other notes)
</table>
<tableCaption confidence="0.845726">
Table 6 Performance Comparison of LiLFeS
Native-Code Compiler to Prolog Systems
</tableCaption>
<bodyText confidence="0.857458">
(Some of the data is overlapped to Table 4)
</bodyText>
<sectionHeader confidence="0.995872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99845475">
Malcino, Takalci et al. (1998) LiLFeS Home Page. Available
at http: //www. is . s .u-tokyo.ac. jp/-mak/lilfes/.
Carl Pollard and Ivan A. Sag (1994) Head-Driven Phrase
Structure Grammar. University of Chicago Press and CSLI
Publications.
Bob Carpenter (1992) The Logic of Typed Feature Structures.
Cambridge University Press.
Bob Carpenter and Yan Qu (1995) An abstract machine for
attribute-value logics. In IWPT &apos;95, pp. 59-70.
Gregor Erbach (1995) ProFIT—Prolog with Features, Inheri-
tance and Templates. In EACL &apos;95.
Bob Carpenter and Gerald Penn (1994) ALE 2.0 User&apos;s Guide.
Carnegie Mellon University Laboratory for Computational
Linguistics Technical Report.
Hassan AIt-Kaci (1991) Warren&apos;s Abstract Machine, A Tuto-
rial Reconstruction. The MIT Press.
Peter Lodewijk Van Roy (1990) Can logic programming
execute as fast as imperative programming? Technical
Report CSD-90-600, University of California, Berkeley.
Kentaro Torisawa and Jun&apos;ichi Tsujii (1996) Computing
phrasal-signs in HPSG prior to parsing. In COLING &apos;96,
pages 949-955.
Tateisi, Yuka et al. (1997) Conversion of LTAG English
Grammar to HPSG. IPSJ Report NL-122. (In Japanese).
The XTAG Research Group (1995) A Lexicalized Tree Ad-
joining Grammar for English. IRCS Research Report 95-
03, IRCS, University of Pennsylvania.
Mitsuishi Yutaka et al. (1998) HPSG-Style Underspecified
Japanese Grammar with Wide Coverage. To appear in
COLING-ACL &apos;98.
Hassan Ait-Kaci et al. (1994) Wild LIFE Handbook (prepub-
lication edition)., a User Manual. PRL Technical Note #2.
Shalom Wintner (1997) An Abstract Machine for Unification
Grammars — with Application to an HPSG Grammar for
Hebrew. Ph.D. thesis, the Technion - Israel Institute of
Technology.
</reference>
<page confidence="0.998361">
811
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989931">LiLFeS — Towards a Practical HPSG Parser *</title>
<author confidence="0.936582">MAKIN° Takaki</author>
<author confidence="0.936582">YOSHIDA Minoru</author>
<author confidence="0.936582">TORISAWA Kentaro</author>
<author confidence="0.936582">TSUJII Jun&apos;ichif</author>
<affiliation confidence="0.944867">Group, Department of Information Science, University of Tokyo</affiliation>
<address confidence="0.718089">Hongo 7-3-1, Bunkyo-Ku, Tokyo 113-0033, Japan</address>
<email confidence="0.506868">mak,mino,torisawa,tsujiilis.s.u-tokyo.ac.jp</email>
<affiliation confidence="0.351924">CCL, UMIST, U.K.</affiliation>
<abstract confidence="0.984513831081081">This paper presents the LiLFeS system, an efficient feature-structure description language for HPSG. The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics, proposed by Carpenter and Qu. Basic design policies, the current status, and performance evaluation of the LiLFeS system are described. The paper discusses two implementations of the LiLFeS. The first one is based on an emulator of the abstract machine, while the second one uses a native-code compiler and therefore is much more efficient than the first one. 1 Motivation Inefficiency is the major reason why the HPSG formalism (Pollard and Sag, 1993) has not been used for practical applications. However, one can claim that HPSG may not be so inefficient; it is just that an efficient implementation of HPSG has not been seriously pursued till now. We set a goal for the performance of our HPSG parser: 100 milliseconds of average parsing time on a sentence in real-world corpora. If our HPSG parser accomplished this goal, it would be capable to parse about 1,000,000 sentences in a day, and could be used for applications such as knowledge acquisition from corpora. 1.1 Existing Systems for Typed Feature Structures (TFSs) Since Typed Feature Structures (TFSs) (Carpenter, 1992) are the basic data structures in HPSG, the efficiency of handling TFSs has been considered as the key to improve the efficiency of an HPSG parser. There are two representative systems that handle TFSs&apos;: ALE (Carpenter and Penn, 1994), a TFS interpreter written in Prolog, and ProFIT * This research is partially funded by the project of Japan Society for the Promotion of Science (JSPS-RFTF96P00502). I LIFE (AIt-lcaci et al., 1994) is also famous, but we do not discuss it because it does not follow Carpenter&apos;s TFS definition. Moreover, our separate experiments show that LIFE is more than 10 times slower than emulator-based LiLFeS. As for AMALIA (Wintner, 1997), we cannot make experiments since it is not freely distributed. His experiments in his dissertation shows that AMALIA is 15 time faster than ALE at maximum; it is close to emulator-based LiLFeS, and is outperformed by native-code compiler of LiLFeS. (Erbach, 1995), a TFS-to-Prolog-term compiler. However, as the comparison of these systems with our system (Section 3.2) shows, neither of these two systems is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example, tends to consume too much memory for execution. It is also difficult, if not impossible, to combine them with other techniques like parallel parsing, etc., because these two systems have been embedded in Prolog. 1.2 Our Approach One of the promising directions of improving the efficiency of handling TFSs while retaining a necessary amount of flexibility is to take up the idea of AMAVL proposed in (Carpenter and Qu, 1995) to design a general programming system based on TFS. LiLFeS is a logic programming system thus designed and developed by our group, based on AMAVL implementation. LiLFeS can be characterized as follows. • Architecture based on an AMAVL implementation, which compiles a TFS into a sequence of abstract machine instructions, and performs unification of the TFS by emulating the execution of those instructions. Although the proposal of such an AMAVL was already made in 1995, no serious implementation has been reported. We believe that LiLFeS is the first serious treatment of the proposal. • Rich language specification: We have adopted a language syntax similar to Prolog. LiLFeS as a prograrruning language has almost the full capabilities of ordinary Prolog systems. Furthermore, we provide efficient built-in predicates that are often required in NLP applications, such as TFS copy, equivalence check, and associative arrays. • Independent language system: In order to develop an efficient and portable language system, we chose not to develop the language depending on an existing high-level language such as Prolog. Instead, we programmed the LiLFeS system from scratch. The independence also allows us to provide various built-in predicates in efficient ways. 1.3 Structure of This Paper Section 2 describes LiLFeS as a programming 807 my list &lt;- [loot]. e list &lt;- [my list]. ne_list &lt;- [my list] [FIRST\ list]. append(e_list, X, X). A &amp; REST\ X), Y, A &amp; REST\ Z) ) :append( X, Y, Z ). Figure 2 Sample LiLFeS Program language. Section 3 gives a brief description of the AMAVL we implemented, the core inference engine of the LiLFeS system. In Section 4, we discuss the current status of the LiLFeS system and the results of experiments on the system performance. Section 5 describes a native-code compiler we are currently developing on the LiL- FeS system, and discusses its performance. 2 LiLFeS as a Programming Language LiLFeS has basically the same syntax as Prolog, except that it uses TFSs instead of terms. Types and features must be defined before being used in TFS terms. 2 show the definition of the predicate ap— LiLFeS. The first paragraph contains the definitions of list, e_list, type example, is a subtype of the type my list, and has two approfeatures, value of feature restricted to the type list one of its subtypes. The type the universal type that subsumes all types. The rest of the program is definite clauses. As can see, the predicate represented TFSs instead of Prolog first-order 3 Abstract Machine for Attribute- Value Logics The Abstract Machine for Attribute-Value Logics (AMAVL) is the unification engine of the LiLFeS system. AMAVL provides (1) efficient representation of TFSs on the memory, and (2) compilation of TFSs into abstract machine codes. 3.1 Representation of a TFS on the Memory AMAVL, as does LiLFeS, requires all TFSs to be In other words, (1) the types and features should be explicitly declared, (2) appropriateness of specifications between types and features should be properly declared, and (3) all TFSs should follow these appropriateness specifications. Provided these requirements are satisfied, 2Note that LiLFeS has built-in list types as Prolog does.</abstract>
<note confidence="0.73388">This program is just an example to illustrate how a TFS is represented in LiLFeS 3For a more formal definition, see (Carpenter, 1992). Figure 1 Compiling TFS</note>
<abstract confidence="0.933638242038216">AMAVL efficiently represents a TFS in memory. The representation of a TFS on memory resembles the graph notation of a TFS; A node is repreby data cells, where n is the number of features outgoing from the node. The first data cell contains the type of the node, and the rest of the data cells contain pointers to the values of the corresponding features. The merit of this representation is that feature names need not be represented in the TFS representation on memory. The requirements on a TFS guarantee that the kinds and number of features are statically defined and constant for a given type, therefore we can determine the offset of the pointer to a given feature only by referring to the type of the given node. 3.2 TFS as an Instruction Sequence Unification is an operation defined between two TFSs. However, in most cases, one of the two TFSs is known in advance at compile-time. We can therefore compile the TFS into a sequence of specialized codes for unification, as illustrated in Figure 1, rather than using a general unification routine. The compiled unification codes are specialized for given specific TFSs and therefore much more efficient than a general unifier. This is because any general unifier has to traverse both TFSs each time unification occurs at run-time. Many studies have been reported for compiling unification of Prolog terms (for example, WAM (AIt-Kaci, 1991)). However, the TFS unification is much more complex than Prolog-term unification, because (1) unification between different types may succeed due to the existence of a type hierarchy, and (2) features must be merged in the fixed-offset TFS representation on memory. AMAVL compiles a type hierarchy and prepares for the complex situations described above. A TFS itself is compiled into a sequence of four of instructions: unification of types), of structuretraversing) and of These instructions refer to the compiled type hierarchy, if necessary. These operations are implemented following the tlo t2 Y Specialized Unifier U result U 808 (Parsing time per sentence, Unit: seconds) Our Goal 0.100 CYK-style 1.050 parser _ Parser based on Torisawa&apos;s algorithm 0.350 Condit&apos;on: 600 sentences from EDR Japanese corpus (average length 21 words), Average in the parsing of successful y parsed 539 sentences Environment: DEC Alpha 500/400MHz with 256MB memory Components Lines WAM/AMAVL Emulator 5,434 LiLFeS-to-WAM/AMAVL Compiler 6,091 Built-in Functions 9,530 TFS Display Routine 2,320 Others (Class Library etc.) 2,374 Total 25,749 Table 1 Source Code Lines of the LiLFeS System original proposal of AMAVL. We also added several other instructions in our AMAVL implementation, such as initialization of instructions for successive unifications and combined instructions for reducing overhead. 4 LiLFeS System The LiLFeS system is designed based on two abstract machines: AMAVL for TFS representation and unification procedures, and Warren&apos;s Abstract Machine (WAM) (AR-Kaci, 1991) for control of execution of definite clause programs. In this section we describe the current status of the LiLFeS system and applications running on it. Thereafter, we discuss the performance of LiLFeS in our experiments. 4.1 Current Status of the LiLFeS System The LiLFeS system is developed as a combination of AMAVL/WAM emulator, TFS compiler, and built-in support functions. They are all written in C++ with the source code of more than 25,000 lines (See Table 1). The source code can be compiled by GNU C++, and we have confirmed operation on Sun Sun0S4/Solaris, DEC Digital UNIX, and Microsoft Windows. We have several practical applications on the LiLFeS system. We currently have several different parsers for HPSG and HPSG grammars of Japanese and English, as follows: • A underspecified Japanese grammar developed our group (Mitsuisi, 1998). Lexicon consists of TFSs each of which has more than 100 nodes. The grammar can produce parse trees for 88% of the corpus of the real world texts (EDR Japanese corpus), 60% of which are given parse This grammar is used for the experiments in the next section. • XHPSG, An HPSG English grammar (Tateisi, 1997). The grammar is converted from the XTAG grammar (XTAG group, 1995), which has more than 300,000 lexical entries. • A naïve parser using a CYK-like algorithm. Although using a simple algorithm, the parser utilizes the full capabilities provided by LiLFeS, as built-in predicates (TFS copy, array op- The grammar does not contain semantic analysis such as coreference resolution. Table 2 Parsing Performance Evaluation with a Practical Grammar eration, etc.). • A parser based on the Torisawa&apos;s parsing algorithm (Torisawa and Tsujii, 1996). This algorithm compiles an HPSG grammar into 2 parts: its CFG skeletons and a remaining part, and parses a sentence in two phases. Although the parser is not a complete implementation of the algorithm, its efficiency benefits from its 2phase parsing, which reduces the amount of unification. These parsers and grammars are used for the performance evaluations in the next section. 4.2 Performance Evaluation We evaluated the performance of the LiLFeS system over three aspects: Parsing performance of LiLFeS, comparison to other TFS systems, and comparison to different Prolog systems. Table 2 shows the performance of HPSG parsers on a real-world corpus. However, even with the sophisticated algorithm, the parsing speed is 3.5 times slower than intended. To achieve our goal, we need a drastic improvement of a performance. We therefore performed the following experiments to find out the problem. Table 3 shows the performance comparison to other TFS systems, ALE and ProF1T. Two grammars are used in the experiments: &amp;quot;Simple&amp;quot; is a small HPSG-like grammar written by our group, while &amp;quot;HPSG&amp;quot; is the small-lexicon HPSG grammar distributed with the ALE package. In the &amp;quot;Simple&amp;quot; experiments, the LiLFeS system is far more efficient than ALE, but is outperformed by ProFTT. However, in the &amp;quot;HPSG&amp;quot; experiment, which has to handle much more complex TFSs than &amp;quot;Simple&amp;quot; experiments, LiLFeS is clearly better than ProFTT. the contrary, with simple data LiLFeS is relatively inefficient. Experiments in Table 4, which show comparisons to Prolog systems, show that the performance of LiLFeS is significantly worse than that of those Prolog systems.</abstract>
<note confidence="0.728407166666667">To summarize, the performance of LiLFeS is far more impressive when it has to handle complex TFSs. This fact indicates that the TFS engine in LiLFeS is efficient but that the other parts, i.e. the 809 Grammar Simple Simple HPSG S stem 1 answer 64 answers LiLFeS system (emulator-based) 5.70 322.4 ALE on SICStus WAM emulation 225.60 10560 ALE on SICStus native-code 67.05 3046 ProFIT on SICStus WAM emulation 2.94 127.51 8.08 ProFIT on SICStus native-code 1.48 64.08 9.78</note>
<abstract confidence="0.960043126436782">(Unit: seconds) Simple: a simple HPSG like grammar, parsed 1000 times by a bottom-up parser, 9-word sentence results in 1 parse-tree HPSG: a toy HPSG grammar distributed with ALE, parsed by a parser distributed with ProFIT, 14-word sentence results in 134 parse-trees *: ALE built-in parser is used instead of parser written in definite clauses f: The parser program is translated to avoid the &amp;quot;call&amp;quot; built-in, which contains some problems in the LiLFeS implementation (Environment: Sun UltraSparc 1/167MHz with 128MB memory) 3 Performance Comparison to Other TFS Systems seconds parts concerning LiLFeS as a general logic programming system, are not yet efficient enough. This means that, in order to improve the LiLFeS system as a whole, we have to include various optimization techniques already encoded in recent Prolog implementations. Thus we decided to redesign and optimize the whole system. The next section describes this optimized LiLFeS. 5 LiLFeS Native-Code Compiler We are currently developing a native-code compiler of LiLFeS in order to attain maximum performance. This section at first describes the design policies of the compiler, and then, describes the current status of implementation. The results of the performance evaluations on the native-code compiler are also presented. 5.1 Design Policies of the LiLFeS Native- Code Compiler The design policies for the LiLFeS native-code compiler are: • Native code output. We chose native-code compiling for optimal efficiency. Although this costs high for development, the resulting efficiency will compensate the cost. • Execution model close to a real machine. We designed the execution model by referring to the implementation of Aquarius Prolog (Van Roy, 1990), an optimizing native-code compiler for Prolog. Aquarius Prolog adopts an execution model with an instruction set that is fine-grained and close to an instruction set of a real machine. As a result, the output code can be optimized up to the real-machine instruction level. In particular, we fully redesigned the AMAVL instructions as fine-grained instructions, which allow extensive optimizations on compiled TFS code. • Static code analysis. The types of variables can be determined by analyzing the flow of data within a program. The result of this dataflow Application fib(30) rev(1000) System &apos; 10 times LiLFeS (emulator-based) 106.4 48.7 SICStus WAM emulation 5.21 4.84 SICStus native-code 1.12 2.02 Aquarius Prolog 1.27 0.953 Naïve calculation Fibonacc&amp;quot;(30) = Naive reverse 1000-element list Sun UltraSparc 1/167MHz with 128MB 4 Performance Comparison to Prolog analysis will help to further optimize in the compilation process. These techniques are the basis of latest Prolog systems. It is therefore expected that LiLFeS augmented with these techniques becomes as efficient as commercially available Prolog systems. 5.2 Current Status of the LiLFeS Nativecode Compiler We are developing the LiLFeS native-code compiler in LiLFeS itself. This is because the best language that manipulates TFSs is LiLFeS; lowlevel languages, such as C, are not appropriate for TFS manipulation. Currently all of the basic components have been implemented. We are now working on further code optimizations and implementation of built-in functions on the native-code compiler. 5.3 Performance Evaluation of the LiL- FeS Native-Code Compiler We evaluated the performance of the LiLFeS native-code compiler with the same experiments as used in Section 4.2. The results of the experiments are shown in Table 5 and Table 6. The results of the native-code compiler are significantly better than those of the emulator-based LiLFeS system. In particular, comparison to Prolog (Table 6) shows that the LiLFeS nativecode compiler achieves a speedup of 20 to 30 times compared to emulator-based LiLFeS, and</abstract>
<note confidence="0.8404546">810 Grammar Simple Simple HPSG System 1 answer 64 answers LiLFeS native-code compiler 1.46 77.51 LiLFeS system (WAM based) 5.70 322.4 ALE on SICStus WAM emulation 225.60 10560 ALE on SICStus native-code 67.05 3046 ProFIT on SICStus WAM emulation 2.94 127.51 8.08 ProFIT on SICStus native-code 1.48 64.08 9.78 (Unit: seconds) (See notes in Table 3 for environment and other notes) Table 5 Performance Comparison of LiLFeS Native-Code Compiler to Other TFS Systems (Some of the data is overlapped to Table 3) approaches to the native-code compiler versions of commercial Prolog systems. We can say that the</note>
<abstract confidence="0.999535205128205">bottleneck of the emulator-based LiLFeS system is effectively eliminated. The result of the comparison to other TFS systems (Table 5) shows a speedup of 3-5 times from the emulator-based LiLFeS. It is still slower than ProFIT + SICStus native-code compiler in some experiments, though the difference is very small. We think the reason is the different traversing order between ProFIT + SICStus (breadth-first) LiLFeS native-code compiler What is notable in those experiments is that the LiLFeS native code compiler shows a far better performance in the &amp;quot;HPSG&amp;quot; experiment than all other systems. Since the &amp;quot;HPSG&amp;quot; experiment focuses on the efficiency of TFS handling, this means that the native code compiler improves the TFS handling capability. We cannot yet perform the experiments on realworld text parsing, because the implementation of the native-code compiler is not completed. However, we can estimate the result from the experiment result on emulator-based LiLFeS (350 milliseconds with sophisticated algorithm) and speed ratio between emulator-based LiLFeS and nativecode compiler (3 to 5 times speed-up). The estimated parsing time is 120ms — 70ms per sentence; so we can say that we will be able to achieve our goal of 100ms in the near future. 6 Conclusion We developed LiLFeS, a logic programming language for TFSs. Using AMAVL emulator as a core of the inference engine, the LiLFeS system achieves high efficiency on complex TFSs. We are now developing a native-code compiler version of LiLFeS; the prototype showed a significant speedup from the emulator-based version. 5We confirmed in the separate experiments that execution time of the &amp;quot;Simple&amp;quot; test varies up to 15% by changing the traversing order.</abstract>
<note confidence="0.830082288888889">Application fib(30) rev(1000) lystem 10 times LiLFeS native-code compiler 2.45 2.29 LiLFeS (emulator-based) 106.4 48.7 SICStus WAM emulation 5.21 4.84 SICStus native-code 2.02 Asuarius Prolog 1.27 0.953 (Unit: seconds) (See notes in Table 4 for environment and other notes) Table 6 Performance Comparison of LiLFeS Native-Code Compiler to Prolog Systems (Some of the data is overlapped to Table 4) References Malcino, Takalci et al. (1998) LiLFeS Home Page. Available //www. is . s Carl Pollard and Ivan A. Sag (1994) Head-Driven Phrase Structure Grammar. University of Chicago Press and CSLI Publications. Bob Carpenter (1992) The Logic of Typed Feature Structures. Cambridge University Press. Bob Carpenter and Yan Qu (1995) An abstract machine for attribute-value logics. In IWPT &apos;95, pp. 59-70. Gregor Erbach (1995) ProFIT—Prolog with Features, Inheritance and Templates. In EACL &apos;95. Bob Carpenter and Gerald Penn (1994) ALE 2.0 User&apos;s Guide. Carnegie Mellon University Laboratory for Computational Linguistics Technical Report. Hassan AIt-Kaci (1991) Warren&apos;s Abstract Machine, A Tutorial Reconstruction. The MIT Press. Peter Lodewijk Van Roy (1990) Can logic programming execute as fast as imperative programming? Technical Report CSD-90-600, University of California, Berkeley. Kentaro Torisawa and Jun&apos;ichi Tsujii (1996) Computing phrasal-signs in HPSG prior to parsing. In COLING &apos;96, pages 949-955. Tateisi, Yuka et al. (1997) Conversion of LTAG English Grammar to HPSG. IPSJ Report NL-122. (In Japanese). The XTAG Research Group (1995) A Lexicalized Tree Adjoining Grammar for English. IRCS Research Report 95- 03, IRCS, University of Pennsylvania. Mitsuishi Yutaka et al. (1998) HPSG-Style Underspecified Japanese Grammar with Wide Coverage. To appear in COLING-ACL &apos;98. Hassan Ait-Kaci et al. (1994) Wild LIFE Handbook (prepublication edition)., a User Manual. PRL Technical Note #2.</note>
<title confidence="0.647716">Shalom Wintner (1997) An Abstract Machine for Unification Grammars — with Application to an HPSG Grammar for Hebrew. Ph.D. thesis, the Technion - Israel Institute of</title>
<note confidence="0.4700235">Technology. 811</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Takalci Malcino</author>
</authors>
<title>LiLFeS Home Page. Available at http: //www. is . s .u-tokyo.ac.</title>
<date>1998</date>
<tech>jp/-mak/lilfes/.</tech>
<marker>Malcino, 1998</marker>
<rawString>Malcino, Takalci et al. (1998) LiLFeS Home Page. Available at http: //www. is . s .u-tokyo.ac. jp/-mak/lilfes/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press and CSLI Publications.</publisher>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag (1994) Head-Driven Phrase Structure Grammar. University of Chicago Press and CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1562" citStr="Carpenter, 1992" startWordPosition="237" endWordPosition="238">sed for practical applications. However, one can claim that HPSG may not be so inefficient; it is just that an efficient implementation of HPSG has not been seriously pursued till now. We set a goal for the performance of our HPSG parser: 100 milliseconds of average parsing time on a sentence in real-world corpora. If our HPSG parser accomplished this goal, it would be capable to parse about 1,000,000 sentences in a day, and could be used for applications such as knowledge acquisition from corpora. 1.1 Existing Systems for Typed Feature Structures (TFSs) Since Typed Feature Structures (TFSs) (Carpenter, 1992) are the basic data structures in HPSG, the efficiency of handling TFSs has been considered as the key to improve the efficiency of an HPSG parser. There are two representative systems that handle TFSs&apos;: ALE (Carpenter and Penn, 1994), a TFS interpreter written in Prolog, and ProFIT * This research is partially funded by the project of Japan Society for the Promotion of Science (JSPS-RFTF96P00502). I LIFE (AIt-lcaci et al., 1994) is also famous, but we do not discuss it because it does not follow Carpenter&apos;s TFS definition. Moreover, our separate experiments show that LIFE is more than 10 time</context>
<context position="6794" citStr="Carpenter, 1992" startWordPosition="1104" endWordPosition="1105">FSs into abstract machine codes. 3.1 Representation of a TFS on the Memory AMAVL, as does LiLFeS, requires all TFSs to be totally well-typed3. In other words, (1) the types and features should be explicitly declared, (2) appropriateness of specifications between types and features should be properly declared, and (3) all TFSs should follow these appropriateness specifications. Provided these requirements are satisfied, 2 Note that LiLFeS has built-in list types as Prolog does. This program is just an example to illustrate how a TFS is represented in LiLFeS 3 For a more formal definition, see (Carpenter, 1992). Figure 1 Compiling TFS AMAVL efficiently represents a TFS in memory. The representation of a TFS on memory resembles the graph notation of a TFS; A node is represented by n+1 continuous data cells, where n is the number of features outgoing from the node. The first data cell contains the type of the node, and the rest of the data cells contain pointers to the values of the corresponding features. The merit of this representation is that feature names need not be represented in the TFS representation on memory. The requirements on a TFS guarantee that the kinds and number of features are stat</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter (1992) The Logic of Typed Feature Structures. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
<author>Yan Qu</author>
</authors>
<title>An abstract machine for attribute-value logics.</title>
<date>1995</date>
<booktitle>In IWPT &apos;95,</booktitle>
<pages>59--70</pages>
<contexts>
<context position="3254" citStr="Carpenter and Qu, 1995" startWordPosition="510" endWordPosition="513">tems is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example, tends to consume too much memory for execution. It is also difficult, if not impossible, to combine them with other techniques like parallel parsing, etc., because these two systems have been embedded in Prolog. 1.2 Our Approach One of the promising directions of improving the efficiency of handling TFSs while retaining a necessary amount of flexibility is to take up the idea of AMAVL proposed in (Carpenter and Qu, 1995) to design a general programming system based on TFS. LiLFeS is a logic programming system thus designed and developed by our group, based on AMAVL implementation. LiLFeS can be characterized as follows. • Architecture based on an AMAVL implementation, which compiles a TFS into a sequence of abstract machine instructions, and performs unification of the TFS by emulating the execution of those instructions. Although the proposal of such an AMAVL was already made in 1995, no serious implementation has been reported. We believe that LiLFeS is the first serious treatment of the proposal. • Rich la</context>
</contexts>
<marker>Carpenter, Qu, 1995</marker>
<rawString>Bob Carpenter and Yan Qu (1995) An abstract machine for attribute-value logics. In IWPT &apos;95, pp. 59-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Erbach</author>
</authors>
<title>ProFIT—Prolog with Features, Inheritance and Templates.</title>
<date>1995</date>
<booktitle>In EACL &apos;95.</booktitle>
<contexts>
<context position="2493" citStr="Erbach, 1995" startWordPosition="389" endWordPosition="390">d by the project of Japan Society for the Promotion of Science (JSPS-RFTF96P00502). I LIFE (AIt-lcaci et al., 1994) is also famous, but we do not discuss it because it does not follow Carpenter&apos;s TFS definition. Moreover, our separate experiments show that LIFE is more than 10 times slower than emulator-based LiLFeS. As for AMALIA (Wintner, 1997), we cannot make experiments since it is not freely distributed. His experiments in his dissertation shows that AMALIA is 15 time faster than ALE at maximum; it is close to emulator-based LiLFeS, and is outperformed by native-code compiler of LiLFeS. (Erbach, 1995), a TFS-to-Prolog-term compiler. However, as the comparison of these systems with our system (Section 3.2) shows, neither of these two systems is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example, tends to consume too much memory for execution. It is also difficult, if not impossible, to combine them with other techniques like parallel parsing, etc., because these two systems have been embedded in Prolog. 1.2 Our Approach One of the promising directions of</context>
</contexts>
<marker>Erbach, 1995</marker>
<rawString>Gregor Erbach (1995) ProFIT—Prolog with Features, Inheritance and Templates. In EACL &apos;95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
<author>Gerald Penn</author>
</authors>
<date>1994</date>
<journal>ALE</journal>
<tech>Technical Report.</tech>
<volume>2</volume>
<institution>User&apos;s Guide. Carnegie Mellon University Laboratory for Computational Linguistics</institution>
<contexts>
<context position="1796" citStr="Carpenter and Penn, 1994" startWordPosition="274" endWordPosition="277"> HPSG parser: 100 milliseconds of average parsing time on a sentence in real-world corpora. If our HPSG parser accomplished this goal, it would be capable to parse about 1,000,000 sentences in a day, and could be used for applications such as knowledge acquisition from corpora. 1.1 Existing Systems for Typed Feature Structures (TFSs) Since Typed Feature Structures (TFSs) (Carpenter, 1992) are the basic data structures in HPSG, the efficiency of handling TFSs has been considered as the key to improve the efficiency of an HPSG parser. There are two representative systems that handle TFSs&apos;: ALE (Carpenter and Penn, 1994), a TFS interpreter written in Prolog, and ProFIT * This research is partially funded by the project of Japan Society for the Promotion of Science (JSPS-RFTF96P00502). I LIFE (AIt-lcaci et al., 1994) is also famous, but we do not discuss it because it does not follow Carpenter&apos;s TFS definition. Moreover, our separate experiments show that LIFE is more than 10 times slower than emulator-based LiLFeS. As for AMALIA (Wintner, 1997), we cannot make experiments since it is not freely distributed. His experiments in his dissertation shows that AMALIA is 15 time faster than ALE at maximum; it is clos</context>
</contexts>
<marker>Carpenter, Penn, 1994</marker>
<rawString>Bob Carpenter and Gerald Penn (1994) ALE 2.0 User&apos;s Guide. Carnegie Mellon University Laboratory for Computational Linguistics Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan AIt-Kaci</author>
</authors>
<title>Warren&apos;s Abstract Machine, A Tutorial Reconstruction.</title>
<date>1991</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8236" citStr="AIt-Kaci, 1991" startWordPosition="1350" endWordPosition="1351">on defined between two TFSs. However, in most cases, one of the two TFSs is known in advance at compile-time. We can therefore compile the TFS into a sequence of specialized codes for unification, as illustrated in Figure 1, rather than using a general unification routine. The compiled unification codes are specialized for given specific TFSs and therefore much more efficient than a general unifier. This is because any general unifier has to traverse both TFSs each time unification occurs at run-time. Many studies have been reported for compiling unification of Prolog terms (for example, WAM (AIt-Kaci, 1991)). However, the TFS unification is much more complex than Prolog-term unification, because (1) unification between different types may succeed due to the existence of a type hierarchy, and (2) features must be merged in the fixed-offset TFS representation on memory. AMAVL compiles a type hierarchy and prepares for the complex situations described above. A TFS itself is compiled into a sequence of four kinds of instructions: ADDNEW (the unification of TFS types), UNIFYVAR (creation of structuresharing), PUSH (feature traversing) and POP (end of PUSH block). These instructions refer to the compi</context>
</contexts>
<marker>AIt-Kaci, 1991</marker>
<rawString>Hassan AIt-Kaci (1991) Warren&apos;s Abstract Machine, A Tutorial Reconstruction. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Lodewijk Van Roy</author>
</authors>
<title>Can logic programming execute as fast as imperative programming?</title>
<date>1990</date>
<tech>Technical Report CSD-90-600,</tech>
<institution>University of California, Berkeley.</institution>
<marker>Van Roy, 1990</marker>
<rawString>Peter Lodewijk Van Roy (1990) Can logic programming execute as fast as imperative programming? Technical Report CSD-90-600, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Computing phrasal-signs in HPSG prior to parsing.</title>
<date>1996</date>
<booktitle>In COLING &apos;96,</booktitle>
<pages>949--955</pages>
<contexts>
<context position="11633" citStr="Torisawa and Tsujii, 1996" startWordPosition="1879" endWordPosition="1882">he experiments in the next section. • XHPSG, An HPSG English grammar (Tateisi, 1997). The grammar is converted from the XTAG grammar (XTAG group, 1995), which has more than 300,000 lexical entries. • A naïve parser using a CYK-like algorithm. Although using a simple algorithm, the parser utilizes the full capabilities provided by LiLFeS, such as built-in predicates (TFS copy, array opThe grammar does not contain semantic analysis such as coreference resolution. Table 2 Parsing Performance Evaluation with a Practical Grammar eration, etc.). • A parser based on the Torisawa&apos;s parsing algorithm (Torisawa and Tsujii, 1996). This algorithm compiles an HPSG grammar into 2 parts: its CFG skeletons and a remaining part, and parses a sentence in two phases. Although the parser is not a complete implementation of the algorithm, its efficiency benefits from its 2- phase parsing, which reduces the amount of unification. These parsers and grammars are used for the performance evaluations in the next section. 4.2 Performance Evaluation We evaluated the performance of the LiLFeS system over three aspects: Parsing performance of LiLFeS, comparison to other TFS systems, and comparison to different Prolog systems. Table 2 sh</context>
</contexts>
<marker>Torisawa, Tsujii, 1996</marker>
<rawString>Kentaro Torisawa and Jun&apos;ichi Tsujii (1996) Computing phrasal-signs in HPSG prior to parsing. In COLING &apos;96, pages 949-955.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuka Tateisi</author>
</authors>
<date>1997</date>
<journal>Conversion of LTAG English Grammar</journal>
<note>to HPSG. IPSJ Report NL-122. (In Japanese).</note>
<contexts>
<context position="11091" citStr="Tateisi, 1997" startWordPosition="1797" endWordPosition="1798">s, DEC Digital UNIX, and Microsoft Windows. We have several practical applications on the LiLFeS system. We currently have several different parsers for HPSG and HPSG grammars of Japanese and English, as follows: • A underspecified Japanese grammar developed by our group (Mitsuisi, 1998). Lexicon consists of TFSs each of which has more than 100 nodes. The grammar can produce parse trees for 88% of the corpus of the real world texts (EDR Japanese corpus), 60% of which are given correct parse trees4. This grammar is used for the experiments in the next section. • XHPSG, An HPSG English grammar (Tateisi, 1997). The grammar is converted from the XTAG grammar (XTAG group, 1995), which has more than 300,000 lexical entries. • A naïve parser using a CYK-like algorithm. Although using a simple algorithm, the parser utilizes the full capabilities provided by LiLFeS, such as built-in predicates (TFS copy, array opThe grammar does not contain semantic analysis such as coreference resolution. Table 2 Parsing Performance Evaluation with a Practical Grammar eration, etc.). • A parser based on the Torisawa&apos;s parsing algorithm (Torisawa and Tsujii, 1996). This algorithm compiles an HPSG grammar into 2 parts: it</context>
</contexts>
<marker>Tateisi, 1997</marker>
<rawString>Tateisi, Yuka et al. (1997) Conversion of LTAG English Grammar to HPSG. IPSJ Report NL-122. (In Japanese).</rawString>
</citation>
<citation valid="true">
<title>A Lexicalized Tree Adjoining Grammar for English.</title>
<date>1995</date>
<booktitle>The XTAG Research Group</booktitle>
<tech>IRCS Research Report 95-03,</tech>
<institution>IRCS, University of Pennsylvania.</institution>
<marker>1995</marker>
<rawString>The XTAG Research Group (1995) A Lexicalized Tree Adjoining Grammar for English. IRCS Research Report 95-03, IRCS, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitsuishi Yutaka</author>
</authors>
<title>HPSG-Style Underspecified Japanese Grammar with Wide Coverage.</title>
<date>1998</date>
<note>To appear in COLING-ACL &apos;98.</note>
<marker>Yutaka, 1998</marker>
<rawString>Mitsuishi Yutaka et al. (1998) HPSG-Style Underspecified Japanese Grammar with Wide Coverage. To appear in COLING-ACL &apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Ait-Kaci</author>
</authors>
<title>Wild LIFE Handbook (prepublication edition)., a User Manual.</title>
<date>1994</date>
<tech>PRL Technical Note #2.</tech>
<marker>Ait-Kaci, 1994</marker>
<rawString>Hassan Ait-Kaci et al. (1994) Wild LIFE Handbook (prepublication edition)., a User Manual. PRL Technical Note #2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Wintner</author>
</authors>
<title>An Abstract Machine for Unification Grammars — with Application to an HPSG Grammar for Hebrew.</title>
<date>1997</date>
<booktitle>Ph.D. thesis, the Technion -</booktitle>
<institution>Israel Institute of Technology.</institution>
<contexts>
<context position="2228" citStr="Wintner, 1997" startWordPosition="347" endWordPosition="348">cy of handling TFSs has been considered as the key to improve the efficiency of an HPSG parser. There are two representative systems that handle TFSs&apos;: ALE (Carpenter and Penn, 1994), a TFS interpreter written in Prolog, and ProFIT * This research is partially funded by the project of Japan Society for the Promotion of Science (JSPS-RFTF96P00502). I LIFE (AIt-lcaci et al., 1994) is also famous, but we do not discuss it because it does not follow Carpenter&apos;s TFS definition. Moreover, our separate experiments show that LIFE is more than 10 times slower than emulator-based LiLFeS. As for AMALIA (Wintner, 1997), we cannot make experiments since it is not freely distributed. His experiments in his dissertation shows that AMALIA is 15 time faster than ALE at maximum; it is close to emulator-based LiLFeS, and is outperformed by native-code compiler of LiLFeS. (Erbach, 1995), a TFS-to-Prolog-term compiler. However, as the comparison of these systems with our system (Section 3.2) shows, neither of these two systems is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example</context>
</contexts>
<marker>Wintner, 1997</marker>
<rawString>Shalom Wintner (1997) An Abstract Machine for Unification Grammars — with Application to an HPSG Grammar for Hebrew. Ph.D. thesis, the Technion - Israel Institute of Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>