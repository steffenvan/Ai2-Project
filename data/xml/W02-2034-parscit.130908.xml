<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000857">
<title confidence="0.896799">
Learning to Disambiguate Potentially Subjective Expressions
</title>
<author confidence="0.940194">
Janyce Wiebe and Theresa Wilson
</author>
<affiliation confidence="0.998188">
Department of Computer Science and Intelligent Systems Program
University of Pittsburgh
</affiliation>
<address confidence="0.557578">
Pittsburgh, PA, USA, 15260
</address>
<sectionHeader confidence="0.864593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934444444444">
The goal of this work is recognizing opinionated
and evaluative (subjective) language in text.
The ability to recognize such language would
be beneficial for many NLP applications such
as question answering, information extraction,
summarization, and genre detection. This pa-
per focuses on disambiguating potentially sub-
jective expressions in context, based on the den-
sity of other clues in the surrounding text.
</bodyText>
<sectionHeader confidence="0.995509" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968803921569">
The goal of this work is to recognize opinion-
ated and evaluative (subjective) language in text
(Banfield, 1982). The ability to recognize sub-
jective language would be beneficial for NLP ap-
plications such as question answering, informa-
tion extraction, and genre detection.
Recent work by Wiebe and colleagues (2001b;
2001a; 2000) focused on learning potentially
subjective expressions from corpora. This paper
focuses on the mutual disambiguation of such
features in context. Many natural language ex-
pressions have both subjective and objective us-
ages, so a problem for recognizing subjective
language is determining when instances of ex-
pressions are indeed subjective in the context in
which they appear. We have discovered that the
density of other potential clues in the surround-
ing context is a strong influence: if a sufficient
number of other clues are nearby, a clue is more
likely to be subjective than if there are not.
There are two parameters to this process, corre-
sponding to &amp;quot;sufficient number&amp;quot; and &amp;quot;nearby.&amp;quot;
Values for these parameters are chosen indepen-
dently using manual annotations of subjective
expressions in a mixture of Wall Street Journal
(WSJ) and newsgroup (NG) data (Section 5).
The selected density features are evaluated
with respect to document-level classes in WSJ
data (Section 6). All of the parameters chosen
using the manual annotations result in increases
in precision over baseline in the test data, and
the majority of the increases are large.
The document-level classes are identified by
the WSJ itself: Editorials, Letters to the Ed-
itor, Arts 6 Leisure Reviews, and Viewpoints;
together, we call these opinion pieces. With
this data, we are not restricted to the confines
of corpora manually annotated in detail, which
is necessarily small. To assess the subjectiv-
ity of the sentences being recognized, Section 7
presents the results of an annotation study in
which sentences identified automatically using
density features are manually annotated by two
judges. Agreement is high for sentences classi-
fied with certainty, and most of the sentences
are classified as subjective by both judges, or
are near sentences that are.
This work is also an interesting case study
of using data annotated at different levels, and
exploiting existing document-level annotations
to learn linguistic knowledge.
</bodyText>
<sectionHeader confidence="0.975771" genericHeader="introduction">
2 Subjectivity
</sectionHeader>
<bodyText confidence="0.999962301587302">
Subjectivity in natural language refers to aspects
of language used to express emotion, evaluation,
opinion and speculation. In this work, we adopt
the annotation scheme of (Wiebe et al., 1999).
Under that scheme, a sentence is subjective if
it contains a significant expression of emotion,
evaluation, opinion, or speculation, attributed
to either the writer or someone mentioned in
the text. Otherwise, the sentence is objective.
In (Wiebe et al., 1999), multiple judges anno-
tated a corpus with subjective/objective classi-
fications, rating the certainty of their answers
on a scale from 0 to 3. For the 85% of the corpus
for which the certainty ratings of the judges was
2 or 3, the average pairwise Kappa value was
0.80. Thus, when the annotators are certain of
their answers, which they are for the majority
of sentences, their agreement is high. Wiebe et
al. (1999) developed a classifier to perform sub-
jectivity tagging using this data, with good re-
sults in 10-fold cross validation experiments (an
average accuracy 20 percentage points higher
than baseline for all sentences and 30 percent-
age points higher on the sentences for which the
annotators&apos; certainty ratings were 2 or 3). In
the current paper, as described below, this data
is further annotated at the expression level and
used for training data to choose density param-
eters.
Table 1 shows examples of subjective and
objective sentences from the annotation study
presented in Section 7. Sentences classified by
both judges as objective are marked &amp;quot;oo&amp;quot; and
those classified by both judges as subjective are
marked &amp;quot;ss&amp;quot;.
Subjectivity analysis could be exploited in
many NLP applications, recognizing inflamma-
tory messages (Spertus, 1997), genre detection
and document routing (Kessler et al., 1997), in-
tellectual attribution in text (Teufel and Moens,
2000), generation and style (Hovy, 1987), ques-
tion answering from multiple perspectives, and
any other application that would benefit from
knowledge of how opinionated language is, and
whether or not the writer purports to objec-
tively present factual material. An information
extraction or summarization system, for exam-
ple, would benefit from distinguishing sentences
intended to present facts from those intended to
present opinions, since many such systems are
meant to extract only facts.
One aspect of subjectivity is highlighted in
this paper. Although some expressions, such as
!, are subjective in all contexts, many may or
may not be subjective, depending on the con-
text in which they appear. A potential subjec-
tive element (PSE) is a linguistic word or ex-
pression that may be used to express subjectiv-
ity. A subjective element is an instance of a po-
tential subjective element, in a particular con-
text, that is indeed subjective in that context
(Wiebe, 1994). This paper focuses on identify-
ing PSE instances that are subjective elements.
</bodyText>
<sectionHeader confidence="0.964891" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999395655737705">
We use training data from (Wiebe et al., 1999;
Wiebe et al., 2001b; Wiebe et al., 2001a) con-
sisting of corpora annotated at the expression
level. In expression-level subjectivity tagging,
the judges first identify the sentences they be-
lieve are subjective. They next identify the sub-
jective elements in the sentence, i.e., the expres-
sions they feel are responsible for the subjec-
tive classification. For example, an annotator
marked two subjective elements in the follow-
ing sentence (indicated with parentheses): They
paid (yet) more for (really good stuff).
Two WSJ datasets, 500 sentences each, were
annotated by two judges, resulting in four sets
of annotations (SE1, SE2, SE3, SEX). In ad-
dition, a newsgroup dataset of 1,132 sentences
was annotated by one judge (SE5).
Document-level opinion-piece data is used as
test data to evaluate the density features (in
Section 6). Recall that the class opinion piece
is the union of Editorial, Letter to the Editor,
Arts 6 Leisure Review, and Viewpoint in the
WSJ. An inspection of some data revealed that
some editorials are not marked as such. Thus,
the opinion-piece data used for evaluation in
this work has been manually refined. The an-
notation instructions are simply to identify any
additional opinion pieces that are not marked
as such. To test the reliability of this annota-
tion, two judges independently annotated two
editions of the WSJ, each approximately 160K
words. This is an &amp;quot;annotation lite&amp;quot; task: with
no training, the annotators achieved Kappa val-
ues of 0.94 and 0.95, and spent an average of
three hours per WSJ edition.
Two datasets (0P1 and 0P2) of four WSJ
editions each were manually annotated as de-
scribed in the previous paragraph. OP1 has a
total of 1,232 articles and 640,975 words, and
0P2 has a total of 1,222 documents and 629,690
words. All instances in OP1 and 0P2 of the
PSEs described in Section 4 were identified. All
training to define the PSE instances in OP1 was
performed on data separate from OP1, and all
training to define the PSE instances in 0P2 was
performed on data separate from 0P2.
Note that opinion-piece data, used as test
data in our evaluations, is noisy. Although
the ratio of subjective to objective sentences
is higher in opinion-piece documents, there are
(1) The outburst of shooting came nearly two weeks after clashes between Moslem worshippers and oo
Somali soldiers.
But now the refugees are streaming across the border and alarming the world. ss
In the middle of the crisis, Erich Honecker was hospitalized with a gall stone operation. oo
It is becoming more and more obvious that his gallstone-age communism is dying with him: ... ss
Not brilliantly, because, after all, this was a performer who was collecting paychecks from lounges ss
at Hiltons and Holiday Inns, but creditably and with the air of someone for whom
&amp;quot;Ten Cents a Dance&amp;quot; was more than a bit autobiographical.
&amp;quot;It was an exercise of blending Michelle&apos;s singing with Susie&apos;s singing,&amp;quot; explained Ms. Stevens. oo
Enlisted men and lower-grade officers were meat thrown into a grinder. ss
&amp;quot;If you believe in God and you believe in miracles, there&apos;s nothing particularly crazy about that.&amp;quot; ss
</bodyText>
<tableCaption confidence="0.99702">
Table 1: Examples from the annotation study in Section 7.
</tableCaption>
<bodyText confidence="0.768304">
objective sentences in opinion-piece documents,
and subjective sentences in the other kinds of
documents.
</bodyText>
<sectionHeader confidence="0.936173" genericHeader="method">
4 PSEs Used
</sectionHeader>
<bodyText confidence="0.941115575757576">
We use PSE features automatically learned
from corpora. The first is a word appearing
just once in the corpus (i.e., a unique word).
Interestingly, the set of all unique words in a
corpus is a high frequency set, with higher than
baseline precision (Wiebe et al., 2001a). This
feature does not require training.
The next two types of PSE are adjectives and
verbs identified using the results of a method
for clustering words according to distributional
similarity (Lin, 1998), as described in (Wiebe,
2000). Distributional similarity has been used
to find similar words in text. The hypothe-
sis behind its use in (Wiebe, 2000) was that
words may be distributionally similar because
they are both potentially subjective (e.g., tragic,
sad, and poignant are identified from bizarre).
The remaining types of PSEs are collocations,
learned from data that is manually annotated
at the expression level with subjective elements.
Roughly speaking, a collocation is judged to be
a PSE if its precision is greater than the maxi-
mum precision of its constituents (Wiebe et al.,
2001a).
Fixed-n-grams are sequences of n wordlpart-
of-speech pairs. Examples from test data OP1
are: a sort of, as he be, be it that, have to pay,
he be a, it be time, it should be, of the century,
rest of us, seem to be, the kind of, the middle
of, the other hand, the quality of, to do so, to
say about.
In ugen-n-grams, one or more of the words is
U, which matches any word that is unique in
</bodyText>
<table confidence="0.996514181818182">
Freq Prec %incPrec
unique words 8288 .32 100%
adjectives 4610 .34 113%
verbs 8862 .25 56%
fixed-2-grams 7584 .22 38%
fixed-3-grams 908 .23 44%
fixed-4-grams 61 .26 63%
ugen-2-grams 407 .43 169%
ugen-3-grams 203 .42 163%
ugen-4-grams 15 .47 194%
baseline 640975 .16
</table>
<tableCaption confidence="0.996139">
Table 2: Results for PSEs in test data OP1
</tableCaption>
<bodyText confidence="0.948980535714286">
the test data. Two examples are (highlyladverb
Uladj) and (LTI adj tolprep Ulverb). Instances
in OP1 matching the first include highly un-
satisfactory, highly unorthodox, highly talented,
highly conjectural, highly erotic. Instances in
OP1 matching the second include: impervious
to reason, strange to celebrate, wise to temper.
Table 2 gives results for the PSEs described
above on test data OP1 (all training was done
on separate data). The precision of a set S
with respect to opinion pieces is the propor-
tion of members of S that appear in opinion
pieces. The baseline precision of .16 appearing
at the bottom of the table is the proportion of
all words in the corpus that appear in opinion
pieces. For each type of feature, Table 2 gives
frequencies (in column Freq), precisions (in col-
umn Prec), and percentage increases in preci-
sion over baseline (column %incPrec). For ex-
ample, the first row gives results for the set of
unique words. There are 8,288 unique words in
OP1. The precision of that set is .32, which is
a 100% improvement over the baseline precision
of .16.
The baseline precision in Table 2 is low be-
0. PSEs = all adjs, verbs, modals, nouns, and
adverbs that appear at least once in an SE
(except not, will, be, have).
</bodyText>
<listItem confidence="0.973316375">
1. PSEinsts = the set of all instances of PSEs
2. HiDensity = {}
3. For P in PSEiusts:
4. leftWin(P) = the 147 words before P
5. rightWin(P) = the 147 words after P
6. density(P) = # of SEs whose first or last
word is in leftWin(P) or rightWin(P)
7. if density(P) &gt; T:
</listItem>
<figure confidence="0.92186575">
HiDensity = HiDensity U {P}
IP SEinst,1
# of HiDensity in SEs
9. prec(HiDensity)=
</figure>
<figureCaption confidence="0.9712905">
Figure 1: Algorithm for calculating density in
subjective element (SE) data
</figureCaption>
<bodyText confidence="0.940447">
cause the distribution is highly skewed in favor
of non-opinion pieces.
</bodyText>
<sectionHeader confidence="0.958175" genericHeader="method">
5 Choosing Density Parameters
from Subjective Element Data
</sectionHeader>
<bodyText confidence="0.999981025974026">
In (Wiebe, 1994), whether a PSE is interpreted
to be subjective depends, in part, on how sub-
jective the surrounding context is. We explore
this idea in the current work, assessing whether
PSEs are more likely to be subjective if they
are surrounded by subjective elements. In par-
ticular, we experiment with a density feature to
decide whether or not a PSE instance is sub-
jective: if a sufficient number of subjective el-
ements are nearby, then the PSE instance is
considered to be subjective; otherwise, it is dis-
carded. The density parameters are a window
size W and a frequency threshold T.
In this section, we explore density in the
manually-annotated subjective-element (SE)
data, and choose density parameters for later
use in automatic disambiguation in separate
test data (in Section 6). The process for cal-
culating density in the subjective-element data
is given in Figure 1. The PSEs are defined
to be all adjectives, verbs, modals, nouns, and
adverbs that appear at least once in a subjec-
tive element, with the exception of some stop
words (line 0 of Figure 1). Note that these
PSEs depend only on the subjective-element
manual annotations, not on the automatically
identified features used elsewhere in the paper,
nor on the document-level opinion-piece classes.
PSEinsts is the set of PSE instances to be dis-
ambiguated (line 1). HiDensity (initialized on
line 2) will be the subset of PSEinsts that are
retained. In the loop, the density of each PSE
instance P is calculated, which is the number
of subjective elements that begin or end in the
W words preceding or following P (line 6). P is
retained if its density is at least T (line 7).
The precision of a set S with respect to
subjective-element classifications is the number
of members of S that appear in subjective el-
ements over the total number of members of
S. Lines 8-9 assess the precision of the original
(PSEinsts) and new (HiDensity) sets of PSE
instances. If prec(HiDensity) is greater than
prec(PSEinsts), then there is evidence that the
number of subjective elements near a PSE in-
stance is related to its subjectivity in context.
The process in Figure 1 was repeated for dif-
ferent parameter settings (T in [1,2,4, ..., 48]
and W in [1,10,20, ..., 490]) on each of the five
subjective-element datasets. To find good pa-
rameter settings, the results for each dataset
were sorted into 5-point precision intervals, and
then sorted by frequency within each interval.
Information for the top three precision intervals
for each dataset are shown in Table 3, specifi-
cally the parameter values (i.e., T and W) and
the frequency and precision of the most frequent
result in each interval. The intervals are in the
rows labeled &amp;quot;Range&amp;quot;. For example, the top
three precision intervals for SE1 are .77-.82,
.82-.87, and .87-.92 (no parameter values yield
higher precision than .92).
The top of Table 3 gives baseline frequen-
cies and precisions, which are IPSEinstsI and
prec(PSEinsts), respectively, in line 8 of Fig-
ure 1.
The parameter values exhibit a range of
frequencies and precisions, with the expected
tradeoff between precision and frequency. We
choose the following parameters to test in Sec-
tion 6 below: for each dataset (e.g., SE1), for
each precision interval whose lower bound is at
least 10 percentage points higher than the base-
line for that dataset, the top two T,W pairs
yielding the highest frequencies in that inter-
val are chosen. Among the five datasets, a total
of 45 parameter pairs were selected.
</bodyText>
<table confidence="0.977997111111111">
# of PSEinsts in SEs
8. prec(PSEinsts) â€”
IHiDensityl
SE1 SE2 SE3 SE3 SES
freq 1566 1245 1167 1108 3303
prec .49 .47 .41 .36 .51
Range .87-.92 .95-1.0 .95-1.0 .95-1.0 .95-1.0
T,W 10,20 12,50 20,50 14,100 10,10
freq 76 12 1 1 3
prec .89 1.0 1.0 1.0 1.0
Range .82-.87 .90-.95 .73-.78 .51-.56 .67-.72
T,W 6,10 12,60 46,190 22,370 26,90
freq 63 22 53 221 664
prec .84 .91 .78 .51 .67
Range .77-.82 .84-.89 .66-.71 .46-.51 .63-.67
T,W 12,40 12,80 18,60 16,310 8,30
freq 292 42 53 358 1504
prec .78 .88 .68 .47 .63
</table>
<tableCaption confidence="0.995325">
Table 3: Most frequent entry in the top 3
</tableCaption>
<bodyText confidence="0.3796805">
precision intervals for each subjective element
dataset
</bodyText>
<listItem confidence="0.893779111111111">
0. PSEinsts = the set of instances in the test
data of all PSEs described in Section 4
1. HiDensity = {}
2. For P in PSEinsts:
3. leftWin(P) = the 147 words before P
4. rightWin(P) = the I47 words after P
5. density(P) = # of PSEinsts whose first
or last word is in leftWin(P) or rightWin(P)
6. if density(P) &gt; T:
</listItem>
<figure confidence="0.657863">
HiDensity= HiDensity U {P}
</figure>
<figureCaption confidence="0.91365">
Figure 2: Algorithm for calculating density in
opinion piece (OP) data
</figureCaption>
<figure confidence="0.99137325">
IPSEinst,I
7. prec(HiDensity)- # of HiDensity in OPs
7. prec(PSEinsts)- # of PSEinsts in OPs
IHiDensityl
</figure>
<sectionHeader confidence="0.93499" genericHeader="method">
6 Density for Disambiguation
</sectionHeader>
<bodyText confidence="0.994616276923077">
In this section, density is exploited as an in-
formative feature for PSE disambiguation. The
process is shown in Figure 2. There are only two
differences between the algorithms in Figures 1
and 2. First, in Figure 1, density was defined
in terms of the number of subjective elements
nearby. However, subjective-element annota-
tions are not available in test data. In Figure
2, density is defined in terms of the number of
other PSE instances nearby, where PSEinsts
consists of all instances of the automatically
identified PSEs described in Section 4 and for
which results are given in Table 2.
Second, in Figure 2, we assess precision with
respect to the document-level classes: the preci-
sion of a set is now the number of set members
appearing in documents that are classified as
opinion pieces divided by the cardinality of the
set (see lines 7-8 of Figure 2).
The test data is corpus OP1.
An interesting question arose when defining
the PSE instances: what should be done with
words that are identified to be PSEs (or parts
of PSEs) according to multiple criteria? For
example, sunny, radiant, and exhilarating are
all unique in corpus OP1, and are all mem-
bers of the adjective PSE feature defined for
testing on OP1. Collocations add additional
complexity. For example, consider the sequence
and splendidly, which appears in the test data.
The sequence and splendidly matches the ugen-
2-gram (andl conj Uladj), and the word splen-
didly is unique (all instances of ugen-n-grams
result in at least two matches: the ugen-n-gram,
and a unique). In addition, more than one n-
gram feature may be matched by a sequence.
For example, is it that matches three fixed-n-
gram features: is it, is it that, and it that.
In the current experiments, the more PSEs a
word matches, the more weight it is given. The
hypothesis behind this treatment is that ad-
ditional matches represent additional evidence
that a PSE instance is subjective. This hypoth-
esis is realized as follows: each match of each
member of each type of PSE is considered to
be a PSE instance. Thus, among them, there
are 11 members in PSEinsts for the 5 phrases
sunny, radiant, exhilarating, and splendidly, and
is it that, one for each of the matches mentioned
above.
The process in Figure 2 was performed with
the 45 parameter-pair values (T and W) chosen
from the subjective-element data as described
in Section 5. Table 4 shows results for a sub-
set of the 45 parameters, namely the most fre-
quent parameter pair chosen from the top six
precision intervals for each training set. The
bottom of the table gives a baseline frequency
and precision in OP1, defined as IPSEinstsI
and prec(PSEinsts), respectively, on line 7 of
Figure 2.
As can be seen, the density features result in
substantial increases in precision. Among all 45
parameter pairs, the minimum percentage in-
crease over baseline is 21%. 24% of the 45 pa-
</bodyText>
<table confidence="0.999806600000001">
SE1 SE2 SE3 SE4 SES
T,W 10,20 12,50 20,50 14,100 10,10
freq 237 3176 170 10510 8
prec .87 .72 .97 .57 1.0
T,W 6,10 12,60 46,190 22,370 26,90
freq 459 5289 1323 21916 787
prec .68 .68 .95 .37 .92
T,W 12,40 12,80 18,60 16,310 8,30
freq 1398 9662 906 24454 3239
prec .79 .58 .87 .34 .67
T,W 12,50 10,70 14,50
freq 3176 10995 1581
prec .72 .55 .81
T,W 20,110 14,110 1,10
freq 5330 12206 21221
prec .73 .53 .34
T,W 6,40 12,100
freq 11426 13637
prec .50 .50
PSE Baseline: Freq=30938, Prec=.28
</table>
<tableCaption confidence="0.884901333333333">
Table 4: Results for high-density PSEs in
test data OP1 using parameters chosen from
subjective-element data
</tableCaption>
<bodyText confidence="0.998152952380953">
rameter pairs yield increases of 200% or more;
38% yield increases between 100% and 199%,
and 38% yield increases between 21%-99%.
Notice that, except for one blip (T,W = 6, 10
under SE1), the precisions decrease and the fre-
quencies increase as we go down each column in
Table 4. The same pattern can be observed in
the full table with all 45 parameter pairs (not
included due to space). But the parameter
pairs are ordered in Table 4 based on
performance in the manually-annotated
subjective-element data, not based on per-
formance in the test data OP1. For exam-
ple, the entry in the first row, first column
(T,W = 10,20) is the parameter pair giving
the highest frequency in the top precision in-
terval of SE1 (frequency and precision in SE1,
using the process of Figure 1). Thus, the rela-
tive precisions and frequencies of the parameter
pairs are carried over from the training to the
test data.
</bodyText>
<sectionHeader confidence="0.984047" genericHeader="method">
7 Sentence Annotation
</sectionHeader>
<bodyText confidence="0.997476833333333">
To assess the subjectivity of sentences with
high-density PSEs, we extracted the sentences
in corpus 0P2 that contain at least one
high-density PSE, and manually annotated
them. We chose the density parameter pair
T,W=12,30, based on its precision and fre-
</bodyText>
<table confidence="0.7710035">
S 0 U
S 98 2 3
0 2 14 0
U 2 11 1
</table>
<tableCaption confidence="0.892283">
Table 5: Sentence annotation contingency table;
</tableCaption>
<bodyText confidence="0.994912833333334">
judge l&apos;s counts are in the rows and judge 2&apos;s
counts are in the columns.
quency in OP1. This parameter setting yields
results that are relatively high precision and low
frequency. We chose a low-frequency setting to
make the annotation study feasible.
133 sentences were so identified. They are
referred to below as the system-identified sen-
tences.
The extracted sentences were independently
annotated by two judges. One is a co-author
of this paper (judge 1), and the other has per-
formed subjectivity annotation before, but is
not otherwise involved in this research (judge 2).
Sentences were annotated according to the cod-
ing instructions of (Wiebe et al., 1999), which,
recall, are to classify a sentence as subjective if
there is a significant expression of subjectivity
in the sentence, of either the writer or someone
mentioned in the text. In addition to the sub-
jective and objective classes, a judge could tag
a sentence &amp;quot;uncertain&amp;quot; if he or she is unsure of
his or her rating.
An equal number (133) of other sentences
were randomly selected from the corpus to serve
as controls. The 133 system-identified sentences
and the 133 control sentences were randomly
mixed together. The judges were asked to an-
notate all 266 sentences, not knowing which are
system-identified and which are control. Each
sentence was presented with the sentence that
precedes and the sentence that follows it in the
corpus, to provide some context for interpreta-
tion.
Judge 1 classified 103 of the system-identified
sentences as subjective; 16 as objective; and
14 as uncertain. Judge 2 classified 102 of the
system-identified sentences as subjective; 27 as
objective; and 4 as uncertain. The contingency
table is given in Table 5.
For most of the sentences (116 out of 133,
or 87% of the corpus), neither judge rated the
sentence as uncertain. The agreement between
judges on those sentences is very high: the
Kappa value is 0.86. With all sentences in-
cluded, the Kappa value is 0.60. Thus, most
of the disagreements involve sentences tagged
as uncertain.
The current paper is concerned with whether
high-density PSEs are indicative of subjective
text. An examination of the data from this per-
spective is illuminating.
For 98 of the sentences (set SS), judges 1
and 2 tagged the sentence as subjective. Among
the other 35 sentences (those tagged objective,
those upon which the judges disagree, etc), 20
(set inBlock) appear in a block of contigu-
ous system-identified sentences that includes a
member of SS. For example, in Table 1, (2.a)
and (2.c) are in SS while (2.b) is in inBlock,
and (3.a) is in SS while (3.b) is in inBlock.
Thus, fully 89% of all sentences are either in
SS or inBlock. Among the 15 other sentences,
6 are adjacent to subjective sentences that were
not identified by our system (so were not an-
notated by the judges). All contain significant
expressions of subjectivity of the writer or some-
one mentioned in the text, the criterion used
in this work for classifying a sentence as sub-
jective. Thus, 93% of the sentences contain-
ing high-density PSEs are subjective or are near
subjective sentences.
</bodyText>
<sectionHeader confidence="0.996181" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999933419354838">
This paper investigates a contextual feature
for recognizing subjectivity, which identifies
clusters of potentially subjective expressions
(PSEs). This density feature involves two pa-
rameters. We select parameter values using
training data manually annotated at the expres-
sion level, and then test them on data annotated
at the document level for opinion pieces. The
PSEs in the training data are defined in terms
of the manual annotations, while the PSEs in
the test data are automatically identified from
text. All of the selected parameters lead to in-
creases in precision on the test data, the ma-
jority leading to increases over 100%. The large
differences between training and testing suggest
that our results are not brittle.
Using a density feature selected from a train-
ing set, sentences containing high-density PSEs
were extracted from a separate test set, and
manually annotated by two judges. Fully 93%
are subjective sentences or are near subjective
sentences.
There are many avenues for future work. Our
immediate plans are to apply the system to
large amounts of data, and then apply informa-
tion extraction and bootstrapping techniques
(Riloff and Jones, 1999) to identify subjective
language that the system does not yet know. In
addition, it would be illuminating to apply our
system to data annotated with discourse trees
(Carlson et al., 2001).
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999566564102564">
A. Banfield. 1982. Unspeakable Sentences.
Routledge and Kegan Paul, Boston.
L. Carlson, D. Marcu, and M. E. Okurowski.
2001. Building a discourse-tagged corpus in
the framework of rhetorical structure theory.
In 2nd SIGDIAL Workshop on Discourse and
Dialogue.
E. Hovy. 1987. Generating Natural Language
under Pragmatic Constraints. Ph.D. thesis,
Yale.
B. Kessler, G. Nunberg, and H. Schutze. 1997.
Automatic detection of text genre. In ACL-
97.
D. Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In COLING-ACL-98.
E. Riloff and R. Jones. 1999. Learning Dictio-
naries for Information Extraction by Multi-
Level Bootstrapping. In AAAI-99.
E. Spertus. 1997. Smokey: Automatic recogni-
tion of hostile messages. In Proc. IAAL
S. Teufel and M. Moens. 2000. What&apos;s yours
and what&apos;s mine: Determining intellectual at-
tribution in scientific texts. In EMNLP-VLC-
2000.
J. Wiebe, R. Bruce, and T. O&apos;Hara. 1999. De-
velopment and use of a gold standard data set
for subjectivity classifications. In ACL-99.
J. Wiebe, R. Bruce, M. Bell, M. Martin, and
T. Wilson. 2001a. A corpus study of evalu-
ative and speculative language. In 2nd SIG-
DIAL Workshop on Discourse and Dialogue.
J. Wiebe, T. Wilson, and M. Bell. 2001b. Iden-
tifying collocations for recognizing opinions.
In ACL-01 Workshop on Collocation.
J. Wiebe. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233-287.
J. Wiebe. 2000. Learning subjective adjectives
from corpora. In AAAI-00.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963205">
<title confidence="0.999419">Learning to Disambiguate Potentially Subjective Expressions</title>
<author confidence="0.994394">Wiebe</author>
<affiliation confidence="0.999205">of Computer Science Systems University of</affiliation>
<address confidence="0.996221">Pittsburgh, PA, USA, 15260</address>
<abstract confidence="0.9973567">The goal of this work is recognizing opinionated evaluative in text. The ability to recognize such language would be beneficial for many NLP applications such as question answering, information extraction, summarization, and genre detection. This paper focuses on disambiguating potentially subjective expressions in context, based on the density of other clues in the surrounding text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Banfield</author>
</authors>
<title>Unspeakable Sentences. Routledge and Kegan Paul,</title>
<date>1982</date>
<location>Boston.</location>
<contexts>
<context position="755" citStr="Banfield, 1982" startWordPosition="106" endWordPosition="107">ystems Program University of Pittsburgh Pittsburgh, PA, USA, 15260 Abstract The goal of this work is recognizing opinionated and evaluative (subjective) language in text. The ability to recognize such language would be beneficial for many NLP applications such as question answering, information extraction, summarization, and genre detection. This paper focuses on disambiguating potentially subjective expressions in context, based on the density of other clues in the surrounding text. 1 Introduction The goal of this work is to recognize opinionated and evaluative (subjective) language in text (Banfield, 1982). The ability to recognize subjective language would be beneficial for NLP applications such as question answering, information extraction, and genre detection. Recent work by Wiebe and colleagues (2001b; 2001a; 2000) focused on learning potentially subjective expressions from corpora. This paper focuses on the mutual disambiguation of such features in context. Many natural language expressions have both subjective and objective usages, so a problem for recognizing subjective language is determining when instances of expressions are indeed subjective in the context in which they appear. We hav</context>
</contexts>
<marker>Banfield, 1982</marker>
<rawString>A. Banfield. 1982. Unspeakable Sentences. Routledge and Kegan Paul, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In 2nd SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu, and M. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In 2nd SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Generating Natural Language under Pragmatic Constraints.</title>
<date>1987</date>
<tech>Ph.D. thesis, Yale.</tech>
<contexts>
<context position="4849" citStr="Hovy, 1987" startWordPosition="751" endWordPosition="752">rther annotated at the expression level and used for training data to choose density parameters. Table 1 shows examples of subjective and objective sentences from the annotation study presented in Section 7. Sentences classified by both judges as objective are marked &amp;quot;oo&amp;quot; and those classified by both judges as subjective are marked &amp;quot;ss&amp;quot;. Subjectivity analysis could be exploited in many NLP applications, recognizing inflammatory messages (Spertus, 1997), genre detection and document routing (Kessler et al., 1997), intellectual attribution in text (Teufel and Moens, 2000), generation and style (Hovy, 1987), question answering from multiple perspectives, and any other application that would benefit from knowledge of how opinionated language is, and whether or not the writer purports to objectively present factual material. An information extraction or summarization system, for example, would benefit from distinguishing sentences intended to present facts from those intended to present opinions, since many such systems are meant to extract only facts. One aspect of subjectivity is highlighted in this paper. Although some expressions, such as !, are subjective in all contexts, many may or may not </context>
</contexts>
<marker>Hovy, 1987</marker>
<rawString>E. Hovy. 1987. Generating Natural Language under Pragmatic Constraints. Ph.D. thesis, Yale.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kessler</author>
<author>G Nunberg</author>
<author>H Schutze</author>
</authors>
<title>Automatic detection of text genre.</title>
<date>1997</date>
<booktitle>In ACL97.</booktitle>
<contexts>
<context position="4755" citStr="Kessler et al., 1997" startWordPosition="735" endWordPosition="738">he annotators&apos; certainty ratings were 2 or 3). In the current paper, as described below, this data is further annotated at the expression level and used for training data to choose density parameters. Table 1 shows examples of subjective and objective sentences from the annotation study presented in Section 7. Sentences classified by both judges as objective are marked &amp;quot;oo&amp;quot; and those classified by both judges as subjective are marked &amp;quot;ss&amp;quot;. Subjectivity analysis could be exploited in many NLP applications, recognizing inflammatory messages (Spertus, 1997), genre detection and document routing (Kessler et al., 1997), intellectual attribution in text (Teufel and Moens, 2000), generation and style (Hovy, 1987), question answering from multiple perspectives, and any other application that would benefit from knowledge of how opinionated language is, and whether or not the writer purports to objectively present factual material. An information extraction or summarization system, for example, would benefit from distinguishing sentences intended to present facts from those intended to present opinions, since many such systems are meant to extract only facts. One aspect of subjectivity is highlighted in this pap</context>
</contexts>
<marker>Kessler, Nunberg, Schutze, 1997</marker>
<rawString>B. Kessler, G. Nunberg, and H. Schutze. 1997. Automatic detection of text genre. In ACL97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL-98.</booktitle>
<contexts>
<context position="9678" citStr="Lin, 1998" startWordPosition="1548" endWordPosition="1549">dy in Section 7. objective sentences in opinion-piece documents, and subjective sentences in the other kinds of documents. 4 PSEs Used We use PSE features automatically learned from corpora. The first is a word appearing just once in the corpus (i.e., a unique word). Interestingly, the set of all unique words in a corpus is a high frequency set, with higher than baseline precision (Wiebe et al., 2001a). This feature does not require training. The next two types of PSE are adjectives and verbs identified using the results of a method for clustering words according to distributional similarity (Lin, 1998), as described in (Wiebe, 2000). Distributional similarity has been used to find similar words in text. The hypothesis behind its use in (Wiebe, 2000) was that words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identified from bizarre). The remaining types of PSEs are collocations, learned from data that is manually annotated at the expression level with subjective elements. Roughly speaking, a collocation is judged to be a PSE if its precision is greater than the maximum precision of its constituents (Wiebe et al., 2001a). F</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by MultiLevel Bootstrapping.</title>
<date>1999</date>
<booktitle>In AAAI-99.</booktitle>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by MultiLevel Bootstrapping. In AAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Spertus</author>
</authors>
<title>Smokey: Automatic recognition of hostile messages.</title>
<date>1997</date>
<booktitle>In Proc. IAAL</booktitle>
<contexts>
<context position="4694" citStr="Spertus, 1997" startWordPosition="728" endWordPosition="729"> percentage points higher on the sentences for which the annotators&apos; certainty ratings were 2 or 3). In the current paper, as described below, this data is further annotated at the expression level and used for training data to choose density parameters. Table 1 shows examples of subjective and objective sentences from the annotation study presented in Section 7. Sentences classified by both judges as objective are marked &amp;quot;oo&amp;quot; and those classified by both judges as subjective are marked &amp;quot;ss&amp;quot;. Subjectivity analysis could be exploited in many NLP applications, recognizing inflammatory messages (Spertus, 1997), genre detection and document routing (Kessler et al., 1997), intellectual attribution in text (Teufel and Moens, 2000), generation and style (Hovy, 1987), question answering from multiple perspectives, and any other application that would benefit from knowledge of how opinionated language is, and whether or not the writer purports to objectively present factual material. An information extraction or summarization system, for example, would benefit from distinguishing sentences intended to present facts from those intended to present opinions, since many such systems are meant to extract only</context>
</contexts>
<marker>Spertus, 1997</marker>
<rawString>E. Spertus. 1997. Smokey: Automatic recognition of hostile messages. In Proc. IAAL</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>What&apos;s yours and what&apos;s mine: Determining intellectual attribution in scientific texts.</title>
<date>2000</date>
<booktitle>In EMNLP-VLC2000.</booktitle>
<contexts>
<context position="4814" citStr="Teufel and Moens, 2000" startWordPosition="744" endWordPosition="747">rent paper, as described below, this data is further annotated at the expression level and used for training data to choose density parameters. Table 1 shows examples of subjective and objective sentences from the annotation study presented in Section 7. Sentences classified by both judges as objective are marked &amp;quot;oo&amp;quot; and those classified by both judges as subjective are marked &amp;quot;ss&amp;quot;. Subjectivity analysis could be exploited in many NLP applications, recognizing inflammatory messages (Spertus, 1997), genre detection and document routing (Kessler et al., 1997), intellectual attribution in text (Teufel and Moens, 2000), generation and style (Hovy, 1987), question answering from multiple perspectives, and any other application that would benefit from knowledge of how opinionated language is, and whether or not the writer purports to objectively present factual material. An information extraction or summarization system, for example, would benefit from distinguishing sentences intended to present facts from those intended to present opinions, since many such systems are meant to extract only facts. One aspect of subjectivity is highlighted in this paper. Although some expressions, such as !, are subjective in</context>
</contexts>
<marker>Teufel, Moens, 2000</marker>
<rawString>S. Teufel and M. Moens. 2000. What&apos;s yours and what&apos;s mine: Determining intellectual attribution in scientific texts. In EMNLP-VLC2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>T O&apos;Hara</author>
</authors>
<title>Development and use of a gold standard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In ACL-99.</booktitle>
<contexts>
<context position="3185" citStr="Wiebe et al., 1999" startWordPosition="483" endWordPosition="486">ed automatically using density features are manually annotated by two judges. Agreement is high for sentences classified with certainty, and most of the sentences are classified as subjective by both judges, or are near sentences that are. This work is also an interesting case study of using data annotated at different levels, and exploiting existing document-level annotations to learn linguistic knowledge. 2 Subjectivity Subjectivity in natural language refers to aspects of language used to express emotion, evaluation, opinion and speculation. In this work, we adopt the annotation scheme of (Wiebe et al., 1999). Under that scheme, a sentence is subjective if it contains a significant expression of emotion, evaluation, opinion, or speculation, attributed to either the writer or someone mentioned in the text. Otherwise, the sentence is objective. In (Wiebe et al., 1999), multiple judges annotated a corpus with subjective/objective classifications, rating the certainty of their answers on a scale from 0 to 3. For the 85% of the corpus for which the certainty ratings of the judges was 2 or 3, the average pairwise Kappa value was 0.80. Thus, when the annotators are certain of their answers, which they ar</context>
<context position="5908" citStr="Wiebe et al., 1999" startWordPosition="922" endWordPosition="925">only facts. One aspect of subjectivity is highlighted in this paper. Although some expressions, such as !, are subjective in all contexts, many may or may not be subjective, depending on the context in which they appear. A potential subjective element (PSE) is a linguistic word or expression that may be used to express subjectivity. A subjective element is an instance of a potential subjective element, in a particular context, that is indeed subjective in that context (Wiebe, 1994). This paper focuses on identifying PSE instances that are subjective elements. 3 Data We use training data from (Wiebe et al., 1999; Wiebe et al., 2001b; Wiebe et al., 2001a) consisting of corpora annotated at the expression level. In expression-level subjectivity tagging, the judges first identify the sentences they believe are subjective. They next identify the subjective elements in the sentence, i.e., the expressions they feel are responsible for the subjective classification. For example, an annotator marked two subjective elements in the following sentence (indicated with parentheses): They paid (yet) more for (really good stuff). Two WSJ datasets, 500 sentences each, were annotated by two judges, resulting in four </context>
<context position="22787" citStr="Wiebe et al., 1999" startWordPosition="3824" endWordPosition="3827"> counts are in the columns. quency in OP1. This parameter setting yields results that are relatively high precision and low frequency. We chose a low-frequency setting to make the annotation study feasible. 133 sentences were so identified. They are referred to below as the system-identified sentences. The extracted sentences were independently annotated by two judges. One is a co-author of this paper (judge 1), and the other has performed subjectivity annotation before, but is not otherwise involved in this research (judge 2). Sentences were annotated according to the coding instructions of (Wiebe et al., 1999), which, recall, are to classify a sentence as subjective if there is a significant expression of subjectivity in the sentence, of either the writer or someone mentioned in the text. In addition to the subjective and objective classes, a judge could tag a sentence &amp;quot;uncertain&amp;quot; if he or she is unsure of his or her rating. An equal number (133) of other sentences were randomly selected from the corpus to serve as controls. The 133 system-identified sentences and the 133 control sentences were randomly mixed together. The judges were asked to annotate all 266 sentences, not knowing which are syste</context>
</contexts>
<marker>Wiebe, Bruce, O&apos;Hara, 1999</marker>
<rawString>J. Wiebe, R. Bruce, and T. O&apos;Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. In ACL-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>M Bell</author>
<author>M Martin</author>
<author>T Wilson</author>
</authors>
<title>A corpus study of evaluative and speculative language.</title>
<date>2001</date>
<booktitle>In 2nd SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="5928" citStr="Wiebe et al., 2001" startWordPosition="926" endWordPosition="929">ct of subjectivity is highlighted in this paper. Although some expressions, such as !, are subjective in all contexts, many may or may not be subjective, depending on the context in which they appear. A potential subjective element (PSE) is a linguistic word or expression that may be used to express subjectivity. A subjective element is an instance of a potential subjective element, in a particular context, that is indeed subjective in that context (Wiebe, 1994). This paper focuses on identifying PSE instances that are subjective elements. 3 Data We use training data from (Wiebe et al., 1999; Wiebe et al., 2001b; Wiebe et al., 2001a) consisting of corpora annotated at the expression level. In expression-level subjectivity tagging, the judges first identify the sentences they believe are subjective. They next identify the subjective elements in the sentence, i.e., the expressions they feel are responsible for the subjective classification. For example, an annotator marked two subjective elements in the following sentence (indicated with parentheses): They paid (yet) more for (really good stuff). Two WSJ datasets, 500 sentences each, were annotated by two judges, resulting in four sets of annotations </context>
<context position="9471" citStr="Wiebe et al., 2001" startWordPosition="1514" endWordPosition="1517">isted men and lower-grade officers were meat thrown into a grinder. ss &amp;quot;If you believe in God and you believe in miracles, there&apos;s nothing particularly crazy about that.&amp;quot; ss Table 1: Examples from the annotation study in Section 7. objective sentences in opinion-piece documents, and subjective sentences in the other kinds of documents. 4 PSEs Used We use PSE features automatically learned from corpora. The first is a word appearing just once in the corpus (i.e., a unique word). Interestingly, the set of all unique words in a corpus is a high frequency set, with higher than baseline precision (Wiebe et al., 2001a). This feature does not require training. The next two types of PSE are adjectives and verbs identified using the results of a method for clustering words according to distributional similarity (Lin, 1998), as described in (Wiebe, 2000). Distributional similarity has been used to find similar words in text. The hypothesis behind its use in (Wiebe, 2000) was that words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identified from bizarre). The remaining types of PSEs are collocations, learned from data that is manually annota</context>
</contexts>
<marker>Wiebe, Bruce, Bell, Martin, Wilson, 2001</marker>
<rawString>J. Wiebe, R. Bruce, M. Bell, M. Martin, and T. Wilson. 2001a. A corpus study of evaluative and speculative language. In 2nd SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>M Bell</author>
</authors>
<title>Identifying collocations for recognizing opinions.</title>
<date>2001</date>
<booktitle>In ACL-01 Workshop on Collocation.</booktitle>
<contexts>
<context position="5928" citStr="Wiebe et al., 2001" startWordPosition="926" endWordPosition="929">ct of subjectivity is highlighted in this paper. Although some expressions, such as !, are subjective in all contexts, many may or may not be subjective, depending on the context in which they appear. A potential subjective element (PSE) is a linguistic word or expression that may be used to express subjectivity. A subjective element is an instance of a potential subjective element, in a particular context, that is indeed subjective in that context (Wiebe, 1994). This paper focuses on identifying PSE instances that are subjective elements. 3 Data We use training data from (Wiebe et al., 1999; Wiebe et al., 2001b; Wiebe et al., 2001a) consisting of corpora annotated at the expression level. In expression-level subjectivity tagging, the judges first identify the sentences they believe are subjective. They next identify the subjective elements in the sentence, i.e., the expressions they feel are responsible for the subjective classification. For example, an annotator marked two subjective elements in the following sentence (indicated with parentheses): They paid (yet) more for (really good stuff). Two WSJ datasets, 500 sentences each, were annotated by two judges, resulting in four sets of annotations </context>
<context position="9471" citStr="Wiebe et al., 2001" startWordPosition="1514" endWordPosition="1517">isted men and lower-grade officers were meat thrown into a grinder. ss &amp;quot;If you believe in God and you believe in miracles, there&apos;s nothing particularly crazy about that.&amp;quot; ss Table 1: Examples from the annotation study in Section 7. objective sentences in opinion-piece documents, and subjective sentences in the other kinds of documents. 4 PSEs Used We use PSE features automatically learned from corpora. The first is a word appearing just once in the corpus (i.e., a unique word). Interestingly, the set of all unique words in a corpus is a high frequency set, with higher than baseline precision (Wiebe et al., 2001a). This feature does not require training. The next two types of PSE are adjectives and verbs identified using the results of a method for clustering words according to distributional similarity (Lin, 1998), as described in (Wiebe, 2000). Distributional similarity has been used to find similar words in text. The hypothesis behind its use in (Wiebe, 2000) was that words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identified from bizarre). The remaining types of PSEs are collocations, learned from data that is manually annota</context>
</contexts>
<marker>Wiebe, Wilson, Bell, 2001</marker>
<rawString>J. Wiebe, T. Wilson, and M. Bell. 2001b. Identifying collocations for recognizing opinions. In ACL-01 Workshop on Collocation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="5776" citStr="Wiebe, 1994" startWordPosition="901" endWordPosition="902">ing sentences intended to present facts from those intended to present opinions, since many such systems are meant to extract only facts. One aspect of subjectivity is highlighted in this paper. Although some expressions, such as !, are subjective in all contexts, many may or may not be subjective, depending on the context in which they appear. A potential subjective element (PSE) is a linguistic word or expression that may be used to express subjectivity. A subjective element is an instance of a potential subjective element, in a particular context, that is indeed subjective in that context (Wiebe, 1994). This paper focuses on identifying PSE instances that are subjective elements. 3 Data We use training data from (Wiebe et al., 1999; Wiebe et al., 2001b; Wiebe et al., 2001a) consisting of corpora annotated at the expression level. In expression-level subjectivity tagging, the judges first identify the sentences they believe are subjective. They next identify the subjective elements in the sentence, i.e., the expressions they feel are responsible for the subjective classification. For example, an annotator marked two subjective elements in the following sentence (indicated with parentheses): </context>
<context position="12784" citStr="Wiebe, 1994" startWordPosition="2093" endWordPosition="2094">n an SE (except not, will, be, have). 1. PSEinsts = the set of all instances of PSEs 2. HiDensity = {} 3. For P in PSEiusts: 4. leftWin(P) = the 147 words before P 5. rightWin(P) = the 147 words after P 6. density(P) = # of SEs whose first or last word is in leftWin(P) or rightWin(P) 7. if density(P) &gt; T: HiDensity = HiDensity U {P} IP SEinst,1 # of HiDensity in SEs 9. prec(HiDensity)= Figure 1: Algorithm for calculating density in subjective element (SE) data cause the distribution is highly skewed in favor of non-opinion pieces. 5 Choosing Density Parameters from Subjective Element Data In (Wiebe, 1994), whether a PSE is interpreted to be subjective depends, in part, on how subjective the surrounding context is. We explore this idea in the current work, assessing whether PSEs are more likely to be subjective if they are surrounded by subjective elements. In particular, we experiment with a density feature to decide whether or not a PSE instance is subjective: if a sufficient number of subjective elements are nearby, then the PSE instance is considered to be subjective; otherwise, it is discarded. The density parameters are a window size W and a frequency threshold T. In this section, we expl</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>J. Wiebe. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In AAAI-00.</booktitle>
<contexts>
<context position="9709" citStr="Wiebe, 2000" startWordPosition="1553" endWordPosition="1554">ntences in opinion-piece documents, and subjective sentences in the other kinds of documents. 4 PSEs Used We use PSE features automatically learned from corpora. The first is a word appearing just once in the corpus (i.e., a unique word). Interestingly, the set of all unique words in a corpus is a high frequency set, with higher than baseline precision (Wiebe et al., 2001a). This feature does not require training. The next two types of PSE are adjectives and verbs identified using the results of a method for clustering words according to distributional similarity (Lin, 1998), as described in (Wiebe, 2000). Distributional similarity has been used to find similar words in text. The hypothesis behind its use in (Wiebe, 2000) was that words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identified from bizarre). The remaining types of PSEs are collocations, learned from data that is manually annotated at the expression level with subjective elements. Roughly speaking, a collocation is judged to be a PSE if its precision is greater than the maximum precision of its constituents (Wiebe et al., 2001a). Fixed-n-grams are sequences of n</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>J. Wiebe. 2000. Learning subjective adjectives from corpora. In AAAI-00.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>