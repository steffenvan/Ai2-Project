<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000415">
<title confidence="0.990289">
Labeled Grammar Induction with Minimal Supervision
</title>
<author confidence="0.988239">
Yonatan Bisk Christos Christodoulopoulos Julia Hockenmaier
</author>
<affiliation confidence="0.9971375">
Department of Computer Science
The University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.905016">
201 N. Goodwin Ave, Urbana, IL 61801
</address>
<email confidence="0.999198">
{bisk1,christod,juliahmr}@illinois.edu
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999083916666667">
Nearly all work in unsupervised grammar
induction aims to induce unlabeled de-
pendency trees from gold part-of-speech-
tagged text. These clean linguistic classes
provide a very important, though unreal-
istic, inductive bias. Conversely, induced
clusters are very noisy. We show here,
for the first time, that very limited hu-
man supervision (three frequent words per
cluster) may be required to induce labeled
dependencies from automatically induced
word clusters.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998462">
Despite significant progress on inducing
part-of-speech (POS) tags from raw text
(Christodoulopoulos et al., 2010; Blunsom
and Cohn, 2011) and a small number of notable
exceptions (Seginer, 2007; Spitkovsky et al.,
2011; Christodoulopoulos et al., 2012), most
approaches to grammar induction or unsupervised
parsing (Klein and Manning, 2004; Spitkovsky
et al., 2013; Blunsom and Cohn, 2010) are
based on the assumption that gold POS tags are
available to the induction system. Although most
approaches treat these POS tags as arbitrary, if
relatively clean, clusters, it has also been shown
that the linguistic knowledge implicit in these
tags can be exploited in a more explicit fashion
(Naseem et al., 2010). The presence of POS tags
is also essential for approaches that aim to return
richer structures than the standard unlabeled
dependencies. Boonkwan and Steedman (2011)
train a parser that uses a semi-automatically
constructed Combinatory Categorial Grammar
(CCG, Steedman (2000)) lexicon for POS tags,
while Bisk and Hockenmaier (2012; 2013) show
that CCG lexicons can be induced automatically
if POS tags are used to identify nouns and verbs.
However, assuming clean POS tags is highly
unrealistic for most scenarios in which one would
wish to use an otherwise unsupervised parser.
In this paper we demonstrate that the simple
“universal” knowledge of Bisk and Hockenmaier
(2013) can be easily applied to induced clus-
ters given a small number of words labeled as
noun, verb or other, and that this small amount
of knowledge is sufficient to produce labeled syn-
tactic structures from raw text, something that has
not yet been proposed in the literature. Specifi-
cally, we will provide a labeled evaluation of in-
duced CCG parsers against the English (Hock-
enmaier and Steedman, 2007) and Chinese (Tse,
2013) CCGbanks. To provide a direct compari-
son to the dependency induction literature, we will
also provide an unlabeled evaluation on the 10 de-
pendency corpora that were used for the task of
grammar induction from raw text in the PASCAL
Challenge on Grammar Induction (Gelling et al.,
2012).
The system of Christodoulopoulos et al. (2012)
was the only participant competing in the PAS-
CAL Challenge that operated over raw text (in-
stead of gold POS tags). However, their approach
did not outperform the six baseline systems pro-
vided. These baselines were two versions of the
DMV model (Klein and Manning, 2004; Gillen-
water et al., 2011) run on varying numbers of in-
duced Brown clusters (described in section 2.1).
We will therefore compare against these baselines
in our evaluation.
Outside of the shared task, Spitkovsky et al.
(2011) demonstrated impressive performance us-
ing Brown clusters but did not provide evaluation
</bodyText>
<page confidence="0.92657">
870
</page>
<note confidence="0.518513333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999312">
for languages other than English.
The system we propose here will use a coarse-
grained labeling comprised of three classes, which
makes it substantially simpler than traditional
tagsets, and uses far fewer labeled tokens than
is customary for weakly-supervised approaches
(Haghighi and Klein, 2006; Garrette et al., 2015).
</bodyText>
<sectionHeader confidence="0.989807" genericHeader="method">
2 Our Models
</sectionHeader>
<bodyText confidence="0.999974482758621">
Our goal in this work will be to produce la-
beled dependencies from raw text. Our approach
is based on the HDP-CCG parser of Bisk and
Hockenmaier (2015) with their extensions to cap-
ture lexicalization and punctuation, which, to our
knowledge, is the only unsupervised approach to
produce labeled dependencies. It first induces a
CCG from POS-tagged text, and then estimates a
model based on Hierarchical Dirichlet Processes
(Teh et al., 2006) over the induced parse forests.
The HDP model uses a hyperparameter which
controls the amount of smoothing to the base mea-
sure of the HDP. Setting this value will prove im-
portant when moving between datasets of drasti-
cally different sizes.
The induction algorithm assumes that a) verbs
may be predicates (with category S), b) verbs can
take nouns (with category N) or sentences as ar-
guments (leading to categories of the form S|N,
(S|N)|N, (S|N)|S etc.), c) any word can act as a
modifier, i.e. have a category of the form X|X
if it is adjacent to a word with category X or
X|Y, and d) modifiers X|X can take nouns or sen-
tences as arguments ((X|X)|N). Our contribution
in this paper will be to show that we can replace
the gold POS tags used by Bisk and Hockenmaier
(2013) with automatically induced word clusters,
and then use very minimal supervision to identify
noun and verb clusters.
</bodyText>
<subsectionHeader confidence="0.996008">
2.1 Inducing Word Clusters
</subsectionHeader>
<bodyText confidence="0.994299833333333">
We will evaluate three clustering approaches:
Brown Clusters Brown clusters (Brown et al.,
1992) assign each word to a single cluster using an
aglomerative clustering that maximizes the proba-
bility of the corpus under a bigram class condi-
tional model. We use Liang’s implementation1.
</bodyText>
<footnote confidence="0.763538">
BMMM The Bayesian Multinomial Mixture
Model2 (BMMM, Christodoulopoulos et al. 2011)
is also a hard clustering system, but has the ability
1https://github.com/percyliang/brown-cluster
2https://github.com/christos-c/bmmm
</footnote>
<bodyText confidence="0.999402833333333">
to incorporate multiple types of features either at
a token level (e.g. f1 context word) or at a type
level (e.g. morphology features derived from the
Morfessor system (Creutz and Lagus, 2006)). The
combination of these features allows BMMM to
better capture morphosyntactic information.
Bigram HMM We also evaluate unsupervised
bigram HMMs, since the soft clustering they pro-
vide may be advantageous over the hard Brown
and BMMM clusters. But it is known that un-
supervised HMMs may not find good POS tags
(Johnson, 2007), and in future work, more sophis-
ticated models (e.g. Blunsom and Cohn (2011)),
might outperform the systems we use here.
In all cases, we assume that we can identify
punctuation marks, which are moved to their own
cluster and ignored for the purposes of tagging and
parsing evaluation.
</bodyText>
<subsectionHeader confidence="0.999698">
2.2 Identifying Noun and Verb Clusters
</subsectionHeader>
<bodyText confidence="0.999995857142857">
To induce CCGs from induced clusters, we need
to label them as {noun, verb, other}. This needs
to be done judiciously; providing every cluster the
verb label, for example, leads to the model iden-
tifying prepositions as the main sentential predi-
cates.
We demonstrate here that labeling three fre-
quent words per cluster is sufficient to outperform
state-of-the-art performance on grammar induc-
tion from raw text in many languages. We emu-
late having a native speaker annotate words for us
by using the universal tagset (Petrov et al., 2012)
as our source of labels for the most frequent three
words per cluster (we map the tags NOUN, NUM,
PRON to noun, VERB to verb, and all others to
other). The final labeling is a majority vote, where
each word type contributes a vote for each label it
can take (see Table 4 for some examples). This ap-
proach could easily be scaled to allow more words
per cluster to vote. But we will see that three per
cluster is sufficient to label most tokens correctly.
</bodyText>
<sectionHeader confidence="0.997928" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999937375">
We will focus first on producing CCG labeled
predicate-argument dependencies for English and
Chinese and will then apply our best settings to
produce a comparison with the tree structures of
the languages of the PASCAL Shared Task. All
languages will be trained on sentences of up to
length 20 (not counting punctuation). All clus-
ter induction algorithms are treated as black boxes
</bodyText>
<page confidence="0.98861">
871
</page>
<bodyText confidence="0.998959115384615">
and run over the complete datasets in advance.
This alleviates having to handle tagging of un-
known words.
To provide an intuition for the performance of
the induced word clusters, we provide two stan-
dard metrics for unsupervised tagging:
Many-to-one (M-1) A commonly used mea-
sure, M-1 relies on mapping each cluster to the
most common POS tag of its words. However, M-
1 can be easily inflated by inducing more clusters.
V-Measure Proposed by Rosenberg and
Hirschberg (2007), V-Measure (VM) measures
the information-theoretic distance between two
clusterings and has been shown to be robust to the
number of induced clusters (Christodoulopoulos
et al., 2010). Both of these metrics are known
to be highly dependent on the gold annotation
standards they are compared against, and may
not correlate with downstream performance at
parsing.
Of more immediate relevance to our task is the
ability to accurately identify nouns and verbs:
Noun, Verb, and Other Recall We measure
the (token-based) recall of our three-way labeling
scheme of clusters as noun/verb/other against the
universal POS tags of each token.
</bodyText>
<sectionHeader confidence="0.965008" genericHeader="method">
4 Experiment 1: CCG-based Evaluation
</sectionHeader>
<bodyText confidence="0.999940434782609">
Experimental Setup For our primary experi-
ments, we train and test our systems on the English
and Chinese CCGbanks, and report directed la-
beled F1 (LF1) and undirected unlabeled F1 (UF1)
over CCG dependencies (Clark et al., 2002). For
the labeled evaluation, we follow the simplifica-
tion of CCGbank categories proposed by Bisk and
Hockenmaier (2015): for English to remove mor-
phosyntactic features, map NP to N and change
VP modifiers (S\NP)|(S\NP) to sentential modi-
fiers (S|S); for Chinese we map both M and QP to
N. In the CCG literature, UF1 is commonly used
because undirected dependencies do not penalize
argument vs. adjunct distinctions, e.g. for prepo-
sitional phrases. For this reason we will include
UF1 in the final test set evaluation (Table 2).
We use the published train/dev/test splits, using
the dev set for choosing a cluster induction algo-
rithm, and then present final performance on the
test data. We induce 36 tags for English and 37
for Chinese to match the number of tags present in
the treebanks (excluding symbol and punctuation
tags).
</bodyText>
<table confidence="0.998909625">
Tagging N Labeling Parsing
M-1 VM / V / O LF1 Gold
Brown 62.4 56.3 85.6 59.4 81.2 23.3
BMMM 66.8 58.7 81.0 81.2 82.7 26.6 38.8
HMM 51.1 41.7 76.3 63.3 82.6 25.8
Brown 66.0 50.1 88.9 28.6 91.3 10.2
BMMM 64.8 50.0 94.4 48.7 87.0 10.5 16.6
HMM 46.3 30.8 68.0 44.6 76.7 3.13
</table>
<tableCaption confidence="0.6693065">
Table 1: Tagging evaluation (M-1, VM, N/V/O
Recall) and directed labeled CCG-Dependency
performance (LF1) as compared to the use of gold
POS tags (Gold) for three clustering algorithms.
</tableCaption>
<bodyText confidence="0.960915230769231">
Results Table 1 presents the parsing and tagging
development results on the two CCG corpora. In
terms of tagging performance, we can see that the
two hard clustering systems significantly outper-
form the HMM, but the relative performance of
Brown and BMMM is mixed.
More importantly, we see that, at least for En-
glish, despite clear differences in tagging perfor-
mance, the parsing results (LF1) are much more
similar. In Chinese, we see that the performance
of the two hard clustering systems is almost iden-
tical, again, not representative of the differences
in the tagging scores. The N/V/O recall scores in
both languages are equally poor predictors of pars-
ing performance. However, these scores show that
having only three labeled tokens per class is suffi-
cient to capture most of the necessary distinctions
for the HDP-CCG. All of this confirms the ob-
servations of Headden et al. (2008) that POS tag-
ging metrics are not correlated with parsing per-
formance. However, since BMMM seems to have
a slight overall advantage, we will be using it as
our clustering system for the remaining experi-
ments.
Since the goal of this work was to produce la-
beled syntactic structures, we also wanted to eval-
uate our performance against that of the HDP-
CCG system that uses gold-standard POS tags. As
we can see in the last two columns of our develop-
ment results in Table 1 and in the final test results
of Table 2, our system is within 2/3 of the labeled
performance of the gold-POS-based HDP-CCG3.
Figure 1 shows an example labeled syntactic
structure induced by the model. We can see
the system successfully learns to attach the final
3To put this result into its full perspective, the LF1 perfor-
mance of a supervised CCG system (Hockenmaier and Steed-
man, 2002), HWDep model, trained on the same length-20
dataset and tested on the simplified CCGbank test set is 80.3.
</bodyText>
<figure confidence="0.8653475">
English
Chinese
</figure>
<page confidence="0.846685">
872
</page>
<table confidence="0.839088666666667">
This Gold
English 26.0 / 51.1 37.1 / 64.9
Chinese 10.3 / 33.5 15.6 / 39.8
</table>
<tableCaption confidence="0.838254">
Table 2: CCG parsing performance (LF1/UF1) on
the test set with and without gold tags.
</tableCaption>
<figure confidence="0.987186333333333">
�N N
N N N S\N (S\S) N N N N (N\N) N N N N
N N
N N
S N\N
N
N
S\S
S
</figure>
<figureCaption confidence="0.999113">
Figure 1: A sample derivation from the WSJ Sec-
</figureCaption>
<equation confidence="0.658436333333333">
S S
tion 22 demonstrating the system is learning most
S
</equation>
<bodyText confidence="0.991394666666667">
of the correct categories of CCGbank but has in-
correctly analyzed the determiner as a preposition.
prepositional phrase, but mistakes the verb for in-
transitive and treats the determiner a as a prepo-
sition. The labeled and undirected recall for this
parse are 5/8 and 7/8 respectively.
</bodyText>
<sectionHeader confidence="0.989427" genericHeader="method">
5 Experiment 2: PASCAL Shared Task
</sectionHeader>
<bodyText confidence="0.999929038461538">
Experimental Setup During the PASCAL
shared task, participants were encouraged to train
over the complete union of the data splits. We
do the same here, use the dev set for choosing
a HDP-CCG hyperparameter, and then present
final results for comparison on the test section.
We vary the hyperparamter for this evaluation
because the datasets fluctuate dramatically in
size from 9K to 700K tokens on sentences up to
length 20. Rather than match all of the tagsets, we
simply induce 49 (excluding punctuation) classes
for every language. The actual tagsets vary from
20 to 304 tags (median 39, mean 78).
Results We now present results for the 10 cor-
pora of the PASCAL shared task (evaluated on all
sentence lengths). Table 3 presents the test per-
formance for each language with the best hyper-
parameter chosen from the set 1100, 1000, 25001.
Also included are the best published results from
the joint tag/dependency induction shared task
(ST) as well as the results from Bisk and Hock-
enmaier (2013), the only existing numbers for
multilingual CCG induction (BH) with gold part-
of-speech tags. Note that the systems in ST do
not have access to any gold-standard POS tags,
whereas our system has access to the gold tags for
</bodyText>
<table confidence="0.977470954545455">
VM N / V / O This ST @15 BH
Czech2500 42 86 / 67 / 67 9.49 33.2 12.2 50.7
English2500 59 87 / 76 / 85 43.8 24.4 51.6 62.9
CHILDES2500 68 84 / 97 / 89 47.2 42.2 47.5 73.3
Portuguese2500 55 88 / 81 / 69 55.5 31.7 55.8 70.5
Dutch1000 50 81 / 81 / 82 39.9 33.7 43.8 54.4
Basque1000 52 2 / 78 / 95 31.1 28.7 35.2 45.0
Swedishl000e50d89/ 74/85 45.8 28.2 52.9 66.9
Slovenel000 50 83/ 7 5/ 79 18.5 19.2 23.6 46.4
N N N S�S N N N� N N N N N S �S d N N N N N N N 58.5
N N N N N N N Nnc punc N pnc punc N 65.1
N N N N 17.8
N Danish100 16.1 31.9 43.7
N N 59 95 / 79 / 82 34.5 44.4
N N N N N
Arabic100N51 85 /76 / 90
N 54 N 34.2 31.8 38.4 59.4
N N
Average 78 / 78 / 82
S TableS3: Tagging VM and N/V/O Recall along-
side Directed Accuracy for our approach and the
����
</table>
<bodyText confidence="0.980289933333333">
best shared task baseline. Additionally, we pro-
vide results for length 15 to compare to previ-
punc
ously published results ([ST]: Best of the PAS-
CAL joint tag/dependency induction shared task
systems; [BH]: Bisk and Hockenmaier (2013).
the three most frequent words of each cluster.
The languages are sorted by the number of non-
punctuation tokens in sentences of up to length
20. Despite our average performance (34.2) being
slightly higher than the shared task (31.8), the st.
deviation is substantial (Q = 15.2 vs QST = 7.5).
It seems apparent from the results that while data
sparsity may play a role in affecting performance,
the more linguistically interesting thread appears
to be morphology. Czech is perhaps a prime ex-
ample, as it has twice the data of the next largest
language (700K tokens vs 336K in English), but
our approach still performs poorly.
Finally, while we saw that the hard clustering
systems outperformed the HMM for our experi-
ments, this is perhaps best explained by analyzing
the average number of gold fine-grained tags per
lexical type in each of the corpora. We found,
counterintuitively, that the “difficult” languages
had lower average number of tags per type (1.01
for Czech, 1.03 for Arabic) than English (1.17)
which was the most ambiguous. This is likely due
to morphology distinguishing otherwise ambigu-
ous lemmas.
</bodyText>
<sectionHeader confidence="0.991346" genericHeader="method">
6 Cluster Analysis
</sectionHeader>
<bodyText confidence="0.999937833333333">
In Table 4, we present the three most frequent
words from several clusters produced by the
BMMM for English and Chinese. We also pro-
vide a noun/verb/other label for each of the words
in the list. One can clearly see that there are many
ambiguous cases where having three labels voting
</bodyText>
<equation confidence="0.915692333333333">
het eN NN S\N S\ \NS N S\S \N N\N N N\N NN
N
N N N N N
</equation>
<page confidence="0.993813">
873
</page>
<subsectionHeader confidence="0.209901">
English Labels Chinese Chinese gloss Labels
</subsectionHeader>
<bodyText confidence="0.356203142857143">
shares, sales, business N, N, N M. 011, Wr4l, tr simultaneously, politics, production O, N, N
the, its, their O, N, N il�, #rT, MOP advance, hold, begin V, V, V
other, interest, chief O, N, O�,�, x1f in, have, for O, V, O
of, in, on O, O, O Q, 6A, AQ China, Taiwan, USA N, N, N
up, expected, made O, V, V th, 44, Vt also, will, then O, O, O
be, make, sell V, V, V )Q, A;,A big, many, high O, N, O *
offer, issue, work N, N, N * &amp; *V, ftA is, desire, representative V, V, N
</bodyText>
<tableCaption confidence="0.8930495">
Table 4: The top three words in BMMM clusters with their noun/verb/other labels. In two cases (marked
with *) all three of the most frequent words also occurred as a verb at least one third of the time.
</tableCaption>
<bodyText confidence="0.99992594117647">
on the class label proves a beneficial signal. We
have also marked two classes with * to draw the
reader’s attention to a fully noun cluster in En-
glish and an other cluster in Chinese which are
highly ambiguous. Specifically, in both of these
cases the frequent words also occur frequently as
verbs, providing additional motivation for a better
soft-clustering algorithm in future work.
How to most effectively use seed knowledge
and annotation is still an open question. Ap-
proaches range from labeling frequent words like
the work of Garrette and Baldridge (2013) to the
recently introduced active learning approach of
Stratos and Collins (2015). In this work, we were
able to demonstrate high noun and verb recall with
the use of a very small set of labeled words be-
cause they correspond to an existing clustering.
In contrast, we found that labeling even the 1000
most frequent words led to very few clusters being
correctly identified; e.g. in English, using the 1000
most frequent words results in identifying 2 verb
and 5 noun clusters, compared to our method’s 9
verb and 16 noun clusters. This is because the
most frequent words tend to be clustered in a few
very large clusters resulting in low coverage.
Stratos and Collins (2015) demonstrated, simi-
larly, that using a POS tagger’s confidence score
to find ambiguous classes can lead to a highly ef-
fective adaptive learning procedure, which strate-
gically labels very few words for a very highly ac-
curate system. Our results align with this research,
leading us to believe that this paradigm of guided
minimal supervision is a fruitful direction for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.996067" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99996804">
In this paper, we have produced the first labeled
syntactic structures from raw text. There remains
a noticeable performance gap due to the use of in-
duced clusters in lieu of gold tags. Based on our
final PASCAL results, there are several languages
where our performance greatly exceeds the cur-
rently published results, but equally many where
we fall short. It also appears to be the case that this
problem correlates with morphology (e.g. Arabic,
Danish, Slovene, Basque, Czech) and some of the
lowest performing intrinsic evaluations of the clus-
tering and N/V/O labeling (Czech and Basque).
In principle, the BMMM is taking morphologi-
cal information into account, as it is provided with
the automatically produced suffixes of Morfessor.
Unfortunately, its treatment of them simply as fea-
tures from a “black box” appears to be too naive
for our purposes. Properly modeling the rela-
tionship between prefixes, stems and suffixes both
within the tag induction and parsing framework is
likely necessary for a high performing system.
Moving forward, additional raw text for train-
ing, as well as enriching the clustering with in-
duced syntactic information (Christodoulopoulos
et al., 2012) may close this gap.
</bodyText>
<sectionHeader confidence="0.991099" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996778">
We want to thank Dan Roth and Cynthia Fisher
for their insight on the task. Additionally, we
would like to thank the anonymous reviewers
for their useful questions and comments. This
material is based upon work supported by the
National Science Foundation under Grants No.
1053856, 1205627, 1405883, by the National In-
stitutes of Health under Grant HD054448, and by
DARPA under agreement number FA8750-13-2-
0008. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the views of the National Science Foundation, the
National Institutes of Health, DARPA or the U.S.
Government.
</bodyText>
<page confidence="0.998282">
874
</page>
<sectionHeader confidence="0.989932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747066037736">
Yonatan Bisk and Julia Hockenmaier. 2012. Simple
Robust Grammar Induction with Combinatory Cat-
egorial Grammars. In Proceedings of the Twenty-
Sixth Conference on Artificial Intelligence (AAAI-
12), pages 1643–1649, Toronto, Canada, July.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
Model for Inducing Combinatory Categorial Gram-
mars. Transactions of the Association for Computa-
tional Linguistics, pages 75–88.
Yonatan Bisk and Julia Hockenmaier. 2015. Prob-
ing the linguistic strengths and limitations of unsu-
pervised grammar induction. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for De-
pendency Parsing. Proceedings of the 2010 Con-
ference on Empirical Methods of Natural Language
Processing, pages 1204–1213, October.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
865–874, Portland, Oregon, USA, June.
Prachya Boonkwan and Mark Steedman. 2011. Gram-
mar Induction from Text Using Small Syntactic Pro-
totypes. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
438–446, Chiang Mai, Thailand, November.
Peter F Brown, Peter V deSouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-Based n-gram Models of Natural Language.
Computational Linguistics, 18.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised PoS induction: How far have we come? In
Proceedings of EMNLP, pages 575–584.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian Mixture Model
for Part-of-Speech Induction Using Multiple Fea-
tures. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, Edinburgh, Scotland, UK., July.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2012. Turning the pipeline into
a loop: iterated unsupervised dependency parsing
and PoS induction. In WILS ’12: Proceedings of
the NAACL-HLT Workshop on the Induction of Lin-
guistic Structure, June.
Stephen Clark, Julia Hockenmaier, and Mark Steed-
man. 2002. Building deep dependency structures
using a wide-coverage ccg parser. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 327–334, Philadelphia,
Pennsylvania, USA, July.
Mathias Creutz and Krista Lagus. 2006. Morfessor in
the Morpho challenge. In Proceedings of the PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes, pages 12–17.
Dan Garrette and Jason Baldridge. 2013. Learning
a Part-of-Speech Tagger from Two Hours of Anno-
tation. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 138–147, Atlanta, Georgia, June.
Dan Garrette, Chris Dyer, Jason Baldridge, and Noah A
Smith. 2015. Weakly-Supervised Grammar-
Informed Bayesian CCG Parser Learning. In Pro-
ceedings of the Association for the Advancement of
Artificial Intelligence.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and
Jo˜ao V Graca. 2012. The PASCAL Challenge
on Grammar Induction. In NAACL HLT Workshop
on Induction of Linguistic Structure, pages 64–80,
Montr´eal, Canada, June.
Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao V Graca,
Fernando Pereira, and Ben Taskar. 2011. Pos-
terior Sparsity in Unsupervised Dependency Pars-
ing. The Journal of Machine Learning Research,
12:455–490, February.
Aria Haghighi and Dan Klein. 2006. Prototype-Driven
Grammar Induction. In Association for Computa-
tional Linguistics, pages 881–888, Morristown, NJ,
USA.
William P. Headden, III, David McClosky, and Eugene
Charniak. 2008. Evaluating unsupervised part-of-
speech tagging for grammar induction. In Proceed-
ings of the 22Nd International Conference on Com-
putational Linguistics - Volume 1, COLING ’08,
pages 329–336, Stroudsburg, PA, USA.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with combina-
tory categorial grammar. In Proceedings of 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 335–342, Philadelphia, Pennsyl-
vania, USA, July.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33:355–396, September.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), January.
</reference>
<page confidence="0.986175">
875
</page>
<reference confidence="0.998890358490566">
Dan Klein and Christopher D Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of
Dependency and Constituency. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL’04), Main Volume, pages
478–485, Barcelona, Spain, July.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244, Cam-
bridge, MA, October.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012. A Universal Part-of-Speech Tagset. In Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2089–2096, Istanbul, Turkey, May.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 410–
420, Prague, Czech Republic, June.
Yoav Seginer. 2007. Fast Unsupervised Incremental
Parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384–391, Prague, Czech Republic, June.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011. Unsupervised Depen-
dency Parsing without Gold Part-of-Speech Tags.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1281–1290, Edinburgh, Scotland, UK., July.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking Out of Local Optima with
Count Transforms and Model Recombination: A
Study in Grammar Induction. In Empirical Methods
in Natural Language Processing.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, September.
Karl Stratos and Michael Collins. 2015. Simple semi-
supervised pos tagging. In Proceedings of the 1st
Workshop on Vector Space Modeling for Natural
Language Processing, pages 79–87, Denver, Col-
orado, June.
Yee-Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Daniel Tse. 2013. Chinese CCGBank: Deep Deriva-
tions and Dependencies for Chinese CCG Parsing.
Ph.D. thesis, The University of Sydney.
</reference>
<page confidence="0.998884">
876
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931327">
<title confidence="0.999585">Labeled Grammar Induction with Minimal Supervision</title>
<author confidence="0.999586">Yonatan Bisk Christos Christodoulopoulos Julia Hockenmaier</author>
<affiliation confidence="0.998783">Department of Computer The University of Illinois at</affiliation>
<address confidence="0.997627">201 N. Goodwin Ave, Urbana, IL</address>
<abstract confidence="0.994939538461538">Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per may be required to induce dependencies from automatically induced word clusters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yonatan Bisk</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Simple Robust Grammar Induction with Combinatory Categorial Grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the TwentySixth Conference on Artificial Intelligence (AAAI12),</booktitle>
<pages>1643--1649</pages>
<location>Toronto, Canada,</location>
<contexts>
<context position="1792" citStr="Bisk and Hockenmaier (2012" startWordPosition="254" endWordPosition="257">n that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) lexicon for POS tags, while Bisk and Hockenmaier (2012; 2013) show that CCG lexicons can be induced automatically if POS tags are used to identify nouns and verbs. However, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we demonstrate that the simple “universal” knowledge of Bisk and Hockenmaier (2013) can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed</context>
</contexts>
<marker>Bisk, Hockenmaier, 2012</marker>
<rawString>Yonatan Bisk and Julia Hockenmaier. 2012. Simple Robust Grammar Induction with Combinatory Categorial Grammars. In Proceedings of the TwentySixth Conference on Artificial Intelligence (AAAI12), pages 1643–1649, Toronto, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Bisk</author>
<author>Julia Hockenmaier</author>
</authors>
<title>An HDP Model for Inducing Combinatory Categorial Grammars. Transactions of the Association for Computational Linguistics,</title>
<date>2013</date>
<pages>75--88</pages>
<contexts>
<context position="2138" citStr="Bisk and Hockenmaier (2013)" startWordPosition="310" endWordPosition="313">oaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) lexicon for POS tags, while Bisk and Hockenmaier (2012; 2013) show that CCG lexicons can be induced automatically if POS tags are used to identify nouns and verbs. However, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we demonstrate that the simple “universal” knowledge of Bisk and Hockenmaier (2013) can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for t</context>
<context position="5309" citStr="Bisk and Hockenmaier (2013)" startWordPosition="834" endWordPosition="837">g this value will prove important when moving between datasets of drastically different sizes. The induction algorithm assumes that a) verbs may be predicates (with category S), b) verbs can take nouns (with category N) or sentences as arguments (leading to categories of the form S|N, (S|N)|N, (S|N)|S etc.), c) any word can act as a modifier, i.e. have a category of the form X|X if it is adjacent to a word with category X or X|Y, and d) modifiers X|X can take nouns or sentences as arguments ((X|X)|N). Our contribution in this paper will be to show that we can replace the gold POS tags used by Bisk and Hockenmaier (2013) with automatically induced word clusters, and then use very minimal supervision to identify noun and verb clusters. 2.1 Inducing Word Clusters We will evaluate three clustering approaches: Brown Clusters Brown clusters (Brown et al., 1992) assign each word to a single cluster using an aglomerative clustering that maximizes the probability of the corpus under a bigram class conditional model. We use Liang’s implementation1. BMMM The Bayesian Multinomial Mixture Model2 (BMMM, Christodoulopoulos et al. 2011) is also a hard clustering system, but has the ability 1https://github.com/percyliang/bro</context>
<context position="14382" citStr="Bisk and Hockenmaier (2013)" startWordPosition="2362" endWordPosition="2366">ize from 9K to 700K tokens on sentences up to length 20. Rather than match all of the tagsets, we simply induce 49 (excluding punctuation) classes for every language. The actual tagsets vary from 20 to 304 tags (median 39, mean 78). Results We now present results for the 10 corpora of the PASCAL shared task (evaluated on all sentence lengths). Table 3 presents the test performance for each language with the best hyperparameter chosen from the set 1100, 1000, 25001. Also included are the best published results from the joint tag/dependency induction shared task (ST) as well as the results from Bisk and Hockenmaier (2013), the only existing numbers for multilingual CCG induction (BH) with gold partof-speech tags. Note that the systems in ST do not have access to any gold-standard POS tags, whereas our system has access to the gold tags for VM N / V / O This ST @15 BH Czech2500 42 86 / 67 / 67 9.49 33.2 12.2 50.7 English2500 59 87 / 76 / 85 43.8 24.4 51.6 62.9 CHILDES2500 68 84 / 97 / 89 47.2 42.2 47.5 73.3 Portuguese2500 55 88 / 81 / 69 55.5 31.7 55.8 70.5 Dutch1000 50 81 / 81 / 82 39.9 33.7 43.8 54.4 Basque1000 52 2 / 78 / 95 31.1 28.7 35.2 45.0 Swedishl000e50d89/ 74/85 45.8 28.2 52.9 66.9 Slovenel000 50 83/ </context>
</contexts>
<marker>Bisk, Hockenmaier, 2013</marker>
<rawString>Yonatan Bisk and Julia Hockenmaier. 2013. An HDP Model for Inducing Combinatory Categorial Grammars. Transactions of the Association for Computational Linguistics, pages 75–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Bisk</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Probing the linguistic strengths and limitations of unsupervised grammar induction.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4246" citStr="Bisk and Hockenmaier (2015)" startWordPosition="649" endWordPosition="652">atural Language Processing (Short Papers), pages 870–876, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics for languages other than English. The system we propose here will use a coarsegrained labeling comprised of three classes, which makes it substantially simpler than traditional tagsets, and uses far fewer labeled tokens than is customary for weakly-supervised approaches (Haghighi and Klein, 2006; Garrette et al., 2015). 2 Our Models Our goal in this work will be to produce labeled dependencies from raw text. Our approach is based on the HDP-CCG parser of Bisk and Hockenmaier (2015) with their extensions to capture lexicalization and punctuation, which, to our knowledge, is the only unsupervised approach to produce labeled dependencies. It first induces a CCG from POS-tagged text, and then estimates a model based on Hierarchical Dirichlet Processes (Teh et al., 2006) over the induced parse forests. The HDP model uses a hyperparameter which controls the amount of smoothing to the base measure of the HDP. Setting this value will prove important when moving between datasets of drastically different sizes. The induction algorithm assumes that a) verbs may be predicates (with</context>
<context position="9692" citStr="Bisk and Hockenmaier (2015)" startWordPosition="1539" endWordPosition="1542">to our task is the ability to accurately identify nouns and verbs: Noun, Verb, and Other Recall We measure the (token-based) recall of our three-way labeling scheme of clusters as noun/verb/other against the universal POS tags of each token. 4 Experiment 1: CCG-based Evaluation Experimental Setup For our primary experiments, we train and test our systems on the English and Chinese CCGbanks, and report directed labeled F1 (LF1) and undirected unlabeled F1 (UF1) over CCG dependencies (Clark et al., 2002). For the labeled evaluation, we follow the simplification of CCGbank categories proposed by Bisk and Hockenmaier (2015): for English to remove morphosyntactic features, map NP to N and change VP modifiers (S\NP)|(S\NP) to sentential modifiers (S|S); for Chinese we map both M and QP to N. In the CCG literature, UF1 is commonly used because undirected dependencies do not penalize argument vs. adjunct distinctions, e.g. for prepositional phrases. For this reason we will include UF1 in the final test set evaluation (Table 2). We use the published train/dev/test splits, using the dev set for choosing a cluster induction algorithm, and then present final performance on the test data. We induce 36 tags for English an</context>
</contexts>
<marker>Bisk, Hockenmaier, 2015</marker>
<rawString>Yonatan Bisk and Julia Hockenmaier. 2015. Probing the linguistic strengths and limitations of unsupervised grammar induction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Conference on Empirical Methods of Natural Language Processing,</booktitle>
<pages>1204--1213</pages>
<contexts>
<context position="1139" citStr="Blunsom and Cohn, 2010" startWordPosition="153" endWordPosition="156">lusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) l</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. Proceedings of the 2010 Conference on Empirical Methods of Natural Language Processing, pages 1204–1213, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A hierarchical pitman-yor process hmm for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>865--874</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="887" citStr="Blunsom and Cohn, 2011" startWordPosition="116" endWordPosition="119">Abstract Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is</context>
<context position="6554" citStr="Blunsom and Cohn (2011)" startWordPosition="1020" endWordPosition="1023">thub.com/christos-c/bmmm to incorporate multiple types of features either at a token level (e.g. f1 context word) or at a type level (e.g. morphology features derived from the Morfessor system (Creutz and Lagus, 2006)). The combination of these features allows BMMM to better capture morphosyntactic information. Bigram HMM We also evaluate unsupervised bigram HMMs, since the soft clustering they provide may be advantageous over the hard Brown and BMMM clusters. But it is known that unsupervised HMMs may not find good POS tags (Johnson, 2007), and in future work, more sophisticated models (e.g. Blunsom and Cohn (2011)), might outperform the systems we use here. In all cases, we assume that we can identify punctuation marks, which are moved to their own cluster and ignored for the purposes of tagging and parsing evaluation. 2.2 Identifying Noun and Verb Clusters To induce CCGs from induced clusters, we need to label them as {noun, verb, other}. This needs to be done judiciously; providing every cluster the verb label, for example, leads to the model identifying prepositions as the main sentential predicates. We demonstrate here that labeling three frequent words per cluster is sufficient to outperform state</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A hierarchical pitman-yor process hmm for unsupervised part of speech induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 865–874, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prachya Boonkwan</author>
<author>Mark Steedman</author>
</authors>
<title>Grammar Induction from Text Using Small Syntactic Prototypes.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>438--446</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="1625" citStr="Boonkwan and Steedman (2011)" startWordPosition="231" endWordPosition="234">2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) lexicon for POS tags, while Bisk and Hockenmaier (2012; 2013) show that CCG lexicons can be induced automatically if POS tags are used to identify nouns and verbs. However, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we demonstrate that the simple “universal” knowledge of Bisk and Hockenmaier (2013) can be easily applied to induced clusters given a small number of words labeled as nou</context>
</contexts>
<marker>Boonkwan, Steedman, 2011</marker>
<rawString>Prachya Boonkwan and Mark Steedman. 2011. Grammar Induction from Text Using Small Syntactic Prototypes. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 438–446, Chiang Mai, Thailand, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-Based n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<contexts>
<context position="5549" citStr="Brown et al., 1992" startWordPosition="869" endWordPosition="872">eading to categories of the form S|N, (S|N)|N, (S|N)|S etc.), c) any word can act as a modifier, i.e. have a category of the form X|X if it is adjacent to a word with category X or X|Y, and d) modifiers X|X can take nouns or sentences as arguments ((X|X)|N). Our contribution in this paper will be to show that we can replace the gold POS tags used by Bisk and Hockenmaier (2013) with automatically induced word clusters, and then use very minimal supervision to identify noun and verb clusters. 2.1 Inducing Word Clusters We will evaluate three clustering approaches: Brown Clusters Brown clusters (Brown et al., 1992) assign each word to a single cluster using an aglomerative clustering that maximizes the probability of the corpus under a bigram class conditional model. We use Liang’s implementation1. BMMM The Bayesian Multinomial Mixture Model2 (BMMM, Christodoulopoulos et al. 2011) is also a hard clustering system, but has the ability 1https://github.com/percyliang/brown-cluster 2https://github.com/christos-c/bmmm to incorporate multiple types of features either at a token level (e.g. f1 context word) or at a type level (e.g. morphology features derived from the Morfessor system (Creutz and Lagus, 2006))</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V deSouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised PoS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>575--584</pages>
<contexts>
<context position="862" citStr="Christodoulopoulos et al., 2010" startWordPosition="112" endWordPosition="115">,christod,juliahmr}@illinois.edu Abstract Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). Th</context>
<context position="8858" citStr="Christodoulopoulos et al., 2010" startWordPosition="1407" endWordPosition="1410">e datasets in advance. This alleviates having to handle tagging of unknown words. To provide an intuition for the performance of the induced word clusters, we provide two standard metrics for unsupervised tagging: Many-to-one (M-1) A commonly used measure, M-1 relies on mapping each cluster to the most common POS tag of its words. However, M1 can be easily inflated by inducing more clusters. V-Measure Proposed by Rosenberg and Hirschberg (2007), V-Measure (VM) measures the information-theoretic distance between two clusterings and has been shown to be robust to the number of induced clusters (Christodoulopoulos et al., 2010). Both of these metrics are known to be highly dependent on the gold annotation standards they are compared against, and may not correlate with downstream performance at parsing. Of more immediate relevance to our task is the ability to accurately identify nouns and verbs: Noun, Verb, and Other Recall We measure the (token-based) recall of our three-way labeling scheme of clusters as noun/verb/other against the universal POS tags of each token. 4 Experiment 1: CCG-based Evaluation Experimental Setup For our primary experiments, we train and test our systems on the English and Chinese CCGbanks,</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised PoS induction: How far have we come? In Proceedings of EMNLP, pages 575–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>A Bayesian Mixture Model for Part-of-Speech Induction Using Multiple Features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="5820" citStr="Christodoulopoulos et al. 2011" startWordPosition="910" endWordPosition="913">Our contribution in this paper will be to show that we can replace the gold POS tags used by Bisk and Hockenmaier (2013) with automatically induced word clusters, and then use very minimal supervision to identify noun and verb clusters. 2.1 Inducing Word Clusters We will evaluate three clustering approaches: Brown Clusters Brown clusters (Brown et al., 1992) assign each word to a single cluster using an aglomerative clustering that maximizes the probability of the corpus under a bigram class conditional model. We use Liang’s implementation1. BMMM The Bayesian Multinomial Mixture Model2 (BMMM, Christodoulopoulos et al. 2011) is also a hard clustering system, but has the ability 1https://github.com/percyliang/brown-cluster 2https://github.com/christos-c/bmmm to incorporate multiple types of features either at a token level (e.g. f1 context word) or at a type level (e.g. morphology features derived from the Morfessor system (Creutz and Lagus, 2006)). The combination of these features allows BMMM to better capture morphosyntactic information. Bigram HMM We also evaluate unsupervised bigram HMMs, since the soft clustering they provide may be advantageous over the hard Brown and BMMM clusters. But it is known that uns</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2011</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2011. A Bayesian Mixture Model for Part-of-Speech Induction Using Multiple Features. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Turning the pipeline into a loop: iterated unsupervised dependency parsing and PoS induction.</title>
<date>2012</date>
<booktitle>In WILS ’12: Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<contexts>
<context position="1002" citStr="Christodoulopoulos et al., 2012" startWordPosition="133" endWordPosition="136">gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonk</context>
<context position="2896" citStr="Christodoulopoulos et al. (2012)" startWordPosition="438" endWordPosition="441">mount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters but did not provide evaluation 870 Proceedings </context>
<context position="20611" citStr="Christodoulopoulos et al., 2012" startWordPosition="3502" endWordPosition="3505">g and N/V/O labeling (Czech and Basque). In principle, the BMMM is taking morphological information into account, as it is provided with the automatically produced suffixes of Morfessor. Unfortunately, its treatment of them simply as features from a “black box” appears to be too naive for our purposes. Properly modeling the relationship between prefixes, stems and suffixes both within the tag induction and parsing framework is likely necessary for a high performing system. Moving forward, additional raw text for training, as well as enriching the clustering with induced syntactic information (Christodoulopoulos et al., 2012) may close this gap. 8 Acknowledgments We want to thank Dan Roth and Cynthia Fisher for their insight on the task. Additionally, we would like to thank the anonymous reviewers for their useful questions and comments. This material is based upon work supported by the National Science Foundation under Grants No. 1053856, 1205627, 1405883, by the National Institutes of Health under Grant HD054448, and by DARPA under agreement number FA8750-13-2- 0008. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2012</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2012. Turning the pipeline into a loop: iterated unsupervised dependency parsing and PoS induction. In WILS ’12: Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building deep dependency structures using a wide-coverage ccg parser.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>327--334</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="9572" citStr="Clark et al., 2002" startWordPosition="1521" endWordPosition="1524">are compared against, and may not correlate with downstream performance at parsing. Of more immediate relevance to our task is the ability to accurately identify nouns and verbs: Noun, Verb, and Other Recall We measure the (token-based) recall of our three-way labeling scheme of clusters as noun/verb/other against the universal POS tags of each token. 4 Experiment 1: CCG-based Evaluation Experimental Setup For our primary experiments, we train and test our systems on the English and Chinese CCGbanks, and report directed labeled F1 (LF1) and undirected unlabeled F1 (UF1) over CCG dependencies (Clark et al., 2002). For the labeled evaluation, we follow the simplification of CCGbank categories proposed by Bisk and Hockenmaier (2015): for English to remove morphosyntactic features, map NP to N and change VP modifiers (S\NP)|(S\NP) to sentential modifiers (S|S); for Chinese we map both M and QP to N. In the CCG literature, UF1 is commonly used because undirected dependencies do not penalize argument vs. adjunct distinctions, e.g. for prepositional phrases. For this reason we will include UF1 in the final test set evaluation (Table 2). We use the published train/dev/test splits, using the dev set for choos</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002. Building deep dependency structures using a wide-coverage ccg parser. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 327–334, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Morfessor in the Morpho challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes,</booktitle>
<pages>12--17</pages>
<contexts>
<context position="6148" citStr="Creutz and Lagus, 2006" startWordPosition="954" endWordPosition="957">ers (Brown et al., 1992) assign each word to a single cluster using an aglomerative clustering that maximizes the probability of the corpus under a bigram class conditional model. We use Liang’s implementation1. BMMM The Bayesian Multinomial Mixture Model2 (BMMM, Christodoulopoulos et al. 2011) is also a hard clustering system, but has the ability 1https://github.com/percyliang/brown-cluster 2https://github.com/christos-c/bmmm to incorporate multiple types of features either at a token level (e.g. f1 context word) or at a type level (e.g. morphology features derived from the Morfessor system (Creutz and Lagus, 2006)). The combination of these features allows BMMM to better capture morphosyntactic information. Bigram HMM We also evaluate unsupervised bigram HMMs, since the soft clustering they provide may be advantageous over the hard Brown and BMMM clusters. But it is known that unsupervised HMMs may not find good POS tags (Johnson, 2007), and in future work, more sophisticated models (e.g. Blunsom and Cohn (2011)), might outperform the systems we use here. In all cases, we assume that we can identify punctuation marks, which are moved to their own cluster and ignored for the purposes of tagging and pars</context>
</contexts>
<marker>Creutz, Lagus, 2006</marker>
<rawString>Mathias Creutz and Krista Lagus. 2006. Morfessor in the Morpho challenge. In Proceedings of the PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes, pages 12–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a Part-of-Speech Tagger from Two Hours of Annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>138--147</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="18355" citStr="Garrette and Baldridge (2013)" startWordPosition="3131" endWordPosition="3134">nt words also occurred as a verb at least one third of the time. on the class label proves a beneficial signal. We have also marked two classes with * to draw the reader’s attention to a fully noun cluster in English and an other cluster in Chinese which are highly ambiguous. Specifically, in both of these cases the frequent words also occur frequently as verbs, providing additional motivation for a better soft-clustering algorithm in future work. How to most effectively use seed knowledge and annotation is still an open question. Approaches range from labeling frequent words like the work of Garrette and Baldridge (2013) to the recently introduced active learning approach of Stratos and Collins (2015). In this work, we were able to demonstrate high noun and verb recall with the use of a very small set of labeled words because they correspond to an existing clustering. In contrast, we found that labeling even the 1000 most frequent words led to very few clusters being correctly identified; e.g. in English, using the 1000 most frequent words results in identifying 2 verb and 5 noun clusters, compared to our method’s 9 verb and 16 noun clusters. This is because the most frequent words tend to be clustered in a f</context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a Part-of-Speech Tagger from Two Hours of Annotation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 138–147, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Chris Dyer</author>
<author>Jason Baldridge</author>
<author>Noah A Smith</author>
</authors>
<title>Weakly-Supervised GrammarInformed Bayesian CCG Parser Learning.</title>
<date>2015</date>
<booktitle>In Proceedings of the Association for the Advancement of Artificial Intelligence.</booktitle>
<contexts>
<context position="4080" citStr="Garrette et al., 2015" startWordPosition="618" endWordPosition="621">ot provide evaluation 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics for languages other than English. The system we propose here will use a coarsegrained labeling comprised of three classes, which makes it substantially simpler than traditional tagsets, and uses far fewer labeled tokens than is customary for weakly-supervised approaches (Haghighi and Klein, 2006; Garrette et al., 2015). 2 Our Models Our goal in this work will be to produce labeled dependencies from raw text. Our approach is based on the HDP-CCG parser of Bisk and Hockenmaier (2015) with their extensions to capture lexicalization and punctuation, which, to our knowledge, is the only unsupervised approach to produce labeled dependencies. It first induces a CCG from POS-tagged text, and then estimates a model based on Hierarchical Dirichlet Processes (Teh et al., 2006) over the induced parse forests. The HDP model uses a hyperparameter which controls the amount of smoothing to the base measure of the HDP. Sett</context>
</contexts>
<marker>Garrette, Dyer, Baldridge, Smith, 2015</marker>
<rawString>Dan Garrette, Chris Dyer, Jason Baldridge, and Noah A Smith. 2015. Weakly-Supervised GrammarInformed Bayesian CCG Parser Learning. In Proceedings of the Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Gelling</author>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Jo˜ao V Graca</author>
</authors>
<title>The PASCAL Challenge on Grammar Induction.</title>
<date>2012</date>
<booktitle>In NAACL HLT Workshop on Induction of Linguistic Structure,</booktitle>
<pages>64--80</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2848" citStr="Gelling et al., 2012" startWordPosition="431" endWordPosition="434"> verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters</context>
</contexts>
<marker>Gelling, Cohn, Blunsom, Graca, 2012</marker>
<rawString>Douwe Gelling, Trevor Cohn, Phil Blunsom, and Jo˜ao V Graca. 2012. The PASCAL Challenge on Grammar Induction. In NAACL HLT Workshop on Induction of Linguistic Structure, pages 64–80, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Gillenwater</author>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Graca</author>
<author>Fernando Pereira</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior Sparsity in Unsupervised Dependency Parsing.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--455</pages>
<contexts>
<context position="3192" citStr="Gillenwater et al., 2011" startWordPosition="488" endWordPosition="492">anks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters but did not provide evaluation 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics for languages other than English</context>
</contexts>
<marker>Gillenwater, Ganchev, Graca, Pereira, Taskar, 2011</marker>
<rawString>Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao V Graca, Fernando Pereira, and Ben Taskar. 2011. Posterior Sparsity in Unsupervised Dependency Parsing. The Journal of Machine Learning Research, 12:455–490, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-Driven Grammar Induction.</title>
<date>2006</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<pages>881--888</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4056" citStr="Haghighi and Klein, 2006" startWordPosition="614" endWordPosition="617">g Brown clusters but did not provide evaluation 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics for languages other than English. The system we propose here will use a coarsegrained labeling comprised of three classes, which makes it substantially simpler than traditional tagsets, and uses far fewer labeled tokens than is customary for weakly-supervised approaches (Haghighi and Klein, 2006; Garrette et al., 2015). 2 Our Models Our goal in this work will be to produce labeled dependencies from raw text. Our approach is based on the HDP-CCG parser of Bisk and Hockenmaier (2015) with their extensions to capture lexicalization and punctuation, which, to our knowledge, is the only unsupervised approach to produce labeled dependencies. It first induces a CCG from POS-tagged text, and then estimates a model based on Hierarchical Dirichlet Processes (Teh et al., 2006) over the induced parse forests. The HDP model uses a hyperparameter which controls the amount of smoothing to the base </context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-Driven Grammar Induction. In Association for Computational Linguistics, pages 881–888, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden</author>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Evaluating unsupervised part-ofspeech tagging for grammar induction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>329--336</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11749" citStr="Headden et al. (2008)" startWordPosition="1891" endWordPosition="1894">e importantly, we see that, at least for English, despite clear differences in tagging performance, the parsing results (LF1) are much more similar. In Chinese, we see that the performance of the two hard clustering systems is almost identical, again, not representative of the differences in the tagging scores. The N/V/O recall scores in both languages are equally poor predictors of parsing performance. However, these scores show that having only three labeled tokens per class is sufficient to capture most of the necessary distinctions for the HDP-CCG. All of this confirms the observations of Headden et al. (2008) that POS tagging metrics are not correlated with parsing performance. However, since BMMM seems to have a slight overall advantage, we will be using it as our clustering system for the remaining experiments. Since the goal of this work was to produce labeled syntactic structures, we also wanted to evaluate our performance against that of the HDPCCG system that uses gold-standard POS tags. As we can see in the last two columns of our development results in Table 1 and in the final test results of Table 2, our system is within 2/3 of the labeled performance of the gold-POS-based HDP-CCG3. Figur</context>
</contexts>
<marker>Headden, McClosky, Charniak, 2008</marker>
<rawString>William P. Headden, III, David McClosky, and Eugene Charniak. 2008. Evaluating unsupervised part-ofspeech tagging for grammar induction. In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 329–336, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="12608" citStr="Hockenmaier and Steedman, 2002" startWordPosition="2043" endWordPosition="2047">his work was to produce labeled syntactic structures, we also wanted to evaluate our performance against that of the HDPCCG system that uses gold-standard POS tags. As we can see in the last two columns of our development results in Table 1 and in the final test results of Table 2, our system is within 2/3 of the labeled performance of the gold-POS-based HDP-CCG3. Figure 1 shows an example labeled syntactic structure induced by the model. We can see the system successfully learns to attach the final 3To put this result into its full perspective, the LF1 performance of a supervised CCG system (Hockenmaier and Steedman, 2002), HWDep model, trained on the same length-20 dataset and tested on the simplified CCGbank test set is 80.3. English Chinese 872 This Gold English 26.0 / 51.1 37.1 / 64.9 Chinese 10.3 / 33.5 15.6 / 39.8 Table 2: CCG parsing performance (LF1/UF1) on the test set with and without gold tags. �N N N N N S\N (S\S) N N N N (N\N) N N N N N N N N S N\N N N S\S S Figure 1: A sample derivation from the WSJ SecS S tion 22 demonstrating the system is learning most S of the correct categories of CCGbank but has incorrectly analyzed the determiner as a preposition. prepositional phrase, but mistakes the verb</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with combinatory categorial grammar. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 335–342, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics,</title>
<date>2007</date>
<pages>33--355</pages>
<contexts>
<context position="2538" citStr="Hockenmaier and Steedman, 2007" startWordPosition="378" endWordPosition="382">er, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we demonstrate that the simple “universal” knowledge of Bisk and Hockenmaier (2013) can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV mod</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33:355–396, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<contexts>
<context position="6477" citStr="Johnson, 2007" startWordPosition="1009" endWordPosition="1010">the ability 1https://github.com/percyliang/brown-cluster 2https://github.com/christos-c/bmmm to incorporate multiple types of features either at a token level (e.g. f1 context word) or at a type level (e.g. morphology features derived from the Morfessor system (Creutz and Lagus, 2006)). The combination of these features allows BMMM to better capture morphosyntactic information. Bigram HMM We also evaluate unsupervised bigram HMMs, since the soft clustering they provide may be advantageous over the hard Brown and BMMM clusters. But it is known that unsupervised HMMs may not find good POS tags (Johnson, 2007), and in future work, more sophisticated models (e.g. Blunsom and Cohn (2011)), might outperform the systems we use here. In all cases, we assume that we can identify punctuation marks, which are moved to their own cluster and ignored for the purposes of tagging and parsing evaluation. 2.2 Identifying Noun and Verb Clusters To induce CCGs from induced clusters, we need to label them as {noun, verb, other}. This needs to be done judiciously; providing every cluster the verb label, for example, leads to the model identifying prepositions as the main sentential predicates. We demonstrate here tha</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>CorpusBased Induction of Syntactic Structure: Models of Dependency and Constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>478--485</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1089" citStr="Klein and Manning, 2004" startWordPosition="145" endWordPosition="148">unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combi</context>
<context position="3165" citStr="Klein and Manning, 2004" startWordPosition="484" endWordPosition="487"> Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters but did not provide evaluation 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics for l</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D Manning. 2004. CorpusBased Induction of Syntactic Structure: Models of Dependency and Constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 478–485, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1234--1244</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="1458" citStr="Naseem et al., 2010" startWordPosition="206" endWordPosition="209">doulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) lexicon for POS tags, while Bisk and Hockenmaier (2012; 2013) show that CCG lexicons can be induced automatically if POS tags are used to identify nouns and verbs. However, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we </context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234–1244, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A Universal Part-of-Speech Tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>2089--2096</pages>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="7340" citStr="Petrov et al., 2012" startWordPosition="1150" endWordPosition="1153">purposes of tagging and parsing evaluation. 2.2 Identifying Noun and Verb Clusters To induce CCGs from induced clusters, we need to label them as {noun, verb, other}. This needs to be done judiciously; providing every cluster the verb label, for example, leads to the model identifying prepositions as the main sentential predicates. We demonstrate here that labeling three frequent words per cluster is sufficient to outperform state-of-the-art performance on grammar induction from raw text in many languages. We emulate having a native speaker annotate words for us by using the universal tagset (Petrov et al., 2012) as our source of labels for the most frequent three words per cluster (we map the tags NOUN, NUM, PRON to noun, VERB to verb, and all others to other). The final labeling is a majority vote, where each word type contributes a vote for each label it can take (see Table 4 for some examples). This approach could easily be scaled to allow more words per cluster to vote. But we will see that three per cluster is sufficient to label most tokens correctly. 3 Experimental Setup We will focus first on producing CCG labeled predicate-argument dependencies for English and Chinese and will then apply our</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A Universal Part-of-Speech Tagset. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 2089–2096, Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8674" citStr="Rosenberg and Hirschberg (2007)" startWordPosition="1381" endWordPosition="1384">ask. All languages will be trained on sentences of up to length 20 (not counting punctuation). All cluster induction algorithms are treated as black boxes 871 and run over the complete datasets in advance. This alleviates having to handle tagging of unknown words. To provide an intuition for the performance of the induced word clusters, we provide two standard metrics for unsupervised tagging: Many-to-one (M-1) A commonly used measure, M-1 relies on mapping each cluster to the most common POS tag of its words. However, M1 can be easily inflated by inducing more clusters. V-Measure Proposed by Rosenberg and Hirschberg (2007), V-Measure (VM) measures the information-theoretic distance between two clusterings and has been shown to be robust to the number of induced clusters (Christodoulopoulos et al., 2010). Both of these metrics are known to be highly dependent on the gold annotation standards they are compared against, and may not correlate with downstream performance at parsing. Of more immediate relevance to our task is the ability to accurately identify nouns and verbs: Noun, Verb, and Other Recall We measure the (token-based) recall of our three-way labeling scheme of clusters as noun/verb/other against the u</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410– 420, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast Unsupervised Incremental Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>384--391</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="943" citStr="Seginer, 2007" startWordPosition="127" endWordPosition="128"> induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Fast Unsupervised Incremental Parsing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 384–391, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Angel X Chang</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised Dependency Parsing without Gold Part-of-Speech Tags.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1281--1290</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="968" citStr="Spitkovsky et al., 2011" startWordPosition="129" endWordPosition="132">ed dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the stan</context>
<context position="3391" citStr="Spitkovsky et al. (2011)" startWordPosition="521" endWordPosition="524">from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters but did not provide evaluation 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics for languages other than English. The system we propose here will use a coarsegrained labeling comprised of three classes, which makes it substantially simpler than traditional tagsets, and uses far fewer labeled tokens than is cus</context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang, and Daniel Jurafsky. 2011. Unsupervised Dependency Parsing without Gold Part-of-Speech Tags. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281–1290, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1114" citStr="Spitkovsky et al., 2013" startWordPosition="149" endWordPosition="152">as. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2013</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="1736" citStr="Steedman (2000)" startWordPosition="247" endWordPosition="248">om and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) lexicon for POS tags, while Bisk and Hockenmaier (2012; 2013) show that CCG lexicons can be induced automatically if POS tags are used to identify nouns and verbs. However, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we demonstrate that the simple “universal” knowledge of Bisk and Hockenmaier (2013) can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Michael Collins</author>
</authors>
<title>Simple semisupervised pos tagging.</title>
<date>2015</date>
<booktitle>In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,</booktitle>
<pages>79--87</pages>
<location>Denver, Colorado,</location>
<contexts>
<context position="18437" citStr="Stratos and Collins (2015)" startWordPosition="3143" endWordPosition="3146">roves a beneficial signal. We have also marked two classes with * to draw the reader’s attention to a fully noun cluster in English and an other cluster in Chinese which are highly ambiguous. Specifically, in both of these cases the frequent words also occur frequently as verbs, providing additional motivation for a better soft-clustering algorithm in future work. How to most effectively use seed knowledge and annotation is still an open question. Approaches range from labeling frequent words like the work of Garrette and Baldridge (2013) to the recently introduced active learning approach of Stratos and Collins (2015). In this work, we were able to demonstrate high noun and verb recall with the use of a very small set of labeled words because they correspond to an existing clustering. In contrast, we found that labeling even the 1000 most frequent words led to very few clusters being correctly identified; e.g. in English, using the 1000 most frequent words results in identifying 2 verb and 5 noun clusters, compared to our method’s 9 verb and 16 noun clusters. This is because the most frequent words tend to be clustered in a few very large clusters resulting in low coverage. Stratos and Collins (2015) demon</context>
</contexts>
<marker>Stratos, Collins, 2015</marker>
<rawString>Karl Stratos and Michael Collins. 2015. Simple semisupervised pos tagging. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 79–87, Denver, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee-Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet Processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="4536" citStr="Teh et al., 2006" startWordPosition="693" endWordPosition="696"> than traditional tagsets, and uses far fewer labeled tokens than is customary for weakly-supervised approaches (Haghighi and Klein, 2006; Garrette et al., 2015). 2 Our Models Our goal in this work will be to produce labeled dependencies from raw text. Our approach is based on the HDP-CCG parser of Bisk and Hockenmaier (2015) with their extensions to capture lexicalization and punctuation, which, to our knowledge, is the only unsupervised approach to produce labeled dependencies. It first induces a CCG from POS-tagged text, and then estimates a model based on Hierarchical Dirichlet Processes (Teh et al., 2006) over the induced parse forests. The HDP model uses a hyperparameter which controls the amount of smoothing to the base measure of the HDP. Setting this value will prove important when moving between datasets of drastically different sizes. The induction algorithm assumes that a) verbs may be predicates (with category S), b) verbs can take nouns (with category N) or sentences as arguments (leading to categories of the form S|N, (S|N)|N, (S|N)|S etc.), c) any word can act as a modifier, i.e. have a category of the form X|X if it is adjacent to a word with category X or X|Y, and d) modifiers X|X</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee-Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical Dirichlet Processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Tse</author>
</authors>
<title>Chinese CCGBank: Deep Derivations and Dependencies for Chinese CCG Parsing.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Sydney.</institution>
<contexts>
<context position="2562" citStr="Tse, 2013" startWordPosition="385" endWordPosition="386">stic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we demonstrate that the simple “universal” knowledge of Bisk and Hockenmaier (2013) can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2</context>
</contexts>
<marker>Tse, 2013</marker>
<rawString>Daniel Tse. 2013. Chinese CCGBank: Deep Derivations and Dependencies for Chinese CCG Parsing. Ph.D. thesis, The University of Sydney.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>