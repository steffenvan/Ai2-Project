<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001027">
<title confidence="0.972175">
Guiding an HPSG Parser using Semantic and Pragmatic Expectations
</title>
<author confidence="0.996025">
Jim Skon
</author>
<affiliation confidence="0.9881985">
Computer and Information Science Department
The Ohio State University
</affiliation>
<address confidence="0.537824">
Columbus, OH 43210, USA
</address>
<email confidence="0.968738">
Internet: skon@cis.ohio-state.edu
</email>
<sectionHeader confidence="0.523717" genericHeader="abstract">
Abstract&apos;
</sectionHeader>
<bodyText confidence="0.999193125">
Efficient natural language generation has been successfully
demonstrated using highly compiled knowledge about speech
acts and their related social actions. A design and prototype
implementation of a parser which utilizes this same pragmatic
knowledge to efficiently guide parsing is presented. Such
guidance is shown to prune the search space and thus avoid
needless processing of pragmatically unlikely constituent
structures.
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999617315789474">
The use of purely syntactic knowledge during the parse
phase of natural language understanding yields considerable
local ambiguity (consideration of impossible subconstituents)
as well global ambiguity (construction of syntactically valid
parses not applicable to the socio-pragmatic context).
This research investigates bringing socio-pragmatic
knowledge to bear during the parse, while maintaining a
domain independent grammar and parser. The particular
technique explored uses knowledge about the pragmatic context
to order the consideration of proposed parse constituents, thus
guiding the parser to consider the best (wrt the expectations)
solutions first. Such a search may be classified as a best-
first search.
The theoretical models used to represent the pragmatic
knowledge in this study are based on Halliday&apos;s Systemic
Grammar and a model of the pragmatics of conversation. The
model used to represent the syntax and domain independent
semantic knowledge is HPSG - Head-driven Phrase Structure
Grammar.
</bodyText>
<sectionHeader confidence="0.885253" genericHeader="background">
BACKGROUND
</sectionHeader>
<bodyText confidence="0.999683944444444">
Patten, Geis and Becker (1992) demonstrate the
application of knowledge compilation to achieve the rapid
generation of natural language. Their mechanism is based on
Halliday&apos;s systemic networks, and on Geis&apos; theory of the
pragmatics of conversation. A model of conversation using
principled compilation of pragmatic knowledge and other
linguistic knowledge is used to permit the application of
pragmatic inference without expensive computation. A
pragmatic component is used to model social action, including
speech acts, and utilize conventions of use involving such
features of context such as politeness, mister, and stylistic
features. These politeness features are critic,41 to the account of
indirect speech acts. This pragmatic knoVIedge is compiled
into course-grained knowledge in the form of a classification
hierarchy. A planner component uses knowledge about
conditions which need to be satisfied (discourse goals) to
produce a set of pragmatic features which characterize a desired
utterance. These features are mapped into the systemic
</bodyText>
<footnote confidence="0.706624666666667">
1 Research Funded by The Ohio State Center for Cognitive
Science and The Ohio State Departments of Computer and
Information Science and Linguistics
</footnote>
<bodyText confidence="0.988209789473684">
grammar (using compiled knowledge) which is then used to
realize the actual utterance.
The syntactic/semantic component used in this study is a
parser based on the HPSG (Head Driven Phrase Structure
Grammar) theory of grammar (Pollard and Sag, 1992). HPSG
models all linguistic constituents in terms of partial
information structures called feature structures.
Linguistic signs incorporate simultaneous representation of
phonological, syntactic, and semantic attributes of
grammatical constituents. HPSG is a lexicalized theory,
with the lexical definitions, rather then phrase structure rules,
specifying most configurational constraints. Control (such as
subcategorization, for example) is asserted by the use of HPSG
constraints - partially filled in feature structures called feature
descriptions, which constrain possible HPSG feature structures
by asserting specific attributes and/or labels.
A HPSG based chart parser, under development at the
author&apos;s university, was used for the implementation part of
this study.
</bodyText>
<sectionHeader confidence="0.865605" genericHeader="method">
FEATURE MAPPING
</sectionHeader>
<bodyText confidence="0.99979225">
Planning &amp; generation of coherent &amp;quot;speech&amp;quot; in a
conversation requires some understanding of the &amp;quot;hearer&apos;s&amp;quot;
perspective. Thus the speaker naturally has some (limited)
knowledge about possible responses from the hearer. This
knowledge can be given to the same planner used for
generation, producing a partial set of pragmatic features or
expectations. These pragmatic expectations can then be
mapped into the systemic grammar, producing a set of
semantic and syntactic expectations about what other
participants in the conversation will say.
The technique explored here is to bring such expectations
to bear during the parse process, guiding the parser to the most
likely solution in a best-first manner. It is thus necessary that
the generated expectations be mapped into a form which can be
directly compared with constituents proposed within the HPSG
parse.
Consider the sentence &amp;quot;Robin promised to come at
noon&amp;quot;, with the following context:
Sandy: &amp;quot;I guess we should get started, what time did they
say they would be here?&amp;quot;
Kim: &amp;quot;Robin promised to come at noon&amp;quot;
A set of plausible partial expectations generated by the
pragmatic and systemic components in anticipation of Kim&apos;s
response might be:
</bodyText>
<equation confidence="0.992665833333333">
((S) (UNMARKED-DECLARATIVE))
((S SUBJECT) (PROPER))
((S BETA) (NONFINITEPRED))
((S PREDICATOR) (PROMISED))
((S BETA TEMPORAL) (PP))
((S BETA PREDICATOR) (ARRIVAL))
</equation>
<bodyText confidence="0.9994385">
In these expectations the first list of each pair (e.g. (S BETA))
represents a functional role within the expected sentence. The
</bodyText>
<page confidence="0.988878">
295
</page>
<bodyText confidence="0.974099722222222">
second list in each pair are sets (in this case singleton) of
expected features for the associated functional roles. These
expected features assert expectations which are both semantic
(e.g. PROMISED) and syntactic (e.g. ((S BETA
TEMPORAL) (PP)) asserts both the existance and location of
a temporal adjunct PP).
Note that in these expectations the temporal adjunct &amp;quot;at
noon&amp;quot; should modify the embedded clause &amp;quot;to come&amp;quot;, as would
be expected in the specified context.
Next consider the possible HPSG parses of the example
sentence. Figures 1 and 2 below illustrate two semantically
distinct parses generated by our HPSG parser.
Mapping expected features into HPSG constraints:
Features generated from pragmatic expectations can be
mapped into constraints on FIPSO structures, stated in terms
of feature descriptions. Below are the HPSG feature
descriptions corresponding to the pragmatically generated
features PP and UNMARKED-DECLARATIVE.
</bodyText>
<equation confidence="0.482537111111111">
PP = [SYNSEMILOCICAT [HEAD prep
MARKING unmarked
cat
phrase
Figure 3.
UNMARKED-DECLARATIVE =
[
DTRSIHEAD-DTRISYNSEM
phrase SUBJ-DTRISYNSEMILOCICATIHEAD noun
</equation>
<figureCaption confidence="0.977409">
Figure 4.
</figureCaption>
<bodyText confidence="0.99378475">
Mapping expected functional roles into HPSG
constituent structure:
Pragmatic expectations are expected within certain
functional roles, such a SUBJECT, PREDICATOR, BETA
(the embedded clause) etc. This structural information must be
used to assert the constraints into the relevant HPSG
substructures. This mapping is not as straightforward as the
feature mapping technique, as the structure induced by the
systemic grammar is &amp;quot;flatter&amp;quot; than the structure produced by
HPSG.
Consider the following pragmatically generated
expectation:
</bodyText>
<equation confidence="0.52553">
((S TEMPORAL) (PP))
</equation>
<bodyText confidence="0.6976355">
Such an expectation may be realized by great variety of
HPSG structural realizations. e.g.:
</bodyText>
<listItem confidence="0.98120025">
1. Kim ran at noon
2. Kim could run home at noon
3. Kim could have been running home at noon
4. Kim ran east at noon.
</listItem>
<bodyText confidence="0.998049">
In these examples modal verb operators (1-3) and multiple
adjuncts (4) vary the actual structural depth of the temporal PP
within the HPSG model. Thus a given systemic role path
may have numerous HPSG constituent path realizations. One
possible mapping technique is to generate constraints
expressing all possible HPSG structural variants. This,
however would lead in many cases to a combinatorial
explosion of constraints. The technique employed by this
study was to add a new clause attribute to verbal HPSG signs,
and use this attribute to embed within the signs a &amp;quot;clausally
flattened&amp;quot; structures. Each HPSG verbal sign in the same
clause structure shares the same clausal value. The clause
value is a structure with labels for each systemic role, where
each label points to the constituent which fills that role in the
given verbal clause. A clausal boundry is said to exist
between distinct clausal domains. A clausal structure is
illustrated in figure 5:
</bodyText>
<figure confidence="0.540548">
H
[PREDICATOR V[promised]
[CLAUSE SUBJECT NP[Robin]
BETA VP(to come at noon]]
[CLAUSE PREDICATOR V[come] Ti
TEMPORAL PP[at noon]]
</figure>
<figureCaption confidence="0.987974">
Figure 5.
</figureCaption>
<bodyText confidence="0.6949708">
The current mapping only considers the mapping of roles
within verbal signs. Similar role structures may exist for
other constituent types, such as for noun phrase. Thus far the
verbal clause boundary definition has been adequate for other
phrasal structures.
</bodyText>
<sectionHeader confidence="0.949284" genericHeader="method">
GUIDING THE HPSG PARSE
</sectionHeader>
<bodyText confidence="0.9956535">
The guidance strategy employed is to evaluate all
proposed edges (i.e. complete and partially complete
constituents) against the expectations, ranking each based on
the relative similarity with the expectations. These edges are
</bodyText>
<figure confidence="0.986237">
NP
Robin
promised to
</figure>
<figureCaption confidence="0.942414">
Figure 1.
</figureCaption>
<figure confidence="0.9955491">
VP c
VP
V
promised
VP
VP
rg—VP
H
V
to come
</figure>
<figureCaption confidence="0.912088">
Figure 2.
</figureCaption>
<figure confidence="0.999155863636364">
NP
Robin
PP
at noon
PP
at noon
VP
H
V
come
NP
ZS.
Robin
VP D
k
come
PP
Fit\NC\N
P p
at noon
promised
1]
</figure>
<page confidence="0.997556">
296
</page>
<bodyText confidence="0.990892514285714">
then placed in an agenda (a list of priority queues) and
removed from the agenda and included in the partial parse in a
best first order.
Critical to the success of a best-first algorithm is the
heuristic evaluation function used to order the proposed
constituents.
The heuristic evaluation function:
The heuristic evaluation function is based on three specific
types of tests:
I. Role match - does a constituent match a role&apos;s set of
expected features?
II. Role path match - is a constituent role path compatible
with the roles of its children?
III. Clausal completeness - are all clausal roles expected for
this constituent present?
Tests II and III above require that constituents under
consideration have roles already assigned to them. For
example, in the case of II, the test requires roles for both the
new constituent and the proposed daughters of the constituent.
But since the parse strategy employeed is bottom-up, role
paths cannot be anchored to a root, and thus fully known, until
Parse completion. The solution to this dilemma is to
hypothesise a constituent&apos;s role using a process similar to
abduction. Two types of knowledge are exploited in this
process. First, roles with features which subsume or are
consistant with a proposed constituent are considered good
candidate roles. Also, roles may also be inferred by projecting
up from the roles already hypothesized for the children. By
intersecting these two sources of role evidence, the list of
hypothesized roles can be refined (by ruling out roles without
both types of evidence). In this manner the hypothesized roles
of later constituents can be refined from descendant
constituents. In the case of roles projected from daughters,
clausal boundary knowledge must be applied to correctly infer
the parent role.
</bodyText>
<sectionHeader confidence="0.987741" genericHeader="evaluation">
EVALUATION &amp; TESTING
</sectionHeader>
<bodyText confidence="0.999997333333333">
The techniques described here have been used successfully
to guide the parsing of several sentences taken from real
conversations. The pragmatic and semantic knowledge already
existed from Patten&apos;s research (Patten, 1992) to generate these
sentences. A subset of this knowledge, judged to represent the
partial knowledge available to a listener, was used to generate
expectations in the form described above.
The parser used in this study by default produced all
possible parses. The modified version attempts to converge on
the &amp;quot;expected&amp;quot; parse first, and terminate. For each sentence
tested the parser converges on the correct parse first. When the
expectations are modified to expect a different parse, a different
(and correct) parse is found first. The results in terms of
speedup vary considerably depending on the level of ambiguity
present in the sentence. The most complex sentence parsed
thus far exhibits considerable speedup. When unguided, the
parser produces 24 parses, and considers a total of 252 distinct
constituents. In the guided case, the parser only considers 39
constituents, and converges on the one &amp;quot;correct&amp;quot; parse first.
Within the current testing environment, this guidence results
in a greater then ten-fold speedup in terms of CPU time.
</bodyText>
<sectionHeader confidence="0.987102" genericHeader="conclusions">
SUMMARY
</sectionHeader>
<bodyText confidence="0.999957161290323">
Pragmatic knowledge about language usage in routine
conversational contexts can be highly compiled. This
knowledge can be used to produce semantic and syntactic
expectations about next turns in conversation, especially of
next turns that are second members of adjacency pairs
(Schegloff &amp; Sacks 1973). By mapping expected features into
HPSG constraints, and by augmenting HPSG sign structures
to model the role structure of systemic grammar, these
expectations can be used as constraints on possible constituent
structures of a HPSG constituent. Given this mapping, the
expectations may then be used to order the parse process,
guiding the parse, and avoiding the consideration of
pragmatically unlikely constructions. This process reduces the
number of constituents considered during parsing, reducing
parse time and permitting the parser to correctly select the
parse most like the pragmatic expectations.
This solution closely follows a classical Al. search
technique called a best-first search. The heuristic evaluation
function used to classify the proposed constituents for best
first ordering uses inference similar to abductive reasoning.
One benefit of this solution is that it retains the
modularity of the syntactic and semantic components, not
requiring a specialized grammar for each contextual domain. In
additional, as the coverage of the grammar increases, the search
space will also increase, and thus possible benefits increase.
Work is continuing on this study. Currently the heuristic
is being enhanced to consider the specificity of an expectation
match, ordering those edges which match the most specific
features first. In addition, work is in progress to extend the
coverage of the grammar and mapping to include the
conversation domain utilized in Patten, Geis &amp; Becker 1992.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996139555555556">
Geis, Mike. L. and Harlow, L. &amp;quot;Politeness Strategies in
French and English: Implications for Second Language
Acquisition&amp;quot;
Mac Gregor, R., &amp;quot;LOOM Users Manual&amp;quot;, University of
Southern California, Informations Sciences Institute,
1991.
Patten, Terry.; Geis, Mike. and Becker, Barbara., &amp;quot;Toward a
Theory of Compilation for Natural-Language Generation,&amp;quot;
Computational Intelligence 8(1), 1992, pp 77-101.
Pollard, Carl and Sag, Ivan A., &amp;quot;Head-Driven Phrase Structure
Grammar&amp;quot;, unpublished manuscript draft, 1992.
Pollard, Carl. and Sag, Ivan A., &amp;quot;Information-Based Syntax
and Semantics: Volume 1, Fundamentals&amp;quot;, Center for the
Study of Language and Information, 1987.
Schegloff, E.A. and Sacks, H. Opening up closings.
Semiotica, 7,4:289-387, 1973.
Winograd, Terry. 1983. &amp;quot;Language as a Cognitive Process&amp;quot;,
Addison-Wesley, Menlo Park, CA.
</reference>
<page confidence="0.997432">
297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.777483">
<title confidence="0.999725">Guiding an HPSG Parser using Semantic and Pragmatic Expectations</title>
<author confidence="0.852465">Skon</author>
<affiliation confidence="0.9885955">Computer and Information Science Department The Ohio State University</affiliation>
<address confidence="0.999998">Columbus, OH 43210, USA</address>
<email confidence="0.9967">Internet:skon@cis.ohio-state.edu</email>
<abstract confidence="0.990819444444444">Efficient natural language generation has been successfully demonstrated using highly compiled knowledge about speech acts and their related social actions. A design and prototype implementation of a parser which utilizes this same pragmatic knowledge to efficiently guide parsing is presented. Such guidance is shown to prune the search space and thus avoid needless processing of pragmatically unlikely constituent structures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>L</author>
<author>L Harlow</author>
</authors>
<title>Politeness Strategies in French and English: Implications for Second Language Acquisition&amp;quot;</title>
<marker>L, Harlow, </marker>
<rawString>Geis, Mike. L. and Harlow, L. &amp;quot;Politeness Strategies in French and English: Implications for Second Language Acquisition&amp;quot;</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mac Gregor</author>
<author>R</author>
</authors>
<title>LOOM Users Manual&amp;quot;,</title>
<date>1991</date>
<institution>University of Southern California, Informations Sciences Institute,</institution>
<marker>Gregor, R, 1991</marker>
<rawString>Mac Gregor, R., &amp;quot;LOOM Users Manual&amp;quot;, University of Southern California, Informations Sciences Institute, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Becker</author>
</authors>
<title>Toward a Theory of Compilation for Natural-Language Generation,&amp;quot;</title>
<date>1992</date>
<journal>Computational Intelligence</journal>
<volume>8</volume>
<issue>1</issue>
<pages>77--101</pages>
<contexts>
<context position="1708" citStr="Becker (1992)" startWordPosition="234" endWordPosition="235">The particular technique explored uses knowledge about the pragmatic context to order the consideration of proposed parse constituents, thus guiding the parser to consider the best (wrt the expectations) solutions first. Such a search may be classified as a bestfirst search. The theoretical models used to represent the pragmatic knowledge in this study are based on Halliday&apos;s Systemic Grammar and a model of the pragmatics of conversation. The model used to represent the syntax and domain independent semantic knowledge is HPSG - Head-driven Phrase Structure Grammar. BACKGROUND Patten, Geis and Becker (1992) demonstrate the application of knowledge compilation to achieve the rapid generation of natural language. Their mechanism is based on Halliday&apos;s systemic networks, and on Geis&apos; theory of the pragmatics of conversation. A model of conversation using principled compilation of pragmatic knowledge and other linguistic knowledge is used to permit the application of pragmatic inference without expensive computation. A pragmatic component is used to model social action, including speech acts, and utilize conventions of use involving such features of context such as politeness, mister, and stylistic </context>
</contexts>
<marker>Becker, 1992</marker>
<rawString>Patten, Terry.; Geis, Mike. and Becker, Barbara., &amp;quot;Toward a Theory of Compilation for Natural-Language Generation,&amp;quot; Computational Intelligence 8(1), 1992, pp 77-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar&amp;quot;,</title>
<date>1992</date>
<note>unpublished manuscript draft,</note>
<contexts>
<context position="3125" citStr="Pollard and Sag, 1992" startWordPosition="440" endWordPosition="443">chy. A planner component uses knowledge about conditions which need to be satisfied (discourse goals) to produce a set of pragmatic features which characterize a desired utterance. These features are mapped into the systemic 1 Research Funded by The Ohio State Center for Cognitive Science and The Ohio State Departments of Computer and Information Science and Linguistics grammar (using compiled knowledge) which is then used to realize the actual utterance. The syntactic/semantic component used in this study is a parser based on the HPSG (Head Driven Phrase Structure Grammar) theory of grammar (Pollard and Sag, 1992). HPSG models all linguistic constituents in terms of partial information structures called feature structures. Linguistic signs incorporate simultaneous representation of phonological, syntactic, and semantic attributes of grammatical constituents. HPSG is a lexicalized theory, with the lexical definitions, rather then phrase structure rules, specifying most configurational constraints. Control (such as subcategorization, for example) is asserted by the use of HPSG constraints - partially filled in feature structures called feature descriptions, which constrain possible HPSG feature structure</context>
</contexts>
<marker>Pollard, Sag, 1992</marker>
<rawString>Pollard, Carl and Sag, Ivan A., &amp;quot;Head-Driven Phrase Structure Grammar&amp;quot;, unpublished manuscript draft, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
</authors>
<title>Information-Based Syntax and Semantics: Volume 1, Fundamentals&amp;quot;, Center for the Study of Language and Information,</title>
<date>1987</date>
<marker>Sag, 1987</marker>
<rawString>Pollard, Carl. and Sag, Ivan A., &amp;quot;Information-Based Syntax and Semantics: Volume 1, Fundamentals&amp;quot;, Center for the Study of Language and Information, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Schegloff</author>
<author>H Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<pages>7--4</pages>
<contexts>
<context position="12466" citStr="Schegloff &amp; Sacks 1973" startWordPosition="1876" endWordPosition="1879">, the parser produces 24 parses, and considers a total of 252 distinct constituents. In the guided case, the parser only considers 39 constituents, and converges on the one &amp;quot;correct&amp;quot; parse first. Within the current testing environment, this guidence results in a greater then ten-fold speedup in terms of CPU time. SUMMARY Pragmatic knowledge about language usage in routine conversational contexts can be highly compiled. This knowledge can be used to produce semantic and syntactic expectations about next turns in conversation, especially of next turns that are second members of adjacency pairs (Schegloff &amp; Sacks 1973). By mapping expected features into HPSG constraints, and by augmenting HPSG sign structures to model the role structure of systemic grammar, these expectations can be used as constraints on possible constituent structures of a HPSG constituent. Given this mapping, the expectations may then be used to order the parse process, guiding the parse, and avoiding the consideration of pragmatically unlikely constructions. This process reduces the number of constituents considered during parsing, reducing parse time and permitting the parser to correctly select the parse most like the pragmatic expect</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Schegloff, E.A. and Sacks, H. Opening up closings. Semiotica, 7,4:289-387, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Language as a Cognitive Process&amp;quot;, Addison-Wesley,</title>
<date>1983</date>
<location>Menlo Park, CA.</location>
<marker>Winograd, 1983</marker>
<rawString>Winograd, Terry. 1983. &amp;quot;Language as a Cognitive Process&amp;quot;, Addison-Wesley, Menlo Park, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>